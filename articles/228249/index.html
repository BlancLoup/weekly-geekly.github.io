<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Probabilistic models: from naive Bayes to LDA, part 1</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="We continue the conversation. The last article was a transition from the previous cycle about graphic models in general ( part 1 , part 2 , part 3 , p...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Probabilistic models: from naive Bayes to LDA, part 1</h1><div class="post__text post__text-html js-mediator-article">  We continue the conversation.  <a href="http://habrahabr.ru/company/surfingbird/blog/226677/">The last article</a> was a transition from the previous cycle about graphic models in general ( <a href="http://habrahabr.ru/company/surfingbird/blog/176461/">part 1</a> , <a href="http://habrahabr.ru/company/surfingbird/blog/177889/">part 2</a> , <a href="http://habrahabr.ru/company/surfingbird/blog/185622/">part 3</a> , <a href="http://habrahabr.ru/company/surfingbird/blog/188812/">part 4</a> ) to a new mini-cycle about thematic modeling: we talked about sampling as a method of inference in graphic models.  And now we begin the path to the Dirichlet latent allocation model (latent Dirichlet allocation) and how all these wonderful sampling algorithms are used in practice.  Today - the first part, in which we will understand where it makes sense to generalize the naive Bayes classifier, and at the same time we will talk a little about clustering. <br><br><img width="600" src="https://habrastorage.org/getpro/habr/post_images/779/c11/2ed/779c112ed2cf4fce27069dfc608f34ac.jpg"><br><a name="habracut"></a><br><br><h3>  Classification: Naive Bayes </h3><br>  Thematic modeling solves the classical text analysis problem ‚Äî how to create a probabilistic model of a large collection of texts, which can then be used, for example, for information retrieval, classification, or, in our case, for content recommendations.  We <a href="http://habrahabr.ru/company/surfingbird/blog/150207/">have already spoken</a> about one of the simplest models trying to solve this problem ‚Äî the well-known naive Bayes classifier.  You can, of course, classify texts and differently - using the support vector method, for example, or logistic regression, or any other classifier - but naive Bayes will be most useful to us now as an example for further generalization. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In the naive Bayes classifier model, each document is assigned a hidden variable corresponding to its theme (taken from a predetermined discrete set, for example, "finance", "culture", "sport"), and the words are conditionally independent with each other for the topic.  Each topic corresponds to a discrete distribution in the words from which they originate;  we simply assume that the words are conditionally independent subject to the subject: <br><img width="600" src="https://habrastorage.org/getpro/habr/post_images/838/a6d/812/838a6d8127f712a1499d2cbe5b6bdd06.png"><br>  As a result, each topic is a discrete distribution in words.  You can imagine a huge dishonest dice, which you throw to get the next word, or a bag of words, in which you, without looking, launch your hand to pull out the next one.  A dishonest bag, however, is harder to imagine - well, let's say, every word on pebbles in a bag occurs several times, and different words are different (all this intuition will still come in handy, bear).  Do you have one bag that says "finance", the other - "culture", the third - "sport": <br><img width="600" src="https://habrastorage.org/getpro/habr/post_images/921/010/e64/921010e64fae5a789cb6d9cf510b4bde.png"><br>  And when you ‚Äúgenerate‚Äù from these bags a new document in naive Bayes, you first choose a bag (throwing a die), and then from this bag you start to choose words one by one: <br><br><img width="600" src="https://habrastorage.org/getpro/habr/post_images/ceb/066/383/ceb066383eec59931a2e6ae3a54a719f.png"><br><br>  The graphic model of this process looks very simple (we talked about how to read these pictures <a href="http://habrahabr.ru/company/surfingbird/blog/176461/">in the first part of the</a> previous cycle, and in the <a href="http://habrahabr.ru/company/surfingbird/blog/177889/">second part there was</a> even an example of the naive Bayes classifier).  On the left, the graphical model is complete, in the center is a model of a single document with a plate that hides repeated identical variables, and on the right is the same model, but for the entire dataset, with explicitly selected Œ± and Œ≤ variables that contain the parameters of all these discrete distributions: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9a3/9a1/514/9a39a15141f97a5fea8efa50953fdaee.png"><img src="https://habrastorage.org/getpro/habr/post_images/ce5/bf2/00f/ce5bf200ffaa5468c88a7c7a4e6d89c6.png"><img src="https://habrastorage.org/getpro/habr/post_images/d72/4af/57a/d724af57afa7d50453b1658360fdfd64.png"><br><br>  It also shows that naive Bayes consists of one discrete distribution (cube), which determines the comparative "weight" of topics, and several discrete distributions (bags), according to the number of topics that show the probabilities of a given word in a topic. <br><br>  And in order to teach the naive Bayes (and any other classifier, you can‚Äôt get anywhere), you need to have a marked set of texts on which you can train these distributions: <br><br><img width="600" src="https://habrastorage.org/getpro/habr/post_images/ec6/e35/567/ec6e35567079b85ae8412827174db7f5.png"><br><br>  When teaching naive Bayes, for each document we know its subject, and it remains to teach only the distribution of words in each topic separately and the probability of the occurrence of topics: <br><img width="600" src="https://habrastorage.org/getpro/habr/post_images/6c6/380/327/6c6380327319525de59c536d6dd1c5b6.png"><br><br>  To do this, it is enough just to calculate how many times a particular word has been encountered in a particular topic (and how many times the topics occur in a dataset), and smooth the result in Laplace. <br><br>  The naive Bayes classifier can immediately see two directions in which it could be expanded and improved: <ul><li>  How necessary is it to have a marked dataset?  It‚Äôs very expensive to mark up datasets with your hands, and dataset is needed big enough because we teach the distribution of all words in a given topic;  maybe you can somehow so without tags, if skillfully? </li><li>  in real datasets, unless tweets will have one well-defined topic;  For example, the news ‚Äú <i>defender of Chelsea and the Brazilian national team David Louis was sold in the PSG for more than 40 million pounds sterling</i> ‚Äù fairly spoil our model, because here the words about sports and finance are combined, and as a result, the sports theme will get ‚Äú contamination "from financial words, or vice versa;  Is it possible to make one document have several topics? </li></ul>  These extensions will lead us gradually to the LDA model. <br><br><h3>  From classification to clustering </h3><br>  It will be logical to start from the first direction, on how to proceed to the processing of datasets without tags, because even if we make a model in which each document has several topics, mark out a large one with hands, and even several topics for one document, it will be extremely is difficult.  So let's imagine that we have a dataset consisting of texts, and we assume, as in naive Bayes, that each document was generated from a single topic.  The only difference is that now we do not know which of them, and the task for us looks like this: <br><br><img width="600" src="https://habrastorage.org/getpro/habr/post_images/0d6/b5f/3b8/0d6b5f3b8b1a35599412154d8801c93b.png"><br><br>  Let's assume (we will do this in LDA too; in order to get rid of it, we need non-parametric methods, which we are still far away from) that we know the number of potential categories (different bags with words), only their content is unknown.  Thus, the probabilistic model looks exactly the same: <br><img src="https://habrastorage.org/getpro/habr/post_images/d72/4af/57a/d724af57afa7d50453b1658360fdfd64.png">  , <br>  and the distribution in it is the same.  The only difference is that we used to know its subject matter for each document, but now we don‚Äôt know.  In other words, the joint distribution, from which documents are generated, is the same: if we denote by <b>w the</b> vector of words and by <i>c the</i> category of document <i>D</i> , its general credibility will be <br><img src="https://habrastorage.org/getpro/habr/post_images/724/442/6aa/7244426aabee0ceb17fbd8f0b7bbf88b.png"><br>  where Œ≤ <sub><i>c</i></sub> is the distribution in the bag of words corresponding to category <i>c</i> ;  and the overall likelihood that needs to be maximized when training is, respectively, <br><img src="https://habrastorage.org/getpro/habr/post_images/eba/a96/55a/ebaa9655aff4c609560c2ef67aca49f3.png"><br>  and maximize it must still Œ± and Œ≤ <sub><i>c</i></sub> .  But if earlier we did this with known <i>c</i> , and the maximization problem was divided into separate topics and was essentially trivial, then now we need to maximize with unknown <i>c</i> , i.e.  actually taking a wait on them: <br><img src="https://habrastorage.org/getpro/habr/post_images/80a/a27/c94/80aa27c94ef78a543215c2d1511fd69a.png"><br>  How to do it? <br><br>  What we have done is actually a completely standard <i>clustering</i> task: there are many objects (texts) in a multidimensional space (words), and we need to break them up into separate classes so that the texts in one class would be as similar as possible to each other. at the other and at the same time as less as possible similar to the texts in other classes.  So a standard tool for learning clustering models can help here - EM-algorithm (from the words Expectation-Maximization, now understand why).  The EM algorithm is designed for situations where there are hidden variables in the model (as we have <i>c</i> ), and we need to maximize the likelihood of the model by its parameters (we have Œ± and Œ≤), but waiting for the hidden variables.  The EM algorithm first initializes the model with some initial values, and then iteratively repeats two steps: <ul><li>  <i>E-step</i> - to find expectations (expectation) of hidden variables with this model; </li><li>  <i>The M-step</i> is to maximize the likelihood (maximization), considering the variables equal to their expectations found in the E-step. </li></ul><br>  In our case, at the E-step, we simply find the probabilities that each document belongs to different categories, with fixed Œ± and Œ≤: <br><img src="https://habrastorage.org/getpro/habr/post_images/fe6/3f0/f56/fe63f0f565f2b4cbae5797b6ce714608.png"><br>  And then at the M-step we consider these membership probabilities to be fixed and optimize Œ± and Œ≤ (preferably with smoothing Œ± <sub>0</sub> and Œ≤ <sub>0</sub> corresponding to certain a priori distributions on parameters Œ± and Œ≤): <br><img src="https://habrastorage.org/getpro/habr/post_images/826/0a5/e65/8260a5e656907b74221266de7fe62bd0.png"><img src="https://habrastorage.org/getpro/habr/post_images/650/920/39b/65092039b902c1210502efb55f4a3ea2.png"><br>  Then all this needs to be repeated until Œ± and Œ≤ converge;  this is the EM algorithm for clustering applied to the case of discrete attributes (words in documents). <br><br>  The general idea of ‚Äã‚Äãwhy the EM algorithm actually works is this: at every E-step, the EM algorithm actually builds an auxiliary function of the model parameters, which in the current approximation concerns the likelihood function and remains less than it everywhere (it minimizes the likelihood function) .  Here is a picture from one of the sources on this topic (frankly, I forgot which one): <br><img width="600" src="https://habrastorage.org/getpro/habr/post_images/acf/a3a/07c/acfa3a07cdf2f97b5ea780c3d312e399.png"><br>  And then at the M-step, the EM algorithm finds the parameters that maximize this auxiliary function.  Obviously, with this we move to a point at which the overall likelihood increases.  I will not go into detail now in the proof that the EM algorithm does all this and really works correctly, only the general idea is important to us. <br><br><h3>  What's next </h3><br>  Today we took the first of two steps from the naive Bayes classifier to the LDA: we moved from the classification task, in which each document in the training dataset should have a fixed category, to the clustering task, in which documents do not have to be categorized.  We note, by the way, that if you still have <i>part of the</i> documents marked up (this is called semi-supervised learning), it is very easy to add to the resulting model.  You just need to fix the corresponding variables <i>c</i> <sub><i>i</i></sub> ( <i>D</i> ), make them equal to one in the marked up topic and zero in all the others, and not to train these variables within the EM-algorithm, leaving their values ‚Äã‚Äãfixed.  Such a ‚Äúseed dataset‚Äù will, by the way, be very useful for further training of the model. <br><br>  Next time we will take the second step towards the latent placement of Dirichlet: from the categories assigned to the entire text, we turn to topics that can be several in each document. </div><p>Source: <a href="https://habr.com/ru/post/228249/">https://habr.com/ru/post/228249/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../228237/index.html">Home sales of Epson Moverio BT-200 video glasses</a></li>
<li><a href="../228239/index.html">Expansion of anti-piracy law. Block content will be without a court order</a></li>
<li><a href="../228241/index.html">Protecting .NET Applications with the Sentinel LDK Envelope</a></li>
<li><a href="../228243/index.html">It's time to learn to neglect gadgets</a></li>
<li><a href="../228247/index.html">OMG Scala is a complex language!</a></li>
<li><a href="../228251/index.html">ABBYY FineReader (2/2) text recognition</a></li>
<li><a href="../228253/index.html">Features of using SailsJS for beginners (Part 1)</a></li>
<li><a href="../228255/index.html">Rostec will open an office in Innopolis</a></li>
<li><a href="../228261/index.html">Formed a preliminary program WebCamp: Mobile Day</a></li>
<li><a href="../228265/index.html">Badoo reports from RIT 2014 conference</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>