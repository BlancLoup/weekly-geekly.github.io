<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>In three articles on least squares: literacy on the theory of probability</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A year and a half ago, I published an article ‚ÄúMathematics on the fingers: the methods of least squares,‚Äù which received a very decent response, which...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>In three articles on least squares: literacy on the theory of probability</h1><div class="post__text post__text-html js-mediator-article">  A year and a half ago, I published an article <a href="https://habr.com/post/277275/">‚ÄúMathematics on the fingers: the methods of least squares,‚Äù</a> which received a very decent response, which, among other things, consisted in what I proposed to draw an owl.  Well, once an owl, then you need to explain again.  In a week exactly on this topic I will begin to give several lectures to geological students;  I take this opportunity to present here the (adapted) main points as a draft.  My main goal is not to give a ready-made recipe from a book about tasty and healthy food, but to tell why it is and what else is in the appropriate section, because the connection between different sections of mathematics is the most interesting! <br><br>  At the moment I intend to break the text into three articles: <br><br><ul><li>  <b>1. Educational program on the theory of probability and how it is connected with the methods of least squares</b> </li><li>  <a href="https://habr.com/post/429980/">2. The smallest squares, the simplest case,</a> and how to program them </li><li>  3. Nonlinear problems </li></ul><br>  I will go to the smallest squares a little to the side, through the maximum likelihood principle, and it requires minimal orientation in probability theory.  This text is intended for the third year of our Faculty of Geology, which means (from the point of view of the involved hardware!) That an interested high school student with appropriate diligence should be able to figure it out. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1>  How justified is the theorever or do you believe in the theory of evolution? </h1><br>  Once I was asked if I believed in the theory of evolution.  Pause right now, think about how you answer it. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/df1/f2b/6c0/df1f2b6c06daa3cf6d1e145eef8254be.png"><br><a name="habracut"></a><br>  Personally, I was taken aback, replied that I find it plausible, and that the question of faith does not arise here at all.  Scientific theory has little to do with faith.  In short, the theory only builds a model of the world around us, there is no need to believe in it.  Moreover, <a href="https://ru.wikipedia.org/wiki/%25D0%25A4%25D0%25B0%25D0%25BB%25D1%258C%25D1%2581%25D0%25B8%25D1%2584%25D0%25B8%25D1%2586%25D0%25B8%25D1%2580%25D1%2583%25D0%25B5%25D0%25BC%25D0%25BE%25D1%2581%25D1%2582%25D1%258C">the Popper criterion</a> requires scientific theory to be able to rebut.  And still the well-founded theory should possess, first of all, predictive power.  For example, if you genetically modify crops in such a way that they will produce pesticides themselves, then it is logical that insects resistant to them will appear.  However, it is significantly less obvious that this process can be slowed down by growing ordinary plants side by side with genetically modified ones.  Based on the theory of evolution, the corresponding modeling made <a href="https://www.sciencedirect.com/science/article/pii/S0167880903002858%3Fvia%253Dihub">such a prediction</a> , and it seems to be <a href="https://www.nature.com/articles/nbt1382">confirmed</a> . <br><br><h5>  And what have the smallest squares? </h5><br>  As I mentioned earlier, I will go to the smallest squares through the maximum likelihood principle.  Let's illustrate with an example.  Suppose we are interested in data on the growth of penguins, but we can measure only a few of these beautiful birds.  It is quite logical to introduce a growth distribution model into the task - most often it is normal.  The normal distribution is characterized by two parameters ‚Äî the mean value and the standard deviation.  For each fixed parameter value, we can calculate the probability that exactly the measurements that we made will be generated.  Further, by varying the parameters, we find those that maximize the probability. <br><br>  Thus, to work with maximum likelihood, we need to operate in terms of probability theory.  Below we define the notion of probability and likelihood ‚Äúon the fingers‚Äù, but first I would like to focus on another aspect.  I surprisingly rarely see people who think about the word "theory" in the phrase "probability theory." <br><br><h5>  What is studying teverver? </h5><br>  As to the sources, meanings and range of applicability of probabilistic estimates, fierce disputes have been going on for more than a hundred years.  For example, <a href="https://en.wikipedia.org/wiki/Bruno_de_Finetti">Bruno De Finetti</a> stated that probability is nothing but a subjective analysis of the likelihood that something will happen, and that this probability does not exist outside the mind.  This is a person's willingness to bet on something happening.  This opinion is the exact opposite of the <a href="https://en.wikipedia.org/wiki/Frequentist_probability">classic / frequentists'</a> view of the probability of a specific outcome of an event, in which it is assumed that the same event can be repeated many times, and the ‚Äúprobability‚Äù of a particular result is related to the frequency of a particular result during repeated tests.  In addition to subjectivists and frequentists, there are also objectivists who assert that probabilities are real aspects of the universe, and not just descriptions of the degree of confidence of the observer. <br><br>  Be that as it may, but all three scientific schools in practice use the same apparatus based on Kolmogorov's axioms.  Let's give an indirect argument, from a subjectivist point of view, in favor of probability theory, built on the axioms of Kolmogorov.  We will give the axioms themselves a bit later, but first let us assume that we have a bookmaker who accepts bets on the next World Cup.  Let us have two events: a = the champion will be the team of Uruguay, b = the champion will be the team of Germany.  The bookmaker estimates the chances of the Uruguay team to win at 40%, the chances of the German team at 30%.  Obviously, both Germany and Uruguay cannot win at the same time, therefore the chance a‚àßb is zero.  Well, at the same time, the bookmaker believes that the probability that either Uruguay will win, or Germany (and not Argentina or Australia) is 80%.  Let's write this in the following form: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b01/b95/384/b01b9538401e50ea965ee6592f3981a1.png"><br><br>  If the bookmaker claims that his degree of confidence in the event <i>a</i> is equal to 0.4, that is, <i>P (a)</i> = 0.4, then the player can choose whether he will bet for or against the statement <i>a</i> , setting amounts that are compatible with the degree of confidence of the bookmaker.  This means that the player can bet that the event <i>a</i> will occur by placing four rubles against the six rubles of the bookmaker.  Or, the player can bet six rubles instead of four rubles of the bookmaker that the event will not happen. <br><br>  If the bookmaker‚Äôs degree of confidence does not accurately reflect the state of the world, then one can expect that in the long run he will lose money to players whose beliefs are more accurate.  Moreover, in this particular example, the player has a strategy in which the bookmaker <b>always</b> loses money.  Let's illustrate it: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/87c/97d/a15/87c97da153047340db4a3bbfb078c308.png"><br><br>  The player makes three bets, and no matter what the outcome of the championship, he always wins.  Please note that in consideration of the winning amount, in principle, it does not include whether Uruguay or Germany are favorites of the championship, the bookmaker‚Äôs loss is guaranteed!  This situation was caused by the fact that the bookmaker was not guided by the basics of the theory of probability, violating the third Kolmogorov axiom, let us give them all three: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7ba/3c9/f5a/7ba3c9f5aaaf940ba9948be6bd5d23be.png"><br><br>  In text form, they look like this: <br><br><ul><li>  1. All probabilities are in the range from 0 to 1 </li><li>  2. Certainly true statements have a probability of 1, and certainly a false probability of 0. </li><li>  3. The third axiom is an axiom of disjunction, it is easy to understand it intuitively, noting that those cases when the statement <i>a</i> is true, together with those cases when <i>b</i> is true, unconditionally covers all those cases when the true statement a‚à®b;  but in the sum of two sets of cases, their intersection occurs twice, so it is necessary to subtract P (ab). </li></ul><br>  In 1931, de Finetti <a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25BE%25D0%25B3%25D0%25B5%25D1%2580%25D0%25B5%25D0%25BD%25D1%2582%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D1%258C_(%25D1%2584%25D0%25B8%25D0%25BB%25D0%25BE%25D1%2581%25D0%25BE%25D1%2584%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F_%25D1%2581%25D0%25BF%25D0%25B5%25D0%25BA%25D1%2583%25D0%25BB%25D1%258F%25D1%2582%25D0%25B8%25D0%25B2%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D1%2582%25D1%2580%25D0%25B0%25D1%2582%25D0%25B5%25D0%25B3%25D0%25B8%25D1%258F)">proved a</a> very strong statement: <br><blockquote>  If the bookmaker is guided by a set of degrees of confidence that violates the axioms of probability theory, then there is such a combination of player bets that guarantees the loss of the bookmaker (player's win) at each bet. <br></blockquote><br>  Probability axioms can be considered as limiting the set of probabilistic beliefs that an agent can hold.  Please note that following the bookmaker Kolmogorov's axioms do not imply that he will win (we leave aside commission questions), but if they are not followed, he will be guaranteed to lose.  Note that other arguments have been put forward in favor of applying probabilities;  but it was precisely the <i>practical</i> success of systems of formation of reasoning based on probability theory that turned out to be an attractive stimulus that caused a revision of many views. <br><br>  So, we slightly lifted the veil of <b>why the</b> Teverver might make sense, but what exactly objects does it manipulate?  The whole theory is built only on three axioms;  in all three, some magic function <i>P is</i> involved.  Moreover, looking at these axioms, it reminds me a lot of the function of the area of ‚Äã‚Äãa figure.  Let's try to see if a square works to determine probability. <br><br>  We define the word "event" as a "subset of unit square."  We define the word ‚Äúevent probability‚Äù as ‚Äúthe area of ‚Äã‚Äãthe corresponding subset‚Äù.  Roughly speaking, we have a large cardboard target, and we close our eyes and shoot at it.  The chances of a bullet falling into a given set are directly proportional to the area of ‚Äã‚Äãthe set.  A reliable event in this case is the whole square, and a deliberately false, for example, any point of the square.  From our definition of probability, it follows that ideally it is impossible to hit a point (our bullet is a material point).  I really like pictures, and draw a lot of them, and theorever is no exception!  Let's illustrate all three axioms: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/205/9fc/35e/2059fc35ef9996e3410786a23ba71570.png"><br><br>  So, the first axiom is fulfilled: the area is non-negative and cannot exceed unity.  A credible event is the whole square, and a deliberately false one is any set of zero area.  And it works great with diszyunkitsy! <br><br><h1>  Maximum likelihood by example </h1><br><h5>  Example one: coin flip </h5><br>  Let's look at the simplest example of a coin flip, aka <a href="https://en.wikipedia.org/wiki/Bernoulli_scheme">Bernoulli scheme</a> .  <i>N</i> experiments are conducted, in each of which one of two events can occur (‚Äúsuccess‚Äù or ‚Äúfailure‚Äù), one with probability <i>p</i> , the second with probability <i>1-p</i> .  Our task is to find the probability of obtaining exactly <i>k</i> successes in these <i>n</i> experiments.  This probability gives us the Bernoulli formula: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ce6/9b2/2ef/ce69b22ef6bfbfa7e2e2276ea3b8df8c.png"><br><br>  Take a regular coin ( <i>p = 0.5</i> ), throw it ten times ( <i>n = 10</i> ), and count how many times the tail falls: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d1a/78d/d1e/d1a78dd1e213244a7b1c31ab6f1381d4.png"><br><br>  This is how the probability density plot looks like: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/86b/a6e/4bc/86ba6e4bcb12aaadb5fa1f0e89a4f699.png"><br><br>  Thus, if we recorded the likelihood of ‚Äúsuccess‚Äù (0.5), and also recorded the number of experiments (10), then the possible number of ‚Äúsuccesses‚Äù can be any integer between 0 and 10, but these outcomes are not equally probable.  It is quite obvious that getting five ‚Äúsuccesses‚Äù is much more likely than none.  For example, the probability to count seven tails is approximately 12%. <br><br>  And now let's look at the same task from the other side.  We have a real coin, but we don‚Äôt know its distribution of a priori probability of ‚Äúsuccess‚Äù / ‚Äúfailure‚Äù.  However, we can throw it ten times and count the number of "success."  For example, we had seven tails.  How does this help us evaluate <i>p</i> ? <br><br>  We can try to fix <i>n</i> = 10 and <i>k</i> = 7 in the Bernoulli formula, leaving <i>p as the</i> free parameter: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f89/4de/196/f894de1965d550e7508a2b2e83d17901.png"><br><br>  Then the Bernoulli formula can be interpreted as the <i>likelihood of the</i> parameter being estimated, (in this case <i>p</i> ).  I even changed the letter in the function, now it is <i>L</i> (from the English. Likehood).  That is, the likelihood is the probability of generating observation data (7 tails of 10 experiments) for a given value of the parameter (s). <br><br>  For example, the likelihood of a balanced coin ( <i>p</i> = 0.5) under the condition that seven tails fall out of ten throws is approximately equal to 12%.  You can plot the function <i>L</i> : <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bca/da0/5e8/bcada05e8fdd206495376852f88aafb8.png"><br><br>  So, we are looking for a value of the parameters that maximizes the likelihood of obtaining those observations that we have.  In this particular case, we have a function of one variable, we are looking for its maximum.  To make it easier to search, I will look for a maximum not of <i>L</i> , but <i>log L.</i>  The logarithm is a strictly monotonic function, so maximization of the one and the other is strictly the same.  And the logarithm of us breaks the product into an amount that is much more convenient to differentiate.  So, we are looking for the maximum of this function: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/915/407/504/915407504c2f92b808ff7c0bd8319320.png"><br><br>  To do this, equate its derivative to zero: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/43b/428/a01/43b428a015cd2f25e188751c0c553418.png"><br><br>  Derivative log x = 1 / x, we get: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d0f/8b7/e30/d0f8b7e3049a4d37fc1489c3a02003d6.png"><br><br>  That is, the maximum likelihood (approximately 27%) is reached at <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0e4/ee1/229/0e4ee12290100d8657e411e2719ed57d.png"><br><br>  Just in case, we calculate the second derivative: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/96f/5de/ba4/96f5deba4461fd3360dbefb29713c802.png"><br><br>  At the point p = 0.7, it is negative, so this point is indeed the maximum of the function L. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/931/bfe/6c9/931bfe6c9f2fb93c0c9ec7036a67b74c.png"><br><br>  And this is the probability density for the Bernoulli scheme with <i>p</i> = 0.7: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c33/90c/180/c3390c180591b89bda8487b04f97ff66.png"><br><br><h5>  Example Two: ADC </h5><br>  Let's imagine that we have a certain constant physical quantity that we want to measure, be it with a ruler or voltage with a voltmeter.  Any measurement gives an <b>approximation of</b> this value, but not the value itself.  The methods that I describe here were developed by Gauss in the late 18th century, when he measured the orbits of celestial bodies. <br><br>  For example, if we measure the battery voltage N times, we get N different measurements.  Which one to take?  Everything!  So, let us have N values ‚Äã‚Äãof Uj: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f5d/7c6/821/f5d7c6821d43ed3b9bf9128d52876b2e.png"><br><br>  Suppose that each dimension of Uj is equal to the ideal value, plus Gaussian noise, which is characterized by two parameters ‚Äî the position of the Gaussian bell and its ‚Äúwidth‚Äù.  Here is the probability density: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ded/0d3/b40/ded0d3b40c8478f5e7a01889b5c8b100.png"><br><br>  That is, having N given values ‚Äã‚Äãof Uj, our task is to find such a parameter, U, which maximizes the likelihood value.  Likelihood (I immediately take the logarithm from it) can be written as follows: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/49c/ad9/a23/49cad9a2301ad0795567364befad5ea5.png"><br><br>  Well, after that everything is strictly as before, we equate the partial derivatives with respect to the parameters that we are looking for: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4fe/d44/7d5/4fed447d51696fde602a9db5a7fe816e.png"><br><br>  We obtain that the most probable estimate of the unknown quantity U can be found as the average of all measurements: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b46/281/4f1/b462814f1a8a19167396798177a42ee5.png"><br><br>  But the most likely sigma parameter is the usual standard deviation: <br><img src="https://habrastorage.org/getpro/habr/post_images/3c5/a74/194/3c5a741948e8a96dbaac24244c7daabd.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/e3f/17f/422/e3f17f4222ee89afaecaffde3453dfec.png"><br><br>  Was it worth bothering to get a simple average of all measurements?  For my taste, it was worth it.  By the way, averaging multiple measurements of a constant value in order to increase the accuracy of measurements is quite standard practice.  For example, <a href="https://www.silabs.com/documents/public/application-notes/an118.pdf">ADC averaging</a> .  By the way, for this Gaussian noise is not necessary, it is enough that the noise is unbiased. <br><br><h5>  Example three, and again one-dimensional </h5><br>  Continuing the conversation, let's take the same example, but make it a bit more complicated.  We want to measure the resistance of a certain resistor.  With the help of a laboratory power supply, we are able to pass through it a certain reference amount of amps, and measure the voltage that will be needed for this.  That is, we will have N pairs of numbers (Ij, Uj) at the input of our appraiser. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2b6/3c7/af0/2b63c7af0ec6024be16c1c4bd4a28dbc.png"><br><br>  Draw these points on the graph;  Ohm's law tells us that we are looking for the slope of the blue line. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e4c/211/d7f/e4c211d7f3c34742ac53e18461818691.png"><br><br>  Write the expression for the likelihood of the parameter R: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/655/c9c/b8d/655c9cb8d1cb7429abd317016d09ce47.png"><br><br>  Again, we equate the corresponding partial derivative to zero: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e7e/737/2bb/e7e7372bbb5f7f138e0e5858039617c7.png"><br><br>  Then the most plausible resistance R can be found by the following formula: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/529/877/57b/52987757bc0127158af6e642591b35d7.png"><br><br>  This result is somewhat less obvious than the simple average of all measurements.  Please note that if we make a hundred measurements in the region of one amp, and one measurement in the kiloampere region, then the previous hundred measurements will have almost no effect on the result.  Let's remember this fact, it will be useful to us in the next article. <br><br><h1>  Fourth example: back to the smallest squares </h1><br>  Surely, you have already noticed that in the last two examples, maximizing the likelihood logarithm is equivalent to minimizing the sum of squares of the estimation error.  Let's look at another example.  Take a calibration balance with the help of reference loads.  Suppose we have N reference weights of mass xj, hang them on the balance sheet and measure the length of the spring, we get N spring lengths yj: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2ca/4c1/d86/2ca4c1d86e78cd4fd2d6f9d7b3052777.png"><br><br>  Hooke's law tells us that the spring tension linearly depends on the applied force, and this force includes the weight of the weights and the weight of the spring itself.  Let the spring stiffness is the parameter <i>a</i> , well, and the spring tension under its own weight is the parameter b.  Then we can write down the likelihood expression of our measurements like this (still under the hypothesis of Gaussian measurement noise): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ce4/e81/011/ce4e81011eae69c44d9e88cfe82fc09a.png"><br><br>  Maximizing the likelihood L is equivalent to minimizing the sum of squares of estimation errors, that is, we can search for the minimum of the function S, defined as follows: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/18e/272/2eb/18e2722ebe081756bd8e871588361d5f.png"><br><br>  In other words, we are looking for a line that minimizes the sum of the squares of the lengths of the green segments: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c5a/a80/f6a/c5aa80f6a2e9575abfa7b3dfdabf5c5a.png" width="300"><br><br>  Well, then no surprises, we equate to partial derivatives: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b09/a29/620/b09a29620438327e5a5bffb827a5dfa6.png"><br><br>  We obtain a system of two linear equations with two unknowns: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0a9/d2f/f75/0a9d2ff75a4cb635434dc1e7eb267699.png"><br><br>  We recall the seventh grade of the school and write out the solution: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/806/982/b11/806982b116726fcc07d1d4935efa3970.png"><br><br><h1>  Conclusion </h1><br>  The least squares methods are a particular case of maximizing the likelihood for those cases where the probability density is Gaussian.  In the case when the density is (not at all) Gaussian, the OLS give an estimate that differs from the MLE (maximum likehood estimation) estimate.  By the way, at one time Gauss hypothesized that the distribution does not play a role, only the independence of the tests is important. <br><br>  As can be seen from this article, the further into the forest, the more cumbersome are the analytical solutions to this problem.  Well, we are not in the eighteenth century, we have computers!  Next time we will see a geometric and, then, a programmer approach to the OLS problem, stay on the line. </div><p>Source: <a href="https://habr.com/ru/post/428768/">https://habr.com/ru/post/428768/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../428756/index.html">Finishing Up Persons Using Machine Learning</a></li>
<li><a href="../428758/index.html">Missing computer skills at high school students</a></li>
<li><a href="../428760/index.html">How to stretch the "gzom" editor: podcast "GLPH"</a></li>
<li><a href="../428762/index.html">Product Design Digest October 2018</a></li>
<li><a href="../428766/index.html">The digest of fresh materials from the world of the frontend for the last week ‚Ññ337 (October 29 - November 4, 2018)</a></li>
<li><a href="../428770/index.html">Keyboard macros for everyday tasks</a></li>
<li><a href="../428772/index.html">Democratization of data in Uber</a></li>
<li><a href="../428774/index.html">GPS firewall for data centers - why you need it and how it works</a></li>
<li><a href="../428776/index.html">New realization of curiosity in AI. Training with a reward that depends on the difficulty of predicting the outcome</a></li>
<li><a href="../428778/index.html">See invisible. Near infrared range (0.9-1.7¬µm)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>