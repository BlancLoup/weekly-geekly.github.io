<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Articles underlying Facebook's vision for computer vision</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Do you know such a company - Facebook? Yes, yes, the one with a website of 1.6 billion users. And if you take all the post-congratulations on your bir...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Articles underlying Facebook's vision for computer vision</h1><div class="post__text post__text-html js-mediator-article">  Do you know such a company - Facebook?  Yes, yes, the one with a website of <b>1.6 billion</b> users.  And if you take all the post-congratulations on your birthday, your shameful children's photos (I have them), that distant relative, like your every status - and here you have a lot of data for analysis. <br><br>  In terms of image analysis, Facebook has come a long way with convolutional neural networks (Convolutional Neural Network, CNN).  In August, Facebook's Artificial Intelligence Unit (Facebook AI Research, abbreviated as FAIR) published a <a href="https://research.facebook.com/blog/learning-to-segment/">blog post</a> about computer vision algorithms that underlie some of their image segmentation algorithms.  In this post, we will summarize and explain the three articles referenced by this blog. <br><br><img src="https://habrastorage.org/files/a3b/9d4/987/a3b9d498742a47d8bd865f249aa5862d.png"><br><a name="habracut"></a><br>  Normally FAIR uses the following sequence of algorithms.  Images are fed to the DeepMask segmentation framework.  The SharpMask algorithm is used to improve the segments selected in the first stage, then objects are classified using MultiPathNet.  Let's take a look at how each of these components works separately. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  <font color="#c75733"><u>Deepmask</u></font> </h2><br><h3>  <font color="#c75733">Introduction</font> </h3><br>  This work, which belongs to Pedro Pinheiro, Roman Collobert, and Peter Dollar, is called ‚ÄúLearning to Segment Object Candidates‚Äù.  To solve the problem of image segmentation, the authors propose an algorithm that, when receiving a fragment of an image, displays the object mask and the probability that the entire object is in the center of the fragment.  This process is applied to the entire image so that a mask is created for each object.  The entire process is carried out using just one CNN, since both components use the same network layers. <br><br><h3>  <font color="#c75733">Input and Output</font> </h3><br>  To begin with we will depict what we expect from the model.  We want for the input image to get a set of masks or silhouettes of each object.  We can present the original image as a set of fragments.  For each input piece at the output, we obtain a binary mask containing the silhouette of the main object on it, as well as an estimate of how likely it is that this fragment contains the object (from -1 to 1). <br><br><img src="https://habrastorage.org/files/329/d4f/2ca/329d4f2ca5d74384a4e16b7f4969c9ea.png"><br>  Each teaching example should contain these three components (note: an example with probability 1 should contain a fragment in the center of which is an object that is contained in the image completely and on the same scale).  The algorithm is applied to different parts of the image (the same set of fragments, which we mentioned earlier).  The results are then combined to form one final image containing all the masks.  Let's see what the algorithm consists of. <br><br><img src="https://habrastorage.org/files/218/335/83a/21833583aaf54ec3b7da3d53d646e9b6.png"><br>  * <b>Figure 1</b> .  <b>Above</b> .  Model architecture: after a general layer of extraction of characteristics, the model diverges into two branches.  The upper branch predicts a segmentation mask for the object in the center, while the lower branch predicts the likelihood that the fragment contains the object. <br>  <b>From below</b> .  Examples of training triples: input fragment x, mask m, and label y.  Green fragments contain objects that satisfy certain conditions and therefore are marked with y = 1. Note that the masks from the negative examples (red) are not used and are shown for illustration only. <br><br><h3>  <font color="#c75733">Network architecture</font> </h3><br>  The network was pre-trained in ImageNet image classification (Transfer Learning in action).  The image passes through a VGG-like model (without fully connected layers) with eight 3x3 convolutional layers and five maxpool 2x2 layers.  Depending on the size of the original image, you will get a certain amount of output (in this case, 512x14x14). <br><br>  <b>Input</b> : 3 xhxw <br>  <b>Output</b> : 512 xh / 16 xw / 16 <br><br>  Then the network is divided into two components described above.  One of them assumes segmentation, while the second determines whether the desired object is in the image. <br><br><h3>  <font color="#c75733">Segmentation Component (Segmentation Head)</font> </h3><br>  Now we take the output data, transfer it through the inner layer and the ReLU layer.  Now we have a layer of w 'xh' (where w 'and h' are smaller than w and h of the original image) pixel classifiers, which determine whether a given pixel is part of an object in the center of the image (if the image size is 28x28, then the classifiers will be less than 784).  Then we take the output values ‚Äã‚Äãof the classifiers, using the bilinear interpolation method, increase the image resolution to the original one and get a binary mask (1 - if the pixel belongs to the desired object, 0 - if not). <br><br><h3>  <font color="#c75733">Objectness Head</font> </h3><br>  The second component of the network determines whether the image contains an object of the desired scale in the center.  By transmitting the output of VGG-like layers through a 2x2 maxpool layer, a dropout module and two fully connected layers, we can get our probability estimate. <br><br><h3>  <font color="#c75733">Training</font> </h3><br>  Both components of the network are trained in parallel, since the loss function is the sum of the loss of logistic regression (the loss of Objectness Head plus the loss of applying Segmentation Head to each fragment).  The error propagation algorithm runs either on an Objectness Head or on a Segmentation Head.  To improve the model, we used the technique of extending the training set (Data Augmentation).  The model was trained using the stochastic gradient descent method on an Nvidia Tesla K40m GPU for five days. <br><br><h3>  <font color="#c75733">What is cool this article</font> </h3><br>  One convolutional neural network.  We did not need an additional step to generate hypotheses of the location of the object (object proposals) or any complex learning process.  This model has a certain simplicity, which provides network flexibility, as well as efficiency and speed. <br><br><h2>  <font color="#c75733"><u>SharpMask</u></font> </h2><br><h3>  <font color="#c75733">Introduction</font> </h3><br>  The previous group of researchers (together with Tsung-Yi Lin) also authored the article entitled ‚ÄúLearning to Refine Object Segments‚Äù.  As the name implies, this article talks about improving the masks created during the DeepMask stage.  The main problem of DeepMask is that this model uses a simple network of direct distribution, which successfully creates ‚Äúrough‚Äù masks, but does not perform segmentation with pixel accuracy.  The reason for this is that, if you remember, in DeepMask there is a bilinear interpolation, which serves to fit the size of the original image.  Therefore, compliance with real boundaries is very approximate.  To solve this problem, the SharpMask model combines the information of low-level characteristics that we get from the first layers of the network with high-level information about the object, which comes from deeper layers.  First, the model creates a coarse mask for each input fragment (the work of DeepMask), and then passes through several refinement modules.  Consider this stage in more detail. <br><br><img src="https://habrastorage.org/files/4b4/0fe/106/4b40fe1062a24c32b6f4086470a68d13.png"><br><br><h3>  <font color="#c75733">Network architecture</font> </h3><br>  The SharpMask idea emerged from the following considerations: since in order to find the exact object mask we need object-level information (high-level), we need a downward approach that builds coarse segments first and then combines the important low-level information from the initial layers.  As can be seen from the figure above, the original image first passes through DeepMask to obtain coarse segmentation, and then enters the input sequence of the refinement modules for a more accurate increase of the image to the size of the original. <br><br><h3>  <font color="#c75733">Refinement module</font> </h3><br>  Let us consider in more detail how the specifying module is arranged.  The objective of this module is to resist the influence of subsampling layers (pooling layer) at the DeepMask stage (the layers that squeezed the 224x224 image to 14x14), increasing the dimension of the masks obtained taking into account the feature maps created during the ascending passage (we can call DeepMask the ascending passage, and SharpMask - downward).  Mathematically speaking, a refinement module is a function that generates an extended mask M, that is, a function from the mask of the previous layer and a map of signs F. The number of refinement modules must be equal to the number of subsample layers used in DeepMask. <br><div style="text-align:center;"><img src="https://habrastorage.org/files/71f/788/657/71f788657ca8480fb1d13a69566d919f.png"></div><br>  But what exactly does the R function do?  Glad you asked.  A naive approach would be to simply combine M and F, since they have the same height and width.  The problem with this approach is creating a color depth channel for each of these components.  The number of such channels in feature maps can be much larger than in a mask.  Thus, a simple join would add too much F. The solution would be to reduce the number of color depth channels for F using a 3x3 convolutional layer, combining c M, passing through another 3x3 convolutional layer, and finally bilinear interpolation (see network architecture diagram ). <br><br><h3>  <font color="#c75733">Training</font> </h3><br>  The same training data used for DeepMask can also be applied to SharpMask.  First, the layers of DeepMask are trained.  Then the weights are frozen, and SharpMask training begins. <br><br><h3>  <font color="#c75733">What is cool this article</font> </h3><br>  This article, based on DeepMask, introduces a new, easy-to-use module.  The authors discovered that they can achieve higher segmentation accuracy by simply using the low-level information available in the earlier layers of the DeepMask stage. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/02d/d25/ed5/02dd25ed5f6025c705bcbc3f8ffe8a9b.png"><br><br><h2>  <a href="http://arxiv.org/pdf/1604.02135v2.pdf"><font color="#c75733"><u>MultiPathNet</u></font></a> </h2><br><h3>  <font color="#c75733">Introduction</font> </h3><br>  DeepMask create coarse mask segments.  SharpMask refines the contours of objects.  And the task of MultiPathNet is to identify or classify objects in masks.  The group consisting of Sergey Zagoruiko, Adam Lerer, Tseng-Yi Lin, Pedro Pinero, Sam Gross, Sumit Chintala and Peter Dollar published an article ‚ÄúMultipath Network for Object Detection‚Äù.  The goal of this article is to improve object recognition methods by directing attention to more precise localization, as well as to complex images with different scales, a large number of obstacles and unnecessary details.  This model uses Fast R-CNN as a starting point (see <a href="https://arxiv.org/pdf/1504.08083.pdf">this article</a> or my previous <a href="https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html">post</a> ).  In essence, this model is a Fast R-CNN implementation with generating hypotheses about the location of an object using DeepMask and SharpMask.  The three main changes described in the article are forged areas (forveal regions), skip connections, and integral loss function.  But before going deep, let's take a look at the network architecture. <br><br><img src="https://habrastorage.org/files/aa1/485/daf/aa1485daff4a43ce8c8adb56d3270b47.png"><br><br><h3>  <font color="#c75733">Network architecture / foveal areas</font> </h3><br>  As in Fast R-CNN, we transmit the input image through the VGG network without fully connected layers.  The ROI pooling layer is used to extract features from region-based hypotheses (as we remember from the Fast R-CNN article, ROI poling is a method of making an image map of the characteristics of a specific area of ‚Äã‚Äãan image).  For each hypothesis, we crop the image in four different ways to view the object at different scales.  These are the ‚Äúfoveal regions‚Äù, which were discussed in the introduction.  These cropped fragments pass through fully connected layers, the output data is combined, and the network splits into a classification head (regression head) and a regression branch.  The authors suggest that these foveal areas will help to more accurately determine the location of the object, because in this way the network will be able to see the object at different scales and in different environments. <br><br><h3>  <font color="#c75733">Skip connections</font> </h3><br>  Thanks to Fast R-CNN, after the last convolutional VGG layer, the input image 32x32 will be quickly compressed to the size 2x2.  The ROI pooling layer will create a 7x7 map, but we still lose a lot of spatial information.  To solve this problem, we combine the attributes from the words conv3, conv4 and conv5, and transfer the result to the formal classifier.  As stated in the article, this association ‚Äúgives the classifier access to information about the signs from different areas of the image.‚Äù <br><br><h3>  <font color="#c75733">Integral loss function</font> </h3><br>  I do not want to delve into this topic, because I think that all mathematics is much better explained in the article itself, but the whole idea is that the authors derived a loss function that works better with many values ‚Äã‚Äãof Intersection over Union (IoU). <br><br><h3>  <font color="#c75733">What is cool this article</font> </h3><br>  If you are a Fast R-CNN fan, then you will definitely like this model.  It uses the core ideas of VGG Net and ROI pooling, while at the same time creating a new way to more accurately localize and classify using forward areas, skip connections and an integral loss function. <br><br>  Facebook seems to have mastered all this CNN science perfectly. <br><br>  If you have something to add, or you can otherwise explain any of the articles, let us know in the comments. <br><br>  DeepMask and SharpMask code.  MultuiPathNet <a href="https://github.com/facebookresearch/multipathnet">code</a> . <br><br>  ‚Üí <a href="http://adeshpande3.github.io/Analyzing-the-Papers-Behind-Facebook%27s-Computer-Vision-Approach/">Link to original</a> <br><br>  ‚Üí <a href="https://adeshpande3.github.io/assets/Sources4.txt">Sources</a> <br><br><blockquote><div class="spoiler">  <b class="spoiler_title">Oh, and come to work with us?</b>  <b class="spoiler_title">:)</b> <div class="spoiler_text">  <a href="http://wunderfund.io/"><b>wunderfund.io</b></a> is a young foundation that deals with <a href="https://en.wikipedia.org/wiki/High-frequency_trading">high-frequency algorithmic trading</a> .  High-frequency trading is a continuous competition of the best programmers and mathematicians of the whole world.  By joining us, you will become part of this fascinating fight. <br><br>  We offer interesting and challenging data analysis and low latency tasks for enthusiastic researchers and programmers.  Flexible schedule and no bureaucracy, decisions are quickly made and implemented. <br><br>  Join our team: <a href="http://wunderfund.io/">wunderfund.io</a> </div></div></blockquote></div><p>Source: <a href="https://habr.com/ru/post/317930/">https://habr.com/ru/post/317930/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../317916/index.html">Test automation by Scrum methodology</a></li>
<li><a href="../317918/index.html">How to make the perfect tutorial for hardcore gamers and their moms</a></li>
<li><a href="../317922/index.html">Depla web applications using Ansistrano</a></li>
<li><a href="../317924/index.html">Three minutes to set up WiFi authorization via SMS</a></li>
<li><a href="../317928/index.html">RxJava. Remove magic</a></li>
<li><a href="../317934/index.html">History of one name, or positioning of a product on a new platform</a></li>
<li><a href="../317936/index.html">December release of ReSharper Ultimate 2016.3</a></li>
<li><a href="../317938/index.html">Heisenbag: Version 1.0</a></li>
<li><a href="../317940/index.html">How to survive in a corporate culture if you are an introvert programmer</a></li>
<li><a href="../317942/index.html">CSS 20 years</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>