<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Developing a simple deep learning model for predicting stock prices using TensorFlow</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Data science expert and STATWORX CEO Sebastian Heinz published on Medium a guide to creating a deep learning model for predicting stock prices on a st...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Developing a simple deep learning model for predicting stock prices using TensorFlow</h1><div class="post__text post__text-html js-mediator-article"> <a href="https://habr.com/company/iticapital/blog/354732/"><img src="https://habrastorage.org/webt/3v/ft/0b/3vft0bvpyb1okrevzbdpsekj1qm.png"></a> <br><br>  Data science expert and STATWORX CEO Sebastian Heinz published on Medium a guide to creating a deep learning model for predicting stock prices on a stock exchange using the TensorFlow framework.  We have prepared an adapted version of this useful material. <a name="habracut"></a><br><br>  <i>The author posted the final Python script and compressed dataset in his <a href="https://github.com/sebastianheinz/stockprediction">repository on GitHub</a></i> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  Import and prepare data </h2><br>  Heinz exported stock data to a csv file.  His dataset contained n = 41266 minutes of data, covering 500 trades in shares from April to August 2017, and also included information on the price of the S &amp; P 500 index. <br><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   data = pd.read_csv('data_stocks.csv') #   date data = data.drop(['DATE'], 1) #   n = data.shape[0] p = data.shape[1] #    numpy- data = data.values</span></span></code> </pre> <br>  This is the S &amp; P index time series, built using pyplot.plot (data ['SP500']): <br><br><img src="https://habrastorage.org/webt/hj/2j/vy/hj2jvydudcnxbhl-gx_t1alkoxm.png"><br><br>  An interesting point: since the ultimate goal is to ‚Äúpredict‚Äù the index value in the near future, its value shifts one minute ahead. <br><br><h2>  Preparation of data for testing and training </h2><br>  The data set was divided into two - one part for testing, and the second for training.  At the same time, data for training accounted for 80% of their total volume and covered the period from April to approximately the end of July 2017, the data for testing ended in August 2017. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#      train_start = 0 train_end = int(np.floor(0.8*n)) test_start = train_end test_end = n data_train = data[np.arange(train_start, train_end), :] data_test = data[np.arange(test_start, test_end), :]</span></span></code> </pre> <br>  There are many approaches to cross-validation of time series, from generating forecasts with or without reconfiguration of the model (refitting) to more complex concepts like <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">bootstrap-resampling of</a> time series.  In the latter case, the data is broken up into repeated samples from the beginning of the seasonal time series decomposition ‚Äî this allows you to simulate samples that follow the same seasonal pattern as the original time series, but do not completely copy its values. <br><br><h2>  Data scaling </h2><br>  Most neural network architectures use input scaling (and sometimes output).  The reason is that most neuron activation functions like sigmoid or hyperbolic tangent ( <a href="https://en.wikipedia.org/wiki/Hyperbolic_function">tanx</a> ) are defined at intervals [-1, 1] or [0, 1], respectively.  Currently, the most commonly used activation of the rectified linear unit (ReLU).  Heinz decided to scale the input data and goals using MinMaxScaler in Python for this purpose: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() scaler.fit(data_train) data_train = scaler.transform(data_train) data_test = scaler.transform(data_test) #  X  y X_train = data_train[:, 1:] y_train = data_train[:, 0] X_test = data_test[:, 1:] y_test = data_test[:, 0]</span></span></code> </pre><br>  <b>Note</b> : Care should be taken when selecting a piece of data and time for scaling.  A common mistake here is to scale the entire dataset before splitting it into test and training data.  This is an error, because scaling starts counting statistics, that is, the minima / maxima of variables.  When realizing time series forecasting in real life, at the time of their generation you cannot have information from future observations.  Therefore, statistics should be calculated on training data, and then the result obtained should be applied to test data.  Taking information ‚Äúfrom the future‚Äù (that is, from a test sample) to generate predictions, the model will issue predictions with a ‚Äúsystem bias‚Äù (bias). <br><br><h2>  Introduction to TensorFlow </h2><br>  TensorFlow is an excellent product, currently it is the most popular framework for solving machine learning problems and creating neural networks.  The product backend is based on C ++, but Python is usually used for management (there is also a wonderful <a href="https://tensorflow.rstudio.com/">TensorFlow library for R</a> ).  TensorFlow uses the concept of graphical representation of computational problems.  This approach allows users to define mathematical operations as data graph elements, variables, and operators.  Since neural networks, in fact, are graphs of data and mathematical operations, TensorFlow is great for working with them and machine learning.  In the example below, a graph is presented that solves the problem of adding two numbers: <br><br><img src="https://habrastorage.org/webt/rj/ro/7j/rjro7j7swv5w-mdbmtewiksrqaq.png"><br><br>  The figure above shows two numbers that need to be folded.  They are stored in variables a and b.  The values ‚Äã‚Äãtravel along the graph and fall into a node represented by a square, where addition occurs.  The result of the operation is written to another variable c.  Used variables can be considered as placeholders.  Any numbers that fall into a and b are added together, and the result is written in c. <br><br>  This is exactly how TensorFlow works - the user defines an abstract representation of a model (neural network) through placeholders and variables.  After that, the first ones are filled with real data and calculations take place.  The test example above is described by the following code in TensorFlow: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  TensorFlow import tensorflow as tf #  a  b    a = tf.placeholder(dtype=tf.int8) b = tf.placeholder(dtype=tf.int8) #   c = tf.add(a, b) #   graph = tf.Session() #   graph.run(c, feed_dict={a: 5, b: 4})</span></span></code> </pre> <br>  After importing the TensorFlow library using tf.placeholder (), two placeholders are defined.  They correspond to the two blue circles on the left side of the image above.  After that, using tf.add () is determined by the operation of addition.  The result of the operation is c = 9. With tuned placeholders, the graph can be executed for any integer values ‚Äã‚Äãa and b.  It is clear that this example is extremely simple, and neural networks in real life is much more complicated, but it allows you to understand the principles of the framework. <br><br><h2>  Placeholders </h2><br>  As stated above, everything starts with placeholders.  In order to implement a model, you need two such elements: X contains input data for the network (stock prices of all S &amp; P 500 elements at time T = t) and output data Y (S &amp; P 500 index values ‚Äã‚Äãat time T = t + 1) . <br><br>  The form of placeholders corresponds to [None, n_stocks], where [None] means that the input data is represented as a two-dimensional matrix, and the output data is a one-dimensional vector.  It is important to understand what form of input and output data neural networks need and organize them accordingly. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  X = tf.placeholder(dtype=tf.float32, shape=[None, n_stocks]) Y = tf.placeholder(dtype=tf.float32, shape=[None])</span></span></code> </pre> <br>  Argument None means that at this point we do not yet know the number of observations that will pass through the graph of the neural network during each launch, so it remains flexible.  Later, the variable batch_size, which controls the number of observations during the training run, will be defined. <br><br><h2>  Variables </h2><br>  In addition to placeholders, there is another important element in the TensorFlow universe - these are variables.  If placeholders are used to store input and target data in a graph, then the variables serve as flexible containers inside the graph.  They are allowed to change during the execution of the graph.  Weights and offsets are represented by variables in order to facilitate adaptation during training.  Variables must be initialized before learning. <br><br>  The model consists of four hidden levels.  The first contains 1024 neurons, which is a little more than twice the amount of input data.  Subsequent hidden levels are always two times less than the previous one - they combine 512, 256 and 128 neurons.  The reduction in the number of neurons at each level compresses the information that the network has processed at the previous levels.  There are other neuron architectures and configurations, but this tutorial uses exactly this model: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    n_stocks = 500 n_neurons_1 = 1024 n_neurons_2 = 512 n_neurons_3 = 256 n_neurons_4 = 128 n_target = 1 #  1:       W_hidden_1 = tf.Variable(weight_initializer([n_stocks, n_neurons_1])) bias_hidden_1 = tf.Variable(bias_initializer([n_neurons_1])) #  2:       W_hidden_2 = tf.Variable(weight_initializer([n_neurons_1, n_neurons_2])) bias_hidden_2 = tf.Variable(bias_initializer([n_neurons_2])) #  3:       W_hidden_3 = tf.Variable(weight_initializer([n_neurons_2, n_neurons_3])) bias_hidden_3 = tf.Variable(bias_initializer([n_neurons_3])) #  4:       W_hidden_4 = tf.Variable(weight_initializer([n_neurons_3, n_neurons_4])) bias_hidden_4 = tf.Variable(bias_initializer([n_neurons_4])) #   :       W_out = tf.Variable(weight_initializer([n_neurons_4, n_target])) bias_out = tf.Variable(bias_initializer([n_target]))</span></span></code> </pre> <br>  It is important to understand what variable sizes are required for different levels.  The rule of thumb of multi-level perceptrons is that the size of the previous level is the first size of the current level for weights matrices.  It sounds difficult, but the point is that each level transmits its output as input to the next level.  The sizes of the displacements are equal to the second size of the weights matrix of the current level, which corresponds to the number of neurons in the level. <br><br><h2>  Network architecture design </h2><br>  After determining the required weights and offsets of the variables, the network topology, it is necessary to determine the network architecture.  Thus, placeholders (data) and variables (weights and displacements) need to be combined into a system of successive matrix multiplications.  Hidden network levels are transformed by activation functions.  These functions are important elements of the network infrastructure, since they introduce nonlinearity into the system.  There are dozens of activation functions, and one of the most common is rectified linear unit (ReLU).  It is this manual that is used in this manual: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   hidden_1 = tf.nn.relu(tf.add(tf.matmul(X, W_hidden_1), bias_hidden_1)) hidden_2 = tf.nn.relu(tf.add(tf.matmul(hidden_1, W_hidden_2), bias_hidden_2)) hidden_3 = tf.nn.relu(tf.add(tf.matmul(hidden_2, W_hidden_3), bias_hidden_3)) hidden_4 = tf.nn.relu(tf.add(tf.matmul(hidden_3, W_hidden_4), bias_hidden_4)) #   (  ) out = tf.transpose(tf.add(tf.matmul(hidden_4, W_out), bias_out))</span></span></code> </pre> <br>  The image below illustrates the network architecture.  The model consists of three main blocks.  The level of input data, hidden levels and output level.  This infrastructure is called a feedforward network.  This means that chunks of data move along the structure strictly from left to right.  In other implementations, for example, in the case of recurrent neural networks, data can flow inside the network in different directions. <br><br><img src="https://habrastorage.org/webt/ty/7c/ei/ty7ceiucia5982vurtorb8ud7iu.png"><br><br><h2>  Cost function </h2><br>  The network cost function is used to generate an estimate of the variance between network predictions and actual observation results during training.  To solve regression problems, use the mean square error function (MSE).  This function calculates the standard deviation between predictions and goals, but in general, any differentiated function can be used to calculate the deviation between the two. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   mse = tf.reduce_mean(tf.squared_difference(out, Y))</span></span></code> </pre> <br>  In doing so, MSE displays specific entities that are useful for solving a general optimization problem. <br><br><h2>  Optimizer </h2><br>  The optimizer undertakes the necessary calculations required to adapt the weights and variable deviations of the neural network during training.  These calculations lead to calculations of the so-called gradients, which indicate the direction of the necessary change in the deviations and weights to minimize the cost function.  Developing a stable and fast optimizer is one of the main tasks of the creators of neural networks. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  opt = tf.train.AdamOptimizer().minimize(mse)</span></span></code> </pre> <br>  In this case, use one of the most common optimizers in the field of machine learning Adam Optimizer.  Adam is an abbreviation for the phrase ‚ÄúAdaptive Moment Estimation‚Äù (adaptive estimation of moments), it is a cross between two other popular optimizers AdaGrad and RMSProp <br><br><h2>  Initializers </h2><br>  Initializers are used to initialize variables before starting learning.  Since neural networks are trained using numerical optimization techniques, the starting points of an optimization problem are one of the most important factors in the search for a good solution.  There are different initializers in TensorFlow, each of which uses its own approach.  This tutorial uses tf.variance_scaling_initializer (), which implements one of the standard initialization strategies. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  sigma = 1 weight_initializer = tf.variance_scaling_initializer(mode="fan_avg", distribution="uniform", scale=sigma) bias_initializer = tf.zeros_initializer()</span></span></code> </pre><br>  Note: In TensorFlow, you can define several initialization functions for different variables within a graph.  However, in most cases, a fairly unified initialization. <br><br><h2>  Neural network setup </h2><br>  After determining the placeholders, variables, initializers, cost functions, and optimizers, the model must be trained.  Usually this is done using the mini-batches training approach.  In the course of such training, random samples of size n = batch_size are selected from the data set for training and loaded into the neural network.  The training dataset is divided into n / batch_size pieces, which are then sequentially sent to the network.  At this point, the placeholders X and Y come into play. They store the input and target data and send them to the neural network. <br><br>  The sampled X data passes through the network to reach the output level.  Then TensorFlow compares the model-generated predictions with the actually observed goals Y in the current ‚Äúrun‚Äù.  After that, TensorFlow performs the optimization step and updates the network parameters. After the weights and deviations are updated, the process is repeated again for a new piece of data.  The procedure is repeated until all the ‚Äúsliced‚Äù pieces of data are sent to the neural network.  The full cycle of such processing is called the "epoch". <br><br>  Network training stops when the maximum number of epochs is reached or when another predetermined stopping criterion is triggered. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   net = tf.Session() #   net.run(tf.global_variables_initializer()) #    plt.ion() fig = plt.figure() ax1 = fig.add_subplot(111) line1, = ax1.plot(y_test) line2, = ax1.plot(y_test*0.5) plt.show() #       epochs = 10 batch_size = 256 for e in range(epochs): #     shuffle_indices = np.random.permutation(np.arange(len(y_train))) X_train = X_train[shuffle_indices] y_train = y_train[shuffle_indices] #  - for i in range(0, len(y_train) // batch_size): start = i * batch_size batch_x = X_train[start:start + batch_size] batch_y = y_train[start:start + batch_size] # Run optimizer with batch net.run(opt, feed_dict={X: batch_x, Y: batch_y}) #   if np.mod(i, 5) == 0: # Prediction pred = net.run(out, feed_dict={X: X_test}) line2.set_ydata(pred) plt.title('Epoch ' + str(e) + ', Batch ' + str(i)) file_name = 'img/epoch_' + str(e) + '_batch_' + str(i) + '.jpg' plt.savefig(file_name) plt.pause(0.01) #    MSE   mse_final = net.run(mse, feed_dict={X: X_test, Y: y_test}) print(mse_final)</span></span></code> </pre> <br>  During the training, the predictions generated by the network on the test set were evaluated, then visualization was performed.  In addition, the images were uploaded to the disk and later, video-animation of the learning process was created from them: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/I_KMlGavtiQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  As you can see, the neural network quickly adapts to the basic form of the time series and continues to search for the best data patterns.  After 10 epochs, we get results very close to the test data.  The final value of the MSE function is 0.00078 (a very small value due to the fact that the goals are scaled).  The average absolute percentage error of the forecast on the test set is 5.31% - a very good result.  It is important to understand that this is only a coincidence with test data, and not real data. <br><br><img src="https://habrastorage.org/webt/uv/t1/bt/uvt1btqumgtwpune4l3xpsyigbs.png"><br><br>  <i>Scatter plot between predicted and real S &amp; P prices</i> <br><br>  This result can be further improved in many ways, from the elaboration of levels and neurons, to the choice of other initialization and activation schemes.  In addition, various types of deep learning models can be used, such as recurrent neural networks - this can also lead to better results. <br><br><h2>  Other materials on finance and stock market from <a href="https://iticapital.ru/">ITI Capital</a> : </h2><br><ul><li>  <a href="https://iticapital.ru/research-education/research/">Analytics and market reviews</a> </li><li>  <a href="https://habrahabr.ru/company/iticapital/blog/352846/">Distrust of authority and economy: the main trends of millenial investment activity</a> </li><li>  <a href="https://habrahabr.ru/company/iticapital/blog/345860/">Where it is more profitable to buy currency: banks vs exchange</a> </li><li>  <a href="https://habrahabr.ru/company/iticapital/blog/350092/">How the implementation of trading systems with artificial intelligence will affect investment management</a> </li><li>  <a href="https://habrahabr.ru/company/iticapital/blog/348792/">Bloomberg: how Ilon Mask‚Äôs sale of flame throwers will change start-up financing</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/354732/">https://habr.com/ru/post/354732/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../354722/index.html">Failover VoIP Cluster 3CX</a></li>
<li><a href="../354724/index.html">Marvin Minsky "The Emotion Machine": Chapter 3 "The Pain"</a></li>
<li><a href="../354726/index.html">Who scans the Internet and does Australia exist</a></li>
<li><a href="../354728/index.html">Own asynchronous tcp-server in 15 minutes with detailed analysis</a></li>
<li><a href="../354730/index.html">We reanimate the Nintendo Switch and PlayStation gaming service after locks on the RKN</a></li>
<li><a href="../354736/index.html">How to work as an IT manager and why your boss doesn't have to be Steve Jobs</a></li>
<li><a href="../354738/index.html">Municipal Moira or What affects the career of an official</a></li>
<li><a href="../354740/index.html">Implementing a RESTful Table in the Atlassian User Interface</a></li>
<li><a href="../354742/index.html">PHP could get better</a></li>
<li><a href="../354744/index.html">Protection against easy DDoS'a</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>