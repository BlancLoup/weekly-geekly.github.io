<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Application of automatic machine learning to neural networks with ‚Äútransformer‚Äù architecture</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="From the Google AI blog 

 Since the publication of information about them in 2017, neural networks of the " transformer " type architecture have been...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Application of automatic machine learning to neural networks with ‚Äútransformer‚Äù architecture</h1><div class="post__text post__text-html js-mediator-article">  <i>From the Google AI blog</i> <br><br>  Since the <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">publication of information</a> about them in 2017, neural networks of the " <a href="https://habr.com/ru/post/341240/">transformer</a> " type architecture have been applied to tasks of various kinds, from <a href="https://openai.com/blog/better-language-models/">generating texts in fantasy style</a> to <a href="https://www.google.com/doodles/celebrating-johann-sebastian-bach">writing musical harmonies</a> .  What is important, the high quality of the work of ‚Äútransformers‚Äù has shown that when applied to sequential tasks, for example, to language modeling and translation, <a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">neural networks with direct propagation</a> can be just as effective as recurrent ones.  And although the popularity of transformer and other direct propagation models used in sequential tasks is growing, their architectures are almost always created manually, unlike the computer vision field, where <a href="https://ru.wikipedia.org/wiki/%25D0%2590%25D0%25B2%25D1%2582%25D0%25BE%25D0%25BC%25D0%25B0%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25BE%25D0%25B5_%25D0%25BE%25D0%25B1%25D1%2583%25D1%2587%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%25BC%25D0%25B0%25D1%2588%25D0%25B8%25D0%25BD">automatic machine learning</a> ( <a href="https://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html">AOM</a> ) approaches have already been discovered by <a href="https://arxiv.org/abs/1802.01548">advanced models</a> that are ahead of those that were manual setting.  Naturally, we were interested in whether the application of the AOM to successive tasks could achieve the same success. <br><a name="habracut"></a><br>  Having conducted an <a href="https://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html">evolutionary</a> neuroarchitecture search (neural architecture search, NAS), and using the translation as a model of successive tasks, we found an <a href="https://arxiv.org/abs/1901.11117">evolved transformer</a> (ET), a new transformer architecture that demonstrates improvements in various <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> (OEE) tasks.  ET not only achieves advanced results in translation, but also demonstrates an improvement in efficiency when modeling a language compared to the original transformer.  We <a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py">publish a</a> new model in the <a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py">Tensor2Tensor</a> library, where it can be used for any consistent task. <br><br><h2>  Technician Development </h2><br>  To begin the evolutionary search for neuroarchitecture, we had to develop new techniques, since the task used to assess the ‚Äúfitness‚Äù of each of the architectures was a demanding computational resource.  As a result, these searches are more demanding than similar searches in the field of computer vision, capable of operating with smaller databases, for example, <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a> .  The first of these techniques is a warm start, sowing the original evolutionary population with the transformer type architectures instead of random models.  This helps to concentrate searches in a deliberately strong area of ‚Äã‚Äãthe search space, which allows us to quickly find the best models. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The second technique is a new method developed by us called progressive dynamic obstacle race (Progressive Dynamic Hurdles, PDH).  This algorithm complements the evolutionary search, allowing you to allocate more resources to the strongest candidates, unlike previous works, where the same amount of resources was allocated to each candidate NAS model.  PDH allows us to complete a model assessment earlier if it is terribly bad, while rewarding prospective architectures with a large amount of resources. <br><br><h2>  Evolved transformer </h2><br>  Using these methods, we conducted a large-scale NAS search on our translation task and found ET.  Like most sequence-to-sequence neural networks (sequence to sequence, seq2seq) architectures, it has an encoder that encodes the incoming sequence in the insert, and a decoder that uses these inserts to create the output sequence.  In the case of a translation, the input sequence is a sentence for translation, and the output is a translation. <br><br>  The most interesting feature of ET is the convolutional layers at the bottom of the modules of both the encoder and decoder, added in a similar branching way to both these places (that is, the inputs pass through two different convolutional layers before folding). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b8a/62d/1d1/b8a62d1d155203861756f0960becaaf0.png"><br>  <i>Comparison of the encoder architecture of a conventional transformer and ET.</i>  <i>Pay attention to the branching convolutional structure at the bottom of the module, independently formed in the encoder and in the decoder.</i>  <i>The decoder is described in detail in <a href="https://arxiv.org/abs/1901.11117">our work</a> .</i> <br><br>  This is especially interesting because the encoder and decoder during the NAS do not share architectures with each other, and the utility of this architecture was discovered independently in the encoder and in the decoder, which speaks in favor of such a scheme.  If the original transformer fully relied on applying attention to the same data that he himself generated [self-attention], ET is a hybrid that takes advantage of both self-attention and wide convolution. <br><br><h2>  Evaluation </h2><br>  To test the effectiveness of this new architecture, we first compared it with the original transformer that worked with the task of translating from English to German, which we used during the search.  We found that ET has the best performance on <a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a> and <a href="https://en.wikipedia.org/wiki/Perplexity">connectivity</a> on all sizes of parameters, and the largest gain in size is comparable to mobile devices (~ 7 million parameters), which indicates efficient use of parameters.  On larger sizes, ET reaches advanced results on WMT '14 En-De with a BLEU score of 29.8 and a SacreBLEU score of 29.2. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1a2/4c4/c60/1a24c4c6085c167a5398fdcea218f75c.png"></div><br>  <i>Comparison of ET and the original transformer on WMT'14 En-De with different volumes.</i>  <i>The greatest advantage is achieved with small sizes, while the ET shows good performance on larger sizes, ahead of the largest transformer with the number of parameters smaller by 37.6% (comparable models are taken in circles).</i> <br><br>  To test generalizability, we compared ET with a transformer on additional problems of natural language processing.  At first, we checked translations for different pairs of languages, and found that the efficiency of ET is higher, and its separation roughly corresponds to what was demonstrated in the English-German translation;  and again, thanks to the efficient use of parameters, the largest gap is observed on medium-sized models.  We also compared the decoders of both models in the <a href="">LM1B</a> modeling language, and saw a significant improvement in connectivity indicators. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6c5/c96/1b6/6c5c961b6a09ba83905d9f886c9063ea.png"><br><br><h2>  Future plans </h2><br>  These results are the first step in exploring the architecture search application for serial models with direct distribution.  The software is distributed as <a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py">open source</a> as part of the <a href="https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html%26sa%3DD%26ust%3D1560456336484000%26usg%3DAFQjCNH0gTJu1qxSq2k-F56iluKbtVmWMg">Tensor2Tensor</a> project, where it can be used on any consecutive problems.  To improve reproducibility, we also open <a href="https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/models/neural_architecture_search">the search space code</a> we used in our search, and <a href="https://github.com/google-research/google-research/blob/master/evolution/progressive_dynamic_hurdles_algorithm/Progressive_Dynamic_Hurdles_Toy_Experiment.ipynb">Colab</a> with the implementation of PDH.  We look forward to the results from the research community, armed with new models, and we hope that others will be able to take these new search techniques as a basis! </div><p>Source: <a href="https://habr.com/ru/post/460099/">https://habr.com/ru/post/460099/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../460089/index.html">Zimbra Collaboration Suite secure upgrade</a></li>
<li><a href="../46009/index.html">How did I get a job at Guerrilla</a></li>
<li><a href="../460091/index.html">Direct printing on T-shirts with Epson SureColor SC ‚Äì F and its difference from silk-screen printing, decal and sublimation</a></li>
<li><a href="../460095/index.html">Caught ban for fork deepNude on gitlab.com</a></li>
<li><a href="../460097/index.html">The Matrix has you: overview of projects using MITER ATT & CK</a></li>
<li><a href="../4601/index.html">Rambler and RBC bypassed TNS Gallup Media</a></li>
<li><a href="../46010/index.html">(a) Slideshow - jQuery plugin for organizing slide shows</a></li>
<li><a href="../460101/index.html">Operation cookie-based XSS | $ 2300 Bug Bounty story</a></li>
<li><a href="../460107/index.html">ISPsystem, forgive and forgive! Why and how we wrote our server control panel</a></li>
<li><a href="../460109/index.html">Angular: when you need to cut the application, and the backend is not yet ready</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>