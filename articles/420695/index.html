<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Ray: Distributed system for using AI</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello colleagues. 

 We hope before the end of August to begin translating a small but truly basic book on the implementation of AI capabilities in th...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Ray: Distributed system for using AI</h1><div class="post__text post__text-html js-mediator-article">  Hello colleagues. <br><br>  We hope before the end of August to begin translating a small but truly <a href="https://www.amazon.com/Pragmatic-AI-Introduction-Cloud-Based-Learning/dp/0134863860/">basic book</a> on the implementation of AI capabilities in the Python language. <br><br><img src="https://habrastorage.org/webt/n1/bi/e-/n1bie-mblp_vav_j89bejegoj8m.jpeg">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Mr. Gift, perhaps, does not need additional advertising (for the curious, the <a href="https://github.com/noahgift/">profile of the master</a> on GitHub): <br><br><img src="https://habrastorage.org/webt/9n/l0/3x/9nl03xcz2j1mpum2a8_c2y4gy-y.jpeg"><br><br>  The article proposed today will briefly describe the Ray library, developed at the University of California (Berkeley) and referred to in Gift‚Äôs petit book.  We hope that as an early teaser - what we need.  Welcome under cat <br><a name="habracut"></a><br>  As the algorithms and techniques of machine learning develop, more and more machine learning applications are required to run on multiple machines at once, and they cannot do without parallelism.  However, the infrastructure for performing machine learning on clusters is still being formed situationally.  Now there are already good solutions (for example, parameter servers or search for hyper parameters) and high-quality distributed systems (for example, Spark or Hadoop), originally created not for working with AI, but practitioners often create the infrastructure for their own distributed systems from scratch.  The mass of excess efforts is spent for it. <br><br>  As an example, consider a conceptually simple algorithm, say, <a href="https://arxiv.org/abs/1703.03864">Evolutionary strategies for learning with reinforcement</a> .  On the pseudocode, this algorithm fits into about a dozen lines, and its implementation in Python is a little more.  However, to effectively use this algorithm on a larger machine or cluster, much more complex software engineering is required.  In the implementation of this algorithm from the authors of this article - thousands of lines of code, here it is necessary to define communication protocols, strategies for serializing and deserializing messages, as well as various ways of processing data. <br><br>  One of <a href="https://github.com/ray-project/ray">Ray‚Äôs</a> goals is to help a practitioner transform a prototype algorithm running on a laptop into a high-performance distributed application that works efficiently on a cluster (or on a single multi-core machine) by adding relatively few lines of code.  Such a framework should, in terms of performance, have all the advantages of a manually optimized system and not require the user to think about planning, data transfer and machine failures. <br><br>  <b>A free framework for AI</b> <br><br>  <b>Communication with other deep learning frameworks</b> : Ray is fully compatible with deep learning frameworks such as TensorFlow, PyTorch and MXNet, so in many applications it is completely natural to use one or more other deep learning frameworks with Ray (for example, in our libraries for reinforcement learning) apply TensorFlow and PyTorch). <br><br>  <b>Communication with other distributed systems</b> : Today, many popular distributed systems are used, however, most of them were designed without taking into account the tasks associated with AI, and therefore do not have the required performance to support AI and do not have an API for expressing applied aspects of AI.  In modern distributed systems there are no (one or the other, depending on the system) such necessary possibilities: <br><br><ul><li>  Support tasks at the level of milliseconds and support the execution of millions of tasks per second </li><li>  Nested parallelism (parallelization of tasks within tasks, for example, parallel simulations when searching for hyperparameters) (see the following figure) </li><li>  Arbitrary dependencies between tasks, dynamically during execution (for example, to not have to wait, adjusting to the pace of slow workers) </li><li>  Tasks that operate on a shared variable state (for example, weights in neural networks or a simulator) </li><li>  Heterogeneous resource support (CPU, GPU, etc.) </li></ul><br><br><img src="https://habrastorage.org/webt/9-/0o/je/9-0ojeen_te_v9xyudtqh59aesq.png"><br><br>  <i>A simple example of nested parallelism.</i>  <i>In our application, two experiments are performed in parallel (each of them is a long-term task), and several parallel processes are modeled in each experiment (each process is also a task).</i> <br><br>  There are two main ways to use Ray: through its low-level APIs and through high-level libraries.  High-level libraries are built on top of low-level APIs.  Currently, they include <a href="https://ray.readthedocs.io/en/latest/rllib.html">Ray RLlib</a> (a scalable library for training with reinforcements) and <a href="https://ray.readthedocs.io/en/latest/tune.html">Ray.tune</a> , an effective library for distributed search of hyperparameters. <br><br>  <b>Ray API Low Level</b> <br><br>  The goal of the Ray API is to provide a natural expression of the most common computational patterns and applications, not limited to fixed patterns such as MapReduce. <br><br>  <i><b>Dynamic task graphs</b></i> <br><br>  The base primitive in the Ray application (task) is a dynamic task graph.  It is very different from the computational graph in TensorFlow.  While in TensorFlow, the computational graph represents a neural network and is executed many times in each individual application, in Ray, the task graph corresponds to the whole application and is executed only once.  The task graph is not known in advance.  It is built dynamically while the application is running, and the execution of one task can initiate the execution of many other tasks. <br><br><img src="https://habrastorage.org/webt/sw/u8/xs/swu8xsuyr5gtqoghtk5gkfgihme.png"><br><br>  <i>An example of a computational graph.</i>  <i>Tasks are shown in white ovals, and objects are shown in blue rectangles.</i>  <i>The arrows indicate that some tasks depend on objects, while others create objects.</i> <br><br>  Arbitrary Python functions can be performed as tasks, and, in an arbitrary order, they can depend on the output of other tasks.  See example below. <br><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    .      , #  . @ray.remote def multiply(x, y): return np.dot(x, y) @ray.remote def zeros(size): return np.zeros(size) #    .     , #       . x_id = zeros.remote((100, 100)) y_id = zeros.remote((100, 100)) #   .    ,     #  . z_id = multiply.remote(x_id, y_id) #  .      ,     . z = ray.get(z_id)</span></span></code> </pre> <br><br>  <b><i>Actors</i></b> <br><br>  With the help of remote functions alone and the above-described handling of tasks, it is impossible to ensure that several tasks simultaneously work on the same shared changeable state.  Such a problem in machine learning arises in different contexts, where the state of the simulator, the weights in the neural network, something completely different can be shared.  Actor abstraction is used in Ray to encapsulate a mutable state that is shared between multiple tasks.  Here is an illustrative example showing how this is done with an Atari simulator. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym @ray.remote <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Simulator</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.env = gym.make(<span class="hljs-string"><span class="hljs-string">"Pong-v0"</span></span>) self.env.reset() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">step</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, action)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.env.step(action) <span class="hljs-comment"><span class="hljs-comment">#  ,    , ,   , #      simulator = Simulator.remote() observations = [] for _ in range(4): #     0.       #    observations.append(simulator.step.remote(0))</span></span></code> </pre> <br><br>  With all the simplicity of the actor is very flexible in use.  For example, in an actor a simulator or a neural network policy can be encapsulated, it can also be used for distributed learning (such as with a parameter server) or for providing policies in a live application. <br><br><img src="https://habrastorage.org/webt/hr/or/fe/hrorfevzvb-zqne47z4xa1octgu.png"><br><br>  <i>Left: Actor issues predictions / actions to a certain number of client processes.</i>  <i>Right: Many parameter server actors perform distributed learning for multiple workflows.</i> <br><br>  <i><b>Parameter server example</b></i> <br><br>  The parameter server can be implemented as a Ray actor as follows: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@ray.remote class ParameterServer(object): def __init__(self, keys, values): #    ,     . values = [value.copy() for value in values] self.parameters = dict(zip(keys, values)) def get(self, keys): return [self.parameters[key] for key in keys] def update(self, keys, values): #        ,  #      for key, value in zip(keys, values): self.parameters[key] += value</span></span></code> </pre> <br><br>  Here is a <a href="https://ray.readthedocs.io/en/latest/example-parameter-server.html">more complete example</a> . <br><br>  To instantiate a parameter server, do this. <br><br><pre> <code class="python hljs">parameter_server = ParameterServer.remote(initial_keys, initial_values)</code> </pre><br><br>  To create four long-running workers, constantly retrieving and updating parameters, let's do this. <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@ray.remote def worker_task(parameter_server): while True: keys = ['key1', 'key2', 'key3'] #     values = ray.get(parameter_server.get.remote(keys)) #     updates = ‚Ä¶ #   parameter_server.update.remote(keys, updates) #  4   for _ in range(4): worker_task.remote(parameter_server)</span></span></code> </pre> <br><br>  <b>Ray High Level Libraries</b> <br><br>  <a href="https://ray.readthedocs.io/en/latest/rllib.html">Ray RLlib</a> is a scalable reinforcement learning library designed for use on multiple machines.  It can be used with the sample training scripts as well as through the Pytho API.  Currently it includes implementations of algorithms: <br><br><ul><li>  A3C </li><li>  DQN </li><li>  Evolutionary strategies </li><li>  PPO </li></ul><br><br>  Work is also underway on the implementation of other algorithms.  RLlib is fully compatible with <a href="https://gym.openai.com/docs/">OpenAI gym</a> . <br><br>  <a href="https://ray.readthedocs.io/en/latest/tune.html">Ray.tune</a> is an effective library for distributed search of hyper <a href="https://ray.readthedocs.io/en/latest/tune.html">parameters</a> .  It provides an API in Python for solving deep learning, reinforced learning, and other tasks that require large computing power.  Here is an illustrative example of this kind: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ray.tune <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> register_trainable, grid_search, run_experiments <span class="hljs-comment"><span class="hljs-comment">#   .     config def my_func(config, reporter): import time, numpy as np i = 0 while True: reporter(timesteps_total=i, mean_accuracy=(i ** config['alpha'])) i += config['beta'] time.sleep(0.01) register_trainable('my_func', my_func) run_experiments({ 'my_experiment': { 'run': 'my_func', 'resources': {'cpu': 1, 'gpu': 0}, 'stop': {'mean_accuracy': 100}, 'config': { 'alpha': grid_search([0.2, 0.4, 0.6]), 'beta': grid_search([1, 2]), }, } })</span></span></code> </pre> <br><br>  Current results can be dynamically visualized using special tools, for example, Tensorboard and VisKit from rllab (or directly read JSON logs).  Ray.tune supports grid search, random search, and more nontrivial early stop algorithms, such as HyperBand. <br><br>  <b>Read more about Ray</b> <br><br><ul><li>  <a href="https://arxiv.org/abs/1712.05889">Academic article</a> </li><li>  <a href="https://github.com/ray-project/ray">Code base</a> </li><li>  <a href="https://ray.readthedocs.io/en/latest/index.html">Documentation</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/420695/">https://habr.com/ru/post/420695/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../420685/index.html">To fix and neutralize: how we tamed silver. A word about bactericides for water</a></li>
<li><a href="../420687/index.html">Using VS Code to call a REST API in Azure IoT Hub [+ helpful stuff]</a></li>
<li><a href="../420689/index.html">Game design from scratch: how to start making games without experience</a></li>
<li><a href="../420691/index.html">Kivy. Xamarin. React Native. Three frameworks - one experiment</a></li>
<li><a href="../420693/index.html">The course "Development in Java" from Mail.Ru Group on Tekhnostrim channel</a></li>
<li><a href="../420697/index.html">Perpetual Theme with PHP and MySQL</a></li>
<li><a href="../420699/index.html">In Egypt, imposed fines for visiting blocked sites</a></li>
<li><a href="../420701/index.html">VSBI invites you to a game design lecture night on August 29</a></li>
<li><a href="../420703/index.html">Summary of the book "Negotiations without defeat. Harvard Method</a></li>
<li><a href="../420705/index.html">8 deep ideas from the book ‚ÄúThe Mentor Tribe‚Äù by Tim Ferris</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>