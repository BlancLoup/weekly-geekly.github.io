<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>An example of a simple neural network, as a result of figuring out what's what</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Neural networks is a topic that arouses great interest and desire to understand it. But, unfortunately, it is far from everyone. When you see volumes ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>An example of a simple neural network, as a result of figuring out what's what</h1><div class="post__text post__text-html js-mediator-article">  Neural networks is a topic that arouses great interest and desire to understand it.  But, unfortunately, it is far from everyone.  When you see volumes of incomprehensible literature, you lose the desire to study, but you still want to be aware of what is happening. <br><br>  In the end, as it seemed to me, there is no better way to figure out than to simply take and create your own small project. <br><a name="habracut"></a><br>  You can read the lyric background, expanding the text, or you can skip this and go directly to the <a href="https://habr.com/ru/post/459822/">description of the neural network.</a> <br><br><div class="spoiler">  <b class="spoiler_title">What is the point of doing your project.</b> <div class="spoiler_text">  Pros: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ol><li>  You better understand how neurons work </li><li>  You better understand how to work with existing libraries. </li><li>  In parallel, you are learning something new. </li><li>  Tickling your Ego by creating something of your own </li></ol><br>  Minuses: <br><br><ol><li>  You create a bicycle, and most likely worse than the existing ones. </li><li>  No one cares about your project. </li></ol><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Select language.</b> <div class="spoiler_text">  At the time of choosing the language, I more or less knew C ++, and was familiar with the basics of Python.  It is easier to work with neurons in Python, but C ++ knew better and there is no simpler parallelization of computations than OpenMP.  Therefore, I chose C ++, and the API under Python, in order not to bother, will create a <a href="http://www.swig.org/">swig</a> that runs on Windows and Linux.  ( <a href="https://www.youtube.com/watch%3Fv%3DYhAFOBcSoLw%26list%3DPL8fA_qjSFodf_RiWz-7441XQu8Cm0F25z%26index%3D4">An example of</a> how to make a library for Python from C code) <br></div></div><br><div class="spoiler">  <b class="spoiler_title">OpenMP and GPU acceleration.</b> <div class="spoiler_text">  Currently, Visual Studio has OpenMP version 2.0., Which has only CPU acceleration.  However, since version 3.0, OpenMP also supports GPU acceleration, and the directive syntax has not become more complicated.  It remains only to wait until OpenMP 3.0 will be supported by all compilers.  For now, for simplicity, only CPU. <br></div></div><br><div class="spoiler">  <b class="spoiler_title">My first rake.</b> <div class="spoiler_text">  In calculating the value of a neuron there is the following point: before we calculate the activation function, we need to add the weights multiplication by the input data.  As taught to do this at the university: before summing up a large vector of small numbers, it must be sorted in ascending order.  So here.  In neural networks, except for slowing down the program N times, this does not do anything.  But I realized this only when I had already tested my network on MNIST. <br></div></div><br><div class="spoiler">  <b class="spoiler_title">Posting a project on GitHub.</b> <div class="spoiler_text">  I'm not the first to upload my creation on GitHub.  But in most cases, clicking on the link, you see only a bunch of code with an inscription in README.md <i>"This is my neural network, see and learn</i> . <i>"</i>  To be better than others at least in this, more or less described the <a href="https://github.com/RadioRedFox/FoxNN">README.md</a> and filled in the <a href="https://github.com/RadioRedFox/FoxNN/wiki">Wiki</a> .  The message is simple - <b><u>fill in the Wiki.</u></b>  An interesting observation: if the title on the Wiki on GitHub is written in Russian, then the <i>anchor</i> for this title does not work. <br></div></div><br><div class="spoiler">  <b class="spoiler_title">License.</b> <div class="spoiler_text">  When you create your own small project, a license is again a way to tickle your Ego.  Here is an interesting <a href="https://habr.com/ru/post/243091/">article</a> on the subject of what a license is needed for.  I opted for <a href="">APACHE 2.0</a> . <br></div></div><br><a name="description"></a><h2>  Network description. </h2><br>  Specifications: <br><div class="scrollable-table"><table><tbody><tr><td>  Title </td><td>  <a href="https://github.com/RadioRedFox/FoxNN">FoxNN (Fox-Neural-Network</a> ) </td></tr><tr><td>  operating system </td><td>  Windows linux </td></tr><tr><td>  Languages </td><td>  C ++, Python </td></tr><tr><td>  Acceleration </td><td>  CPU (GPU in the plans) </td></tr><tr><td>  External dependencies </td><td>  No (pure C ++, STL, OpenMP) </td></tr><tr><td>  Compilation flags </td><td>  -std = c ++ 14 -fopenmp </td></tr><tr><td>  Layers </td><td>  linear (convolutional in the plans) </td></tr><tr><td>  Optimization </td><td>  <a href="http://ruder.io/optimizing-gradient-descent/">Adam, Nesterov</a> </td></tr><tr><td>  Random change of scales </td><td>  there is </td></tr><tr><td>  Wikipedia (instruction) </td><td>  <a href="https://github.com/RadioRedFox/FoxNN/wiki">there is</a> </td></tr></tbody></table></div><br><br>  The main advantage of my library is to create a network with one line of code. <br><br>  It is easy to see that in linear layers the number of neurons in one layer is equal to the number of input parameters in the next layer.  Another obvious statement is that the number of neurons in the last layer equals the number of output values ‚Äã‚Äãof the network. <br><br>  Let's create a network that receives three parameters as input, which has three layers with 5, 4 and 2 neurons. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> foxnn nn = foxnn.neural_network([<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>])</code> </pre> <br>  If you look at the picture, you can just see: first 3 input parameters, then a layer with 5 neurons, then a layer with 4 neurons and, finally, the last layer with 2 neurons. <br><br><img src="https://habrastorage.org/webt/gn/zp/ll/gnzplle8ndd-6unyqbys0l1ppeq.jpeg"><br><br>  By default, all activation functions are sigmoid (I like them more). <br>  If desired, on any layer can be changed to another function. <br><br><div class="spoiler">  <b class="spoiler_title">In the presence of the most popular activation functions.</b> <div class="spoiler_text"><ul><li>  <a href="https://en.wikipedia.org/wiki/Activation_function">sigmoid</a> - sigmoid </li><li>  <a href="https://en.wikipedia.org/wiki/Activation_function">sinusoid</a> - sine </li><li>  <a href="https://en.wikipedia.org/wiki/Activation_function">gaussian</a> - gaus </li><li>  <a href="https://en.wikipedia.org/wiki/Activation_function">relu</a> - linear rectifier </li><li>  <a href="https://en.wikipedia.org/wiki/Activation_function">identity_x - identical</a> </li><li>  <a href="https://en.wikipedia.org/wiki/Activation_function">tan_h</a> - th </li><li>  <a href="https://en.wikipedia.org/wiki/Activation_function">arctan</a> - arctangent </li><li>  <a href="https://en.wikipedia.org/wiki/Activation_function">elu</a> - exponential linear function </li></ul><br></div></div><br><pre> <code class="python hljs">nn.get_layer(<span class="hljs-number"><span class="hljs-number">0</span></span>).set_activation_function(<span class="hljs-string"><span class="hljs-string">"gaussian"</span></span>)</code> </pre><br>  Easy to create learning sample.  The first vector is the input data, the second vector is the target data. <br><br><pre> <code class="python hljs">data = foxnn.train_data() data.add_data([<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>]) <span class="hljs-comment"><span class="hljs-comment">#   ,    </span></span></code> </pre><br>  Network training: <br><br><pre> <code class="python hljs">nn.train(data_for_train=data, speed=<span class="hljs-number"><span class="hljs-number">0.01</span></span>, max_iteration=<span class="hljs-number"><span class="hljs-number">100</span></span>, size_train_batch=<span class="hljs-number"><span class="hljs-number">98</span></span>)</code> </pre><br>  Enable optimization: <br><br><pre> <code class="python hljs">nn.settings.set_mode(<span class="hljs-string"><span class="hljs-string">"Adam"</span></span>)</code> </pre><br>  And a method to simply get the value of the network: <br><br><pre> <code class="python hljs">nn.get_out([<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>])</code> </pre><br><div class="spoiler">  <b class="spoiler_title">A little about the name of the method.</b> <div class="spoiler_text">  Separately, <i>get</i> means how <i>to get</i> , and <i>out</i> - <i>output</i> .  I wanted to get the name " <i>give the output value</i> ", and got it.  Only later noticed what happened <i>vymataytes</i> .  But so fun, and decided to leave. <br></div></div><br><h3>  Testing </h3><br>  It has already entered into the unspoken tradition of testing any network based on <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> .  And I did not become an exception.  All code with comments can be viewed <a href="">here</a> . <br><br><div class="spoiler">  <b class="spoiler_title">Creates a training sample:</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mnist <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MNIST <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> foxnn mndata = MNIST(<span class="hljs-string"><span class="hljs-string">'C:download/'</span></span>) mndata.gz = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span> imagesTrain, labelsTrain = mndata.load_training() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(images, labels)</span></span></span><span class="hljs-function">:</span></span> train_data = foxnn.train_data() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> im, lb <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(images, labels): data_y = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-comment"><span class="hljs-comment"># len(data_y) == 10 data_y[lb] = 1 data_x = im for j in range(len(data_x)): #     (-1, 1) data_x[j] = ((float(data_x[j]) / 255.0) - 0.5) * 2.0 train_data.add_data(data_x, data_y) #     return train_data train_data = get_data(imagesTrain, labelsTrain)</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Create a network: three layers, input 784 parameters, and 10 output:</b> <div class="spoiler_text"><pre> <code class="python hljs">nn = foxnn.neural_network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">512</span></span>, <span class="hljs-number"><span class="hljs-number">512</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>]) nn.settings.n_threads = <span class="hljs-number"><span class="hljs-number">7</span></span> <span class="hljs-comment"><span class="hljs-comment">#     7  nn.settings.set_mode("Adam") #   </span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">We teach:</b> <div class="spoiler_text"><pre> <code class="python hljs">nn.train(data_for_train=train_data, speed=<span class="hljs-number"><span class="hljs-number">0.001</span></span>, max_iteration=<span class="hljs-number"><span class="hljs-number">10000</span></span>, size_train_batch=<span class="hljs-number"><span class="hljs-number">98</span></span>)</code> </pre><br></div></div><br>  What happened: <br><br>  In about 10 minutes (only CPU acceleration), you can get an accuracy of 75%.  With the optimization of Adam in 5 minutes you can get an accuracy of 88% percent.  In the end, I managed to achieve an accuracy of 97%. <br><br><div class="spoiler">  <b class="spoiler_title">The main disadvantages (already in the plans for revision):</b> <div class="spoiler_text"><ol><li>  In Python, errors are not yet stretched, i.e.  in python, the error will not be intercepted and the program will simply end with an error. </li><li>  So far, training is indicated in iterations, and not in epochs, as is customary in other networks. </li><li>  No GPU acceleration </li><li>  There are no other types of layers yet. </li><li>  It is necessary to fill in the project on PyPi. </li></ol><br></div></div><br>  For a small project completeness, this article was lacking.  If at least ten people are interested and play around, then there will already be a victory.  Welcome to my <a href="https://github.com/RadioRedFox/FoxNN">github</a> . <br><br>  PS: If you need to create something of your own to figure it out, do not be afraid and create. </div><p>Source: <a href="https://habr.com/ru/post/459822/">https://habr.com/ru/post/459822/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../45981/index.html">Label tag</a></li>
<li><a href="../459810/index.html">Microservices or monolith: looking for a solution</a></li>
<li><a href="../459814/index.html">What are you, Rendering Engine? Or how does the browser display module work?</a></li>
<li><a href="../459816/index.html">Neural Networks and Deep Learning, Chapter 3, Part 2: Why does regularization help reduce retraining?</a></li>
<li><a href="../459820/index.html">Just swipe the card: as in the New York subway using OS / 2</a></li>
<li><a href="../459828/index.html">News of the week: price of a Hyperloop ticket in Russia, mining on the Apollo computer program, AI-bot in StarCraft II</a></li>
<li><a href="../45983/index.html">The story with the "hacker Danila"</a></li>
<li><a href="../459830/index.html">Of course, they gave power and turn from a machine gun. Cancer and more ... experience with medicine</a></li>
<li><a href="../459832/index.html">9 rules cool extension for Visual Studio</a></li>
<li><a href="../459840/index.html">XAML Hot Reload Preview for Xamarin.Forms has been released</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>