<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Recurrent neural network in 10 lines of code appreciated the feedback from viewers of the new episode of ‚ÄúStar Wars‚Äù</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello, Habr! We recently received from Izvestia an order to conduct a public opinion survey on the film Star Wars: The Force Awakens, which premiered ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Recurrent neural network in 10 lines of code appreciated the feedback from viewers of the new episode of ‚ÄúStar Wars‚Äù</h1><div class="post__text post__text-html js-mediator-article">  Hello, Habr!  We recently received from Izvestia an order to conduct a public opinion survey on the film Star Wars: The Force Awakens, which premiered on December 17th.  To do this, we decided to analyze the tone of the Russian segment of Twitter on several relevant hashtags.  The results were expected from us in just 3 days (and this is at the very end of the year!), So we needed a very fast way.  We found several similar online services on the Internet (including <a href="http://www.sentiment140.com/">sentiment140</a> and <a href="https://www.csc.ncsu.edu/faculty/healey/tweet_viz/tweet_app/">tweet_viz</a> ), but it turned out that they do not work with Russian and for some reason analyze only a small percentage of tweets.  The <a href="http://www.alchemyapi.com/developers/getting-started-guide/twitter-sentiment-analysis">AlchemyAPI</a> service would help us, but the limit of 1000 requests per day also did not suit us.  Then we decided to make our own blackjack tonality analyzer and the rest, creating a simple recurrent memory neural network.  The results of our study were used in the <a href="http://izvestia.ru/news/601050">article</a> ‚ÄúIzvestia‚Äù, published on January 3. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/2eb/2ea/ba0/2eb2eaba083941f98a34a7778317d154.jpg" height="450"></div><br><br>  In this article I will talk a little about this kind of networks and introduce a pair of cool tools for home experiments that will allow even schoolchildren to build neural networks of any complexity with a few lines of code.  Welcome under cat. <br><a name="habracut"></a><br><h4>  What is RNN? </h4><br>  The main difference between recurrent networks (Recurrent Neural Network, RNN) and traditional networks is the logic of the network, in which each neuron interacts with itself.  As a rule, a signal which is some sequence is transmitted to the input of such networks.  Each element of such a sequence is alternately transmitted to the same neurons, which return their prediction to themselves along with its next element, until the sequence ends.  Such networks, as a rule, are used when working with serial information - mainly with texts and audio / video signals.  Elements of the recurrent network are depicted as normal neurons with an additional cyclic arrow, which demonstrates that, in addition to the input signal, the neuron also uses its additional hidden state.  If you ‚Äúexpand‚Äù such an image, you get a whole chain of identical neurons, each of which receives its own sequence element at the input, produces a prediction and passes it further along the chain as a kind of memory cell.  You need to understand that this is an abstraction, because it is one and the same neuron that works out several times in a row. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/a71/91e/86a/a7191e86a40565f276bed7327c2c8ead.png" alt="Expanded Recurrent Neural Network Scheme"><br><br>  Such a neural network architecture allows solving such tasks as predicting the last word in a sentence, for example the word ‚Äúsun‚Äù in the phrase ‚Äúthe sun shines in a clear sky‚Äù. <br><br>  Modeling memory in a neural network in a similar way introduces a new dimension to the description of its work process ‚Äî time.  Let the neural network receive a sequence of data at the input, for example, a text word by word or a word by letter.  Then each next element of this sequence arrives at the neuron at a new conditional point in time.  By this time, the neuron already has accumulated experience from the beginning of the arrival of information.  In the example of the sun, the vector characterizing the preposition ‚Äúin‚Äù will appear as x <sub>0</sub> , the word ‚Äúsky‚Äù as x <sub>1</sub> and so on.  As a result, as h <sub>t there</sub> should be a vector close to the word "sun". <br><br>  The main difference between different types of recurrent neurons from each other lies in how the memory cell is processed inside them.  The traditional approach involves the addition of two vectors (signal and memory) with the subsequent calculation of the activation of the sum, for example, a hyperbolic tangent.  It turns out the usual grid with one hidden layer.  A similar scheme is drawn as follows: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4bc/36f/34c/4bc36f34c9e82339c34a157d9bb9c435.png" alt="Simplest Recurrent Neural Network" height="270"></div><br><br>  But the memory implemented in this way is very short.  Since each time the information in memory is mixed with the information in the new signal, after 5-7 iterations, the information is already completely overwritten.  Returning to the task of predicting the last word in a sentence, it should be noted that within one sentence such a network will work well, but if it comes to a longer text, the patterns in its beginning will no longer make any contribution to the network‚Äôs solutions closer to the end text, as well as the error on the first elements of the sequences in the learning process ceases to contribute to the overall network error.  This is a very conditional description of this phenomenon, in fact, it is a fundamental problem of neural networks, which is called the <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">problem of a vanishing gradient</a> , and because of it, the third ‚Äúwinter‚Äù of deep learning at the end of the 20th century began when the neural networks were 1.5 decades have lost the lead to support vector machines and boosting algorithms. <br><br>  To overcome this deficiency, an LSTM-RNN network ( <i>Long Short-Term Memory Recurent Neural Network</i> ) was invented, in which additional internal transformations were added that operate on the memory more carefully.  Here is its scheme: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9cf/4d1/ea3/9cf4d1ea3d472997666e9caf703a8b1b.png" alt="LSTM Recurrent Network" height="270"></div><br>  Let's go more in detail on each of the layers: <br><br>  The first layer calculates how much at this step it needs to forget the previous information ‚Äî essentially factors to the components of the memory vector. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/c6b/647/214/c6b6472145df4c5fbdcc10fc1be07143.png" height="35" alt="Formula 1"></div><br><br>  The second layer calculates how interesting the new information is for it, which came with a signal - the same factor, but for observation. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/d58/ba7/a63/d58ba7a6390841339c00ec4645f3a6d8.png" height="70" alt="Formula 2"></div><br><br>  On the third layer, a linear combination of memory and observation is calculated with only the weights computed for each of the components.  This is how a new state of memory is obtained, which is passed on in the same way. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/ab8/276/b36/ab8276b3613c4201903b2a360abc49be.png" height="35" alt="Formula 3"></div><br><br>  It remains to calculate the output.  But since a part of the input signal is already in memory, it is not necessary to consider the activation for the entire signal.  First, the signal passes through a sigmoid, which decides which part of it is important for further decisions, then the hyperbolic tangent "smears" the memory vector in the range from -1 to 1, and at the end these two vectors multiply. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/748/424/64e/74842464efe44558bd3f204a98113b3e.png" height="70" alt="Formula 4"></div><br><br>  The h <sub>t</sub> and C <sub>t</sub> thus obtained are transmitted further along the chain.  Of course, there are many variations of exactly what activation functions are used by each layer, slightly modify the schemes themselves and so on, but the essence remains the same - first they forget a part of the memory, then they remember a part of the new signal, and then the result is calculated on the basis of this data.  I took the pictures <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">from here</a> , there you can also see a few examples of more complex LSTM schemes. <br><br>  I will not talk here in detail about how such networks are trained, let me just say that the algorithm is used BPTT (Backpropagation Through Time), which is a generalization of the standard algorithm in case there is time on the network.  You can read about this algorithm <a href="http://andrew.gibiansky.com/blog/machine-learning/recurrent-neural-networks/">here</a> or <a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/">here</a> . <br><br><h4>  Using LSTM-RNN </h4><br>  Recurrent neural networks built on similar principles are very popular, here are some examples of similar projects: <br><ul><li>  <a href="http://cs.stanford.edu/people/karpathy/deepimagesent/">abstract pictures</a> </li><li>  <a href="http://people.idsia.ch/~juergen/blues/">Music Creation</a> </li><li>  <a href="http://www.bioinf.jku.at/software/LSTM_protein/">protein classification</a> </li><li>  <a href="http://www.cs.toronto.edu/~graves/handwriting.html">generation of human handwriting</a> </li></ul><br>  There are also successful examples of using LSTM grids as one of the layers in hybrid systems.  Here is an example of a hybrid network that answers questions on the picture from the ‚ÄúHow many books are shown?‚Äù Series: <br><div style="text-align:center;"><img src="https://habrastorage.org/files/246/909/928/2469099284e64c6f92bff9f41b5080af.png" height="400" alt="Hybrid network layout using LSTM"></div><br>  Here the LSTM network works in conjunction with the pattern recognition module in the pictures.  Here you can compare different hybrid architectures to solve this problem. <br><br><h4>  Theano and keras </h4><br>  For the Python language, there are many very powerful libraries for creating neural networks.  Without aiming to provide at least a complete overview of these libraries, I want to introduce you to the <a href="http://deeplearning.net/software/theano/">Theano</a> library.  Generally speaking, out of the box is a very effective toolkit for working with multidimensional tensors and graphs.  Realizations of most algebraic operations on them are available, including the search for extremums of tensor functions, the calculation of derivatives, and so on.  And all this can be effectively parallelized and run calculations using CUDA technologies on video cards. <br><br>  It sounds great if it were not for the fact that Theano itself generates and compiles C ++ code.  Maybe this is my prejudice, but I am very suspicious of this kind of systems, because, as a rule, they are filled with an incredible number of bugs that are very difficult to find, perhaps because of this I have not paid enough attention to this library for a long time.  But Theano was developed at the Canadian institute <a href="http://www.mila.umontreal.ca/">MILA</a> under the leadership of Yoshua Bengio, one of the most famous specialists in the field of deep learning of our time, and for my brief experience with it, of course, I did not find any mistakes. <br><br>  However, Theano is only a library for efficient calculations, you need to independently implement backpropagation, neurons and everything else on it.  For example, <a href="http://deeplearning.net/tutorial/code/lstm.py">here is the code</a> using only Theano of the same LSTM network about which I spoke above, and about 650 lines in it, which does not at all correspond to the title of this article.  But maybe I would never have tried to work with Theano, if it were not for the amazing library of <a href="http://keras.io/">keras</a> .  Being in fact only sugar for the interface of Theano, it just solves the problem stated in the header. <br><br>  At the core of any code using keras is a model object that describes the order in which and which layers your neural network contains.  For example, the model that we used to assess the tonality of Star Wars tweets took as input a sequence of words, so its type was <br><pre><code class="python hljs">model = Sequential()</code> </pre> <br>  After declaring the model type, layers are sequentially added to it, for example, you can add an LSTM layer with <a href="http://keras.io/layers/recurrent/">this</a> command: <br><pre> <code class="python hljs">model.add(LSTM(<span class="hljs-number"><span class="hljs-number">64</span></span>))</code> </pre><br>  After all the layers are added, the model should be compiled, if desired, specifying the type of the loss function, the optimization algorithm and a few more settings: <br><pre> <code class="python hljs">model.compile(loss=<span class="hljs-string"><span class="hljs-string">'binary_crossentropy'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, class_mode=<span class="hljs-string"><span class="hljs-string">"binary"</span></span>)</code> </pre><br>  The compilation takes a couple of minutes, after which the model has clear, fit (), predict (), predict_proba () and evaluate () methods available to everyone.  So simple, in my opinion this is an ideal option in order to begin to dive into the depths of deep learning.  When the opportunities of keras will be missed and you want, for example, to use your own loss functions, you can go down a level and write a part of the code on Theano.  By the way, if programs that other programs generate themselves also frighten someone, you <a href="http://keras.io/backend/">can</a> connect the fresh <a href="https://www.tensorflow.org/">TensorFlow</a> from Google as a backend to keras, but for now it works much slower. <br><br><h4>  Tweet analysis </h4><br>  Let us return to our original task - to determine whether the Russian viewers liked Star Wars or not.  I used the simple <a href="https://github.com/ckoepp/TwitterSearch">TwitterSearch</a> library as a handy tool to follow on Twitter search results.  Like all open APIs of large systems, Twitter has certain <a href="https://dev.twitter.com/rest/public/rate-limiting">limitations</a> .  The library allows you to call a callback after each request, so it is very convenient to arrange pauses.  Thus, about 50,000 tweets in Russian were downloaded using the following hashtags: <br><ul><li>  #starwars </li><li>  #star Wars </li><li>  #star #wars </li><li>  #star Wars </li><li>  # AwakeningPower </li><li>  #TheForceAwakens </li><li>  # awakening # force </li></ul><br><br>  While they were pumped out, I started searching for a training sample.  In English, there are several marked tweet corps in free access, the largest of which is the Stanford training <a href="http://help.sentiment140.com/for-students/">sample of</a> sentiment140 mentioned at the very beginning, and there is also a <a href="http://neuro.compute.dtu.dk/wiki/Sentiment_analysis">list of</a> small datasets.  But they are all in English, and the task was set specifically for Russian.  In this regard, I would like to express a special gratitude to the graduate student (probably already former?) Of the Institute of Informatics Systems named.  A.P. Ershova of the SB RAS of Yulia Rubtsova, who laid out the corpus of almost 230,000 marked (with more than 82% accuracy) tweets in open access.  There would be more people in our country who donate support the community.  In general, they worked with this dataset, you can read about it and download it <a href="http://study.mokoron.com/">here</a> . <br><br>  I cleared all tweets from unnecessary, leaving only continuous sequences of Cyrillic characters and numbers that I drove through <a href="http://snowball.tartarus.org/algorithms/russian/stemmer.html">PyStemmer</a> .  Then I replaced the same words with the same numeric codes, eventually getting a dictionary of about 100,000 words, and tweets presented themselves as sequences of numbers, they are ready for classification.  I didn‚Äôt clean out the low-frequency garbage, because the grid is smart and would guess that there is too much. <br><br>  Here is our neural network code on keras: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sequence <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> np_utils <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.core <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense, Dropout, Activation <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.embeddings <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Embedding <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.recurrent <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LSTM max_features = <span class="hljs-number"><span class="hljs-number">100000</span></span> maxlen = <span class="hljs-number"><span class="hljs-number">100</span></span> batch_size = <span class="hljs-number"><span class="hljs-number">32</span></span> model = Sequential() model.add(Embedding(max_features, <span class="hljs-number"><span class="hljs-number">128</span></span>, input_length=maxlen)) model.add(LSTM(<span class="hljs-number"><span class="hljs-number">64</span></span>, return_sequences=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model.add(LSTM(<span class="hljs-number"><span class="hljs-number">64</span></span>)) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>)) model.add(Activation(<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'binary_crossentropy'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, class_mode=<span class="hljs-string"><span class="hljs-string">"binary"</span></span>) model.fit( X_train, y_train, batch_size=batch_size, nb_epoch=<span class="hljs-number"><span class="hljs-number">1</span></span>, show_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span> ) result = model.predict_proba(X)</code> </pre><br><br>  Except for imports and variable declarations, exactly 10 lines came out, and it could be written in one.  Let's run through the code.  Online 6 layers: <br><br><ol><li>  The Embedding layer, which prepares features, the settings indicate that there are 100,000 different features in the dictionary, and the grid should wait for a sequence of no more than 100 words. </li><li>  Then, two LSTM layers, each of which outputs a tensor dimension of batch_size / length of a sequence / units in LSTM, and the second gives a matrix of batch_size / units in LSTM.  To make the second one understand the first one, the return_sequences = True flag is set. </li><li>  The dropout layer is responsible for retraining.  It resets the random half of features and prevents co-adaptation of scales in layers (we believe the <a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">Canadians</a> to use the word). </li><li>  Dense-layer is a normal linear unit, which weightedly summarizes the components of the input vector. </li><li>  The last activation layer pushes this value in the interval from 0 to 1 so that it becomes a probability.  In essence, Dense and Activation in this order is a logistic regression. </li></ol><br><br>  In order for learning to occur on the GPU when executing this code, you need to set the appropriate flag, for example: <br><br><pre> <code class="bash hljs">THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python myscript.py</code> </pre><br><br>  On the GPU, this very model was trained almost 20 times faster than on the CPU - about 500 seconds on dataset of 160,000 tweets (a third of the tweets went for validation). <br><br>  For such tasks there are no clear rules for the formation of the network topology.  We honestly spent half a day experimenting with different configurations, and this one showed the best accuracy - 75%.  We compared the result of grid prediction with an ordinary logistic regression, which showed 71% accuracy on the same dataset when text was vectorized using the tf-idf method and about the same 75%, but using tf-idf for bigrams.  The reason that the neural network almost did not overtake the logistic regression is most likely because the training sample was still too small (to be honest, such a network requires at least 1 million tweets of the training sample) and is noisy.  The training took place in just 1 epoch, since then we fixed a strong retraining. <br><br>  The model predicted the likelihood of a tweet positive;  we considered positive feedback with this probability from 0.65, negative - to 0.45, and the interval between them - neutral.  By day, the dynamics are as follows: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/df0/54c/44f/df054c44ff6f491a945972855d5dc2d3.png"></div><br><br>  In general, it is clear that people liked the film rather.  Although I personally do not really :) <br><br><h4>  Network examples </h4><br>  I selected 5 examples of tweets from each group (the indicated number is the probability that the feedback is positive): <br><br><h5>  Positive tonality </h5><br>  0.9945: <br><blockquote>  You can breathe out calmly, the new Star Wars are old school excellent.  Abrams - cool, as always.  Scenario, music, actors and filming - perfect. - snowdenny (@maximlupashko) <a href="https://twitter.com/maximlupashko/status/677487036652257281">December 17, 2015</a> </blockquote><br><br>  0.9171: <br><blockquote>  I advise everyone to go to star wars super film‚Äî Nikolay (@ shans9494) <a href="https://twitter.com/shans9494/status/679328766905204736">December 22, 2015</a> </blockquote><br><br>  0.8428: <br><blockquote>  POWER WAKE UP!  YES WILL ARRIVE WITH YOU THE POWER OF TODAY AT THE PREMIER OF THE MIRACLE THAT YOU WAITED FOR 10 YEARS!  <a href="https://twitter.com/hashtag/TheForceAwakens%3Fsrc%3Dhash">#TheForceAwakens</a> <a href="https://twitter.com/hashtag/StarWars%3Fsrc%3Dhash">#StarWars</a> - Vladislav Ivanov (@Mrrrrrr_J) <a href="https://twitter.com/Mrrrrrr_J/status/677248112600227840">December 16, 2015</a> </blockquote><br><br>  0.8013: <br><blockquote>  Although I am not a fan of <a href="https://twitter.com/hashtag/StarWars%3Fsrc%3Dhash">#StarWars</a> , but this performance is wonderful!  <a href="https://twitter.com/hashtag/StarWarsForceAwakens%3Fsrc%3Dhash">#StarWarsForceAwakens</a> <a href="https://t.co/1hHKdy0WhB">https://t.co/1hHKdy0WhB</a> - Oksana Storozhuk (@atn_Oksanasova) <a href="https://twitter.com/atn_Oksanasova/status/677134660707688449">December 16, 2015</a> </blockquote><br><br>  0.7515: <br><blockquote>  Who looked star wars today?  I am I :)) - Anastasiya Ananich (@NastyaAnanich) <a href="https://twitter.com/NastyaAnanich/status/678354891597836288">December 19, 2015</a> </blockquote><br><br><br><h5>  Mixed pitch </h5><br>  0.6476: <br><blockquote>  New Star Wars is better than the first episode, but worse than all the others - Igor Larionov (@ Larionovll1013) <a href="https://twitter.com/Larionovll1013/status/678254071581683713">December 19, 2015</a> </blockquote><br><br>  0.6473: <br><div class="spoiler">  <b class="spoiler_title">plot spoiler</b> <div class="spoiler_text"><blockquote>  Han Solo will die.  Enjoy watching.  #starwars - Nick Silicone (@nicksilicone) <a href="https://twitter.com/nicksilicone/status/677141919202484224">December 16, 2015</a> </blockquote><br></div></div><br><br>  0.6420: <br><blockquote>  All around Star Wars.  Am I alone in the subject?  : / - Olga (@dlfkjskdhn) <a href="https://twitter.com/dlfkjskdhn/status/678231087605248000">December 19, 2015</a> </blockquote><br><br>  0.6389: <br><blockquote>  To go or not to go to Star Wars, that is the question - annet_p (@anitamaksova) <a href="https://twitter.com/anitamaksova/status/677422879714201600">December 17, 2015</a> </blockquote><br><br>  0.5947: <br><blockquote>  Star Wars left a double impression.  And not very good.  In some places it was not felt that they were the very ... something alien was slipping‚Äî Kolot Eugene (@ KOLOT1991) <a href="https://twitter.com/KOLOT1991/status/678912128867303424">December 21, 2015</a> </blockquote><br><br><br><h5>  Negative tonality </h5><br>  0.3408: <br><blockquote>  There are so many conversations around, are I really not a fan of Star Wars?  <a href="https://twitter.com/hashtag/StarWars%3Fsrc%3Dhash">#StarWars</a> <a href="https://twitter.com/hashtag/StarWarsTheForceAwakens%3Fsrc%3Dhash">#StarWarsTheForceAwakens</a> - modern mind (@ modernmind3) <a href="https://twitter.com/modernmind3/status/677392497191092224">December 17, 2015</a> </blockquote><br><br>  0.1187: <br><blockquote>  they pulled my poor heart out of my chest and shattered it into millions and millions of fragments. <a href="https://twitter.com/hashtag/StarWars%3Fsrc%3Dhash">#StarWars</a> - Remi Evans (@Remi_Evans) <a href="https://twitter.com/Remi_Evans/status/679276760551264256">December 22, 2015</a> </blockquote><br><br>  0.1056: <br><blockquote>  I hate the knock-outs, I got star wars from me - the nayla's pajamas (@harryteaxxx) <a href="https://twitter.com/harryteaxxx/status/677579921842184193">December 17, 2015</a> </blockquote><br><br>  0.0939: <br><blockquote>  I woke up and realized that the new Star Wars was disappointing. - Tim Frost (@Tim_Fowl) <a href="https://twitter.com/Tim_Fowl/status/678497018499735552">December 20, 2015</a> </blockquote><br><br>  0.0410: <br><blockquote>  I am disappointed <a href="https://twitter.com/hashtag/%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B1%25D1%2583%25D0%25B6%25D0%25B4%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5%25D1%2581%25D0%25B8%25D0%25BB%25D1%258B%3Fsrc%3Dhash"># waking up the</a> force - Eugenjkee;  Star Wars (@eugenjkeee) <a href="https://twitter.com/eugenjkeee/status/678614150092820480">December 20, 2015</a> </blockquote><br><br><br>  PS Already after the study was conducted, they came across an <a href="http://anthology.aclweb.org/C/C14/C14-1008.pdf">article</a> in which they praise convolutional networks for solving this problem.  Next time we try them, in keras they are also <a href="http://keras.io/layers/convolutional/">supported</a> .  If one of the readers decides to check himself, write in the comments about the results, it is very interesting.  May the Power of Big Data be with you! </div><p>Source: <a href="https://habr.com/ru/post/274027/">https://habr.com/ru/post/274027/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../274017/index.html">Comparison of sorting algorithms</a></li>
<li><a href="../274019/index.html">Soviet school: address programming language</a></li>
<li><a href="../274021/index.html">Trading and "iron": What are stock exchange data centers</a></li>
<li><a href="../274023/index.html">Select PHP version for a specific CMS and do not cry</a></li>
<li><a href="../274025/index.html">Dagger 2 and the structure of the application for Android</a></li>
<li><a href="../274029/index.html">Free hosting control panels. Goddess vesta</a></li>
<li><a href="../274033/index.html">A little bit about working with containers</a></li>
<li><a href="../274035/index.html">SVG highs and lows</a></li>
<li><a href="../274037/index.html">How to kill the smartphone Sony Xperia C2305 using a router D-Link DIR-300</a></li>
<li><a href="../274041/index.html">Interactive Client Map - Apache Spark Streaming and Yandex.Maps</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>