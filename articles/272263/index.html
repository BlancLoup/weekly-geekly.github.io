<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to monitor all layers of infrastructure</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Once I figured out that 1 minute of idle time on hh.ru in the daytime affects about 30,000 users. We constantly solve the problem of reducing the numb...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to monitor all layers of infrastructure</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/549/f19/c10/549f19c106684dc5bb498d3519c305a3.png" alt="image"><br><br>  Once I figured out that 1 minute of idle time on hh.ru in the daytime affects about 30,000 users.  We constantly solve the problem of reducing the number of incidents and their duration.  We can reduce the number of problems with the right infrastructure, application architecture - this is a separate issue, we will not take it into account yet.  Let's talk better about how to quickly understand what is happening in our infrastructure.  This is where monitoring helps us. <br><br>  In this article, using the example of hh.ru, I will tell and show you how to monitor all layers of the infrastructure: <br><ul><li>  client-side metrics </li><li>  metrics from frontend (nginx logs) </li><li>  network (which can be obtained from TCP) </li><li>  application (logs) </li><li>  database metrics (postgresql in our case) </li><li>  operating system (cpu usage may also be useful) </li></ul><br><a name="habracut"></a><br><h1>  Formulate the task </h1><br>  I formulated for myself the tasks that monitoring should solve: <br><ul><li>  find out what's broken </li><li>  quickly find out where it broke </li><li>  see what is fixed </li><li>  capacity planning: do we have enough resources to grow </li><li>  optimization planning: choose where you need to optimize for the effect </li><li>  control of optimizations: if after the release of optimization in the production effect is not visible, then the release should be rolled back, and the code should be thrown out (or reformulated) </li></ul><br>  First, a few words about the hh.ru architecture: all logic is distributed among several dozens of services on java / python, pages for users are ‚Äúcollected‚Äù by our frontik collection service, the main database is postgresql, on frontends and internal balancers nginx + haproxy is used. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/files/a4b/72e/8b5/a4b72e8b506a4ca7876b480afd8338d4.png" alt="image"><br><br>  Now let's start monitoring all layers.  In the end, I want: <br><ul><li>  see how the system works from the user's point of view </li><li>  see what happens in each subsystem </li><li>  know what resources go </li><li>  look at it all in time: as it was a minute ago, yesterday, last Monday </li></ul><br>  That is, we will talk about graphs.  About alerts, workflow, KPI, you can view <a href="http://www.slideshare.net/NikolaySivko/rootconf2015">my slides</a> from RootConf 2015 (found the same ‚Äú <a href="https://www.youtube.com/watch%3Fv%3DmbcYXzE-J2Y">screen</a> ‚Äù). <br><br><h1>  Client-side metrics </h1><br><br><img src="https://habrastorage.org/files/abd/574/fc9/abd574fc94004105b4f48105456cacef.png" alt="image"><br>  The most reliable information about how the user sees the site is in the browser.  It can be obtained through the <a href="http://www.w3.org/TR/navigation-timing/">Navigation timing API</a> , our js snapshot hits the metrics to the GET server with a request with parameters, then it is accepted by nginx: <br><pre><code class="nginx hljs"><span class="hljs-attribute"><span class="hljs-attribute">location</span></span> = /stat { <span class="hljs-attribute"><span class="hljs-attribute">return</span></span> <span class="hljs-number"><span class="hljs-number">204</span></span>; <span class="hljs-attribute"><span class="hljs-attribute">access_log</span></span> /var/log/nginx/clientstat.access.log; }</code> </pre> <br>  then we just parse the resulting log and get the following graph: <br><br><img src="https://habrastorage.org/files/11f/57a/275/11f57a275d9146dfbb961fb761fd551a.png" alt="image"><br><br>  These are stacked 95th percentiles since the main stages of rendering the page.  "Processing" can be defined differently, in this case, the user sees the page much earlier, but our frontend developers decided that they need to see it that way. <br>  We see that the channels are normal (in the ‚Äútransfer‚Äù stage), there are no emissions in the ‚Äúserver‚Äù, ‚Äútcp‚Äù stages. <br><br><h1>  What to remove from the frontend </h1><br>  Our main monitoring is based on the metrics we remove from the frontends.  On this layer we want to find out: <br><ul><li>  Are there any bugs?  How many? </li><li>  Fast or slow?  One user or all? </li><li>  How many requests?  As usual / failure / bots? </li><li>  Is the channel tupit to customers? </li></ul><br>  All this is in the nginx log, you only need to slightly <a href="http_core_module.html">expand</a> its default format: <br><ul><li>  $ request_time - time from receiving the first byte of the request to writing the last byte of the response to the socket (we can assume that we take into account the transmission of the response through the network) </li><li>  $ upstream_response_time - how much the backend thought </li><li>  Optional: $ upstream_addr, $ upstream_status, $ upstream_cache_status </li></ul><br>  The histogram is well suited for visualizing the entire request flow. <br>  We have already determined what a quick request is (up to 500ms), medium (500ms-1s), slow (1s +). <br>  We draw requests per second along the Y axis, the response time is reflected in color, and we also added errors (HTTP-5xx). <br>  Here is an example of our ‚Äútraffic light‚Äù based on $ request_time: <br><br><img src="https://habrastorage.org/files/549/f19/c10/549f19c106684dc5bb498d3519c305a3.png" alt="image"><br><br>  We see that we have a bit more than 2krps at the peak, the majority of requests are fast (there is an exact legend in the tooltip), on this day there were 2 error shots affecting 10-15% of requests. <br>  Let's see how the graph differs if we take $ upstream_response_time as a basis: <br><br><img src="https://habrastorage.org/files/893/474/883/893474883930400ba916b1c120218bc0.png" alt="image"><br><br>  It can be seen that fewer requests reach the backends (some are given from the nginx cache), there are almost no ‚Äúslow‚Äù requests, that is, in the previous graph red is mainly the contribution of the transmission over the network to the user, rather than waiting for the backend response. <br>  On any of the graphs, you can consider in more detail the scale of the number of errors: <br><br><img src="https://habrastorage.org/files/4c9/c1c/7fe/4c9c1c7febb84351bfcc3cdbcf6233da.png" alt="image"><br><br>  It is immediately clear that there were 2 outliers: the first short ~ 500rps, the second longer ~ 100rps. <br>  Errors sometimes need to be expanded to url: <br><img src="https://habrastorage.org/files/1d4/a15/9e0/1d4a159e0ec6491f9afd9137f74f51ce.png" alt="image"><br><br>  In this case, the errors are not on any one URL, but are distributed across all the main pages. <br>  On the main dashboard, we also have a schedule with HTTP-4xx errors, we separately look at the number of HTTP-429 (we give this status if the limit on the number of requests from one IP is exceeded). <br><br><img src="https://habrastorage.org/files/ee9/21e/4fe/ee921e4fef5a44bc86d3f7ac35fb0b99.png" alt="image"><br><br>  Here you can see that some bot came.  More detailed information on bots is easier to see in the log with your eyes, in monitoring, it is enough for us to simply have the fact that something has changed.  All errors can be broken down by specific statuses; this is done quite simply and quickly. <br><br>  Sometimes it is useful to look at the traffic structure, which URLs generate how much traffic. <br><br><img src="https://habrastorage.org/files/0ab/f44/2a6/0abf442a6ec54da784b83c9239011fd7.png" alt="image"><br><br>  On this graph, the layout of the static content traffic, the outburst is the release of the new release, the users had css and js caches. <br><br>  About the breakdown of metrics by urls should be mentioned separately: <br><ul><li>  manual rules for parsing basic urls become obsolete </li><li>  you can try to somehow normalize urls: <br>  throw out query arguments, id and other parameters in the URL itself <br>  / vacancy / 123? arg = value -&gt; / vacancy / $ id </li><li>  after all normalizations, the dynamic top of the URLs works well (by rps or the sum of $ upstream_response_time), you detail only the most significant requests, for the rest, total metrics are considered </li><li>  You can also make a separate top for errors, but with a cutoff from the bottom, so that there is no garbage </li></ul><br>  Our logs are parsed on each server, on the charts we usually see the final picture, but you can look at the server section too.  For example, this is convenient for balancing evaluation: <br><br><img src="https://habrastorage.org/files/96c/5d0/77f/96c5d077f3c948bbb64a54dc2630d67e.png" alt="image"><br><br>  Here you can clearly see how they were withdrawing from the front2 cluster, at which time requests were processed by 2 neighboring servers.  The total number of requests did not fail, that is, the users did not affect these works. <br>  Similar graphs based on metrics from the nginx log allow you to see well how our site works, but they do not reveal the cause of the problems. <br><br><h1>  Collector layer </h1><br>  Instant monitoring of any program gives an answer to the question: where is the control now (calculations, waiting for data from other subsystems, working with the disk, memory)?  If we consider this task in time, we need to understand at what stages (actions) it took time. <br>  In our collector service, all significant stages of each request are logged, for example: <br><br><pre> <code class="bash hljs">2015-10-14 15:12:00,904 INFO: timings <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> xhh.pages.vacancy.Page: prepare=0.83 session=4.99 page=123.84 xsl=36.63 postprocess=13.21 finish=0.23 flush=0.49 total=180.21 code=200</code> </pre> <br>  During the processing of the vacancy page (handler = "xhh.pages.vacancy.Page") we spent: <br><ul><li>  ~ 5ms per user session processing </li><li>  ~ 124ms waiting for all requests to services behind the page data </li><li>  ~ 37ms for templating </li><li>  ~ 13ms to localize </li></ul><br>  From this log, we take the 95th percentile for each stage of each handler (as we parse the logs, I will tell on a more illustrative example just below), we get graphs for all pages, for example, for a vacancy page: <br><br><img src="https://habrastorage.org/files/e28/645/e33/e28645e3336e49688ab2339b4647fd83.png" alt="image"><br><br>  There are clearly visible emissions within specific stages. <br>  Percentiles have a number of drawbacks (for example, it is difficult to combine data from several measurements: servers / files / etc), but in this case the picture is much clearer than the histogram. <br>  If we need, for example, to get a more or less exact ratio of any stages, then we can look at the sum of times.  Summing up the response times with a breakdown by handlers, we can estimate how to occupy our entire cluster. <br><br><img src="https://habrastorage.org/files/c37/b94/7ab/c37b947ab1704d5385fad3e42bcc41ef.png" alt="image"><br><br>  On the Y axis, we have the so-called ‚Äúresource‚Äù seconds spent per second.  It is very convenient that the ratio immediately takes into account the frequency of requests, and if we have one very heavy handler rarely called, then it will not hit the top.  If you do not take into account the parallelization of processing of some stages, then we can assume that at the peak we spend 200 processor cores for all requests. <br><br>  We also applied this technique to the profiling problem of templating. <br>  The collector logs which template was rendered for each request: <br><pre> <code class="bash hljs">2015-10-14 15:12:00,865 INFO: applied XSL ambient/pages/vacancy/view.xsl <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 22.50ms</code> </pre> <br>  The monitoring service agent that we use can parse logs, for this we need to write something like this config: <br><pre> <code class="hljs tex">plugin: logparser config: file: /var/log/hh-xhh.log regex: '(?P&lt;datetime&gt;<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">d</span></span></span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">{4}</span></span></span></span>-<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">d</span></span></span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">{2}</span></span></span></span>-<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">d</span></span></span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">{2}</span></span></span></span> <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">d</span></span></span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">{2}</span></span></span></span>:<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">d</span></span></span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">{2}</span></span></span></span>:<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">d</span></span></span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">{2}</span></span></span></span>),<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">d</span></span></span></span>+ [^:]+: applied XSL (?P&lt;xsl_file&gt;<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">S</span></span></span></span>+) in (?P&lt;xsl_time&gt;<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">d</span></span></span></span>+<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">.</span></span></span></span><span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">d</span></span></span></span>+)ms' time_field: datetime time_field_format: 2006-01-02 15:04:05 metrics: - type: rate name: xhh.xsl.calls.rate labels: xsl_file: =xsl_file - type: rate name: xhh.xsl.cpu_time.rate value: ms2sec:xsl_time labels: xsl_file: =xsl_file - type: percentiles name: xhh.xsl.time.percentiles value: ms2sec:xsl_time args: [50, 75, 95, 99] labels: xsl_file: =xsl_file</code> </pre><br>  From the received metrics we draw the schedule: <br><br><img src="https://habrastorage.org/files/cbf/2e9/ff5/cbf2e9ff5e5046e6a879721855d57bd6.png" alt="image"><br><br>  If suddenly with the release we will have the time to overlay one template (or all at once), we will see it.  We can also choose what exactly makes sense to optimize ... <br><br><img src="https://habrastorage.org/files/356/40a/99a/35640a99ab074157b86ed039d06fb2cb.png" alt="image"><br><br>  ... and check that the optimization has brought results.  In this example, a generic piece was optimized, which is included in all other templates. <br>  What happens on the collectors, we now understand quite well, moving on. <br><br><h1>  Internal balancer </h1><br><br><img src="https://habrastorage.org/files/06a/99b/6d7/06a99b6d7670405db86022d51cf2351a.png" alt="image"><br><br>  In case of breakdowns, as a rule, the question arises: if a specific service broke down (if yes, which one) or all at once (for example, if there are problems with databases, other storages, a network).  Each service is behind the balancers with nginx, the logs for each service are written in a separate access log. <br>  A simple schedule of top-5 services by the number of errors greatly simplified our lives: <br><br><img src="https://habrastorage.org/files/4ff/cd7/019/4ffcd70196de44aa9720ca27eacc5f24.png" alt="image"><br><br>  Here you can see that the most errors occur in the session service log.  If we have a new service / log, then it will be distributed automatically and the new data will be immediately taken into account in this top.  We also have histograms of response time for all services, but more often we look at this graph. <br><br><h1>  Services </h1><br><br><img src="https://habrastorage.org/files/d38/113/59d/d3811359d942463e84524c4948275786.png" alt="image"><br><br>  In the service logs we also have the request processing stages: <br><pre> <code class="bash hljs">2015-10-21 10:47:10.532 [Grizzly-worker(14)] INFO r.hh.health.monitoring.TimingsLogger: response: 200; context: GET /hh-session/full; total time 10 ms; jersey<span class="hljs-comment"><span class="hljs-comment">#beforeHandle=+0; HHid#BeforeClient=+0; HHid#Client=+6; DB#getHhSession=+3; pbMappers=+0; makeHeaderSession=+1; currentSessionInBothBodyAndHeader=+0; jersey#afterHandle=+0;</span></span></code> </pre> <br>  Visualize: <br><br><img src="https://habrastorage.org/files/452/f3a/101/452f3a10197a4fe5a9c2aea49bf9ac09.png" alt="image"><br><br>  It can be seen that the database is responsible for the constant time, and the hhid service at attendance peaks slows down a bit. <br><br><h1>  Database </h1><br><img src="https://habrastorage.org/files/64e/203/f35/64e203f358114047964d5996db66ef7b.png" alt="image"><br><br>  All the major databases we are working on postgresql.  We outsource database administration to postgresql-consulting, but we ourselves also need to understand what is happening with the database, since it is not advisable to pull the DBA for all the problems. <br>  The main question that worried us is: is everything OK now with the database?  Based on <a href="http://www.postgresql.org/docs/9.4/static/pgstatstatements.html">pg_stat_statements,</a> we draw a graph of the average query <a href="http://www.postgresql.org/docs/9.4/static/pgstatstatements.html">execution</a> time: <br><br><img src="https://habrastorage.org/files/351/51f/c33/35151fc33cac40a796ff61e9667d03f3.png" alt="image"><br><br>  According to it, it is clear whether something unusual is happening in the database now or not (unfortunately nothing can be done on the available data except the average).  Then we want to know what the base is doing at the moment, while we are primarily interested in what requests load CPU and disks: <br><br><img src="https://habrastorage.org/files/de6/707/ea5/de6707ea530d422d8337d313d118b28e.png" alt="image"><br><br>  These are top-2 requests for CPU usage (we remove top50 + other).  Here you can clearly see the release of a specific query to the hh_tests database.  If necessary, it can be completely copied and viewed. <br><br>  Postgresql also has statistics about how many requests are waiting for disk operations.  For example, we want to know what caused this outlier on / dev / sdc1: <br><img src="https://habrastorage.org/files/6f1/dc3/d6e/6f1dc3d6e8f54b6ba22788c72854f43a.png" alt="image"><br><br>  Having built a graph of top requests for io, we easily get an answer to our question: <br><br><img src="https://habrastorage.org/files/fdc/d7d/53b/fdcd7d53b3d1466b98ecc38113d6a1de.png" alt="image"><br><br><h1>  Network </h1><br><br><img src="https://habrastorage.org/files/02b/553/ff9/02b553ff95ab4e59b02a102b1ccbe2ae.png" alt="image"><br><br>  The network is a rather unstable environment.  Often, when we do not understand what is happening, we can write off the brakes of services to problems with the network.  For example: <br><ul><li>  hhsession waited for a response from hhid 150ms </li><li>  the hhid service thinks that it responded to this request in 5ms (we all requests have the identifier $ request_id, we can recognize specific requests from the logs) </li><li>  <i>between them is only a network</i> </li></ul><br>  You do not have ping results for yesterday between these hosts.  How to exclude a network? <br><br><h1>  TCP RTT </h1><br>  TCP Round Trip Time - the time from sending a packet to receiving an ACK.  A monitoring agent on each server removes rtt for all current connections.  For ip from our network, we aggregate the times separately, we get something like these metrics: <br><pre> <code class="hljs 1c">{ <span class="hljs-string"><span class="hljs-string">"name"</span></span>: <span class="hljs-string"><span class="hljs-string">"netstat.connections.inbound.rtt"</span></span>, <span class="hljs-string"><span class="hljs-string">"plugin"</span></span>: <span class="hljs-string"><span class="hljs-string">"netstat"</span></span>, <span class="hljs-string"><span class="hljs-string">"source_hostname"</span></span>: <span class="hljs-string"><span class="hljs-string">"db17"</span></span>, <span class="hljs-string"><span class="hljs-string">"listen_ip"</span></span>: <span class="hljs-string"><span class="hljs-string">"0.0.0.0"</span></span>, <span class="hljs-string"><span class="hljs-string">"listen_port"</span></span>: <span class="hljs-string"><span class="hljs-string">"6503"</span></span>, <span class="hljs-string"><span class="hljs-string">"remote_ip: "</span></span><span class="hljs-number"><span class="hljs-number">192.168</span></span>.<span class="hljs-number"><span class="hljs-number">1.48</span></span><span class="hljs-string"><span class="hljs-string">", "</span></span>percentile<span class="hljs-string"><span class="hljs-string">": "</span></span><span class="hljs-number"><span class="hljs-number">95</span></span><span class="hljs-string"><span class="hljs-string">", }</span></span></code> </pre><br>  Based on these metrics, you can look backwards at how the network worked between any two hosts on the network. <br>  But TCP RTT values ‚Äã‚Äãdon't even closely match ICMP RTT (what ping shows).  The fact is that TCP is quite complex, it uses many different optimizations (SACK, etc.), well illustrated by the picture from <a href="http://www.cs.virginia.edu/~cs458/slides/module15-tcp3V2.ppt">this presentation</a> : <br><br><img src="https://habrastorage.org/files/03e/164/834/03e1648346e84e2b93760af5092e0bba.png" alt="image"><br><br>  Each TCP connection has one RTT counter, it is clear that RTT # 1 is more or less honest, in the second case we sent 3 segments during the measurement, in the third case we sent 1 segment, we received several ACKs. <br>  In practice, TCP RTT is fairly stable between the selected hosts. <br><br><img src="https://habrastorage.org/files/4ed/cd6/b9a/4edcd6b9af3d407db92e7d4270eb8fee.png" alt="image"><br><br>  This is a TCP RTT between the master database and the replica in another data center.  You can clearly see how the connection disappeared while working on the channel, how the network was slowed down after recovery.  At the same time TCP RTT ~ 7ms with ping ~ 0.8ms. <br><br><h1>  OS metrics </h1><br>  We, like everyone, are looking at CPU, memory, disk usage, disk IO utilization, traffic on network interfaces, packet rate, swap usage.  Separately, we need to mention swap i / o - a very useful metric, which allows us to understand whether swap was actually used or if it just contains unused pages. <br><br>  But these metrics are not enough if you want to retroactively understand what was happening on the server.  For example, look at the CPU graph on the database master server: <br><br><img src="https://habrastorage.org/files/da1/88e/79e/da188e79edf3437e8d233b51701b8b3b.png" alt="image"><br>  What was it? <br><br>  To answer such questions, we have metrics for all running processes: <br><ul><li>  CPU per process + user </li><li>  Memory per process + user </li><li>  Disk I / O per process + user </li><li>  Swap per process + user </li><li>  Swap I / O per process + user </li><li>  FD usage per process + user </li></ul><br>  Around this interface, we can ‚Äúplay‚Äù with metrics in real time: <br><br><img src="https://habrastorage.org/files/113/060/073/113060073b4e48b5af603aaf153b290f.png" alt="image"><br><br>  In this case, we run a backup on the server, almost never used pg_dump, but the contribution of pbzip2 is clearly visible. <br><br><h1>  We summarize </h1><br>  By monitoring you can understand a lot of different tools.  For example, you can use an external service to check the availability of the main page once a minute, so you can learn about the main failures.  This was not enough for us, we wanted to learn to see a complete picture of the quality of the site‚Äôs work.  With the current decision, we see all the meaningful variations from the usual picture. <br><br>  The second part of the task is to reduce the duration of incidents.  Here, the carpet carpet helped us a lot by monitoring all the components.  Separately, we had to be confused with the simple visualization of a large number of metrics, but after we made good dashboards, life became much simpler. <br><br>  - <br>  <i>The article was written based on my report on Highload ++ 2015, slides are available <a href="http://www.slideshare.net/NikolaySivko/web-54778827">here</a></i> . <br>  <i>I can not fail to mention the monitoring service <a href="https://okmeter.io/">okmeter.io</a> , which we have been <a href="https://okmeter.io/">using</a> for more than a year.</i> </div><p>Source: <a href="https://habr.com/ru/post/272263/">https://habr.com/ru/post/272263/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../272253/index.html">Let's Encrypt go public beta: HTTPS everywhere, to everyone, from now on and forever free</a></li>
<li><a href="../272255/index.html">The news called for the road: a super-fast, energy-efficient optical coprocessor for big data</a></li>
<li><a href="../272257/index.html">Access tables from C extensions for Postgres</a></li>
<li><a href="../272259/index.html">Law enforcement authorities dismantled the Dorkbot botnet</a></li>
<li><a href="../272261/index.html">Do you still store 404backup.zip on the server? I'm on 200D</a></li>
<li><a href="../272265/index.html">History of one optimization: transmission and processing of the results of the battle</a></li>
<li><a href="../272267/index.html">Kudu - a new data storage engine in the Hadoop ecosystem</a></li>
<li><a href="../272269/index.html">memset - side of darkness</a></li>
<li><a href="../272275/index.html">We invite speakers to the PHDays VI forum: tell us about your cyberwar</a></li>
<li><a href="../272277/index.html">When people make mistakes</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>