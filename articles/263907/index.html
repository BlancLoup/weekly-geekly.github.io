<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Correlation, covariance and deviation (part 3)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the first part, we talked about the essence of the transformation of the deviation and its application to the matrix of squares of distances. In th...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Correlation, covariance and deviation (part 3)</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/62f/ced/aa3/62fcedaa30d148f0b90eb962585b6ce9.jpg"><br><br>  <a href="http://habrahabr.ru/post/262601/">In the first part,</a> we talked about the essence of the transformation of the deviation and its application to the matrix of squares of distances.  <a href="http://habrahabr.ru/post/262703/">In the second, a</a> little fog was put on the spectra of simple geometric sets. <br><br>  In this article, we will try to uncover the meaning of the deviation transformation, for which we turn to the applied problems associated with data processing and analysis.  Let us show how the transformation of the deviation of the distance matrix with statistics is related to <a href="https://ru.wikipedia.org/wiki/%25D0%2594%25D0%25B8%25D1%2581%25D0%25BF%25D0%25B5%25D1%2580%25D1%2581%25D0%25B8%25D1%258F_%25D1%2581%25D0%25BB%25D1%2583%25D1%2587%25D0%25B0%25D0%25B9%25D0%25BD%25D0%25BE%25D0%25B9_%25D0%25B2%25D0%25B5%25D0%25BB%25D0%25B8%25D1%2587%25D0%25B8%25D0%25BD%25D1%258B">variance</a> , <a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25BE%25D1%2580%25D1%2580%25D0%25B5%25D0%25BB%25D1%258F%25D1%2586%25D0%25B8%25D1%258F">correlation</a> and <a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25BE%25D0%25B2%25D0%25B0%25D1%2580%25D0%25B8%25D0%25B0%25D1%2586%25D0%25B8%25D1%258F">covariance</a> . <br><a name="habracut"></a><br><h2>  7. Centering and rationing of one-dimensional coordinates </h2><br>  We will warm up in a simple and understandable way - centering and normalizing the data.  May we have a series of numbers <img src="https://tex.s2cms.ru/svg/V_i">  .  Then the centering operation is reduced to finding the average (set centroid) 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://tex.s2cms.ru/svg/%5Coverline%7BV%7D%3D1%2Fn%5Csum%5Climits_%7Bi%3D1%7D%5En%7BV_i%7D%5Cquad%20(7.1)"><br><br>  and the construction of a new set as the difference between the original numbers and their centroid (average): <br><br><img src="https://tex.s2cms.ru/svg/X_i%3DV_i-%5Coverline%7BV%7D%5Cquad%20(7.2)"><br><br>  Centering is the first step to the original coordinate system (FCS) of the original set, since the sum of centered coordinates is 0. The second step is normalizing the sum of squares of centered coordinates to 1. To perform this operation, we need to calculate this amount (or more precisely, the average): <br><br><img src="https://tex.s2cms.ru/svg/S%3D1%2Fn%5Csum%5Climits_%7Bi%3D1%7D%5En%20%7BX_i%5E2%7D%3D1%2Fn%5Csum%5Climits_%7Bi%3D1%7D%5En%20%7B(V_i-%5Coverline%7BV%7D)%5E2%7D%3D1%2Fn%5Csum%5Climits_%7Bi%3D1%7D%5En%20%7B(V_i%5E2-2V_i%5Coverline%7BV%7D%2B%5Coverline%7BV%7D%5E2)%7D%3D%5Coverline%7BV%5E2%7D-%5Coverline%7BV%7D%5E2%5Cquad%20(7.3)"><br><br>  Now we can construct the FCS of the original set as a set of the eigenvalue <i>S</i> and the normalized numbers (coordinates): <br><br><img src="https://tex.s2cms.ru/svg/x_i%3DX_i%2F%5Csqrt%7BS%7D%5Cquad%20(7.4)"><br><br>  The squares of the distances between points of the original set are defined as the differences of the squares of the components of the eigenvector multiplied by the eigenvalue.  Note that the eigenvalue <i>S</i> turned out to be equal to the <a href="https://ru.wikipedia.org/wiki/%25D0%2594%25D0%25B8%25D1%2581%25D0%25BF%25D0%25B5%25D1%2580%25D1%2581%25D0%25B8%25D1%258F_%25D1%2581%25D0%25BB%25D1%2583%25D1%2587%25D0%25B0%25D0%25B9%25D0%25BD%25D0%25BE%25D0%25B9_%25D0%25B2%25D0%25B5%25D0%25BB%25D0%25B8%25D1%2587%25D0%25B8%25D0%25BD%25D1%258B">variance of the</a> initial set (7.3). <br><br>  So, for <i>any set of numbers,</i> you can determine your own coordinate system, that is, select the value of the eigenvalue (also known as variance) and calculate the coordinates of the eigenvector by centering and normalizing the original numbers.  Cool. <br><br>  Exercise for those who love to "feel with their hands."  Build ssk for the set {1, 2, 3, 4}. <br><div class="spoiler">  <b class="spoiler_title">Answer.</b> <div class="spoiler_text">  Own number (variance): 1.25. <br>  The eigenvector: {-1.342, -0.447, 0.447, 1.342}. <br></div></div><br><h2>  8. Centering and <i>orthoration of</i> multidimensional coordinates </h2><br>  What if instead of a set of numbers we are given a set of vectors - pairs, triples and other dimensions of numbers.  That is, a point (node) is defined not by one coordinate, but by several.  How in this case to construct SSC? <br><br>  Yes, you can build a matrix of squares of distances, then determine the deviation matrix and calculate the spectrum for it.  But we learned about this <a href="http://habrahabr.ru/post/262601/">not so long ago</a> .  Usually they did (and do) differently. <br><br>  We introduce the notation of the component set.  We are given points (nodes, variables, vectors, tuples) <img src="https://tex.s2cms.ru/svg/V_i">  and each point is characterized by numerical components <img src="https://tex.s2cms.ru/svg/V_%7Bia%7D">  .  Please note that the second index <img src="https://tex.s2cms.ru/svg/a">  Is the number of the component (matrix columns), and the first index <img src="https://tex.s2cms.ru/svg/i">  - the number of the point (node) of the set (matrix row). <br><br>  What do we do next?  That's right - we center the components.  That is, for each column (components) we find the centroid (average) and subtract it from the value of the components: <br><br><img src="https://tex.s2cms.ru/svg/%5Coverline%7BV_a%7D%3D1%2Fn%5Csum%5Climits_%7Bi%3D1%7D%5En%20%7BV_%7Bia%7D%20%5Cquad%20(7.1')"><br><br><img src="https://tex.s2cms.ru/svg/X_%7Bia%7D%3DV_%7Bia%7D-%5Coverline%7BV_a%7D%20%5Cquad%20(7.2')"><br><br>  We got a centered data matrix (IDC) <img src="https://tex.s2cms.ru/svg/X_%7Bia%7D">  . <br>  The next step, we seem to need to calculate the variance for each component and normalize them.  But we will not do this.  Because although in this way we really get normalized vectors, we need something to make these vectors independent, that is, <i>orthonormal</i> .  The operation of valuation does not rotate the vectors (but only changes their length), and we need to expand the vectors perpendicular to each other.  How to do it? <br><br>  The correct (but still useless) answer is to calculate the eigenvectors and numbers (spectrum).  Useless because we have not built a matrix for which the spectrum can be considered.  Our centered data matrix (IDC) is not square - you cannot calculate eigenvalues ‚Äã‚Äãfor it.  Accordingly, we need to build a square matrix on the basis of the MCD.  This can be done by multiplying the IDC by itself (squaring it). <br><br>  But here - attention!  Non-square matrix can be squared in two ways - by multiplying the original by the <a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D1%2580%25D0%25B0%25D0%25BD%25D1%2581%25D0%25BF%25D0%25BE%25D0%25BD%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BC%25D0%25B0%25D1%2582%25D1%2580%25D0%25B8%25D1%2586%25D0%25B0">transposed one</a> .  And vice versa - by multiplying the transposed by the original.  The dimension and meaning of the two matrices obtained are different. <br><br>  Multiplying the IDC by the transposed, we get the correlation matrix: <br><br><img src="https://tex.s2cms.ru/svg/G_%7Bij%7D%3D%5Csum_a%7BX_%7Bia%7DX_%7Bja%7D%7D%3DXX%5ET%20%5Cquad%20(8.1)"><br><br>  From this definition (there are others) it follows that the elements of the correlation matrix are scalar products of centered vectors.  Accordingly, the elements of the main diagonal reflect the square of the length of these vectors. <br>  The values ‚Äã‚Äãof the matrix are not normalized (they are usually normalized, but for our purposes this is not necessary).  The dimension of the correlation matrix coincides with the number of initial points (vectors). <br><br>  Now, we interchange the matrices multiplied in (8.1) and obtain <i>the covariance matrix</i> (again, omitting the factor <i>1 / (1-n)</i> , which is usually normalized by the values ‚Äã‚Äãof covariance): <br><br><img src="https://tex.s2cms.ru/svg/C_%7Bab%7D%3D%5Csum_i%7BX_%7Bia%7DX_%7Bib%7D%7D%3DX%5ETX%20%5Cquad%20(8.2)"><br><br>  Here components are multiplied (and not vectors).  Accordingly, the dimension of the covariance matrix is ‚Äã‚Äãequal to the number of initial components.  For pairs of numbers, the covariance matrix has a dimension of 2x2, for triples it is 3x3, and so on. <br><br>  Why is the dimension of correlation and covariance matrices important?  The point is that since the correlation and covariance matrices come from the product of the same vector, they have the same set of eigenvalues, the same <i>rank</i> (the number of independent dimensions) of the matrix.  As a rule, the number of vectors (points) far exceeds the number of components.  Therefore, the rank of the matrices is judged by the dimension of the covariance matrix. <br><br>  The diagonal elements of covariance reflect the variance of the components.  As we saw above, variance and eigenvalues ‚Äã‚Äãare closely related.  Therefore, it can be said that in the first approximation, the eigenvalues ‚Äã‚Äãof the covariance matrix (and, hence, correlations) are equal to the diagonal elements (and if there is no inter-component dispersion, then they are equal in any approximation). <br><br>  If the task is to find simply the spectrum of matrices (eigenvalues), then it is more convenient to solve it for the covariance matrix, since, as a rule, their dimension is small.  But if we also need to find eigenvectors (define our own coordinate system) for the initial set, then we need to work with the correlation matrix, since it is this that reflects the multiplication of vectors.  It is possible that the optimal algorithm is a combination of the diagonalizations of the two matrices ‚Äî first, find the eigenvalues ‚Äã‚Äãfor covariance and then determine the eigenvectors of the correlation matrix on their basis. <br><br>  Well, since we have gone so far, we will mention that the notorious <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%25B3%25D0%25BB%25D0%25B0%25D0%25B2%25D0%25BD%25D1%258B%25D1%2585_%25D0%25BA%25D0%25BE%25D0%25BC%25D0%25BF%25D0%25BE%25D0%25BD%25D0%25B5%25D0%25BD%25D1%2582">method of principal components</a> consists in calculating the spectrum of the covariance / correlation matrix for a given set of vector data.  The found spectrum components are located along the main axes of the data ellipsoid.  From our consideration this follows because the main axes are those axes, the dispersion (scatter) of the data for which is maximum, and therefore the maximum value of the spectrum. <br><br>  True, there may be negative variances, and then the analogy with an ellipsoid (pseudo-ellipsoid?) Is no longer obvious. <br><br><h2>  9. The distance deviation matrix is ‚Äã‚Äãa vector correlation matrix. </h2><br>  All this is fine, but where is the transformation of the deviation? <br><br>  Consider the situation when we know not a set of numbers (vectors) characterizing some points (nodes), but a set of distances between points (and between all).  Is this information sufficient to determine the SSC (own coordinate system) of the set? <br><br>  We gave the answer in the <a href="http://habrahabr.ru/post/262601/">first part</a> - yes, completely.  Here we will show that the matrix of deviations of squares of distances constructed by the formula (1.3 ') and the matrix of correlation of centered vectors defined by us above (8.1) is <i>the same matrix</i> . <br><br>  How did this happen?  Themselves in shock.  To verify this, we must substitute the expression for the element of the matrix of squares of distances <br><br><img src="https://tex.s2cms.ru/svg/D_%7Bij%7D%3D%5Csum_a%7B(V_%7Bia%7D-V_%7Bja%7D)%5E2%7D%20%5Cquad%20(9.1)"><br><br>  in the deviation conversion formula: <br><br><img src="https://tex.s2cms.ru/svg/G_%7Bij%7D%3D(%5Coverline%7BD_i%7D%2B%5Coverline%7BD_j%7D-%5Coverline%7BD%7D-D_%7Bij%7D)%20%20%5Cquad%20(9.2)"><br><br>  Note that the average value of the matrix of squares of distances reflects the variance of the original set (provided that the distances in the set are the sum of the squares of the components): <br><br><img src="https://tex.s2cms.ru/svg/%5Coverline%7BD%7D%3D1%2Fn%5E2%5Csum_%7Bij%7D%7BD_%7Bij%7D%7D%3D2%5Csum_a%7B%0A(%5Coverline%7BV_a%5E2%7D-%5Coverline%7BV_a%7D%5E2)%7D%20%20%5Cquad%20(9.3)"><br><br>  Substituting (9.1) and (9.3) into (9.2), after simple abbreviations, we arrive at the expression for the correlation matrix (8.1): <br><br><img src="https://tex.s2cms.ru/svg/G_%7Bij%7D%3D%5Csum_a%7B%5Cleft(%5Coverline%7BV_a%7D%5E2%2BV_%7Bia%7DV_%7Bja%7D-%5Coverline%7BV_a%7D(V_%7Bia%7D%2BV_%7Bja%7D)%5Cright)%7D%0A%3D%5Csum_a%7B%5Cleft((V_%7Bia%7D-%5Coverline%7BV_a%7D)(V_%7Bja%7D-%5Coverline%7BV_a%7D)%5Cright)%7D%0A%3D%5Csum_a%7BX_%7Bia%7DX_%7Bja%7D%7D%0A%3DXX%5ET%20%5Cquad%20(9.4)"><br><br>  So, we have seen that applying the deviation operation to the matrix of Euclidean distances, we get the known correlation matrix.  The rank of the correlation matrix coincides with the rank of the covariance matrix (the number of components of the Euclidean space).  This circumstance allows us to build the spectrum and our own coordinate system for the original points based on the distance matrix. <br><br>  For an arbitrary distance matrix (not necessarily Euclidean), the potential rank (number of dimensions) is one less than the number of source vectors.  The calculation of the spectrum (own coordinate system) allows you to determine the main (main) components that affect the distance between points (vectors). <br><br>  The matrix of distances between cities, for example, is obviously non-Euclidean, - no components (characteristics of cities) are specified.  The deviation transformation nevertheless allows us to determine the spectrum of such a matrix and the own coordinates of cities. <br><br>  But not in this article.  That's all for now, thanks for your time. <br></div><p>Source: <a href="https://habr.com/ru/post/263907/">https://habr.com/ru/post/263907/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../263897/index.html">UI performance comparison in WPF, Qt, WinForms and FLTK</a></li>
<li><a href="../263899/index.html">In app purchase using Soomla. Fast and easy</a></li>
<li><a href="../263901/index.html">Webinar second. Bug work, work tools and customer relationship building methods</a></li>
<li><a href="../263903/index.html">New critical vulnerabilities in Android: What is the problem, and how to protect</a></li>
<li><a href="../263905/index.html">‚ÄúSpeed ‚Äã‚Äãis such a feature that no one ever refuses‚Äù - an interview with Dmitry Zhemerov from JetBrains</a></li>
<li><a href="../263909/index.html">Telerik's VirtualQueryableCollectionView weak spot</a></li>
<li><a href="../263913/index.html">Implementing a search engine with Python rankings (Part 2)</a></li>
<li><a href="../263915/index.html">Architectural solutions for the Bitrix24 telephony system</a></li>
<li><a href="../263919/index.html">Gadgets as a source of constant distractions</a></li>
<li><a href="../263925/index.html">"Mindless" use of fonts</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>