<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Autopilot on your own: sensor fusion from the phone and open training data</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello. We continue to collect autopilot on computer vision from githubs and sticks ( start here ). Today we are connecting to the motion sensors of th...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Autopilot on your own: sensor fusion from the phone and open training data</h1><div class="post__text post__text-html js-mediator-article"><p>  Hello.  We continue to collect autopilot on computer vision from githubs and sticks ( <a href="https://habrahabr.ru/post/325704/">start here</a> ).  Today we are connecting to the motion sensors of the smartphone (accelerometer, gyroscope and GPS receiver) on Android, we are developing an uncomplicated sensor fusion and finally closing it with the code to collect the training sample.  Bonuses - <a href="https://play.google.com/store/apps/details%3Fid%3Dru.pilotguru.recorder">Android application</a> for recording all sensors synchronized with video and more than an hour of <a href="https://habr.com/ru/post/329484/">tagged data</a> as part of import substitution of <a href="https://medium.com/udacity/challenge-2-using-deep-learning-to-predict-steering-angles-f42004a36ff3">foreign contests</a> .  All code is still <a href="https://github.com/waiwnf/pilotguru">on github</a> . </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6aa/2b6/1ed/6aa2b61ed1ca3ffae63de6e230c26486.jpg"></div><br><p>  These are three-axis <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B8%25D0%25BA%25D1%2580%25D0%25BE%25D1%258D%25D0%25BB%25D0%25B5%25D0%25BA%25D1%2582%25D1%2580%25D0%25BE%25D0%25BC%25D0%25B5%25D1%2585%25D0%25B0%25D0%25BD%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B8%25D0%25B5_%25D1%2581%25D0%25B8%25D1%2581%25D1%2582%25D0%25B5%25D0%25BC%25D1%258B">MEMS</a> accelerometer and gyroscope, they will be extremely useful to us. <a name="habracut"></a></p><br><p>  In a nutshell, I‚Äôll remind you (for the details, welcome to the <a href="https://habrahabr.ru/post/325704/">previous post</a> that the ultimate goal is to train a system that will be able to give out control actions for the car (video steering angle and the desired speed or acceleration) by video frame from the windshield.) video from the races, where the machine is controlled by a man, and time-synchronized information about the angle of rotation and speed.In modern cars, this data can be read through the <a href="https://ru.wikipedia.org/wiki/Controller_Area_Network">CAN bus</a> , but first you need to buy and connect a special CAN adapter  ter, and secondly, to deal with the decoding of the protocol and data format, which are different for different manufacturers. Instead of all this trouble, we calculate the control actions indirectly - from raw data from an ordinary smartphone, which is not connected to anything else. </p><br><p>  In the last series, we learned how to determine the angular speed of a car turning in a horizontal plane based on only video frames using <a href="https://github.com/raulmur/ORB_SLAM2">the</a> <a href="https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping">video-SLAM</a> <a href="https://github.com/raulmur/ORB_SLAM2">library</a> .  Unfortunately, in practice this library very inaccurately calculates the <em>translational</em> speed of the device.  And without knowing the translational speed, we cannot train even the steering component for the autopilot separately, since the angular velocity <img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/omega.gif" alt="\ omega">  depends on the combination of the degree of rotation of the steering wheel (equivalently, the turning radius <img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/r.gif" alt="r">  ) and the forward speed of the car <img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/v.gif" alt="v">  . </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/301/a14/957/301a14957b92a64d0de80d407ae5d6e1.png"></div><br><p>  To get a forward speed, today we are switching from video processing to motion sensors, which almost any smartphone has: a GPS receiver, an accelerometer and a gyroscope.  Let's look at the pros and cons of each, combine the information and get the instantaneous speed for each frame of video.  Moreover, we will also take information about the rotations from the gyroscope, which will make it possible to abandon the more capricious optical SLAM and the calibration of the smartphone camera that it needs.  As a result, the new process of collecting data dries up to three "button clicks": <a href="https://play.google.com/store/apps/details%3Fid%3Dru.pilotguru.recorder">we put the application</a> on the phone - we record the track on the race - we process it with <a href="https://github.com/waiwnf/pilotguru">one command</a> on the computer. </p><br><p>  As a result, we obtain a frame-by-frame annotation with the translational speed and the speed of turns: </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/HvfqpzvW2E8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h1 id="dostupnye-datchiki-dvizheniya">  Available motion sensors </h1><br><p>  We will work with android.  Whatever the platform - for those who know English, an <a href="https://www.youtube.com/watch%3Fv%3DC7JQ7Rpwn2k">interesting video</a> about how information from different sensors can complement calibration and reduce errors.  We are interested in <a href="https://developer.android.com/guide/topics/sensors/sensors_motion.html">motion sensors</a> (accelerometer and gyroscope) and <a href="https://developer.android.com/guide/topics/location/strategies.html">location</a> (GPS) <a href="https://developer.android.com/guide/topics/sensors/sensors_motion.html">sensors</a> . </p><br><p>  From the point of view of the API, everything is simple - we ask the user for permissions, subscribe for updates, and get a stream of events with a sensor reading and measurement time.  Details can be found <a href="">in the source</a> . </p><br><p>  Consider the pros and cons of sensors. </p><br><h2 id="gps">  GPS </h2><br><p>  GPS is, as it turned out, 90% success in determining translational speed, at least in relatively open spaces.  From GPS come in the <a href="https://developer.android.com/reference/android/location/Location.html">Location</a> format: </p><br><ul><li>  Estimation of the absolute position (latitude, longitude). </li><li>  An estimate of the <a href="https://developer.android.com/reference/android/location/Location.html">accuracy of the coordinates</a> is the radius of the confidence circle of 68% around the estimate.  That is, in ~ 68% of measurements, the present position of the device must be within the limits of a given radius around the estimate.  As a rule, on more or less open space, the accuracy is within 3-4 meters. </li><li>  Absolute speed (no direction), estimated from past measured GPS positions. </li></ul><br><p>  A huge <strong>plus of</strong> GPS data is that the <strong>measurement errors are independent</strong> , that is, they do not accumulate over time: the new measurement is not affected by how many measurements there were before that and the large errors.  This is a very important feature on which the rest of the approach relies. </p><br><p>  <strong>The disadvantage</strong> is that <strong>measurements are rarely taken</strong> , about 1 time per second, and there is no sense to carry them out much more often - the movement of the device would still be comparable to a measurement error.  As a result, firstly, GPS data is late in reacting to a change in speed, and secondly, it is more difficult to remove noise from them ‚Äî to do this, you need to look at the measurements that are next in time, which aggravates the time vagueness even more.  Here is an example of a speed chart with GPS: </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c1f/bd7/6da/c1fbd76da002193d835c5d50b8e77237.png"></div><br><p>  Here, the first 25 seconds (constant acceleration and probable delay of GPS measurements) and noise between 30 and 40 seconds are suspicious.  For the seed, the same graph after processing data from the accelerometer and gyroscope: </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4b0/a90/c66/4b0a90c66033f9c26631884ecff37781.png"></div><br><p>  As you can see, there are improvements in both indicators: we react to accelerations earlier, and the outburst at 35 seconds is smoothed out. </p><br><h2 id="inercialnye-datchiki-akselerometr-i-giroskop">  Inertial sensors: accelerometer and gyroscope </h2><br><p>  Inertial sensors measure acceleration, linear (accelerometer) and centripetal (gyroscope).  The result of their measurements is an estimate of the <em>change in the movement of the</em> device relative to the previous point in time.  Mathematically, this is reflected in the use <a href="https://developer.android.com/guide/topics/sensors/sensors_overview.html">of a</a> <em>device-related</em> <a href="https://developer.android.com/guide/topics/sensors/sensors_overview.html">coordinate</a> <em>system</em> : </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dac/426/a20/dac426a20377621bf7385cf2f65a739e.png"></div><br><p> <a href="https://developer.android.com/guide/topics/sensors/sensors_motion.html">The accelerometer</a> provides linear accelerations along the three axes of the device, the <a href="https://developer.android.com/guide/topics/sensors/sensors_motion.html">gyroscope</a> - the angular velocity around the same three axes. </p><br><p>  <strong>Plus</strong> inertial sensors - the possibility of very <strong>frequent measurements</strong> , at 400 Hz and above, which by an order of magnitude overlaps the frequency of video frames at ~ 30 Hz.  The main <strong>disadvantage</strong> is the lack of communication with a fixed coordinate system.  The sensors are capable of measuring only the change relative to the previous position, therefore, to calculate the results in a fixed coordinate system, their readings <strong>need</strong> to <strong>be integrated</strong> over time, and the <strong>measurement errors accumulate</strong> - the longer the measurement period, the greater the total error at the next time point. </p><br><p>  As you can see, GPS and inertial sensors have mirror opposite advantages and disadvantages, which means you need to combine the strengths of both sources, which is what we proceed to. </p><br><h1 id="sensor-fusion-obedinyaem-informaciyu">  Sensor fusion: combine information </h1><br><p>  Having a data record from GPS and inertial sensors, it is required to evaluate: </p><br><ol><li>  Absolute translational speed with high temporal resolution. </li><li>  The angular velocity of rotation <em>around the vertical axis of the car</em> (ie, in the plane of the road at each time point). </li></ol><br><p>  We have already calculated the horizontal rotation in the <a href="https://habrahabr.ru/post/325704/">last post</a> , but as it turned out, the old way (through video analysis) is more complicated and capricious than working directly with a gyroscope, so it‚Äôs better to change the approach. </p><br><h2 id="postupatelnaya-skorost">  Forward speed </h2><br><p>  It would seem that much simpler.  From school physics we know by definition </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/velocity-integral-definition.gif" alt="\ vec {v} (t_1) = \ vec {v} (t_0) + \ int_ {t_0} ^ {t_1} \ vec {a} (t) dt"></p><br><p> But we need speed in a fixed coordinate system, and the coordinate system of the accelerometer rotates with the device.  Denote by <code>R</code> the rotation matrix from the device‚Äôs coordinate system to the fixed one, then </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/velocity-integral-with-rotation.gif" alt="\ vec {v} (t_1) = \ vec {v} (t_0) + \ int_ {t_0} ^ {t_1} R (t) \ vec {a} _ {D} (t) dt"></p><br><p>  Where <img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/acceleration-device-axes-notation.gif" alt="\ vec {a} _ {D}">  - acceleration along the coordinate axes of the device.  In turn, the rotation matrix is ‚Äã‚Äãobtained simply by integrating the rotations measured by the gyroscope ( <img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/w-capital.gif" alt="W">  - derivative of the rotation matrix, it can be calculated, knowing the angular velocity around three axes): </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/rotation-integral-definition.gif" alt="R (t_1) = R (t_0) + \ int_ {t_0} ^ {t_1} (I + W (t)) d t"></p><br><p>  And everything?  This was not the case, it works only for a spherical accelerometer in a vacuum.  Let's look at the real data from the smartphone, lying motionless on the table: </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/acceleration-still-raw-per-axis-350w.png" alt="Raw accelerometer readings per axis"></p><br><p>  Observations: </p><br><ul><li>  The force of gravity is not automatically deducted, because the accelerometer cannot fundamentally distinguish the effect of the attraction of the earth from the effect of the acceleration of the phone.  So you need to manually determine the direction and make an amendment.  Theoretically, the android offers such an amendment <a href="https://developer.android.com/guide/topics/sensors/sensors_motion.html">at the OS level</a> , but in practice it didn't really work for me. </li><li>  The total acceleration / attraction along three axes passes over 10 m / s <sup>2</sup> , which is very far from the tabular 9.81 m / s <sup>2</sup> - in 10 minutes it will roll 0.2 * 600 = 120 m / s = 432 km / h. </li></ul><br><p>  The second experiment is to write the total acceleration from the phone lying on the table with the screen up, and then with the screen down, and compare: </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/acceleration-magnitude-diff-screen-up-down-350w.png" alt="Overall measured acceleration up and down comparison"></p><br><p>  Observations: </p><br><ul><li>  The difference in excess of 0.15 m / s <sup>2 is</sup> simply a change in orientation, that is, there is a noticeable systematic error in the local coordinate system. </li><li>  Constant noise measurements in the region of 0.05 m / s <sup>2</sup> , and as we see from the integrals above - errors accumulate over time.  And when the noises are summed up, then even with unbiased noise (i.e., when the expectation of the measurement coincides with the true value), according to the <a href="https://ru.wikipedia.org/wiki/%25D0%25A6%25D0%25B5%25D0%25BD%25D1%2582%25D1%2580%25D0%25B0%25D0%25BB%25D1%258C%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D1%258C%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2582%25D0%25B5%25D0%25BE%25D1%2580%25D0%25B5%25D0%25BC%25D0%25B0">central limit theorem, the</a> variance of the sum of <code>n</code> measurements will be <img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/o-sqrt-n.gif" alt="O \ left (\ sqrt {n} \ right)">  . </li></ul><br><h3 id="model-shuma-i-avtokalibrovka">  Model noise and auto calibration </h3><br><p>  So, to get a real acceleration in a fixed coordinate system, corrections to the "raw" measurements of the accelerometer are necessary.  Let's apply a simple model: </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/accelerometer-noise-model.gif" alt="v (t_1) = v (t_0) + \ int_ {t_0} ^ {t_1} \ left (\ vec {g} + R (t) \ cdot \ left (\ vec {a} ^ {RAW} (t) + \ vec {h} \ right) \ right) dt"></p><br><p>  Where </p><br><ul><li><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/a-raw.gif" alt="\ vec {a} ^ {RAW}">  - raw accelerometer measurement, in the device coordinate system. </li><li><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/h.gif" alt="\ vec {h}">  - compensation of the accelerometer systematic error, also in the device coordinate system. </li><li><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/r-capital.gif" alt="R">  - rotation matrix from the device coordinate system to the fixed coordinate system. </li><li><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/g.gif" alt="\ vec {g}">  - constant force of the in the fixed coordinate system. </li></ul><br><p>  The rotation matrix will be obtained by integrating the angular velocities from the gyroscope, but the remaining parameters are unknown (including <img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/g.gif" alt="\ vec {g}">  because  unknown initial orientation of the smartphone).  Find the calibration parameters with GPS data.  Recall that GPS measurements are pretty accurate.  So the <strong>speed calculated by inertial sensors should be close to the speed according to GPS</strong> .  We formalize this intuition as an optimization problem.  For each interval between adjacent GPS measurements (about 1 second), we calculate the distance traveled according to inertial data, and according to GPS data, and assign the target function to the L2 metric over the entire recording time: </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/img/readme/calibration-objective-velocities.gif" alt="\ min _ {\ vec {g}, \ vec {h}, \ vec {v} _0} \ sum_ {i = 1} ^ n \ left (\ left \ | \ sum_ {k = k_i} ^ {k_ {i +1}} (\ tau_k - \ tau_ {k-1}) \ vec {v} _k ^ {IMU} \ right \ | - (t_i - t_ {i-1}) v ^ {GPS} _i \ right) ^ 2"></p><br><p>  Where </p><br><ul><li><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/i.gif" alt="i">  - GPS measurement index. </li><li><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/k-i-to-k-i-plus-1.gif" alt="k_i \ dots k_ {i + 1}">  - indices of measurement of inertial sensors that fall into the gap between measurements GPS <code>i</code> and <code>i+1</code> . </li><li><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/img/readme/tau-j.gif" alt="\ tau_j">  time <img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/j.gif" alt="j">  measurement of inertial sensors. </li></ul><br><p>  Also denote <img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/img/readme/delta-tau-j-def.gif" alt="\ Delta \ tau_j \ equiv \ tau_j - \ tau_ {j-1}">  and <img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/img/readme/delta-t-i-def.gif" alt="\ Delta t_i \ equiv t_i - t_ {i-1}">  , and substituting corrections to the raw measurements of the accelerometer, we obtain the final form of the objective function: </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/img/readme/calibration-objective-final.gif" alt="\ min _ {\ vec {g}, \ vec {h}, \ vec {v} _0} \ sum_ {i = 1} ^ n \ left (\ left \ | \ sum_ {k = k_i} ^ {k_ {i +1}} \ Delta \ tau_k \ left (\ vec {v} _0 + (\ tau_k - \ tau_0) \ vec {g} + \ sum_ {j = 1} ^ k \ Delta \ tau_j R_j \ cdot (\ vec {a} _j ^ {RAW} + \ vec {h}) \ right) \ right \ | - v ^ {GPS} _i \ Delta t_i \ right) ^ 2"></p><br><p>  It looks scary, but in fact it is a simple quadratic function, it is easy to take derivatives analytically and optimize using any numerical method, for example <a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS">L-BFGS</a> . </p><br><p>  For a start, check on a short segment in half a minute: </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/imu-gps-velocity-short-calibration-success-350w.png" alt="calibration calibrated"></p><br><p>  Here, ‚Äúby eye‚Äù it is not very clear which graph is more correct, but it is possible to declare success, at least in the sense that it was possible to select calibration parameters of the amendments, with which the velocity estimates from two different sources are very close.  Now let's try the same approach on a longer recording, about 10 minutes: </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/imu-gps-velocity-long-calibration-fail-350w.png" alt="Something went wrong"></p><br><p>  Here of course a complete failure.  It turns out that the model of simple corrections is insufficient for long time intervals, that is, there are important sources of distortions that it does not take into account.  It can be </p><br><ul><li>  Systematic errors ("care") of the gyroscope. </li><li>  Accelerometer and gyroscope white noise accumulated over time <img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/o-sqrt-t.gif" alt="O (\ sqrt {t})">  by the <a href="https://ru.wikipedia.org/wiki/%25D0%25A6%25D0%25B5%25D0%25BD%25D1%2582%25D1%2580%25D0%25B0%25D0%25BB%25D1%258C%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D1%258C%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2582%25D0%25B5%25D0%25BE%25D1%2580%25D0%25B5%25D0%25BC%25D0%25B0">central limit theorem</a> . </li><li>  Interaction with the engine vibrations (through the car body) when driving. </li></ul><br><h3 id="lokalnaya-avtokalibrovka-skolzyaschim-oknom">  Local auto calibration by sliding window </h3><br><p>  To deal with the problems of a simple model of amendments for inertial sensors can be different.  Possible options: </p><br><ol><li>  Improve accuracy by simulating unaccounted sources of error (gyroscope drift, white noise accumulation). </li><li>  Cheat </li></ol><br><p>  We cheat.  After all, in the end, we are not interested in the calibration model itself, but in the final speed values.  And if the calibration copes on short segments, you can use the standard technique with a sliding window: </p><br><ul><li>  we divide the full time interval into overlapping short segments, </li><li>  on each segment, separately, we calibrate and calculate the speed by integration, </li><li>  we average the result for each point in time over the segments that include it: </li></ul><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/sliding-window.png" alt="sliding window"></p><br><p>  Final result: </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/imu-gps-velocity-sliding-window-calibrated-final-350w.png" alt="Final result - long-range time series calibrated with overlapping sliding windows"></p><br><p>  We declare success with a forward speed, we return to the angular speed of turns. </p><br><h2 id="uglovaya-skorost-povorotov">  Corner speed </h2><br><p>  The angular speed of rotation is needed to calculate the radius of rotation (and, accordingly, the angle of rotation of the steering wheel): <img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/r-eq-v-by-omega.gif" alt="r = v / \ omega">  .  The gyroscope measures the angular velocity directly, but in three-dimensional space: in addition to the rotation around the vertical axis (yaw - actually turns), the data also reflects rotation around the transverse (pitch - change of the road gradient, passage of speed bumps) and longitudinal (roll - ride one side in a rut or pit): </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/img/readme/pitch-roll-yaw.png" alt="pitch-roll-yaw"></p><br><p>  We need to select from the three-dimensional rotations only the component around the vertical axis (in the coordinate system of the car).  Accordingly, you need to get the direction of this axis.  To do this, we use the observation that the magnitude of rotation around the vertical axis is much larger than around the longitudinal and transverse (turns at 90 degrees is the norm, but fortunately, there is no change of slope and track).  So you can simply take the axis around which the total rotation was maximum, for the vertical. </p><br><p>  Mathematically, it is convenient to isolate the dominant rotation axis by presenting each elementary rotation (that is, measuring a gyroscope) in the form of a quaternion.  Rotation around the unit axis <code>(x,y,z)</code> at an angle <img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/theta.gif" alt="\ theta">  seems to be a quaternion </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/quaternion-definition.gif" alt="q ((x, y, z), \ theta) = \ begin {pmatrix} x \ sin (\ theta / 2) \\ y \ sin (\ theta / 2) \\ z \ sin (\ theta / 2) \\ cos (\ theta / 2) \ end {pmatrix}"></p><br><p>  The convenience is that the first three components of the quaternion characterize simultaneously the direction of the axis of rotation and the amount of rotation.  Therefore, a good result is provided by <a href="http://www.chemometrics.ru/materials/textbooks/pca.htm">the principal component method</a> applied simply to the first three components of the quaternion.  After selecting the dominant axis, it turns out this picture: </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/02-imu/rotations-after-pca.png" alt="Rotations after PCA"></p><br><p>  It can be seen that a noticeably larger proportion of rotations passes around the selected dominant axis than around the other two perpendicular to it. </p><br><p>  By the way, in the <a href="https://habrahabr.ru/post/325704/">previous post</a> , the principal component method was also applied, but then I did not realize to apply it to rotations directly, but selected a horizontal plane along a three-dimensional trajectory (that is, based on displacements instead of rotations).  The new method is better not only because it does not completely need information about the displacements, but also because the vertical axis stands out <em>in the coordinate system of the device</em> .  That is, with the new approach, the vertical axis is perpendicular to the <em>local</em> plane of the road, instead of the <em>median</em> plane of the entire trajectory in the old approach.  For example, now the vertical axis turns (together with the car when moving from the road uphill to the road downhill).  As a result, the selection of the horizontal rotation of the total rotation has become more accurate. </p><br><h1 id="dannye-a-nameprocessed-data-torrenta">  Data </h1><br><p>  Bonus to the <a href="https://github.com/waiwnf/pilotguru">code</a> I want to share the <a href="https://github.com/waiwnf/pilotguru/raw/master/data/open/torrents/pilotguru-data-20170523.torrent">already processed data</a> for those who are interested to play with the training of their models.  In the torrent there are more than an hour of raw data recorded on the roads near Moscow, plus (in <code>postprocessed</code> directories) the results of calculating the translational speed and the angular speed of turns.  If you use - it will be interesting to know about the results! </p><br><p>  That's all, in the next series - learn how to predict the control actions from video. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/329484/">https://habr.com/ru/post/329484/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../329474/index.html">Intel provides access to clDNN [high-performance library for deep learning]</a></li>
<li><a href="../329476/index.html">Russian Code Cup - in the wake of the qualifying round</a></li>
<li><a href="../329478/index.html">Survival Checklist</a></li>
<li><a href="../329480/index.html">The online store's productivity is sketchy or where to start improving it</a></li>
<li><a href="../329482/index.html">A selection of free veeam tools</a></li>
<li><a href="../329488/index.html">Visual editor of letters on React + Redux. Overview, example of use and expansion</a></li>
<li><a href="../329492/index.html">Industrial mitap # 3: safe process automation is in focus</a></li>
<li><a href="../329494/index.html">About generics in PHP and why we need them</a></li>
<li><a href="../329498/index.html">Autoboxing and unboxing in Java</a></li>
<li><a href="../329500/index.html">How we won the hackathon in New York</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>