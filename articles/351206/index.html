<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Own game analytics for $ 300 per month</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="There are many products on the market that are suitable for game analytics: Mixpanel, Localytics, Flurry, devtodev, deltaDNA, GameAnalytics. But still...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Own game analytics for $ 300 per month</h1><div class="post__text post__text-html js-mediator-article"><p> <a href="https://habrahabr.ru/post/351206/"><img src="https://habrastorage.org/webt/s9/gn/mk/s9gnmkicajwqww2csfc3uhagkyi.png" alt="Meme on analytics: - Their analyst does not know how;  - Make your own;  - Long, and a lot of data;  - And the cloud?  - Are you henna overeat?  Expensive !;  - $ 300 for 1kk DAU"></a> </p><br><p>  There are many products on the market that are suitable for game analytics: Mixpanel, Localytics, Flurry, devtodev, deltaDNA, GameAnalytics.  But still many game studios are building their decision. </p><br><p>  I have worked and work with many gaming companies.  I noticed that as projects grow, studios need advanced analytics scenarios.  After several gaming companies became interested in this approach, it was decided to document it in a series of two articles. </p><br><p>  Answers to the questions "Why?", "How to do it?"  and how much does it cost?"  You will find under the cut. </p><a name="habracut"></a><br><h2>  Why? </h2><br><p>  Universal harvester is good for simple tasks.  But if you need to do something more complicated, its capabilities are not enough.  For example, the aforementioned analytics systems, in varying degrees, have limitations in the functional: </p><br><ul><li>  on the number of parameters in the event </li><li>  on the number of parameters in the funnel events </li><li>  to update frequency </li><li>  on the amount of data </li><li>  for the period of calculation of data </li><li>  on the number of funnel conditions </li><li>  and other restrictions of the same kind </li></ul><br><p>  Turnkey solutions do not give access to raw data.  For example, if you need to conduct a more detailed study.  We should not forget about machine learning and predictive analytics.  With a significant amount of analytical data, you can play around with machine learning scenarios: predicting user behavior, recommending purchases, personalized offers, etc. </p><br><blockquote>  Game studios often build their own solutions <strong>not to replace, but in addition</strong> to the existing ones. </blockquote><p>  So, what does your analytics system look like? </p><br><p>  A common approach when building analytics is <a href="https://en.wikipedia.org/wiki/Lambda_architecture">lambda architecture</a> , the analytics is divided into ‚Äúhot‚Äù and ‚Äúcold‚Äù ways.  The hot path is the data that must be processed with a minimum delay (the number of players online, payments, etc.). </p><br><p>  The cold path is the data that is processed periodically (reports for the day / month / year), as well as raw data for long-term storage. </p><br><p> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/b5b/9f4/084/b5b9f40848f200b408e48229602cf383.png" alt="Typical lambda-architecture"></a> </p><br><p>  For example, this is useful when launching marketing campaigns.  It is convenient to see how many users came from the campaigns, how many of them made the payment.  This will help to disable ineffective advertising channels as quickly as possible.  Given the marketing budgets of games, it can save a lot of money. </p><br><p>  Everything else belongs to the cold path: periodic cuts, custom reports, etc. </p><br><p>  Lack of flexibility of the universal system, just, and pushes to develop their own solutions.  As the game evolves, the need for a detailed analysis of user behavior increases.  No universal analytics system can compare with the ability to build SQL queries on data. </p><br><p>  Therefore, studios are developing their own solutions.  Moreover, the solution is often sharpened for a specific project. </p><br><p>  The studios that developed their system were unhappy that it had to be constantly maintained and optimized.  After all, if there are many projects, or they are very large, the amount of data collected is growing very quickly.  Its system begins to slow down more and require significant investments in optimization. </p><br><h2>  How? </h2><br><h3>  <strong>Technical risks</strong> </h3><br><p>  The development of an analytics system is not an easy task. <br>  Below is an example of the requirements from the studio that I was targeting. </p><br><ul><li>  Storage of large amounts of data:&gt; 3Tb </li><li>  High service load: from 1000 events per second </li><li>  Query language support (preferably SQL) </li><li>  Ensuring an acceptable request processing rate: &lt;10 min </li><li>  Infrastructure resiliency </li><li>  Providing data visualization tools </li><li>  Aggregation of regular reports </li></ul><br><p>  This is not a complete list. </p><br><p>  When I wondered how to make a decision, I was guided by the following priorities / Wishlist: </p><br><ul><li>  quickly </li><li>  cheap </li><li>  reliably </li><li>  SQL support </li><li>  possibility of horizontal scaling </li><li>  effective work with at least 3Tb data, again scaling </li><li>  the ability to process data real time </li></ul><br><p>  Since the activity in games is periodic, the solution should ideally adapt to the peaks of the load.  For example, during featureing, the load increases many times. </p><br><p>  Take, for example, Playerunknown's Battleground.  We will see clearly defined peaks during the day. </p><br><p> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/f58/65d/d13/f5865dd13e3ecc865c0b2d46c7ca244f.png" alt="Playerunknown &amp; # 039;  s Battleground Daily Peaks"></a> <br>  Source: <a href="https://steamdb.info/app/578080/graphs/">SteamDB</a> </p><br><p>  And if you look at the growth of Daily Active Users (DAU) over the course of a year, you can see a rather fast pace. </p><br><p> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/d4d/086/7ce/d4d0867ce32601cac104fb149a18cc8d.png" alt="Playerunknown &amp; # 039;  s Battleground Year Growth"></a> <br>  Source: <a href="https://steamdb.info/app/578080/graphs/">SteamDB</a> </p><br><p>  Despite the fact that the game is a hit, I have seen similar growth charts in regular projects.  During the month, the number of users increased from 2 to 5 times. </p><br><p>  You need a solution that is easy to scale, but you don‚Äôt want to pay for pre-reserved capacity, but add them as the load increases. </p><br><h3>  <strong>SQL based solution</strong> </h3><br><p>  The solution to the forehead is to take some SQL database, send all the data there in raw form.  Out of the box solves the problem of the query language. </p><br><p>  Directly, data from gaming clients cannot be sent to the repository, so a separate service is needed, which will deal with buffering events from clients and sending them to the database. </p><br><p>  In this scheme, analysts should directly send requests to the database, which is fraught.  If the query is heavy, the database can get up.  Therefore, we need a replica database purely for analysts. </p><br><p>  An example of the architecture below. </p><br><p> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/175/401/549/175401549237aa367d1b1750d8613151.png" alt="SQL Server Approach Architecture"></a> </p><br><p>  This architecture has several disadvantages: </p><br><ul><li>  You can forget about real-time data. </li><li>  SQL is a powerful tool, but event data often does not fall on the relational schema, so you have to invent crutches, such as p0-p100500 parameters for events </li><li>  Given the amount of analytical data collected per day, the size of the database will grow by leaps and bounds, you need to partition, etc. </li><li>  The analyst can give birth to a request that will be executed for several hours, or maybe even a day, thereby blocking other users.  Do not give everyone their own replica of the database? </li><li>  If SQL is on-prem, you will need to constantly take care of fault tolerance, sufficient free space, and so on.  If in the cloud - can fly into a pretty penny </li></ul><br><h3>  <strong>Apache Stack Solution</strong> </h3><br><p>  Here the stack is quite large: Hadoop, Spark, Hive, NiFi, Kafka, Storm, etc. </p><br><p> <a href=""><img src="https://dzone.com/storage/temp/3361824-pipeline-2.png" alt="Apache stack architecture"></a> <br>  Source: <a href="https://dzone.com/articles/lambda-architecture-with-apache-spark">dzone.com</a> </p><br><p>  Such architecture will precisely cope with any loads and will be as flexible as possible.  This is a complete solution that will allow you to process data in real time, and build furious queries on cold data. </p><br><p>  But, in fact, the load specified in the requirements is difficult to call BigData, they can be counted on a single node.  Therefore, Hadoop-based solutions are an obvious overkill. </p><br><p>  You can start with Spark Standalone, it will be much easier and cheaper. </p><br><p>  For receiving and preprocessing events, I would prefer Apache NiFi instead of Spark. </p><br><p>  Cluster management can greatly simplify <a href="https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes">Kubernetes on AKS</a> . </p><br><p>  <strong>Pros</strong> : </p><br><ul><li>  The most flexible solution, data handlers can be written as pure code, and use Spark SQL </li><li>  Open Source stack, big community </li><li>  Good scalability, you can always throw everything on Hadoop </li></ul><br><p>  <strong>Cons</strong> : </p><br><ul><li>  Managing and maintaining the infrastructure manually, although it can be simplified with AKS </li><li>  Virtualka billing is not very profitable if they are idle most of the time. </li><li>  Ensuring the resiliency of the solution is not an easy task, which also needs to be addressed by yourself. </li><li>  Since requests are written by code - not everyone is suitable, everyone is used to SQL, the code can not be mastered </li></ul><br><h3>  <strong>Cloud Platform Solution</strong> </h3><br><h4>  Azure Event Hubs </h4><br><p>  Azure Event Hubs is the simplest high-throughput event hub.  From cloud platforms - the most suitable option for receiving large volumes of analytics from clients. </p><br><p>  <strong>Pros</strong> : </p><br><ul><li>  Autoscale </li><li>  Guaranteed reliability and fault tolerance </li><li>  Pretty low price </li></ul><br><p>  <strong>Cons</strong> : </p><br><ul><li>  Scanty event / queue management capabilities </li><li>  No duplicate message tracking mechanisms </li><li>  Auto scaling up only </li></ul><br><h4>  HDInsight </h4><br><p>  HDInsight is a platform that allows you to deploy a ready-to-use Hadoop cluster, with certain products like Spark.  You can fold it away immediately, since it is an obvious search for such volumes of data, and it is very expensive. </p><br><h4>  Azure databricks </h4><br><p>  Azure Databricks is such a Spark on steroids in the cloud.  <a href="https://databricks.com/">Databricks</a> is a product from Spark authors.  A comparison of the two products can be found <a href="https://databricks.com/product/comparing-databricks-to-apache-spark">here</a> .  From the buns that I personally liked: </p><br><ul><li>  Support for multiple versions of Spark </li><li>  Complex solution, can work with stream of events </li><li>  Faster than vanilla Spark, due to various optimizations (data skipping, auto caching) </li><li>  Nice web-interface, honed under the teamwork <br><ul><li>  Interactive notepads with support for SQL, Python, R, Scala </li><li>  Real Time Collaboration </li><li>  Notebook versioning, github integration </li><li>  Publishing notebooks as interactive dashboards </li></ul></li><li>  Native support for cloud storage </li><li>  All buns clouds: <br><ul><li>  Easy scaling </li><li>  Billing for the cluster, but with autoscaling and auto-terminate </li><li>  Job system that creates and kills an on-demand cluster </li></ul></li></ul><br><p>  <strong>Pros</strong> : </p><br><ul><li>  Autoscale </li><li>  No need to maintain infrastructure </li><li>  Convenient buns for analytics in the form of interactive Notebooks </li><li>  Flexible minute billing for the actual computing time </li></ul><br><p>  <strong>Cons</strong> : </p><br><ul><li>  Inconvenient job debug </li><li>  Lack of local emulator for job debugging </li></ul><br><h4>  Azure Data Lake Analytics (ADLA) </h4><br><p>  Azure Data Lake Analytics is a cloud platform from Microsoft, which is in many ways similar to Databricks. </p><br><p>  Billing is about the same, but a little more understandable, in my opinion.  There is no cluster concept at all, there is no concept of node and its size.  There is an abstract Analytics Unit (AU).  We can say that 1 AU = hour of work of one abstract node. </p><br><p>  Since ADLA is a Microsoft product, it is better integrated into the ecosystem.  For example, jobs for ADLA can be written both in the Azure portal and in Visual Studio.  There is a local emulator ADLA, you can debug the job on your machine before running on big data.  Normal debugging of custom C # code is supported, with breakpoint. </p><br><p> With job parallelization, the approach is slightly different than in databricks.  Since there is no cluster concept, when running a job, you can specify the number of allocated AUs.  Thus, you yourself can choose how much the job should parallel. </p><br><p>  Among the cool features - a detailed job'a plan.  Shows how much and what data was processed, how much processing took at each stage.  This is a powerful tool for optimization and debugging. </p><br><p>  The main job language is U-SQL.  As for the custom code - there is no choice, only C #.  But many see this as an advantage. </p><br><p>  <strong>Pros</strong> : </p><br><ul><li>  Scaling at job level </li><li>  Convenient U-SQL </li><li>  Good integration with Microsoft's ecosystem of products, including Azure services </li><li>  Convenient job debugging on a local machine </li><li>  Delatal job plans </li><li>  The ability to work through the portal Azure </li></ul><br><p>  <strong>Cons</strong> : </p><br><ul><li>  Can not handle stream of events, for this you need a separate solution </li><li>  No interactive collaboration tools like Azure Databricks </li><li>  Only one language for custom code </li><li>  Additional tools are needed to set up timer processing. </li></ul><br><h4>  Azure Stream Analytics </h4><br><p>  Cloud platform for streaming events.  Pretty handy thing.  Out of the box has a tool for debugging / testing directly in the portal.  Talking in T-SQL dialect.  It supports different types of windows for aggregation.  Able to work with many data sources as input and output. </p><br><p>  In spite of all the advantages, it is hardly suitable for something complex.  Stay either in performance or in cost. </p><br><p>  The functional due to which it is worth considering is integration with PowerBI, which allows you to set up real-time statistics in a couple of clicks. </p><br><p>  <strong>Pros:</strong> </p><br><ul><li>  Scaling </li><li>  Integration with all cloud services out of the box </li><li>  T-SQL support </li><li>  Convenient debugging of requests </li></ul><br><p>  <strong>Minuses:</strong> </p><br><ul><li>  You cannot scale without shutting down the job </li><li>  There are no tools for breeding job'a between prod / dev scripts </li><li>  High price </li><li>  Poor performance with heavy queries like DISTINCT </li></ul><br><h3>  Hybrid solution </h3><br><p>  No one forbids combining cloud platforms and OSS solutions.  For example, instead of Apache Kafka / NiFi, you can use Azure Event Hubs if there is no additional logic for event transformation. </p><br><p>  For everything else, you can leave Apache Spark, for example. </p><br><h3>  <strong>Specific Numbers</strong> </h3><br><p>  With the possibilities figured out, now about the price.  Below is an example of the calculation that I did for one of the studios. </p><br><p>  I used the <a href="https://azure.microsoft.com/en-us/pricing/calculator/">Azure Pricing Calculator</a> to calculate the cost. </p><br><p>  I calculated prices for working with cold data in the West Europe region. </p><br><p>  For simplicity, I only considered compute power.  I did not take into account the repository, since its size strongly depends on the specific project. </p><br><p>  At this stage, I have included in the table prices for buffer systems only for comparison.  There are prices for minimum clusters / sizes from which to start. </p><br><h4>  Apache Stack Cost on Bare VM </h4><br><table><thead><tr><th>  Decision </th><th>  Price </th></tr></thead><tbody><tr><td>  Spark </td><td>  $ 204 </td></tr><tr><td>  Kafka </td><td>  $ 219 </td></tr><tr><td>  Total </td><td>  $ 433 </td></tr></tbody></table><br><h4>  Cost of platform solution based on ADLA </h4><br><table><thead><tr><th>  Decision </th><th>  Price </th></tr></thead><tbody><tr><td>  Azure Data Lake Analytics </td><td>  $ 108 </td></tr><tr><td>  Azure Event Hubs </td><td>  $ 11 </td></tr><tr><td>  Total </td><td>  $ 119 </td></tr></tbody></table><br><h4>  Cost of a platform solution based on Azure Databricks </h4><br><table><thead><tr><th>  Decision </th><th>  Price </th></tr></thead><tbody><tr><td>  Azure databriks </td><td>  $ 292 </td></tr><tr><td>  Azure Event Hubs </td><td>  $ 11 </td></tr><tr><td>  Total </td><td>  $ 303 </td></tr></tbody></table><br><h4>  More details on calculations </h4><br><h4>  <strong>Kafka on Bare VMs</strong> </h4><br><p>  In order to provide a more or less reliable solution, you need at least 3 nodes: </p><br><p><code>1 x Zookeeper (Standard A1) = $43.8 / month</code> <br> <code>2 x Kafka Nodes (Standard A2) = $175.2 / month</code> <br> <code>Total: $219</code> </p> <br><p>  For the sake of fairness, it is worth noting that such a Kafka configuration will pull a much larger bandwidth than is needed in the requirements.  Therefore, Kafka can be more profitable if you need higher bandwidth. </p><br><h4>  <strong>Spark on Bare VMs</strong> </h4><br><p>  I think the minimum configuration is worth talking about: 4 vCPU, 14GB RAM. <br>  From the cheapest VM I chose Standard D3v2. </p><br><p> <code>1 x Standard D3v2 = $203.67 / month</code> </p> <br><h4>  <strong>Azure databricks</strong> </h4><br><p>  Databricks has two types of clusters: Standard and Serverless (beta). </p><br><p>  Standard cluster in Azure Databricks includes at least 2 nodes: </p><br><ul><li>  Driver - hosts notepads and processes requests related to them, as well as being a Spark Master and supporting SparkContext. </li><li>  Worker - actually, a worker who handles all requests </li></ul><br><p>  Honestly, I do not know what is meant by serverless, but what I noticed about this type is: </p><br><ul><li>  All the same, choose the type of worker node, their number (from and to) </li><li>  Serverless creates nodes right in a subscription, in a separate resource group. </li><li>  Auto terminate feature is missing </li><li>  Supports only R / Python / SQL queries. </li><li>  Includes at least 2 nodes </li></ul><br><p>  Databricks also includes two shooting galleries, Premium has several of its own features, such as access control on notebooks.  But I considered the minimum Standard. </p><br><p>  Considering in the calculator, I came across one interesting point - the Driver node is missing there.  Since the minimum size of <strong>any</strong> cluster, as a result, 2 nodes, the cost in the calculator is not complete.  Therefore, I counted pens. </p><br><p>  Databricks itself is billed for DBU - computational power.  Each type of node has its DBU ratio per hour. </p><br><p>  For a worker, I took the minimum DSv2 (\ $ 0.272 / hour), it corresponds to 0.75 DBU. <br>  For the driver, I took the cheapest F4 instance (\ $ 0.227 / hour), it corresponds to 0.5 DBU. </p><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">DSv2</span></span> = (<span class="hljs-variable"><span class="hljs-variable">$0</span></span>.<span class="hljs-number"><span class="hljs-number">272</span></span> + <span class="hljs-variable"><span class="hljs-variable">$0</span></span>.<span class="hljs-number"><span class="hljs-number">2</span></span> * <span class="hljs-number"><span class="hljs-number">0</span></span>.<span class="hljs-number"><span class="hljs-number">75</span></span> DBU  ) * <span class="hljs-number"><span class="hljs-number">730</span></span>  = <span class="hljs-variable"><span class="hljs-variable">$308</span></span>.<span class="hljs-number"><span class="hljs-number">06</span></span> F4 = (<span class="hljs-variable"><span class="hljs-variable">$0</span></span>.<span class="hljs-number"><span class="hljs-number">227</span></span> + <span class="hljs-variable"><span class="hljs-variable">$0</span></span>.<span class="hljs-number"><span class="hljs-number">2</span></span> * <span class="hljs-number"><span class="hljs-number">0</span></span>.<span class="hljs-number"><span class="hljs-number">75</span></span> DBU  ) * <span class="hljs-number"><span class="hljs-number">730</span></span>  = <span class="hljs-variable"><span class="hljs-variable">$275</span></span>.<span class="hljs-number"><span class="hljs-number">21</span></span> Total: <span class="hljs-variable"><span class="hljs-variable">$583</span></span>.<span class="hljs-number"><span class="hljs-number">27</span></span></code> </pre> <br><p>  This is a calculation based on the work of this small cluster 24/7.  But in fact, thanks to the possibilities of auto-terminate, this figure can be significantly reduced.  The minimum idle-timeout for the cluster is 10 minutes. </p><br><p>  If we take the axiom that 12 hours a day are working with the cluster (full-time, taking into account floating hours), then the cost will already be <code>$583 * 0.5 = $291.5</code> . </p><br><p>  If analysts do not dispose of the cluster 100% of the working time, then the figure may be even less. </p><br><h5>  <strong>Azure Data Lake Analytics</strong> </h5><br><p>  Price in Europe \ $ 2 per <a href="https://blogs.msdn.microsoft.com/azuredatalake/2016/10/12/understanding-adl-analytics-unit/">Analytics Unit</a> per hour. <br>  Analytics Unit - in fact, one node.  $ 2 / hour for a node is a little expensive, but it‚Äôs billed every minute.  Usually Job takes at least a minute. </p><br><p>  If Job is large, then you need more AU to parallelize it. </p><br><p>  Then I realized that it‚Äôs not very good to poke a finger into the sky.  Therefore, previously conducted a small test.  I generated json files of 100 MB each, only 1 GB, put them into the store, launched a simple query in Azure Data Lake Analytics for data aggregation and looked at how long it would take to process 1 GB.  I got 0.09 AU / h. </p><br><p>  Now you can roughly calculate how much data processing will cost.  Suppose that per month we have accumulated 600 GB of data.  We must process all this data at least once. </p><br><p> <code>600  * 0.09AU * $2 = $108</code> </p> <br><p>  These are fairly rough calculations of the minimum configuration for analytics. </p><br><h3>  Brief summary </h3><br><p>  The solution based on SQL database does not have sufficient flexibility and performance. </p><br><p>  The Apache Stack-based solution is very strong and flexible, although expensive for the stated requirements.  Plus requires cluster support handles.  This is Open Source, so vendor lock'a can not be afraid.  Plus, the Apache Stack can cover two tasks at once, processing cold and hot data, which is a plus. </p><br><p>  If you are not afraid of administrative difficulties, then this is the perfect solution. <br>  If you are constantly working with analytics, with large volumes, then having your own cluster can be a more profitable solution. </p><br><p>  There are several solutions among cloud platforms. </p><br><p>  For event buffering - EventHub.  With small volumes it turns out cheaper Kafka. </p><br><p>  For processing cold data - two suitable options: </p><br><ul><li>  Azure Databricks (Preview) is a cool tool with interactive notebooks and built-in Spark.  It can handle both hot and cold data.  Not very expensive, support for many languages, cluster auto-management, and many more goodies. </li><li>  Azure Data Lake Analytics - does not have a cluster, job-level parallelization, good integration with Visual Studio, convenient debugging tools, billing by the minute </li></ul><br><p>  If there are no resources to support the infrastructure, as well as a fairly cheap start is needed, then these options will be very attractive. </p><br><p>  Azure Databricks can be a cheaper option if jobs are running continuously and in large quantities. </p><br><p>  Having offered the designated options to several studios, many became interested in platform solutions.  They can be fairly painlessly integrated into existing processes and systems, without unnecessary administrative effort. </p><br><p>  Below, I review a detailed overview of the architecture based on the Azure Data Lake Analytics platform solution. </p><br><h2>  Game analytics on Azure Event Hub / Azure Data / Azure Data Factory / Azure Data Analytics / Azure Stream Analytics / Power BI </h2><br><h3>  <strong>Architecture</strong> </h3><br><p>  Having estimated the pros and cons, I took up the implementation of lambda architecture on Azure.  It looks like this: </p><br><p> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/1e6/b14/7f1/1e6b147f159687debab8b0bfd69bd4d3.png" alt="Azure Lambda Architecture"></a> </p><br><p>  <strong>Azure Event Hub</strong> is a queue, a buffer that can receive a huge number of messages.  There is also a nice feature of writing raw data to the storage.  In this case, Azure Data Lake Storage (ADLS). </p><br><p>  <strong>Azure Data Lake Store</strong> is a repository based on HDFS.  Used in conjunction with the Azure Data Lake Analytics service. </p><br><p>  <strong>Azure Data Lake Analytics</strong> - analytics service.  Allows you to build U-SQL queries to the data lying in different sources.  The fastest source is ADLS.  In particularly difficult cases, you can write custom code for queries in C #.  There is a handy toolset in Visual Studio, with detailed query profiling. </p><br><p>  <strong>Azure Stream Analytics</strong> is a service for processing data flow.  In this case, it is used to aggregate "hot" data and transfer it for visualization in PowerB </p><br><p>  <strong>Azure Functions</strong> is a service for hosting serverless applications.  This architecture is used for "custom" processing of the event queue. </p><br><p>  <strong>Azure Data Factory</strong> is a rather controversial tool.  Allows you to organize data pipelines.  In this particular architecture, it is used to run batchy.  That is, it runs queries in the ADLA, calculating slices for a certain time. </p><br><p>  <strong>PowerBI</strong> is a business analytics tool.  Used to organize all dashboards on the game.  Able to display realtime data. </p><br><p>  The same solution, but in a different perspective. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/af3/a97/e62/af3a97e62023516443f736b4518386c8.png" alt="Azure Lambda Architecture full perspective"></p><br><p>  Here you can clearly see that in the Event Hubs, clients can both directly and through API Gateway.  Similarly, in the Event Hubs, you can throw and server analytics. </p><br><h3>  <strong>Event queue handling</strong> </h3><br><h4>  Cold data </h4><br><p>  After entering the EventHub, events take two paths: cold and hot.  The cold road leads to the ADLS repository.  There are several options for saving events. </p><br><h5>  EventHub's Capture </h5><br><p>  The easiest way is to use the Capture feature of EventHub.  It allows you to automatically save the raw data entering the hub in one of the storages: Azure Storage or ADLS.  The feature allows you to customize the file naming pattern, although it is very limited. </p><br><p>  Although the feature is useful, it is not suitable in all cases.  For example, it did not suit me, since the time used in the file pattern corresponds to the arrival time of an event in the EventHub. </p><br><p>  In fact, in games, events can be accumulated by customers, and then sent in bundles.  In this case, the events will get into the wrong file. </p><br><blockquote>  The organization of the data in the file structure is very important for the effectiveness of the ADLA.  The overhead of opening / closing a file is quite large, so ADLA will be most effective when working with large files.  Experimentally, I found the optimal size - from 30 to 50 MB.  Depending on the load, it may be necessary to split files by day / hour. </blockquote><p>  Another reason is the lack of the ability to decompose events into folders, depending on the type of the event itself.  When it comes to analytics, queries will have to be as efficient as possible.  The best way to filter unnecessary data is to not read the files at all. </p><br><p>  If events are mixed within the file by type (for example, authorization events and economic events), then some of the computing power of the analytics will be spent on discarding unnecessary data. </p><br><p>  <strong>Pros:</strong> </p><br><ul><li>  Quickly set up </li><li>  It just works without any problems. </li><li>  Cheap </li></ul><br><p>  <strong>Minuses:</strong> </p><br><ul><li>  Supports only AVRO format when saving events to storage </li><li>  It has quite limited file naming capabilities. </li></ul><br><h5>  Stream Analytics (Cold Data) </h5><br><p>  Stream Analytics allows you to write SQL-like queries to the stream of events.  There is support for EventHub'a as a data source and ADLS as output.  Thanks to these requests, already transformed / aggregated data can be added to the storage. </p><br><p>  Similarly, it has scant file naming capabilities. </p><br><p>  <strong>Pros:</strong> </p><br><ul><li>  Quick and easy setup </li><li>  Supports multiple formats for I / O events </li></ul><br><p>  <strong>Minuses:</strong> </p><br><ul><li>  It has quite limited file naming capabilities. </li><li>  High price </li><li>  Lack of dynamic scaling </li></ul><br><h5>  Azure Functions (Cold Data) </h5><br><p>  The most flexible solution.  In Azure Functions there is a binding for the EventHub, and do not bother with the analysis of the queue.  Azure Functions are also automatically colored. </p><br><p>  It was on this decision that I stopped, since I was able to place events in folders corresponding to the time when the event was generated, and not its arrival.  Also, the events themselves could be scattered in folders, according to the type of event. </p><br><p>  There are two options for billing: </p><br><ul><li>  Consumption Plan - trash serverless, pay for the used memory per second.  Under heavy loads can be expensive </li><li>  App Service Plan - in this embodiment, Azure Functions has a server, its type can be selected, up to free, there is the possibility of autoscaling.  This option in my case turned out to be cheaper. </li></ul><br><p>  <strong>Pros:</strong> </p><br><ul><li>  Raw File Naming Flexibility </li><li>  Dynamic scaling </li><li>  There is a built-in integration with EventHub </li><li>  Low cost solution, with correctly selected billing </li></ul><br><p>  <strong>Minuses:</strong> </p><br><ul><li>  It is necessary to write custom code </li></ul><br><h4>  Hot data </h4><br><h5>  Stream Analytics (Hot Data) </h5><br><p>  Again, Stream Analytics is the easiest solution to aggregate hot data.  The pros and cons are about the same as for the cold road.  The main advantage of Stream Analytics is integration with PowerBI.  Hot data can be shipped in "real" time. </p><br><p>  <strong>Pros:</strong> </p><br><ul><li>  Quick and easy setup </li><li>  It has many conclusions, including SQL, Blob Storage, PowerBI </li></ul><br><p>  <strong>Minuses:</strong> </p><br><ul><li>  The submobility T-SQL used in Stream Analytics still has its limitations, for solving some problems you can rest on the limits </li><li>  Price </li><li>  Lack of dynamic scaling </li></ul><br><h5>  Azure Functions (Hot Data) </h5><br><p>  All the same as in cold data.  I will not describe in detail. </p><br><p>  <strong>Pros:</strong> </p><br><ul><li>  Fully custom logic </li><li>  Dynamic scaling </li><li>  Integrated EventHub Integration </li><li>  Low cost solution, with correctly selected billing </li></ul><br><p>  <strong>Minuses:</strong> </p><br><ul><li>  It is necessary to write custom code </li><li>  Since stateless functions, separate state storage is needed </li></ul><br><h2>  We consider the price of a complete solution. </h2><br><p>  So, the calculation for the load is 1000 events per second. </p><br><table><thead><tr><th>  Decision </th><th>  Price </th></tr></thead><tbody><tr><td>  Azure EventHub </td><td>  $ 10.95 </td></tr><tr><td>  Azure Stream Analytics </td><td>  $ 80.30 </td></tr><tr><td>  Azure functions </td><td>  $ 73.00 </td></tr><tr><td>  Azure Data Lake Store </td><td>  $ 26.29 </td></tr><tr><td>  Azure Data Lake Analytics </td><td>  $ 108.00 </td></tr></tbody></table><br><p>  In most cases, Stream Analytics may not be needed, so the total will be <strong>from $ 217 to $ 297.</strong> </p><br><p>  Now, about how I thought.  The cost of Azure Data Lake Analytics I took from the calculations above. </p><br><h3>  <strong>Calculate Azure Event Hub</strong> </h3><br><p>  Azure Event Hub - bills for every million messages, as well as for bandwidth per second. </p><br><p>  The capacity of one throughput unit (TU) is 1000 events / s or 1MB / s, whichever comes first. </p><br><p>  We count for 1000 messages per second, that is, 1 TUs are needed.  Price for TU at the time of writing <code>$0.015</code> for <strong>Basic</strong> shooting.  It is believed that in the month of 730 hours. </p><br><p> <code>1 TU * $0.015 * 730  = $10.95</code> </p> <br><p>  We count the number of messages per month, taking into account the same load during the month (ha! This does not happen): </p><br><p> <code>1000 * 3600  * 730  = 2 628 000 000 </code> </p> <br><p>  We consider the price for the number of incoming events.  For Western Europe, at the time of this writing, the price was <code>$0.028</code> per million events: </p><br><p> <code>2 628 000 000 / 1 000 000 * $0.028 = $73.584</code> </p> <br><p>  Total <code>$10.95 + $73.584 = $84.534</code> . </p><br><p>  Something comes out a lot.  Given that the events are usually quite small - it is not profitable. </p><br><p>  It is necessary for the client to write an algorithm for packing several events into one (most often they do this).  This will not only reduce the number of events, but also <strong>reduce the number of required TUs with a further increase in load</strong> . </p><br><p>  I took the unloading of real events from the existing system and counted the average size - 0.24KB.  The maximum allowable event size in an EventHub is 256KB.  Thus, we can pack approximately 1000 events into one. </p><br><p>  But there is a subtle point: even though the maximum size of the event and 256KB, <strong>they are multiplied in multiples of 64KB</strong> .  That is, the maximum packed message will be counted as 4 events. </p><br><p>  We recalculate taking into account this optimization. </p><br><p> <code>$73.584 / 1000 * 4 = $0.294</code> </p> <br><p>  Now this is much better.  Now let's calculate what bandwidth we need. </p><br><p> <code>1000 events per second / 1000 events in batch * 256KB = 256KB/s</code> </p> <br><p>  This calculation shows another important feature.  Without a batching event, you would need 2.5MB / s, which would require 3TU.  And we thought that only 1TU was needed, because we send 1000 events per second.  But the bandwidth limit would have come sooner. </p><br><p>  In any case, we can keep within 1 TU instead of 3!  And the calculations can not be changed. </p><br><p>  We consider the price for TU. </p><br><p>  Total we get <code>$10.95 + $0.294 = $11.244</code> . </p><br><p>  Compare with the price excluding the package of events: <code>(1 - $11.244 / $84.534) * 100 = 86.7%</code> . <br>  86% more profitable! </p><br><p>  Event packaging must be considered when implementing this architecture. </p><br><h2>  Calculation Azure Data Lake Store </h2><br><p>  So, let's estimate the approximate growth order of the storage size.  We have already calculated that with a load of 1000 events per second, we get 256KB / s. <br> <code>256 * 3600  * 730  = 657 000 M = 641 </code> </p> <br><p>  This is a pretty big number.  Most likely 1000 events per second will be only part of the time of day, but nevertheless, it is worthwhile to calculate the worst variant. </p><br><p> <code>641  * $0.04 = $25.64</code> </p> <br><p>  Another ADLS is billed for every 10,000 file transactions.  Transactions is any action with a file: read, write, delete.  Fortunately, the removal is free =). </p><br><p>  Let's calculate what we are worth only the data record.  We will use the previous calculations, we collect 2,628,000,000 events per month, but we pack 1,000 of them into one event, therefore 2,628,000 events. </p><br><p> <code>2 628 000   / 10000  * $0.05 = $13.14</code> </p> <br><p>  Something is not very expensive, but can be reduced if you record 1000 events in batches.  Packaging should be done at the client application level, and batch records at the event processing level from the EventHub. </p><br><p> <code>$13.14 / 1000 = $0.0134</code> </p> <br><p>  Now this is not bad.  But again, you need to consider the batching when parsing the EventHub queue. </p><br><p>  Total <code>$26.28 + $0.0134 = $26.2934</code> </p><br><h2>  Calculate Azure Functions </h2><br><p>  Using Azure Functions is possible for both cold and hot paths.  Similarly, they can be deployed as one application, or separately. </p><br><p>  I will consider the easiest option when they are spinning as one application. </p><br><p>  So, we have a load of 1000 events per second.  This is not very much, but not a little.  Earlier, I said that Azure Functions can handle events in batches, and this is done more efficiently than processing events separately. </p><br><p>  If you take the size of a batch of 1000 events, then the load becomes <code>1000 / 1000 = 1   </code> .  What a ridiculous figure. </p><br><p>  Therefore, you can deploy everything into one application, and such a load will be pulled by one minimum instance S1.  Its cost is $ 73.  You can, of course, take B1, it is even cheaper, but I would be reinsured, and would stop at S1. </p><br><h2>  Stream Analytics calculation </h2><br><p>  Stream Analytics is only needed for advanced real-time scenarios when you need sliding window mechanics.  This is a rather rare scenario for games, since the main statistics are calculated on the basis of the window per day, and are reset when the next day arrives. </p><br><p>  If you need Stream Analytics, the guidelines recommend starting with a size of 6 Streaming Units (SUs), which is equal to one selected node.  Next, you need to look at the workload, and scale SUs accordingly. </p><br><p>  In my experience, if the queries do not include DISTINCT, or the window is rather small (an hour), one SU is enough. </p><br><p> <code>1 SU * $0.110 * 730 hours = $80.3</code> </p> <br><h2>  Results </h2><br><p>  The existing solutions offered on the market are quite powerful.  But they are still not enough for advanced tasks, they always have either performance limits or restrictions on customization.  And even the average games begin to abut pretty quickly.  This prompts to develop your own solution. </p><br><p>  Having faced the choice of a stack of technologies, I estimated the price.  The Apache stack is capable of handling all tasks and workloads, but they need to be managed manually.  If you can not easily scale it, then it is very expensive, especially if the machines are not 100% loaded 24/7.  Plus, if you are not familiar with the stack, such a solution is not suitable for a cheap and quick start. </p><br><p>  If you don‚Äôt want to invest in infrastructure development and support, you need to look towards cloud platforms.  Game analytics requires mainly periodic calculations.  Once a day, for example.  Therefore, the ability to pay only for what you use - just to the point. </p><br><p>  The cheapest and fastest start will give a solution based on ADLA.  A richer and more flexible solution - Azure Databricks. </p><br><p>  There may also be hybrid options. </p><br><p>  Those studios we worked with preferred cloud solutions as the easiest option to integrate into existing processes and systems. </p><br><p>  When using cloud services, you need to be very careful when building a solution.  It is necessary to study the principles of pricing and take into account the necessary optimization to reduce cost. </p><br><p>  As a result, calculations show that for 1000 requests per second, which is an average, a custom analytics system can be obtained for $ 300 per month.  Which is pretty cheap.  At the same time, there is no need to invest anything in the development of <strong>your</strong> solution.  What is interesting is that the variant with ADLA, unlike other solutions, when idle does not consume any money at all.  Therefore, it is very interesting for dev &amp; test scripts. </p><br><p>  In the following articles I will talk in detail about the technical aspects of implementation. </p><br><p>  In the same place I will tell about unpleasant moments.  For example, Azure Stream Analytics for gaming scenarios did not perform well.  Many queries are tied to the DAU, and its calculation requires the calculation of the unique using DISTINCT.  It killed productivity, and poured out in kopek.  Solved the problem with simple code on Azure Functions + Redis. </p><br><h2>  I like, I want, I want, I want </h2><br><p>  ‚Äî   Microsoft,        .       ,        ,     .          . </p><br><p>     ,    ,   ,      ,    ,      .       .        . </p><br><p>   ,   ,     . </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/351206/">https://habr.com/ru/post/351206/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../351194/index.html">Pitfalls Service Workers</a></li>
<li><a href="../351196/index.html">Do I need to save data from employees</a></li>
<li><a href="../351198/index.html">Organization of information systems production processes. Part 4. Implementation of the information system</a></li>
<li><a href="../351200/index.html">SOC is people. Downloading expo or how to become a level 20 analyst</a></li>
<li><a href="../351204/index.html">CUBA Platform: Roadmap 2018</a></li>
<li><a href="../351208/index.html">What should be the product manager. One opinion from Yandex</a></li>
<li><a href="../351212/index.html">Transient Woodpecker: a short history of the object "Chernobyl-2"</a></li>
<li><a href="../351214/index.html">Ideal requirements, and how to deal with it</a></li>
<li><a href="../351216/index.html">Strong typing for TypeScript Vue.js applications</a></li>
<li><a href="../351218/index.html">Flask Mega-Tutorial, Part XV: Improving Application Structure</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>