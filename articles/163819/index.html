<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Pre-training of the neural network using a limited Boltzmann machine</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hey. As planned in the past post about limited Boltzmann machines, this will consider the use of RBM for the pre-training of an ordinary multilayered ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Pre-training of the neural network using a limited Boltzmann machine</h1><div class="post__text post__text-html js-mediator-article">  Hey.  As planned in the <a href="http://habrahabr.ru/post/159909/">past post</a> about limited Boltzmann machines, this will consider the use of RBM for the pre-training of an ordinary <a href="http://ru.wikipedia.org/wiki/%25D0%259C%25D0%25BD%25D0%25BE%25D0%25B3%25D0%25BE%25D1%2581%25D0%25BB%25D0%25BE%25D0%25B9%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25BF%25D0%25B5%25D1%2580%25D1%2586%25D0%25B5%25D0%25BF%25D1%2582%25D1%2580%25D0%25BE%25D0%25BD_%25D0%25A0%25D1%2583%25D0%25BC%25D0%25B5%25D0%25BB%25D1%258C%25D1%2585%25D0%25B0%25D1%2580%25D1%2582%25D0%25B0">multilayered network of direct distribution</a> .  Such a network is usually trained in <a href="http://habrahabr.ru/post/154369/">the back-propagation error algorithm</a> , which depends on many parameters, and so far there is no exact algorithm for choosing these training parameters, as well as the optimal network architecture.  Many heuristics have been developed to reduce the search space, as well as methods for assessing the quality of selected parameters (for example, <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3D%25D0%25A1%25D0%25BA%25D0%25BE%25D0%25BB%25D1%258C%25D0%25B7%25D1%258F%25D1%2589%25D0%25B8%25D0%25B9_%25D0%25BA%25D0%25BE%25D0%25BD%25D1%2582%25D1%2580%25D0%25BE%25D0%25BB%25D1%258C">cross-validation</a> ).  Moreover, it turns out that the back-propagation algorithm itself is not so good.  Although <a href="http://psych.stanford.edu/~jlm/papers/PDP/Volume%25201/Chap8_PDP86.pdf">Rumelhart, Hinton, and Williams showed</a> convergence of the back-propagation algorithm (here is even more <a href="http%253A%252F%252Fciteseerx.ist.psu.edu%252Fviewdoc%252Fdownload%253Bjsessionid%253D55F98377691054EF92812E081CF56519%253Fdoi%253D10.1.1.53.1048%2526rep%253Drep1%2526type%253Dpdf%26ei%3DGy_cUISxPMOr4ATV_oD4Bw%26usg%3DAFQjCNEBoUglj9BSwrt-DD-sWV5PKAZeRg%26sig2%3DxbKJCkbhzZJ9-W-1O9Crug%26bvm%3Dbv.1355534169,d.bGE">mathematical proof of convergence</a> ), but there is a small nuance: the algorithm converges with infinitesimally small changes in weights (that is, with the learning rate tending to zero).  And even that is not all.  Typically, this algorithm teaches small networks with one or two hidden layers due to the fact that the effect of training <i>does not reach</i> the distant layers.  Next, we will talk more about why it <i>does not reach</i> , and apply the weights initialization technique with the help of a trained RBM, which was developed by <a href="http://www.cs.toronto.edu/~hinton/">Jeffrey Hinton</a> . <br><br><a name="habracut"></a><br><br><h4>  Attenuation and explosive growth of weight changes </h4><br>  To begin, let's take a look at the graphs that look the same. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/storage2/cf1/ed8/c03/cf1ed8c034ae57e58bf5c5af2c2b8b27.png"><br><br>  They depict the dependence of the <i>average weight change in the layer</i> (y-axis) on the epochs of training (the x-axis).  A network of the following structure was studied: six layers, 5 hidden 100 neurons and an output of 26 neurons (the number of classes, or letters in the English alphabet). <br><br><table><tbody><tr><td rowspan="2">  input dimension = 841 </td><td>  layer -5 </td><td>  layer -4 </td><td>  layer -3 </td><td>  layer -2 </td><td>  layer -1 </td><td>  output </td></tr><tr><td>  100 neurons </td><td>  100 neurons </td><td>  100 neurons </td><td>  100 neurons </td><td>  100 neurons </td><td>  26 neurons </td></tr></tbody></table><br><br>  As you can see, the dynamics are basically the same, but the minimum average change in weights in the output layer is of the same order as the maximum - in the third hidden layer, and in the far layers - even less.  In this case, we observe a gradient decay, and the opposite picture is called ‚Äúexplosive growth of the gradient‚Äù (in English literature this is called ‚Äúvanishing and exploding gradients‚Äù), to see this effect, it is enough to initialize the weights with large values.  In order to understand mathematics, I advise you to refer, for example, to these articles: <br><br><ul><li>  <a href="http://jmlr.csail.mit.edu/proceedings/papers/v9/glorot10a/glorot10a.pdf">Understanding the training network of deep feedforward neural networks</a> </li><li>  <a href="http://arxiv.org/pdf/1211.5063v1.pdf">Understanding the exploding gradient problem</a> </li></ul><br><br>  And we will now try to figure out on the fingers the causes of this phenomenon, based on how it was presented <a href="https://class.coursera.org/neuralnets-2012-001/">in the course on neural networks on the cursor</a> .  So, what is the direct passage of a signal through a neural network?  On each layer, a linear combination of the input signal of the layer and the weights of each neuron is calculated, the result is a scalar in each neuron, and then the value of the activation function is calculated.  As a rule, the activation function, in addition to the nonlinearity we need, helps to limit the output values ‚Äã‚Äãof neurons, but we consider only the activation function of a general form.  Thus, the output is a vector, each component of which is within the previously known limits. <br><br><img src="https://habrastorage.org/storage2/f05/1b6/adc/f051b6adc22d893df895ff8d438bc95b.png"><br><br>  Now look at the back pass.  Gradients on the output layer are easily calculated and <i>significant</i> changes in the weights of the neurons of the output layer are obtained.  For any hidden layer, <a href="http://habrahabr.ru/post/154369/">recall the formula for</a> calculating the gradient: <br><br><img src="https://habrastorage.org/storage2/888/dc3/286/888dc32866bff0247bcb89085d4da36d.gif"><br><br>  If we replace the expression dE / dz, where k is involved, delta, <br><br><img src="https://habrastorage.org/storage2/e0d/f05/550/e0df055503d62d643c65cb70ee14e0e4.gif"><br><br>  we will see that the reverse passage is very similar to the straight line, but it is completely linear, with all the ensuing consequences.  A delta is called a local gradient / neuron error.  During the reverse passage of the delta can be interpreted as exits (in the other direction) of the corresponding neurons.  The value of the weight gradient of the neuron of the <i>deeper</i> layer is proportional to the value of the derivative of the activation function at the point obtained in the forward passage.  Recall the geometric <a href="http://ru.wikipedia.org/wiki/%25D0%259F%25D1%2580%25D0%25BE%25D0%25B8%25D0%25B7%25D0%25B2%25D0%25BE%25D0%25B4%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2584%25D1%2583%25D0%25BD%25D0%25BA%25D1%2586%25D0%25B8%25D0%25B8">definition of the derivative</a> at a point: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/72f/d92/483/72fd92483165e704a54ce8854a341cb8.png" alt="image"><br><br>  x0 is calculated and fixed during the forward pass.  These are the red lines in the picture above.  And it turns out that the linear combination of weights and local gradients of neurons is multiplied by this value, which is the tangent of the tangent angle at x0, and the range of tangent values ‚Äã‚Äãis known to be in the range from minus infinity to plus infinity. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/097/2a6/6d7/0972a66d7ba1c8e2957c9c1a36c5d4d7.gif" alt="image"><br><br>  The reverse pass for one neuron of the hidden layer is as follows: <br><br><img src="https://habrastorage.org/storage2/469/f58/355/469f58355fcd9c22c312ff4f19a90976.png"><br><br>  What we have in the end: <br><ul><li>  explosive growth can occur if the weights are too large, or the value of the derivative at the point is too large </li><li>  damping occurs if the value of the weights or the derivative at the point is very small </li></ul><br><br>  As a rule, <i>shallow nets</i> do not suffer from this due to the small number of layers, since the effect does not have time to accumulate during the backward pass (we will consider an example of accumulation in the next section). <br><br>  If sigmoid is used as an activation function <br><br><img src="https://habrastorage.org/storage2/ac0/13a/471/ac013a4718c49986395e54f10767fdc4.gif"><br><br>  with a derivative of the following form: <br><br><img src="https://habrastorage.org/storage2/60d/348/3b3/60d3483b3dc4f0f9a159f19512e55733.gif"><br><br>  then for Œ± = 1 we can easily verify that the maximum value reached by the derivative is 0.25 at the point x = 0.5, and the minimum value is 0 at the points x = 0 and x = 1 (in the interval from 0 to 1, the range of values ‚Äã‚Äãis logistic functions): <br><br><img src="https://habrastorage.org/storage2/8a2/2b4/39d/8a22b439d88f233dfd8650434d398617.gif"><br><br>  Consequently, for networks with a logistic activation function, explosive growth and attenuation largely depend on the value of the weights, although there is a permanent attenuation due to the fact that at best the tangent of the tangent angle is only 0.25, and these values, as we will see in next section, from layer to layer are multiplied together: <br><br><ul><li>  for very small weights, attenuation occurs </li><li>  with sufficiently large scales - growth </li></ul><br><br><h4>  Simple example </h4><br>  To illustrate the above, consider an example. <br><br><img src="https://habrastorage.org/storage2/60c/ab3/2c9/60cab32c9a6c59c73deefc207774b5a0.png"><br><br>  Given a simple network that minimizes the square of the Euclidean distance, with the logistic activation function, we find the gradients of weights with the input image x = 1 and the target t = 0.5. <br><br>  After the direct passage, we obtain the output values ‚Äã‚Äãof each neuron: 0.731058578630005, 0.188143725087062, 0.407022630083087, 0.307029145835665, 0.576159944190332.  Denote them by y1, ..., y5. <br><br><ul><li><img src="https://habrastorage.org/storage2/77b/84b/7ce/77b84b7ceffd56c23ef265efa085f70c.gif"></li><li><img src="https://habrastorage.org/storage2/d94/8e5/8d0/d948e58d0bc97656453d12d7785c44e2.gif"></li><li><img src="https://habrastorage.org/storage2/c48/59d/612/c4859d61218f7b08d7f515d979d9fdc7.gif"></li><li><img src="https://habrastorage.org/storage2/57b/269/ef7/57b269ef7f1be8c71dd319b80c4b089b.gif"></li><li><img src="https://habrastorage.org/storage2/32c/3fe/9ca/32c3fe9caa774118c5d895dc7a6f43be.gif"></li></ul><br><br>  As you can see, the gradient decays.  In the course of the calculation, it is noticeable that the accumulation occurs due to the multiplication of the weights and the values ‚Äã‚Äãof the derivatives of the current layer and all subsequent ones.  Although the initial values ‚Äã‚Äãare far from small, compared to how the weights are initialized in practice, these values ‚Äã‚Äãare not enough to suppress the permanent attenuation from the tangents of the tangent angle. <br><br><h4>  Pretraining </h4><br>  Before talking about the method proposed by <a href="http://www.cs.toronto.edu/~hinton/">Jeffrey Hinton</a> , it is worth noting immediately that today there is no formal mathematical proof that preliminary fine-tuning of the scales guarantees an improvement in the quality of the network.  So, <a href="http://www.cs.toronto.edu/~hinton/absps/dbm.pdf">Hinton suggested</a> using <a href="http://habrahabr.ru/post/159909/">limited Boltzmann machines</a> to pre-tune the scales.  As in the post about RBM, we will now talk only about binary input images, although this model <a href="http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf">extends to input images consisting of real numbers</a> . <br><br>  Suppose we have a network of direct distribution of the following structure: <code>x1 -&gt; x2 -&gt; ... -&gt; xn</code> , where each xi is the number of neurons in the layer, xn is the output layer.  Let x0 be the dimension of the input image, which is fed to the input, i.e.  on layer x1.  We also have a dataset for training, data (we denote D0) are pairs of the ‚Äúinput, expected output‚Äù type, and we want to train the network using <a href="http://habrahabr.ru/post/154369/">the back-propagation error algorithm</a> .  But before that, we can accomplish a fine initialization of the scales, for this: <br><br><ul><li>  We consistently train one RBM for each consecutive pair of layers, including the dummy layer x0, and we end up with exactly n pieces of RBM.  To begin with, the first pair <code>x0 &lt;-&gt; x1</code> is taken, she is trained on a set of input images from D0 (this is training without a teacher).  Then, for each input image from D0, the derivation of the probabilities of the hidden layer and sampling is made, we get a set of binary vectors D1.  Then, using D1, the pair <code>x1 &lt;-&gt; x2</code> trained.  And so on to <code>x{n-1} &lt;-&gt; xn</code> . </li><li>  We disassemble each RBM, and take the second layers with their own weights. </li><li>  From the selected second layers of each RBM we compose the initial network <code>x1 -&gt; x2 -&gt; ... -&gt; xn</code> . </li></ul><br><br><img src="https://habrastorage.org/storage2/73f/fe5/805/73ffe5805dab61dbf81a2698678c4054.png"><br><br>  Such a network was called <a href="http://deeplearning.net/tutorial/DBN.html">Deep Belief Networks</a> . <br><br>  As I said, the effectiveness of this method is not proven, as well as the effectiveness of any fine initialization, but the fact remains that it works.  The explanation for this effect can be given as follows: when learning the very first RBM, we create a model that generates some hidden signs from visible states, that is, we immediately put the weights at some minimum necessary to calculate these signs;  with each subsequent layer, signs of signs are calculated, and weights are always placed in a state sufficient to calculate these hierarchical signs;  when it comes to training with a teacher, in fact, only 2-3 layers of the output will be effectively trained, based on those hyper-signs that were calculated earlier, and those, in turn, will change slightly to suit the task.  Hence, we can assume that it is not necessary to initialize all layers in this way, and the output, and possibly 1-2 hidden ones, can be initialized usually, for example, by selecting numbers from the normal distribution with center 0 and variance 0.01. <br><br><h4>  Initial data </h4><br>  Experiments play an important role in this post, so you should immediately make a reservation about the sets used for training and model testing.  I generated several small and easy to summarize sets so that no more than 30 minutes were spent on learning networks. <br><br><ul><li>  <a href="https://docs.google.com/open%3Fid%3D0B4bl7YMqDnViZlZTNE03WmlRSWs">The training set</a> consists of images of large print letters of the English alphabet of three fonts ( <a href="http://en.wikipedia.org/wiki/Arial">Arial</a> , <a href="http://en.wikipedia.org/wiki/Courier_(typeface)">Courier</a> and <a href="http://en.wikipedia.org/wiki/Times_(typeface)">Times</a> as representatives of different <i><a href="http://www.fonts.ru/help/class/">breeds</a></i> ) of 29 by 29 pixels in size, and their images with some random noises. </li><li>  <a href="https://docs.google.com/open%3Fid%3D0B4bl7YMqDnViMGlFZFd6YXZqeHM">The cross-validation set</a> consists of the same set of fonts, plus <a href="http://en.wikipedia.org/wiki/Tahoma_(typeface)">Tahoma</a> and a bit more noise </li><li>  <a href="https://docs.google.com/open%3Fid%3D0B4bl7YMqDnViR09talQ2UDFDUlU">The test set</a> consists of the same data set as the cross-validation, differs only in the randomness factor in the generation of noise </li></ul><br><br>  In all experiments, learning will stop when the following condition is met (both learning of direct distribution networks and Boltzmann machines).  We introduce the notation: <br><ul><li><img src="https://habrastorage.org/storage2/ecf/9cb/539/ecf9cb5390774d49a738187c476d726e.gif">  - This is a cross-validation error of the current era. </li><li><img src="https://habrastorage.org/storage2/7ca/5f6/70a/7ca5f670a36c23abc3e4b002129b23d4.gif">  - this is a cross-validation error in the past era </li><li><img src="https://habrastorage.org/storage2/7cb/41f/89e/7cb41f89ec5a82d5f69bf18eeb57762a.gif">  - this is the minimum cross-validation error achieved from the beginning of the learning process to the current era </li><li>  <i>k</i> - stop parameter with an increase in the cross-validation error, from the interval (0, 1) </li></ul><br>  So, the learning process will be interrupted if the following condition is met: <br><img src="https://habrastorage.org/storage2/fec/399/0f5/fec3990f5f96f98f902fdabe9548aba9.gif"><br>  those.  if the increase in cross-validation error is greater than a certain percentage of the minimum error, then stop. <br><br>  In direct propagation networks, the output layer will be the <a href="http://habrahabr.ru/post/155235/">softmax layer</a> , and the entire network is trained to minimize <a href="http://en.wikipedia.org/wiki/Cross_entropy">cross-entropy</a> , respectively, the error value is calculated in the same way.  And the <a href="http://habrahabr.ru/post/159909/">limited Boltzmann machines</a> will use the <a href="http://en.wikipedia.org/wiki/Hamming_distance">Hamming distance</a> between the binary input and the binary recovered manner as a measure of error. <br><br><h4>  Experiments </h4><br>  To begin with, let's see what result will give us a network with one hidden layer of 100 neurons, i.e.  its structure will be <code>100 -&gt; 26</code> .  We illustrate each learning process with a schedule with a learning error and cross-validation. <br><br>  Learning options: <br><ul><li>  LearningRate = 0.001 </li><li>  BatchSize = full batch </li><li>  RegularizationFactor = 0 </li><li>  Momentum = 0.9 </li><li>  NeuronLocalGainLimit: Limit = [0.1, 10];  Bonus = 0.05;  Penalty = 0.95 </li><li>  CrossValidationStopFactor is 0.05 </li></ul><br><br><img src="https://habrastorage.org/storage2/d8d/3c5/682/d8d3c56820e8b85f7cab96e2ff10060a.png"><br><br>  The result on the test set, the training took about 35 seconds: <br>  Error: 1.39354424678178 <br>  Wins: 231 of 312, 74.03% <br><br>  If the pre-trained network shows the result below this, it will be very sad -).  We will pre-train a network of 5 layers of the following structure: <code>100 -&gt; 300 -&gt; 100 -&gt; 50 -&gt; 26</code> .  But first, let's see how the learning process and the result look like if we train such a network without pre-training. <br><br>  Learning options: <br><ul><li>  LearningRate = 0.001 </li><li>  BatchSize = full batch </li><li>  RegularizationFactor = 0.1 </li><li>  MaxEpoches = 2000 </li><li>  MinError = 0.1 </li><li>  Momentum = 0 </li><li>  NeuronLocalGainLimit: not setted </li><li>  CrossValidationStopFactor is 0.001 </li></ul><br><br><img src="https://habrastorage.org/storage2/40d/55b/938/40d55b938f3590055926197d3ac8f4cc.png"><br><br>  The results on the test set are insignificant, about 3 hits from 312. Training took 4300 seconds, which is almost an hour and twenty (training was interrupted for the 2000 era, although it could have continued further).  For 2000 epochs, an error value of 3.25710954400557 was reached, while in the previous test already at the second epoch it was 3.2363407570674, and at the last one 0.114535596947578. <br><br>  And now let's train RBMs for each consecutive pair.  For starters, it will be <code>841 &lt;-&gt; 100</code> with the following parameters: <br><br><ul><li>  LearningRate = 0.001 </li><li>  BatchSize = 10 </li><li>  Momentum = 0.9 </li><li>  NeuronLocalGainLimit: Limit = [0.001, 1000];  Bonus = 0.0005;  Penalty = 0.9995 </li><li>  GibbsSamplingChainLength = 35 </li><li>  UseBiases = True </li><li>  CrossValidationStopFactor is 0.5 </li><li>  MinErrorChange = 0.0000001 </li></ul><br><br><img src="https://habrastorage.org/storage2/e45/446/7b3/e454467b330ecc9073dc004de501dca1.png"><br><br>  Training took 1274 seconds.  The first pair is easy to visualize, as was done in the previous post.  The following patterns are obtained: <br><img src="https://habrastorage.org/storage2/3ca/0e8/368/3ca0e83689081586afa874a458da214e.png" alt="image"><br>  Increase a little: <br><img src="https://habrastorage.org/storage2/bd7/4f1/3f0/bd74f13f01417dbd1754ca96819a3e4d.png" alt="image"><br><br>  Then we train the second pair of <code>100 &lt;-&gt; 300</code> , it took 2095 seconds: <br><br><ul><li>  LearningRate = 0.001 </li><li>  BatchSize = 10 </li><li>  Momentum = 0.9 </li><li>  NeuronLocalGainLimit: Limit = [0.001, 1000];  Bonus = 0.0005;  Penalty = 0.9995 </li><li>  GibbsSamplingChainLength = 35 </li><li>  UseBiases = True </li><li>  CrossValidationStopFactor is 0.1 </li></ul><br><br><img src="https://habrastorage.org/storage2/18f/ec9/73c/18fec973c7c387811decf9a4e17c5435.png"><br><br>  We train the third pair <code>300 &lt;-&gt; 100</code> , took 1300 seconds: <br><br><ul><li>  LearningRate = 0.001 </li><li>  BatchSize = 10 </li><li>  Momentum = 0.9 </li><li>  NeuronLocalGainLimit: Limit = [0.001, 1000];  Bonus = 0.0005;  Penalty = 0.9995 </li><li>  GibbsSamplingChainLength = 35 </li><li>  UseBiases = True </li><li>  CrossValidationStopFactor is 0.1 </li></ul><br><br><img src="https://habrastorage.org/storage2/2ed/13a/a13/2ed13aa138ffe9ece2ebb4060bda276f.png"><br><br>  We don‚Äôt touch the weights of the output layer and the first hidden layer and initialize it with random values ‚Äã‚Äãfrom the normal distribution with center 0 and variance 0.01.  Now we put all layers into one direct distribution network and train with the following parameters: <br><br><ul><li>  LearningRate = 0.001 </li><li>  BatchSize = full batch </li><li>  RegularizationFactor = 0 </li><li>  Momentum = 0.9 </li><li>  NeuronLocalGainLimit: Limit = [0.01, 100];  Bonus = 0.005;  Penalty = 0.995 </li><li>  CrossValidationStopFactor is 0.01 </li></ul><br><br><img src="https://habrastorage.org/storage2/8aa/470/6e1/8aa4706e1a9ac698317427e785f795d0.png"><br><br>  The result on the test set, while learning took only 17 epochs or 36 seconds: <br>  Error: 1.40156176438071 <br>  Wins: 267 of 312, 85.57% <br><br>  The increase in quality was almost 11% compared to the network with one hidden layer.  I, frankly, expected more, but here it is already a matter <i>of</i> data <i>complexity</i> .  As experiments have shown and confirmed the read articles, the more complex and large data we have, the greater the increase in quality will be obtained. <br><br><h4>  Conclusion </h4><br>  Pre-training is obviously an effective way -) Perhaps some mathematical genius will prove its effectiveness in theory, and not only in practice, as it is now. <br><br>  Suppose there is a task to recognize captcha.  We have downloaded 100,000 images, only 10,000 of them are manually recognized. We would like to train the neural network, but we feel sorry not to use the remaining 90,000 thousand images.  This is where RBM comes to the rescue.  Using an array of 100,000 images, we train a deep belief network, and then on a set of 10,000 hand-recognized images, we train a deep network.  Profit! <br><br><h4>  Links </h4><br><ul><li>  <a href="http://psych.stanford.edu/~jlm/papers/PDP/Volume%25201/Chap8_PDP86.pdf">Learning Internal Representations by Error Propagation</a> </li><li>  <a href="http://jmlr.csail.mit.edu/proceedings/papers/v9/glorot10a/glorot10a.pdf">Understanding the training network of deep feedforward neural networks</a> </li><li>  <a href="http://arxiv.org/pdf/1211.5063v1.pdf">Understanding the exploding gradient problem</a> </li><li>  <a href="http://www.cs.toronto.edu/~hinton/absps/dbm.pdf">Deep Boltzmann Machines</a> </li><li>  <a href="http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf">A Practical Guide to Training Restricted Boltzmann Machines</a> </li><li>  <a href="http://deeplearning.net/tutorial/DBN.html">Deep Belief Networks</a> </li></ul><br><br>  Thanks to the user <a href="https://habrahabr.ru/users/othbihe/" class="user_link">OTHbIHE</a> for help in editing the article! </div><p>Source: <a href="https://habr.com/ru/post/163819/">https://habr.com/ru/post/163819/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../163807/index.html">Facebook? Twitter? Instagram? We all did it 40 years ago.</a></li>
<li><a href="../163811/index.html">How to become a puppeteer or Puppet for beginners</a></li>
<li><a href="../163813/index.html">Introduction to the development of WinRT applications in HTML / JavaScript. Styling applications</a></li>
<li><a href="../163815/index.html">New IBM programs for students and IT professionals</a></li>
<li><a href="../163817/index.html">Gold and silicon fever - what is common?</a></li>
<li><a href="../163821/index.html">EBay data center and adiabatic wetting</a></li>
<li><a href="../163825/index.html">DevCon 2013: last chance for a 25% discount</a></li>
<li><a href="../163827/index.html">Live Suggest.io search: +7 to relevance algorithms</a></li>
<li><a href="../163829/index.html">Network Rendering on Hybrid Cluster</a></li>
<li><a href="../163831/index.html">Amazing Mars</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>