<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Introduction to parallel computing</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A parallel machine is called, roughly speaking, a set of processors, memory, and some methods of communication between them. This may be a dual-core p...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Introduction to parallel computing</h1><div class="post__text post__text-html js-mediator-article">  A parallel machine is called, roughly speaking, a set of processors, memory, and some methods of communication between them.  This may be a dual-core processor in your (no longer new) laptop, a multiprocessor server, or, for example, a cluster (supercomputer).  You may not know anything about such computers, but you know exactly why they are being built: speed, speed, and again speed.  However, speed is not the only advantage. <br><br>  After completing the not very trivial task of creating such a device, designers and developers still have to think about how to make it work.  After all, the techniques and algorithms used for old single-processor single-threaded machines, as a rule, are not suitable. <br><br>  What is most surprising is that in universities they are not in a hurry to translate the curriculum into parallel computing!  At the same time today you need to try to find a computer with one core.  In my native Carleton University, parallel computing courses are not part of the Bachelor of Computer Science mandatory program, and are only available to those who have completed the basic courses of the first three years.  At the same level are courses on distributed computing, and some can be confusing. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <a name="habracut"></a><br><br>  What is the difference?  Equipment that is usually in one physical place participates in parallel computing, they are closely interconnected and all parameters of their work are known to the programmer.  In distributed computing, there is no close constant connection between nodes, according to the name, they are distributed over a certain territory and the operating parameters of this system are dynamic and not always known. <br><br>  Classical algorithms that are taught the last half century, no longer suitable for parallel systems.  As you might guess, the algorithm for a parallel machine is called a parallel algorithm.  It often depends heavily on the architecture of a particular machine and is not as versatile as its classic ancestor. <br><br>  You may ask: why it was necessary to invent a new structure, then pause over new algorithms, and could not it continue to accelerate conventional computers?  First, there is no way to make one super-fast processor that can be compared with modern parallel super-computers;  and if there is, then it will take a lot of resources.  In addition, many tasks are well solved by parallel machines, primarily due to this structure!  Calculation of complex chaotic systems like weather, simulation of elementary particle interactions in physics, modeling at the nano-level (yes, yes, nanotechnology!), Data mining (about which we <a href="http://hexlet.ru/blog/data_mining/">have a blog</a> on the site), cryptography ... the list goes on and on. <br>  For a programmer, as an end user of a parallel machine, two options are possible: parallelism may or may not be parallel to it.  In the first case, the programmer writes programs with a calculation on the computer architecture, takes into account the parameters of this particular machine.  In the second, the programmer may not know that there is a non-classical computer in front of him, and all his classical methods manage to work due to the forethought of the system itself. <br><br>  Here is an example of a multi-core processor - beloved by many SUN UltraSPARC T1 / T2: <br><br><img src="http://hexlet.ru/uploads/images/00/00/01/2010/09/27/c54592.gif"><br><br>  8 cores, each supports 4 (for T1) or 8 (for T2) hardware threads (thread), which by simple multiplication gives us 32/64 (respectively) threads in the processor.  Theoretically, this means that such a processor can perform 32/64 tasks of a single-threaded processor in the same time.  But, of course, a lot of resources are spent on switching between threads and other ‚Äúinternal kitchen‚Äù. <br><br>  And here, for example, the famous coolest graphic units like nVidia GeForce 9600 GT, which has 64 cores on board. <br><br><img src="http://hexlet.ru/uploads/images/00/00/01/2010/09/27/6cb053.png"><br><br>  Latest GPUs have up to a thousand cores!  We will talk about why graphic units have so many cores a bit later.  And now look better at the example of the parallelism hierarchy in supercomputers: <br><br><img src="http://hexlet.ru/uploads/images/00/00/01/2010/09/27/c4b714.png"><br><br>  It is clear that typing a bunch of powerful computers is not a problem.  The problem is to make them work together, that is, to connect to a fast network.  Often in such clusters use a high-speed switch and all computers simply connect to a fast local area network.  Here, for example, is in our university: <br><br><img src="http://hexlet.ru/uploads/images/00/00/01/2010/09/27/981f1e.png"><br><br>  256 cores: 64 units, each with 4 cores of 2.2 GHz and 8 GB of RAM each.  To connect using the switch Foundry SuperX and Gigabit network.  OS - Linux Rocks.  The switch is often the most expensive element: try to find a fast switch on 64 ports!  This is an example of one of the biggest problems of parallel computers: the speed of information exchange.  Processors have become very fast, and memory and tires are still slow.  Not to mention hard drives - compared to processors, they look like tools of the Stone Age! <br><br><h4>  Classification </h4><br><br>  Let's finally understand the types of parallel machines.  They differ in the type of memory - shared (shared) or distributed (distributed), and the type of management - SIMD and MIMD.  It turns out four types of parallel machines. <br><br><h5>  Common memory </h5><br><br>  A classic computer looks like this: <br><br><img src="http://hexlet.ru/uploads/images/00/00/01/2010/09/27/784283.png"><br><br>  And the most obvious way to expand this circuit in view of several processors is this: <br><br><img src="http://hexlet.ru/uploads/images/00/00/01/2010/09/27/2cb46c.png"><br><br>  Instead of one processor, there are several; there is more memory, respectively, but in general, the same scheme.  It turns out that all processors share common memory and if processor 2 needs some information on which processor 3 is working, then it will receive it from shared memory at the same speed. <br><br>  Here‚Äôs what a Quad Pentium built like this looks like: <br><br><img src="http://hexlet.ru/uploads/images/00/00/01/2010/09/27/dd967b.png"><br><br><h5>  Distributed memory </h5><br><br>  As you probably yourself guessed, in this case, each processor has its own memory (let me remind you that this is not about the internal memory of the processor). <br><br><img src="http://hexlet.ru/uploads/images/00/00/01/2010/09/27/4c773c.png"><br><br>  An example is the cluster described above: it is essentially a bunch of separate computers, each of which has its own memory.  In this case, if the processor (or computer) 2 needs information from computer 3, then it will take more time: you will need to request information and transfer it (in the case of a cluster, over a local network).  The network topology will affect the speed of information exchange, therefore different types of structures have been developed. <br><br>  The simplest option that comes to mind is a simple two-dimensional array (mesh): <br><br><img src="http://hexlet.ru/uploads/images/00/00/01/2010/09/27/9c7ec9.png"><br><br>  Or cube: <br><br><img src="http://hexlet.ru/uploads/images/00/00/01/2010/09/27/b17539.png"><br><br>  IBM Blue Gene, a descendant of the famous IBM Deep Blue, who defeated a man of chess, has a similar structure.  Similar, because in fact, Blue Gene is not a cube, but a torus - the extreme elements are connected: <br><br><img src="http://hexlet.ru/uploads/images/00/00/01/2010/09/27/09deb6.png"><br><br>  By the way, it was called Gene, because it is actively used in genetic research. <br><br>  Another interesting structure that had to come to someone's head is the tree that everyone loved: <br><br><img src="http://hexlet.ru/uploads/images/00/00/01/2010/09/27/b446f3.png"><br><br>  Since the depth (or height) of a binary tree is logn, the transmission of information from the two most distant nodes will cover a distance of 2 * logn, which is very good, but such a structure is still not used often.  First, to divide such a network into two isolated subnets, it is enough to cut one wire (remember the min-cut problem?) In the case of a two-dimensional array, you need to cut the sqrt (n) wires!  Do the cube yourself.  And secondly, too much traffic passes through the root node! <br><br>  In the 80s four-dimensional hypercubes were popular: <br><br><img src="http://hexlet.ru/uploads/images/00/00/01/2010/09/27/223c94.png"><br><br>  These are two three-dimensional cubes with connected vertices.  They also built cubes of even greater dimension, but now they are hardly used, including because there is a huge amount of wires! <br><br>  In general, the design of the network to solve a problem is an interesting topic.  For example, the so-called Omega Network: <br><br><img src="http://hexlet.ru/uploads/images/00/00/01/2010/09/27/38c9cc.png"><br><br>  With memory sorted out, now about the management. <br><br><h4>  SIMD vs.  MIMD </h4><br><br>  SIMD - Singe Instruction stream, Multiple Data stream.  The control node is one, it sends instructions to all other processors.  Each processor has its own set of data to work. <br>  MIMD - Multiple Instruction stream, Multiple Data Stream.  Each processor has its own control unit, each processor can execute different instructions. <br><br>  SIMD-systems are usually used for specific tasks, requiring, as a rule, not so much the flexibility and versatility of a computer as the computational power itself.  Media processing, scientific research (the same simulations and modeling), or, for example, Fourier transforms of giant matrices.  Therefore, in graphic units, such a mad number of cores: these are SIMD systems, and a truly ‚Äúsmart‚Äù processor (as in your computer) is usually the same: it manages a bunch of simple and non-universal cores. <br><br><img src="http://hexlet.ru/uploads/images/00/00/01/2010/09/27/0bdbf4.png"><br><br>  Since the ‚Äúcontrolling‚Äù processor sends the same instructions to all the ‚Äúworking‚Äù processors, programming such systems requires some dexterity.  Here is a simple example: <br><br> <code>if (B == 0) <br> then C = A <br> else C = A/B</code> <br>  The initial state of the processor memory is: <br><br><img src="http://hexlet.ru/uploads/images/00/00/01/2010/09/27/2838f2.png"><br><br>  The first line was executed, the data was considered, now the second line is started: then <br><br><img src="http://hexlet.ru/uploads/images/00/00/01/2010/09/27/20d4af.png"><br><br>  In this case, the second and third processors do nothing, because the variable B is not suitable for them under the condition.  Accordingly, the third line is executed next, and this time the other two processors ‚Äúrest‚Äù: <br><br><img src="http://hexlet.ru/uploads/images/00/00/01/2010/09/27/22a44c.png"><br><br>  Examples of SIMD machines are old ILLiac IV, MPP, DAP, CM-1/2 Connection Machine, modern vector units, specific co-processors, and graphic units like nVidia GPU. <br><br>  MIMD machines have broader functionality, which is why they are used in our user computers.  If you have at least a dual-core processor in a laptop - you are the lucky owner of a shared memory MIMD machine!  MIMD distributed memory is supercomputers like IBM Blue Gene, which we talked about above, or clusters. <br><br>  Here is the result of the whole classification: <br><br><img src="http://hexlet.ru/uploads/images/00/00/01/2010/09/27/cfd79a.png"><br><br>  On this introduction to the topic can be considered complete.  Next time we will talk about how the speeds of parallel machines are calculated, write our first parallel program using recursion, run it in a small cluster and learn how to analyze its speed and resource consumption. </div><p>Source: <a href="https://habr.com/ru/post/126930/">https://habr.com/ru/post/126930/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../126923/index.html">Now on Aport.Ru also search Yandex</a></li>
<li><a href="../126924/index.html">The rebels defaced the registry‚Äôs main page .LY</a></li>
<li><a href="../126925/index.html">Lotus style communication</a></li>
<li><a href="../126926/index.html">About the book ‚ÄúMS Visual C ++ 2010 in the .NET environment. Programmer‚Äôs Library</a></li>
<li><a href="../126929/index.html">Cheap iPhone 4 will be available</a></li>
<li><a href="../126937/index.html">Droider Chart. Release 66, honey</a></li>
<li><a href="../126941/index.html">PHP 5.3.8 released</a></li>
<li><a href="../126942/index.html">Internet Desktop Threat Activity Monitor</a></li>
<li><a href="../126944/index.html">Debian Server for a newbie</a></li>
<li><a href="../126945/index.html">Original texts: Yandex against copy-paste</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>