<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How is the musical search. Lecture in Yandex</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Usually under the music search understand the ability to respond to text requests for music. The search should understand, for example, that ‚ÄúFriday‚Äù ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How is the musical search. Lecture in Yandex</h1><div class="post__text post__text-html js-mediator-article">  Usually under the music search understand the ability to respond to text requests for music.  The search should understand, for example, that ‚ÄúFriday‚Äù is not always the day of the week, or to find a song according to the words ‚Äúyou want sweet oranges‚Äù.  But the tasks of musical search are not limited to this.  It happens that you need to recognize a song that a user sang, or one that plays in a cafe.  And you can also find a common in the compositions to recommend the user music for its taste.  Elena Kornilina and Yevgeny Krofto told how this is done and what difficulties arise for students of the Small ShAD. <br><br><iframe src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://video.yandex.ru/iframe/ya-events/m-70910-15043a8e098-9259b3e03abd1724/&amp;xid=17259,15700002,15700021,15700186,15700190,15700253&amp;usg=ALkJrhjWzy7pLWuJ-t1V5sDYGN9XyqiQVA" width="450" height="147" frameborder="0" scrolling="no" allowfullscreen="1"></iframe><br><a name="habracut"></a><br><div class="slideshow"><iframe src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=http://www.slideshare.net/slideshow/embed_code/33543934&amp;xid=17259,15700002,15700021,15700186,15700190,15700253&amp;usg=ALkJrhgsl7m_NiuHh7Lpllp3kWIOBWU-vw" width="425" height="355" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe></div><br><br><div class="slideshow"><iframe src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=http://www.slideshare.net/slideshow/embed_code/33544096&amp;xid=17259,15700002,15700021,15700186,15700190,15700253&amp;usg=ALkJrhjNOfTw3L-JUizatHslnqtLg4yJow" width="425" height="355" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe></div><br><h4>  How is the text musical search </h4><br>  According to some sources, about thirty million different music tracks have accumulated in the world today.  These data are taken from various databases, where these tracks are listed and cataloged.  Working with a music search, you will interact with the usual objects: artists, albums and tracks.  To begin with, let's look at how to distinguish musical requests from non-musical in the search.  First of all, of course, by keywords in the request.  Markers of musicality can be both words that clearly indicate that they are looking for music, for example, ‚Äúlisten‚Äù or ‚Äúwho sings a song‚Äù, and more blurry: ‚Äúdownload‚Äù, ‚Äúonline‚Äù.  At the same time, in the seemingly quite simple case, when a part of the name of the desired track, artist or album is present in the request, problems may arise related to ambiguity. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      For example, take the names of five domestic performers: <br><ul><li>  Friday </li><li>  Pizza </li><li>  Movie </li><li>  30.02 </li><li>  Aquarium </li></ul><br>  If the user asks such a request, it‚Äôs not so easy to understand at once what exactly he wants to find.  Therefore, the probabilities of one or another user's intention are estimated, and the results of several vertical searches are displayed on the issue page. <br><br>  On the other hand, performers love to distort words in their names, track and album names.  Because of this, many of the linguistic extensions used in the search simply fail.  For example: <br><ul><li>  TI NAKAYA ODNA00 (Dorn) </li><li>  Sk8 (Nerves) </li><li>  Crawled up a dapoddai (pull) </li><li>  N1nt3nd0 </li><li>  Oxxxymiron </li><li>  dom! no </li><li>  P! Nk </li><li>  Sk8ter boi (Avril Lavign) </li></ul><br>  In addition, there are many artists with similar or even identical names.  Suppose a query came in the search [agilera].  At first glance, everything is clear, the user is looking for Christina Aguilera.  But still there is some chance that the user needed a completely different artist - Paco Aguilera. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b77/372/381/b77372381bacc933af743239801f7f28.png" width="640"><br><br>  A very common situation with the joint performance of two or more performers.  For example, the song Can't Remember to Forget You can be attributed directly to two performers: Shakira and Rihanna.  Accordingly, the base should provide for the possibility of adding joint performers. <br><br>  Another feature is regionality.  In different regions there may be performers with the same names.  Therefore, it is necessary to take into account which region the request came from, and to give the most popular artist in this region in the search results.  For example, there are performers with the name Zara in both Russia and Turkey.  At the same time, their audiences practically do not overlap. <br><br>  With the names of the tracks, too, everything is so simple.  Cover versions are very common, when one artist writes his interpretation of another artist's track.  At the same time, the cover version may even be more popular than the original.  For example, in most cases, on request [saw the night] the version of the group Zdob si Zdub will be more relevant than the original track of the group ‚ÄúKino‚Äù.  The situation is similar with remixes. <br><br>  It is very important to be able to find tracks by quoting from the text.  Often the user may not remember or know neither the name of the track, nor the name of the artist, but only some line from the song.  However, it is necessary to take into account that some common phrases can occur at once in a variety of tracks. <br><br>  To solve this problem, several methods are used at once.  First, it takes into account the history of user requests, to which genres and artists he has addictions.  Secondly, the important role played by the popularity of the track and the artist: how often click on a particular link in the search, how long they listen to the track. <br><br><h4>  Translation difficulties </h4><br>  Very often, different versions of spelling of names of performers and composers are fixed in different countries.  Therefore, it is necessary to take into account that the user can search for any of these options, and different spellings can occur in the database.  The name of Peter Ilyich Tchaikovsky has about 140 spellings.  Here are just some of them: <br><ul><li>  Petr Ilyich Chaikovsky </li><li>  Peter Ilych Tchaikovsky </li><li>  Pyotr Ilyich Tchaikovsky </li><li>  Pyotr Il'ic Ciaikovsky </li><li>  PI Tchaikovski </li><li>  Pyotr Il'yich Tcha√Økovsky </li><li>  Piotr I. Tchaikovsky </li><li>  Pyotr ƒ∞lyi√ß √áaykovski </li><li>  Peter Iljitsch Tschaikowski </li><li>  Pjotr ‚Äã‚ÄãIljitsch Tschaikowski </li></ul><br>  By the way, when we talked about the main objects with which we have to work in a musical search, we did not mention the composers.  In most cases, this object really does not play an important role, but not in the case of classical music.  Here the situation is almost the opposite, and it is necessary to rely on the author of the work, and the performer is a minor object, although it is impossible not to take this object into consideration either.  There is also a feature in the names of albums and tracks. <br><br><h4>  Audio analysis </h4><br>  In addition to the metadata of the track and lyrics in a musical search, you can use this data obtained by analyzing the audio signal directly.  This allows you to solve several problems at once: <br><ul><li>  Recognition of music on the fragment recorded on the microphone; </li><li>  Singing recognition; </li><li>  Search for fuzzy duplicates; </li><li>  Search for cover versions and remixes; </li><li>  Selection of a melody from a polyphonic signal; </li><li>  Music classification; </li><li>  Auto-tagging; </li><li>  Search for similar / recommendations. </li></ul><br>  A digital audio signal can be represented as an image of a sound wave: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0b0/07b/0cd/0b007b0cdc65d4814fc1c6ae0309d121.png" width="640"><br><br>  If we will increase the detail of this image, i.e.  to stretch on the time scale, then sooner or later we will see points placed at regular intervals.  These points represent the moments at which the amplitude of oscillations of a sound wave is measured: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/74d/3c6/a34/74d3c6a3444ae8aed391bc1b444070c2.png" width="640"><br><br>  The narrower the intervals between the points, the higher the sampling rate of the signal, and the wider the frequency range that can be encoded in this way. <br><br>  We see that the amplitude of oscillations depends on time and correlates with the volume of the sound.  And the oscillation frequency is directly related to the pitch.  How do we get information about the oscillation frequency: convert the signal from the time domain to the frequency domain?  This is where the <a href="http://ru.wikipedia.org/wiki/%25CF%25F0%25E5%25EE%25E1%25F0%25E0%25E7%25EE%25E2%25E0%25ED%25E8%25E5_%25D4%25F3%25F0%25FC%25E5">Fourier transform</a> comes to the rescue.  It allows decomposing a periodic function into a sum of harmonics with different frequencies.  The coefficients at the terms will give us the frequencies that we wanted to receive.  However, we want to get the spectrum of our signal without losing its temporal component.  To do this, use the <a href="http://ru.wikipedia.org/wiki/%25CF%25F0%25E5%25EE%25E1%25F0%25E0%25E7%25EE%25E2%25E0%25ED%25E8%25E5_%25D4%25F3%25F0%25FC%25E5">window Fourier transform</a> .  In fact, we divide our audio signal into short segments - frames - and instead of a single spectrum we get a set of spectra - separately for each frame.  Putting them together we get something like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/36b/2e6/53f/36b2e653f785ccac833c49698266e2f2.png" width="640"><br><br>  Time is displayed on the horizontal axis, and frequency is shown on the vertical axis.  The color indicates the amplitude, i.e.  how much signal power is at a particular time and in a particular layer of frequencies. <br><br><h4>  Feature classification </h4><br>  Such spectrograms are used most of the methods of analysis used in the musical search.  And before moving on, we need to figure out which signs of the spectrogram may be useful to us.  To classify signs in two ways.  First, by time scale: <br><ul><li>  <b>Frame-level</b> - features related to one column of the matrix. </li><li>  <b>Segment-level</b> - signs that combine several frames. </li><li>  <b>Global-level</b> - features that describe the entire track. </li></ul><br>  Secondly, signs can be classified according to the level of presentation, i.e.  how high-level abstractions and concepts describe these features. <br><ul><li>  <b>Low-level:</b> <br><ul><li>  Zero Crossing Rate allows you to distinguish between music and speech; </li><li>  Short-time energy - reflects the change in energy over time; </li><li>  Spectral Centroid - the center of mass of the spectrum; </li><li>  Spectral Bandwidth - variation relative to the center of mass; </li><li>  Spectral Flatness Measure - characterizes the "smoothness" of the spectrum.  It helps to distinguish a signal similar to noise from signals with a pronounced tone. </li></ul></li><li>  <b>Middle-level:</b> <br><ul><li>  Beat tracker; </li><li>  Pitch Histogram; </li><li>  Rhythm Patterns. </li></ul></li><li>  <b>High-level:</b> <br><ul><li>  Music genres; </li><li>  Mood: cheerful, sad, aggressive, calm; </li><li>  Vocal / Instrumental; </li><li>  The perceived speed of the music (slow, fast, medium); </li><li>  Gender vocalist </li></ul></li></ul><br>  Having watched the <a href="http://tech.yandex.ru/education/m/shad/talks/1809/">lecture</a> to the end, you will learn exactly how these signs help to solve the problems of musical search, as well as how computer vision and machine learning have to do with it. </div><p>Source: <a href="https://habr.com/ru/post/220917/">https://habr.com/ru/post/220917/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../220903/index.html">Eversync - now with iOS and Windows Phone support</a></li>
<li><a href="../220905/index.html">IBM's nanofrezer has created the world's smallest magazine cover</a></li>
<li><a href="../220909/index.html">About seven cylinders</a></li>
<li><a href="../220911/index.html">Consoles games: like falling in love with video games turned into an interesting game dev</a></li>
<li><a href="../220913/index.html">Nimbus Note - now on Android</a></li>
<li><a href="../220919/index.html">Ilon Musk officially confirmed the successful landing of the first stage of the Falcon-9</a></li>
<li><a href="../220921/index.html">Is the string operator + simple?</a></li>
<li><a href="../220927/index.html">R + C + CUDA = ...</a></li>
<li><a href="../220931/index.html">Ultimaker 2</a></li>
<li><a href="../220937/index.html">Samsung will develop Tizen as a platform for smart home and is preparing a new flagship</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>