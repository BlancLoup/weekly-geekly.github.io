<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Nomad: problems and solutions</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I launched the first service in Nomad in September 2016. At the moment, I use as a programmer and do support as an administrator of two Nomad clusters...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Nomad: problems and solutions</h1><div class="post__text post__text-html js-mediator-article"><p>  I launched the first service in <a href="https://www.nomadproject.io/">Nomad</a> in September 2016.  At the moment, I use as a programmer and do support as an administrator of two Nomad clusters - one ‚Äúhome‚Äù for my personal projects (6 micro virtual machines in Hetzner Cloud and ArubaCloud in 5 different European data centers) and a second worker (about 40 private virtual and physical servers in two data centers). </p><br><p>  Since then, quite a lot of experience has been gained with the Nomad environment, in the article I will describe the Nomad problems we have encountered and how to cope with them. </p><br><p><img src="https://habrastorage.org/webt/k5/9m/pp/k59mpp5iyvtxtj2q9nrvthzpelo.jpeg"><br>  <em>Yamal Nomad makes Continous Delivery of your software instance ¬© National Geographic Russia</em> </p><a name="habracut"></a><br><h2 id="1-kolichestvo-servernyh-nod-na-odin-datacentr">  1. Number of server nodes per data center </h2><br><p>  <strong>Solution: one server node is enough for one data center.</strong> </p><br><p>  The <a href="https://www.nomadproject.io/docs/internals/architecture.html">documentation</a> does not explicitly indicate how many server nodes are required in one data center.  It is indicated only that a region needs 3-5 nodes, which is logical for the raft protocol consensus. </p><br><p><img src="https://habrastorage.org/webt/go/yt/gu/goytgumjr0zxxqodicboxfgipze.png"></p><br><p>  At the beginning I planned 2-3 server nodes in each data center to provide redundancy. </p><br><p>  In fact of use it turned out: </p><br><ol><li>  This is simply not necessary, since if a node fails in a data center, the role of a server node for agents in this data center will be performed by other server nodes in this region. </li><li>  It turns out even worse if the problem number 8 is not solved.  When re-election wizard mismatch can occur and Nomad will restart some of the services. </li></ol><br><h2 id="2-resursy-servera-dlya-servernoy-nody">  2. Server resources for server node </h2><br><p>  <strong>Solution: a small virtual machine is enough for a server node.</strong>  <strong>On the same server, it is allowed to run other non-resource service services.</strong> </p><br><p>  Memory consumption by the Nomad daemon depends on the number of running tasks.  CPU consumption is based on the number of tasks and the number of servers / agents in the region (not linear). </p><br><p>  In our case: for 300 running tasks, memory consumption is about 500 MB for the current master node. </p><br><p>  In a working cluster, a virtual machine for a server node: 4 CPU, 6 GB RAM. <br>  Additionally launched: Consul, Etcd, Vault. </p><br><h2 id="3-konsensus-pri-nehvatke-datacentrov">  3. Consensus on the lack of data centers </h2><br><p>  <strong>Solution: we make three virtual datacenters and three server nodes for two physical datacenters.</strong> </p><br><p>  Nomad's work within the region is based on the raft protocol.  To work correctly, you need at least 3 server nodes located in different data centers.  This will enable correct operation with the complete loss of network connectivity with one of the data centers. </p><br><p>  But we have only two data centers.  We go to a compromise: we choose a data center, which we trust more, and make an additional server node in it.  We do this by introducing an additional virtual data center, which will physically be located in the same data center (see subsection 2 of problem 1). </p><br><p>  <strong>Alternative solution: we break data centers into separate regions.</strong> </p><br><p>  As a result, data centers operate independently and consensus is needed only within a single data center.  Inside the data center in this case it is better to make 3 server nodes by implementing three virtual data centers in one physical one. </p><br><p>  This option is less convenient for the distribution of tasks, but gives 100% guarantee of the independence of the work of services in case of network problems between data centers. </p><br><h2 id="4-server-i-agent-na-odnom-servere">  4. "Server" and "agent" on one server </h2><br><p>  <strong>Solution: valid if you have limited number of servers.</strong> </p><br><p>  The Nomad documentation says that doing so is undesirable.  But if you are not able to allocate separate virtual machines for server nodes, you can place the server and agent node on one server. </p><br><p>  Simultaneous launch means starting the Nomad daemon in both client mode and server mode. </p><br><p>  What does it threaten with?  With a heavy load on the CPU of this server, the Nomad server node will become unstable, there may be a loss of consensus and heartbits, and services will be restarted. <br>  To avoid this, we increase the limits from the description of Problem 8. </p><br><h2 id="5-realizaciya-prostranstv-imyon-namespaces">  5. Implementation of namespaces (namespaces) </h2><br><p>  <strong>Solution: perhaps through the organization of a virtual data center.</strong> </p><br><p>  Sometimes you need to run some of the services on separate servers. </p><br><p>  The first solution is simple, but more demanding of resources.  We divide all services into groups according to their purpose: frontend, backend, ... Add meta attributes to servers, prescribe attributes for launching all services. </p><br><p>  The second solution is simple.  We add new servers, prescribe meta attributes for them, prescribe these launch attributes for the necessary services, for all other services we prescribe a launch ban on servers with this attribute. </p><br><p>  The third decision is difficult.  Create a virtual data center: run Consul for the new data center, launch the Nomad server node for this data center, not forgetting the number of server nodes for the region.  Now you can run individual services in this dedicated virtual data center. </p><br><h2 id="6-integraciya-s-vault">  6. Vault integration </h2><br><p>  <strong>Solution: avoid Nomad's cyclic dependencies &lt;-&gt; Vault.</strong> </p><br><p>  A running Vault should have no dependencies on Nomad.  The Vault address prescribed in Nomad should preferably point directly to the Vault, without balancers strings (but valid).  Backup Vault in this case can be done through DNS - Consul DNS or external. </p><br><p>  If the Vault data is written in the Nomad configuration files, then Nomad tries to access the Vault at startup.  If access fails, the Nomad refuses to start. </p><br><p>  I made a mistake with a cyclic dependency a long time ago, with this, for a short time, almost completely destroying the Nomad cluster.  The Vault was run correctly, regardless of Nomad, but Nomad looked at the Vault address through the balancers that were running in the Nomad itself.  Reconfiguration and reboot of the Nomad server nodes caused the reboot of the balancer services, which led to a failure to start the server nodes themselves. </p><br><h2 id="7-zapusk-vazhnyh-statefull-servisov">  7. Launching important statefull services </h2><br><p>  <strong>Solution: valid, but I do not.</strong> </p><br><p>  Can I run PostgreSQL, ClickHouse, Redis Cluster, RabbitMQ, MongoDB via Nomad? </p><br><p>  Imagine that you have a set of important services for which most of the other services are tied.  For example, DB in PostgreSQL / ClickHouse.  Or shared short-term storage in Redis Cluster / MongoDB.  Or data bus in Redis Cluster / RabbitMQ. </p><br><p>  All these services in some form implement a fault-tolerant scheme: Stolon / Patroni for PostgreSQL, its own raft implementation in Redis Cluster, its own cluster implementation in RabbitMQ, MongoDB, ClickHouse. </p><br><p>  Yes, all these services can be easily run through Nomad with binding to specific servers, but why? </p><br><p>  Plus - ease of launch, a single script format, like the rest of the services.  No need to bother with scripts ansible / something else. </p><br><p>  Minus - an additional point of failure, which does not give any advantages.  Personally, I completely dropped the Nomad cluster twice for different reasons: once "home", once working.  This was in the early stages of introducing Nomad and due to inaccuracy. <br>  Also, Nomad begins to behave badly and restart services due to problem # 8.  But even if that problem is solved, the danger remains. </p><br><h2 id="8-stabilizaciya-raboty-i-restartov-servisov-v-nestabilnoy-seti">  8. Stabilization of work and restarts of services in an unstable network. </h2><br><p>  <strong>Solution: use of the heartbit tuning options.</strong> </p><br><p>  By default, Nomad is configured so that any short-term network problem or CPU load causes loss of consensus and re-election of the master or marking the agent node unavailable.  And this leads to spontaneous reloads of services and transfer them to other nodes. </p><br><p>  Statistics of the "home" cluster until the problem is corrected: the maximum lifetime of the container before the restart is about 10 days.  It is still aggravated by launching the agent and server on one server and placing it in 5 different data centers in Europe, which implies a greater load on the CPU and a less stable network. </p><br><p>  Statistics of the working cluster before the problem is fixed: the maximum lifetime of the container before the restart is more than 2 months.  Everything is relatively good here because of the separate servers for Nomad server nodes and the excellent network between data centers. </p><br><p>  Default values </p><br><pre><code class="plaintext hljs">heartbeat_grace = "10s" min_heartbeat_ttl = "10s" max_heartbeats_per_second = 50.0</code> </pre> <br><p>  Judging by the code: in this configuration, the Hits are made every 10 seconds.  If two heartbits are lost, the re-election of the master or the transfer of services from the agent node begin.  The controversial settings, in my opinion.  Edit them depending on the application. </p><br><p>  If you have all the services running in several instances and are separated by data centers, then most likely you don‚Äôt have a long time to determine if the server is unavailable (approximately 5 minutes, in the example below), we do less often the interval of hits and the longer the time to determine inaccessibility.  This is an example of setting up my "home" cluster: </p><br><pre> <code class="plaintext hljs"> heartbeat_grace = "300s" min_heartbeat_ttl = "30s" max_heartbeats_per_second = 10.0</code> </pre> <br><p>  If you have good network connectivity, separate servers for server nodes, and the determination period for server unavailability is important (there is some service running in one instance and it is important to transfer it quickly), then we increase the period for determining unavailability (heartbeat_grace).  Optionally, you can make more frequent hartbits (by reducing min_heartbeat_ttl) - this will slightly increase the load on the CPU.  An example of a working cluster configuration: </p><br><pre> <code class="plaintext hljs"> heartbeat_grace = "60s" min_heartbeat_ttl = "10s" max_heartbeats_per_second = 50.0</code> </pre> <br><p>  These settings completely eliminate the problem. </p><br><h2 id="9-zapusk-periodicheskih-zadach">  9. Run periodic tasks </h2><br><p>  <strong>Solution: Nomad periodic services can be used, but "cron" is more convenient for support.</strong> </p><br><p>  Nomad has the ability to periodically launch a service. </p><br><p>  The only plus is the simplicity of this configuration. </p><br><p>  The first minus is that if the service starts frequently, it will clutter up the task list.  For example, when running every 5 minutes - 12 extra tasks will be added to the list every hour, until the GC Nomad triggers, which deletes old tasks. </p><br><p>  The second minus is not clear how to properly set up monitoring of such a service.  How to understand that the service starts, works and performs its work to the end? </p><br><p>  As a result, for myself, I came to the "cron" implementation of periodic tasks: </p><br><ol><li>  This may be a regular cron in a constantly running container.  Cron periodically runs a script.  Script-healthcheck is easily added to such a container, which checks for any flag that the script runs. </li><li>  It can be a constantly running container, with a constantly running service.  A periodic launch has already been implemented inside the service.  On such a service, it is easy to add either a similar script-healthcheck or http-healthcheck, which checks the status immediately by its "insides". </li></ol><br><p>  At the moment, most of the time I write on Go, respectively, I prefer the second option with http healthcheck - Go and periodic launch, and http healthcheck are added with a few lines of code. </p><br><h2 id="10-obespechenie-rezervirovaniya-servisov">  10. Providing service redundancy. </h2><br><p>  <strong>Solution: no simple solution.</strong>  <strong>There are two options more difficult.</strong> </p><br><p>  The provisioning scheme provided by the Nomad developers is to support the number of services running.  You say to the nomad ‚Äúlaunch me 5 service instances‚Äù and he launches them somewhere.  There is no control over the distribution.  Instances can run on the same server. </p><br><p>  If the server is down, the instances are transferred to other servers.  While instances are being transferred, the service does not work.  This is a bad backup option. </p><br><p>  We do it right: </p><br><ol><li>  We distribute instansa across servers through <a href="https://www.nomadproject.io/docs/job-specification/constraint.html">distinct_hosts</a> . </li><li>  We distribute instansa on data centers.  Unfortunately, only through the creation of a copy of the script of the type service1, service2 with the same content, different names and an indication of the launch in different data centers. </li></ol><br><p>  In Nomad 0.9, a functional will appear that will eliminate this problem: it will be possible to distribute services as a percentage between servers and data centers. </p><br><h2 id="11-web-ui-nomad">  11. Web UI Nomad </h2><br><p>  <strong>Solution: the built-in UI is terrible, hashi-ui is beautiful.</strong> </p><br><p>  The console client performs most of the required functionality, but sometimes you want to see the charts, press the buttons ... </p><br><p>  UI is built into Nomad.  It is not very convenient (even worse than the console). </p><br><p><img src="https://habrastorage.org/webt/0s/fv/6y/0sfv6yrspbj5easwnweyx8yzsme.png"></p><br><p>  The only alternative I know is <a href="https://github.com/jippi/hashi-ui">hashi-ui</a> . </p><br><p><img src="https://habrastorage.org/webt/vd/4x/rv/vd4xrvrrnewmotnnio-yxbvriis.png"></p><br><p>  In fact, now I personally need a console client only for "nomad run".  And even this is in plans to transfer to CI. </p><br><h2 id="12-podderzhka-oversubscription-po-pamyati">  12. Memory oversubscription support </h2><br><p>  <strong>Solution: no.</strong> </p><br><p>  In the current version of Nomad, be sure to specify a strict memory limit for the service.  If you exceed the limit - the service will be killed OOM Killer. </p><br><p>  Oversubscription is when service limits can be specified "from and to".  Some services require more memory at startup than during normal operation.  Some services may temporarily consume more memory than usual. </p><br><p>  The choice is strictly limited or soft - a topic for discussion, but, for example, Kubernetes gives the programmer a choice.  Unfortunately, in current versions of Nomad there is no such possibility.  I admit that will appear in future versions. </p><br><h2 id="13-ochistka-servera-ot-servisov-nomad">  13. Cleaning the server from Nomad services. </h2><br><p>  <strong>Decision:</strong> </p><br><pre> <code class="plaintext hljs">sudo systemctl stop nomad mount | fgrep alloc | awk '{print $3}' | xargs -I QQ sudo umount QQ sudo rm -rf /var/lib/nomad sudo docker ps | grep -v '(-1|-2|...)' | fgrep -v IMAGE | awk '{print $1}' | xargs -I QQ sudo docker stop QQ sudo systemctl start nomad</code> </pre> <br><p>  Sometimes "something goes wrong."  On the server, kills the agent node and it refuses to start.  Or the agent node stops responding.  Or the agent node "loses" services on this server. <br>  This sometimes happened with the old versions of Nomad, now it either does not happen, or very rarely. </p><br><p>  What then is the easiest way to do, given that the drain of the server will not give the desired result?  Manually clear the server: </p><br><ol><li>  Stop the nomad agent. </li><li>  We do umount on the mount it creates. </li><li>  Delete all agent data. </li><li>  Remove all containers by filtering service containers (if any). </li><li>  We start the agent. </li></ol><br><h2 id="14-kak-luchshe-razvorachivat-nomad">  14. What is the best way to deploy Nomad? </h2><br><p>  <strong>Solution: of course, through Consul.</strong> </p><br><p>  Consul in this case is not an extra layer, but an organically integrated service that provides more advantages than minuses: DNS, KV storage, search for services, monitoring accessibility of the service, the ability to safely exchange information. </p><br><p>  In addition, it unfolds as easily as Nomad itself. </p><br><h2 id="15-chto-luchshe---nomad-ili-kubernetes">  15. Which is better - Nomad or Kubernetes? </h2><br><p>  <strong>Solution: depends on ...</strong> </p><br><p>  Previously, I sometimes had the idea to start migration to Kubernetes - I was so annoyed by the periodic spontaneous restart of services (see problem number 8).  But after a complete solution of the problem, I can say: Nomad suits me at the moment. </p><br><p>  On the other hand: in Kubernetes, there is also a semi-spontaneous restart of services ‚Äî when the scheduler Kubernetes reallocates instances depending on the load.  This is not very cool, but most likely it is configured there. </p><br><p>  Nomad advantages: very easy to deploy infrastructure, simple scripts, good documentation, built-in support for Consul / Vault, which in turn gives: a simple solution to the problem of storing passwords, built-in DNS, easy to set up healschacks. </p><br><p>  Pros Kubernetes: now it is a "standard de facto."  Good documentation, many ready-made solutions, with a good description and standardization of the launch. </p><br><p>  Unfortunately, I do not have the same great expertise in Kubernetes to unequivocally answer the question of what to use for the new cluster.  Depends on the planned needs. <br>  If you have a lot of namespaces planned (problem number 5) or your specific services consume a lot of memory at the start, then freeing it up (problem number 12) is definitely Kubernetes, since  These two problems in Nomad are not completely resolved or uncomfortable. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/435132/">https://habr.com/ru/post/435132/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../435120/index.html">Lambda functions in SQL ... let me think</a></li>
<li><a href="../435122/index.html">How the flames were implemented in Doom on the Playstation</a></li>
<li><a href="../435124/index.html">Masterpieces of world column design: studio transformer with a variable number of lanes</a></li>
<li><a href="../435126/index.html">Experience in organizing and conducting corporate conferences for analysts</a></li>
<li><a href="../435128/index.html">Pi-Sonos: a hobby out of control</a></li>
<li><a href="../435134/index.html">Simplify working with databases in Qt using QSqlRelationalTableModel</a></li>
<li><a href="../435136/index.html">Sergey and the scientific method</a></li>
<li><a href="../435138/index.html">How to take control of network infrastructure. Part Three Network security</a></li>
<li><a href="../435142/index.html">Examining Tracing with eBPF: Guide and Examples</a></li>
<li><a href="../435144/index.html">Introduction to Spring Boot: Creating a Simple Java REST API</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>