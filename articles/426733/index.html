<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Iron will not fail. How I prepare dozens of servers a day for a fight</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Check one server - no problem. You take the check-list and check it in order: processor, memory, disks. But with a hundred servers, this method is unl...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Iron will not fail. How I prepare dozens of servers a day for a fight</h1><div class="post__text post__text-html js-mediator-article">  Check one server - no problem.  You take the check-list and check it in order: processor, memory, disks.  But with a hundred servers, this method is unlikely to work well.  To eliminate the human factor, to make checks more reliable and faster, it is necessary to automate the process.  Who needs to know how to do this better than a hosting provider?  Artyom Artemyev on HighLoad ++ Siberia told us which methods can be used, what is better to run with hands, and what is great at automating.  Further, the text version of the report with tips that anyone who works with iron and needs to regularly check its performance will be able to repeat. <br><br><img src="https://habrastorage.org/webt/eg/iq/xc/egiqxczvizqa8elpiks7967fg3y.png"><br><br>  <strong>About the speaker:</strong> Artyom Artemyev ( <a href="https://habr.com/users/artemirk/" class="user_link">artemirk</a> ), technical director in a large hosting provider FirstVDS, he himself works with iron. <br><a name="habracut"></a><br>  FirstVDS has two data centers.  The first one is our own, we built our own building, brought and installed our own racks, serve ourselves, worry about the current and cooling of the data center.  The second data center is a large room in a large data center that has been rented, everything is easier with it, but it also exists.  In total, it is 60 racks and about 3000 iron servers.  There was something to practice and test different approaches, which means that we are waiting for almost confirmed recommendations.  Let's start to view or read the report. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/eQjNQ2RnjUY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  About 6-7 years ago, we realized that simply putting an operating system on a server is not enough.  The OS is worth it, the server is alert and ready for battle.  We start it on production - strange reloads and hangs begin.  It is not clear what to do - the process is underway; it is hard, expensive, painful to transfer the entire working draft to a new piece of iron.  Where to run? <br><br>  Modern deployment methods allow you to avoid this and transport the server in 5 seconds, but our clients (especially 6 years ago) did not fly in the clouds, walked on the ground and used ordinary pieces of iron. <br><br><img src="https://habrastorage.org/webt/li/kc/fa/likcfazjalyfvnjz_vq17bqqjpm.png"><br><br>  In this article I will tell you which methods we tried, which ones we got accustomed to, which ones weren't taken root, which ones we run well with our hands, and how to automate all this.  I will give you advice and you can repeat it in your company if you work with iron and you have such a need. <br><br><h2>  What is the problem? <br></h2><br>  In theory, checking the server is not a problem.  Initially, we had a process, as in the picture below.  A man sits down, takes a check-list, checks: processor, memory, disks, wrinkles forehead, makes a decision. <br><br><img src="https://habrastorage.org/webt/bq/75/ac/bq75acnj5b9-wk-mhonnk06vtbc.png"><br><br>  Then 3 servers were installed per month.  But, when servers become more and more, this person starts crying and complaining that he is dying at work.  A person is increasingly mistaken because verification has become a routine. <br><br>  <strong>We made a decision: we automate!</strong>  And the person will do more useful things. <br><br><h3>  Small excursion <br></h3><br><img src="https://habrastorage.org/webt/yp/xx/ar/ypxxarfkx9huiayenmyjsiauh2g.png"><br><br>  I will clarify what I mean when I talk about the server today.  We, like everyone, save space in racks and use high-density servers.  Today it is 2 units, which can fit either 12 nodes of single-processor servers, or 4 nodes of dual-processor servers.  That is, each server gets 4 disks - everything is fair.  Plus, there are two power supplies in the rack, that is, everything is reserved and everyone likes it. <br><br><h2>  Where does iron come from? <br></h2><br>  Iron to our data center is brought by our suppliers - as a rule, these are Supermicro and Intel.  In the data center, our guys-operators, install servers in an empty space in the rack and connect two wiring, network and power.  It is also the responsibility of operators to configure the BIOS in the server.  That is, connect the keyboard, monitor and configure two parameters: <code>Restore on AC/Power Loss ‚Äî [Power On]</code> , so that the server is always turned on as soon as power appears.  It should work non-stop.  The second <code>First boot device ‚Äî [PXE]</code> , that is, we put the first boot device on the network, otherwise we will not be able to reach the server, since it‚Äôs not a fact that it has disks at once, etc. <br><br><img src="https://habrastorage.org/webt/ab/hf/i2/abhfi2f6a412ead4dsnxyjjmhe8.png"><br><br>  After that, the operator opens the accounting panel of iron servers, in which you need to fix the fact of server installation, for which it is indicated: <br><br><ul><li>  rack; </li><li>  sticker; </li><li>  network ports; </li><li>  power ports; </li><li>  unit number </li></ul><br>  After that, the network port where the operator has installed the new server, for the purpose of security, goes to a special quarantine VLAN, on which DHCP, Pxe, TFtp also hangs.  Next on the server, our favorite Linux is loaded, which has all the necessary utilities, and the diagnostic process starts. <br><br>  Since the server still has the first boot device over the network, the servers that go to production, the port switches to another VLAN.  There is no DHCP in another VLAN, and we are not afraid that we will unintentionally reinstall our production server.  For this we have a separate VLAN. <br><br>  It so happens that the server was installed, everything is fine, but it never loaded into the diagnostic system.  This happens, as a rule, due to the fact that during a VLAN switching delay, not all network switches quickly switch VLANs, etc. <br><br><img src="https://habrastorage.org/webt/q0/qa/wl/q0qawlis6oq5tykqjri3hif8bwg.png"><br><br>  Then the operator receives the task to restart the server by hand.  Previously, there was no IPMI, we put remote outlets and fixed which port the server was in, pulled the outlet over the network, and the server rebooted. <br><br>  But managed outlets also do not always work well, so now we manage server power over IPMI.  But when the server is new, IPMI is not configured, it can be rebooted only by going up and pressing a button.  Therefore, a person is sitting, waiting - the lamp is lit - runs and presses the button.  Such is his job. <br><br>  If after that the server does not boot, then it is entered into a special list for repair.  This list includes servers on which diagnostics did not start, or its results were not satisfactory.  An individual person - who loves iron - sits every day and makes out ‚Äî assembles, looks, why he does not work. <br><br><h2>  CPU <br></h2><br>  Everything is fine, the server has started, we are starting to test.  We first test the processor as one of the most important elements. <br><br><img src="https://habrastorage.org/webt/8k/6m/n0/8k6mn0cwmprfgrqoanohbc1lyyo.png"><br><br>  The first impulse was to use the application from the vendor.  We have almost all Intel processors - went to the site, downloaded the Intel Processor Diagnostic Tool - everything is fine, it shows a lot of interesting information, including the operating time of the server in hours and the power consumption schedule. <br><br>  But the problem is that Intel PTD works under Windows, which we no longer liked.  To run a check in it, you just need to move the mouse, press the ‚ÄúSTART‚Äù button, and the check will begin.  The result is displayed on the screen, but it is not possible to export it anywhere.  This does not suit us, because the process is not automated. <br><br><img src="https://habrastorage.org/webt/ax/zo/t1/axzot1gbcsfefqbkta32bixrxk4.png"><br><br>  Let's go read the forums and find the two easiest ways. <br><br><ol><li>  <strong>The perpetual loop cat / dev / zero&gt; / dev / null</strong> .  You can check in top - 100% one core is consumed.  We count the number of cores, run the required number of cat / dev / zero, multiplied by the required number of cores.  Everything works great! <br></li><li>  <strong>Utility / bin / stress</strong> .  She builds the matrix in memory and begins to constantly turn them over.  Everything is also good - the processor heats up, there is a load. <br></li></ol><br>  We give servers in production, users come back and say that the processor is unstable.  Checked - the processor is unstable.  They began to investigate, took the server, which passes the checks, but falls in battle, enabled debug in the Linux kernel, collected Core dump.  Before rebooting, the server resets everything in memory before a crash. <br><br><img src="https://habrastorage.org/webt/i1/iu/z0/i1iuz0zoafq9lm8aaimgyqzyiic.png"><br><br>  Processors incorporate various optimizations for frequent operations.  We can see the flags reflecting what optimizations the processor supports, for example, optimizing floating-point operations, optimizing multimedia, etc.  But our / bin / stress, and the perpetual loop just burn through the processor in one operation and do not use additional features.  An investigation revealed that the CPU crashes when trying to use the functionality of one of the built-in flags. <br><br>  The first impulse was to leave / bin / stress - let the processor warm.  Then in the cycle we will run through all the flags, jerk them.  While thinking how to implement this, what commands to call to call the functions of each flag, read the forums. <br><br>  At the overclockers' forum, we came across an interesting project for finding primes <a href="https://www.mersenne.org/">Great Internet Mersenne Prime Search</a> .  Scientists have made a distributed network to which everyone can connect and help find a prime number.  Scientists do not believe anyone, so the program works very cunningly: first you start it, it calculates the primes that it already knows, and compares the result with what it knows.  If the result does not match, then the processor is working incorrectly.  We really liked this property: for any nonsense, it is prone to falling. <br><br>  In addition, the goal of the project is to find as many primes as possible, so the program is constantly optimized for the properties of new processors, as a result, it pulls a lot of flags. <br><br>  Mprime has no time limit, if not stopped, it works forever.  We run it for 30 minutes. <br><br><pre> <code class="hljs perl">/usr/bin/timeout <span class="hljs-number"><span class="hljs-number">30</span></span><span class="hljs-keyword"><span class="hljs-keyword">m</span></span> /opt/mprime -t /bin/<span class="hljs-keyword"><span class="hljs-keyword">grep</span></span> -i error /root/result.txt</code> </pre><br>  After the work is completed, we check that there are no errors in result.txt, and we look at the kernel logs, in particular, we look for errors in the / proc / kmsg file. <br><br><h3>  Another tour <br></h3><br><img src="https://habrastorage.org/webt/es/ic/kj/esickjc00tooyxznjt_6lyumimg.png"><br><br>  On January 3, 2018, the 50th Mersenne prime number was found (2 <sup>p</sup> -1).  This number is only 23 million digits.  It can be downloaded to view - <a href="">this</a> is <a href="">a 12 MB zip archive</a> . <br><br>  Why do we need prime numbers?  First, any RSA encryption uses prime numbers.  The more primes we know, the more secure your SSH key is.  Secondly, scientists test their hypotheses and mathematical theorems, and we are not against helping scientists - it costs us nothing.  It turns out a win-win story. <br><br><img src="https://habrastorage.org/webt/v0/nd/cl/v0ndcljwlkwops-dtp9nwk9bsru.png"><br><br>  So, the processor is working, everything is fine.  It remains to find out what kind of processor.  We use the dmidecode -t processor and see all the slots that are in the motherboard, and what processors are in these slots.  This information enters our accounting system, we will interpret it later. <br><br><h3>  Catch <br></h3><br>  Thus, surprisingly, you can find broken legs.  / bin / stress and the perpetual cycle worked, and Mprime fell.  They drove for a long time, searched, discovered - the result on the picture below - everything is clear. <br><br><img src="https://habrastorage.org/webt/13/ae/gc/13aegcfaw9ozlrzzzgog3povm5w.jpeg"><br><br>  Such a processor simply did not start.  The operator was very strong, took the wrong processor - but could deliver. <br><br><img src="https://habrastorage.org/webt/gj/xr/_w/gjxr_wu8lxf_cgsf4ehxs84-yb8.jpeg"><br><br>  Another wonderful case.  The black row in the photo below is the fans, the arrow shows how the air is blowing.  See: the radiator stands across the stream.  Of course, everything is overheated and turned off. <br><br><img src="https://habrastorage.org/webt/oh/-c/hp/oh-chp3arnnkslirn8elylfcfww.jpeg"><br><br><h2>  Memory <br></h2><br>  Memory is pretty simple.  These are cells in which we write information, and after a while we read it again.  If the same thing we recorded remains there, then this cell is intact. <br><br><img src="https://habrastorage.org/webt/91/gl/_s/91gl_slwgtws8webfnnjxfajui8.png"><br><br>  Everyone knows a good, straightforwardly classic, <a href="https://www.memtest86.com/">Memtest86 +</a> program, which runs from any medium, over a network or even from a floppy disk.  It is designed to check as many memory cells as possible.  Any busy cells can no longer be checked.  Therefore, memtest86 + has a minimum size so as not to occupy memory.  Unfortunately, <strong>memtest86 + displays its statistics only on the screen</strong> .  We tried to expand it somehow, but it all came up against the fact that there is not even a network stack inside the program.  To expand it, you would need to bring along a Linux kernel and everything else. <br><br>  There is a paid version of this program, which already knows how to dump information to disk.  But on our servers there is not always a disk, and there is not always a file system on these disks.  A network drive, as we have already found out, cannot be connected. <br><br>  We began to dig further and found a similar program <a href="http://pyropus.ca/software/memtester/">Memtester</a> .  This program works from the OS level of Linux.  Its biggest disadvantage is that the OS itself and Memtester occupy some memory cells, and these cells will not be checked. <br><br>  Memtester is started with the command: memtester `cat / proc / meminfo | grep MemFree |  awk '{print $ 2-1024}' `k 5 <br><br>  Here we transfer the amount of free memory minus 1 MB.  This is done because otherwise Memtester occupies the entire memory, and the down killer kills it.  We drive this test for 5 cycles, at the output we have a tablet with either OK or fail. <br><br><table width="300"><tbody><tr><td>  Stuck address </td><td>  ok </td></tr><tr><td>  Random value </td><td>  ok </td></tr><tr><td>  Compare XOR </td><td>  ok </td></tr><tr><td>  Compare SUB </td><td>  ok </td></tr><tr><td>  Compare MUL </td><td>  ok </td></tr><tr><td>  Compare DIV </td><td>  ok </td></tr><tr><td>  Compare OR </td><td>  ok </td></tr><tr><td>  Compare AND </td><td>  ok </td></tr></tbody></table><br><br>  The final result is saved and further analyzed for failures. <br><br><img src="https://habrastorage.org/webt/ah/55/kt/ah55kt9fxuqwkmqsz8-dozgunic.png"><br><br>  To understand the scale of the problem - our smallest server has 32 GB of memory, our Linux image with Memtester occupies 60 MB, <strong>we do not check 2% of memory</strong> .  But according to statistics for the last 6 years, there was no such thing that a memory got frankly in production.  This is a compromise to which we agree, and which is dear to us to correct - and we live with it. <br><br>  Along the way, we also collect dmidecode -t memory, which gives all the memory banks that we have on the motherboard (usually up to 24 pieces), and which dies are in each bank.  This information is useful if we want to upgrade the server - we will know where to add something, how many slats to take and which server to go to. <br><br><h2>  Storage devices <br></h2><br>  6 years ago, all the discs were with pancakes that were spinning.  A separate story was to collect just a list of all disks.  There were several different approaches, because I could not believe that you can just watch ls / dev / sd.  But in the end we stopped to watch ls / dev / sd * and ls / dev / cciss / c0d *.  In the first case, this is a SATA device, in the second - SCSI and SAS. <br><br><img src="https://habrastorage.org/webt/e7/gm/br/e7gmbraujo_ko4qmfanfwnru71y.png"><br><br>  Literally this year, they started selling nvme-disks and added the nvme list here. <br><br>  After the list of disks is compiled, we try to read 0 bytes from it in order to understand that this is a block device and everything is fine.  If we could not read it, then we consider that this is some kind of a ghost, and we have never had such a disk. <br><br>  The first approach to checking disks was obvious: ‚ÄúLet's write random data to disk and see the speed‚Äù - <code>dd -o nocache -o direct if=/dev/urandom of=${disk}</code> .  As a rule, pancake disks give out 130-150 MB / s.  Having screwed up our eyes, we decided for ourselves that 90 MB / s is the figure after which there are good disks, all that is smaller is faulty. <br><br>  But users began to return again and say that the disks are bad.  It turned out that the cunning physics joked with us again. <br><br><img src="https://habrastorage.org/webt/m_/uh/tf/m_uhtfarwgyskpslnxxkufxynty.png"><br><br>  There is an angular velocity, and, as a rule, when you run -dd, it writes near the spindle.  If for some reason the spindle speed has degraded, then it is less noticeable than if you write from the edge of the disk. <br><br>  I had to change the principle of verification.  Now we check in three places: near the spindle, in the middle and outside.  Probably, you can only check the outside, but so it has historically developed.  And what works, do not touch. <br><br>  You can use <strong>smartctl</strong> to ask the disk how it is.  We think of a good drive: <br><br><ul><li>  There are no Reallocated Sectors ( <strong>Reallocated Sectors Count = 0)</strong> , that is, all sectors that left the factory are working. </li><li>  We <strong>do not use discs older than 4 years</strong> , although they are quite working.  Before we introduced this practice, we had disks for 7 years.  Now we believe that after 4 years the disk has paid off, and we are not ready to accept the risk of wear. </li><li>  There are no sectors that are going to be Reallocated ( <strong>Current_Pending_Sector = 0</strong> ). </li><li>  <strong>UltraDMA CRC Error Count = 0</strong> - these are errors on the SATA cable.  If there is an error, you just need to change the wire, you do not need to change the disk. </li></ul><br><img src="https://habrastorage.org/webt/01/jy/48/01jy48l5jyw-x1yx7wse-yawhdq.png"><br><br>  Spread SSDs are generally beautiful disks, they work fast, they do not make noise, they do not heat up.  We believe that a good SSD has a write speed of more than 200 MB / s.  Our clients love low prices, and server models that give out 320-350 MB / s do not always come to us. <br><br>  For SSD, we also watch smartctl.  Same Reallocated, Power_On_Hours, Current_Pending_Sector.  All SSD disks are able to display the degree of wear, it shows the parameter Media_Wearout_Indicator.  We wear discs up to 5% of life, and only then remove.  Such disks sometimes find a second life in the personal needs of employees.  For example, I recently found out that in 2 years such a disk has worn out by another 1% in an employee‚Äôs laptop, although we have 95% of this disk under the SSD cache in about 10 months. <br><br>  But the problem is that not all disk manufacturers have agreed on the parameter names, and this Media_Wearout_Indicator, for example, Toshiba is called Percent_Lifetime_Used, other manufacturers have Wear Leveling Count, Percent Lifetime Remaining, or simply. * Wear. *. <br><br>  Crucial does not have this parameter at all.  Then we simply consider the amount of disc rewriting - ‚Äúbyte writed‚Äù - how many bytes we have already written to this disk.  Further, according to the specification, we are trying to figure out how many overwrites this disk is designed by the manufacturer.  By elementary mathematics we determine how long he will live.  If it's time to change - change. <br><br><h2>  RAID <br></h2><br>  I don‚Äôt know why in the modern world our clients still want RAIDs.  People buy RAID, put 4 SSDs there, which are much faster than this RAID (6 Gbps).  They have some kind of instruction, and they collect it.  I think this is almost unnecessary stuff. <br><br>  There used to be 3 manufacturers: Adaptec;  3ware;  Intel  We had 3 utilities, we bother, but we made diagnostics for everyone.  Now LSI has bought everyone - there is only one utility left. <br><br>  When our diagnostic system sees RAID, it parses the logical volume into separate disks so that you can measure the speed of each disk, read it Smart.  After that, it remains for the RAID to check the battery.  Who does not know - there is enough batteries for RAID in order to twist all the disks for another 2 hours.  That is, you turn off the server, take out, and it rotates the disk for another 2 hours to complete all the recordings. <br><br><h2>  Network <br></h2><br>  With the network, everything is pretty simple - inside the data center should be less than 300 Mbps.  If less, then you need to repair.  We also look at errors on the interface.  <strong>Errors on the network interface should not be at all</strong> , and if they are, then everything is bad. <br><br><img src="https://habrastorage.org/webt/ad/m2/yw/adm2yw4vmjpztcicdovupo84l20.png"><br><br>  Along the way, we try to update the BIOS and IPMI firmware.  It turned out that we do not like all BIOSes.  We still have BIOSes that do not know how to use UEFI and other features that we use.  We try to update it automatically, but this does not always work, everything is not very simple there.  If it does not work, then the person goes and updates it with his hands. <br><br>  IPMI Supermicro we do not give to the world, we have it on gray addresses through OpenVPN.  Nevertheless, we fear that one day another vulnerabilities will come out and we will suffer.  Therefore, we try to just the firmware IPMI was always the last.  If not, then update. <br><br>  From the strange recently got out that Intel on 10 and 40-gigabit network cards does not include PXE boot.  It turns out that if the server is in a rack in which there is only a 40-gigabit card, then it is impossible to boot over the network, because you need to boot into a gigabit card.  We are separately flashing network cards on 40G so that they will have PXE and be able to live further. <br><br>  <strong>After everything is checked, the server immediately goes on sale</strong> .  Calculates its price at which it is put on the site and sold. <br><br><img src="https://habrastorage.org/webt/by/ca/ah/bycaah2szao5f0-lcrkk5zcwzyo.png"><br><br>  In total, we have about 350 checks per month, 69% of servers are healthy, 31% are not healthy.  This is due to the fact that we have a rich history, some servers are already for 10 years.  Most of the servers that have not passed the test, we just throw. <br><br><blockquote>  For the curious: we have 3 clients who still live on the Pentium IV, and do not want to go anywhere.  They lack 512 MB of RAM. <br></blockquote><br>  The future has come!  If I would gossip this system today ... <br><br><img src="https://habrastorage.org/webt/4v/bb/e_/4vbbe_vodbx8hzm_zk-nf2oejf4.png"><br><br>  The excellent utility <a href="https://ezix.org/project/wiki/HardwareLiSter">Hardware Lister</a> (lshw) came out, which is able to communicate with the kernel, beautifully display what kind of hardware is in the kernel, what the kernel could detect.  Do not need all these dances.  If you repeat - I strongly advise you to look at this utility and use it.  Everything will become much easier. <br><br><h1>  Results: <br></h1><br><ul><li>  Compromise is not bad, it's just a matter of price.  If the solution is very expensive, you need to look for the level when both reliability and price are acceptable. </li><li>  Non-core programs are sometimes great for testing.  It remains only to find them. </li><li>  Test everything you can reach! </li></ul><br><blockquote>  The next big <a href="http://www.highload.ru/moscow/2018/">HighLoad ++</a> is November 8 and 9 in Moscow.  The program includes well-known specialists and new names, traditional and new tasks.  In the DevOps section, for example, already taken: <br><br><ul><li>  David O'Brien (Xirus), who will discuss the eternal - ‚Äú <a href="http://www.highload.ru/moscow/2018/abstracts/4075">Metrics!</a>  <a href="http://www.highload.ru/moscow/2018/abstracts/4075">Metrics!</a>  <a href="http://www.highload.ru/moscow/2018/abstracts/4075">Metrics!</a>  " <br></li><li>  Vladimir Kolobaev (Avito) with the report ‚Äú <a href="http://www.highload.ru/moscow/2018/abstracts/4161">Monitoring to developers!</a>  <a href="http://www.highload.ru/moscow/2018/abstracts/4161">Technology to the community!</a>  <a href="http://www.highload.ru/moscow/2018/abstracts/4161">Profit - all!</a>  " <br></li><li>  Elena Grahovac (N26) will present the " <a href="http://www.highload.ru/moscow/2018/abstracts/4187">Best Practices for Native Cloud Services</a> ." <br></li></ul><br>  Study the <a href="http://www.highload.ru/moscow/2018/abstracts">list of</a> reports and hurry to join.  Or <a href="http://eepurl.com/VYVaf">subscribe</a> to our newsletter, and will regularly receive reviews of reports, reports on new articles and videos. <br></blockquote></div><p>Source: <a href="https://habr.com/ru/post/426733/">https://habr.com/ru/post/426733/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../426723/index.html">Microsoft partners are hoping to create a time capsule on the moon</a></li>
<li><a href="../426725/index.html">How to do things when they do not want to do</a></li>
<li><a href="../426727/index.html">Eme? CDM? DRM? CENC? IDK! What you need to make your own video player in the browser</a></li>
<li><a href="../426729/index.html">TypeScript magic school: generics and type extensions</a></li>
<li><a href="../426731/index.html">CSS: interesting border-radius features</a></li>
<li><a href="../426735/index.html">Welcome to the JETHACK hackathon</a></li>
<li><a href="../426737/index.html">Briefly about the architecture of neuromorphic processors: a view from the inside</a></li>
<li><a href="../426739/index.html">Proxy files from AWS S3 using nginx</a></li>
<li><a href="../426741/index.html">Memo for those who are planning for the first time to recruit interns</a></li>
<li><a href="../426743/index.html">Ways to use the blockchain turned somewhere not there. Sony announced the creation of a DRM-system based on the chain of blocks</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>