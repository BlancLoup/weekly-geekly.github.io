<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How we built a fast and reliable ad views repository</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="One of the unobtrusive, but important functions of our sites ads - saving and displaying the number of their views. Our sites have been following ad v...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How we built a fast and reliable ad views repository</h1><div class="post__text post__text-html js-mediator-article">  One of the unobtrusive, but important functions of <a href="https://habr.com/company/kolesa/profile/">our sites ads</a> - saving and displaying the number of their views.  Our sites have been following ad views for more than 10 years.  The technical implementation of the functionality has changed several times during this time, and now it is a (micro) service on Go, working with Redis as a cache and task queue, and with MongoDB as a persistent storage.  Several years ago, he learned to work not only with the amount of ad views, but also with the statistics for each day.  But to do all this really quickly and reliably, he learned recently. <br><br><img src="https://habrastorage.org/webt/ee/nn/jp/eennjposwyrztis3a7s6cbkhq2e.png" alt="image"><br><br>  In total for projects, the service processes ~ 300 thousand requests for reading and ~ 9 thousand requests for writing per minute, 99% of which are executed up to 5ms.  This is, of course, not astronomical indicators and not launching rockets to Mars - but also not such a trivial task as simple storage of numbers may seem.  It turned out that doing all this, ensuring the preservation of data without loss and reading consistent, actual values ‚Äã‚Äãrequires some effort, which we describe below. <br><a name="habracut"></a><br><h3>  Tasks and project overview </h3><br>  Although viewing counters are not so critical for a business as, say, processing payments or <a href="https://habr.com/company/kolesa/blog/431342/">requests for a loan</a> , they are primarily important to our users.  People are fascinated by tracking the popularity of their ads: some even call support when they notice inaccurate information about views (this happened with one of the previous implementations of the service).  In addition, we store and display detailed statistics in users' personal accounts (for example, to assess the effectiveness of using paid services).  All this forces us to treat carefully the saving of each viewing event and the display of the most relevant values. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In general, the functionality and principles of the project look like this: <br><br><ul><li>  A web page or application screen makes a request for ad counters (the request is usually asynchronous to prioritize the output of basic information).  And if the page of the ad itself is displayed, the client will instead ask to increase and return the updated amount of views. </li><li>  While processing read requests, the service tries to retrieve information from the Redis cache, and the missing one complements by executing a request to MongoDB. </li><li>  Write requests are sent to 2 structures in radish: a queue of incremental updates (processed in the background, asynchronously) and a cache of total views. </li><li>  The background process in the same service reads items from the queue, accumulates them in the local buffer, and periodically writes it to MongoDB. </li></ul><br><h3>  View counters recording: pitfalls </h3><br>  Although the steps described above look quite simple, the problem here is the organization of interaction between the database and the microservice instances so that the data is not lost, duplicated, and not delayed. <br><br>  Using only one repository (for example, only MongoDB) would solve some of these problems.  In fact, before the service worked, until we came up against the problem of scaling, stability and speed. <br><br>  A naive implementation of moving data between storages could lead, for example, to such anomalies: <br><br><ul><li>  Loss of data during competitive write to cache: <br><ol><li>  Process <b>A</b> increases the Redis cache view count, but discovers that there is no data for this entity (it can be either a new ad or an old one that has been extruded from the cache), so the process must first get this value from MongoDB. <br></li><li>  Process <b>A</b> receives a view counter from MongoDB ‚Äî for example, the number 5;  then adds 1 to it and is about to write to Redis <b>6</b> . </li><li>  Process <b>B</b> (initiated by, say, another user of the site who simultaneously approached the same ad) does the same in parallel. </li><li>  Process <b>A</b> writes the value <b>6</b> to Redis. </li><li>  Process <b>B</b> writes the value <b>6</b> to Redis. </li><li>  As a result, one view is lost due to race while recording data. <br>  <i>The scenario is not so unlikely: for example, we have a paid service that places an ad on the main page of the site.</i>  <i>For a new announcement, such a course of events may lead to the loss of multiple views at once due to their sudden influx.</i> </li></ol></li><li>  An example of another scenario is data loss when moving views from Redis to MongoDb: <br><br><ol><li>  The process takes the pending value from Redis and stores it into its memory for later writing to MongoDB. </li><li>  The write request ends with an error (or the process crashes before it is executed). </li><li>  The data is lost again, which will become obvious the next time the cached value is preempted and replaced with a value from the database. </li></ol><br></li></ul><br>  There may be other errors, the causes of which also lie in the non-atomic nature of operations between databases, for example, a conflict while simultaneously removing and increasing views of the same entity. <br><br><h3>  View counters recording: solution </h3><br>  Our approach to storing and processing data in this project is based on the expectation that at any given time MongoDB can refuse more likely than Redis.  This, of course, is not an absolute <i>rule</i> - at least not for every project - but in our environment we are really used to observing periodic timeouts on requests to MongoDB caused by the performance of disk operations, which was one of the reasons for losing some of the events. <br><br>  To avoid many of the problems mentioned above, we use task queues for deferred storage and lua scripts that enable atomically changing data in several radish structures at once.  With this in mind, in detail the scheme of saving views looks like this: <br><br><ol><li>  When a write request gets into microservice, it executes the lua-script <b>IncrementIfExists</b> to increase the counter only if it already exists in the cache.  The script immediately returns <b>-1</b> if there is no data for the viewed entity in the radish;  otherwise, it increases the value of the cached hits in <a href="https://redis.io/commands/hincrby">HINCRBY</a> , adds an event to the queue for later saving to MongoDB (we call it <i>pending queue</i> ) via <a href="https://redis.io/commands/lpush">LPUSH</a> , and returns the updated amount of hits. <br></li><li>  If the IncrementIfExists returns a positive number, this value is returned to the client and the request is completed. <br><br>  Otherwise, microservice takes the view counter from MongoDb, increases it by 1 and sends it to radishes. <br></li><li>  Writing to radishes is done through another lua script, <b>Upsert</b> , which saves the amount of views to the cache if it is still empty, or increments them by 1 if someone else managed to fill the cache between steps 1 and 3. <br></li><li>  Upsert also adds a viewing event to the pending queue, and returns the updated amount, which is then sent to the client. <br></li></ol><br>  Due to the fact that lua-scripts <a href="https://redis.io/commands/eval">are executed atomically</a> , we avoid a lot of potential problems that could be caused by a competitive entry. <br><br>  Another important detail is ensuring the safe transfer of updates from the pending queue to MongoDB.  To do this, we used the ‚Äúsecure queue‚Äù template described in the <a href="https://redis.io/commands/rpoplpush">Redis documentation</a> , which significantly reduces the chances of data loss by creating a copy of the processed items in a separate, one more queue until they are permanently stored in the persistent storage. <br><br>  To better understand the steps of the whole process, we have prepared a little visualization.  First, look at the usual, successful scenario (the steps are numbered in the upper right corner and are described in detail below): <br><br><img src="https://habrastorage.org/webt/0v/al/bq/0valbqz3kj6du62z0foxfcb6bay.gif" alt="image"><br><br><ol><li>  Microservice receives a write request </li><li>  The request handler sends it to the lua script, which writes the scan to the cache (immediately making it readable) and to the queue for subsequent processing. </li><li>  Background gorutina (periodically) performs the <a href="https://redis.io/commands/brpoplpush">BRPopLPush</a> operation, which atomically moves an item from one queue to another (we call it the ‚Äúprocessing queue‚Äù - the queue with the items currently being processed).  The same element is then stored in a buffer in the process memory. <br></li><li>  Another write request is received and processed, which leaves us with 2 items in the buffer and 2 items in the processing queue. </li><li>  After some timeout, the background process decides to flush the buffer in MongoDB.  Writing a set of values ‚Äã‚Äãfrom the buffer is performed in one request, which has a positive effect on throughput.  Also, before recording, the process tries to combine several views into one, summing up their values ‚Äã‚Äãfor the same ads. <br>  <i>Each of our projects uses 3 instances of microservice, each with its own buffer, which is saved to the database every 2 seconds.</i>  <i>During this time, about 100 elements accumulate in one buffer.</i> <i><br></i> </li><li>  After successful writing, the process removes the items from the processing queue, signaling that the processing has been completed successfully. <br></li></ol><br>  When all subsystems are in order, some of these steps may seem redundant.  And the attentive reader may also have a question about what makes the gopher sleeping in the lower left corner. <br>  Everything is explained when considering the script when MongoDB is unavailable: <br><br><img src="https://habrastorage.org/webt/hl/jd/bs/hljdbsmwzxn6i2k5xfgj2mm02em.gif" alt="Example of service operation in case of failure of MongoDB"><br><br><ol><li>  The first step is identical to the events from the previous scenario: the service receives 2 requests for recording views and processes them. <br></li><li>  The process lost connection with MongoDB (the process itself, of course, still does not know about it). <br>  The handler, as before, tries to flush its buffer into the database, but this time without success.  It returns to the expectation of the next iteration. <br></li><li>  Another background gorutin wakes up and checks the processing queue.  She discovers that elements have been added to her for a long time;  concluding that their processing failed, she moves them back to the pending queue. <br></li><li>  After some time, the connection with MongoDB is restored. <br></li><li>  The first background gorutin tries again to perform the write operation ‚Äî this time successfully ‚Äî and eventually eventually removes the elements from the processing queue. <br></li></ol><br>  In this scheme, there are several important timeouts and heuristics derived through testing and common sense: for example, elements are moved back from the processing queue to the pending queue after 15 minutes of their inactivity.  In addition, Gorutin, who is responsible for this task, performs <a href="https://redis.io/commands/set">blocking</a> before executing, so that several instances of microservice do not attempt to restore hung-up views at the same time. <br><br>  Strictly speaking, even these measures do not provide theoretically reasonable guarantees (for example, we ignore scenarios like the process hangs for 15 minutes) - but in practice it works quite reliably. <br><br>  Also in this scheme there are still at least 2 known vulnerabilities that are important to be aware of: <br><br><ul><li>  If microservice fell immediately after successfully saving to MongoDb, but before clearing the processing queue list, then this data will be considered unsaved - and after 15 minutes will be saved again. <br>  <i>To reduce the likelihood of such a scenario, we have provided repeated attempts to remove from the processing queue in case of errors.</i>  <i>In reality, we have not yet observed such cases in production.</i> <br></li><li>  When the radish is reloaded, it can lose not only the cache, but also part of the unsaved scans from the queues, since it is configured to periodically save the <a href="https://redis.io/topics/persistence">RDB snapshots</a> every few minutes. <br>  <i>Although in theory this can be a serious problem (especially if the project deals with really critical data), in practice the nodes are restarted extremely rarely.</i>  <i>At the same time, according to monitoring, the elements are held in queues for less than 3 seconds, that is, the possible amount of losses is very limited.</i> <br></li></ul><br>  It may seem that the problems turned out more than we would like.  However, in reality, it turns out that the scenario from which we were originally protected - the failure of MongoDB - is indeed a much more real threat, and the new data processing scheme successfully ensures the availability of the service and prevents losses. <br><br>  One of the clearest examples of this was the case when the MongoDB instance on one of the projects was unavailable all night by ridiculous chance.  All this time, the viewing counters were accumulated and rotated in radish from one queue to another, until in the end they were not stored in the database after the incident was resolved;  most users did not even notice the failure. <br><br><h3>  Reading view counters </h3><br>  Requests for reading are much easier than writing: microservice first checks the cache in radish;  all that is not found in the cache is filled with data from MongoDb and returned to the client. <br><br>  There is no pass-through cache entry for read operations, to avoid the overhead of protecting against concurrent writing.  The smarter cache is still quite good, since most often it is already warmed up thanks to other write requests. <br><br>  The daily view statistics are read from MongoDB directly, since it is requested less often, and it is more difficult to cache it.  It also means that when the database is unavailable, reading the statistics stops working;  but it affects only a small part of users. <br><br><h3>  Data storage scheme in MongoDB </h3><br>  The MongoDB collection scheme for the project is based on <a href="https://www.mongodb.com/blog/post/schema-design-for-time-series-data-in-mongodb">these recommendations from the database developers themselves</a> , and looks like this: <br><br><ul><li>  Views are saved in 2 collections: one contains their total amount, the other contains statistics by day. <br></li><li>  The data in the collection with statistics is organized on the principle of <b>one document per advertisement per month</b> .  For new listings, a document is inserted into the collection, filled with thirty-one zero for the current month;  According to the article mentioned above, this allows you to immediately allocate enough space for a document on disk, so that the database does not have to move it when adding data. <br>  <i>This item makes the process of reading statistics a bit awkward (requests must be formed by months on the side of microservice), but overall the scheme remains fairly intuitive.</i> <br></li><li>  For recording, an <b>upsert</b> operation is used to update within one request and, if necessary, create a document for the required entity. <br></li></ul><br>  We do not yet use the transactional capabilities of MongoDb to update several collections at the same time, which means we risk that data can be written only in one collection.  For the time being, we are just logging such cases;  there are a few of them, and so far this does not present the same significant problem as other scenarios. <br><br><h3>  Testing </h3><br>  I would not trust my own words about the fact that the described scripts really work if they were not covered by tests. <br><br>  Since most of the project code works closely with radish and MongoDb, most of the tests in it are integration tests.  The test environment is supported through docker-compose, which means it is deployed quickly, provides reproducibility by resetting and restoring state every time it starts, and provides an opportunity to experiment without affecting other people's databases. <br><br>  There are 3 main areas of testing in this project: <br><br><ol><li>  Validation of business logic in typical scenarios, so-called.  happy-path.  These tests answer the question - when all subsystems are in order, does the service work according to functional requirements? </li><li>  Verification of negative scenarios in which the service is expected to continue.  For example, does the service really lose data when MongoDb crashes? <br>  Are we confident that the information remains consistent with periodic timeouts, freezes, and concurrent write operations? </li><li>  Verification of negative scenarios in which we do not expect the service to continue, but the minimum level of functionality should still be provided.  For example, there is no chance that the service will continue to store and give away data when neither radish nor Mongo is available - but we want to be sure that in such cases it does not fall, it waits for the system to recover and then returns to work. </li></ol><br>  To check for failed scenarios, the service business logic code works with the database client interfaces, which are replaced with implementations in the required tests, returning errors and / or simulating network delays.  We also simulate the parallel operation of several instances of the service using the " <a href="http://www.jerf.org/iri/post/2929">environment object</a> " pattern.  This is a variant of the well-known ‚Äúinversion of control‚Äù approach, where functions do not address dependencies themselves, but receive them through the environment object passed in the arguments.  In addition to other advantages, the approach allows simulating several independent copies of the service in one test, each of which has its own pool of connections to the database and more or less effectively reproduces the production environment.  Some tests run each such instance in parallel and make sure that they all see the same data, and race conditions are missing. <br><br>  We also conducted a rudimentary, but still quite useful stress test based on <br>  <a href="https://github.com/JoeDog/siege">siege</a> , which helped to roughly estimate the allowable load and speed of response from the service. <br><br><h3>  About performance </h3><br>  For 90% of requests, the processing time is very insignificant, and most importantly - stable;  Here is an example of measurements on one of the projects for several days: <br><br><img src="https://habrastorage.org/webt/ln/bk/zy/lnbkzy7-wnykbaelv4vzsd8b53q.png" alt="image"><br><br>  Interestingly, a write (which is actually a write + read operation, since it returns updated values) is slightly faster than a read (but only from the point of view of a client who does not observe the actual pending write). <br>  A regular morning increase in delays is a side effect of the work of our analytics team, which daily collects its own statistics based on the data from the service, creating us an ‚Äúartificial highload‚Äù. <br><br>  The maximum processing time is relatively long: among the slowest requests, new and unpopular ads show themselves (if the ad has not been viewed and is displayed only in lists - its data does not fall into the cache and is read from MongoDB), group requests for multiple ads at once (they were worth to make a separate schedule), as well as possible network delays: <br><br><img src="https://habrastorage.org/webt/6f/pz/v9/6fpzv9b8lmswdsjhmrn4gk_qugw.png" alt="image"><br><br><h3>  Conclusion </h3><br>  Practice, to some extent, counterintuitively, showed that using Redis as the main repository for the display service increased overall stability and improved its overall speed. <br><br>  The main load of the service is read requests, 95% of which are returned from the cache, and therefore they work very quickly.  Write requests are deferred, although from the end user's point of view, they also work quickly and become visible to all clients immediately.  In general, almost all customers receive responses in less than 5ms. <br><br>  As a result, the current version of microservice based on Go, Redis and MongoDB successfully works under load and is able to survive the periodic unavailability of one of the data stores.  Based on previous experience with infrastructure problems, we identified the main error scenarios and successfully defended against them, so that most users are not inconvenienced.  And we, in turn, receive much less complaints, alerts and messages in the logs - and are ready for further increase in attendance. </div><p>Source: <a href="https://habr.com/ru/post/431902/">https://habr.com/ru/post/431902/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../431890/index.html">How to place an order on the exchange of freelancing</a></li>
<li><a href="../431892/index.html">We use Veeam Backup & Replication to test new systems and applications before upgrading</a></li>
<li><a href="../431894/index.html">In December, will decide on the mandatory registration of base stations LPWAN</a></li>
<li><a href="../431898/index.html">It is all about Agile - 2: features of the introduction of agile development</a></li>
<li><a href="../431900/index.html">In the Linux kernel, the word fuck was replaced by hug</a></li>
<li><a href="../431904/index.html">How we have unloaded HR specialists: infomats for issuing payment sheets</a></li>
<li><a href="../431906/index.html">PIFR - a method of generating a 3D mask, regardless of the angle of rotation of the face</a></li>
<li><a href="../431908/index.html">Setting up the Tinkoff Bank API. How is your intuition ....? Or a song about Oauth 2.0</a></li>
<li><a href="../431910/index.html">PSEFABRIC is a new approach to network management and automation. Step to the ideal</a></li>
<li><a href="../431912/index.html">In the US, the largest bot is not arrested: what does this mean for the digital community?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>