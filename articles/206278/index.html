<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>New version of HP Vertica: Crane number 7</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In December 2013, a new, seventh version of HP Vertica was released. In the continuation of the tradition of large construction of ‚Äúnot small data‚Äù, t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>New version of HP Vertica: Crane number 7</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/d38/37f/481/d3837f4814f5dab20ed174e1f7e8bdb6.jpg" alt="image"><br>  In December 2013, a new, seventh version of HP Vertica was released.  In the continuation of the tradition of large construction of ‚Äúnot small data‚Äù, the version was named ‚ÄúCrane‚Äù (the sixth version was called ‚ÄúBulldozer‚Äù).  In this article I will describe what has changed in the new version. <br><br><h4>  Work with unstructured data - Flex Zone </h4><br>  The most important step up the big data ladder in the new version of HP Vertica is the emergence of support for direct work with unstructured data in CSV and JSON formats.  The sixth version supported loading data from CSV files and querying them as to external global tables.  If the file data had a previously unknown, floating structure, then the only way to load and work with such data in Vertica was to pre-process them in external applications, such as ETL tools. <br><br>  Now Vertica is able to work with unstructured data as easily as with structured data.  It looks like this: <br><img src="https://habrastorage.org/getpro/habr/post_images/06f/1ea/fe9/06f1eafe9ba23c37d02eb9e5e264a633.jpg" alt="image">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The HP Vertica Flex Zone is a special area for storing and processing unstructured data.  In the Vertica database, you can create flex tables, load data from files with CSV and JSON formats into them, and query them by connecting these data in queries with Vertica relational tables.  The loaded data in flex tables is stored on the nodes of the server cluster in a special format, but according to the same principles as the relational data of the database.  Compression, mirroring and data segmentation are also supported for them (distribution between the nodes of the cluster).  With this storage, the unstructured data, when processed, takes full advantage of the Vertica MPP architecture, works in a fault-tolerant scalable architecture, and participates in backups. <br><a name="habracut"></a><br>  A great advantage of Flex Zone is that it is not an external solution integrated with Vertica (like HDFS / Hadoop connectors), but a native native support for unstructured data.  This gives flexibility and lack of dependence on errors and changing versions of an external product.  It also guarantees the speed of work with hybrid processing in queries using structured and unstructured data tables. <br><br>  Where is the flex zone useful?  A simple example: you need to load files with a dynamic (floating) structure.  Moreover, it is not known in advance which data will be further demanded in the analysis, and which will not.  Variants of solutions to this problem on the sixth version of Vertica would be: <br><ol><li>  Create a table in the database with all possible fields that may be present in the files.  Track the appearance of new columns in files and add them to the table.  For JSON formats, write and maintain an additional data loader, which parses the format and writes data to the table.  The solution is productive, but expensive to develop and maintain. </li><li>  Create a task in Hadoop to parse and write the processed files to CSV format, and store the result of processing in HDFS.  Connect from HDFS the received files as external tables to Vertica, specifying the required fields for processing.  As new fields appear, recreate the connection of external tables with a modified structure.  The solution is fast in development, but expensive to maintain and not productive. </li></ol><br>  For the seventh version of Vertica, this task of processing, storing and working with files will be reduced to just two SQL statements: <br><ol><li>  Define a flex table (create table) </li><li>  Load data into flex table (copy) </li></ol><br>  Vertica allows for flex tables to describe immediately materialized columns (create / alter table).  Such columns are filled with values ‚Äã‚Äãimmediately when they are loaded and are stored as standard columns of the Vertica tables in the column structure.  The remaining columns can be accessed dynamically through the created view, which can be rebuilt using specialized functions.  Such an approach allows materializing the relevant and key fields in the queries, obtaining other data from the stored unstructured data during the processing of the request.  Such a hybrid approach allows you to retain high query performance even for unstructured data. <br><br>  In summary, I would like to summarize: The Flex Zone allows you to easily and securely store unstructured data, extracting information on demand from it.  This saves on the complexity of the development and maintenance of the solution to work with such data and provides excellent opportunities in the field of analysis of various data. <br><br>  Since the Flex zone runs on Vertica cluster servers, it is stored and processed in the same place, it is licensed in the same way as structured data, that is, by the amount of source data.  Here I would like to notice one big drawback with such licensing: unlike structured data, flex tables load the entire volume of these files for storage in the database, which means everything that is loaded will be licensed.  If only half of the fields from an unstructured file are used, the storage of such data is expensive in terms of licenses.  For such cases, it is nevertheless more convenient to use HDFS unstructured files as storage and process them in Hadoop, delivering the processing results to Vertica.  However, the licensing scheme is too vaguely described in the documentation, I will clarify its provisions and further write off on this issue. <br><br><h4>  Working with HDFS files with HCatalog </h4><br>  In the sixth version of Vertica was the ability to download flat files from HDFS.  It was also possible to include such files for analysis in queries as external tables.  The main disadvantage here is the requirement to accurately describe the structure of the file being loaded from HDFS.  In the new version of Vertica, support for working with flat HDFS files has been extended to support metadata for the description of flat tables through a connector to the HCatalog interface, which serves to store the description of the structure of flat files (metadata) for Apache Hive.  It looks like this: <br><img src="https://habrastorage.org/getpro/habr/post_images/431/ca9/346/431ca934643a5daa57778a909265c538.png" alt="image"><br><br>  When loading data from HDFS files or querying them as connected external tables, Vertica receives metadata for the files from the HCatalog server.  The server then loads the file data itself from the HDFS cluster and processes it.  The processing capabilities of Hadoop / Hive are not used.  The Vertica documentation says that due to efficient server solutions in the field of data analysis, it is more profitable to collect data and further process it on the Vertica side than on the Hadoop side.  An additional advantage of this approach is the cost of development and maintenance.  By the way, in the same documentation it is added that in the case of massive and heavy data processing it is recommended to do this on Hadoop, here Vertica will be less effective.  So we can say that Vertica in a certain sense not only integrates with Hadoop, but also tries to replace it in the field of analytical data processing, providing both its own algorithms for processing unstructured data and its data storage in the Flex zone. <br><br><h4>  Large cluster management </h4><br>  Any MPP always has an hour X sooner or later: the number of cluster servers reaches a critical level when the costs of message transfer and network coordination begin to be the bottleneck of the system.  Apparently, the HP Vertica server has grown to companies in which hundreds and thousands of servers are not a miracle, but a working moment.  Therefore, in the seventh version, the possibilities of network interaction of the cluster servers were significantly expanded.  Now, in addition to the usual broadcast mode of Vertica, you can enable the ‚ÄúLarge Cluster Arrangement‚Äù mode, which allows you to optimize Vertica for clusters with a large number of servers.  Vertica documentation recommends using it both with a number of 120 servers in a cluster, and with a smaller number of servers if there is a high-loaded network activity between them.  The principle of a large cluster mode is that its nodes are combined into groups in which coordinators of network interaction are appointed.  Within each group, nodes communicate with each other, and between groups, the coordinators of groups are responsible for networking. <br><br>  There is another MPP problem that is critical for large clusters: this is the fault tolerance of a group of servers (racks).  As you know, Vertica mirrors the data between neighboring nodes.  Prior to the seventh version, this happened automatically and it was not possible to specify the order of mirroring.  This jeopardized the work of the cluster as a whole if it had multiple server racks and one of the racks was turned off.  It turned out that the neighboring nodes disappeared from the cluster and he was forced to stop his work.  In the seventh version, this problem was solved with the help of ‚ÄúFault Groups‚Äù.  Now for the database it is possible to paint server groups by racks and paint on them working cluster nodes: <br><img src="https://habrastorage.org/getpro/habr/post_images/562/ac9/6d5/562ac96d5e36a2ef9db5c7b794ffc4a1.jpg" alt="image"><br><br>  When data is segmented, the Vertica server will ensure that the mirror of these servers will be stored in different fault tolerant groups.  Thus, shutting down one server group (rack) will not cause the cluster to stop, as the other group will have a mirror that they can connect to in order for the cluster to work. <br><br><h4>  JDBC API Key-Value </h4><br>  Initially, HP Vertica was designed as an analytical data server that allows you to quickly perform analysis on large amounts of data.  Gradually, the server capabilities grew and now Vertica can already be called a full-fledged BigData server, which allows processing both structured and unstructured data.  Any BigData server must, in particular, be able to not only quickly analyze data, but also look for values ‚Äã‚Äãby key, that is, support fast access to data, acting as a Key-Value server.  The Vertica team seriously decided to cover all the BigData requirements in their product and implemented this feature through an extension for Vertica JDBC drivers.  Initially, searching for information by key in Vertica looked like a normal SELECT query in the database with a filter in WHERE by a key, as a result of which the query was distributed among the servers of the cluster, where everything started, until one of the cluster nodes found the desired result and returned it to the calling client session .  It looks like this: <br><img src="https://habrastorage.org/getpro/habr/post_images/ae3/dad/015/ae3dad0153701f7f064897285cbea951.jpg" alt="image"><br><br>  Now, using the Key-Value interface, you can query the data as follows: <br><pre><code class="java hljs">VGet get = conn.prepareGet(<span class="hljs-string"><span class="hljs-string">"public"</span></span>, <span class="hljs-string"><span class="hljs-string">"users"</span></span>); get.addPredicate(<span class="hljs-string"><span class="hljs-string">"id"</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>); ResultSet rs = get.execute();</code> </pre> <br>  When executing this code, the JDBC interface will immediately determine which cluster server the required entry is on and receive it: <br><img src="https://habrastorage.org/getpro/habr/post_images/c36/5d5/9fc/c365d59fcb804ebeb01a6be0c5df0180.jpg" alt="image"><br><br>  Work with Key-Value is achieved due to certain rules of records segmentation and the presence of a PK / UK key in the processed table.  Without these rules on the table being processed, this method will not work. <br><br>  Summarizing, we can say that the Key-Value interface allows you to organize a highly efficient way to quickly find and retrieve data on a given key for multiple sessions, without unnecessary loads on the cluster servers by processor time, disk and network activity.  I think this will be primarily appreciated by companies related to the web, for which the speed of information retrieval by key is always important. <br><br><h4>  Client Drivers </h4><br>  Vertica's JDBC driver has added support for the JDBC 4 standard. This is primarily useful for solutions that use JNDI.  It also expanded support for obtaining database metadata through drivers and improved the work of the connection pool. <br><br>  Support for ‚Äúconnection failover‚Äù and ‚Äúconnection load balancing‚Äù has been introduced for all drivers.  You can specify a list of hosts by which you can connect in turn, if the previous one in the list was not available.  You can also specify the connection balancing option for the connection.  Then, when connecting to the first available host in the list, the connected server can redirect the client‚Äôs connection to another, less loaded, available host.  These features virtually eliminate the need to use a third-party load balancer, performing its functions. <br><br>  For user authentication, the Kerberos version 5 protocol is also available in client drivers. <br><br>  The drivers optimized batch inserts (batch inserts).  According to the vendor, now the insertion speed is much higher and less resources are required on the client side. <br><br><h4>  Performance and fault tolerance </h4><br>  Optimized cluster operation in case of server failure.  This is achieved due to the uniform redistribution of computational loads between other nodes of the cluster.  Previously, the neighbor that had its mirror had to work for the failed node, which led to a general decrease in cluster performance due to the expectations of the working nodes, while the one working for two nodes would work out the calculations for both itself and the fallen neighbor. <br><br>  To speed up the processing of calculating the number of unique field values ‚Äã‚Äãin tables with a large number of records, functions of approximate counting the number with an error are added.  This allows you to significantly speed up the execution of requests, if the error is allowed. <br><br>  The optimizer now supports a new partial grouping algorithm using the hybrid GROUPBY PIPELINED and GROUPBY HASH algorithms.  This allows you to more efficiently perform data aggregation queries using part of the sorted columns, which reduces the memory and swap consumption on the disk for hashing. <br><br>  The operation of the MERGE operator is accelerated for tables that have PK / UK keys, by which data is combined, and the set of columns and values ‚Äã‚Äãfor insertion and changes is the same.  If these conditions are not met, then the tables are joined according to the algorithm of previous versions without optimization. <br><br>  The work of the data loader COPY has been improved, where now the parsing and downloading file is more efficiently parsed by splitting it into several parts and parallel processing between the node's cores. <br><br><h4>  SDK extension </h4><br>  In previous versions of Vertica, it was possible in C to extend the functionality of the server by writing your own scalar (simple) functions, data transformation functions (olap) and functions for processing the loaded data with the COPY and CREATE EXTERNAL TABLE operators.  Now it is also available on the Java platform.  Especially for this Java SDK is included in the new version of HP Vertica. <br><br><h4>  Other extensions </h4><br>  Significantly expanded the functionality of the web console (Management Console).  More and more, this tool is turning from a system operation monitoring tool into a full-fledged cluster administration tool.  Now the console supports viewing plans and profiles for executing queries and launching the designer to optimize and create new projections.  Even support for console skins has been added, although I don‚Äôt understand why this is needed. <br><br>  Significantly improved Database Designer itself.  Reworked algorithms for determining the best candidates for segmentation and packing of columns.  The ability to analyze the correlation of columns (close connection between them) is added, that is, data profiling is actually used in the analysis.  Added functions that allow programmatically create a design area for the specified tables and queries, analyze them and add new projections or modify existing ones, with a more optimal structure.  This allows you to automate the work of tuning the execution of data queries.  For example, you can independently develop and schedule the collection of heavy queries at a specified time, analyze them and get recommendations for optimizing data structures, applying them automatically to the database or saving them as a script for manual execution later.  Additionally, the concept of query groups is introduced, which allows the designer to identify typical queries and optimize for the weight of the frequency of their use.  For example, it allows not to build extra projections on heavy nightly requests, if they are performed once a day. <br><br>  Improved server installation system.  Vertica now checks for more OS settings on its own and tries to correct them.  In case of impossibility, recommendations for manual configuration are issued during the installation. <br><br>  The functionality of the SQL language, functions, service tables, configuration parameters, etc. was also expanded.  All this can be read in more detail in the HP Vertica documentation. <br><br><h4>  Summarizing the article </h4><br>  HP Vertica has really grown from a low power bulldozer to a high strong crane.  Construction BigData is in full swing.  The scope of changes, taking into account the fact that packs for the sixth version continued to come out with functionality, is really impressive. <br><br>  Author Konstantinov Alexey <br>  Data Warehouse Architect <br>  EasyData Company <br><br>  Please refer to the author when using article information. </div><p>Source: <a href="https://habr.com/ru/post/206278/">https://habr.com/ru/post/206278/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../206256/index.html">Python Meetup: November Meeting</a></li>
<li><a href="../206258/index.html">How and why I decided to start my own business</a></li>
<li><a href="../206262/index.html">.vimrc for frontendder</a></li>
<li><a href="../206264/index.html">Inventing JPEG</a></li>
<li><a href="../206276/index.html">Creating your own applications for installation through the standard Parallels Cloud Server tools</a></li>
<li><a href="../206280/index.html">For pioneers. On the waves with Jolla (UPD)</a></li>
<li><a href="../206282/index.html">Ubiquiti NanoBridge M5: the first installation experience or wifi bridge out of the box</a></li>
<li><a href="../206288/index.html">Chelyabinsk mathematician published an attempt to prove P = NP</a></li>
<li><a href="../206290/index.html">Tesla Tower: electrical calculation</a></li>
<li><a href="../206294/index.html">How important it is to write good code</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>