<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Parsing 25TB with AWK and R</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="How to read this article : I apologize for the fact that the text was so long and chaotic. To save your time, I begin each chapter with the introducti...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Parsing 25TB with AWK and R</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/webt/9d/3y/lc/9d3ylcjuqiv6r7vrv6p52apvmne.jpeg"></div><br>  <i><b>How to read this article</b> : I apologize for the fact that the text was so long and chaotic.</i>  <i>To save your time, I begin each chapter with the introduction of ‚ÄúWhat I have learned,‚Äù in which I state the essence of the chapter with one or two sentences.</i> <i><br><br></i>  <i><b>‚ÄúJust show the solution!‚Äù</b> If you just want to see what I‚Äôve come to, go to the chapter ‚ÄúI‚Äôm becoming more resourceful,‚Äù but I think it‚Äôs more interesting and useful to read about failures.</i> <br><br>  Recently, I was instructed to customize the processing of a large amount of the original DNA sequences (technically, this is an SNP chip).  It was necessary to quickly obtain data on a given genetic location (called SNP) for subsequent modeling and other tasks.  With the help of R and AWK I managed to clear and organize the data in a natural way, greatly speeding up the processing of requests.  It was hard for me and required numerous iterations.  This article will help you to avoid some of my mistakes and show you what I finally did. <br><a name="habracut"></a><br>  First, some introductory notes. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  Data </h2><br>  Our university center for the processing of genetic information provided us with data in the form of a TSV of 25 TB.  I got them broken into 5 packages compressed with Gzip, each of which contained about 240 four-gigabyte files.  Each row contained data for one SNP of one person.  In total, data on ~ 2.5 million SNPs and ~ 60 thousand people were transmitted.  In addition to the SNP information, there were numerous columns in the files with numbers reflecting various characteristics, such as reading intensity, frequency of different alleles, etc.  There were about 30 columns with unique values. <br><br><h4>  purpose </h4><br>  As with any data management project, the most important thing was to determine how the data would be used.  In this case, <b>we will mostly select models and workflows for SNPs based on SNPs</b> .  That is, at the same time we will need data for only one SNP.  I had to learn how to retrieve all the records related to one of the 2.5 million SNPs as easily as possible, faster and cheaper. <br><br><h1>  How not to do that </h1><br>  I quote a suitable cliche: <br><br><blockquote>  I couldn‚Äôt fail a thousand times, I just discovered a thousand ways to not parse a bunch of data in a format that is convenient for queries. </blockquote><br><h2>  First try </h2><br>  <b>What I learned</b> : there is no cheap way to hit 25 TB at a time. <br><br>  After listening to the Vanderbilt University subject ‚ÄúAdvanced methods for processing big data,‚Äù I was sure that this was in the bag.  Perhaps an hour or two will be spent on setting up a Hive server to run through all the data and report on the result.  Since our data is stored in AWS S3, I used the <a href="https://aws.amazon.com/athena/">Athena</a> service, which allows you to apply Hive SQL queries to S3 data.  No need to configure / raise the Hive-cluster, and even pay only for the data you are looking for. <br><br>  After I showed Athena my data and its format, I ran a few tests with similar requests: <br><br><pre><code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> intensityData <span class="hljs-keyword"><span class="hljs-keyword">limit</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span>;</code> </pre> <br>  And quickly got well-structured results.  Is done. <br><br>  While we have not tried to use the data in the work ... <br><br>  I was asked to pull out all the information on the SNP to test the model on it.  I ran the query: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> intensityData <span class="hljs-keyword"><span class="hljs-keyword">where</span></span> snp = <span class="hljs-string"><span class="hljs-string">'rs123456'</span></span>;</code> </pre> <br>  ... and waited.  After eight minutes and more than 4 TB of the requested data, I got the result.  Athena charges for the amount of data found, at $ 5 per terabyte.  So this single request cost $ 20 and eight minutes of waiting.  To get rid of the model according to all data, it was necessary to wait 38 years and pay $ 50 million. Obviously, this did not suit us. <br><br><h2>  It was necessary to use Parquet ... </h2><br>  <b>What I learned</b> : be careful with the size of your Parquet files and their organization. <br><br>  At first, I tried to rectify the situation by converting all the TSVs into <a href="https://parquet.apache.org/">parquet files</a> .  They are convenient for working with large data sets, because the information in them is stored in a column view: each column lies in its own memory / disk segment, unlike text files, in which the lines contain elements of each column.  And if you need to find something, it is enough to read the necessary column.  In addition, each file in the column stores a range of values, so if the desired value is not in the range of the column, Spark will not waste time scanning the entire file. <br><br>  I launched <a href="https://aws.amazon.com/glue/">AWS Glue's</a> simple task to convert our TSV to Parquet and threw new files into Athena.  It took about 5 hours.  But when I launched the request, it took about the same amount of time and a little less money.  The fact is that Spark, trying to optimize the task, simply unpacked one TSV chunk and put it into its own Parquet chunk.  And since each chunk was large enough to hold the full records of many people, every SNP was stored in each file, so Spark had to open all the files to extract the necessary information. <br><br>  Curiously, the default (and recommended) type of compression in Parquet ‚Äî snappy ‚Äî is not splitable.  Therefore, each executor stuck on the unpacking task and download a full 3.5 GB dataset. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f42/584/fb3/f42584fb3e65319eef46f117c11525f3.png"><br><h2>  We understand the problem </h2><br>  <b>What I learned</b> : sorting is difficult, especially if the data is distributed. <br><br>  It seemed to me that now I understood the essence of the problem.  I only needed to sort the data by SNP column, not by people.  Then several SNPs will be stored in a separate data chunk, and then the clever Parquet function ‚Äúopen only if the value is in the range‚Äù will manifest itself in all its glory.  Unfortunately, sorting the billions of lines scattered across a cluster has proven to be a challenge. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-0" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1105127759318319105"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  AWS doesn't exactly want to return the money due to ‚ÄúI am a scattered student.‚Äù  After I started sorting on Amazon Glue, it worked for 2 days and ended in a crash. <br><br><h2>  What about partitioning? </h2><br>  <b>What I learned</b> : Spark partitions should be balanced. <br><br>  Then I got the idea to partition the data into chromosomes.  There are 23 of them (and a few more, if we take into account mitochondrial DNA and unscrambled regions). <br>  This will divide the data into smaller portions.  If you add to the Spark-export function in the Glue script just one line <code>partition_by = "chr"</code> , then the data should be laid out in buckets. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/652/f42/3dc/652f423dc8806401b6638a3cf8c1480b.png"><br>  <i>The genome consists of numerous fragments, which are called chromosomes.</i> <br><br>  Unfortunately it did not work.  The chromosomes have different sizes, and therefore a different amount of information.  This means that the tasks that Spark sent to the workers were not balanced and were performed slowly, because some nodes finished earlier and were idle.  However, the tasks were completed.  But when one SNP was requested, the imbalance again caused problems.  The cost of processing SNP in larger chromosomes (that is, where we want to get data from) has decreased by only about 10 times.  Many, but not enough. <br><br><h2>  And if you divide into even smaller partitions? </h2><br>  <b>What I learned</b> : never try to do 2.5 million partitions at all. <br><br>  I decided to walk in full and partized every SNP.  This ensured the same size of partitions.  <b>The bad was the idea</b> .  I used Glue and added the innocent line <code>partition_by = 'snp'</code> .  The task has started and started to run.  A day later, I checked and saw that nothing was recorded in S3 so far, so I killed the task.  It seems that Glue was writing intermediate files to a hidden place in S3, and many files, perhaps a couple of million.  As a result, my mistake cost more than a thousand dollars and did not make my mentor happy. <br><br><h2>  Partitioning + sorting </h2><br>  <b>What I learned</b> : sorting is still difficult, as well as setting up Spark. <br><br>  The last attempt at partitioning was that I partitioned the chromosomes, and then sorted each partition.  In theory, this would speed up each request, because the desired SNP data would have to be within several Parquet chunks within a given range.  Alas, sorting even partitioned data has proven difficult.  As a result, I switched to EMR for a custom cluster and used eight powerful instances (C5.4xl) and Sparklyr to create a more flexible workflow ... <br><br><pre> <code class="scala hljs"># <span class="hljs-type"><span class="hljs-type">Sparklyr</span></span> snippet to partition by chr and sort w/in partition # <span class="hljs-type"><span class="hljs-type">Join</span></span> the raw data <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> the snp bins raw_data group_by(chr) %&gt;% arrange(<span class="hljs-type"><span class="hljs-type">Position</span></span>) %&gt;% <span class="hljs-type"><span class="hljs-type">Spark_write_Parquet</span></span>( path = <span class="hljs-type"><span class="hljs-type">DUMP_LOC</span></span>, mode = <span class="hljs-symbol"><span class="hljs-symbol">'overwrit</span></span>e', partition_by = c(<span class="hljs-symbol"><span class="hljs-symbol">'ch</span></span>r') )</code> </pre> <br>  ... but the task still has not been completed.  I set it up differently: I increased the memory allocation for each query executor, I used nodes with a large amount of memory, I used broadcast variables (broadcasting variable), but every time it turned out to be half-measures, and gradually the performers began to fail, until everything stopped. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-1" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1128703858610450434"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br><h1>  Becoming more creative </h1><br>  <b>What I learned</b> : sometimes special data requires special solutions. <br><br>  Each SNP has a position value.  This number corresponds to the number of bases lying along its chromosome.  This is a good and natural way to organize our data.  At first I wanted to partition into regions of each chromosome.  For example, positions 1 - 2000, 2001 - 4000, etc.  But the problem is that the SNPs are unevenly distributed across the chromosomes, so therefore the size of the groups will vary greatly. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f46/a8e/17b/f46a8e17b9af8d2ae9777c47017764c6.png"><br><br>  As a result, I came to a breakdown into categories (rank) positions.  According to the data already loaded, I drove a request to get a list of unique SNPs, their positions and chromosomes.  Then sorted the data inside each chromosome and collected the SNPs into groups (bin) of a given size.  Let's say 1000 SNPs each.  This gave me an SNP relationship with a chromosome-group. <br><br>  In the end, I made groups (bin) for 75 SNPs, I will explain the reason below. <br><br><pre> <code class="bash hljs">snp_to_bin &lt;- unique_snps %&gt;% group_by(chr) %&gt;% arrange(position) %&gt;% mutate( rank = 1:n() bin = floor(rank/snps_per_bin) ) %&gt;% ungroup()</code> </pre> <br><h2>  First attempt with Spark </h2><br>  <b>What I learned</b> : Spark integration is fast, but partitioning is still expensive. <br><br>  I wanted to read this small (2.5 million lines) data frame in Spark, merge it with the raw data, and then partition it into the newly added <code>bin</code> column. <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment"># Join the raw data with the snp bins data_w_bin &lt;- raw_data %&gt;% left_join(sdf_broadcast(snp_to_bin), by ='snp_name') %&gt;% group_by(chr_bin) %&gt;% arrange(Position) %&gt;% Spark_write_Parquet( path = DUMP_LOC, mode = 'overwrite', partition_by = c('chr_bin') )</span></span></code> </pre> <br>  I used <code>sdf_broadcast()</code> , so Spark finds out that it must send a data frame to all nodes.  This is useful if the data is small and is required for all tasks.  Otherwise, Spark tries to be smart and distributes data as needed, which can cause brakes. <br><br>  And again, my idea did not work: the tasks worked for some time, completed the merge, and then, like the executors started by partitioning, they began to fail. <br><br><h2>  Adding AWK </h2><br>  <b>What I have learned</b> : do not sleep when you are taught the basics.  Surely someone already solved your problem back in the 1980s. <br><br>  Up to this point, the cause of all my failures with Spark was the data jumble in the cluster.  Perhaps the situation can be improved by preprocessing.  I decided to try to divide the raw text data into chromosome columns, so I hoped to provide Spark with ‚Äúpre-partitioned‚Äù data. <br><br>  I looked at StackOverflow for how to break down the column values, and found <a href="https://unix.stackexchange.com/questions/114061/extract-data-from-a-file-and-place-in-different-files-based-on1-column-value">such an excellent answer.</a>  With AWK, you can split a text file by column values ‚Äã‚Äãby writing to the script, rather than sending the results to <code>stdout</code> . <br><br>  For the sample, I wrote a Bash script.  I downloaded one of the packed TSVs, then unpacked it with <code>gzip</code> and sent it to <code>awk</code> . <br><br><pre> <code class="bash hljs">gzip -dc path/to/chunk/file.gz | awk -F <span class="hljs-string"><span class="hljs-string">'\t'</span></span> \ <span class="hljs-string"><span class="hljs-string">'{print $1",..."$30"&gt;"chunked/"$chr"_chr"$15".csv"}'</span></span></code> </pre> <br>  It worked! <br><br><h2>  Filling cores </h2><br>  <b>What I learned</b> : <code>gnu parallel</code> is a magic thing, everyone should use it. <br><br>  The separation was rather slow, and when I ran <code>htop</code> to test the use of a powerful (and expensive) EC2 instance, it turned out that I only use one core and about 200 MB of memory.  To solve the problem and not lose a lot of money, you had to figure out how to parallelize the work.  Fortunately, in the absolutely amazing <a href="https://www.datascienceatthecommandline.com/">Data Science</a> book <a href="https://www.datascienceatthecommandline.com/">at the Command Line of</a> Jeron Janssens, I found a chapter on parallelization.  From it, I learned about <code>gnu parallel</code> , a very flexible method of implementing multithreading in Unix. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/835/7c0/e45/8357c0e45f4162d53ca1c3da0c78444a.png" width="300"></div><br>  When I started the split using the new process, everything was fine, but there was a bottleneck - downloading S3 objects to disk was not too fast and not completely parallelized.  To fix this, I did this: <br><br><ol><li>  I found out that it is possible to realize the stage of S3-downloading directly in the pipeline, completely eliminating intermediate storage on the disk.  This means that I can avoid writing raw data to disk and using even smaller, and therefore cheaper, storage on AWS. <br></li><li>  <code>aws configure set default.s3.max_concurrent_requests 50</code> command, <code>aws configure set default.s3.max_concurrent_requests 50</code> greatly increased the number of threads that AWS CLI uses (10 by default). <br></li><li>  I switched to a speed-optimized network instance EC2, with the letter n in the name.  I found that the loss of computational power when using n-instances is more than offset by an increase in download speed.  For most tasks, I used c5n.4xl. <br></li><li>  Changed <code>gzip</code> to <a href="https://linux.die.net/man/1/pigz"><code>pigz</code></a> , this is a gzip tool that can do cool stuff for parallelization of the initially not parallelized task of unpacking files (this helped the least). <br></li></ol><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Let S3 use as many threads as it wants aws configure set default.s3.max_concurrent_requests 50 for chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do aws s3 cp s3://$batch_loc$chunk_file - | pigz -dc | parallel --block 100M --pipe \ "awk -F '\t' '{print \$1\",...\"$30\"&gt;\"chunked/{#}_chr\"\$15\".csv\"}'" # Combine all the parallel process chunks to single files ls chunked/ | cut -d '_' -f 2 | sort -u | parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}' # Clean up intermediate data rm chunked/* done</span></span></code> </pre> <br>  These steps are combined with each other so that everything works very quickly.  Due to the increase in download speed and the rejection of writing to disk, I could now process a 5-terabyte package in just a few hours. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-2" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1129416944233226240"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  This tweet should have mentioned 'TSV'.  Alas. <br><br><h2>  Using re-parsed data </h2><br>  <b>What I learned</b> : Spark loves uncompressed data and does not like to combine partitions. <br><br>  Now the data was in S3 in an unpacked (read, shared) and semi-ordered format, and I could return to Spark again.  A surprise was waiting for me: I again failed to achieve the desired!  It was very difficult to tell Spark exactly how the data is partized.  And even when I did it, it turned out that there were too many partitions (95 thousand), and when I reduced the number to reasonable limits with the help of <code>coalesce</code> , it ruined my partitioning.  I am sure this can be fixed, but in a couple of days of searching I could not find a solution.  In the end, I completed all the tasks in Spark, although it took some time, and my separated Parquet files were not very small (~ 200 Kb).  However, the data lay where needed. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ae5/43b/236/ae543b236b8d37d4a6794aa63d9ada94.png"><br>  <i>Too small and uneven, wonderful!</i> <br><br><h2>  Testing local Spark requests </h2><br>  <b>What I learned</b> : Spark has too much overhead in solving simple problems. <br><br>  Having downloaded the data in a thoughtful format, I was able to test the speed.  I set up a script for R to start a local Spark server, and then I loaded the Spark data frame from the specified Parquet group repository (bin).  I tried to load all the data, but could not get Sparklyr to recognize the partitioning. <br><br><pre> <code class="scala hljs">sc &lt;- <span class="hljs-type"><span class="hljs-type">Spark_connect</span></span>(master = <span class="hljs-string"><span class="hljs-string">"local"</span></span>) desired_snp &lt;- <span class="hljs-symbol"><span class="hljs-symbol">'rs3477173</span></span>9' # <span class="hljs-type"><span class="hljs-type">Start</span></span> a timer start_time &lt;- <span class="hljs-type"><span class="hljs-type">Sys</span></span>.time() # <span class="hljs-type"><span class="hljs-type">Load</span></span> the desired bin into <span class="hljs-type"><span class="hljs-type">Spark</span></span> intensity_data &lt;- sc %&gt;% <span class="hljs-type"><span class="hljs-type">Spark_read_Parquet</span></span>( name = <span class="hljs-symbol"><span class="hljs-symbol">'intensity_dat</span></span>a', path = get_snp_location(desired_snp), memory = <span class="hljs-type"><span class="hljs-type">FALSE</span></span> ) # <span class="hljs-type"><span class="hljs-type">Subset</span></span> bin to snp and then collect to local test_subset &lt;- intensity_data %&gt;% filter(<span class="hljs-type"><span class="hljs-type">SNP_Name</span></span> == desired_snp) %&gt;% collect() print(<span class="hljs-type"><span class="hljs-type">Sys</span></span>.time() - start_time)</code> </pre> <br>  The execution took 29.415 seconds.  Much better, but not too good for mass testing anything.  In addition, I could not speed up work using caching, because when I tried to cache a data frame in memory, Spark always fell, even when I allocated more than 50 GB of memory for a dataset that weighed less than 15. <br><br><h2>  Return to AWK </h2><br>  <b>What I learned</b> : associative arrays in AWK are very effective. <br><br>  I realized that I can achieve a higher speed.  I remembered that in the awesome <a href="http://www.grymoire.com/Unix/Awk.html">AWK tutorial of Bruce Barnett</a> I read about a cool feature called ‚Äú <a href="http://www.grymoire.com/Unix/Awk.html">associative arrays</a> ‚Äù.  In fact, these are key-value pairs, which for some reason were named differently in AWK, and therefore I somehow did not even mention them.  <a href="https://ro-che.info/">Roman Chepliak</a> recalled that the term ‚Äúassociative arrays‚Äù is much older than the term ‚Äúkey-value pair‚Äù.  Even if you <a href="https://books.google.com/ngrams/graph%3Fcontent%3Dkey-value%2Bstore%252Cassociative%2Barray%26year_start%3D1800%26year_end%3D2000%26corpus%3D15%26smoothing%3D3%26share%3D%26direct_url%3Dt1%253B%252Cassociative%2520array%253B%252Cc0">search for a key value in Google Ngram</a> , you will not see this term there, but you will find associative arrays!  In addition, the "key-value pair" is most often associated with databases, so it is much more logical to compare with hashmap.  I realized that I can use these associative arrays to associate my SNPs with a group table (bin table) and raw data without Spark. <br><br>  For this, in the AWK script I used the <code>BEGIN</code> block.  This is a piece of code that is executed before the first data line is transferred to the main body of the script. <br><br><pre> <code class="cpp hljs">join_data.awk BEGIN { FS=<span class="hljs-string"><span class="hljs-string">","</span></span>; batch_num=substr(chunk,<span class="hljs-number"><span class="hljs-number">7</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>); chunk_id=substr(chunk,<span class="hljs-number"><span class="hljs-number">15</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">while</span></span>(getline &lt; <span class="hljs-string"><span class="hljs-string">"snp_to_bin.csv"</span></span>) {bin[$<span class="hljs-number"><span class="hljs-number">1</span></span>] = $<span class="hljs-number"><span class="hljs-number">2</span></span>} } { print $<span class="hljs-number"><span class="hljs-number">0</span></span> &gt; <span class="hljs-string"><span class="hljs-string">"chunked/chr_"</span></span>chr<span class="hljs-string"><span class="hljs-string">"_bin_"</span></span>bin[$<span class="hljs-number"><span class="hljs-number">1</span></span>]<span class="hljs-string"><span class="hljs-string">"_"</span></span>batch_num<span class="hljs-string"><span class="hljs-string">"_"</span></span>chunk_id<span class="hljs-string"><span class="hljs-string">".csv"</span></span> }</code> </pre> <br>  The <code>while(getline...)</code> command loaded all the lines from the CSV group (bin), set the first column (SNP name) as the key for the associative array <code>bin</code> and the second value (group) as the value.  Then, in the <code>{</code> <code>}</code> block that is executed for all lines of the main file, each line is sent to the output file, which receives a unique name depending on its group (bin): <code>..._bin_"bin[$1]"_...</code> <br><br>  The <code>batch_num</code> and <code>chunk_id</code> corresponded to the data provided by the pipeline, which made it possible to avoid a race condition, and each execution thread running <code>parallel</code> wrote to its own unique file. <br><br>  Since I scattered all the raw data in folders on the chromosomes left over from my previous experiment with AWK, I could now write another Bash script to process on the chromosome at a time and give the S3 deeper partitioned data. <br><br><pre> <code class="bash hljs">DESIRED_CHR=<span class="hljs-string"><span class="hljs-string">'13'</span></span> <span class="hljs-comment"><span class="hljs-comment"># Download chromosome data from s3 and split into bins aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv' | parallel "echo 'reading {}'; aws s3 cp "$DATA_LOC"{} - | awk -v chr=\""$DESIRED_CHR"\" -v chunk=\"{}\" -f split_on_chr_bin.awk" # Combine all the parallel process chunks to single files and upload to rds using R ls chunked/ | cut -d '_' -f 4 | sort -u | parallel "echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds" rm chunked/*</span></span></code> </pre> <br>  The script has two <code>parallel</code> sections. <br><br>  In the first section, data is read from all files containing information on the desired chromosome, then this data is distributed into streams that spread the files into the corresponding groups (bin).  To avoid a race condition, when several streams are written to a single file, AWK sends the file names for writing data to different places, for example, <code>chr_10_bin_52_batch_2_aa.csv</code> .  As a result, a lot of small files are created on the disk (for this I used terabyte EBS volumes). <br><br>  The pipeline from the second section of <code>parallel</code> passes through the groups (bin) and combines their individual files into shared CSV with <code>cat</code> , and then sends them for export. <br><br><h2>  Broadcasting in R? </h2><br>  <b>What I learned</b> : you can access <code>stdin</code> and <code>stdout</code> from an R-script, and therefore use it in a pipeline. <br><br>  In the Bash script, you might notice this line: <code>...cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R...</code>  <code>...cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R...</code>  It translates all concatenated group files (bin) into the following R script.  <code>{}</code> is a special <code>parallel</code> technique that any data it sends to the specified stream inserts data directly into the command itself.  The <code>{#}</code> option provides a unique ID of the execution thread, and <code>{%}</code> is the slot number of the task (repeated, but never at the same time).  A list of all options can be found in the <a href="https://www.gnu.org/software/parallel/parallel_tutorial.html">documentation.</a> <br><br><pre> <code class="lisp hljs"><span class="hljs-meta"><span class="hljs-meta">#!/usr/bin/env Rscript library(readr) library(aws.s3) # Read first command line argument data_destination &lt;- commandArgs(trailingOnly = TRUE)[1] data_cols &lt;- list(SNP_Name = 'c', ...) s3saveRDS( read_csv( file("stdin"), col_names = names(data_cols), col_types = data_cols ), object = data_destination )</span></span></code> </pre> <br>  When the variable <code>file("stdin")</code> transferred to <code>readr::read_csv</code> , the data translated into the R script is loaded into the frame, which is then written to S3 directly in the form of a <code>.rds</code> file using <code>aws.s3</code> . <br><br>  RDS is something like the younger version of Parquet, with no frills column storage. <br><br>  After completing the Bash script, I received a bundle of <code>.rds</code> files in S3, which allowed me to use efficient compression and built-in types. <br><br>  Despite the use of brake R, everything worked very quickly.  Not surprisingly, the fragments on R that are responsible for reading and writing data are well optimized.  After testing on one medium-size chromosome, the task was completed on the C5n.4xl instance in about two hours. <br><br><h2>  S3 restrictions </h2><br>  <b>What I learned</b> : thanks to the smart implementation of the paths, S3 can process many files. <br><br>  I was worried if S3 would be able to process the many files transferred to it.  I could make the file names meaningful, but how will S3 search for them? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/841/0dc/c34/8410dcc34a563c683dd7602dc66d884a.png"><br> <i>  S3    ,        <code>/</code> . <a href="https://aws.amazon.com/s3/faqs/"> FAQ- S3.</a></i> <br><br> , S3            -      .  (bucket)   ,   ‚Äî    . <br><br>          Amazon, ,    ¬´-----¬ª  .    :       get-,       . ,      20 . bin-. ,   ,      (,      ,      ).          . <br><br><h2>    ? </h2><br>   :     ‚Äî     . <br><br>       : ¬´    ?¬ª      ( gzip CSV-   7  )      .     ,  R     Parquet ( Arrow)     Spark.       R,         ,         ,       . <br><br><h2>   </h2><br> <b>  </b> :     ,    . <br><br>       ,      . <br>     EC2  ,                 ( ,  Spark    ).  ,          ,    AWS-      10 . <br><br>      R      . <br><br>   S3 ,       . <br><br><pre> <code class="bash hljs">library(aws.s3) library(tidyverse) chr_sizes &lt;- get_bucket_df( bucket = <span class="hljs-string"><span class="hljs-string">'...'</span></span>, prefix = <span class="hljs-string"><span class="hljs-string">'...'</span></span>, max = Inf ) %&gt;% mutate(Size = as.numeric(Size)) %&gt;% filter(Size != 0) %&gt;% mutate( <span class="hljs-comment"><span class="hljs-comment"># Extract chromosome from the file name chr = str_extract(Key, 'chr.{1,4}\\.csv') %&gt;% str_remove_all('chr|\\.csv') ) %&gt;% group_by(chr) %&gt;% summarise(total_size = sum(Size)/1e+9) # Divide to get value in GB # A tibble: 27 x 2 chr total_size &lt;chr&gt; &lt;dbl&gt; 1 0 163. 2 1 967. 3 10 541. 4 11 611. 5 12 542. 6 13 364. 7 14 375. 8 15 372. 9 16 434. 10 17 443. # ‚Ä¶ with 17 more rows</span></span></code> </pre> <br>    ,    ,   ,     <code>num_jobs</code>  ,       . <br><br><pre> <code class="bash hljs">num_jobs &lt;- 7 <span class="hljs-comment"><span class="hljs-comment"># How big would each job be if perfectly split? job_size &lt;- sum(chr_sizes$total_size)/7 shuffle_job &lt;- function(i){ chr_sizes %&gt;% sample_frac() %&gt;% mutate( cum_size = cumsum(total_size), job_num = ceiling(cum_size/job_size) ) %&gt;% group_by(job_num) %&gt;% summarise( job_chrs = paste(chr, collapse = ','), total_job_size = sum(total_size) ) %&gt;% mutate(sd = sd(total_job_size)) %&gt;% nest(-sd) } shuffle_job(1) # A tibble: 1 x 2 sd data &lt;dbl&gt; &lt;list&gt; 1 153. &lt;tibble [7 √ó 3]&gt;</span></span></code> </pre> <br>      purrr     . <br><br><pre> <code class="bash hljs">1:1000 %&gt;% map_df(shuffle_job) %&gt;% filter(sd == min(sd)) %&gt;% pull(data) %&gt;% pluck(1)</code> </pre> <br>     ,    .       Bash-    <code>for</code> .       10 .    ,             .  ,        . <br><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> DESIRED_CHR <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-string"><span class="hljs-string">"16"</span></span> <span class="hljs-string"><span class="hljs-string">"9"</span></span> <span class="hljs-string"><span class="hljs-string">"7"</span></span> <span class="hljs-string"><span class="hljs-string">"21"</span></span> <span class="hljs-string"><span class="hljs-string">"MT"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-comment"><span class="hljs-comment"># Code for processing a single chromosome fi</span></span></code> </pre> <br>     : <br><br><pre> <code class="bash hljs">sudo shutdown -h now</code> </pre> <br> ‚Ä¶   !   AWS CLI       <code>user_data</code>   Bash-    .     ,         . <br><br><pre> <code class="bash hljs">aws ec2 run-instances ...\ --tag-specifications <span class="hljs-string"><span class="hljs-string">"ResourceType=instance,Tags=[{Key=Name,Value=&lt;&lt;job_name&gt;&gt;}]"</span></span> \ --user-data file://&lt;&lt;job_script_loc&gt;&gt;</code> </pre> <br><h1> ! </h1><br> <b>  </b> : API        . <br><br> -        .      ,     .     API   .        <code>.rds</code>  Parquet-,       ,    .       R-. <br><br>      ,        ,    <code>get_snp</code> .       <a href="https://pkgdown.r-lib.org/">pkgdown</a> ,        . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a75/afb/f3a/a75afbf3a2c7c8ef5fa2a873f8ba50b9.png"><br><br><h2>   </h2><br> <b>  </b> :     ,   ! <br><br>          SNP      ,     (binning)   .     SNP,          (bin).      ( )    . <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Part of get_snp() ... # Test if our current snp data has the desired snp. already_have_snp &lt;- desired_snp %in% prev_snp_results$snps_in_bin if(!already_have_snp){ # Grab info on the bin of the desired snp snp_results &lt;- get_snp_bin(desired_snp) # Download the snp's bin data snp_results$bin_data &lt;- aws.s3::s3readRDS(object = snp_results$data_loc) } else { # The previous snp data contained the right bin so just use it snp_results &lt;- prev_snp_results } ...</span></span></code> </pre> <br>       ,       .    ,      . , <code>dplyr::filter</code>           ,           ,    . <br><br>  ,   <code>prev_snp_results</code>   <code>snps_in_bin</code> .     SNP   (bin),   ,       .        SNP   (bin)    : <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Get bin-mates snps_in_bin &lt;- my_snp_results$snps_in_bin for(current_snp in snps_in_bin){ my_snp_results &lt;- get_snp(current_snp, my_snp_results) # Do something with results }</span></span></code> </pre> <br><h1>  results </h1><br>    (  )    ,   .  ,           .      . <br><br>       ,       ,     ,     ‚Ä¶ <br><br>   .       .       (  ),  ,   (bin)   ,    SNP     0,1 ,     ,     S3 . <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-3" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1134151057385369600"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br><h2>  Conclusion </h2><br>   ‚Äî   .   ,     . ,    . ,   ,         ,     .  ,       ,  ,        ,    .  ,       ,    ,        ,      -     . <br><br>     .     ,        ,  ¬´¬ª  ,    .          . <br><br><h3>   : </h3><br><ul><li>      25   ; <br></li><li>      Parquet-   ; <br></li><li>   Spark   ; <br></li><li>      2,5  ; <br></li><li>    ,    Spark; <br></li><li>      ; <br></li><li>   Spark  ,      ; <br></li><li>  ,    ,  -       1980-; <br></li><li> <code>gnu parallel</code> ‚Äî   ,    ; <br></li><li> Spark        ; <br></li><li>  Spark        ; <br></li><li>    AWK  ; <br></li><li>    <code>stdin</code>  <code>stdout</code>  R-,       ; <br></li><li>     S3    ; <br></li><li>     ‚Äî     ; <br></li><li>     ,    ; <br></li><li> API        ; <br></li><li>     ,   ! <br></li></ul></div><p>Source: <a href="https://habr.com/ru/post/456392/">https://habr.com/ru/post/456392/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../456380/index.html">Open Day at the Faculty of Programming in Netology</a></li>
<li><a href="../456382/index.html">Collaboration and automation in the frontend. What we have learned for 13 schools</a></li>
<li><a href="../456386/index.html">Open libraries for visualizing audio content</a></li>
<li><a href="../456388/index.html">Diagram of the development of diagnostic capabilities in PVS-Studio</a></li>
<li><a href="../45639/index.html">jsForms: update test system</a></li>
<li><a href="../4564/index.html">List of "excuses" to abandon your blog</a></li>
<li><a href="../45640/index.html">LirnikBand - Sveta. Song about the internet</a></li>
<li><a href="../456400/index.html">Why competing is better than cramming: our experience of gamification training</a></li>
<li><a href="../456402/index.html">Teeth of wisdom: pull-pull</a></li>
<li><a href="../456404/index.html">Looper - Plugin for Sketch</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>