<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Installation Kubernetes 1.8 Bare Metal</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Quite a lot of Kubernetes articles are written on the Internet, but most of them are based on kubeadm and minikube. Of course, it‚Äôs great that you can...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Installation Kubernetes 1.8 Bare Metal</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/kv/lv/kq/kvlvkqpmnhmzlng10ltmkrpullo.png">  Quite a lot of Kubernetes articles are written on the Internet, but most of them are based on kubeadm and minikube.  Of course, it‚Äôs great that you can easily deploy a cluster in a couple of clicks, but I would like to have more understanding of what Kubernetes consists of.  I will try to correct this situation with this guide. <br><a name="habracut"></a><br>  <b>Purpose:</b> Kubernetes cluster with authentication using SSL keys and tokens. <br>  <b>Given:</b> 2 virtuals on Centos 7 (if desired, the manual can be easily adapted to other distributions) <br><br>  c00test01 - master / minion - 10/10/12/01 <br>  c00test02 - minion - 10/10/12/102 <br>  <i>Note: installation was performed with firewalld and selinux disabled.</i> <br><br><h4>  1. Installing Etcd </h4><br><pre><code class="bash hljs">$ yum install etcd-3.1.9</code> </pre> <br>  Rule config 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <div class="spoiler">  <b class="spoiler_title">/etc/etcd/etcd.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">ETCD_NAME=c00test01 ETCD_DATA_DIR=/var/lib/etcd #[cluster] ETCD_INITIAL_ADVERTISE_PEER_URLS=https://10.10.12.101:2380 ETCD_INITIAL_CLUSTER=c00test01=https://10.10.12.101:2380 ETCD_INITIAL_CLUSTER_STATE=new ETCD_INITIAL_CLUSTER_TOKEN=etcd-k8-cluster ETCD_LISTEN_PEER_URLS=https://0.0.0.0:2380 ETCD_ADVERTISE_CLIENT_URLS=https://10.10.12.101:2379 ETCD_LISTEN_CLIENT_URLS=https://0.0.0.0:2379 #[proxy] ETCD_PROXY="off" #[security] ETCD_CA_FILE=/etc/etcd/certs/ca.crt ETCD_TRUSTED_CA_FILE=/etc/etcd/certs/ca.crt ETCD_CERT_FILE=/etc/etcd/certs/server.crt ETCD_KEY_FILE=/etc/etcd/certs/server.key ETCD_CLIENT_CERT_AUTH=true ETCD_PEER_CA_FILE=/etc/etcd/certs/ca.crt ETCD_PEER_TRUSTED_CA_FILE=/etc/etcd/certs/ca.crt ETCD_PEER_CERT_FILE=/etc/etcd/certs/peer.crt ETCD_PEER_KEY_FILE=/etc/etcd/certs/peer.key ETCD_PEER_CLIENT_CERT_AUTH=true</code> </pre></div></div><br>  Replace the direction of the ExecStart unit in the init-script: <br><pre> <code class="bash hljs">$ mkdir /usr/lib/systemd/system/etcd.service.d</code> </pre><br><div class="spoiler">  <b class="spoiler_title">/usr/lib/systemd/system/etcd.service.d/etcd.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Service] ExecStart= ExecStart=/usr/bin/etcd</code> </pre></div></div><br><pre> <code class="bash hljs">$ chmod -R 644 /usr/lib/systemd/system/etcd.service.d $ chown -R root:root /usr/lib/systemd/system/etcd.service.d $ systemctl daemon-reload</code> </pre><br>  Download easy-rsa: <br><br><pre> <code class="bash hljs">$ mkdir /tmp/easyrsa $ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /tmp/easyrsa $ curl -sSL -O https://storage.googleapis.com/kubernetes-release/easy-rsa/easy-rsa.tar.gz $ tar xzf easy-rsa.tar.gz $ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> easy-rsa-master/easyrsa3</code> </pre><br>  We generate self-signed certificates: <br><br><pre> <code class="bash hljs">$ ./easyrsa --batch init-pki <span class="hljs-comment"><span class="hljs-comment">#  $ ./easyrsa --batch --req-cn="10.10.12.101" build-ca nopass #  etcd $ ./easyrsa --batch --subject-alt-name="IP:10.10.12.101 DNS:c00test01" build-server-full server nopass #   peer- etcd $ ./easyrsa --batch --subject-alt-name "IP:10.10.12.101 DNS:c00test01" build-server-full peer nopass #   kube-apiserver  flannel $ ./easyrsa --batch build-client-full client nopass</span></span></code> </pre><br>  Copy to the etcd certificates directory: <br><br><pre> <code class="bash hljs">$ mkdir /etc/etcd/certs $ cp -p pki/ca.crt /etc/etcd/certs/ca.crt $ cp -p pki/issued/* /etc/etcd/certs/ $ cp -p pki/private/* /etc/etcd/certs/ $ chmod ‚ÄìR 440 /etc/etcd/certs/ <span class="hljs-comment"><span class="hljs-comment">#  $ rm -rf /tmp/easyrsa/pki/</span></span></code> </pre><br>  We start etcd: <br><br><pre> <code class="bash hljs">$ systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> etcd &amp;&amp; systemctl start etcd</code> </pre><br>  Load the network config in etcd: <br><br><div class="spoiler">  <b class="spoiler_title">/tmp/flannel-conf.json</b> <div class="spoiler_text"><pre> <code class="json hljs">{ <span class="hljs-attr"><span class="hljs-attr">"Network"</span></span>: <span class="hljs-string"><span class="hljs-string">"172.96.0.0/12"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"SubnetLen"</span></span>: <span class="hljs-number"><span class="hljs-number">24</span></span>, <span class="hljs-attr"><span class="hljs-attr">"Backend"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"Type"</span></span>: <span class="hljs-string"><span class="hljs-string">"vxlan"</span></span> } }</code> </pre></div></div><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># 'cluster.lan'       $ /usr/bin/etcdctl --cert-file=/etc/flanneld/certs/client.crt --key-file=/etc/flanneld/certs/client.key --ca-file=/etc/flanneld/certs/ca.crt --no-sync --peers=https://10.10.12.101:2379 set /cluster.lan/network/config &lt; /tmp/flannel-conf.json</span></span></code> </pre><br><h4>  2. Install the flannel </h4><br><pre> <code class="bash hljs">$ yum install flannel-0.7.1</code> </pre><br>  Rule the config: <br><br><div class="spoiler">  <b class="spoiler_title">/ etc / sysconfig / flanneld</b> <div class="spoiler_text"><pre> <code class="plaintext hljs"># 'cluster.lan'       FLANNEL_ETCD="https://c00test01:2379" FLANNEL_ETCD_ENDPOINTS="https://c00test01:2379" FLANNEL_ETCD_KEY="/cluster.lan/network" FLANNEL_ETCD_PREFIX="/cluster.lan/network" FLANNEL_ETCD_CAFILE="/etc/flanneld/certs/ca.crt" FLANNEL_ETCD_CERTFILE="/etc/flanneld/certs/client.crt" FLANNEL_ETCD_KEYFILE="/etc/flanneld/certs/client.key" FLANNEL_OPTIONS="-etcd-cafile /etc/flanneld/certs/ca.crt -etcd-certfile /etc/flanneld/certs/client.crt -etcd-keyfile /etc/flanneld/certs/client.key"</code> </pre></div></div><br>  Copy certificates: <br><br><pre> <code class="bash hljs">$ mkdir ‚ÄìR /etc/flanneld/certs $ cp /etc/etcd/certs/ca.pem /etc/flanneld/certs/ca.crt $ cp /etc/etcd/certs/client.crt /etc/flanneld/certs/client.crt $ cp /etc/etcd/certs/client.key /etc/flanneld/certs/client.key $ chmod ‚ÄìR 440 /etc/flanneld/certs/</code> </pre><br>  We start the flannel: <br><br><pre> <code class="bash hljs">$ systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> flanneld &amp;&amp; systemctl start flannel</code> </pre><br><h4>  3. Install and run Docker </h4><br><pre> <code class="bash hljs">$ yum install docker-1.12.6 $ systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> docker &amp;&amp; systemctl start docker</code> </pre><br>  We proceed to install / configure Kubernetes itself. <br><br><h4>  4. Install apiserver </h4><br>  Install ready-to-use RPMs <a href="https://kojipkgs.fedoraproject.org/packages/kubernetes/1.8.1/1.fc28/x86_64/">from here</a> : <br><br><pre> <code class="bash hljs">$ mkdir /tmp/k8s $ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /tmp/k8s $ rpms=(kubernetes-master kubernetes-client kubernetes-node) $ <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-variable"><span class="hljs-variable">${rpms[*]}</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> wget https://kojipkgs.fedoraproject.org/packages/kubernetes/1.8.1/1.fc28/x86_64/<span class="hljs-variable"><span class="hljs-variable">${i}</span></span>-1.8.1-1.fc28.x86_64.rpm; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span> $ yum install kubernetes-master-1.8.1-1.fc28.x86_64.rpm kubernetes-client-1.8.1-1.fc28.x86_64.rpm kubernetes-node-1.8.1-1.fc28.x86_64.rpm</code> </pre><br>  We generate certificates: <br><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /tmp/easyrsa $ ./easyrsa --batch init-pki <span class="hljs-comment"><span class="hljs-comment">#  $ ./easyrsa --batch --req-cn="10.10.12.101" build-ca nopass #  apiserver-.  alt-names     ip  dns  - $ ./easyrsa --batch --subject-alt-name="IP:172.30.0.1 DNS:kubernetes DNS:kubernetes.default DNS:kubernetes.default.svc DNS:kubernetes.default.svc.cluster.lan IP:10.10.12.101 DNS:c00test01" build-server-full server nopass #  kubelet.       ,  alt-names   hostname ,    #       'kubectl log', 'kubectl exec'    'certificate signed by unknown authority' $ ./easyrsa --batch --subject-alt-name "DNS:c00test01 DNS:c00test02" build-server-full apiserver-kubelet-client nopass #  kubelet  kubectl $ ./easyrsa --batch build-client-full kubelet nopass $ ./easyrsa --batch build-client-full kubectl nopass</span></span></code> </pre><br>  Copy them to the kubernetes directory: <br><br><pre> <code class="bash hljs">$ cp -p pki/ca.crt /etc/kubernetes/certs/ca.crt $ cp -p pki/issued/* /etc/kubernetes/certs/ $ cp -p pki/private/* /etc/kubernetes/certs/ $ chown ‚ÄìR kube:kube /etc/kubernetes/certs/ $ chmod ‚ÄìR 440 /etc/kubernetes/certs/ <span class="hljs-comment"><span class="hljs-comment">#  $ rm -rf /tmp/easyrsa/pki/</span></span></code> </pre><br>  Copy the etcd certificates to the kubernetes directory: <br><br><pre> <code class="bash hljs">$ mkdir /etc/kubernetes/certs/etcd $ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /etc/etcd/certs $ cp ca.crt /etc/kubecnetes/certs/etcd/ca.crt $ cp client.crt /etc/kubecnetes/certs/etcd/client.crt $ cp client.key /etc/kubecnetes/certs/etcd/client.key</code> </pre><br>  Remove / etc / kubernetes / config as all connection settings will be specified in * .kubeconfig files <br><br><pre> <code class="bash hljs">rm /etc/kubernetes/config</code> </pre><br>  Rule the config: <br><br><div class="spoiler">  <b class="spoiler_title">/ etc / kubernetes / apiserver</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">KUBE_API_ADDRESS="--bind-address=0.0.0.0" KUBE_API_PORT="--secure-port=6443" # KUBELET_PORT="--kubelet-port=10250" KUBE_ETCD_SERVERS="--etcd-servers=https://c00test01:2379 --etcd-cafile=/etc/kubernetes/certs/etcd/ca.crt --etcd-certfile=/etc/kubernetes/certs/etcd/client.crt --etcd-keyfile=/etc/kubernetes/certs/etcd/client.key" KUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=172.30.0.0/16" KUBE_ADMISSION_CONTROL="--admission-control=Initializers,NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota" KUBE_API_ARGS="--tls-cert-file=/etc/kubernetes/certs/server.crt \ --tls-private-key-file=/etc/kubernetes/certs/server.key \ --tls-ca-file=/etc/kubernetes/certs/ca.crt \ --client-ca-file=/etc/kubernetes/certs/ca.crt \ --kubelet-certificate-authority=/etc/kubernetes/certs/ca.crt \ --kubelet-client-certificate=/etc/kubernetes/certs/apiserver-kubelet-client.crt \ --kubelet-client-key=/etc/kubernetes/certs/apiserver-kubelet-client.key \ --token-auth-file=/etc/kubernetes/tokens/known_tokens.csv \ --service-account-key-file=/etc/kubernetes/certs/server.crt \ --bind-address=0.0.0.0 \ --insecure-port=0 \ --apiserver-count=1 \ --basic-auth-file=/etc/kubernetes/certs/basic.cnf \ --anonymous-auth=false \ --allow-privileged=true"</code> </pre></div></div><br>  Create a directory for tokens and an empty file in it: <br><br><pre> <code class="bash hljs">$ mkdir /etc/kubernetes/tokens $ touch /etc/kubernetes/tokens/known_tokens.csv</code> </pre><br>  Allow apiserver to bind privileged ports: <br><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">setcap</span></span> cap_net_bind_service=ep /usr/bin/kube-apiserver</code> </pre><br>  Create a file for basic auth: <br><br><pre> <code class="bash hljs">$ touch /etc/kubernetes/certs/basic.cnf</code> </pre><br>  Filled by the rule: username, password, id <br>  id is an arbitrary unique number <br><br>  For example: <br><br><div class="spoiler">  <b class="spoiler_title">/etc/kubernetes/certs/basic.cnf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">admin,password,001 deploy,deploy,002</code> </pre></div></div><br>  You can do without basic auth, but sometimes you have to use it, as is the case with deploy to the cluster <a href="http://docs.ansible.com/ansible/latest/kubernetes_module.html">using ansible tools</a> . <br><br>  We generate tokens.  Please note that tokens will be the same for all kubelet and kube-proxy.  To generate different, it is enough to add - &lt;host_name&gt; to the account name.  (primer: system: kubelet-c00test02) <br><br><div class="spoiler">  <b class="spoiler_title">generate_tokes.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash accounts=(system:controller_manager system:scheduler system:kubectl system:dns system:kubelet system:proxy) for account in ${accounts[*]}; do token=$(dd if=/dev/urandom bs=128 count=1 2&gt;/dev/null | base64 | tr -d "=+/" | dd bs=32 count=1 2&gt;/dev/null) echo "${token},${account},${account}" &gt;&gt; "/etc/kubernetes/tokens/known_tokens.csv" echo "${token}" &gt; "/etc/kubernetes/tokens/${account}.token" done</span></span></code> </pre></div></div><br>  Start the api-server: <br><br><pre> <code class="bash hljs">$ systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> kube-apiserver &amp;&amp; systemctl start kube-apiserver</code> </pre><br>  Configure the controller-manager. <br><br>  Rule configs: <br><br><div class="spoiler">  <b class="spoiler_title">/ etc / kubernetes / controller-manager</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">KUBE_CONTROLLER_MANAGER_ARGS="--kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \ --service-account-private-key-file=/etc/kubernetes/certs/server.key \ --root-ca-file=/etc/kubernetes/certs/ca.crt "</code> </pre></div></div><br><div class="spoiler">  <b class="spoiler_title">/etc/kubernetes/controller-manager.kubeconfig</b> <div class="spoiler_text"><pre> <code class="plaintext hljs"># 'cluster.lan'       apiVersion: v1 kind: Config current-context: controller-manager-to-cluster.lan preferences: {} clusters: - cluster: certificate-authority: /etc/kubernetes/certs/ca.crt server: https://c00test01:6443 name: cluster.lan contexts: - context: cluster: cluster.lan user: controller-manager name: controller-manager-to-cluster.lan users: - name: controller-manager user: #  token   /etc/kubernetes/tokens/system:controller-manager.token token: cW6ha9WHzTK9Y4psT9pMKcUqfr673ydF</code> </pre></div></div><br>  Start the controller-manager: <br><br><pre> <code class="bash hljs">$ systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> kube-controller-manager &amp;&amp; systemctl start kube-controller-manager</code> </pre><br>  Configure kube-scheduler. <br><br>  Rule configs: <br><br><div class="spoiler">  <b class="spoiler_title">/ etc / kubernetes / scheduler</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">KUBE_SCHEDULER_ARGS="--kubeconfig=/etc/kubernetes/scheduler.kubeconfig"</code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">/etc/kubernetes/scheduler.kubeconfig</b> <div class="spoiler_text"><pre> <code class="plaintext hljs"># 'cluster.lan'       apiVersion: v1 kind: Config current-context: scheduler-to-cluster.lan preferences: {} clusters: - cluster: certificate-authority: /etc/kubernetes/certs/ca.crt server: https://c00test01:6443 name: cluster.lan contexts: - context: cluster: cluster.lan user: scheduler name: scheduler-to-cluster.lan users: - name: scheduler user: #  token   /etc/kubernetes/tokens/system:scheduler.token token: A2cU20Q9MkzdK8ON6UnVaP1nusWNKrWT</code> </pre></div></div><br>  Start kube-scheduler: <br><br><pre> <code class="bash hljs">$ systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> kube-scheduler &amp;&amp; systemctl start kube-scheduler</code> </pre><br>  Customize kubectl. <br><br>  Rule the config: <br><br><div class="spoiler">  <b class="spoiler_title">/etc/kubernetes/kubectl.kubeconfig</b> <div class="spoiler_text"><pre> <code class="plaintext hljs"># 'cluster.lan'       apiVersion: v1 kind: Config current-context: kubectl-to-cluster.lan preferences: {} clusters: - cluster: certificate-authority: /etc/kubernetes/certs/ca.crt server: https://c00test01:6443 name: cluster.lan contexts: - context: cluster: cluster.lan user: kubectl name: kubectl-to-cluster.lan users: - name: kubectl user: client-certificate: /etc/kubernetes/certs/kubectl.crt client-key: /etc/kubernetes/certs/kubectl.key</code> </pre></div></div><br>  For convenience, so that when calling kubectl, you do not specify the location before the config with the ‚Äìkubeconfig parameter, you can create a .kube directory in your user's hamster and copy the kubectl config there by renaming it to config <br><br>  For example: <br>  /home/user1/.kube/config <br><br>  Install kube-dns. <br><br><div class="spoiler">  <b class="spoiler_title">Deploy kube-dns replication controller</b> <div class="spoiler_text"><pre> <code class="bash hljs">$ cat &lt;&lt;EOF | kubectl create ‚Äìf ‚Äì apiVersion: v1 kind: ReplicationController metadata: name: kube-dns-v20 namespace: kube-system labels: k8s-app: kube-dns version: v20 kubernetes.io/cluster-service: <span class="hljs-string"><span class="hljs-string">"true"</span></span> spec: replicas: 1 selector: k8s-app: kube-dns version: v20 template: metadata: labels: k8s-app: kube-dns version: v20 spec: <span class="hljs-comment"><span class="hljs-comment">#   #       - affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - c00test01 #   containers: - name: kubedns image: gcr.io/google_containers/kubedns-amd64:1.8 resources: limits: memory: 170Mi requests: cpu: 100m memory: 70Mi livenessProbe: httpGet: path: /healthz-kubedns port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: httpGet: path: /readiness port: 8081 scheme: HTTP initialDelaySeconds: 3 timeoutSeconds: 5 args: # command = "/kube-dns" # 'cluster.lan'       - --domain=cluster.lan. - --dns-port=10053 ports: - containerPort: 10053 name: dns-local protocol: UDP - containerPort: 10053 name: dns-tcp-local protocol: TCP - name: dnsmasq image: gcr.io/google_containers/kube-dnsmasq-amd64:1.4 livenessProbe: httpGet: path: /healthz-dnsmasq port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 args: - --cache-size=1000 - --no-resolv - --server=127.0.0.1#10053 - --log-facility=- ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - name: healthz image: gcr.io/google_containers/exechealthz-amd64:1.2 resources: limits: memory: 50Mi requests: cpu: 10m memory: 50Mi args: # 'cluster.lan'       - --cmd=nslookup kubernetes.default.svc.cluster.lan 127.0.0.1 &gt;/dev/null - --url=/healthz-dnsmasq - --cmd=nslookup kubernetes.default.svc.cluster.lan 127.0.0.1:10053 &gt;/dev/null - --url=/healthz-kubedns - --port=8080 - --quiet ports: - containerPort: 8080 protocol: TCP dnsPolicy: Default tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - key: CriticalAddonsOnly operator: Exists EOF</span></span></code> </pre></div></div><br><div class="spoiler">  <b class="spoiler_title">Deploy kube-dns service</b> <div class="spoiler_text"><pre> <code class="bash hljs">$ cat &lt;&lt;EOF | kubectl create ‚Äìf ‚Äì apiVersion: v1 kind: Service metadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: <span class="hljs-string"><span class="hljs-string">"true"</span></span> kubernetes.io/name: <span class="hljs-string"><span class="hljs-string">"KubeDNS"</span></span> spec: selector: k8s-app: kube-dns <span class="hljs-comment"><span class="hljs-comment">#  10    , #     apiserver- '-‚Äìservice-cluster-ip-range' clusterIP: 172.30.0.10 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP EOF</span></span></code> </pre></div></div><br>  Installation wizard is complete.  However, looking at the status of pods, we will see that kube-dns is in pending status.  This is because as long as we have not set up minyens, in which Kubernetes will settle pods. <br><br>  For minyenov, you must perform steps 2 and 3 (install flannel and docker). <br><br><h4>  5. Installing and configuring kubelet </h4><br>  We already have everything installed on the c00test01 node, but we need to install packages on the c00test02 node: <br><br><pre> <code class="bash hljs">$ mkdir /tmp/k8s $ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /tmp/k8s $ rpms=(kubernetes-client kubernetes-node);<span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-variable"><span class="hljs-variable">${rpms[*]}</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> wget https://kojipkgs.fedoraproject.org/packages/kubernetes/1.8.1/1.fc28/x86_64/<span class="hljs-variable"><span class="hljs-variable">${i}</span></span>-1.8.1-1.fc28.x86_64.rpm; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span> $ yum install kubernetes-client-1.8.1-1.fc28.x86_64.rpm kubernetes-node-1.8.1-1.fc28.x86_64.rpm</code> </pre><br>  You also need to copy the certificates and keys from the master node to the / etc / kubernetes / certs directory: <br>  kubelet.crt <br>  kubelet.key <br>  apiserver-kubelet-client.crt <br>  apiserver-kubelet-client.key <br><br>  Rule configs: <br><br><div class="spoiler">  <b class="spoiler_title">/ etc / kubernetes / kubelet</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">KUBELET_ADDRESS="--address=0.0.0.0" # KUBELET_PORT="--port=10250" KUBELET_HOSTNAME="--hostname-override=c00test02" KUBELET_ARGS="--register-node=true \ --tls-cert-file=/etc/kubernetes/certs/apiserver-kubelet-client.crt \ --tls-private-key-file=/etc/kubernetes/certs/apiserver-kubelet-client.key \ --require-kubeconfig=true \ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \ --pod-manifest-path=/etc/kubernetes/manifests \ --cgroup-driver=systemd \ --allow-privileged=true \ --cluster-domain=cluster.lan \ --authorization-mode=Webhook \ --fail-swap-on=false \ --cluster-dns=172.30.0.10"</code> </pre></div></div><br>  Take ca.crt from the wizard: <br><br><pre> <code class="bash hljs">$ base64 /etc/kubernetes/certs/ca.crt</code> </pre><br><div class="spoiler">  <b class="spoiler_title">/etc/kubernetes/kubelet.kubeconfig</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: v1 kind: Config current-context: kubelet-to-cluster.lan # change 'cluster.lan to your cluster name' preferences: {} clusters: - cluster: certificate-authority-data: &lt;_ca.crt__base64&gt; server: https://c00test01:6443 name: cluster.lan # change 'cluster.lan to your cluster name' contexts: - context: cluster: cluster.lan # change 'cluster.lan to your cluster name' user: kubelet name: kubelet-to-cluster.lan # change 'cluster.lan to your cluster name' users: - name: kubelet user: client-certificate: /etc/kubernetes/certs/kubelet.crt client-key: /etc/kubernetes/certs/kubelet.key</code> </pre></div></div><br><div class="spoiler">  <b class="spoiler_title">/ etc / kubernetes / proxy</b> <div class="spoiler_text">  &lt;source lang = "basic&gt; <br>  KUBE_PROXY_ARGS = "- kubeconfig = / etc / kubernetes / proxy.kubeconfig --cluster-cidr = 172.30.0.0 / 16" <br></div></div><br><div class="spoiler">  <b class="spoiler_title">/etc/kubernetes/proxy.kubeconfig</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: v1 kind: Config current-context: proxy-to-cluster.lan # change 'cluster.lan to your cluster name' preferences: {} contexts: - context: cluster: cluster.lan # change 'cluster.lan to your cluster name' user: proxy name: proxy-to-cluster.lan # change 'cluster.lan to your cluster name' clusters: - cluster: certificate-authority-data: &lt;_ca.crt__base64&gt; server: https://c00test01:6443 name: cluster.lan # change 'cluster.lan to your cluster name' users: - name: proxy user: client-certificate: /etc/kubernetes/certs/kubelet.crt client-key: /etc/kubernetes/certs/kubelet.key</code> </pre></div></div><br>  We start kubelet and kube-proxy: <br><br><pre> <code class="bash hljs">$ systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> kubelet &amp;&amp; systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> kube-proxy $ systemctl start kubelet &amp;&amp; systemctl start kube-proxy</code> </pre><br>  Installation is complete, you can check the status of nodes and pod-kube-dns. <br><br><pre> <code class="bash hljs">$ kubectl get nodes NAME STATUS ROLES AGE VERSION c00test01 Ready master 5m v1.8.1 c00test02 Ready &lt;none&gt; 4m v1.8.1</code> </pre><br><pre> <code class="bash hljs">$ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system kube-dns-v20-2jqsj 3/3 Running 0 3m</code> </pre><br>  The following materials were used to write this guide: <br><br>  ‚Üí <a href="https://kubernetes.io/docs/home/">Official documentation</a> <br>  ‚Üí <a href="https://github.com/kubernetes/contrib/tree/master/ansible">Kubernetes Ansible</a> </div><p>Source: <a href="https://habr.com/ru/post/342232/">https://habr.com/ru/post/342232/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../342222/index.html">The digest of fresh materials from the world of the frontend for the last week ‚Ññ288 (November 6 - 12, 2017)</a></li>
<li><a href="../342224/index.html">Hackquest 2017. Results & Writeups</a></li>
<li><a href="../342226/index.html">New features for monitoring Java applications in Zabbix 3.4</a></li>
<li><a href="../342228/index.html">The latest Windows Server version 1709 is available in the Azure Pack Infrastructure cloud from InfoboxCloud</a></li>
<li><a href="../342230/index.html">RedmineUP - platform for product teams</a></li>
<li><a href="../342234/index.html">Top 10 Test Automation Tools 2018</a></li>
<li><a href="../342236/index.html">Origin of accounting objects</a></li>
<li><a href="../342238/index.html">Russia ranked first at the World Robot Olympiad</a></li>
<li><a href="../342240/index.html">Welcome to iOS-meetup SuperJob</a></li>
<li><a href="../342242/index.html">How to move to work in the United States: a selection of useful materials</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>