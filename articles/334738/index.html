<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Machine learning: from Iris to Telecom</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Mobile operators, providing a variety of services, accumulate a huge amount of statistical data. I represent the department that implements the subscr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Machine learning: from Iris to Telecom</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/web/506/82b/111/50682b11102b439486894fa7bfd3c37a.jpg"><br><br>  Mobile operators, providing a variety of services, accumulate a huge amount of statistical data.  I represent the department that implements <a href="http://sushkov.ru/articles/Article_pcrf.htm">the subscriber traffic management system</a> , which, during the operation of the operator, generates hundreds of gigabytes of statistical information per day.  I was interested in the question: how in these Big Data (Big Data) to reveal the maximum of useful information?  No wonder that one of the V in the definition of Big Data is an additional income. <br><br>  I took on this task, not being an expert in data research.  Immediately a lot of questions arose: what technical means to use for analysis?  At what level is it enough to know mathematics, statistics?  What methods of machine learning need to know and how deep?  Is it better to start with a specialized language for studying R or Python data? 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      As my experience has shown, for the initial level of data research, it‚Äôs not at all necessary.  But for a quick dive I didn‚Äôt have enough of a simple example that clearly showed the full algorithm for researching data.  In this article, using the example of <a href="https://ru.wikipedia.org/wiki/%25D0%2598%25D1%2580%25D0%25B8%25D1%2581%25D1%258B_%25D0%25A4%25D0%25B8%25D1%2588%25D0%25B5%25D1%2580%25D0%25B0">Iris Fisher,</a> we will go all the way through the initial training, and then apply the understanding obtained to the real data of the telecommunications operator.  Readers who are already familiar with data mining can skip straight to a chapter on Telecom. <br><a name="habracut"></a><br><h2>  Terms </h2><br>  To begin, let's understand the subject of study.  Now the terms Artificial Intelligence, Machine Learning, and Deep Machine Learning are often used interchangeably, but in fact there is a well-defined hierarchy: <br><br><img src="https://habrastorage.org/web/bad/417/856/bad4178569834da1a6f09fc3e9e5c4f2.png"><br><br><ul><li>  Artificial Intelligence includes all tasks in which machines perform intellectual tasks, such as playing checkers or chess, assistants capable of recognizing speech and giving answers to questions, various robots. </li><li>  Machine Learning is a narrower concept and belongs to the class of tasks for which the computer is trained to perform certain actions, having previously known correct answers, for example, classifying objects according to a set of features or recommending music and movies. </li><li>  Under the Deep Learning implies tasks that are solved using neural networks and Big Data, such as pattern recognition or text translation. </li></ul><br>  In the article we will talk about Machine Learning.  There are two ways to learn: <br><br><ul><li>  with teacher </li><li>  without teacher </li></ul><br>  With a teacher is when we have data with the correct answers.  Then the algorithm can be trained on this data set, and then apply it to the prediction.  Such algorithms include classification and regression.  Classification is the assignment of objects to a particular class by a set of attributes.  For example, recognition of license plates, or in medicine, diagnosis of diseases, or credit scoring in the banking sector.  Regression is the prediction of a real variable, such as stock prices. <br><br>  Without a teacher (self-study) is the search for hidden patterns in the data.  Such algorithms include clustering.  For example, all large retail chains are looking for patterns in their customers' purchases and are trying to work with target groups of customers, rather than with the general mass. <br><br>  Regression, classification and clustering are the main data research algorithms, and therefore we will consider them. <br><br><h2>  Research data </h2><br>  The data mining algorithm consists of a specific sequence of steps.  Depending on the task and available data, the set of steps may vary, but the general direction is always determined: <br><br><ul><li>  Collection and cleaning of data.  As practice shows, this stage can take up to 90% of the time of the entire data analysis; </li><li>  Visual analysis of data, their distribution, statistics; </li><li>  Analysis of dependence (correlation) between variables (features); </li><li>  Selection and identification of features that will be used to build models; </li><li>  Separation of data for training models and test; </li><li>  Building models on data for training / assessment of results on test data; </li><li>  Interpretation of the resulting model, visualization of results. </li></ul><br>  With the algorithm figured out, and what tools to use for analysis?  There are lots of tools, from Excel to specialized tools, for example, MathLab.  We will take Python co specialized libraries.  Do not be afraid of difficulties, everything is simple: <br><br><ul><li>  Download Python and all math packs in one distribution called <a href="https://www.continuum.io/downloads">Anaconda</a> </li><li>  Installation under Linux will not cause problems: <b>bash Anaconda2-4.4.0-Linux-x86_64.sh</b> </li></ul><br><ul><li>  Run: <b>jupyter notebook</b> </li></ul><br><ul><li>  This automatically opens the browser: </li></ul><br><img src="https://habrastorage.org/web/6cb/f5b/790/6cbf5b790a4344bfb25c9259b6aa11e5.png"><br><ul><li>  Check that the application is running: <b>print ‚ÄúHelloWorld!‚Äù</b> </li></ul><br><ul><li>  Press Ctrl + Enter, see that everything is ok. </li></ul><br>  For self-study work in IPython Notebook on the Internet there is a lot of information, for example, a simple introduction: <a href="https://habrahabr.ru/post/218869/">Review Ipython Notebook 2.0</a> . <br><br>  And we begin our study! <br><br><h3>  Collecting and cleaning data </h3><br>  In the example of Irises for us all the data collected and filled.  Just load them and see: <br><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  : import numpy as np import pandas as pd from sklearn import datasets from sklearn import linear_model from sklearn.cluster import KMeans from sklearn import cross_validation from sklearn import metrics from pandas import DataFrame %pylab inline</span></span></code> </pre> <br>  Further: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    : iris = datasets.load_iris() #     print iris.feature_names #   ,  10  : print iris.data[:10] #    : print iris.target_names print iris.target</span></span></code> </pre> <br><img src="https://habrastorage.org/web/ba0/c03/9e7/ba0c039e76b4410babae401bd3c697fd.png"><br><br>  We see that the data set consists of the length / width of two types of Iris petals: sepal and petal.  Do not ask me where they are at Iris.  The target variable is an Iris variety: 0 - Setosa, 1 - Versicolor, 2 - Virginica.  Accordingly, our task is to try to find the dependencies between the size of the petals and the Iris varieties. <br><br>  For the convenience of data manipulation we make DataFrame from them: <br><br><pre> <code class="python hljs">iris_frame = DataFrame(iris.data) <span class="hljs-comment"><span class="hljs-comment">#     ,   : iris_frame.columns = iris.feature_names #     : iris_frame['target'] = iris.target #      : iris_frame['name'] = iris_frame.target.apply(lambda x : iris.target_names[x]) # ,  : iris_frame</span></span></code> </pre> <br>  It seemed to work out what they wanted: <br><br><img src="https://habrastorage.org/web/bb5/c73/107/bb5c7310793b4b7dbc26f873d65fa70f.png"><br><br><br><h3>  Descriptive statistics </h3><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     : pyplot.figure(figsize(20, 24)) plot_number = 0 for feature_name in iris['feature_names']: for target_name in iris['target_names']: plot_number += 1 pyplot.subplot(4, 3, plot_number) pyplot.hist(iris_frame[iris_frame.name == target_name][feature_name]) pyplot.title(target_name) pyplot.xlabel('cm') pyplot.ylabel(feature_name[:-4])</span></span></code> </pre> <br><img src="https://habrastorage.org/web/e94/427/a56/e94427a566634b0c878aa77d3e06b7d5.png"><br><br>  Looking at these histograms, an experienced researcher can immediately draw the first conclusions.  I only see that the distribution of some variables seems to be normal.  Let's try to do more clearly.  We build a table with dependencies between signs and color the points depending on the Iris varieties: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns sns.pairplot(iris_frame[[<span class="hljs-string"><span class="hljs-string">'sepal length (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'sepal width (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'petal length (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'petal width (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'name'</span></span>]], hue = <span class="hljs-string"><span class="hljs-string">'name'</span></span>)</code> </pre> <br><img src="https://habrastorage.org/web/da2/c02/127/da2c02127c57435eb8d66a833b624a06.png"><br><br>  Here, even an inexperienced researcher can see that "petal width (cm)" and "petal length (cm)" have a strong relationship - the points are stretched along one line.  And in principle, according to the same features, a classification can be built, since  dots by color are grouped quite compactly.  But, for example, using the variables ‚Äúsepal width (cm)‚Äù and ‚Äúsepal length (cm)‚Äù, one cannot build a qualitative classification, since  points related to Versicolor and Virginica varieties are intermingled. <br><br><h3>  Dependence between variables </h3><br>  Now let's look at the mathematical values ‚Äã‚Äãof dependencies: <br><br><pre> <code class="python hljs">iris_frame[[<span class="hljs-string"><span class="hljs-string">'sepal length (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'sepal width (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'petal length (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'petal width (cm)'</span></span>]].corr()</code> </pre> <br><img src="https://habrastorage.org/web/742/4b2/19d/7424b219df1949429c317e0c55b4a33b.png"><br><br>  In a more visual form, we construct a heat map of the dependence of the signs: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns corr = iris_frame[[<span class="hljs-string"><span class="hljs-string">'sepal length (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'sepal width (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'petal length (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'petal width (cm)'</span></span>]].corr() mask = np.zeros_like(corr) mask[np.triu_indices_from(mask)] = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> sns.axes_style(<span class="hljs-string"><span class="hljs-string">"white"</span></span>): ax = sns.heatmap(corr, mask=mask, square=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, cbar=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, annot=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, linewidths=<span class="hljs-number"><span class="hljs-number">.5</span></span></code> </pre> <br><img src="https://habrastorage.org/web/f69/ca9/26e/f69ca926efe24b878b53bf7df51c5ef7.png"><br><br>  The values ‚Äã‚Äãof the correlation coefficient are interpreted as follows: <br><br><ul><li>  Up to 0.2 - very weak correlation </li><li>  To 0.5 - weak </li><li>  Up to 0.7 - average </li><li>  Up to 0.9 - high </li><li>  More than 0.9 - very high </li></ul><br>  Indeed, we see that between the variables ‚Äúpetal length (cm)‚Äù and ‚Äúpetal width (cm)‚Äù a very strong dependence of 0.96 has been revealed. <br><br><h3>  We select and create signs </h3><br>  In the first approximation, you can simply include all variables in the model and see what happens.  Then you can think about what signs to remove and which ones to create. <br><br><h3>  Training and test data </h3><br>  We divide data into data for training and test data.  Typically, the sample is divided into training and test in a percentage of 66/33, 70/30 or 80/20.  Other splits are possible depending on the data.  In our example, we assign 30% of the entire sample to the test data (parameter test_size = 0.3): <br><br><pre> <code class="python hljs">train_data, test_data, train_labels, test_labels = cross_validation.train_test_split(iris_frame[[<span class="hljs-string"><span class="hljs-string">'sepal length (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'sepal width (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'petal length (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'petal width (cm)'</span></span>]], iris_frame[<span class="hljs-string"><span class="hljs-string">'target'</span></span>], test_size = <span class="hljs-number"><span class="hljs-number">0.3</span></span>, random_state = <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-comment"><span class="hljs-comment">#  ,      : print train_data print test_data print train_labels print test_labels</span></span></code> </pre> <br><h3>  Model building cycle - result evaluation </h3><br>  We turn to the most interesting. <br><br><h4>  Linear Regression - LinearRegression </h4><br>  How to visualize linear regression?  If you look at the relationship between two variables, then this is the line holding so that the vertical distances from the line to the points are minimal in sum.  The most common optimization method is minimization of the mean-square error using the gradient descent algorithm.  The explanation of the gradient descent is much where, for example, in the section ‚ÄúWhat is a gradient descent?‚Äù.  But you can not read and perceive linear regression as an abstract algorithm for finding the line that most closely follows the direction of the distribution of objects.  We build a model using variables that, as we understood earlier, have a strong relationship - these are ‚Äúpetal length (cm)‚Äù and ‚Äúpetal width (cm)‚Äù: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> polyval, stats fit_output = stats.linregress(iris_frame[[<span class="hljs-string"><span class="hljs-string">'petal length (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'petal width (cm)'</span></span>]]) slope, intercept, r_value, p_value, slope_std_error = fit_output print(slope, intercept, r_value, p_value, slope_std_error)</code> </pre> <br>  We look at the quality metrics of the model: <br><br>  <b>(0.41641913228540123, -0.3665140452167277, 0.96275709705096657, 5.7766609884916033e-86, 0.009612539319328553)</b> <b><br></b>  Of the most interesting is the correlation coefficient between the variables r_value with a value of 0.96275709705096657.  We have already seen it before, and here we are once again convinced of its existence.  Draw a graph with points and a regression line: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt plt.plot(iris_frame[[<span class="hljs-string"><span class="hljs-string">'petal length (cm)'</span></span>]], iris_frame[[<span class="hljs-string"><span class="hljs-string">'petal width (cm)'</span></span>]],<span class="hljs-string"><span class="hljs-string">'o'</span></span>, label=<span class="hljs-string"><span class="hljs-string">'Data'</span></span>) plt.plot(iris_frame[[<span class="hljs-string"><span class="hljs-string">'petal length (cm)'</span></span>]], intercept + slope*iris_frame[[<span class="hljs-string"><span class="hljs-string">'petal length (cm)'</span></span>]], <span class="hljs-string"><span class="hljs-string">'r'</span></span>, linewidth=<span class="hljs-number"><span class="hljs-number">3</span></span>, label=<span class="hljs-string"><span class="hljs-string">'Linear regression line'</span></span>) plt.ylabel(<span class="hljs-string"><span class="hljs-string">'petal width (cm)'</span></span>) plt.xlabel(<span class="hljs-string"><span class="hljs-string">'petal length (cm)'</span></span>) plt.legend() plt.show()</code> </pre> <br><img src="https://habrastorage.org/web/de3/367/4d7/de33674d773c454187f42dc12ad4f8f7.png"><br><br>  We see that, indeed, the regression line found well repeats the direction of distribution of points.  Now, if we have available, for example, the length of the leaflet pental, we will be able to determine with great accuracy how wide it is! <br><br><h4>  Classification </h4><br>  How to intuitively present the classification?  If you look at the task of division into two classes of objects that have two signs (for example, you need to separate apples and bananas if their sizes are known), then the classification is reduced to drawing a line on a plane that divides objects into two classes.  If it is necessary to divide into a larger number of classes, then several lines are drawn.  If you look at objects with three variables, then three-dimensional space and the task of holding planes are represented.  If variables are N, then you just need to imagine a hyperplane in N-dimensional space). <br><br>  So, we take the most famous classification learning algorithm: Stochastic Gradient Descent.  With the gradient descent, we have already met in linear regression, and stochastic says that for the speed of work, not all sampling is used, but random data.  And apply it to the SVM (Support Vector Machine) classification method: <br><br><pre> <code class="python hljs">train_data, test_data, train_labels, test_labels = cross_validation.train_test_split(iris_frame[[<span class="hljs-string"><span class="hljs-string">'sepal length (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'sepal width (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'petal length (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'petal width (cm)'</span></span>]], iris_frame[[<span class="hljs-string"><span class="hljs-string">'target'</span></span>]], test_size = <span class="hljs-number"><span class="hljs-number">0.3</span></span>, random_state = <span class="hljs-number"><span class="hljs-number">0</span></span>) model = linear_model.SGDClassifier(alpha=<span class="hljs-number"><span class="hljs-number">0.001</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">100</span></span>, random_state = <span class="hljs-number"><span class="hljs-number">0</span></span>) model.fit(train_data, train_labels) model_predictions = model.predict(test_data) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> metrics.accuracy_score(test_labels, model_predictions) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> metrics.classification_report(test_labels, model_predictions)</code> </pre> <br>  We look at the quality metrics of the model: <br><br><img src="https://habrastorage.org/web/bd0/528/491/bd052849167d445f87872ab06a884b76.png"><br><br>  In fact, you can evaluate the model, not really understanding the essence of the values ‚Äã‚Äãof metrics: if accuracy, precision and recall is more than 0.85, then this is a good model, if more than 0.95, then it is excellent. <br><br>  In short, the metrics used in the example reflect the following: <br><br><ul><li>  accuracy is the main metric that shows the proportion of correct model responses.  Its value is equal to the ratio of the number of correct answers given by the model to the number of all objects.  But it does not fully reflect the quality of the model.  Therefore, precision and recall are introduced. </li></ul><br>  These metrics are given both in terms of the quality of recognition of each class (iris variety) and the total values.  We look at the total values: <br><br><ul><li>  precision (accuracy) - this metric shows how much we can trust the model, in other words, how many ‚Äúfalse positives‚Äù we have.  The value of the metric is equal to the ratio of the number of responses that the model considers correct, and they were indeed correct (this number is denoted by ‚Äútrue positives‚Äù) to the sum of ‚Äútrue positives‚Äù and the number of objects that the model considered correct, but in fact they were incorrect (this number denoted by "false positives").  In the form of the formula: precision = "true positives" / ("true positives" + "false positives") </li><li>  recall (completeness) - this metric shows how much a model can even detect the correct answers, in other words, how many ‚Äúfalse passes‚Äù we have.  Its numerical value is equal to the ratio of the answers that the model considers correct, and they were indeed correct to the number of all the correct answers in the sample.  In the form of the formula: recall = "true positives" / "all positives" </li><li>  f1-score (f-measure) is a combination of precision and recall </li><li>  support - just the number of objects found in the class </li></ul><br>  There are also important model metrics: PR-AUC and ROC-AUC, you can get acquainted with them, for example, here: <a href="https://habrahabr.ru/company/ods/blog/328372/">Metrics in machine learning tasks</a> . <br><br>  Thus, we see that the values ‚Äã‚Äãof the metrics in our example are very good.  Let's look at the schedule.  For clarity, the sample is drawn in two coordinates and color by class. <br><br>  First, let's display the test sample as it is: <br><br><img src="https://habrastorage.org/web/c5f/733/ee0/c5f733ee0e964f07979847aed4470401.png"><br><br>  Then, as our model predicted.  We see that the points on the border (which I circled in red) were classified incorrectly: <br><br><img src="https://habrastorage.org/web/30a/712/2d5/30a7122d53c5407cbb552266d13ad1d7.png"><br><br>  But most of the objects predicted correctly! <br><br><h4>  Cross-validation </h4><br>  Somehow a very suspiciously good result ... What could be wrong?  For example, we accidentally broke the data into a training and test sample.  To remove this chance, the so-called cross-validation is applied.  This is when the data is divided several times into a training and test sample, and the result of the algorithm is averaged. <br><br>  Let us check the operation of the algorithm on 10 random samples: <br><br><pre> <code class="python hljs">train_data, test_data, train_labels, test_labels = cross_validation.train_test_split(iris_frame[[<span class="hljs-string"><span class="hljs-string">'sepal length (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'sepal width (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'petal length (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'petal width (cm)'</span></span>]], iris_frame[<span class="hljs-string"><span class="hljs-string">'target'</span></span>], test_size = <span class="hljs-number"><span class="hljs-number">0.3</span></span>) model = linear_model.SGDClassifier(alpha=<span class="hljs-number"><span class="hljs-number">0.001</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">100</span></span>, random_state = <span class="hljs-number"><span class="hljs-number">0</span></span>) scores = cross_validation.cross_val_score(model, train_data, train_labels, cv=<span class="hljs-number"><span class="hljs-number">10</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> scores.mean()</code> </pre> <br>  We look at the result.  He expectedly deteriorated: <b>0.860909090909</b> <br><br><h4>  Selection of optimal algorithm parameters </h4><br>  What else can be done to optimize the algorithm?  You can try to find the parameters of the algorithm itself.  We see that alpha = 0.001, n_iter = 100 are passed to the algorithm.  Let's find the optimal values ‚Äã‚Äãfor them. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> grid_search train_data, test_data, train_labels, test_labels = cross_validation.train_test_split(iris_frame[[<span class="hljs-string"><span class="hljs-string">'sepal length (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'sepal width (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'petal length (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'petal width (cm)'</span></span>]], iris_frame[<span class="hljs-string"><span class="hljs-string">'target'</span></span>], test_size = <span class="hljs-number"><span class="hljs-number">0.3</span></span>) parameters_grid = { <span class="hljs-string"><span class="hljs-string">'n_iter'</span></span> : range(<span class="hljs-number"><span class="hljs-number">5</span></span>,<span class="hljs-number"><span class="hljs-number">100</span></span>), <span class="hljs-string"><span class="hljs-string">'alpha'</span></span> : np.linspace(<span class="hljs-number"><span class="hljs-number">0.0001</span></span>, <span class="hljs-number"><span class="hljs-number">0.001</span></span>, num = <span class="hljs-number"><span class="hljs-number">10</span></span>), } classifier = linear_model.SGDClassifier(random_state = <span class="hljs-number"><span class="hljs-number">0</span></span>) cv = cross_validation.StratifiedShuffleSplit(train_labels, n_iter = <span class="hljs-number"><span class="hljs-number">10</span></span>, test_size = <span class="hljs-number"><span class="hljs-number">0.3</span></span>, random_state = <span class="hljs-number"><span class="hljs-number">0</span></span>) grid_cv = grid_search.GridSearchCV(classifier, parameters_grid, scoring = <span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>, cv = cv)grid_cv.fit(train_data, train_labels) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> grid_cv.best_estimator_</code> </pre> <br>  At the output we get a model with optimal parameters: <br><br>  <b>SGDClassifier (alpha = 0.00089999999999999998, average = False, class_weight = None,</b> <b><br></b>  <b>epsilon = 0.1, eta0 = 0.0, fit_intercept = True, l1_ratio = 0.15,</b> <b><br></b>  <b>learning_rate = 'optimal', loss = 'hinge', n_iter = 96, n_jobs = 1,</b> <b><br></b>  <b>penalty = 'l2', power_t = 0.5, random_state = 0, shuffle = True, verbose = 0,</b> <b><br></b>  <b>warm_start = False)</b> <b><br></b> <br>  We see that in it alpha = 0.0009, n_iter = 96.  We substitute these values ‚Äã‚Äãinto the model: <br><br><pre> <code class="python hljs">train_data, test_data, train_labels, test_labels = cross_validation.train_test_split(iris_frame[[<span class="hljs-string"><span class="hljs-string">'sepal length (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'sepal width (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'petal length (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'petal width (cm)'</span></span>]], iris_frame[<span class="hljs-string"><span class="hljs-string">'target'</span></span>], test_size = <span class="hljs-number"><span class="hljs-number">0.3</span></span>) model = linear_model.SGDClassifier(alpha=<span class="hljs-number"><span class="hljs-number">0.0009</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">96</span></span>, random_state = <span class="hljs-number"><span class="hljs-number">0</span></span>) scores = cross_validation.cross_val_score(model, train_data, train_labels, cv=<span class="hljs-number"><span class="hljs-number">10</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> scores.mean()</code> </pre> <br>  We look, it became a little better: <b>0.915505050505</b> <br><br><h4>  We select and create signs </h4><br>  It's time to experiment with the signs.  Let's remove from the model less significant features, namely ‚Äúsepal length (cm)‚Äù and ‚Äúsepal width (cm)‚Äù.  We drive into the model: <br><br><pre> <code class="python hljs">train_data, test_data, train_labels, test_labels = cross_validation.train_test_split(iris_frame[[<span class="hljs-string"><span class="hljs-string">'petal length (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'petal width (cm)'</span></span>]], iris_frame[<span class="hljs-string"><span class="hljs-string">'target'</span></span>], test_size = <span class="hljs-number"><span class="hljs-number">0.3</span></span>) model = linear_model.SGDClassifier(alpha=<span class="hljs-number"><span class="hljs-number">0.0009</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">96</span></span>, random_state = <span class="hljs-number"><span class="hljs-number">0</span></span>) scores = cross_validation.cross_val_score(model, train_data, train_labels, cv=<span class="hljs-number"><span class="hljs-number">10</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> scores.mean()</code> </pre> <br>  We look, it became even better: <b>0.937727272727</b> <br>  To illustrate the approach, let's make a new sign: the area of ‚Äã‚Äãa petal sheet and see what happens. <br><br><pre> <code class="python hljs">iris_frame[<span class="hljs-string"><span class="hljs-string">'petal_area'</span></span>] = <span class="hljs-number"><span class="hljs-number">0.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">150</span></span>): iris_frame[<span class="hljs-string"><span class="hljs-string">'petal_area'</span></span>][k] = iris_frame[<span class="hljs-string"><span class="hljs-string">'petal length (cm)'</span></span>][k] * iris_frame[<span class="hljs-string"><span class="hljs-string">'petal width (cm)'</span></span>][k]</code> </pre><br>  Substitute in the model: <br><br><pre> <code class="python hljs">train_data, test_data, train_labels, test_labels = cross_validation.train_test_split(iris_frame[[<span class="hljs-string"><span class="hljs-string">'petal_area'</span></span>]], iris_frame[<span class="hljs-string"><span class="hljs-string">'target'</span></span>], test_size = <span class="hljs-number"><span class="hljs-number">0.3</span></span>) model = linear_model.SGDClassifier(alpha=<span class="hljs-number"><span class="hljs-number">0.0009</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">96</span></span>, random_state = <span class="hljs-number"><span class="hljs-number">0</span></span>) scores = cross_validation.cross_val_score(model, train_data, train_labels, cv=<span class="hljs-number"><span class="hljs-number">10</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> scores.mean()</code> </pre> <br>  It's funny, but in our example it turns out that the petal area of ‚Äã‚Äãthe petal (or rather, not even the area, because the petals are not rectangles, but the ‚Äúproduct of length by width‚Äù) is most accurately predicted by the Iris variety: <b>0.94237373737374</b> <br><br>  Perhaps this can be explained by the fact that the variables 'petal length (cm)' and 'petal width (cm)', and so well divides the Irises into classes, and their work also ‚Äústretches‚Äù the classes along the straight line: <br><br><img src="https://habrastorage.org/web/ad7/cbd/07d/ad7cbd07dcdc4b868f3ed18f574a6c03.png"><br><br>  We got acquainted with the main ways of model optimization, now let's consider the clustering algorithm - an example of machine learning without a teacher. <br><br><h4>  Clustering - K-means </h4><br>  The essence of clustering is extremely simple - it is necessary to divide the existing objects into groups, so that the groups include similar objects.  We now do not have the correct answers to train the model, so the algorithm must itself group objects according to the "proximity" of the location of objects to each other. <br><br>  For example, consider the most famous K-means algorithm.  It is not for nothing that K-Means are called, since  The method is based on finding K cluster centers so that the average distances from them to objects that belong to them are minimal.  First, the algorithm determines K arbitrary centers, then all objects are distributed in proximity to these centers.  Got K clusters of objects.  Then, in these clusters, the centers are re-calculated by the average distance to the objects, and the objects are again redistributed.  The algorithm works until the centers of the clusters stop moving to any particular delta. <br><br><pre> <code class="python hljs">train_data, test_data, train_labels, test_labels = cross_validation.train_test_split(iris_frame[[<span class="hljs-string"><span class="hljs-string">'sepal length (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'sepal width (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'petal length (cm)'</span></span>,<span class="hljs-string"><span class="hljs-string">'petal width (cm)'</span></span>]], iris_frame[[<span class="hljs-string"><span class="hljs-string">'target'</span></span>]], test_size = <span class="hljs-number"><span class="hljs-number">0.3</span></span>) model = KMeans(n_clusters=<span class="hljs-number"><span class="hljs-number">3</span></span>) model.fit(train_data) model_predictions = model.predict(test_data) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> metrics.accuracy_score(test_labels, model_predictions) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> metrics.classification_report(test_labels, model_predictions)</code> </pre> <br>  We look at the results: <br><br><img src="https://habrastorage.org/web/8d0/893/197/8d08931975b94ca9a2b559c5351555a0.png"><br><br>  We see that even with the default parameters it turns out very well: accuracy, precision and recall are greater than 0.9.  We are convinced on the pictures.  We see a decent, but not everywhere accurate result: <br><br><img src="https://habrastorage.org/web/86d/177/0c8/86d1770c8e4b4dd08f21afbdd41a2006.png"><br><br><img src="https://habrastorage.org/web/898/725/8f5/8987258f54724d59abc7827222578819.png"><br><br>  The algorithm has a drawback - for its operation, you need to specify the number of clusters that we want to find.  And if it is inadequate, the results of the algorithm will be useless.  Let's see what happens if you specify the number of clusters, for example, 5: <br><br><img src="https://habrastorage.org/web/74f/4a8/a15/74f4a8a156ef445eacbdc5d4c784ad74.png"><br><br>  We see that in practice the result is not applicable.  There are algorithms for determining the optimal number of clusters, but in this article we will not dwell on them. <br><br><h3>  Conclusion on the Iris Research </h3><br>  So, on the example of Irises, we considered three main methods of machine learning: regression, classification and clustering.  Spent optimization algorithms and visualization of results.  We obtained very good results, but this was expected on a specially prepared data set. <br><br>  A complete Python Notebook can be found on <a href="https://github.com/ASushkov/Iris">Github</a> .  Go to Telecom. <br><br><h2>  Telecom </h2><br>  Telecom has tasks that are solved in other areas with the help of data analysis (banks, insurance, retail): <br><br><ul><li>  Predicting customer churn (Churn Prevention); </li><li>  Fraud Prevention; </li><li>  Identification of similar subscribers (subscriber base segmentation); </li><li>  Cross sales (Cross-Sale) and raising the amount of the sale (Up-Sale); </li><li>  Identify subscribers who strongly influence their surroundings (Alpha subscribers). </li></ul>  In addition, there are specific tasks: <br><ul><li>  Prediction of network resource consumption by subscribers: traffic volume, number of calls, SMS; </li><li>  The study of the movement of subscribers to optimize the network. </li></ul>  Where does a telecom operator get data for analysis?  Of the various information systems and equipment that is involved in providing services to subscribers: <br><br><ul><li>  The billing system stores data on payments and expenses of subscribers, tariffs, personal data; </li><li>  Data on which sites the subscriber visited is extracted from the <a href="https://ru.wikipedia.org/wiki/Deep_packet_inspection">DPI</a> equipment; </li><li>  From base stations you can get geodata based on the location of the subscriber; </li><li>  Service equipment generates data on the consumption of communication services by the subscriber. </li></ul><br>  My goal was to determine which tasks you can try to solve using the data generated by the subscriber traffic management system.  In order for the billing system to charge the subscriber‚Äôs traffic correctly, it needs to know: who / where / when / what type and volume of traffic was consumed.  This information comes from the equipment in the form of so-called CDR (Call Data Record) files.  <a href="https://ru.wikipedia.org/wiki/IMSI">IMSI</a> and <a href="https://ru.wikipedia.org/wiki/MSISDN">MSISDN</a> subscriber identifiers, location accurate to base station CELL ID, <a href="https://ru.wikipedia.org/wiki/IMEI">IMEI</a> subscriber equipment identifier, session timestamp and information about the consumed service are recorded in these files in csv format. <br><br>  To maintain confidentiality, all data for the study were impersonal and replaced with random values ‚Äã‚Äãin compliance with the format.  Let's look at the data: <br><br><img src="https://habrastorage.org/web/21b/1cc/142/21b1cc1427f34b38888d2dc45a8d2fae.png"><br><br>  What machine learning algorithms can be applied to this data?  You can, for example, aggregate the consumption of different types of traffic by subscribers for a certain period and perform clustering.  You should get something like this: <br><br><img src="https://habrastorage.org/web/c4a/307/a91/c4a307a91bf9411a88e1033c6eac79f6.png"><br><br>  Those.  if, for example, the result of clustering showed that subscribers were divided into groups that use Youtube, social networks and listen to music in different ways, then you can make tariffs that take into account their interests.  I suppose that telecom operators do this, producing tariff lines with payment differentiation according to the type of traffic. <br><br>  What else can be analyzed in the available data?  There are several cases with subscribers equipment.  The operator knows the model of the subscriber's device and can, for example, offer certain services only to Samsung users.  Or, knowing the coordinates of the base stations, you can draw a heat map of the distribution of Samsung phones (I have no real coordinates, so the <a href="https://alexeysushkov.carto.com/builder/573a4146-d194-41aa-8cbd-a155fe5e328e/embed">map</a> has no relation to reality): <br><br><img src="https://habrastorage.org/web/b71/2dd/493/b712dd4937074457bb322a86a1499861.png"><br><br>  It may happen that in a certain region there will be a percentage of them more than in others.  Then this information can be offered to Samsung for advertising campaigns or opening stores selling smartphones.  Then you can look at the Top models of devices from which subscribers access the Internet: <br><br><img src="https://habrastorage.org/web/0e1/8c5/a3c/0e18c5a3ce3749268b6c428a3ee2237d.png"><br><br>  To mask the current state of affairs, the outdated IMEI database was taken, but this does not change the essence of the approach.  The list shows that most of the devices are Apple, modems and Samsung, and at the end Meizu, Micromax and Xiaomi appear. <br><br>  Actually, these are all applications of the source data that I could find in a short time.  Of course, using this data, you can look at various statistics and time series, analyze emissions, etc., but in order to identify any dependence by machine learning ... unfortunately, I have not yet found how to do this. <br><br>  ,      :             , ..      ,    . <br><br><h2>   </h2><br><ul><li>       .      ,        . </li><li>  ,    ,          ,     . </li></ul></div><p>Source: <a href="https://habr.com/ru/post/334738/">https://habr.com/ru/post/334738/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../334726/index.html">Or meteor</a></li>
<li><a href="../334730/index.html">In a section: the news aggregator on Android with backend. Sequential integration system</a></li>
<li><a href="../334732/index.html">Event digest for HR specialists in IT-area for August 2017</a></li>
<li><a href="../334734/index.html">WI-FI in the subway: Catch me if you can. Challenges of configuring dynamic networks</a></li>
<li><a href="../334736/index.html">Why correcting spelling, grammar and punctuation errors in a product is very important</a></li>
<li><a href="../334740/index.html">We invite you to the –∞–ø Java and Linux - Fight for microseconds ‚Äô</a></li>
<li><a href="../334742/index.html">It's time to recover data. Do you know where they are?</a></li>
<li><a href="../334744/index.html">As we multiplayer for NFS MW wrote</a></li>
<li><a href="../334748/index.html">6 reasons why real estate automation requires breaking stereotypes</a></li>
<li><a href="../334750/index.html">5 easy ways to mess up a font</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>