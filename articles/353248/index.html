<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Reachability of the lower limit of the execution time of a commit distributed fault-tolerant transactions</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Foreword 


 Recently I read another article from the series: "we are better than a two-phase commit". Here I will not analyze the content of this art...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Reachability of the lower limit of the execution time of a commit distributed fault-tolerant transactions</h1><div class="post__text post__text-html js-mediator-article"><h2 id="predislovie">  Foreword </h2><br><p>  Recently I read another article from the series: "we are better than a two-phase commit".  Here I will not analyze the content of this article (although I am thinking about giving a detailed analysis).  The task of my opus is to propose the most efficient version of a distributed commit in terms of time delays.  Of course, such a commit is a high price.  However, the goal is to give an assessment and show that a two-phase commit is not a brake, as many believe. </p><br><p>  It is also worth noting that there will not be full-scale experiments and fake comparisons.  Algorithms and theoretical analysis will be simply given.  If desired, you can independently implement and test in practice.  Of course, it would be much better if this was described in the current article, but everything depends on free time and motivation.  In my opinion, to describe the algorithms is more important than to bring graphics, because  graphics on algorithms can draw almost everyone, the opposite is not true. </p><a name="habracut"></a><br><p>  After such a preface will proceed. </p><br><h2 id="vvedenie">  Introduction </h2><br><p>  <strong>Definition</strong>  <em>RTT</em> - message time back and forth. <br>  <strong>Definition</strong>  <em>Hop</em> - the time of one shipment. </p><br><p>  <strong>Theorem</strong> .  Time 1 RTT is equal to the time of two hops. <br>  <em>Proof</em> .  It is obvious. </p><br><p>  <strong>Definition</strong>  <em>Distributed commit</em> is the process of making atomic changes between at least two distributed system members. </p><br><p>  <strong>Definition</strong>  <em>A two-phase commit</em> is a two-phase commit.  The first phase is an atomic operation to check the possibility of starting a transaction and blocking the participants of the commit.  The second phase is the collection of responses from the participants and the application of the transaction with the release of locks. </p><br><p>  <strong>Theorem</strong> .  A two-phase distributed commit cannot be made faster than 1 RTT. <br>  <em>Proof</em> .  To conduct a two-phase commit, it is necessary, at a minimum, to send a request from the client to all participants and receive a response on completion.  For this you need 2 hop or 1 RTT. </p><br><p>  <strong>Definition</strong>  <em>Failsafe commit</em> is a commit that continues to be executed even if one or more commit members fail. </p><br><p>  <strong>Theorem</strong> .  A two-phase, fault-tolerant distributed commit for 1 RTT is possible. </p><br><p>  To prove this theorem, it suffices to give the method and conditions, when possible.  It is clear that this is not always possible, because  in the case of competitive access of distributed transactions to the same resource, such transactions should be queued up to this resource.  So, they will be executed sequentially.  In this case, talking about 1 RTT will be somewhat fun.  However, even conventional algorithms, under favorable conditions, give times markedly longer than 1 RTT. </p><br><p>  The proof of this theorem will be devoted to the further part of the article. </p><br><h2 id="dvuhfaznyy-kommit">  Biphasic commit </h2><br><p>  Consider the classic two-phase commit scheme with the coordinator. </p><br><p>  The sequence is as follows: </p><br><p>  <em>1st hop</em> .  The client sends a request to the coordinator. <br>  <em>2nd hop</em> .  The transaction coordinator sends a request to the participants for blocking - 1st phase. <br>  <em>3rd hop</em> .  Participants successfully take locks and send a response that they are ready for transactions. <br>  <em>4th hop</em> .  The coordinator sends a message on the application of operations to all participants - the 2nd phase. <br>  <em>5th hop</em> .  Participants report on the success of the application of the coordinator. <br>  <em>6th hop</em> .  The coordinator responds to the client. </p><br><p>  Total 3 RTT. </p><br><p>  Now add fault tolerance.  We will assume that the coordinator and the participants are included in the relevant consensus groups.  We will also assume favorable conditions, i.e.  the group leader does not change and the consensus is completed safely.  Let us prove the lemma: </p><br><p>  <strong>Lemma</strong>  A leader-based distributed consensus cannot be accomplished faster than 1 RTT. <br>  <em>Proof</em> .  To achieve consensus, the request should be sent to the leader.  Wherein: </p><br><p>  <em>1st hop</em> .  The leader sends the request to other participants in the consensus (follower). <br>  <em>2nd hop</em> .  Participants send confirmation to the leader. <br>  Without these phases, consensus is impossible. </p><br><p>  <strong>Lemma</strong>  Consensus is possible for 1 RTT. <br>  <em>Proof</em> : take the Raft algorithm.  In the case of liveliness of the leader and the majority of participants in a consensus, the adoption of an agreed decision on the leader occurs after receiving answers from the participants, i.e.  after 1 RTT. </p><br><p>  <strong>Definition</strong>  Making an agreed decision is a group consensus <em>agreement</em> . </p><br><p>  It should be noted that after this the system guarantees that this agreement will remain in the system, even though the agreement has not reached other participants at this moment.  In the event of a leader falling, a failover occurs, during which the new leader must complete these changes.  However, this is not the subject of consideration of the lemma, since  we consider the potential, i.e.  some ideal conditions that can lead to the desired result - the achievement of consensus.  Why we do not consider all possible conditions?  Yes, because there is a theorem that <a href="https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf">consensus in an asynchronous system is impossible</a> .  Therefore, it is important to understand what is the minimum possible time in the most favorable situations without disturbing the correctness of the algorithm, which is obliged to retain its invariants and in case of violation of these conditions at any stage.  These two lemmas give an exhaustive answer, which suggests that the shortest possible agreement time is achievable. </p><br><p>  This theorem can be generalized by proving that it is impossible to reach a consensus faster than 1 RTT, throwing out the condition of having a leader.  However, this is beyond the scope of this article.  The idea of ‚Äã‚Äãthe proof is to consider the dissemination of knowledge about other participants in the system and whether they have the appropriate message: for 1 hop, you can only send data, but don‚Äôt find out if they reached and what state the recipient was in. </p><br><p>  So, for fault tolerance, take a consensus with 1 RTT and add to our two-phase commit: </p><br><p>  <em>1st hop</em> .  The client sends a request to the leader of the coordinator. <br>  <em>2nd and 3rd hop</em> .  The coordinator leader agrees to start the transaction. <br>  <em>4th hop</em> .  The transaction coordinator sends the request to the leaders of the participants for blocking - 1st phase. <br>  <em>5th and 6th hop</em> .  Participants successfully take locks with preservation of knowledge in their consensus groups. <br>  <em>7th hop</em> .  Leaders of participants send the answer that they are ready to conduct transactions. <br>  <em>8th and 9th hop</em> .  The leader of the coordinator will coordinate information about all participants in the system. <br>  <em>10th hop</em> .  The leader of the coordinator sends out a message on the application of operations to all the leaders of the participants - phase 2. <br>  <em>11th and 12th hop</em> .  Leaders agree on commit and apply changes. <br>  <em>13th hop</em> .  Participants report on the success of the application of the leader coordinator. <br>  <em>14th hop</em> .  The coordinator responds to the client. </p><br><p>  Total 7 RTT.  Not bad.  Fault tolerance is ‚Äúonly‚Äù 4 RTTs.  They arise from the fact that the coordinator and the participants 2 times consistently come to their own consensus, for which this time is spent. </p><br><p>  In the above scheme, some non-optimalities can be noticed.  Let's fix them. </p><br><h2 id="optimizaciya-kommita">  Commit optimization </h2><br><p>  The first obvious optimization is sending a response to the client immediately after collecting the responses of successful locks from the participants.  Since  Since these answers are fault-tolerant, the participants will never forget about them, which means that the transaction will be executed sooner or later even in the event of loss of the notes, re-election of the leader, etc.  Here, however, there is one slippery moment. </p><br><p>  It lies in the fact that in fact the coordinator makes the final decision on whether to commit the final transaction or not.  Those.  even if all the participants said OK, but some participant blunted due to, for example, a change of leader, the coordinator has every right to roll back the transaction.  And if so, then you can remove only 10-13 hops, but not the 8th and 9th.  But this is not bad, since we have a decrease of 2 RTT, i.e.  5 RTT instead of 7. </p><br><p>  At the same time, 10-13 hops do not disappear anywhere, just the client does not need to wait for them.  The coordinator and the participants will finish their affairs in parallel with the client.  And the client will receive his confirmation a little earlier.  This knowledge will be in the system, just a little later.  Here we use the magic of asynchrony, consensus and the impossibility of proving to an outside participant that we cheated and cut the corner a little.  Those.  if the client suddenly wants to immediately read the data that we have just committed and go immediately to some participant, he will come across a lock (if it was not removed by the 2nd phase), and this request will hang until it is removed .  However, within the framework of our theoretical research, this fact is absolutely not important, since  we are preparing the perfect conditions.  And in the case of imperfect ones, as already mentioned above, we will wait for several eternities (since consensus will require eternity, but we need to hold several of them, and consistently). </p><br><p>  The next knight's move is somewhat more complicated and elegant. </p><br><p>  Consider the very beginning of the transaction.  There, the client sends a request to the coordinator and then he initiates a two-phase commit and sends these requests to the other participants.  The idea immediately arises to execute such requests simultaneously, i.e.  send a request to the coordinator and participants in parallel.  On this way, an insidious trap awaits us. </p><br><p>  The fact is that the client is not a fault-tolerant entity, i.e.  he may fall.  Imagine that he sent a request to the participants, they took the lock and wait, but the request to the coordinator for some reason did not reach and the client fell.  Thus, there is no one to start a two-phase commit and there is no one to roll it back in case of conflicts / problems and so on.  Participants will permanently block records and no one will help them.  Therefore, this optimization is incorrect.  Participants have the right to commit only after the decision of the coordinator, who is responsible for the transaction and rolls it back if necessary. </p><br><p>  To go further, you need a completely different look at the problem.  And for this we begin, oddly enough, with a consensus. </p><br><h2 id="optimizaciya-konsensusa">  Consensus optimization </h2><br><p>  It would seem that there can be optimized?  After all, we with Raft reach the minimum possible execution time - 1 RTT.  In fact, it can be faster - for 0 RTT. </p><br><p>  To do this, we recall that in addition to the consensus itself, 1 more RTT is required to send a request from the client to the leader and receive a response.  Those.  for the removed consensus group, 2 RTT is required in this case, which we see in a two-phase commit in 2 examples: sending and committing on the coordinator, sending and committing on the participants.  Total 4 RTTs at once, and another 1 RTT - per second phase commit at the coordinator. </p><br><p>  It is clear that a leader-based consensus for a remote client can never be faster than 2 RTTs.  In fact, we first need to deliver the message to the leader, and then the leader is obliged to send the group members and receive a response from them.  No options. </p><br><p> The only option is to get rid of the weak link - the leader.  Indeed, moreover, that all records must pass through it, even if it falls, the group becomes inaccessible for a sufficiently long time.  The leader of the consensus is the weakest link, and the restoration of the leader is the most fragile and non-trivial part of the consensus.  Therefore, you just need to get rid of it. </p><br><p>  <strong>Definition</strong>  Broadcast messages are sending the same message to all group members. </p><br><p>  To do this, we take the <a href="http://gridem.blogspot.com/2016/05/replicated-object-part-7-masterless.html">consensus</a> widely known in narrow circles <a href="http://gridem.blogspot.com/2016/05/replicated-object-part-7-masterless.html">without a master</a> .  The basic idea is to ensure that the broadcasters achieve the same status on the participants.  To do this, it is enough to make 2 broadcasts, i.e.  just 1 RTT.  The first Broadcast to the participants of the system can be made by the client.  Response broadcasters from the participants can reach the client.  If the client sees the same state (and he sees it in the case of, for example, the absence of concurrent requests), he will be able to understand the analysis of the content of the response broadcasts, that his request will be committed sooner or later.  In fact, using such an algorithm, all participants in the consensus, including the client, simultaneously realize that the request was committed.  And this will happen after 2 broadcasts, i.e.  1 RTT.  Since  the client still has to spend 1 RTT to send a message to the group and get an answer, then we have a paradoxical conclusion that the consensus effectively occurred in the 0 RTT. </p><br><h2 id="analogiya">  Analogy </h2><br><p>  To go further, let us use the most powerful analysis tool - the analogy.  Let's go back to the Raft algorithm.  What is happening in it?  It consists of two phases: </p><br><p>  <em>1st phase</em> : the leader sends a request to the participants and waits for a response. <br>  <em>Phase 2</em> : after the answer, the leader makes an agreed decision individually and sends it to the system participants. </p><br><p>  Nothing like?  That's right, this is a two-phase commit, with only some reservations: </p><br><ol><li>  The Raft algorithm does not need to wait for a response from all participants.  In a two-phase commit for a successful transaction, you must wait for a successful response from all participants. </li><li>  In the Raft algorithm, the participant cannot say neOK.  More precisely, theoretically, he can do so (for example, the place is over), but this neOK will be similar to the absence of an answer.  In a two-phase commit, everything is stricter: if at least one of the participants said a neOK, then the whole transaction should be quarantined and rolled back.  This is the very essence of two-phase: first we ask the consent of everyone, and only after universal unanimous approval we roll changes.  The consensus in this sense is more democratic, since  requires majority approval. </li></ol><br><p>  At the same time, they have in common that there is a dedicated decision-making driver (leader or coordinator), and there are 2 phases - preliminary and final. </p><br><p>  Accordingly, all we need is to abandon the coordinator in a two-phase commit, i.e.  do exactly the same thing that we did for consensus by giving up the leader. </p><br><p>  Let's forget about fault tolerance for a while and see how commit will look in this case. </p><br><h2 id="samokoordinaciya">  Self-coordination </h2><br><p>  <strong>Definition</strong>  <em>A two-phase commit without a coordinator</em> consists of 2 phases: </p><br><ol><li>  All participants send to all other participants their decision: OK or neOK. </li><li>  Each participant after receiving an OK from everyone commits the changes or rolls them back, if at least one responded to a non-QC. </li></ol><br><p>  After that, for reliability, each participant can send information to the others that a commit has occurred and you can release locks, but this is not necessary. </p><br><p>  Why did we suddenly do not need a coordinator?  The fact is that the coordinator followed the transaction process, including whether the nodes were alive.  Those.  in case of problems with the participants, the coordinator rolled back the transaction.  The problem was only in the coordinator itself, since  he could not look after himself.  Therefore, often a <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2003-96.pdf">two-phase commit is called blocking</a> . </p><br><p>  <strong>Definition</strong>  <em>Self-coordinating transactions</em> - transactions that do not require a dedicated coordinator. </p><br><p>  However, adding fault tolerance to the role of the coordinator becomes redundant, since  each participant who is a consensus group can stand for itself.  Thus, we come to self-coordinating transactions without the need for a dedicated coordinator.  An important difference from the usual two-phase commit with the coordinator is that the coordinator can decide at any time to roll back the transaction, even if all participants gave a positive answer.  In self-coordinating transactions such non-deterministic behavior is unacceptable, since  Each participant makes a decision based on the responses of other participants and this decision should be the same. </p><br><p>  <strong>Theorem</strong> .  Self-coordinating transactions give strict consistency (strict consistency = linearizability + serializability). <br>  <em>Proof</em> .  Actually, the proof is based on the simple fact that a two-phase commit also provides such a guarantee.  Indeed, in a scheme without a coordinator, each participant is himself a coordinator, i.e.  there passes a two-phase commit as if it is the most important.  So, it preserves all invariants of a two-phase commit.  It is easy to be convinced of this, if we recall that each participant broadcasts by the broadcasted answers to all the others.  Those.  each receives OK responses from everyone else, acting as a coordinator to commit a transaction commit. </p><br><p>  We describe the minimum number of hopes under favorable circumstances: <br>  <em>1st hop</em> .  The client sends a message to all participants in the transaction. <br>  <em>2nd hop</em> .  All participants send a response to the client and each other. </p><br><p>  After the 2nd hop, the client has all the necessary information to make a decision about the commit.  Thus, only 1 RTT is required for a commit. </p><br><h2 id="otkazoustoychivost-i-dostupnost">  Fault tolerance and availability </h2><br><p>  The attentive reader may ask: what to do in the event of a client falling?  After all, if the participants of the system can be made fault tolerant, then we cannot make such requirements to the client, i.e.  he may fall at any time.  It is clear that after the client sends requests to all the participants in the system, the distributed commit is able to complete without the participation of the client.  And what to do if the client managed to send only some of them and safely fell? </p><br><p>  In this case, we oblige the client to do the following: the client must send each participant information about all the other participants in our transaction.  Thus, each participant knows all the other participants and sends them their result.  However, any participant, if he did not receive a request from the client, can choose one of the following behaviors: </p><br><ol><li>  Immediately answer that he does not accept the transaction, i.e.  sends neoc.  In this case, the lock is rolled back.  The participant, as always, sends his answer to the other participants. </li><li>  If the request from another participant contains all the necessary information to complete the transaction commit for this participant, then you can decide on successful blocking of the corresponding records (1st phase) and send OK.  To do this, the client must send each participant of the transaction information about all other participants and all the necessary data to make a distributed commit. </li></ol><br><p>  In any case, we get that all participants either get OK, or in the absence of the necessary information, someone reports the neOK and the transaction is rolled back.  Those.  in the event of a client crashing, each participant is able to either finish the job or correctly roll back the client‚Äôs actions. </p><br><p>  It remains to make the system participants fault tolerant.  To do this, put them in the consensus of the group without a dedicated leader.  Those.  Each participant will not represent a separate node, but a set of nodes in the consensus group. </p><br><p>  The commit algorithm will look like this: </p><br><ol><li>  The client sends to each node belonging to the group of participants in the transaction, its request. </li><li>  Upon receiving a request, each node sends to all the other nodes and the client a response regarding the speculative execution of the first phase of the commit as if it were executed at the current step of consensus.  In reality, we do not know whether this will actually happen or not, because  in the case of competitive requests from other customers, the consensus may reorder the current unapplied actions. </li><li>  The client receives all requests from all nodes of all participants.  If all the nodes during the speculative execution answered OK and the consensus step was the same for each node from the consensus groups, then this means that the speculative implementation of the first phase will actually occur and you can make a decision about the commit. </li></ol><br><p>  In fact, the condition of receiving a response from all the nodes of each group is redundant.  However, a more detailed consideration of the easing of this requirement is beyond the scope of this article. </p><br><h2 id="vyvody">  findings </h2><br><p>  Total we get 2 hop or 1 RTT.  Taking into account the fact that the message between the client and the server cannot be removed, the effective processing time of the commit on the server side is zero, i.e.  as if the server instantly processed a distributed, highly available failover transaction and sent a response to the client. </p><br><p>  Thus, we have an important theoretical and applied fact: the lower limit of the execution time of a distributed fault-tolerant commit is achievable. </p><br><h2 id="literatura">  Literature </h2><br><p>  <a href="https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf">Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson, 1983,</a> <a href="https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf"><br></a>  <a href="https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf"><em>Impossibility of distributed consensus with one faulty process</em></a> </p><br><p>  <a href="http://gridem.blogspot.com/2016/05/replicated-object-part-7-masterless.html">G. Demchenko, 2016, <em>Masterless Consensus Algorithm</em></a> </p><br><p>  <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2003-96.pdf">Jim Gray, Leslie Lamport, 2003, <em>Consensus on Transaction Commit</em></a> </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/353248/">https://habr.com/ru/post/353248/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../353232/index.html">Generation of site pages by means of service workers</a></li>
<li><a href="../353234/index.html">Flask Mega-Tutorial, Part XIX: Deploying Based on Docker Containers</a></li>
<li><a href="../353236/index.html">The history of gaming analytics platforms</a></li>
<li><a href="../353238/index.html">Docker. Start</a></li>
<li><a href="../353246/index.html">We invite you to a lecture evening on game design on April 18th at VSBI</a></li>
<li><a href="../353250/index.html">Dofoil Hunt with Windows Defender ATP</a></li>
<li><a href="../353252/index.html">Manage copying Active Directory attributes when duplicating user accounts</a></li>
<li><a href="../353254/index.html">How in hh.ru test job search</a></li>
<li><a href="../353256/index.html">How fast is AMP really?</a></li>
<li><a href="../353258/index.html">About decorators, end-to-end functionality, CQRS and layered architecture</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>