<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How Yandex recognizes music from a microphone</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Search in the catalog of music is a task that can be solved in different ways, both from the user's point of view and technologically. Yandex has lear...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How Yandex recognizes music from a microphone</h1><div class="post__text post__text-html js-mediator-article"><img src="http://cho.justos.org:9119/counter.gif" width="1" height="1">  Search in the catalog of music is a task that can be solved in different ways, both from the user's point of view and technologically.  Yandex has learned quite a long time ago to look for both song titles and song <a href="http://yandex.ru/yandsearch%3Ftext%3DOh%2520I%2520beg%2520you%2520can%2520I%2520follow%2520Oh%2520I%2520ask%2520you%2520why%2520not%2520always%26lr%3D213">lyrics</a> .  We also know how to respond to requests for music made by voice in <a href="http://mobile.yandex.ru/apps/search/">Yandex.Search</a> for iOS and Android, today we‚Äôll talk about searching for an audio signal, and if specifically for a fragment of a musical work recorded from a microphone.  This is the function that is built into the <a href="http://mobile.yandex.ru/apps/music/">Yandex.Music mobile application</a> : <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d80/4ca/9b5/d804ca9b596223f12287a024e91a02d6.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/afa/19d/045/afa19d045b2bce238895a5f2856f9174.gif"><br><br>  In the world there are only a few specialized companies that are professionally engaged in the recognition of music tracks.  As far as we know, from search companies, Yandex was the first to help the Russian user in this task.  Despite the fact that we still have a lot to do, the quality of recognition is already comparable with the leaders in this area.  In addition, the search for music by audio fragment is not the most trivial and illuminated topic in RuNet;  we hope that many will be curious to know the details. <br><a name="habracut"></a><br><div class="spoiler">  <b class="spoiler_title">About the achieved quality level</b> <div class="spoiler_text">  The basic quality we call the percentage of valid requests, to which we gave the relevant answer - now about 80%.  The relevant response is the track that contains the user's request.  We consider as valid only those requests from the Yandex.Music application that actually contain a musical recording, and not just noise or silence.  When we request a work that is not known to us, we consider the answer as obviously irrelevant. </div></div><br>  Technically, the task is formulated as follows: a 10-second fragment of an audio signal recorded on a smartphone (we call it a request) arrives at the server, after which among the known tracks we need to find exactly the one from which the fragment was recorded.  If the fragment is not contained in any known track, as well as if it is not a musical record at all, you need to answer ‚Äúnothing found‚Äù.  Answer the most similar sound tracks in the absence of an exact match is not required. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  Base tracks </h4><br>  As in a web search, in order to search well, you need to have a large database of documents (in this case, tracks), and they must be correctly labeled: for each track you need to know the name, artist and album.  As you probably already guessed, we had such a base.  Firstly, this is a huge number of entries in Yandex.Music, officially provided by copyright holders for listening.  Secondly, we have collected a selection of music tracks posted on the Internet.  So we got 6 million tracks that users are most interested in. <br><div class="spoiler">  <b class="spoiler_title">Why do we need tracks from the Internet, and what we do with them</b> <div class="spoiler_text">  Our goal as a search engine is completeness: we must give a relevant answer for every valid query.  The base Yandex.Music is not some popular artists, not all copyright holders are still <a href="">involved</a> in this project.  On the other hand, the fact that we do not have the right to give users to listen to any tracks from the service does not mean at all that we cannot recognize them and report the name of the artist and the name of the song. <br><br>  Since we are the mirror of the Internet, we have collected ID3 tags and descriptors for each popular track on the Web to identify those works that are not in the Yandex.Music database.  It is enough to store only these metadata - for them we show music videos, when there were only records from the Internet. </div></div><br><h4>  Unpromising approaches </h4><br>  What is the best way to compare the fragment with the tracks?  Immediately discard the obviously inappropriate options. <br><ol><li>  <b>Bitwise comparison</b>  Even if the signal is received directly from the optical output of a digital player, inaccuracies will arise as a result of transcoding.  And during the transmission of the signal there are many other sources of distortion: the loudspeaker of the sound source, the acoustics of the room, the <a href="http://blog.faberacoustical.com/2010/ios/iphone/iphone-4-audio-and-frequency-response-limitations/">uneven frequency response of the microphone</a> , even digitization from the microphone.  All this makes inapplicable even fuzzy bitwise comparison. </li><li>  <b>Watermarks</b> .  If Yandex itself released music or participated in the production cycle of releasing all the records played on the radio, in cafes and discos - one could embed an analogue of <a href="http://en.wikipedia.org/wiki/Digital_watermarking">the watermarks</a> into the tracks.  These tags are invisible to the human ear, but are easily recognized by algorithms. </li><li>  <b>Loose comparison of spectrograms.</b>  We need a way to lax comparison.  Let's look at the <a href="http://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25BF%25D0%25B5%25D0%25BA%25D1%2582%25D1%2580%25D0%25BE%25D0%25B3%25D1%2580%25D0%25B0%25D0%25BC%25D0%25BC%25D0%25B0">spectrograms of the</a> original track and the recorded fragment.  They can be considered as images, and you can search for the most similar among images of all tracks (for example, comparing them as vectors using one of the well-known metrics, such as <a href="http://en.wikipedia.org/wiki/Norm_(mathematics)">L¬≤</a> ): <br><img src="https://habrastorage.org/getpro/habr/post_images/033/904/d77/033904d778d5d333b9e61d279b3e77d8.png" alt="image"><br>  But in the application of this method "in the forehead" there are two difficulties: <br>  a) comparison with 6 million images is obviously an expensive operation.  Even the coarsening of the full spectrogram, which generally preserves the properties of a signal, yields several megabytes of uncompressed data. <br>  b) it turns out that some differences are more indicative than others. </li></ol><br><br>  As a result, for each track we need the minimum number of the most characteristic (that is, briefly and accurately describing the track) attributes. <br><br><h4>  What signs are not afraid of distortion? </h4><br>  The main problems arise from noise and distortion in the path from the source of the signal to digitization from the microphone.  For different tracks, it is possible to compare the original with a fragment recorded in different artificially noisy conditions - and by many examples it is possible to find which characteristics are best preserved.  It turns out that the spectrogram peaks work well, identified in one way or another - for example, as the local maximum points of the amplitude.  The height of the peaks does not fit (the microphone's frequency response changes them), but their location on the frequency-time grid does not change much with noise.  This observation, in one form or another, is used in many well-known solutions ‚Äî for example, in <a href="http://echoprint.me/">Echoprint</a> .  On average, about 300 thousand peaks are obtained per track - this amount of data is much more realistic to compare with the millions of tracks in the database than the full query spectrogram. <br><img src="https://habrastorage.org/getpro/habr/post_images/2be/6f6/25e/2be6f625ebebe18e5499cd1436e88d1f.png" alt="image"><br><br>  But even if we take only the locations of the peaks, the identity of the set of peaks between the query and the segment of the original is a bad criterion.  For a large percentage of the fragments that are known to us, he finds nothing.  The reason - inaccuracies when writing a request.  Noise adds some peaks, suppresses others;  The frequency response of the entire signal transmission medium can even shift the frequency of the peaks.  So we come to a non-strict comparison of the set of peaks. <br><br>  We need to find in the entire database the segment of the track, most similar to our query.  I.e: <ul><li>  first in each track to find such a shift in time, where the maximum number of peaks would coincide with the request; </li><li>  then from all the tracks choose the one where the coincidence was greatest. </li></ul><br>  To do this, we build a histogram: for each frequency of the peak, which is present both in the request and in the track, we postpone +1 on the Y axis in the offset where there was a match: <br><img src="https://habrastorage.org/getpro/habr/post_images/ad4/dcc/707/ad4dcc7071ed920dc0b1e50d3e99ac17.png" alt="image"><br>  The track with the highest column in the histogram is the most relevant result - and the height of this column is a measure of the proximity between the request and the document. <br><br><h4>  Fight for search accuracy </h4><br>  Experience shows that if we look for all the peaks equivalently, we will often find the wrong tracks.  But the same measure of proximity can be applied not only to the entire set of document peaks, but also to any subset - for example, only to the most reproducible (resistant to distortion).  At the same time, this will also reduce the cost of building each histogram.  Here is how we choose such peaks. <br><br>  <b>Time selection:</b> first, inside one frequency, along the time axis from the beginning to the end of the recording, we launch an imaginary ‚Äúdescending blade‚Äù.  When each peak is detected that is higher than the current blade position, it cuts off the ‚Äútip‚Äù ‚Äîthe difference between the blade position and the height of the freshly detected peak.  Then the blade rises to the original height of this peak.  If the blade does not ‚Äúfind‚Äù a peak, it falls a little under its own weight. <br><img src="https://habrastorage.org/getpro/habr/post_images/406/496/c2b/406496c2bdef7a47e230b487016f9f3a.png" alt="image"><br><br>  <b>Diversity in frequencies: in</b> order to give preference to the most diverse frequencies, we raise the blade not only at the very frequency of the next peak, but also (to a lesser extent) in the frequencies adjacent to it. <br><br>  <b>Selection by frequency:</b> then, within one time interval, among all the frequencies, we select the most contrast peaks, i.e.  the largest local maximums among the cut off "tops". <br><img src="https://habrastorage.org/getpro/habr/post_images/7b2/318/8e1/7b23188e134a83ab5fce5a7c0d2f7963.png" alt="image"><br>  When selecting peaks, there are several parameters: the speed of lowering the blade, the number of selectable peaks in each time interval and the neighborhood of the influence of peaks on neighbors.  And we picked up such a combination of them, at which the minimum number of peaks remains, but almost all of them are resistant to distortion. <br><br><h4>  Search Acceleration </h4><br>  So, we have found a proximity metric that is well resistant to distortion.  It provides good search accuracy, but you also need to ensure that our search quickly responds to the user.  First you need to learn how to choose a very small number of candidate tracks for calculating the metric in order to avoid complete search of tracks when searching. <br><br>  <b>Improving key uniqueness</b> : One could build an index <br>  <code> </code> ‚Üí <code>(,   )</code> . <br>  Alas, such a "dictionary" of possible frequencies is too poor (256 "words" - the intervals into which we divide the entire frequency range).  Most queries contain such a set of ‚Äúwords‚Äù that is found in most of our 6 million documents.  It is necessary to find more <b>distinctive</b> (discriminative) keys - which are likely to occur in relevant documents, and less likely in irrelevant ones. <br><br>  A pair of closely spaced peaks are well suited for this.  Each pair is much less common. <br>  This win has its price - less likely to play in a distorted signal.  If for individual peaks it is on average P, then for pairs it is P <sup>2</sup> (that is, obviously less).  To compensate for this, we include each peak in several pairs at once.  This slightly increases the size of the index, but drastically reduces the number of documents considered in vain - by almost 3 orders of magnitude: <br><div class="spoiler">  <b class="spoiler_title">Win score</b> <div class="spoiler_text">  For example, if you include each peak in 8 pairs and ‚Äúpack‚Äù each pair in 20 bits (then the number of unique values ‚Äã‚Äãof pairs increases to ‚âà1 million), then: <br><ul><li>  the number of keys in the query grows 8 times </li><li>  the number of documents per key is reduced by ‚âà4000 times: ‚âà1 million / 256 </li><li>  total, the number of documents considered in vain decreases by ‚âà500 times: ‚âà4000 / 8 </li></ul><br></div></div><br><img src="https://habrastorage.org/getpro/habr/post_images/31e/af6/8b5/31eaf68b553f885e23352d47571bf40c.png" alt="image"><br><br>  Having selected a small number of documents with the help of pairs, you can proceed to their ranking.  Histograms can be applied to pairs of peaks with the same success, replacing the coincidence of one frequency with the coincidence of both frequencies in a pair. <br><br>  <b>Two-stage search</b> : to further reduce the volume of calculations, we divided the search into two stages: <br><ol><li>  We make a preliminary selection (pruning) of tracks for a very sparse set of the most contrasting peaks.  Selection parameters are selected so as to narrow the circle of documents as much as possible, but to save the most relevant result among them. </li><li>  The guaranteed best answer is chosen - for selected tracks, the exact relevance of a more complete sample of peaks, already on an index with a different structure, is considered: <br>  <code></code> ‚Üí <code>( ,   )</code> . <br></li></ol>  Such a two-stage accelerated the search 10 times.  Interestingly, in 80% of cases, the result of even coarse ranking at the first stage coincides with the most relevant answer obtained at the second stage. <br><br>  As a result of all the described optimizations, the entire database required for the search became 15 times smaller than the track files themselves. <br><br>  <b>Index in memory</b> : And finally, in order not to wait for the disk to be accessed for each request, the entire index is located in RAM and distributed across multiple servers, since  occupies units terabytes. <br><br><h4>  Nothing found? </h4><br>  It happens that for the requested fragment either there is no suitable track in our database, or the fragment is not a recording of any track at all.  How to make a decision when it is better to answer ‚Äúnothing is found‚Äù than to show the ‚Äúleast inappropriate‚Äù track?  It is not possible to cut off at some threshold of relevance - for different fragments the threshold differs many times, and a single value for all cases simply does not exist.  But if you sort the selected documents by relevance, the shape of the curve of its values ‚Äã‚Äãgives a good criterion.  If we know the relevant answer, a sharp drop (difference) in relevance is clearly visible on the curve, and vice versa - a gentle curve suggests that no suitable tracks have been found. <br><img src="https://habrastorage.org/getpro/habr/post_images/4a3/67c/c5c/4a367cc5c99a3d10d7681a7b5d0e1181.png" alt="image"><br><br><h4>  What's next </h4><br>  As already mentioned, we are at the beginning of a long journey.  There are a number of studies and improvements ahead to improve the quality of the search: for example, in cases of pace distortion and increased noise.  We will definitely try to apply machine learning to use a more diverse set of features and automatically select the most effective ones. <br><br>  In addition, we are planning incremental recognition, i.e.  give an answer for the first seconds of the fragment. <br><br><h4>  Other Music Search Tasks </h4><br>  <a href="http://en.wikipedia.org/wiki/Music_information_retrieval">The field of informational search for music is</a> far from exhausted by a task with a <a href="http://en.wikipedia.org/wiki/Audio_fingerprinting">fragment from a microphone</a> .  Working with a ‚Äúclean‚Äù, un-noisy signal that has only undergone compression, allows <b>you</b> to <b>find duplicate tracks</b> in an extensive music collection, as well as to detect potential <b>copyright infringements</b> .  And the search for inaccurate coincidences and different types of <b>similarity</b> is a whole direction, including the search for cover versions and remixes, the extraction of musical characteristics (rhythm, genre, composer) for building recommendations, as well as the search for plagiarism. <br><br>  Separately, highlight the task of searching for the <b>sang passage</b> .  It, in contrast to the recognition of a piece of a musical recording, requires a fundamentally different approach: instead of audio recording, as a rule, a musical representation of the piece is used, and often a request.  The accuracy of such solutions turns out to be much worse (at least, due to an incomparably greater variation of the query variations), and therefore they only recognize the most popular works. <img src="https://habrastorage.org/getpro/habr/post_images/afa/19d/045/afa19d045b2bce238895a5f2856f9174.gif"><br><br><h4>  What to read </h4><br><ul><li>  Avery Wang: <a href="http://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf">An Industrial-Strength Audio Search Algorithm</a> , Proc.  2003 ISMIR International Symposium on Music Information Retrieval, Baltimore, MD, Oct.  2003. This article for the first time (as far as we know) suggests using spectrogram peaks and peak pairs as features that are resistant to typical signal distortion. </li><li>  D. Ellis (2009): <a href="http://labrosa.ee.columbia.edu/matlab/fingerprint/">Robust Landmark-Based Audio Fingerprinting</a> .  This paper gives a concrete example of the implementation of the selection of peaks and their pairs using the ‚Äúdecaying threshold‚Äù (in our free translation - ‚Äúdescending blade‚Äù). </li><li>  Jaap Haitsma, Ton Kalker (2002): <a href="http://ismir2002.ismir.net/proceedings/02-FP04-2.pdf">"A Highly Robust Audio Fingerprinting System</a> . <a href="http://ismir2002.ismir.net/proceedings/02-FP04-2.pdf">"</a>  This article proposed to encode consecutive blocks of audio with 32 bits, each bit describes the change in energy in its frequency range.  The described approach is easily generalized to the case of arbitrary coding of a sequence of blocks of an audio signal. </li><li>  Nick Palmer: <a href="http://palmnet.me.uk/uni/FYP/Audio%2520Fingerprinting.pdf">"Review of audio fingerprinting algorithms for a fingerprint generation</a> . <a href="http://palmnet.me.uk/uni/FYP/Audio%2520Fingerprinting.pdf">"</a>  The main interest in this work is a review of existing approaches to solving the described problem.  The stages of a possible implementation are also described. </li><li>  Shumeet Baluja, Michele Covell: <a href="http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/pubs/archive/33031.pdf">"Audio Fingerprinting: Combining Computer Vision &amp; Data Stream Processing</a> . <a href="http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/pubs/archive/33031.pdf">"</a>  The article, written by colleagues from Google, describes an approach based on wavelets using computer vision techniques. </li><li>  Arunan Ramalingam, Sridhar Krishnan: <a href="http://cecs.uci.edu/~papers/icme05/defevent/papers/cr1646.pdf">‚ÄúTransform Features For Audio Fingerprinting‚Äù</a> (2005).  In this article, it is proposed to describe a fragment of audio using a Gaussian mixture model on top of various attributes, such as Shannon entropy, Renyi entropy, spectolol centroids, meelpstral coefficients, and others.  Comparative recognition quality values ‚Äã‚Äãare given. </li><li>  Dalibor Mitrovic, Matthias Zeppelzauer, Christian Breiteneder: <a href="http://publik.tuwien.ac.at/files/PubDat_186351.pdf">"Features for Content-Based Audio Retrieval</a> . <a href="http://publik.tuwien.ac.at/files/PubDat_186351.pdf">"</a>  Survey work about audio signs: how to choose them, what properties they should have and what they exist. </li><li>  Natalia Miranda, Fabiana Piccoli: <a href="https://lc.fie.umich.mx/~camarena/78716_1.pdf">"Using a GPU to Speed ‚Äã‚ÄãUp"</a> .  The article proposes the use of a GPU to speed up signature calculations. </li><li>  Shuhei Hamawaki, Shintaro Funasawa, Jiro Katto, Hiromi Ishizaki, Keiichiro Hoashi, Yasuhiro Takishima: <a href="http://link.springer.com/content/pdf/10.1007%252F978-3-540-92892-8_32.pdf">"Feature Analysis and Normalization of the Rhythm for Music with Different Bit Rates."</a>  The article focuses on increasing the robustness of the audio signal representation based on the chalk-cepstral coefficients (MFCC).  For this, the method of normalization of cepstrum (CMN) is used. </li></ul></div><p>Source: <a href="https://habr.com/ru/post/181219/">https://habr.com/ru/post/181219/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../181207/index.html">Mataerial: 3D printer that prints on weight</a></li>
<li><a href="../181209/index.html">(Video) Demonstration at the stand: Extremely productive Extreme Networks</a></li>
<li><a href="../18121/index.html">PREG: conditional masks</a></li>
<li><a href="../181213/index.html">Is relational mapping of collections an alternative to object-relational mapping?</a></li>
<li><a href="../181217/index.html">Win32 / Syndicasec.A backdoor used in cyber espionage operation</a></li>
<li><a href="../18122/index.html">Banners</a></li>
<li><a href="../181223/index.html">Apple mobile devices in the corporate sector. Configuration profiles</a></li>
<li><a href="../181227/index.html">Pegman's Story in Google Street View</a></li>
<li><a href="../181231/index.html">Does the programmer need free * lyushki (version 2023)</a></li>
<li><a href="../181233/index.html">Using IAM Roles with Powershell AWS Utilities</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>