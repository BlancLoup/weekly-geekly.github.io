<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to distinguish shampoo from champignons, and skewers from champagne ... Elasticsearch - search for goods in store databases</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Task 


 One of the main tasks of the application for storing and analyzing purchases is to search for the same or very close products in the database...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to distinguish shampoo from champignons, and skewers from champagne ... Elasticsearch - search for goods in store databases</h1><div class="post__text post__text-html js-mediator-article"><h1 id="zadacha">  Task </h1><br><p>  One of the main tasks of the application for storing and analyzing purchases is to search for the same or very close products in the database, where unsuited and incomprehensible product names obtained from checks are collected.  There are two types of input request: </p><br><ol><li>  A specific name with abbreviations that can only be understood by cashiers at a local supermarket, or by avid shoppers. </li><li>  A query in natural language entered by the user into the search string. </li></ol><br><p>  Requests of the first type, as a rule, come from the products in the check itself, when the user needs to find cheaper products.  Our task is to select the most similar analogue of the goods from the check in other stores nearby.  Here it is important to choose the most appropriate brand of product and, if possible, volume. </p><br><p><img src="https://habrastorage.org/webt/yp/_d/ny/yp_dnyvhrlrrrr6v0pa_6sz_ibk.jpeg"></p><a name="habracut"></a><br><p>  The second type of request is a simple user request to search for a specific product in the nearest store.  The request can be a very general, non-uniquely describing product.  There may be slight deviations from the request.  For example, if a user is looking for milk at 3.2%, and in our database there is only milk at 2.5%, then we still want to show at least this result. </p><br><p><img src="https://habrastorage.org/webt/ar/0-/03/ar0-03doeo4rgkrbni26iz9rek4.jpeg"></p><br><h1 id="osobennosti-dataseta--problemy-dlya-resheniya">  Features dataset - problems to solve </h1><br><p>  The information in the product check is far from perfect.  There are many not always clear abbreviations, grammatical errors, misprints, various translites, Latin letters in the middle of the Cyrillic alphabet and character sets that make sense only for the internal organization in a particular store. <br>  For example, a baby apple-banana puree with cottage cheese can easily be written on the check like this: </p><br><p><img src="https://habrastorage.org/webt/kv/gg/23/kvgg23lcyv2h5gulck5k_9x1a-s.jpeg"></p><cut></cut><br><h1 id="o-tehnologii">  About technology </h1><br><p>  Elasticsearch is a fairly popular and affordable technology for implementing a search.  This is a search engine with JSON REST API using Lucene and written in Java.  The main advantages of Elastic are speed, scalability and fault tolerance.  Similar engines are used for complex search in the database of documents.  For example, search taking into account the morphology of the language or search by geo-coordinates. </p><br><h1 id="napravleniya-dlya-eksperimentov-i-uluchsheniy">  Directions for Experiments and Improvements </h1><br><p>  To understand how you can improve your search, you need to parse the search system into its component customizable components.  For our case, the structure of the system looks like this. </p><br><ol><li>  The input string for the search passes through the analyzer, which in a certain way splits the string into tokens - search units that are searched among data that is also stored as tokens. </li><li>  Then there is a direct search for these tokens for each document in the existing database.  After finding a token in a particular document (which is also represented in the database as a token set), its relevance is calculated according to the selected Similarity model (we will call it the relevance model).  This may be a simple <a href="https://en.wikipedia.org/wiki/Tf%25E2%2580%2593idf">TF / IDF</a> (Term Frequency - Inverse Document Frequency), and there may be other more complex or specific models. </li><li>  In the next step, the number of points scored by each individual token is aggregated in a certain way.  Aggregation parameters are specified by query semantics.  An example of such aggregations can be additional weights for certain tokens (added value), conditions for the mandatory presence of a token, and so on.  The result of this stage is Score - the final assessment of the relevance of a particular document from the database relative to the initial request. </li></ol><br><p><img src="https://habrastorage.org/webt/_u/yg/zp/_uygzp1ohyyhtksrfuwbfvmi3r0.jpeg"></p><br><p>  Three separately configurable components can be distinguished from the search device, each of which can be distinguished with its own ways and means of improvement. </p><br><ol><li>  Analyzers </li><li>  Similarity model </li><li>  Query-time improvements </li></ol><br><p>  Next, we consider each component separately and analyze the specific settings of parameters that helped improve the search in the case of product names. </p><br><h3 id="query-time-uluchsheniya">  Query-time improvements </h3><br><p>  To understand what we can improve in a query, we give an example of the initial query. </p><br><pre><code class="sql hljs">{ "query": { "multi_match": { "query": "  105", "type": "most_fields", "fields": ["name"], "minimum_should_match": "70%" } }, ‚Äúsize‚Äù: 100, ‚Äúmin_score‚Äù: 15 }</code> </pre> <br><p>  We use the most_fields query type, since we foresee the need for a combination of several analyzers for the ‚Äúproduct name‚Äù field.  This type of query allows you to combine search results for different attributes of an object containing the same text, analyzed in different ways.  An alternative to this approach is the use of best_fields or cross_fields queries, but they are not suitable for our case, since the search among the various attributes of the object is calculated (eg: name and description).  We also have the task to take into account different aspects of one attribute - the name of the product. </p><br><p>  What can be customized: </p><br><ul><li>  Weighted combination of various analyzers. <br>  Initially, all search elements have the same weight - and therefore, the same significance.  This can be changed by adding the 'boost' parameter, which takes numeric values.  If the parameter is greater than 1, the search element will have a greater impact on the results, and less than 1 will have a smaller impact. </li><li>  Separating analyzers into 'should' and 'must'. <br>  Namely, certain analyzers must match, and some must be optional, that is, insufficient.  In our case, an example of the benefits of such a separation can be a number analyzer.  If the product name in the query and the product name in the database match only the number, then this is not a sufficient condition for their equivalence.  We do not want to see such products as a result.  At the same time, if the request is ‚Äúcream 10%‚Äù, then we want all cream with 10% fat to have a big advantage over cream with 20% fat. </li><li>  Parameter minimum_should_match.  How many tokens must necessarily match in the request and the document from the database?  This parameter works in conjunction with the type of our request (most_fields) and checks the minimum number of matching tokens for each of the fields (in our case, for each analyzer). </li><li>  The min_score parameter.  Threshold, eliminating documents with insufficient points.  The snag is that there is no known maximum max.  The resulting Score depends on the specific request and on the specific database of documents.  Sometimes it can be 150, and sometimes 2, but both of these values ‚Äã‚Äãwill mean that the object from the database is relevant to the query.  We cannot compare the speeds of the results of different queries. <br><ul><li>  Constant. <br>  After sufficient observation of the total scores for different queries, an approximate limit can be found, after which the results become inappropriate for most queries.  This is the easiest, but also the most stupid solution that leads to poor-quality search. </li><li>  Attempt to analyze the resulting rates for a specific query after the search with the minimum or zero min_score. <br>  The idea is that after a certain point you can see a sharp jump in the direction of decreasing the speed.  It remains only to determine this jump in order to stop in time.  Such an approach would work well on such queries: <br><img src="https://habrastorage.org/webt/9z/qo/-u/9zqo-uovb_75ytduyti4byhmeyo.png"><br>  The jump can be found by statistical methods.  But, unfortunately, not all requests have this jump present and are easy to define. </li><li>  Calculate the ideal speed and set min_score as a certain percentage of the ideal, which can be done in two ways: <br><ul><li>  From the detailed description of the calculations provided by Elastic itself when setting the parameter explain: true.  This is a difficult task, requiring a comprehensive understanding of the query architecture and algorithms for computing used by Elastic. </li><li>  By a little tricks.  We receive a request, add a new product to our database with the same name, do a search and get the maximum speed.  Since there will be a 100% match in the name, the resulting value will be perfect.  It is this approach that we use in our system, as concerns about the high cost of this operation have not been confirmed regarding time. </li></ul></li></ul></li><li>  Change the scoring algorithm, which is responsible for the total value of relevance.  This may be taking into account the distance to the store (give more points to products that are closer), prices of goods (give more points to products with the most likely price) and so on. </li></ul><br><h3 id="analizatory">  Analyzers </h3><br><p>  The analyzer analyzes the input line in three stages and issues tokens at the output - search units: </p><br><p><img src="https://habrastorage.org/webt/cd/vc/u8/cdvcu8tw-9wsrgelal2ii0waeio.jpeg"></p><br><p>  First, changes occur at the character level of the string.  This can be a replacement, delete or add characters to the string.  Then the tokenizer comes into play, which is designed to divide the string into tokens.  Standard tokenizer splits a string into tokens according to punctuation marks.  The final step is the received tokens are filtered and processed. </p><br><p>  Consider what variations of stages have become useful in our case. </p><br><h5 id="char-filters">  Char filters </h5><br><ul><li>  According to the specifics of the Russian language, it would be useful to process such characters as nd and  and replace them with and and e, respectively. </li><li>  Transliteration is the transmission of characters from one script by characters from another.  In our case, this is the processing of names written in Latin or interspersed with both alphabets.  Transliteration can be implemented using a plug-in ( <a href="https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis-icu.html">ICU Analysis Plugin</a> ) as a token filter (that is, it does not process the original string, but the final tokens).  We decided to write our own transliteration, as we were not quite happy with the algorithm in the plugin.  The idea is to first replace the four-character set occurrences (eg, "SHCH =&gt; y"), then the three-character, etc. The order in which the character filters are applied is important, since the result will depend on the order. </li><li>  Replace the Latin c, surrounded by Cyrillic, with Russian c.  The need for this was revealed after analyzing the names in the database - very many names in Cyrillic included Latin c, which means Cyrillic c.  When as if the name is completely in Latin, then Latin c means Cyrillic k or c.  Therefore, before transliterating, it is necessary to replace the character c. </li><li>  Removing too large numbers from titles.  Sometimes in the names of the products there is an internal identification number (for example, 3387522 K.Ts. Sunflower oil 0.9 ml), which does not carry any meaning in the general case. </li><li>  Replace commas with dots.  Why do you need it?  So that numerals, such as milk fat 3.2 and 3.2, are equivalent </li></ul><br><h5 id="tokenizer">  Tokenizer </h5><br><ul><li>  Standard Tokenizer - splits lines according to space and punctuation marks (eg, "Twix Extra 2‚Äù -&gt; ‚ÄúTwix‚Äù, ‚ÄúExtra‚Äù, ‚Äú2‚Äù) </li><li>  EdgeNGram tokenizer - breaks each word into tokens of a given length (usually a range of numbers), starting with the first character (for example, for N = [3, 6]: "tweaks extra 2" -&gt; "twi", "tweak", ‚ÄúTwix‚Äù, ‚ÄúEx‚Äù, ‚ÄúExt‚Äù, ‚ÄúExtra‚Äù, ‚ÄúExtra‚Äù) </li><li>  Tokenizer for numbers - selects only numbers from a string by searching for a regular expression (eg, "Twix Extra 2 4.5" -&gt; "2", "4.5") </li></ul><br><h5 id="token-filter">  Token filter </h5><br><ul><li>  Lowercase filter </li><li>  Stemming filter - performs a stemming algorithm for each token.  Stemming is to determine the initial form of the word (eg, ‚Äúrice‚Äù -&gt; ‚Äúrice‚Äù) </li><li>  Phonetic analysis.  It is necessary in order to minimize the effect of typos and different ways of writing the same word on search results.  The table presents the various algorithms available for phonetic analysis and the result of their work in problem cases.  In the first case (Shampoo / champagne / champignon / champignons) success is determined by the generation of different encodings, in the rest - the same. </li></ul><br><p><img src="https://habrastorage.org/webt/gi/oi/x6/gioix68jytjyfxexdt3pvjpatze.png"></p><br><h3 id="similarity-model">  Similarity model </h3><br><p>  The relevance model is needed in order to determine to what extent the coincidence of one token affects the relevance of the object from the base with respect to the query.  Suppose if in the request and the product from the database the token 'for' coincides, this is certainly not bad, but it says little about the compliance of the product with the request.  Thus, the coincidence of different tokens carries different values.  In order to take this into account and need a relevancy model.  Elastic provides many different models.  If you understand the truth in them, then you can choose a very specific and suitable model for a specific case.  The choice can be based on the number of frequently used words (by the type of the same token 'for'), assessment of the importance of long tokens (longer means better? Worse? Doesn't matter?), What results we want to see at the end and so on.  Examples of models that are offered in Elastic are TF-IDF (the simplest and most understandable model), <a href="https://en.wikipedia.org/wiki/Okapi_BM25">Okapi BM25</a> (improved TF-IDF, the default model), Divergence from randomness, Divergence from independence and so on.  Each model also has customizable parameters.  After several experiments with the model, the Okapi BM25 default model showed the best result, but with different parameters from the pre-set ones. </p><br><h1 id="ispolzovanie-kategoriy">  Use of categories </h1><br><p>  In the course of the search operation, a very important additional information about the product became available - its category.  You can read more about how we implemented categorization in the article <a href="https://habr.com/post/430216/">How I understood that I eat a lot of sweets, or classification of goods by checks in the appendix</a> .  Until then, we based our search only on the comparison of the names of goods, it is now possible to compare the category of the request and the goods in the database. <br>  Possible options for using this information are a mandatory match on the category category (drawn up as a result filter), an additional advantage in the form of points for products with a matching category (as is the case with numbers) and sorting the results by category (first matching, then all the others).  For our case, the last option worked best.  This is due to the fact that our categorization algorithm is too good to use the second method, and not good enough to use the first one.  We are confident enough in the algorithm and want the coincidence in the category to be a big advantage.  If additional points are added to the score (the second method), goods with the same category will go up in the list, but they will still lose to some goods that have more matched in name.  However, the first method does not suit us either, since errors in categorization are still not excluded.  Sometimes a request may contain insufficient information to correctly determine the category, or there are too few products in this category in the nearest user radius.  In this case, we still want to show results with a different category, but still relevant to the product name. <br>  The second method is also good because it does not ‚Äúspoil‚Äù the speed of the products as a result of the search, and allows you to continue to use the calculated minimum rate without obstacles. <br>  The specific implementation of sorting can be seen in the final query. </p><br><h1 id="finalnaya-model">  Final model </h1><br><p>  The selection of the final search model included the generation of various search engines, their evaluation and comparison.  Most often, the comparison took place in one of the parameters.  Suppose in one particular case we needed to calculate the best size for the edgeNgram tokenizer (that is, to select the most efficient range N).  To do this, we generated the same search engines with only one difference in the size of this range.  After that it was possible to identify the best value for this parameter. <br>  The models were evaluated using the nDCG (normalized discounted cumulative gain) metric - a metric for assessing the quality of ranking.  Predefined queries were sent to each search model, after which the nDCG metric was calculated using the search results obtained. <br>  Analyzers that are included in the final model: </p><br><p><img src="https://habrastorage.org/webt/nb/-m/ti/nb-mtitufklquryy-5o3m2trrl4.jpeg"></p><br><p><img src="https://habrastorage.org/webt/5n/y3/6g/5ny36gkpfblbdtr5xvu8b4fx_r4.jpeg"></p><br><p><img src="https://habrastorage.org/webt/2r/er/zv/2rerzvbqvsdgezshkhpfeds2mc0.jpeg"></p><br><p><img src="https://habrastorage.org/webt/5h/tv/69/5htv691pabkoks90jq08h6nmdxo.jpeg"></p><br><p>  The default model (Okapi - BM25) with the parameter b = 0.5 was chosen as the relevance model.  The default value is 0.75.  This parameter shows to what extent the length of the product name normalizes the tf (term frequency) variable.  A smaller number in our case works better, since a long name very often does not affect the significance of a single word.  That is, the word ‚Äúchocolate‚Äù in the name ‚Äúchocolate with crushed hazelnuts in a package of 25pcs‚Äù does not lose its value because the name is rather long. </p><br><p>  The final query looks like this: </p><br><p><img src="https://habrastorage.org/webt/ol/9y/5n/ol9y5nfbppfwyxhqaskziiypuru.jpeg"></p><br><p>  Experimentally revealed the best combination of analyzers and weights. </p><br><p>  As a result of sorting, products with a matching category go to the beginning of the results, and then all the others.  Sorting by points (skoru) is stored within each subset. </p><br><p>  For simplicity, this query specifies a threshold for a score of 15, although in our system we use the calculated parameter that was described earlier. </p><br><h1 id="v-buduschem">  In future </h1><br><p>  There are thoughts to improve the search by solving one of the most embarrassing problems of our algorithm, which is the existence of a million and one different way to shorten a word of 5 letters.  It can be solved by first processing the names in order to reveal the abbreviations.  One of the solutions is an attempt to compare the abbreviated name from our database with one of the names from the database with ‚Äúcorrect‚Äù full names.  This decision meets its own specific obstacles, but it seems a promising change. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/433070/">https://habr.com/ru/post/433070/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../433058/index.html">Quintet as a basic entity for describing the subject area</a></li>
<li><a href="../433060/index.html">Why I do not believe microbench marks</a></li>
<li><a href="../433062/index.html">AXIS P1367 vs IDIS DC-B3303X: Compare CCTV Cameras</a></li>
<li><a href="../433064/index.html">Incident management: ‚Äúgive away cannot be left‚Äù or the art of placing commas</a></li>
<li><a href="../433066/index.html">HighLoad Cup # 2. Championship for backend-developers back in the ranks</a></li>
<li><a href="../433072/index.html">How to hacked the copy protection console Sega Dreamcast</a></li>
<li><a href="../433074/index.html">Switching to Kotlin in the Android project: Tips and Tricks</a></li>
<li><a href="../433076/index.html">How we made our Android Gallery library to view media content</a></li>
<li><a href="../433078/index.html">We write trading robots using the StockSharp graphic framework. Part 2</a></li>
<li><a href="../433080/index.html">Pocket OLAP in Javascript and IndexedDB Performance</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>