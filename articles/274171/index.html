<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Kaggle and Titanic - another solution using Python</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I want to share my experience with the task of the famous Kaggle machine learning competition. This competition is positioned as a competition for beg...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Kaggle and Titanic - another solution using Python</h1><div class="post__text post__text-html js-mediator-article">  I want to share my experience with the task of the <a href="https://www.kaggle.com/c/titanic">famous</a> Kaggle machine learning competition.  This competition is positioned as a competition for beginners, and I just had almost no practical experience in this area.  I knew a bit of theory, but I almost didn‚Äôt deal with real data and did not work closely with python.  In the end, after spending a couple of New Year's Eve evenings, I scored 0.80383 (the first quarter of the rating). <br><br><img src="https://habrastorage.org/files/fb4/856/19f/fb485619f6a24cd791db6792433c1bf0.jpeg"><br><br><a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1>  Titanic </h1><br><br>  We include suitable for the work of music and begin the study. <br><br><iframe width="420" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/nXOE6EQtKcU%3Ffeature%3Doembed&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhifcnZ1RRZxSwA4FNms7goVWXbY4g" frameborder="0" allowfullscreen=""></iframe><br><br>  The contest "on the Titanic" has already been repeatedly noted on Habr√©.  I would especially like to mention the latest article from the list - it turns out that researching data can be no less intriguing than a good detective novel, and an excellent result (0.81340) can be obtained not only by the Random Forest classifier. <br><br><ul><li>  <a href="http://habrahabr.ru/company/microsoft/blog/268039/">habrahabr.ru/company/microsoft/blog/268039</a> - Predicting the survival of passengers of the Titanic using Azure Machine Learning </li><li>  <a href="http://habrahabr.ru/post/165001/">habrahabr.ru/post/165001</a> - Data Mining: Primary data processing using DBMS </li><li>  <a href="http://habrahabr.ru/company/mlclass/blog/248779/">habrahabr.ru/company/mlclass/blog/248779</a> - When there is really a lot of data: Vowpal Wabbit </li><li>  <a href="http://habrahabr.ru/post/272201/">habrahabr.ru/post/272201</a> - The steady beauty of indecent models </li><li>  <a href="http://habrahabr.ru/post/202090/">habrahabr.ru/post/202090</a> - Basics of data analysis in python using pandas + sklearn </li><li>  <a href="http://habrahabr.ru/company/mlclass/blog/270973/">habrahabr.ru/company/mlclass/blog/270973</a> - Titanic on Kaggle: you do not finish reading this post to the end </li></ul><br><br>  I would also like to note an article about another competition.  From it you can understand exactly how the brain of the researcher should work and that most of the time should be devoted to preliminary analysis and data processing. <br><br><ul><li>  <a href="http://habrahabr.ru/post/270367/">habrahabr.ru/post/270367</a> - How I won the competition BigData from Beeline </li></ul><br><br><h1>  Tools </h1><br><br>  To solve the problem, I use the Python-technology stack.  This approach is not the only possible one: there are R, Matlab, Mathematica, Azure Machine Learning, Apache Weka, Java-ML and I think the list can be continued for a long time.  Using Python has a number of advantages: there are really a lot of libraries and they are of excellent quality, and since most of them are wrappers over C-code, they are also quite fast.  In addition, the constructed model can be easily put into operation. <br><br>  I must admit that I am not a very big fan of scripting non-strictly typed languages, but the wealth of libraries for python does not allow it to be ignored in any way. <br><br>  We will run everything under Linux (Ubuntu 14.04).  You need: python 2.7, seaborn, matplotlib, sklearn, xgboost, pandas.  In general, only pandas and sklearn are required, and the rest are needed for illustration. <br><br>  Under Linux, libraries for Python can be installed in two ways: by the regular package manager (deb) manager or via the Python utility pip. <br><br>  Installing deb packages is easier and faster, but often the libraries are outdated there (stability is above all). <br><br><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#     /usr/lib/python2.7/dist-packages/ $ sudo apt-get install python-matplotlib</span></span></code> </pre> <br><br>  Installing packages via pip is longer (it will be compiled), but with it you can count on getting fresh versions of packages. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#     /usr/local/lib/python2.7/dist-packages/ $ sudo pip install matplotlib</span></span></code> </pre><br><br>  So how is it better to install packages?  I use a compromise: I put massive and requiring multiple dependencies to build NumPy and SciPy from DEB-packages. <br><br><pre> <code class="bash hljs">$ sudo apt-get install python $ sudo apt-get install python-pip $ sudo apt-get install python-numpy $ sudo apt-get install python-scipy $ sudo apt-get install ipython</code> </pre><br><br>  And the rest, lighter packages, install via pip. <br><br><pre> <code class="bash hljs">$ sudo pip install pandas $ sudo pip install matplotlib==1.4.3 $ sudo pip install skimage $ sudo pip install sklearn $ sudo pip install seaborn $ sudo pip install statsmodels $ sudo pip install xgboost</code> </pre><br><br>  If I have forgotten something, then all the necessary packages are usually easily calculated and installed in a similar way. <br><br>  Users of other platforms need to take similar steps to install packages.  But there is a much simpler option: there are already precompiled distributions with python and almost all the necessary libraries.  I have not tried them myself, but at first glance they look promising. <br><br><ul><li>  <a href="https://www.continuum.io/downloads">www.continuum.io/downloads</a> - Anaconda </li><li>  <a href="https://www.enthought.com/products/canopy/">www.enthought.com/products/canopy</a> - Canopy </li></ul><br><br><h1>  Data </h1><br><br>  Download the <a href="https://www.kaggle.com/c/titanic/data">source data</a> for the problem and see what we were given. <br><br><pre> <code class="bash hljs">$ wc -l train.csv test.csv 892 train.csv 419 test.csv 1311 total</code> </pre><br><br>  Frankly speaking, we don‚Äôt have a lot of data - only 891 passengers in the train-sample, and 418 in the test-sample (one line goes to the header with a list of fields). <br><br>  Open train.csv in any tabular processor (I use LibreOffice Calc) to visually see the data. <br><br><pre> <code class="bash hljs">$ libreoffice --calc train.csv</code> </pre><br><br>  We see the following: <br><ul><li>  Not everyone's age is filled </li><li>  Tickets have a strange and inconsistent format. </li><li>  Names have a title (miss, mr, mrs, etc.) </li><li>  There are very few people who have cabin numbers ( <a href="http://habrahabr.ru/company/mlclass/blog/270973/">there</a> is a chilling story about why) </li><li>  In the existing rooms of the cabins, the deck code is apparently registered (as <a href="http://habrahabr.ru/company/mlclass/blog/270973/">it turned out</a> ) </li><li>  Also, according to the <a href="http://habrahabr.ru/company/mlclass/blog/270973/">article</a> , the side is encrypted in the cabin room. </li><li>  Sort by name.  It is evident that many traveled in families, and the scale of the tragedy is visible - often the families were separated, only a part survived. </li><li>  Sort by ticket.  It can be seen that several people traveled along the same ticket code at once, often with different surnames.  A quick glance seems to show that people with the same ticket number often share the same fate. </li><li>  Some passengers do not have a landing port </li></ul><br><br>  It seems about everything is clear, we go directly to working with data. <br><br><h1>  Data loading </h1><br><br>  In order not to make noise, the next code will immediately cite all used imports: <br><div class="spoiler">  <b class="spoiler_title">Script header</b> <div class="spoiler_text">  # coding = utf8 <br><br>  import pandas as pd <br>  import numpy as np <br>  import matplotlib.pyplot as plt <br>  import xgboost as xgb <br>  import re <br>  import seaborn as sns <br>  from sklearn.linear_model import LinearRegression <br>  from sklearn.linear_model import LogisticRegression <br>  from sklearn.linear_model import SGDClassifier <br>  from sklearn.ensemble import RandomForestClassifier <br>  from sklearn.grid_search import GridSearchCV <br>  from sklearn.neighbors import KNeighborsClassifier <br>  from sklearn.naive_bayes import GaussianNB <br>  from sklearn.svm import SVC <br>  from sklearn.feature_selection import SelectKBest, f_classif <br>  from sklearn.cross_validation import StratifiedKFold <br>  from sklearn.cross_validation import KFold <br>  from sklearn.cross_validation import cross_val_score <br>  from sklearn.preprocessing import StandardScaler <br>  from sklearn import metrics <br><br>  pd.set_option ('display.width', 256) <br></div></div><br><br>  Probably most of my motivation for writing this article is caused by the enthusiasm from working with the <a href="http://pandas.pydata.org/">pandas</a> package.  I knew about the existence of this technology, but I could not even imagine how pleasant it was to work with it.  Pandas is Excel on the command line with convenient I / O functionality and tabular data processing. <br><br>  Load both selections. <br><br><pre> <code class="python hljs">train_data = pd.read_csv(<span class="hljs-string"><span class="hljs-string">"data/train.csv"</span></span>) test_data = pd.read_csv(<span class="hljs-string"><span class="hljs-string">"data/test.csv"</span></span>)</code> </pre><br><br>  We collect both samples (train-sample and test-sample) into one total all-sample. <br><br><pre> <code class="python hljs">all_data = pd.concat([train_data, test_data])</code> </pre><br><br>  Why do this, because in the test sample there is no field with a resulting survival flag?  The complete sample is useful for calculating statistics for all other fields (averages, medians, quantiles, minima and maxima), as well as the relationships between these fields.  That is, considering statistics only for train-sampling, we actually ignore some very useful information for us. <br><br><h1>  Data analysis </h1><br><br>  Data analysis in Python can be done in several ways at once, for example: <br><br><ul><li>  Manually prepare data and output via <a href="http://matplotlib.org/">matplotlib</a> </li><li>  Use everything ready in <a href="http://stanford.edu/~mwaskom/software/seaborn/">seaborn</a> </li><li>  Use text output with grouping in <a href="http://pandas.pydata.org/">pandas</a> </li></ul><br><br>  We will try all three, but first we will launch the simplest text version.  We derive survival statistics depending on the class and gender. <br><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">"===== survived by class and sex"</span></span>) print(train_data.groupby([<span class="hljs-string"><span class="hljs-string">"Pclass"</span></span>, <span class="hljs-string"><span class="hljs-string">"Sex"</span></span>])[<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>].value_counts(normalize=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>))</code> </pre><br><div class="spoiler">  <b class="spoiler_title">Result</b> <div class="spoiler_text"><pre> ===== survived by class and sex
 Pclass Sex Survived
 1 female 1 0.968085
                 0 0.031915
         male 0 0.631148
                 1 0.368852
 2 female 1 0.921053
                 0 0.078947
         male 0 0.842593
                 1 0.157407
 3 female 0 0.500000
                 1 0.500000
         male 0 0.864553
                 1 0.135447
 dtype: float64
</pre><br></div></div><br><br>  We see that women were first planted into the boats first ‚Äî the woman‚Äôs survival rate is 96.8%, 92.1% and 50% depending on the class of the ticket.  The chance of a man‚Äôs survival is much lower and amounts to 36.9%, 15.7% and 13.5% respectively. <br><br>  With the help of pandas, we quickly calculate a summary of all the numerical fields of both samples - separately for men and for women. <br><br><pre> <code class="python hljs">describe_fields = [<span class="hljs-string"><span class="hljs-string">"Age"</span></span>, <span class="hljs-string"><span class="hljs-string">"Fare"</span></span>, <span class="hljs-string"><span class="hljs-string">"Pclass"</span></span>, <span class="hljs-string"><span class="hljs-string">"SibSp"</span></span>, <span class="hljs-string"><span class="hljs-string">"Parch"</span></span>] print(<span class="hljs-string"><span class="hljs-string">"===== train: males"</span></span>) print(train_data[train_data[<span class="hljs-string"><span class="hljs-string">"Sex"</span></span>] == <span class="hljs-string"><span class="hljs-string">"male"</span></span>][describe_fields].describe()) print(<span class="hljs-string"><span class="hljs-string">"===== test: males"</span></span>) print(test_data[test_data[<span class="hljs-string"><span class="hljs-string">"Sex"</span></span>] == <span class="hljs-string"><span class="hljs-string">"male"</span></span>][describe_fields].describe()) print(<span class="hljs-string"><span class="hljs-string">"===== train: females"</span></span>) print(train_data[train_data[<span class="hljs-string"><span class="hljs-string">"Sex"</span></span>] == <span class="hljs-string"><span class="hljs-string">"female"</span></span>][describe_fields].describe()) print(<span class="hljs-string"><span class="hljs-string">"===== test: females"</span></span>) print(test_data[test_data[<span class="hljs-string"><span class="hljs-string">"Sex"</span></span>] == <span class="hljs-string"><span class="hljs-string">"female"</span></span>][describe_fields].describe())</code> </pre><br><div class="spoiler">  <b class="spoiler_title">Result</b> <div class="spoiler_text"><pre> ===== train: males
               Age Fare Pclass SibSp Parch
 count 453.000000 577.000000 577.000000 577.000000 577.000000
 mean 30.726645 25.523893 2.389948 0.429809 0.235702
 std 14.678201 43.138263 0.813580 1.061811 0.612294
 min 0.420000 0.000000 1.000000 0.000000 0.000000
 25% 21.000000 7.895800 2.000000 0.000000 0.000000
 50% 29.000000 10.500000 3.000000 0.000000
 75% 39.000000 26.550000 3.000000 0.000000 0.000000
 max 80.000000 512.329200 3.000000 8.000000 5.000000
 ===== test: males
               Age Fare Pclass SibSp Parch
 count 205.000000 265.000000 266.000000 266.000000 266.000000
 mean 30.272732 27.527877 2.334586 0.379699 0.274436
 std 13.389528 41.079423 0.808497 0.843735 0.883745
 min 0.330000 0.000000 1.000000 0.000000 0.000000
 25% 22.000000 7.854200 2.000000 0.000000
 50% 27.000000 13.000000 3.000000 0.000000 0.000000
 75% 40.000000 26.550000 3.000000 1.000000 0.000000
 max 67.000000 262.375000 3.000000 8.000000 9.000000
 ===== train: females
               Age Fare Pclass SibSp Parch
 count 261.000000 314.000000 314.000000 314.000000 314.000000
 mean 27.915709 44.479818 2.159236 0.694268 0.649682
 std 11.110146 57.997698 0.857290 1.156520 1.022846
 min 0.750000 6.750000 1.000000 0.000000 0.000000
 25% 18.000000 12.071875 1.000000 0.000000
 50% 27.000000 23.000000 2.000000 0.000000
 75% 37.000000 55.000000 3.000000 1.000000 1.000000
 max 63.000000 512.329200 3.000000 8.000000 6.000000
 ===== test: females
               Age Fare Pclass SibSp Parch
 127.000000 152.000000 152.000000 152.000000 152.000000
 mean 30.272362 49.747699 2.144737 0.565789 0.598684
 std 15.428613 73.108716 0.887051 0.974313 1.105434
 min 0.170000 6.950000 1.000000 0.000000 0.000000
 25% 20.500000 8.626050 1.000000 0.000000
 50% 27.000000 21.512500 2.000000 0.000000
 75% 38.500000 55.441700 3.000000 1.000000 1.000000
 max 76.000000 512.329200 3.000000 8.000000 9.000000
</pre><br></div></div><br><br>  It is seen that in the middle and percentiles everything is completely flat.  But for men, samples differ in maximums by age and by ticket price.  Women in both samples also have a difference in the maximum age. <br><br><h1>  Data Digest Build </h1><br><br>  Let's collect a small digest for the full sample - it will be needed for the further conversion of the samples.  In particular, we need the values ‚Äã‚Äãthat will be substituted for the missing ones, as well as various reference books for translating text values ‚Äã‚Äãinto numeric values.  The fact is that many classifiers can work only with numbers, so somehow we have to translate categorical attributes into numeric ones, but regardless of the way of conversion, we will need reference books of these values. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">DataDigest</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.ages = <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> self.fares = <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> self.titles = <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> self.cabins = <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> self.families = <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> self.tickets = <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_title</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(name)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> pd.isnull(name): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-string"><span class="hljs-string">"Null"</span></span> title_search = re.search(<span class="hljs-string"><span class="hljs-string">' ([A-Za-z]+)\.'</span></span>, name) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> title_search: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> title_search.group(<span class="hljs-number"><span class="hljs-number">1</span></span>).lower() <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-string"><span class="hljs-string">"None"</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_family</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(row)</span></span></span><span class="hljs-function">:</span></span> last_name = row[<span class="hljs-string"><span class="hljs-string">"Name"</span></span>].split(<span class="hljs-string"><span class="hljs-string">","</span></span>)[<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> last_name: family_size = <span class="hljs-number"><span class="hljs-number">1</span></span> + row[<span class="hljs-string"><span class="hljs-string">"Parch"</span></span>] + row[<span class="hljs-string"><span class="hljs-string">"SibSp"</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> family_size &gt; <span class="hljs-number"><span class="hljs-number">3</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-string"><span class="hljs-string">"{0}_{1}"</span></span>.format(last_name.lower(), family_size) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-string"><span class="hljs-string">"nofamily"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-string"><span class="hljs-string">"unknown"</span></span> data_digest = DataDigest() data_digest.ages = all_data.groupby(<span class="hljs-string"><span class="hljs-string">"Sex"</span></span>)[<span class="hljs-string"><span class="hljs-string">"Age"</span></span>].median() data_digest.fares = all_data.groupby(<span class="hljs-string"><span class="hljs-string">"Pclass"</span></span>)[<span class="hljs-string"><span class="hljs-string">"Fare"</span></span>].median() data_digest.titles = pd.Index(test_data[<span class="hljs-string"><span class="hljs-string">"Name"</span></span>].apply(get_title).unique()) data_digest.families = pd.Index(test_data.apply(get_family, axis=<span class="hljs-number"><span class="hljs-number">1</span></span>).unique()) data_digest.cabins = pd.Index(test_data[<span class="hljs-string"><span class="hljs-string">"Cabin"</span></span>].fillna(<span class="hljs-string"><span class="hljs-string">"unknown"</span></span>).unique()) data_digest.tickets = pd.Index(test_data[<span class="hljs-string"><span class="hljs-string">"Ticket"</span></span>].fillna(<span class="hljs-string"><span class="hljs-string">"unknown"</span></span>).unique())</code> </pre><br><br>  A little explanation of the digest fields: <br><ul><li>  ages - directory of medians of ages by sex; </li><li>  fares - a reference book of medians of ticket prices depending on the class of the ticket; </li><li>  titles - directory titles; </li><li>  families ‚Äî a directory of family identifiers (last name + number of family members); </li><li>  cabins - directory of cabin identifiers; </li><li>  tickets - directory of ticket identifiers. </li></ul><br><br>  We build reference books for recovering missing data (medians) using a combined sample.  But reference books for the translation of categorical signs - only for test data.  The idea was the following: let's say we have the last name ‚ÄúIvanov‚Äù in the train-set, but there is no this name in the test-set.  The knowledge inside the classifier that ‚ÄúIvanov‚Äù survived (or did not survive) does not help in the evaluation of the test set, since this name still does not exist in the test set.  Therefore, in the directory add only those names that are in the test-set.  An even more correct way would be to add only the intersection of signs to the directory (only those signs that are in both sets) - I tried, but the verification result deteriorated by 3 percent. <br><br><h1>  Select the signs </h1><br><br>  Now we need to highlight the signs.  As already mentioned, many classifiers can only work with numbers, so we need: <br><br><ul><li>  Convert categories to numeric representation </li><li>  Select implicit signs, that is, those that are not explicitly given (title, deck) </li><li>  Do something with missing values </li></ul><br><br>  There are two ways to convert a categorical feature into a numeric one.  We can consider the problem on the example of the passenger floor. <br><br>  In the first version, we simply change the floor to a certain number, for example, we can replace female with 0, and male with 1 ( <em>roundwheel and wand - very convenient to remember</em> ).  This option does not increase the number of signs, however, the ‚Äúmore‚Äù and ‚Äúless‚Äù relation now appears inside the sign for its values.  In the case when there are many values, such an unexpected property of the feature is not always desirable and can lead to problems in geometric classifiers. <br><br>  The second conversion option is to have two columns, ‚Äúsex_male‚Äù and ‚Äúsex_female‚Äù.  In the case of a male, we will assign sex_male = 1, sex_female = 0.  In the case of the female, vice versa: sex_male = 0, sex_female = 1.  We now avoid ‚Äúmore‚Äù / ‚Äúless‚Äù relations, but now we have more signs, and the more signs, the more data we need to train the classifier ‚Äî this problem is known as the <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D1%2580%25D0%25BE%25D0%25BA%25D0%25BB%25D1%258F%25D1%2582%25D0%25B8%25D0%25B5_%25D1%2580%25D0%25B0%25D0%25B7%25D0%25BC%25D0%25B5%25D1%2580%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D0%25B8">‚Äúcurse of dimensionality‚Äù</a> .  Especially difficult is the situation when there are a lot of attribute values, for example ticket IDs, in such cases, for example, you can fold back rarely occurring values ‚Äã‚Äãby substituting some special tag instead of them - thus reducing the total number of attributes after the extension. <br><br>  A small spoiler: we bet first on the Random Forest classifier.  Firstly, <em>everyone does this</em> , and secondly, it does not require expansion of features, is resistant to the scale of feature values ‚Äã‚Äãand is calculated quickly.  Despite this, we are preparing the signs in a general universal form, since the main goal set before us is to explore the principles of working with sklearn and possibilities. <br><br>  Thus, we replace some categorical signs with numbers, some expand, some and replace and expand.  We do not save on the number of signs, because in the future we can always choose which ones will be involved in the work. <br><br>  In most manuals and examples from the network, the original data sets are very freely modified: the original columns are replaced with new values, unnecessary columns are deleted, etc.  There is no need for this as long as we have a sufficient amount of RAM: it is always better to add new features to the set without altering the existing data, since later pandas will always allow us to select only the ones we need. <br><br>  Create a method for converting datasets. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_index</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(item, index)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> pd.isnull(item): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">-1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">try</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> index.get_loc(item) <span class="hljs-keyword"><span class="hljs-keyword">except</span></span> KeyError: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">-1</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">munge_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data, digest)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># Age -         data["AgeF"] = data.apply(lambda r: digest.ages[r["Sex"]] if pd.isnull(r["Age"]) else r["Age"], axis=1) # Fare -         data["FareF"] = data.apply(lambda r: digest.fares[r["Pclass"]] if pd.isnull(r["Fare"]) else r["Fare"], axis=1) # Gender -  genders = {"male": 1, "female": 0} data["SexF"] = data["Sex"].apply(lambda s: genders.get(s)) # Gender -  gender_dummies = pd.get_dummies(data["Sex"], prefix="SexD", dummy_na=False) data = pd.concat([data, gender_dummies], axis=1) # Embarkment -  embarkments = {"U": 0, "S": 1, "C": 2, "Q": 3} data["EmbarkedF"] = data["Embarked"].fillna("U").apply(lambda e: embarkments.get(e)) # Embarkment -  embarkment_dummies = pd.get_dummies(data["Embarked"], prefix="EmbarkedD", dummy_na=False) data = pd.concat([data, embarkment_dummies], axis=1) #     data["RelativesF"] = data["Parch"] + data["SibSp"] # -? data["SingleF"] = data["RelativesF"].apply(lambda r: 1 if r == 0 else 0) # Deck -  decks = {"U": 0, "A": 1, "B": 2, "C": 3, "D": 4, "E": 5, "F": 6, "G": 7, "T": 8} data["DeckF"] = data["Cabin"].fillna("U").apply(lambda c: decks.get(c[0], -1)) # Deck -  deck_dummies = pd.get_dummies(data["Cabin"].fillna("U").apply(lambda c: c[0]), prefix="DeckD", dummy_na=False) data = pd.concat([data, deck_dummies], axis=1) # Titles -  title_dummies = pd.get_dummies(data["Name"].apply(lambda n: get_title(n)), prefix="TitleD", dummy_na=False) data = pd.concat([data, title_dummies], axis=1) #         -1      (  ) data["CabinF"] = data["Cabin"].fillna("unknown").apply(lambda c: get_index(c, digest.cabins)) data["TitleF"] = data["Name"].apply(lambda n: get_index(get_title(n), digest.titles)) data["TicketF"] = data["Ticket"].apply(lambda t: get_index(t, digest.tickets)) data["FamilyF"] = data.apply(lambda r: get_index(get_family(r), digest.families), axis=1) #   age_bins = [0, 5, 10, 15, 20, 25, 30, 40, 50, 60, 70, 80, 90] data["AgeR"] = pd.cut(data["Age"].fillna(-1), bins=age_bins).astype(object) return data</span></span></code> </pre><br><br>  A small explanation of the addition of new features: <br><ul><li>  add our own cabin index </li><li>  add our own deck index (cut out from the cabin room) </li><li>  add your own ticket index </li><li>  add your own title index (cut out from the name) </li><li>  add own index of family identifier (form from family name and number of family) </li></ul><br><br>  In general, we add to the signs in general everything that comes to mind.  It can be seen that some signs duplicate each other (for example, expansion and replacement of gender), some clearly correlate with each other (ticket class and ticket price), some are clearly meaningless (the port of landing is unlikely to affect survival).  We will deal with all this later - when we make the selection of signs for training. <br><br>  Let's transform both available sets and also create a combined set again. <br><br><pre> <code class="python hljs">train_data_munged = munge_data(train_data, data_digest) test_data_munged = munge_data(test_data, data_digest) all_data_munged = pd.concat([train_data_munged, test_data_munged])</code> </pre><br><br>  Although we are aiming at using Random Forest, I want to try other classifiers.  And with them there is the following problem: many classifiers are sensitive to the scale of features.  In other words, if we have one attribute with values ‚Äã‚Äãfrom [‚Äì10.5] and a second characteristic with values ‚Äã‚Äã[0, 0,000], then the same percentage error on both signs will lead to a large difference in absolute value and the classifier will interpret the second characteristic as more important. <br><br>  To avoid this, we reduce all numeric (and we no longer have any other) signs to the same scale [-1,1] and zero mean value.  To do this in sklearn can be very simple. <br><br><pre> <code class="python hljs">scaler = StandardScaler() scaler.fit(all_data_munged[predictors]) train_data_scaled = scaler.transform(train_data_munged[predictors]) test_data_scaled = scaler.transform(test_data_munged[predictors])</code> </pre><br><br>  First, we calculate the scaling factors (the complete set again came in handy), and then we scale both sets individually. <br><br><h1>  Feature selection </h1><br><br>  Well, the moment has come when we can select those signs with which we will work further. <br><br><pre> <code class="python hljs">predictors = [<span class="hljs-string"><span class="hljs-string">"Pclass"</span></span>, <span class="hljs-string"><span class="hljs-string">"AgeF"</span></span>, <span class="hljs-string"><span class="hljs-string">"TitleF"</span></span>, <span class="hljs-string"><span class="hljs-string">"TitleD_mr"</span></span>, <span class="hljs-string"><span class="hljs-string">"TitleD_mrs"</span></span>, <span class="hljs-string"><span class="hljs-string">"TitleD_miss"</span></span>, <span class="hljs-string"><span class="hljs-string">"TitleD_master"</span></span>, <span class="hljs-string"><span class="hljs-string">"TitleD_ms"</span></span>, <span class="hljs-string"><span class="hljs-string">"TitleD_col"</span></span>, <span class="hljs-string"><span class="hljs-string">"TitleD_rev"</span></span>, <span class="hljs-string"><span class="hljs-string">"TitleD_dr"</span></span>, <span class="hljs-string"><span class="hljs-string">"CabinF"</span></span>, <span class="hljs-string"><span class="hljs-string">"DeckF"</span></span>, <span class="hljs-string"><span class="hljs-string">"DeckD_U"</span></span>, <span class="hljs-string"><span class="hljs-string">"DeckD_A"</span></span>, <span class="hljs-string"><span class="hljs-string">"DeckD_B"</span></span>, <span class="hljs-string"><span class="hljs-string">"DeckD_C"</span></span>, <span class="hljs-string"><span class="hljs-string">"DeckD_D"</span></span>, <span class="hljs-string"><span class="hljs-string">"DeckD_E"</span></span>, <span class="hljs-string"><span class="hljs-string">"DeckD_F"</span></span>, <span class="hljs-string"><span class="hljs-string">"DeckD_G"</span></span>, <span class="hljs-string"><span class="hljs-string">"FamilyF"</span></span>, <span class="hljs-string"><span class="hljs-string">"TicketF"</span></span>, <span class="hljs-string"><span class="hljs-string">"SexF"</span></span>, <span class="hljs-string"><span class="hljs-string">"SexD_male"</span></span>, <span class="hljs-string"><span class="hljs-string">"SexD_female"</span></span>, <span class="hljs-string"><span class="hljs-string">"EmbarkedF"</span></span>, <span class="hljs-string"><span class="hljs-string">"EmbarkedD_S"</span></span>, <span class="hljs-string"><span class="hljs-string">"EmbarkedD_C"</span></span>, <span class="hljs-string"><span class="hljs-string">"EmbarkedD_Q"</span></span>, <span class="hljs-string"><span class="hljs-string">"FareF"</span></span>, <span class="hljs-string"><span class="hljs-string">"SibSp"</span></span>, <span class="hljs-string"><span class="hljs-string">"Parch"</span></span>, <span class="hljs-string"><span class="hljs-string">"RelativesF"</span></span>, <span class="hljs-string"><span class="hljs-string">"SingleF"</span></span>]</code> </pre><br><br>  Just put a comment on unnecessary and start training.  What exactly is not needed - <em>you decide</em> . <br><br><h1>  Once again the analysis </h1><br><br>  Since now we have a column in which the range is recorded in which the age of the passenger falls, we estimate the survival rate depending on the age (range). <br><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">"===== survived by age"</span></span>) print(train_data.groupby([<span class="hljs-string"><span class="hljs-string">"AgeR"</span></span>])[<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>].value_counts(normalize=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) print(<span class="hljs-string"><span class="hljs-string">"===== survived by gender and age"</span></span>) print(train_data.groupby([<span class="hljs-string"><span class="hljs-string">"Sex"</span></span>, <span class="hljs-string"><span class="hljs-string">"AgeR"</span></span>])[<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>].value_counts(normalize=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) print(<span class="hljs-string"><span class="hljs-string">"===== survived by class and age"</span></span>) print(train_data.groupby([<span class="hljs-string"><span class="hljs-string">"Pclass"</span></span>, <span class="hljs-string"><span class="hljs-string">"AgeR"</span></span>])[<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>].value_counts(normalize=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>))</code> </pre><br><div class="spoiler">  <b class="spoiler_title">Result</b> <div class="spoiler_text"><pre> ===== survived by age
 AgeR Survived
 (0, 5] 1 0.704545
           0 0.295455
 (10, 15] 1 0.578947
           0 0.421053
 (15, 20] 0 0.656250
           1 0.343750
 (20, 25] 0 0.655738
           1 0.344262
 (25, 30] 0 0.611111
           1 0.388889
 (30, 40] 0 0.554839
           1 0.445161
 (40, 50] 0 0.616279
           1 0.383721
 (5, 10] 0 0.650000
           1 0.350000
 (50, 60] 0 0.595238
           1 0.404762
 (60, 70] 0 0.764706
           1 0.235294
 (70, 80] 0 0.800000
           1 0.200000
 dtype: float64
 ===== survived by gender and age
 Sex AgeR Survived
 female (0, 5] 1 0.761905
                   0 0.238095
         (10, 15] 1 0.750000
                   0 0.250000
         (15, 20] 1 0.735294
                   0 0.264706
         (20, 25] 1 0.755556
                   0 0.244444
         (25, 30] 1 0.750000
                   0 0.250000
         (30, 40] 1 0.836364
                   0 0.163636
         (40, 50] 1 0.677419
                   0 0.322581
         (5, 10] 0 0.700000
                   1 0.300000
         (50, 60] 1 0.928571
                   0 0.071429
         (60, 70] 1 1.000000
 male (0, 5] 1 0.652174
                   0 0.347826
         (10, 15] 0 0.714286
                   1 0.285714
         (15, 20] 0 0.870968
                   1 0.129032
         (20, 25] 0 0.896104
                   1 0.103896
         (25, 30] 0 0.791667
                   1 0.208333
         (30, 40] 0 0.770000
                   1 0.230000
         (40, 50] 0 0.781818
                   1 0.218182
         (5, 10] 0 0.600000
                   1 0.400000
         (50, 60] 0 0.857143
                   1 0.142857
         (60, 70] 0 0.928571
                   1 0.071429
         (70, 80] 0 0.800000
                   1 0.200000
 dtype: float64
 ===== survived by class and age
 Pclass AgeR Survived
 1 (0, 5] 1 0.666667
                   0 0.333333
         (10, 15] 1 1.000000
         (15, 20] 1 0.800000
                   0 0.200000
         (20, 25] 1 0.761905
                   0 0.238095
         (25, 30] 1 0.684211
                   0 0.315789
         (30, 40] 1 0.755102
                   0 0.244898
         (40, 50] 1 0.567568
                   0 0.432432
         (50, 60] 1 0.600000
                   0 0.400000
         (60, 70] 0 0.818182
                   1 0.181818
         (70, 80] 0 0.666667
                   1 0.333333
 2 (0, 5] 1 1.000000
         (10, 15] 1 1.000000
         (15, 20] 0 0.562500
                   1 0.437500
         (20, 25] 0 0.600000
                   1 0.400000
         (25, 30] 0 0.580645
                   1 0.419355
         (30, 40] 0 0.558140
                   1 0.441860
         (40, 50] 1 0.526316
                   0 0.473684
         (5, 10] 1 1.000000
         (50, 60] 0 0.833333
                   1 0.166667
         (60, 70] 0 0.666667
                   1 0.333333
 3 (0, 5] 1 0.571429
                   0 0.428571
         (10, 15] 0 0.571429
                   1 0.428571
         (15, 20] 0 0.784615
                   1 0.215385
         (20, 25] 0 0.802817
                   1 0.197183
         (25, 30] 0 0.724138
                   1 0.275862
         (30, 40] 0 0.793651
                   1 0.206349
         (40, 50] 0 0.933333
                   1 0.066667
         (5, 10] 0 0.812500
                   1 0.187500
         (50, 60] 0 1.000000
         (60, 70] 0 0.666667
                   1 0.333333
         (70, 80] 0 1.000000
 dtype: float64
</pre><br></div></div><br><br>  We see that the chances of survival are great for children under 5 years old, and already in old age the chance to survive decreases with age.  But this does not apply to women - a woman has a great chance of survival at any age. <br><br>  Let's try visualization from <a href="http://stanford.edu/~mwaskom/software/seaborn/">seaborn</a> - it gives very beautiful pictures, although I am more used to the text. <br><br><pre> <code class="python hljs">sns.pairplot(train_data_munged, vars=[<span class="hljs-string"><span class="hljs-string">"AgeF"</span></span>, <span class="hljs-string"><span class="hljs-string">"Pclass"</span></span>, <span class="hljs-string"><span class="hljs-string">"SexF"</span></span>], hue=<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>, dropna=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) sns.plt.show()</code> </pre><br><br><img src="https://habrastorage.org/files/418/edf/72a/418edf72a3aa44eb97b9fdfb39c307c6.png"><br><br>  Beautiful, but for example the correlation in a pair of "class-floor" is not very clear. <br><br>  Let us <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html">evaluate the</a> importance of our features using the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html">SelectKBest</a> algorithm. <br><br><pre> <code class="python hljs">selector = SelectKBest(f_classif, k=<span class="hljs-number"><span class="hljs-number">5</span></span>) selector.fit(train_data_munged[predictors], train_data_munged[<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>]) scores = -np.log10(selector.pvalues_) plt.bar(range(len(predictors)), scores) plt.xticks(range(len(predictors)), predictors, rotation=<span class="hljs-string"><span class="hljs-string">'vertical'</span></span>) plt.show()</code> </pre><br><br><img src="https://habrastorage.org/files/3bf/0ba/24e/3bf0ba24e88d47ecb44c01c218aac755.png"><br><br>  Here you have <a href="https://ru.wikipedia.org/wiki/F-%25D1%2582%25D0%25B5%25D1%2581%25D1%2582">an article</a> describing exactly how he does it.  Other strategies can be specified in the SelectKBest parameters. <br><br>  In principle, we already know everything - gender is very important.  Titles are important - but they have a strong correlation with sex.  The ticket class is important and in some way the F deck. <br><br><h1>  Grading grade </h1><br><br>  Before starting any classification, we need to understand how we will evaluate it.  In the case of Kaggle contests, everything is very simple: we just read their rules.  In the case of the Titanic, the estimate will be the ratio of the correct classifier ratings to the total number of passengers.  In other words, this estimate is called <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a> . <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But before sending the classification result for the test sample to an assessment in Kaggle, we would be nice to first understand for ourselves at least the approximate quality of our classifier. To understand this, we can only use train-sampling, since only it contains labeled data. But the question remains - how exactly? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Often in examples you can see something like this:</font></font><br><br><pre> <code class="python hljs">classifier.fit(train_X, train_y) predict_y = classifier.predict(train_X) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> metrics.accuracy_score(train_y, predict_y)</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">That is, we train the classifier on the train-set, after which we check it on it. Undoubtedly, to some extent, this gives a certain assessment of the quality of the classifier‚Äôs work, but in general this approach is incorrect. The classifier should not describe the data on which he was trained, but some model that generated this data. Otherwise, the classifier perfectly adapts to the train-sample, when checking it shows excellent results, but when checking on some other data set it merges with a bang. What is called </font></font><a href="https://en.wikipedia.org/wiki/Overfitting"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">overfitting</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The correct approach would be to divide the available train-set into a number of pieces. We can take a few of them, train the classifier on them, and then check his work for the rest. You can produce this process several times just by shuffling the pieces. In sklearn, this process is called </font></font><a href="http://scikit-learn.org/stable/modules/cross_validation.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cross-validation</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">You can already imagine in your head the cycles that will share data, produce training and assessment, but the trick is that all you need to implement this in sklearn is to determine a strategy.</font></font><br><br><pre> <code class="python hljs">cv = StratifiedKFold(train_data[<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>], n_folds=<span class="hljs-number"><span class="hljs-number">3</span></span>, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Here we define a rather complicated process: the training data will be divided into three pieces, and the records will fall into each piece in a random way (to level the possible dependence on the order), besides the strategy will track the ratio of classes in each piece to be approximately equal. </font><font style="vertical-align: inherit;">Thus, we will perform three measurements on pieces 1 + 2 vs 3, 1 + 3 vs 2, 2 + 3 vs 1 - after that we will be able to get an average assessment of the accuracy of the classifier (which will characterize the quality of work), as well as the variance of the assessment (which will be characterize the stability of his work).</font></font><br><br><h1>  Classification </h1><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Now let's test the work of various classifiers. </font></font><br><br> <a href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">KNeighborsClassifier</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br><pre> <code class="python hljs">alg_ngbh = KNeighborsClassifier(n_neighbors=<span class="hljs-number"><span class="hljs-number">3</span></span>) scores = cross_val_score(alg_ngbh, train_data_scaled, train_data_munged[<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>], cv=cv, n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>) print(<span class="hljs-string"><span class="hljs-string">"Accuracy (k-neighbors): {}/{}"</span></span>.format(scores.mean(), scores.std()))</code> </pre><br><br> <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SGDClassifier</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br><pre> <code class="python hljs">alg_sgd = SGDClassifier(random_state=<span class="hljs-number"><span class="hljs-number">1</span></span>) scores = cross_val_score(alg_sgd, train_data_scaled, train_data_munged[<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>], cv=cv, n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>) print(<span class="hljs-string"><span class="hljs-string">"Accuracy (sgd): {}/{}"</span></span>.format(scores.mean(), scores.std()))</code> </pre><br><br> <a href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SVC</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br><pre> <code class="python hljs">alg_svm = SVC(C=<span class="hljs-number"><span class="hljs-number">1.0</span></span>) scores = cross_val_score(alg_svm, train_data_scaled, train_data_munged[<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>], cv=cv, n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>) print(<span class="hljs-string"><span class="hljs-string">"Accuracy (svm): {}/{}"</span></span>.format(scores.mean(), scores.std()))</code> </pre><br><br> <a href="http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GaussianNB</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br><pre> <code class="python hljs">alg_nbs = GaussianNB() scores = cross_val_score(alg_nbs, train_data_scaled, train_data_munged[<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>], cv=cv, n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>) print(<span class="hljs-string"><span class="hljs-string">"Accuracy (naive bayes): {}/{}"</span></span>.format(scores.mean(), scores.std()))</code> </pre><br><br> <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">LinearRegression</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">linear_scorer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(estimator, x, y)</span></span></span><span class="hljs-function">:</span></span> scorer_predictions = estimator.predict(x) scorer_predictions[scorer_predictions &gt; <span class="hljs-number"><span class="hljs-number">0.5</span></span>] = <span class="hljs-number"><span class="hljs-number">1</span></span> scorer_predictions[scorer_predictions &lt;= <span class="hljs-number"><span class="hljs-number">0.5</span></span>] = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> metrics.accuracy_score(y, scorer_predictions) alg_lnr = LinearRegression() scores = cross_val_score(alg_lnr, train_data_scaled, train_data_munged[<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>], cv=cv, n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>, scoring=linear_scorer) print(<span class="hljs-string"><span class="hljs-string">"Accuracy (linear regression): {}/{}"</span></span>.format(scores.mean(), scores.std()))</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The linear_scorer method is needed because LinearRegression is a regression that returns any real number. </font><font style="vertical-align: inherit;">Accordingly, we divide the scale by the border of 0.5 and reduce any numbers to two classes - 0 and 1. </font></font><br><br> <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">LogisticRegression</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br><pre> <code class="python hljs">alg_log = LogisticRegression(random_state=<span class="hljs-number"><span class="hljs-number">1</span></span>) scores = cross_val_score(alg_log, train_data_scaled, train_data_munged[<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>], cv=cv, n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>, scoring=linear_scorer) print(<span class="hljs-string"><span class="hljs-string">"Accuracy (logistic regression): {}/{}"</span></span>.format(scores.mean(), scores.std()))</code> </pre><br><br> <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RandomForestClassifier</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br><pre> <code class="python hljs">alg_frst = RandomForestClassifier(random_state=<span class="hljs-number"><span class="hljs-number">1</span></span>, n_estimators=<span class="hljs-number"><span class="hljs-number">500</span></span>, min_samples_split=<span class="hljs-number"><span class="hljs-number">8</span></span>, min_samples_leaf=<span class="hljs-number"><span class="hljs-number">2</span></span>) scores = cross_val_score(alg_frst, train_data_scaled, train_data_munged[<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>], cv=cv, n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>) print(<span class="hljs-string"><span class="hljs-string">"Accuracy (random forest): {}/{}"</span></span>.format(scores.mean(), scores.std()))</code> </pre><br><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I did something like this</font></font></b> <div class="spoiler_text"> Accuracy (k-neighbors): 0.698092031425/0.0111105442611 <br> Accuracy (sgd): 0.708193041526/0.0178870678457 <br> Accuracy (svm): 0.693602693603/0.018027360723 <br> Accuracy (naive bayes): 0.791245791246/0.0244349506813 <br> Accuracy (linear regression): 0.805836139169/0.00839878201296 <br> Accuracy (logistic regression): 0.806958473625/0.0156323100754 <br> Accuracy (random forest): 0.827160493827/0.0063488824349 <br></div></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Random Forest won the algorithm and its dispersion is not bad - it seems it is stable. </font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Even better </font></font></h1><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Everything seems to be good and you can send the result, but there is only one muddy moment left: each classifier has its own parameters - how can we understand that we have chosen the best option? </font><font style="vertical-align: inherit;">Without a doubt, you can sit for a long time and sort through the parameters manually - but what if you </font></font><a href="http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">entrust</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> this work to a computer?</font></font><br><br><pre> <code class="python hljs">alg_frst_model = RandomForestClassifier(random_state=<span class="hljs-number"><span class="hljs-number">1</span></span>) alg_frst_params = [{ <span class="hljs-string"><span class="hljs-string">"n_estimators"</span></span>: [<span class="hljs-number"><span class="hljs-number">350</span></span>, <span class="hljs-number"><span class="hljs-number">400</span></span>, <span class="hljs-number"><span class="hljs-number">450</span></span>], <span class="hljs-string"><span class="hljs-string">"min_samples_split"</span></span>: [<span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>], <span class="hljs-string"><span class="hljs-string">"min_samples_leaf"</span></span>: [<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>] }] alg_frst_grid = GridSearchCV(alg_frst_model, alg_frst_params, cv=cv, refit=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>, n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>) alg_frst_grid.fit(train_data_scaled, train_data_munged[<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>]) alg_frst_best = alg_frst_grid.best_estimator_ print(<span class="hljs-string"><span class="hljs-string">"Accuracy (random forest auto): {} with params {}"</span></span> .format(alg_frst_grid.best_score_, alg_frst_grid.best_params_))</code> </pre><br><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It turns out even better!</font></font></b> <div class="spoiler_text"> Accuracy (random forest auto): 0.836139169473 with params {'min_samples_split': 6, 'n_estimators': 350, 'min_samples_leaf': 2} <br></div></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Selection can be made even thinner if there is time and desire - either by changing the parameters, or using a different selection strategy, for example, </font></font><a href="http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RandomizedSearchCV</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> We try xgboost </font></font></h1><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Everyone praises xgboost - let's try it too. </font></font><br><br><pre> <code class="python hljs">ald_xgb_model = xgb.XGBClassifier() ald_xgb_params = [ {<span class="hljs-string"><span class="hljs-string">"n_estimators"</span></span>: [<span class="hljs-number"><span class="hljs-number">230</span></span>, <span class="hljs-number"><span class="hljs-number">250</span></span>, <span class="hljs-number"><span class="hljs-number">270</span></span>], <span class="hljs-string"><span class="hljs-string">"max_depth"</span></span>: [<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>], <span class="hljs-string"><span class="hljs-string">"learning_rate"</span></span>: [<span class="hljs-number"><span class="hljs-number">0.01</span></span>, <span class="hljs-number"><span class="hljs-number">0.02</span></span>, <span class="hljs-number"><span class="hljs-number">0.05</span></span>]} ] alg_xgb_grid = GridSearchCV(ald_xgb_model, ald_xgb_params, cv=cv, refit=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>, n_jobs=<span class="hljs-number"><span class="hljs-number">1</span></span>) alg_xgb_grid.fit(train_data_scaled, train_data_munged[<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>]) alg_xgb_best = alg_xgb_grid.best_estimator_ print(<span class="hljs-string"><span class="hljs-string">"Accuracy (xgboost auto): {} with params {}"</span></span> .format(alg_xgb_grid.best_score_, alg_xgb_grid.best_params_))</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> For some reason, the training hung when using all the cores, so I limited myself to one thread (n_jobs = 1), but in single-threaded mode, training and classification in xgboost works very quickly. </font></font><br><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The result is also not bad</font></font></b> <div class="spoiler_text"> Accuracy (xgboost auto): 0.835016835017 with params {'n_estimators': 270, 'learning_rate': 0.02, 'max_depth': 2} <br></div></div><br><br><h1>  Result </h1><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The classifier is selected, the parameters are calculated - it remains to generate the result and send it to Kaggle for review. </font></font><br><br><pre> <code class="python hljs">alg_test = alg_frst_best alg_test.fit(train_data_scaled, train_data_munged[<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>]) predictions = alg_test.predict(test_data_scaled) submission = pd.DataFrame({ <span class="hljs-string"><span class="hljs-string">"PassengerId"</span></span>: test_data[<span class="hljs-string"><span class="hljs-string">"PassengerId"</span></span>], <span class="hljs-string"><span class="hljs-string">"Survived"</span></span>: predictions }) submission.to_csv(<span class="hljs-string"><span class="hljs-string">"titanic-submission.csv"</span></span>, index=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> In general, it is worth noting a few points in such competitions, which seemed interesting to me: </font></font><br><br><ul><li>         ‚Äî           .         ; </li><li>     -        .           -      Kaggle; </li><li>       ‚Äî                ‚Äî               ; </li><li>    .         ‚Äî        -,        , ,    ; </li><li>               ‚Äî      ,   <em></em> .    ‚Äî          ?  ,       ,            ‚Äî          - . </li></ul><br><br><h1> ,         </h1><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Looking through the top contestants, it is impossible not to notice the people who scored 1 (all the answers are correct) - and some got it from the very first attempt. </font></font><br><br><img src="https://habrastorage.org/files/c9d/9d7/1a7/c9d9d71a7597491d9193edcda5f2fb5d.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The next option comes to mind: someone registered an account with which he began to select (no more than 10 attempts per day are allowed) the correct answers. If I understand correctly, this is a kind </font></font><a href="https://ru.wikipedia.org/wiki/%25D0%2597%25D0%25B0%25D0%25B4%25D0%25B0%25D1%2587%25D0%25B8_%25D0%25BD%25D0%25B0_%25D0%25B2%25D0%25B7%25D0%25B2%25D0%25B5%25D1%2588%25D0%25B8%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">of weighing task</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">However, after thinking a little more, it is impossible not to smile at our guess: we are talking about a task, the answers for which have long been known! In fact, the death of Titanic was a shock to his contemporaries, and films, books and documentaries were devoted to this event. And most likely somewhere there is a complete list of names of passengers of the Titanic with a description of their fate. But this is no longer true for machine learning.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">However, from this it is possible and necessary to draw a conclusion that I am going to apply in the following competitions - not necessarily (if this is not prohibited by the rules of the competition) to be limited only to the data that the organizer issued. </font><font style="vertical-align: inherit;">For example, by a certain time and place, weather conditions, the state of securities markets, exchange rates, whether the day is a holiday can be identified - in other words, you can marry data from the organizers with any available public data sets that can help in describing the characteristics of the model.</font></font><br><br><h1>  Code </h1><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The full script code is </font></font><a href="https://github.com/mazurkin/titanic"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">here</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Do not forget to choose the signs for training.</font></font></div><p>Source: <a href="https://habr.com/ru/post/274171/">https://habr.com/ru/post/274171/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../274161/index.html">Interfaces: How to create forms for subscribing to email newsletters and alerts</a></li>
<li><a href="../274163/index.html">ASP.NET 5. Token authentication</a></li>
<li><a href="../274165/index.html">Familiar stranger. What is Bitrix24?</a></li>
<li><a href="../274167/index.html">Why do you need best practices in IT infrastructure management?</a></li>
<li><a href="../274169/index.html">Push notifications in android. Rakes, crutches and bicycles</a></li>
<li><a href="../274173/index.html">Underground carders market. Translation of the book "KingPIN". Chapter 29. ‚ÄúOne Plat and Six Classics‚Äù</a></li>
<li><a href="../274175/index.html">Fourier transform boundedness or why you should trust your hearing</a></li>
<li><a href="../274177/index.html">12 best overseas developer conferences in 2016</a></li>
<li><a href="../274179/index.html">OpenOCD, GDB and (strongly) remote debugging</a></li>
<li><a href="../274181/index.html">Unit Tests in ABAP. Part Three Every kind of fuss</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>