<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Configuring Active / Passive PostgreSQL Cluster using Pacemaker, Corosync, and DRBD (CentOS 5.5)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This article explains how to configure your PostgreSQL Active / Passive cluster using Pacemaker, Corosync and DRBD. 
 Prepared by Raphael Marangoni, f...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Configuring Active / Passive PostgreSQL Cluster using Pacemaker, Corosync, and DRBD (CentOS 5.5)</h1><div class="post__text post__text-html js-mediator-article">  This article explains how to configure your PostgreSQL Active / Passive cluster using Pacemaker, Corosync and DRBD. <br>  Prepared by Raphael Marangoni, from the team BRLink Servidor Linux <br><a name="habracut"></a><br><h4>  1. Notes: </h4><br><br><h5>  Linux distribution: </h5><br>  The Centos 5.5 distribution was taken as a basis, but you can use Red Hat Linux or Fedore Core as a basis. <br>  We will use DRBD for replicating PostgreSQL data between nodes and you need to have a disk or partition exclusively of DRBD. <br>  Remember: disk partitioning must be done during installation. <br><br><h5>  Network equipment / Topology: </h5><br>  We use two Gigabit NIC's per node, one (eth0) to connect to the network (LAN), and the other (eth1) with a crossover connection cable for both nodes.  A crossover cable serves for system performance. <br>  And so, we will use two physical nodes, node1.clusterbr.int and node2.clusterbr.int: <br>  node1.clusterbr.int: ip of the first node 10.0.0.191 (LAN) and ip 172.16.0.1 (cross connect) <br>  node2.clusterbr.int: ip 10.0.0.192 (LAN) and IP 172.16.0.2 (cross connect) <br>  dbip.clusterbr.int: cluster ip, 10.0.0.190 (All applications must specify this IP to access PostgreSQL) <br>  * Cross connect I will write as cross over 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h5>  Wheels: </h5><br>  Both nodes have two sections: <br>  / dev / sda: for OSes; <br>  / dev / sdb: for DRBD. <br><br><h5>  PostgreSQL: </h5><br>  PostgreSQL version 8.5, all data that will be stored on DRBD will be used within our cluster. <br><br><h4>  2. Preparation of nodes: </h4><br>  Disable SELINUX: <br> <code>vi /etc/selinux/config</code> <br> <br>  (We rule only this line, the rest is left as is) <br> <code>SELINUX=disabled</code> <br> <br>  Configure the hostname and gateway nodes: <br> <code>vi /etc/sysconfig/network</code> <br> <br>  node1: <br> <code>NETWORKING=yes <br> NETWORKING_IPV6=no <br> HOSTNAME=node1.clusterbr.int <br> GATEWAY=10.0.0.9 <br> node2: <br> NETWORKING=yes <br> NETWORKING_IPV6=no <br> HOSTNAME=node2.clusterbr.int <br> GATEWAY=10.0.0.9</code> <br> <br>  Configuring network interfaces: <br>  node1: <br>  LAN interface: <br> <code>vi /etc/sysconfig/network-scripts/ifcfg-eth0 <br> <br> DEVICE=eth0 <br> BOOTPROTO=static <br> IPADDR=10.0.0.191 <br> NETMASK=255.255.255.0 <br> ONBOOT=yes <br> HWADDR=a6:1e:3d:67:66:78</code> <br> <br>  Cross-over / DRBD interface: <br> <code>vi /etc/sysconfig/network-scripts/ifcfg-eth1 <br> <br> DEVICE=eth1 <br> BOOTPROTO=static <br> IPADDR=172.16.0.1 <br> NETMASK=255.255.255.0 <br> ONBOOT=yes <br> HWADDR=ee:ef:ff:9a:9a:57</code> <br> <br>  node2: <br>  LAN interface: <br> <code>vi /etc/sysconfig/network-scripts/ifcfg-eth0 <br> <br> DEVICE=eth0 <br> BOOTPROTO=static <br> IPADDR=10.0.0.192 <br> NETMASK=255.255.255.0 <br> ONBOOT=yes <br> HWADDR=52:52:a1:1a:62:32</code> <br> <br>  Cross-over / DRBD interface: <br> <code>vi /etc/sysconfig/network-scripts/ifcfg-eth1 <br> <br> DEVICE=eth1 <br> BOOTPROTO=static <br> IPADDR=172.16.0.2 <br> NETMASK=255.255.255.0 <br> ONBOOT=yes <br> HWADDR=1a:18:b2:50:96:1e</code> <br> <br>  DNS Setup: <br> <code>vi /etc/resolv.conf <br> <br> search clusterbr.int <br> nameserver 10.0.0.9</code> <br> <br>  Hostname setting: <br>  The configuration is the same on both nodes: <br> <code>vi /etc/hosts <br> <br> 127.0.0.1 localhost.localdomain localhost <br> 10.0.0.191 node1.clusterbr.int node1 <br> 10.0.0.192 node2.clusterbr.int node2 <br> 10.0.0.190 dbip.clusterbr.int node2</code> <br> <br>  Check the network: <br>  node1: <br>  Ping node2 (via LAN interface): <br> <code>ping -c 2 node2 <br> <br> [root@node1 ~]# ping -c 2 node2 <br> PING node2 (10.0.0.192) 56(84) bytes of data. <br> 64 bytes from node2 (10.0.0.192): icmp_seq=1 ttl=64 time=0.089 ms <br> 64 bytes from node2 (10.0.0.192): icmp_seq=2 ttl=64 time=0.082 ms <br> --- node2 ping statistics --- <br> 2 packets transmitted, 2 received, 0% packet loss, time 999ms <br> rtt min/avg/max/mdev = 0.082/0.085/0.089/0.009 ms</code> <br> <br>  Ping node2 (via cross-over interface): <br> <code>ping -c 2 172.16.0.2 <br> <br> [root@node1 ~]# ping -c 2 172.16.0.2 <br> PING 172.16.0.2 (172.16.0.2) 56(84) bytes of data. <br> 64 bytes from 172.16.0.2: icmp_seq=1 ttl=64 time=0.083 ms <br> 64 bytes from 172.16.0.2: icmp_seq=2 ttl=64 time=0.083 ms <br> --- 172.16.0.2 ping statistics --- <br> 2 packets transmitted, 2 received, 0% packet loss, time 999ms <br> rtt min/avg/max/mdev = 0.083/0.083/0.083/0.000 ms</code> <br> <br>  node2: <br>  Ping node1 (via LAN interface): <br> <code>ping -c 2 node1 <br> [root@node2 ~]# ping -c 2 node1 <br> PING node1 (10.0.0.191) 56(84) bytes of data. <br> 64 bytes from node1 (10.0.0.191): icmp_seq=1 ttl=64 time=0.068 ms <br> 64 bytes from node1 (10.0.0.191): icmp_seq=2 ttl=64 time=0.063 ms <br> --- node1 ping statistics --- <br> 2 packets transmitted, 2 received, 0% packet loss, time 999ms <br> rtt min/avg/max/mdev = 0.063/0.065/0.068/0.008 ms</code> <br> <br>  Ping node1 (via cross-over interface): <br> <code>ping -c 2 172.16.0.1 <br> <br> [root@node2 ~]# ping -c 2 172.16.0.1 <br> PING 172.16.0.1 (172.16.0.1) 56(84) bytes of data. <br> 64 bytes from 172.16.0.1: icmp_seq=1 ttl=64 time=1.36 ms <br> 64 bytes from 172.16.0.1: icmp_seq=2 ttl=64 time=0.075 ms <br> --- 172.16.0.1 ping statistics --- <br> 2 packets transmitted, 2 received, 0% packet loss, time 1001ms <br> rtt min/avg/max/mdev = 0.075/0.722/1.369/0.647 ms</code> <br> <br>  Set up initialization parameters: <br>  (The change is made only in this line, the rest is left as is) <br> <code>vi /etc/inittab <br> <br> id:3:initdefault:</code> <br> <br>  Let's look at the running services: <br> <code>chkconfig --list | grep 3:sim <br> <br> [root@node1 ~]# chkconfig --list | grep 3:sim <br> acpid 0:n√É¬£o 1:n√É¬£o 2:sim 3:sim 4:sim 5:sim 6:n√É¬£o <br> anacron 0:n√É¬£o 1:n√É¬£o 2:sim 3:sim 4:sim 5:sim 6:n√É¬£o <br> apmd 0:n√É¬£o 1:n√É¬£o 2:sim 3:sim 4:sim 5:sim 6:n√É¬£o <br> atd 0:n√É¬£o 1:n√É¬£o 2:n√É¬£o 3:sim 4:sim 5:sim 6:n√É¬£o <br> cpuspeed 0:n√É¬£o 1:sim 2:sim 3:sim 4:sim 5:sim 6:n√É¬£o <br> crond 0:n√É¬£o 1:n√É¬£o 2:sim 3:sim 4:sim 5:sim 6:n√É¬£o <br> irqbalance 0:n√É¬£o 1:n√É¬£o 2:sim 3:sim 4:sim 5:sim 6:n√É¬£o <br> kudzu 0:n√É¬£o 1:n√É¬£o 2:n√É¬£o 3:sim 4:sim 5:sim 6:n√É¬£o <br> network 0:n√É¬£o 1:n√É¬£o 2:sim 3:sim 4:sim 5:sim 6:n√É¬£o <br> rawdevices 0:n√É¬£o 1:n√É¬£o 2:n√É¬£o 3:sim 4:sim 5:sim 6:n√É¬£o <br> sshd 0:n√É¬£o 1:n√É¬£o 2:sim 3:sim 4:sim 5:sim 6:n√É¬£o <br> syslog 0:n√É¬£o 1:n√É¬£o 2:sim 3:sim 4:sim 5:sim 6:n√É¬£o</code> <br> <br>  Now after performing the above described actions, we will overload our nodes. <br><br><h4>  3. Install the necessary software </h4><br>  We put the necessary package: <br> <code>yum install -y postgresql84** gcc perl-mailtools perl-dbi php-pgsql</code> <br> <br>  Add a repository: <br> <code><a href=""></a> rpm -Uvh download.fedora.redhat.com/pub/epel/5/x86_64/epel-release-5-4.noarch.rpm</code> <br> <br>  Now we install the ClusterLabs EPEL repository: <br> <code><a href=""></a> wget -O /etc/yum.repos.d/pacemaker.repo clusterlabs.org/rpm/epel-5/clusterlabs.repo</code> <br> <br>  Install the cluster and DRBD packages: <br> <code>yum install -y pacemaker corosync drbd83 kmod-drbd83 heartbeat</code> <br> <br><h5>  4. Configure DRBD </h5><br>  First, we need to configure DRBD on both nodes: <br> <code>vi /etc/drbd.conf <br> <br> global { <br> usage-count no; <br> } <br> common { <br> syncer { rate 100M; } <br> protocol C; <br> } <br> resource postgres { <br> startup { <br> wfc-timeout 0; <br> degr-wfc-timeout <br> 120; <br> } <br> disk { on-io-error detach; } <br> on node1.clusterbr.int { <br> device /dev/drbd0; <br> disk /dev/sdb; <br> address 172.16.0.1:7791; <br> meta-disk internal; <br> } <br> on node2.clusterbr.int { <br> device /dev/drbd0; <br> disk /dev/sdb; <br> address 172.16.0.2:7791; <br> meta-disk internal; <br> } <br> }</code> <br> <br><h4>  Main configuration items: </h4><br>  resource: Refer to the resource that will be managed by DRBD, and we called it ‚ÄúPostgres‚Äù <br>  disk: Specify the device that will use DRBD (disk or partition) <br>  address: The IP address and port that DRBD will use (we specified the cross-over interfaces) <br>  syncer: data transfer rate between nodes <br>  If you have any questions or concerns, you can always use the manual: <a href="http://www.drbd.org/users-guide-emb/">www.drbd.org/users-guide-emb</a> <br><br>  After this configuration, we can create metadata about the PostgreSQL resources. <br>  Doing the following: <br>  node1: <br> <code>drbdadm create-md postgres <br> <br> [root@node1 ~]# drbdadm create-md postgres <br> Writing meta data... <br> initializing activity log <br> NOT initialized bitmap <br> New drbd meta data block successfully created.</code> <br> <br>  node2: <br> <code>drbdadm create-md postgres <br> <br> [root@node2 ~]# drbdadm create-md postgres <br> Writing meta data... <br> initializing activity log <br> NOT initialized bitmap <br> New drbd meta data block successfully created.</code> <br> <br>  Next, we need to create a resource before connecting, again, perform the following actions on both nodes: <br> <code>drbdadm up postgres</code> <br> <br>  Now we can do the initial synchronization between the nodes, synchronize to the main node by selecting node1: <br> <code>drbdadm -- --overwrite-data-of-peer primary postgres</code> <br> <br>  To check the synchronization look: <br> <code>cat /proc/drbd <br> <br> [root@node1 ~]# cat /proc/drbd <br> version: 8.3.8 (api:88/proto:86-94) <br> GIT-hash: d78846e52224fd00562f7c225bcc25b2d422321d build by mockbuild@builder10.centos.org, 2010-06-04 08:04:09 <br> 0: cs:SyncSource ro:Primary/Secondary ds:UpToDate/Inconsistent C r---- <br> ns:48128 nr:0 dw:0 dr:48128 al:0 bm:2 lo:0 pe:0 ua:0 ap:0 ep:1 wo:b oos:8340188 <br> [&gt;....................] sync'ed: 0.6% (8144/8188)M delay_probe: 7 <br> finish: 0:11:29 speed: 12,032 (12,032) K/sec</code> <br> <br>  Wait for the synchronization to complete.  The process can take a long time, it all depends on the size, disk performance and of course the speed of the network cluster interfaces. <br>  After the synchronization process is over, we can look at the state of the postgre resources <br>  node1: <br> <code>cat /proc/drbd <br> <br> [root@node1 ~]# cat /proc/drbd <br> version: 8.3.8 (api:88/proto:86-94) <br> GIT-hash: d78846e52224fd00562f7c225bcc25b2d422321d build by mockbuild@builder10.centos.org, 2010-06-04 08:04:09 <br> 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r---- <br> ns:8388316 nr:0 dw:0 dr:8388316 al:0 bm:512 lo:0 pe:0 ua:0 ap:0 ep:1 wo:b oos:0</code> <br> <br>  node2: <br> <code>cat /proc/drbd <br> <br> [root@node2 ~]# cat /proc/drbd <br> version: 8.3.8 (api:88/proto:86-94) <br> GIT-hash: d78846e52224fd00562f7c225bcc25b2d422321d build by mockbuild@builder10.centos.org, 2010-06-04 08:04:09 <br> 0: cs:Connected ro:Secondary/Primary ds:UpToDate/UpToDate C r---- <br> ns:0 nr:8388316 dw:8388316 dr:0 al:0 bm:512 lo:0 pe:0 ua:0 ap:0 ep:1 wo:b oos:0</code> <br> <br><h5>  5. Configure PostgreSQL </h5><br>  First, we need to start the DRBD service on both nodes. <br> <code>/etc/init.d/drbd start</code> <br> <br>  As previously mentioned, node1 is our main node and therefore we will execute the command on it: <br> <code>cat /proc/drbd <br> <br> [root@node1 ~]# cat /proc/drbd <br> version: 8.3.8 (api:88/proto:86-94) <br> GIT-hash: d78846e52224fd00562f7c225bcc25b2d422321d build by mockbuild@builder10.centos.org, 2010-06-04 08:04:09 <br> 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r---- <br> ns:8388316 nr:0 dw:0 dr:8388316 al:0 bm:512 lo:0 pe:0 ua:0 ap:0 ep:1 wo:b oos:0</code> <br> <br>  Primary / Secondary means that the local server is primary and the other is secondary. <br><br>  Next we need to format DRBD and select ext3 as the file system. <br>  node1: <br> <code>mkfs.ext3 /dev/drbd0</code> <br> <br>  After that, we can mount the device as a standard PostgreSQL location. <br>  Mount the partition on the node: <br>  node1: <br> <code>mount -t ext3 /dev/drbd0 /var/lib/pgsql</code> <br> <br>  Next, change the owner and mount group: <br>  node1: <br> <code>chown postgres.postgres /var/lib/pgsql</code> <br> <br>  Now we initialize the PostgreSQL database: <br>  node1: <br> <code>su - postgres <br> initdb /var/lib/pgsql/data <br> exit</code> <br> <br>  I prefer to verify authentication on the cluster and ip nodes: <br>  node1: <br> <code>echo "host all all 10.0.0.191/32 trust" &gt;&gt; /var/lib/pgsql/data/pg_hba.conf <br> echo "host all all 10.0.0.192/32 trust" &gt;&gt; /var/lib/pgsql/data/pg_hba.conf <br> echo "host all all 10.0.0.190/32 trust" &gt;&gt; /var/lib/pgsql/data/pg_hba.conf</code> <br> <br>  Next we will configure PostgreSQL to work on all interfaces: <br>  node1: <br> <code>vi /var/lib/pgsql/data/postgresql.conf</code> <br> <br>  Uncomment only this line: <br> <code>listen_addresses = '0.0.0.0'</code> <br> <br>  Run PostgreSQL: <br>  node1: <br> <code>/etc/init.d/postgresql start</code> <br> <br>  Now create a user with administrator rights to manage PostgeSQL: <br>  node1: <br> <code>su - postgres <br> createuser --superuser admpgsql --pwprompt</code> <br> <br>  You need to set the password on admpgsql <br><br>  After that we will create a database and fill it: <br>  node1: <br> <code>su - postgres <br> createdb pgbench <br> pgbench -i pgbench</code> <br> <br>  pgbench will fill in some base information: <br> <code>pgbench -i pgbench <br> <br> -bash-3.2$ pgbench -i pgbench <br> NOTA: tabela "pgbench_branches" n√É¬£o existe, ignorando <br> NOTA: tabela "pgbench_tellers" n√É¬£o existe, ignorando <br> NOTA: tabela "pgbench_accounts" n√É¬£o existe, ignorando <br> NOTA: tabela "pgbench_history" n√É¬£o existe, ignorando <br> creating tables... <br> 10000 tuples done. <br> 20000 tuples done. <br> 30000 tuples done. <br> 40000 tuples done. <br> 50000 tuples done. <br> 60000 tuples done. <br> 70000 tuples done. <br> 80000 tuples done. <br> 90000 tuples done. <br> 100000 tuples done. <br> set primary key... <br> NOTA: ALTER TABLE / ADD PRIMARY KEY criar√É¬° √É ndice impl√É cito "pgbench_branches_pkey" na tabela "pgbench_branches" <br> NOTA: ALTER TABLE / ADD PRIMARY KEY criar√É¬° √É ndice impl√É cito "pgbench_tellers_pkey" na tabela "pgbench_tellers" <br> NOTA: ALTER TABLE / ADD PRIMARY KEY criar√É¬° √É ndice impl√É cito "pgbench_accounts_pkey" na tabela "pgbench_accounts" <br> vacuum...done.</code> <br> <br>  Now we turn to the database to check: <br>  node1: <br> <code>psql -U admpgsql -d pgbench <br> select * from pgbench_tellers; <br> <br> psql -U admpgsql -d pgbench <br> psql (8.4.5) <br> Digite "help" para ajuda. <br> <br> pgbench=# select * from pgbench_tellers; <br> tid | bid | tbalance | filler <br> -----+-----+----------+-------- <br> 1 | 1 | 0 | <br> 2 | 1 | 0 | <br> 3 | 1 | 0 | <br> 4 | 1 | 0 | <br> 5 | 1 | 0 | <br> 6 | 1 | 0 | <br> 7 | 1 | 0 | <br> 8 | 1 | 0 | <br> 9 | 1 | 0 | <br> 10 | 1 | 0 | <br> (10 registros)</code> <br> <br>  At this setting is over. <br><br>  Check PostgreSQL for node2: <br>  But before we start working with the Pacemaker service, it‚Äôs better to make sure that postgre will work on node2. <br>  First, on node1 we have to stop postgresql: <br>  node1: <br> <code>/etc/init.d/postgresql stop</code> <br> <br>  Then install DRBD once: <br> <code>umount /dev/drbd0</code> <br> <br>  Now we will make node1 as secondary on DRBD: <br> <code>drbdadm secondary postgres</code> <br> <br>  Now on node2 let's make DRBD primary: <br>  node2: <br> <code>drbdadm primary postgres</code> <br> <br>  Next, install the DRBD device: <br> <code>mount -t ext3 /dev/drbd0 /var/lib/pgsql/</code> <br> <br>  and finally run PostgreSQL: <br> <code>/etc/init.d/postgresql start</code> <br> <br>  Now let's check if we can access pgbench by node2: <br> <code>psql -U admpgsql -d pgbench <br> select * from pgbench_tellers; <br> <br> [root@node2 ~]# psql -U admpgsql -d pgbench <br> psql (8.4.5) <br> Digite "help" para ajuda. <br> <br> pgbench=# select * from pgbench_tellers; <br> tid | bid | tbalance | filler <br> -----+-----+----------+-------- <br> 1 | 1 | 0 | <br> 2 | 1 | 0 | <br> 3 | 1 | 0 | <br> 4 | 1 | 0 | <br> 5 | 1 | 0 | <br> 6 | 1 | 0 | <br> 7 | 1 | 0 | <br> 8 | 1 | 0 | <br> 9 | 1 | 0 | <br> 10 | 1 | 0 | <br> (10 registros)</code> <br> <br>  After we make sure everything is working fine, we need to initialize our cluster: <br>  node2: <br> <code>/etc/init.d/postgresql stop <br> umount /dev/drbd0 <br> drbdadm secondary postgres <br> /etc/init.d/drbd stop</code> <br> <br>  node1: <br> <code>drbdadm primary postgres <br> /etc/init.d/drbd stop</code> <br> <br>  We need to make all services be disabled on both nodes by executing the commands on both nodes: <br> <code>chkconfig --level 35 drbd off <br> chkconfig --level 35 postgresql off</code> <br> <br><h4>  6. Configure Corosync (openAIS) </h4><br>  Making the setting of Corosync <br>  node1: <br> <code>export ais_port=4000 <br> export ais_mcast=226.94.1.1 <br> export ais_addr=`ip address show eth0 | grep "inet " | tail -n 1 | awk '{print $4}' | sed s/255/0/`</code> <br> <br>  Then check the data: <br> <code>env | grep ais_</code> <br> <br>  The variable ais_addr should contain the network address that the cluster will listen to. In our case, this is 10.0.0.0. <br>  Next we will create a corosync configuration file: <br> <code>cp /etc/corosync/corosync.conf.example /etc/corosync/corosync.conf <br> sed -i.gres "s/.*mcastaddr:.*/mcastaddr:\ $ais_mcast/g" /etc/corosync/corosync.conf <br> sed -i.gres "s/.*mcastport:.*/mcastport:\ $ais_port/g" /etc/corosync/corosync.conf <br> sed -i.gres "s/.*bindnetaddr:.*/bindnetaddr:\ $ais_addr/g" /etc/corosync/corosync.conf</code> <br> <br>  Add the following to our config: <br> <code>cat &lt;&lt;-END &gt;&gt;/etc/corosync/corosync.conf <br> aisexec { <br> user: root <br> group: root <br> } <br> END <br> cat &lt;&lt;-END &gt;&gt;/etc/corosync/corosync.conf <br> service { <br> # Load the Pacemaker Cluster Resource Manager <br> name: pacemaker <br> ver: 0 <br> } <br> END</code> <br> <br>  The <code>/etc/corosync/corosync.conf</code> file looks like this: <br> <code>compatibility: whitetank <br> <br> totem { <br> version: 2 <br> secauth: off <br> threads: 0 <br> interface { <br> ringnumber: 0 <br> bindnetaddr: 10.0.0.0 <br> mcastaddr: 226.94.1.1 <br> mcastport: 4000 <br> } <br> } <br> <br> logging { <br> fileline: off <br> to_stderr: yes <br> to_logfile: yes <br> to_syslog: yes <br> logfile: /tmp/corosync.log <br> debug: off <br> timestamp: on <br> logger_subsys { <br> subsys: AMF <br> debug: off <br> } <br> } <br> <br> amf { <br> mode: disabled <br> } <br> aisexec { <br> user: root <br> group: root <br> } <br> service { <br> # Load the Pacemaker Cluster Resource Manager <br> name: pacemaker <br> ver: 0 <br> }</code> <br> <br>  From node1 we will throw the configuration on node2: <br> <code>scp /etc/corosync/* node2:/etc/corosync/</code> <br> <br>  On both nodes we need to create a catalog of heads: <br> <code>mkdir /var/log/cluster/</code> <br> <br>  Now run corosync <br>  node1: <br> <code>/etc/init.d/corosync start</code> <br> <br>  Let's check whether our service is in order: <br>  node1: <br> <code>grep -e "Corosync Cluster Engine" -e "configuration file" /var/log/messages <br> <br> [root@node1 bin]# grep -e "Corosync Cluster Engine" -e "configuration file" /var/log/messages <br> Apr 7 12:37:21 node1 corosync[23533]: [MAIN ] Corosync Cluster Engine ('1.2.0'): started and ready to provide service. <br> Apr 7 12:37:21 node1 corosync[23533]: [MAIN ] Successfully read main configuration file '/etc/corosync/corosync.conf'.</code> <br> <br>  Check our interfaces that corosync should listen to: <br> <code>grep TOTEM /var/log/messages <br> <br> [root@node1 bin]# grep TOTEM /var/log/messages <br> Apr 7 12:37:21 node1 corosync[23533]: [TOTEM ] Initializing transport (UDP/IP). <br> Apr 7 12:37:21 node1 corosync[23533]: [TOTEM ] Initializing transmit/receive security: libtomcrypt SOBER128/SHA1HMAC (mode 0). <br> Apr 7 12:37:21 node1 corosync[23533]: [TOTEM ] The network interface [10.0.0.191] is now up. <br> Apr 7 12:37:21 node1 corosync[23533]: [TOTEM ] A processor joined or left the membership and a new membership was formed.</code> <br> <br>  Next, look at the pacemaker: <br> <code>grep pcmk_startup /var/log/messages <br> <br> [root@node1 bin]# grep pcmk_startup /var/log/messages <br> Apr 7 12:37:21 node1 corosync[23533]: [pcmk ] info: pcmk_startup: CRM: Initialized <br> Apr 7 12:37:21 node1 corosync[23533]: [pcmk ] Logging: Initialized pcmk_startup <br> Apr 7 12:37:21 node1 corosync[23533]: [pcmk ] info: pcmk_startup: Maximum core file size is: 4294967295 <br> Apr 7 12:37:21 node1 corosync[23533]: [pcmk ] info: pcmk_startup: Service: 9 <br> Apr 7 12:37:21 node1 corosync[23533]: [pcmk ] info: pcmk_startup: Local hostname: node1</code> <br> <br>  We look in the processes of our corosync: <br> <code>ps axf <br> <br> [root@node1 bin]# ps axf <br> (should contain something like this) <br> 23533 ? Ssl 0:00 corosync <br> 23539 ? SLs 0:00 \_ /usr/lib/heartbeat/stonithd <br> 23540 ? S 0:00 \_ /usr/lib/heartbeat/cib <br> 23541 ? S 0:00 \_ /usr/lib/heartbeat/lrmd <br> 23542 ? S 0:00 \_ /usr/lib/heartbeat/attrd <br> 23543 ? S 0:00 \_ /usr/lib/heartbeat/pengine <br> 23544 ? S 0:00 \_ /usr/lib/heartbeat/crmd</code> <br> <br>  If everything went well, then we can transfer the corosync to node2 <br>  node2: <br> <code>/etc/init.d/corosync start</code> <br> <br>  Perform a cluster check on both nodes: <br> <code>crm_mon -1 <br> <br> [root@node1 ~]# crm_mon -1 <br> ============ <br> Last updated: Fri Oct 29 17:44:36 2010 <br> Stack: openais <br> Current DC: node1.clusterbr.int - partition with quorum <br> Version: 1.0.9-89bd754939df5150de7cd76835f98fe90851b677 <br> 2 Nodes configured, 2 expected votes <br> 0 Resources configured. <br> ============ <br> <br> Online: [ node1.clusterbr.int node2.clusterbr.int ]</code> <br> <br>  We must be sure that both sites are defined as online. <br><br>  Add corosync to autoload on both nodes: <br> <code>chkconfig --level 35 corosync on</code> <br> <br><h4>  7. Configure Pacemaker </h4><br>  Important commands to manage the cluster: <br><br>  Cluster configuration check: <br> <code>crm_verify -L</code> <br> <br>  Get the list and status of the cluster: <br> <code>crm_mon -1</code> <br> <br>  Cluster configuration list: <br> <code>crm configure show</code> <br> <br>  List of open crm consoles <br> <code>crm</code> <br> <br>  Stonith setup <br>  When checking the cluster configuration, we should get some errors: <br> <code>crm_verify -L</code> <br> <br>  Thus, to disable Stonith, we need to execute the following command on one of the nodes: <br> <code>crm configure property stonith-enabled=false</code> <br> <br>  Now the cluster configuration check should pass without errors: <br> <code>crm_verify -L</code> <br> <br>  <b>Cluster basic settings</b> <br><br>  Run the command on any node from our nodes: <br> <code>crm configure property no-quorum-policy=ignore</code> <br> <br>  Setting the value will change the resource to another node, thus if synchronization fails at one of the nodes, synchronization will be intercepted by another node. <br> <code>crm configure rsc_defaults resource-stickiness=100</code> <br> <br>  Let's see our configuration: <br> <code>crm configure show <br> <br> [root@node1 ~]# crm configure show <br> node node1.clusterbr.int <br> node node2.clusterbr.int <br> property $id="cib-bootstrap-options" \ <br> dc-version="1.0.9-89bd754939df5150de7cd76835f98fe90851b677" \ <br> cluster-infrastructure="openais" \ <br> expected-quorum-votes="2" \ <br> stonith-enabled="false" \ <br> no-quorum-policy="ignore" <br> rsc_defaults $id="rsc-options" \ <br> resource-stickiness="100"</code> <br> <br>  DBIP configuration <br><br>  Add to the DBIP config the ip-address of our cluster: <br> <code>crm configure primitive DBIP ocf:heartbeat:IPaddr2 \ <br> params ip=10.0.0.190 cidr_netmask=24 \ <br> op monitor interval=30s</code> <br> <br>  Checking status: <br> <code>crm_mon -1 <br> <br> [root@node1 ~]# crm_mon -1 <br> ============ <br> Last updated: Fri Oct 29 17:47:53 2010 <br> Stack: openais <br> Current DC: node1.clusterbr.int - partition with quorum <br> Version: 1.0.9-89bd754939df5150de7cd76835f98fe90851b677 <br> 2 Nodes configured, 2 expected votes <br> 1 Resources configured. <br> ============ <br> <br> Online: [ node2.clusterbr.int node1.clusterbr.int ] <br> <br> DBIP (ocf::heartbeat:IPaddr2): Started node2.clusterbr.int</code> <br> <br>  Note that the state of the cluster shows where our resource is working, at the moment the work is on node2, but it can also work on node1 <br><br>  DRBD configuration on cluster <br><br>  Add DRBD to our cluster: <br> <code>crm configure primitive drbd_postgres ocf:linbit:drbd \ <br> params drbd_resource="postgres" \ <br> op monitor interval="15s"</code> <br> <br>  Configure the primary and secondary nodes: <br> <code>crm configure ms ms_drbd_postgres drbd_postgres \ <br> meta master-max="1" master-node-max="1" \ <br> clone-max="2" clone-node-max="1" \ <br> notify="true"</code> <br> <br>  Install DRBD: <br> <code>crm configure primitive postgres_fs ocf:heartbeat:Filesystem \ <br> params device="/dev/drbd0" directory="/var/lib/pgsql" fstype="ext3"</code> <br> <br>  Configuring PostgreSQL on a cluster <br><br>  Add postgresql to our cluster: <br> <code>crm configure primitive postgresql ocf:heartbeat:pgsql \ <br> op monitor depth="0" timeout="30" interval="30"</code> <br> <br>  Now we need to add our services to the postgres group <br> <code>crm configure group postgres postgres_fs DBIP postgresql <br> crm configure colocation postgres_on_drbd inf: postgres ms_drbd_postgres:Master</code> <br> <br>  Configuring postgre to run after: <br> <code>DRBDcrm configure order postgres_after_drbd inf: ms_drbd_postgres:promote postgres:start</code> <br> <br>  Let's look at the cluster configuration <br> <code>crm configure show <br> <br> [root@node1 ~]# crm configure show <br> node node1.clusterbr.int <br> node node2.clusterbr.int <br> primitive DBIP ocf:heartbeat:IPaddr2 \ <br> params ip="10.0.0.190" cidr_netmask="24" \ <br> op monitor interval="30s" <br> primitive drbd_postgres ocf:linbit:drbd \ <br> params drbd_resource="postgres" \ <br> op monitor interval="15s" <br> primitive postgres_fs ocf:heartbeat:Filesystem \ <br> params device="/dev/drbd0" directory="/var/lib/pgsql" fstype="ext3" <br> primitive postgresql ocf:heartbeat:pgsql \ <br> op monitor interval="30" timeout="30" depth="0" \ <br> meta target-role="Started" <br> group postgres postgres_fs DBIP postgresql \ <br> meta target-role="Started" <br> ms ms_drbd_postgres drbd_postgres \ <br> meta master-max="1" master-node-max="1" clone-max="2" clone-node-max="1" notify="true" <br> colocation postgres_on_drbd inf: postgres ms_drbd_postgres:Master <br> order postgres_after_drbd inf: ms_drbd_postgres:promote postgres:start <br> property $id="cib-bootstrap-options" \ <br> dc-version="1.0.9-89bd754939df5150de7cd76835f98fe90851b677" \ <br> cluster-infrastructure="openais" \ <br> expected-quorum-votes="2" \ <br> stonith-enabled="false" \ <br> no-quorum-policy="ignore" <br> rsc_defaults $id="rsc-options" \ <br> resource-stickiness="100" <br> [root@node1 ~]#</code> <br> <br>  Configure the preferred node: <br> <code>crm configure location master-prefer-node1 DBIP 50: node1.clusterbr.int</code> <br> <br>  Checking status: <br> <code>crm_mon -1 <br> <br> [root@node2 ~]# crm_mon -1 <br> ============ <br> Last updated: Fri Oct 29 19:54:09 2010 <br> Stack: openais <br> Current DC: node2.clusterbr.int - partition with quorum <br> Version: 1.0.9-89bd754939df5150de7cd76835f98fe90851b677 <br> 2 Nodes configured, 2 expected votes <br> 2 Resources configured. <br> ============ <br> <br> Online: [ node2.clusterbr.int node1.clusterbr.int ] <br> <br> Master/Slave Set: ms_drbd_postgres <br> Masters: [ node2.clusterbr.int ] <br> Slaves: [ node1.clusterbr.int ] <br> Resource Group: postgres <br> postgres_fs (ocf::heartbeat:Filesystem): Started node2.clusterbr.int <br> DBIP (ocf::heartbeat:IPaddr2): Started node2.clusterbr.int <br> postgresql (ocf::heartbeat:pgsql): Started node2.clusterbr.int</code> <br> <br>  If you have errors, you must overload both nodes so that the new corosync configurations are accepted by the system.  Also, after rebooting the system, we need to connect to the DBIP (10.0.0.190) via TCP port 5432 to the postgres service. <br><br>  Cluster management <br><br>  Transferring a resource to another node: <br> <code>crm resource migrate postgres node1.clusterbr.int</code> <br> <br>  Removing the migrate command: <br> <code>crm resource unmigrate postgres</code> <br> <br>  We clean our messages: <br> <code>crm resource cleanup postgres</code> <br> <br>  Stop the PostgreSQL service <br> <code>crm resource stop postgresql</code> <br> <br>  Starting PostgreSQL Service <br> <code>crm resource start postgresql</code> <br> <br><h4>  8. Create a web interface for service status </h4><br>  The web interface will be convenient for monitoring our cluster. <br>  Start apache: <br> <code>/etc/init.d/httpd start <br> chkconfig --level 35 httpd on</code> <br> <br>  Create a directory for the cluster (DocumentRoot): <br> <code>mkdir /var/www/html/cluster/</code> <br> <br>  To generate pages do the following: <br> <code>crm_mon --daemonize --as-html /var/www/html/cluster/index.html</code> <br> <br>  and let's set this whole thing to autoload: <br> <code>echo "crm_mon --daemonize --as-html /var/www/html/cluster/index.html" &gt;&gt; /etc/rc.d/rc.local</code> <br> <br>  Check the availability of our face in the browser: <br> <code><a href="http://10.0.0.190/cluster"></a> 10.0.0.190/cluster</code> <br> <br><h4>  9. Installing phppgAdmin to work with postgresql </h4><br>  Perform actions on both nodes: <br> <code>mkdir /download <br> cd /download <br> wget 'http://downloads.sourceforge.net/project/phppgadmin/phpPgAdmin%20%5Bbeta%5D/phpPgAdmin-5.0/phpPgAdmin-5.0-beta2.tar.bz2?r=http%3A%2F%2Fphppgadmin.sourceforge.net%2F%3Fpage%3Ddownload&amp;ts=1288189530&amp;use_mirror=ufpr'</code> <br> <br>  Install: <br> <code>tar -jxvf phpPgAdmin-5.0-beta2.tar.bz2 <br> mv phpPgAdmin-5.0-beta2 /var/www/html/cluster-pgadmin <br> chown apache.apache -R /var/www/html/cluster-pgadmin</code> <br> <br>  Go to the browser at <a href="http://10.0.0.190/cluster-pgadmin">10.0.0.190/cluster-pgadmin</a> <br>  P.S.  Login and password were entered during postgresql installation <br><br><h4>  10. Network access </h4><br>  If you need access to postgresql from the local network, do not forget to set up authentication on Postgres <br><br>  Here we set the MD5 network authentication to 10.0.0.0/24 <br> <code>echo "host all all 10.0.0.0/24 md5"&gt;&gt; /var/lib/pgsql/data/pg_hba.conf</code> <br> <br>  Restart postgres: <br> <code>crm resource stop postgresql <br> crm resource start postgresql</code> <br> <br><h4>  11. Monitoring </h4><br>  Cluster monitoring is a mandatory part of the entire script execution process.  I suggest you monitor using Zabbix, to do this, install Zabbix agent on each node and configure monitoring for these items: <br><br>  1. Checking ping for accessibility (10.0.0.191, 10.0.0.192 and 172.16.0.1, 172.16.0.2) <br>  2. Check availability DBIP (cluster ip) 10.0.0.190 <br>  3. Checking TCP port 5432 for DBIP 10.0.0.190 <br>  4. Checking the CPU, RAM and Disk <br>  5. You can use the monitor_drbd.sh script (the result returns 1 when everything is OK and 0 when there are problems) <br>  Script monitor_drbd.sh for Zabbix: <br> <code>#!/bin/bash <br> <br> CHECK=`cat /proc/drbd | grep UpToDate/UpToDate | cut -d: -f5 | cut -c1-17` <br> STRING_OK="UpToDate/UpToDate" <br> <br> # Comparando as duas. <br> if [ "$CHECK" == "$STRING_OK" ] ; then <br> # Is ok, returning 1 <br> echo 1; <br> else <br> # Not ok, returning 0 <br> echo 0; <br> fi</code> <br> <br>  PS If you see typos or do not match in the translation, very much I ask you to write in the LAN. <br>  I will quickly fix everything for the benefit of society. </div><p>Source: <a href="https://habr.com/ru/post/108833/">https://habr.com/ru/post/108833/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../108820/index.html">Installing an unsupported Wifi card in the HP Pavilion dv6-1319er</a></li>
<li><a href="../108823/index.html">Apache will breathe new life into Google Wave</a></li>
<li><a href="../108824/index.html">How to update your Facebook, LinkedIn & Twitter status using spring-social</a></li>
<li><a href="../108828/index.html">The release of the second part of the book "Smashing Magazine": add yourself to the book!</a></li>
<li><a href="../108831/index.html">NLP: spell check - an inside look (part 1)</a></li>
<li><a href="../108835/index.html">Local Linux kernel vulnerability (and more), DoS</a></li>
<li><a href="../108837/index.html">A project at the idea stage? (thinking out loud)</a></li>
<li><a href="../108838/index.html">Windows Phone 7 jailbreak published</a></li>
<li><a href="../108840/index.html">Phrases overtook FarmVille in the list of the most popular applications</a></li>
<li><a href="../108842/index.html">Project managers are dying alone and for nothing: raise the flag!</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>