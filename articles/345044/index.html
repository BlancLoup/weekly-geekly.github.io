<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to make an internal product external. Experience team Yandeks.Trekera</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Recently, we have opened for external users Yandex.Tracker - our system for managing tasks and processes. In Yandex, it is used not only to create ser...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to make an internal product external. Experience team Yandeks.Trekera</h1><div class="post__text post__text-html js-mediator-article"><p>  Recently, we have opened for external users <a href="https://yandex.ru/tracker%3Futm_source%3Dhabr%26utm_medium%3Dyandexchannel%26utm_campaign%3Dabouttracker1">Yandex.Tracker</a> - our system for managing tasks and processes.  In Yandex, it is used not only to create services, but even to purchase cookies in the kitchen. </p><br><p>  As you know, the smaller the company, the simpler tools it can use.  If you can greet each employee personally in the morning, then even a chat in Telegram is enough for you to work.  When individual teams appear, not only will you not be able to greet everyone personally, but you can also get confused in task statuses. </p><br><p> <a href="https://habrahabr.ru/company/yandex/blog/345044/"><img src="https://habrastorage.org/webt/ah/uc/aq/ahucaq9mdvxr3agdztqg2hze4jy.png"></a> <br>  <em>Word cloud in ticket headers in the internal Yandex.Tracker</em> </p><br><p>  At this stage, it is important to preserve the transparency of the processes: all parties should have the opportunity at any time to find out about the progress of the task or, for example, leave a comment that does not disappear in the flow of the working chat.  For small teams, a tracker is a kind of news feed with the latest news from the life of their company. </p><br><p>  Today we will tell Habrahabr readers why Yandex decided to create its own tracker, how it works inside, and what difficulties we had to face when opening it outside. </p><a name="habracut"></a><br><p>  More than six thousand people are currently working in Yandex.  Despite the fact that many of its parts are arranged as independent start-ups with their teams of different sizes, there‚Äôs always a need to understand what is happening in the people on the next floor - their work may overlap with yours, their improvements can help you, and some processes, on the contrary can negatively affect yours.  In such a situation, it is difficult, for example, to call a colleague from another workspace on Slack.  Especially when the transparency of the task is important for many people from different directions. </p><br><p>  At some point we started using the well-known Djiru.  This is a good tool, the functionality of which, in principle, suited everyone, but it was difficult to integrate it with our internal services.  In addition, on a scale of thousands of people who need a single space, where everyone can navigate without a flashlight, Jyra was no longer enough.  It happened that she went under load, even though she worked on our servers.  Yandex grew, the number of tickets also increased, and updates to new versions took more and more time (the last upgrade took six months).  It was necessary to change something. </p><br><p>  At the end of 2011, we had several solutions to the problem: </p><br><ul><li>  Increase the performance of the old tracker.  They dropped the idea, because  redoing the architecture of another product themselves - bad.  That would mean at least putting an end to updates. </li><li>  Saw the tracker into several independent copies (instances) to reduce the load on each.  The idea is not new, it is used by large companies in similar cases.  As a result, end-to-end reports, filtering, linking, and transfer of tasks between copies would not work.  All this was critical for the company. </li><li>  Purchase another tool.  Considered the possible options for trackers.  Most of them do not allow for easy scaling, that is, the cost of infrastructure improvements and subsequent changes for our needs would be higher than the cost of our own design. </li><li>  Write your tracker.  Risky option.  Gives the most freedom and opportunity in case of success.  In the event of failure, we have problems with one of the key development and planning tools.  As you may have guessed, we took a chance and chose this option.  The expected benefits outweighed the disadvantages and risks. </li></ul><br><p>  Development of own tracker began in January 2012.  First, the tracker team itself transported its tasks to the new service several months after the start of work on the project.  Then the process of moving other teams began.  Each team put forward its own requirements for functionality, they were worked out, the tracker gained new features, then transported the team.  It took two years to completely move all the teams and close Jira. </p><br><p>  But let's go back a little and look at the list of requirements that was compiled for the new service: </p><br><ul><li>  Fault tolerance.  As you may <a href="https://habrahabr.ru/company/yandex/blog/243033/">have heard</a> , the company regularly conducts exercises with the disconnection of one of the DCs.  The service should experience them imperceptibly both for the user and for the team, without the need to perform manual actions at the start of the exercise. </li><li>  Scalable.  The tasks in the tracker have no statute of limitations.  The developer or manager may need to look at the current task, as well as the one that was closed 7 years ago.  And this means that we cannot delete or archive old data. </li><li>  Integration with the company's internal services.  Close cohesion with our numerous services was required by most of the Yandex teams: integration with long-term planning services, version control systems, employee catalogs, etc. </li></ul><br><p>  And at the time of collecting the requirements, we decided on the technologies that we will use to create a tracker: </p><br><ul><li>  Java 7 (now 8) for backend. </li><li>  Node.js + BEMHTML + i-bem for the frontend. </li><li>  MongoDB as the main data storage: automatic failover, good speed and the ability to easily enable sharding, schemaless (convenient for user fields in tasks). </li><li>  Elasticsearch for quick search and aggregation on an arbitrary field.  Flexible configuration options for analyzers for sadzes, percolator and other elastic buns also played a role in the selection. </li><li>  ZooKeeper for discovery backends service.  Backends interact with each other to invalidate caches, distribute tasks, and collect their own metrics.  With the help of ZooKeeper and the client to it, discovery can be organized very easily. </li><li>  File storage as a service, which removes the headache of replication and backups when storing user attachments. </li><li>  Hystrix to communicate with external services.  To prevent cascading trips, do not load adjacent services if they experience problems. </li><li>  Nginx for termination https and rate limits.  As practice has shown, to terminate https inside java is not the best idea from the point of view of performance, so we shifted this task to nginx.  Rate limiter is also safer to organize on his side. </li></ul><br><p>  As with any other public and internal services of Yandex, we also had to think about the requirements for the performance margin and scalability of the service.  An example for understanding the situation.  At the time of the start of system design, we had about 1 million tasks and 3 thousand users.  Today, the service has almost 9 million tasks and more than 6 thousand users. </p><br><p>  By the way, despite the fairly decent number of users in the internal tracker, most of the requests come to the tracker via the API from Yandex services integrated with it.  It is they who create the main load: </p><br><p><img src="https://habrastorage.org/webt/r7/jf/t-/r7jft-m7nm6tzs23t9a4dkfc1so.png"></p><br><p>  Below you can see the percentages of responses in the middle of the working day: </p><br><p><img src="https://habrastorage.org/webt/tc/tr/ox/tctroxbqfsqg4bfhgkp6bcsfujc.png"></p><br><p>  We try to regularly evaluate future service load.  We build the forecast for 1-2 years, then with the help of <a href="https://overload.yandex.net/login/%3Fnext%3D/">Lunapark we</a> check that the service will withstand it: </p><br><p><img src="https://habrastorage.org/webt/ct/7v/am/ct7vamsklvamaadk4czzbftycic.png"></p><br><p>  This graph shows that the task search API starts giving out a noticeable number of errors only after 500-600 rps.  This made it possible to estimate that, taking into account the growing load from internal customers and the growth in the amount of data, we will withstand the load after 2 years. </p><br><p>  In addition to the high load of the service, other unpleasant stories can happen, which you should be able to handle so that users do not notice this.  We list some of them. </p><br><ol><li><p>  Data center failure. <br>  A very unpleasant situation, which nevertheless occurs regularly due to the teachings.  What happens with this?  The worst case is when the master monga was in a disconnected DC.  But even in this case, no intervention by the developer or admin is required due to automatic failover.  In elastica, the situation is slightly different: part of the data was in a single copy because  the replication factor is 1. Therefore, it creates new shards on the surviving nodes so that all shards have a backup copy again.  Meanwhile, the balancer over the backend receives a timeout of connections in those requests that were executed on the instances in the disabled DC, or an error from the running backend, whose request went to the missing DC and did not return.  Depending on the circumstances, the balancer may try to repeat the request or return an error to the user.  But in the end, the balancer will understand that the instances from the disconnected DC are unavailable to him and will stop sending requests there, checking in the background whether the DC did not work and whether it is time to return the load there. </p><br></li><li><p>  Loss of connectivity between backend and base / index due to network issues. <br>  Slightly simpler situation at first glance.  Since the balancer over the backend regularly checks its state, the situation when the backend cannot reach the base pops up very quickly.  And the balancer again removes the load from this backend.  There is a danger that if all backends lose connection with the base, they will all be closed, which ultimately will affect 100% of requests. </p><br></li><li>  High load requests in the search tracker. <br>  Search by tasks, their filtering, sorting and aggregation are very time-consuming operations.  Therefore, it is this part of the API that has the strictest load limits.  Previously, we manually found those who filled us with requests and asked them to reduce the load.  Now this is happening more and more, so the inclusion of rate limits made it possible not to notice an excessively active API client. </li></ol><br><h3 id="yandekstreker--dlya-vseh">  Yandex.Tracker - for all </h3><br><p>  Other companies were interested in our service more than once - they learned about our internal tool from those who left Yandex, but could not forget the Tracker.  And last year inside we decided to prepare Tracker for going out into the world - to make a product out of it for other companies. <br>  We immediately began to work out the architecture.  We faced a big challenge in scaling the service to hundreds of thousands of organizations.  Prior to this, the service was developed over the years for one of our companies, taking into account only its needs and nuances.  It became clear that the current architecture will require strong improvements. </p><br><p>  As a result, we had two solutions. </p><br><table><tbody><tr><td>  <b>Separate instances for each organization</b> </td><td>  <b>An instance that can take in thousands of organizations</b> </td></tr><tr><td>  <b>Pros:</b> <br><ul><li>  The minimum number of changes in the already written code </li></ul><br>  <b>Minuses:</b> <br><ul><li>  Expensive resources </li><li>  Difficult monitoring </li><li>  The complexity of the calculations and migrations </li></ul><br></td><td>  <b>Pros:</b> <br><ul><li>  Reasonable use of resources </li><li>  Fast deployment and relatively fast migrations </li><li>  Simple monitoring </li></ul><br>  <b>Minuses:</b> <br><ul><li>  Labor intensity </li></ul><br></td></tr></tbody></table><br><p>  Obviously, the stability of the service for external users is no less important than for internal ones, so it is necessary to duplicate databases, search, backend and frontend in several data centers.  This made the first option much more difficult to maintain - there were many points of failure.  Therefore, we have chosen the second option. </p><br><p>  Rewriting the main part of the project took us two months, for this task it was a record time.  However, in order not to wait, we raised several copies of the tracker on the dedicated hardware, so that we could test the front-end and interaction with related services. </p><br><p> Separately, it is worth noting that even at the design stage, we made a fundamental decision to keep one code base for both Trackers: internal and external.  This allows not to engage in copying code from one project to another, not to reduce the speed of releases and release opportunities outside almost immediately after they appear in our internal tracker. </p><br><p>  But as it turned out, it was not enough to add another parameter to all methods of the application, we also encountered the following problems: </p><br><ol><li>  Non-reticularity monga and elastic.  It was impossible to put the data in one instance, the elastic is bad for a large number of indices, Monga also could not accommodate all organizations.  Therefore, the backend was divided into several large instances, each of which can serve the organizations assigned to it.  Each instance is fault tolerant.  At the same time it is possible to drag the organization between them. </li><li>  The need to perform cron tasks for each organization.  Here we had to solve the problem with each task individually.  Somewhere replaced pull data on push.  Somewhere one cron task generated a separate task for each organization. </li><li>  Different set of task fields in each organization.  Because of the optimization of working with them, we had to write a separate cache for them. </li><li>  Update index mapping.  A fairly common operation that occurs when the tracker is updated to the new version.  Added a phased update mapping mechanism. </li><li>  Opening API on external users.  I had to add a rate limiter, close access to the service API. </li><li>  Availability of support service for rare actions.  Our support and developers do not have the right to look at the user data of organizations, which means that all actions should be carried out by administrators of companies.  Added for them a number of adminok in the interface. </li></ol><br><p>  A separate moment - performance evaluation.  Because of the many reworkings, it was necessary to estimate the speed of work, the number of organizations that would fit in the instance, as well as the supported rps.  Therefore, we conducted regular shooting, having previously placed a large number of organizations in our test tracker.  According to the results, the load limit was determined, after which new organizations would have to be placed in a new instance. </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/kKk42FLaw-c" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  We made one more dedicated dedicated tracker instance to place a <a href="https://demo.tracker.yandex.ru/">demo version</a> in it.  To get into it, you just have to have an account on Yandex.  Some features are blocked in it (for example, file uploading), but you can get acquainted with the real interface of the tracker. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/345044/">https://habr.com/ru/post/345044/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../345034/index.html">How to speed up the selection process</a></li>
<li><a href="../345036/index.html">Glitter and misery of Java web frameworks</a></li>
<li><a href="../345038/index.html">SberShift: press five times and get into the system</a></li>
<li><a href="../345040/index.html">Change the PID of a process in Linux using a kernel module</a></li>
<li><a href="../345042/index.html">IP PBX Zeon. Setting integration Bitrix24</a></li>
<li><a href="../345046/index.html">How to successfully teach yourself to program</a></li>
<li><a href="../345048/index.html">The birth of the supernova: how new features appear on the example of 3D visitors counting</a></li>
<li><a href="../345050/index.html">Micromodule data centers - solution for small VPS / VDS providers</a></li>
<li><a href="../345052/index.html">Focus groups for user research: participant's impressions, criticism and adaptation of the method</a></li>
<li><a href="../345056/index.html">Exfiltration in Metasploit: DNS Tunnel for Meterpreter</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>