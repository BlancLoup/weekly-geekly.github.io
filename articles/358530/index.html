<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Introduction to Data Engineering. ETL, star schema and airflow</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The ability of the data scientist to extract value from data is closely related to how well the data storage and processing infrastructure is develope...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Introduction to Data Engineering. ETL, star schema and airflow</h1><div class="post__text post__text-html js-mediator-article">  The ability of the data scientist to extract value from data is closely related to how well the data storage and processing infrastructure is developed in the company.  This means that the analyst should not only be able to build models, but also have sufficient skills in the field of data engineering to meet the needs of the company and take on more and more ambitious projects. <br><br>  At the same time, despite all the importance, education in the field of data engineering continues to be very limited.  I was lucky, because I managed to work with many engineers who patiently explained to me every aspect of working with data, but not all have this capability.  That is why I decided to write this article - an introduction to data engineering, in which I will talk about what ETL is, the difference between SQL and JVM-oriented ETL, normalization and partitioning of data, and finally consider an example query in Airflow. <br><br><img src="https://habrastorage.org/webt/ye/zb/xk/yezbxkj2ygilru3tiblwbeeidtk.png"><br><a name="habracut"></a><br><h3>  Data Engineering </h3><br>  Maxime Beauchemin, one of the developers of Airflow, described data engineering as follows: ‚ÄúThis is an area that can be viewed as a mixture of business intelligence and databases that brings in more programming elements.  This area includes a specialization in working with distributed big data systems, an expanded Hadoop ecosystem, and scalable computing. " 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Among the many skills of a data engineer, we can single out one that is most important ‚Äî the ability to develop, build, and maintain data warehouses.  The lack of a quality data storage infrastructure means that any activity related to data analysis is either too expensive or unscalable. <br><br><h3>  ETL: Extract, Transform, Load </h3><br>  Extract, Transform, and Load are 3 conceptually important steps that determine how most modern data pipelines are arranged.  Today, this is the basic model of how raw data is made ready for analysis. <br><br><img src="https://habrastorage.org/webt/ta/_0/4r/ta_04r3ins2j2nyg2nv1ed2tmke.png"><br><br>  <b>Extract.</b>  This is the step at which sensors take input from various sources (user logs, copies of a relational database, external data set, etc.), and then pass them on for subsequent transformations. <br><br>  <b>Transform.</b>  This is the ‚Äúheart‚Äù of any ETL, the stage when we apply business logic and do filtering, grouping and aggregation to convert raw data into analysis data ready for analysis.  This procedure requires an understanding of business objectives and the availability of basic knowledge in the field. <br><br>  <b>Load.</b>  Finally, we load the processed data and send it to the place of final use.  The resulting data set can be used by end users, or it can be an input stream to another ETL. <br><br><h3>  What ETL framework to choose? </h3><br>  There are several open source platforms in the world of batch data processing that you can try to play with.  Some of them are: Azkaban is an open-source workflow manager from Linkedin, which features lightweight dependency management in Hadoop, Luigi is a framework from Spotify, based on Python and Airflow, which is also based on Python, from Airbnb. <br><br>  Each platform has its pros and cons, many experts try to compare them (see <a href="https://www.quora.com/Which-is-a-better-data-pipeline-scheduling-platform-Airflow-or-Luigi">here</a> and <a href="http://bytepawn.com/luigi-airflow-pinball.html">here</a> ).  When choosing a particular framework, it is important to consider the following characteristics: <br><br><img src="https://habrastorage.org/webt/c2/0z/lq/c20zlq4dzqssynwodr8ilwb3_sq.png"><br><br>  <b>Configuration.</b>  ETLs are by their nature quite complex, so it‚Äôs important how the user of the framework will design them.  Is it based on the user interface or are the queries created in any programming language?  Today, the second method is gaining more and more popularity, because programming of the pipelines makes them more flexible, allowing you to change any detail. <br><br>  <b>Monitoring errors and alerts.</b>  Bulk and long batch requests sooner or later fall with an error, even if there are no bugs in the job itself.  As a result, monitoring and error reporting come to the fore.  How well does the framework visualize query progress?  Do alerts arrive on time? <br><br>  <b>Backfilling data.</b>  Often, after building the finished pipeline, we need to go back and re-process historical data.  Ideally, we would not like to build two independent jobs, one for the reverse of historical data and one for current activities.  How easy is backfilling with this framework?  Is the solution obtained scalable and effective? <br><br><h3>  2 paradigms: SQL vs. JVM </h3><br>  As we found out, companies have a huge choice of which tools to use for ETL, and for the novice data scientist, it‚Äôs not always clear what framework to devote time to.  This is about me: in Washington Post Labs, the sequence of jobs was carried out primitively, with the help of Cron, on Twitter, ETL jobs were built in Pig, and now in Airbnb we write pipelines to Hive via Airflow.  Therefore, before you go to this or that company, try to find out exactly how ETL is organized in them.  Simplified, two main paradigms can be distinguished: <i>SQL</i> and <i>JVM-oriented ETL.</i> <br><br>  <b>JVM-oriented ETL is</b> usually written in a JVM-oriented language (Java or Scala).  Building data pipelines in such languages ‚Äã‚Äãmeans defining data transformations through key-value pairs, however, it becomes easier to write user-defined functions and test jobs, since there is no need to use another programming language to do this.  This paradigm is very popular among engineers. <br><br>  <b>SQL-oriented ETL is</b> most often written in SQL, Presto or Hive.  In them, almost everything revolves around SQL and tables, which is very convenient.  At the same time, writing custom functions can be problematic, since it requires the use of another language (for example, Java or Python).  This approach is popular among data scientists. <br><br>  Having worked with both paradigms, I still prefer SQL-oriented ETL, because, being a beginner data scientist, it is much easier to learn SQL than Java or Scala (if, of course, you are not familiar with them) and concentrate on learning new practices than to impose it on top of learning a new language. <br><br><h3>  Data modeling, normalization and star schema </h3><br>  In the process of building a high-quality analytical platform, the main goal of the system designer is to make analytical queries easy to write, and various statistics considered effective.  To do this, first of all you need to determine <i>the data model.</i> <br><br>  As one of the first steps in data modeling, it is necessary to understand the extent to which tables should be <i><a href="http://ru.wikipedia.org/wiki/%25D0%259D%25D0%25BE%25D1%2580%25D0%25BC%25D0%25B0%25D0%25BB%25D1%258C%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2584%25D0%25BE%25D1%2580%25D0%25BC%25D0%25B0">normalized</a></i> .  In the general case, normalized tables are distinguished by simpler schemes, more standardized data, and also exclude some types of redundancy.  At the same time, the use of such tables leads to the fact that to establish the relationship between the tables requires more accuracy and diligence, queries become more difficult (more JOIN-s), and also need to support more ETL jobs. <br><br>  On the other hand, it is much easier to write queries to denormalized tables, since all dimensions and metrics are already connected.  However, given the larger size of the tables, data processing becomes slower (‚ÄúYou can argue, because everything depends on how the data is stored and what requests are. For example, you can store large tables in Hbase and access individual columns, then the queries will be fast ‚Äù- lane comment). <br><br>  Among all the data models that are trying to find the perfect balance between the two approaches, one of the most popular (we use it in Airbnb) is <a href="http://ru.wikipedia.org/wiki/%25D0%25A1%25D1%2585%25D0%25B5%25D0%25BC%25D0%25B0_%25D0%25B7%25D0%25B2%25D0%25B5%25D0%25B7%25D0%25B4%25D1%258B">the star scheme</a> .  This scheme is based on the construction of normalized tables (fact tables and dimension tables), from which, in which case, denormalized tables can be obtained.  As a result, this design attempts to strike a balance between the ease of analytics and the complexity of ETL support. <br><br><img src="https://habrastorage.org/webt/x6/-y/1q/x6-y1qfbij4li8bl9kmwpszbqlq.png"><br><br><h3>  Fact tables and dimension tables </h3><br>  To better understand how to build denormalized tables from fact tables and dimension tables, we will discuss the roles of each of them: <br><br>  <b>Fact tables</b> often contain transactional data at specific points in time.  Each row in a table can be extremely simple and most often is a single transaction.  We have a lot of fact tables in Airbnb that store data by type of transaction: booking, ordering, cancellation, etc. <br><br>  <b>Dimension tables</b> contain slowly changing attributes of certain keys from the fact table, and they can be connected to it by these keys.  The attributes themselves can be organized within a hierarchical structure.  Airbnb, for example, has dimension tables with users, orders, and markets that help us analyze data in detail. <br><br>  Below is a simple example of how fact tables and dimension tables (normalized) can be combined to answer a simple question: how many bookings have been made in the last week for each of the markets? <br><br><pre><code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> b.dim_market , <span class="hljs-keyword"><span class="hljs-keyword">SUM</span></span>(a.m_bookings) <span class="hljs-keyword"><span class="hljs-keyword">AS</span></span> m_bookings <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> ( <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> id_listing , <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">AS</span></span> m_bookings , m_a <span class="hljs-comment"><span class="hljs-comment"># not used (for illustration only) , m_b # not used (for illustration only) , m_c # not used (for illustration only) FROM fct_bookings WHERE ds BETWEEN '{{ last_sunday }}' AND '{{ this_saturday }}' ) a JOIN ( SELECT id_listing , dim_market , dim_x # not used (for illustration only) , dim_y # not used (for illustration only) , dim_z # not used (for illustration only) FROM dim_listings WHERE ds BETWEEN '{{ latest_ds }}' ) b ON (a.id_listing = b.id_listing) GROUP BY b.dim_market ;</span></span></code> </pre> <br><h3>  Timestamp Partitioning </h3><br>  Now, when the cost of storing data is very small, companies can afford to store historical data in their vaults, rather than throwing it away.  The flip side of this trend is that with the accumulation of data, analytical queries become ineffective and slow.  Along with such SQL principles as ‚Äúfilter data more often and earlier‚Äù and ‚Äúuse only those fields that are needed‚Äù, we can single out another one that allows increasing the efficiency of queries: <a href="http://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B5%25D0%25BA%25D1%2586%25D0%25B8%25D0%25BE%25D0%25BD%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5">data partitioning</a> . <br><br>  The basic idea of ‚Äã‚Äãpartitioning is quite simple - instead of storing data in one piece, we divide them into several independent parts.  All parts retain the primary key from the original piece, so you can access any data fairly quickly. <br><br>  In particular, the use of a timestamp as a key for which the partitioning takes place has several advantages.  First, in S3-type repositories, raw data is often sorted by timestamp and stored in directories, also labeled.  Secondly, the batch-ETL job usually takes about one day, that is, new data partitions are created every day for each job.  Finally, many analytical queries include counting the number of events that have occurred over a certain time period, so partitioning over time is very useful here. <br><br><h3>  Backfilling of historical data </h3><br>  Another important advantage of using a timestamp as a partitioning key is the ease of backfilling data.  If the ETL pipeline is already built, then it calculates the metrics and measurements in advance, not retrospectively.  Often we would like to look at the established trends by calculating measurements in the past - this process is called <i>backfilling</i> . <br><br>  Backfilling is so common that Hive has built-in <a href="http://cwiki.apache.org/confluence/display/Hive/DynamicPartitions">dynamic partitioning</a> to perform the same SQL queries across several partitions at once.  We illustrate this idea with an example: let it be required to fill in the number of bookings for each market for dashboards, starting with <i>earliest_ds</i> and ending with the <i>latest_ds</i> .  One of the possible solutions looks like this: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> OVERWRITE <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> bookings_summary <span class="hljs-keyword"><span class="hljs-keyword">PARTITION</span></span> (ds= <span class="hljs-string"><span class="hljs-string">'{{ earliest_ds }}'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> dim_market , <span class="hljs-keyword"><span class="hljs-keyword">SUM</span></span>(m_bookings) <span class="hljs-keyword"><span class="hljs-keyword">AS</span></span> m_bookings <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> fct_bookings <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> ds = <span class="hljs-string"><span class="hljs-string">'{{ earliest_ds }}'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">GROUP</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> dim_market ; <span class="hljs-comment"><span class="hljs-comment"># after many insertions from '{{ earliest_ds + 1 day }}' to '{{ latest_ds - 1 day }}' INSERT OVERWRITE TABLE bookings_summary PARTITION (ds= '{{ latest_ds }}') SELECT dim_market , SUM(m_bookings) AS m_bookings FROM fct_bookings WHERE ds = '{{ latest_ds }}' GROUP BY dim_market ;</span></span></code> </pre> <br>  Such a query is possible, but it is too cumbersome, since we perform the same operation, only on different partitions.  Using dynamic partitioning, we can simplify everything to a single query: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> OVERWRITE <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> bookings_summary <span class="hljs-keyword"><span class="hljs-keyword">PARTITION</span></span> (ds) <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> dim_market , <span class="hljs-keyword"><span class="hljs-keyword">SUM</span></span>(m_bookings) <span class="hljs-keyword"><span class="hljs-keyword">AS</span></span> m_bookings , ds <span class="hljs-comment"><span class="hljs-comment"># For Hive to know we are using dynamic partitions FROM fct_bookings WHERE ds BETWEEN '{{ earliest_ds }}' AND '{{ latest_ds }}' GROUP BY dim_market , ds ;</span></span></code> </pre> <br>  Note that we added <i>ds</i> to the <i>SELECT</i> and <i>GROUP BY</i> expressions, extended the range in the <i>WHERE</i> operation, and changed the syntax from <i>PARTITION (ds = '{{ds}}}) to PARTITION (ds)</i> .  The beauty of dynamic partitioning is that we wrapped <i>GROUP BY ds</i> around the necessary operations to insert query results into all partitions in one run.  This approach is very effective and is used in many Airbnb pipelines. <br><br>  Now, consider all the concepts studied on the example of ETL Jobs in Airflow. <br><br><h3>  Directed Acyclic Graph (DAG) </h3><br>  It would seem that from the point of view of the ETL idea, the Jobs are very simple, but in fact they are often very confusing and consist of many combinations of Extract, Transform, and Load operations.  In this case, it is very useful to visualize the entire data stream using a graph in which the node displays the operation, and the arrow shows the relationship between the operations.  Given that each operation is performed once, and the data goes further along the graph, it is directional and acyclic, hence the name. <br><br><img src="https://habrastorage.org/webt/ss/02/zt/ss02ztw6owkeb2cdk5pajepie7o.png"><br><br>  One of the features of the Airflow interface is the presence of a mechanism that allows you to <a href="http://airflow.apache.org/ui.html">visualize the pipeline data</a> through the DAG.  The author of the pipeline must define the interrelationships between operations so that Airflow writes the ETL Jobe specification to a separate file. <br><br>  At the same time, in addition to DAGs, which determine the order in which operations are launched, there are operators in Airflow that specify what needs to be done within the pipeline.  Usually there are 3 types of operators, each of which simulates one of the stages of the ETL process: <br><br><ul><li>  <b>Sensors:</b> open data stream after a certain time, or when data from the input source become available (by analogy with Extract). </li><li>  <b>Operators:</b> run certain commands (execute a python file, query in Hive, etc.).  By analogy with Transform, operators are engaged in data transformation. </li><li>  <b>Transfers:</b> transfer data from one place to another (as in the Load stage). </li></ul><br><h3>  Simple example </h3><br>  Below is a simple example of how to declare a DAG file and define the structure of a graph using the operators in Airflow, which we discussed above: <br><br><pre> <code class="python hljs"><span class="hljs-string"><span class="hljs-string">""" A DAG docstring might be a good way to explain at a high level what problem space the DAG is looking at. Links to design documents, upstream dependencies etc are highly recommended. """</span></span> <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> datetime <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> datetime, timedelta <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> DAG <span class="hljs-comment"><span class="hljs-comment"># Import the DAG class from airflow.operators.sensors import NamedHivePartitionSensor from airflow.operators.hive_operator import HiveOperator ### You can import more operators as you see fit! # from airflow.operators.bash_operator import BashOperator # from airflow.operators.python_operator import PythonOperator # setting some default arguments for the DAG default_args = { 'owner': 'you', 'depends_on_past': False, 'start_date': datetime(2018, 2, 9), } # Instantiate the Airflow DAG dag = DAG( dag_id='anatomy_of_a_dag', description="This describes my DAG", default_args=default_args, schedule_interval=timedelta(days=1)) # This is a daily DAG. # Put upstream dependencies in a dictionary wf_dependencies = { 'wf_upstream_table_1': 'upstream_table_1/ds={{ ds }}', 'wf_upstream_table_2': 'upstream_table_2/ds={{ ds }}', 'wf_upstream_table_3': 'upstream_table_3/ds={{ ds }}', } # Define the sensors for upstream dependencies for wf_task_id, partition_name in wf_dependencies.iteritems(): NamedHivePartitionSensor( task_id=wf_task_id, partition_names=[partition_name], dag=dag ) # Put the tasks in a list tasks = [ ('hql', 'task_1'), ('hql', 'task_2'), ] # Define the operators in the list above for directory, task_name in tasks: HiveOperator( task_id=task_name, hql='{0}/{1}.hql'.format(directory, task_name), dag=dag, ) # Put the dependencies in a map deps = { 'task_1': [ 'wf_upstream_table_1', 'wf_upstream_table_2', ], 'task_2': [ 'wf_upstream_table_1', 'wf_upstream_table_2', 'wf_upstream_table_3', ], } # Explicitly define the dependencies in the DAG for downstream, upstream_list in deps.iteritems(): for upstream in upstream_list: dag.set_dependency(upstream, downstream)</span></span></code> </pre> <br>  When the graph is built, you can see the following image: <br><br><img src="https://habrastorage.org/webt/9r/-i/ho/9r-ihomnbfbftaklp56oebitbmc.png"><br><br>  So, I hope that in this article I managed to quickly and efficiently immerse you in an interesting and diverse area - Data Engineering.  We learned what ETL is, the advantages and disadvantages of various ETL platforms.  Then they discussed data modeling and the ‚Äústar‚Äù scheme, in particular, and also considered the differences between fact tables and measurement tables.  Finally, having considered such concepts as data partitioning and backfilling, we switched to the example of a small ETL job in Airflow.  Now you can independently study the work with data, increasing the baggage of your knowledge.  See you! <br><br>  ‚Äî‚Äî‚Äî‚Äî <br><br>  Robert notes an insufficient number of data engineering programs in the world, but we are doing so, and not for the first time.  In October, <a href="https://goo.gl/sgAkFq">Data Engineer 3.0</a> starts with us, register and expand your professional capabilities! </div><p>Source: <a href="https://habr.com/ru/post/358530/">https://habr.com/ru/post/358530/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../358520/index.html">Java and Linux - features of operation</a></li>
<li><a href="../358522/index.html">Go: we accelerate sampling of large tables from MySQL</a></li>
<li><a href="../358524/index.html">We will again be counted: National biometric platform and ‚Äúpass-through identifier‚Äù</a></li>
<li><a href="../358526/index.html">Number all real numbers on the interval [0,1]</a></li>
<li><a href="../358528/index.html">REST-API automatic documentation system in Laravel projects</a></li>
<li><a href="../358532/index.html">Open webinar: ‚ÄúTranslation difficulties: 2 and 3 versions‚Äù</a></li>
<li><a href="../358536/index.html">The Telegram v. Russia case will be considered by the European Court of Human Rights. Also sent a complaint to the Moscow City Court</a></li>
<li><a href="../358538/index.html">New orienteering: how to determine your location</a></li>
<li><a href="../358540/index.html">First Atlassian User Group in Minsk</a></li>
<li><a href="../358544/index.html">One day as a user support employee. What does it change?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>