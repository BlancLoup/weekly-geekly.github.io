<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How we beat the twilight between testing and exploitation</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Some time ago we in HeadHunter discovered the ‚Äútwilight zone‚Äù when transferring a new version of the site from testing to operation. Lack of attention...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How we beat the twilight between testing and exploitation</h1><div class="post__text post__text-html js-mediator-article">  Some time ago we in HeadHunter discovered the ‚Äútwilight zone‚Äù when transferring a new version of the site from testing to operation.  Lack of attention to the difference between test and combat infrastructure periodically led to the fall of the site. <br><br><img src="https://habrastorage.org/files/9f2/0ab/ee0/9f20abee072444ba877d14fc96587dc3.jpg" alt="Exit dusk"><br><br>  The old test benches differed noticeably in their internal structure from the working cluster.  The init-scripts for launching services differed, the configuration files differed in location and content.  The interaction of services with each other occurred without taking into account the characteristics of the combat environment. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      I will show the logic of our solution, which allowed us to achieve qualitatively new test results. <br><br>  This article continues my <a href="https://vimeo.com/148867025">report on SQA Days-18</a> . <br><a name="habracut"></a><br><h1>  Configuration files </h1><br>  The first moment we saw was different configuration files on test stands and in battle.  In our case, these were even different files by location.  If we talk about their content, they were written by different people and in different ways.  What does it mean?  This means that each package brought with it to the combat servers the default settings file that was in / var / lib / ... or / usr / shared / ... These are the settings that are convenient in the test environment.  And those that need to be replaced were already in the configuration file in / etc / package_name.  Settings were recorded by system administrators when they received the corresponding task in Jira. <br><br>  Did such an interaction scheme guarantee that the config files in the battle are configured correctly?  Absolutely not.  Files with settings that were used in the combat environment, have not been tested!  And so testers and developers were frequent guests in operation with the question: ‚ÄúWe asked this week to change this setting.  Have you registered it?  Show what you got. ‚Äù <br><br>  Why did this situation arise?  Because on the test stands, as we used to see them, services live on the same server.  In the same file system.  And if the settings of one of them need to be changed in the / etc / default / jetty file, the settings of the other will have to be changed somehow differently.  To do this, special init scripts were written on the test bench to manage different services.  And in the same place, in the init script, the config files that were in non-standard places were indicated. <br><br><h1>  To resolve a conflict </h1><br>  How to resolve this conflict?  We decided that we need isolation of services from each other at the file system level.  After all, in battle, each service revolves on a separate server or virtual server. <br><br>  Perhaps chroot can solve the problem of isolating services on a test bench at the file system level.  Then each service will have its own / etc folder and its configuration files, which are located in the same place where they are located on the combat servers.  And the log files will be in the same folders as in combat.  And this solution brings to the solution of the problem, how to use the same config files on the test bench as in the production environment. <br><br><h1>  Is file system isolation enough? </h1><br>  On the old test benches, all services listened to localhost, each at its own port.  And communicated with each other through localhost.  However, on this site, each service lives on its server.  And, moreover, each service is launched in 2 or more copies.  This solution is necessary, firstly, for load distribution and horizontal scalability of the service.  And, secondly, to ensure the reliability of the service.  In cases where one of the servers must be stopped for maintenance, the others take on the same part of their work. <br><br>  Thus, to ensure that requests are distributed across multiple servers that serve a single server, you need an internal load balancer. <br><br><h1>  Sysadmin profitable? </h1><br>  And here we saw another item for our to-do list.  Balancer configuration, this is what we have never tested before!  Therefore, the risk associated with the creation of the nginx configuration, which acts as a balancer, rested entirely with the sysadmins.  And they had no opportunity to check its correct work anywhere, except on a live site.  And sometimes the experiments did not end very well ... <br><br>  Interesting.  It turns out that system administrators can benefit from the implementation of the new test bench operation scheme.  Perhaps they will be able to attract new stands to the project. <br><br><h1>  Balancer </h1><br>  And there will be a load balancer on the new stands.  Then we need to configure the nginx configuration of the IP and ports of the servers in the upstream.  It may be more convenient to really allocate to each service a server on which it can work. <br>  So, in addition to isolating servers by file system, we added isolation over IP.  In addition, ancillary services, such as rsyslog, will be able to work on the same principle as on this site.  Perhaps, only if the config files of each service are the same as in combat. <br><br>  And this is the third item on our to-do list.  How to ensure the use of the same configuration files and with the same content, both on test benches and on combat servers? <br><br>  How to achieve the same configs? <br>  If we have already decided that each service will be launched on a separate server, then maybe we can use the deploy scripts that the system administrators have?  And with their help, lay out the same configuration files on the test bench as on the big site?  Yes, we can do that.  Given the fact that the passwords from payment systems or SMS mailings should remain secret. <br><br>  Then, finally, it will be possible to store the display scripts and configs on GitHub, because the secret data has been cleaned from there.  Both developers and testers will stop visiting system administrators with requests to share the settings as they are implemented on the site. <br><br><h1>  Configs on github </h1><br>  To hide passwords in configuration files, we use variables.  Because the files of configs themselves became Jinja2 templates for Ansible.  We wrote our calculation system using it.  And Ansible allows us to have two sets of variables, i.e.  two pairs of folders group_vars and host_vars, where the values ‚Äã‚Äãof variables are defined.  One set is in the playbooks folder, and the other is in the folder with the inventory file.  And one of these sets always takes precedence over the other. <br><br>  Thus, we put in GitHub not only display scripts and config-files, but also one set of variable values, if they are not secret.  These can be memory limitations for applications, the number of threads or forknuf processes, and timeouts.  Those.  those values ‚Äã‚Äãthat differ in the battle and on the test stand. <br><br><h1>  Keep secrets </h1><br>  Secret values, such as passwords to payment systems and services of SMS mailings, as well as passwords to the database, are in the private repository of system administrators and are not available to testers. <br><br>  On the test bench, instead of a private set of variables from the operation service, testers use their values ‚Äã‚Äãfrom their repository.  There, they define both passwords specific to the test bench, and the values ‚Äã‚Äãof memory limits, the number of threads and processes, or timeouts that are specific to the test environment. <br><br>  Separate server for each stand - wasteful <br>  From the above, we can see what requirements those individual servers for the services that we will run on the test bench must meet: <br><br><ul><li>  file system isolation; </li><li>  a separate IP address; </li><li>  init to run our and related packages; </li><li>  opensshd for Ansible. </li></ul><br>  These requirements can satisfy Linux, LXC containers.  An additional advantage of using them is that they use shared memory, allocating it as applications inside containers request it.  And, unlike virtual machines, do not bite off immediately a large piece of RAM.  Thanks to this we save memory. <br><br><h1>  What are the benefits of our decision? </h1><br>  As a result of the fact that we <br><br><ul><li>  identified a pseudo server for each service on the test bench; </li><li>  use the same calculation scripts and the same settings files as in the battle; </li><li>  the internal balancer was repeated on a Linux container inside the stand; </li><li>  are forced to first build the package from the Git branch in order to roll it out to your pseudo-server using deploy scripts, </li></ul><br>  We have achieved some results. <br><br>  First, we moved from testing the code branch to testing the package for our distribution.  Now we can be sure that in combat the package will be able to install on a clean server and create all the necessary folders with sufficient rights for it to work. <br><br>  Secondly, we began to test the very config files that will be used on this site.  And, therefore, we have eliminated the human factor in writing configs, which could lead to the fall of the site.  The differences between the stand and the production environment we made in variables. <br><br>  Third, we started testing the balancer configuration.  Thus, at the stage of preparing the task, we check the interaction of services with each other precisely in the infrastructure that works in battle. <br><br>  And fourthly.  Now we can run two or more instances of each service.  We can not only debug the work of retries in nginx, but also test how the site behaves during the release of the new version. <br><br>  Imagine that now you can run AutoTests at the moment when we simulate the release of a new version of the site!  And to achieve stable operation of the stand at a time when one server is already working on the new software version, the second one is stopped for updating, and the third one is still responding with the old version.  Here is a worthy task! <br><br><h1>  Result </h1><br>  Summarize.  Reengineering of the testing process and refactoring of test benches gave an excellent result.  For 2 years the site uptime exceeded 99.9%.  This is a great indicator if you count it in minutes.  For one month a simple site is less than 43 minutes.  At the same time, we have tightened the definition downtime 3 times, from 60 500 errors per second to 20. <br><br>  And for internet business, which HeadHunter is, improving uptime means real money saving.  Add to this the clients who were attracted by hh.ru due to the more stable operation of the site than before.  What do you think, is uptime site a key success factor for your business? <br><br>  So, 4 simple steps: <br><br><ul><li>  allocate to each service a separate server on the stand; </li><li>  test the package, not the branch with the code; </li><li>  use the calculation scripts from operation and test the config files; </li><li>  repeat the balancer and test the release process (the <a href="http://habrahabr.ru/users/irreality/" class="user_link">irreality</a> has already been <a href="http://habrahabr.ru/company/hh/blog/271221/">told</a> about how to do this). </li></ul><br>  The victory of light is inevitable! <br><br></div><p>Source: <a href="https://habr.com/ru/post/272067/">https://habr.com/ru/post/272067/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../272055/index.html">Thinking out loud about TypeScript</a></li>
<li><a href="../272057/index.html">Accident history on the UPS</a></li>
<li><a href="../272059/index.html">New dynamic objects and JSON support in InterSystems Cach√©</a></li>
<li><a href="../272061/index.html">Reservation of internal and external communication channels, static routing, corporate network on MikroTik</a></li>
<li><a href="../272063/index.html">Using LRDIMM Modules in High-Performance Servers</a></li>
<li><a href="../272071/index.html">How do residents of different cities communicate</a></li>
<li><a href="../272073/index.html">Underground carders market. Translation of the book "KingPIN". Chapter 18. "Briefing"</a></li>
<li><a href="../272075/index.html">Creating a desktop application with Electron and web technologies</a></li>
<li><a href="../272077/index.html">How to send an IDoc from SAP MII to SAP ERP</a></li>
<li><a href="../272083/index.html">NetApp becomes more efficient with inline dedupe</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>