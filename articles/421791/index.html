<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Ethical Issues of Artificial Intelligence</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The author of the article is Alexey Malanov, an expert in the development of anti-virus technologies at Kaspersky Lab. 

 Artificial intelligence brea...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Ethical Issues of Artificial Intelligence</h1><div class="post__text post__text-html js-mediator-article">  <i>The author of the article is Alexey Malanov, an expert in the development of anti-virus technologies at Kaspersky Lab.</i> <br><br>  Artificial intelligence breaks into our lives.  In the future, everything will probably be cool, but so far some questions have arisen, and more and more often these questions touch upon aspects of morality and ethics.  Is it possible to mock thinking AI?  When will it be invented?  What prevents us from already writing the laws of robotics, putting morality in them?  What surprises does machine learning present us to now?  Is it possible to deceive machine learning, and how difficult is it? <a name="habracut"></a><br><br><h1>  Strong and Weak AI - different things </h1><br>  There are two different things: Strong and Weak AI. <br>  Strong AI (true, general, real) is a hypothetical machine that can think and be aware of itself, solve not only highly specialized tasks, but also learn something new. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Weak AI (narrow, superficial) - these are already existing programs for solving well-defined tasks, such as image recognition, driving, playing Go, and so on. In order not to be confused and not mislead anyone, we prefer to call Weak AI "machine learning (machine learning). <br><br><h1>  A strong AI will not be soon </h1><br>  About Strong AI it is not yet known whether it will ever be invented at all.  On the one hand, so far technologies have evolved with acceleration, and if it goes on like this, then there are five years left. <br><br><img src="https://habrastorage.org/webt/_i/sv/en/_isvenid4vjjjfsvl5nhnve4xee.jpeg"><br><br>  On the other hand, few processes in nature actually take place exponentially.  More often, we still see the logistic curve. <br><br><img src="https://habrastorage.org/webt/72/uq/hj/72uqhj_g_zj1dn6wq62ewvczwya.png"><br><br>  While we are somewhere on the left of the graph, it seems to us that this is an exhibitor.  For example, more recently, Earth‚Äôs population has grown with such acceleration.  But at some point there is a "saturation", and growth slows down. <br><br>  When experts are <a href="https://www.technologyreview.com/s/607970/experts-predict-when-artificial-intelligence-will-exceed-human-performance/">interviewed</a> , it turns out that on average, to wait another 45 years. <br><br><img src="https://habrastorage.org/webt/3r/ro/h4/3rroh4slw6_2yu-lpodoret9vxq.png"><br><br>  What is curious, North American scientists believe that AI will surpass a person in 74 years, and Asian scientists - in just 30. Maybe in Asia they know something like that ... <br><br>  The same scientists predicted that the car would translate better than a man by 2024, write school essays by 2026, drive trucks by 2027, play Go, also by 2027.  With Guo, the bobble has already come out, because this moment came in 2017, just 2 years after the forecast. <br><br>  Well, in general, forecasts for 40+ years ahead are a thankless task.  It means "sometime."  For example, the cost-effective energy of thermonuclear fusion is also predicted in 40 years.  The same forecast was given 50 years ago, when it was just begun to be studied. <br><br><img src="https://habrastorage.org/webt/pi/3u/v-/pi3uv-ptktxeymxbjxl2_mxrujs.png" align="right"><h1>  Strong AI raises a host of ethical issues. </h1><br>  Although a strong AI will wait for a long time, but we know for sure that there will be enough ethical problems.  The first class of problems - we can offend the AI.  For example: <br><br><ul><li>  Is it ethical to torture an AI if he is able to feel pain? </li><li>  Is it normal to leave an AI without communication for a long time if he is able to feel loneliness? </li><li>  Can you use it as a pet?  And what about a slave?  And who will control it and how, because this is a program that works "lives" in your "smartphone"? </li></ul><br>  Now no one will be indignant if you offend your voice assistant, but if you treat the dog badly, you will be convicted.  And this is not because it is made of flesh and blood, but because it feels and experiences a bad relationship, as it will with Strong AI. <br><br>  The second class of ethical problems - AI can offend us.  Hundreds of such examples can be found in films and books.  How to explain the AI, what do we want from it?  People for AI are like ants for workers building a dam: for the sake of a great goal, you can crush a couple. <br><br>  Science fiction is playing a trick on us.  We used to think that Skynet and Terminators are not there yet, and they will be soon, but for now you can relax.  AI in films is often malicious, and we hope that this will not happen in life: after all, we were warned, and we are not as stupid as film characters.  At the same time, in our thoughts about the future, we forget to think well about the present. <br><br><h1>  Machine learning is here </h1><br>  Machine learning allows you to solve a practical problem without explicit programming, but by learning from precedents.  You can read more in the article ‚Äú <a href="https://www.kaspersky.ru/blog/machine-learning-explained/13605/">In simple words: how machine learning works</a> .‚Äù <br><br>  Since we teach the machine to solve a specific problem, the resulting mathematical model (the so-called algorithm) cannot suddenly want to enslave / save humanity.  Do normally - it will be normal.  What can go wrong? <br><br><img src="https://habrastorage.org/webt/cr/ng/y6/crngy6nspkfqx_7uxxt_trxffnk.png" align="right"><h1>  Bad intentions </h1><br>  First, the problem itself may not be ethical enough.  For example, if we teach drones to kill people with machine learning. <br><img src="https://habrastorage.org/webt/2w/cq/oi/2wcqoiuu8xuass9vd7uulmkkc9m.png"><br><br>  <a href="https://www.youtube.com/watch%3Fv%3DTlO2gcs1YvM">https://www.youtube.com/watch?v=TlO2gcs1YvM</a> <br><br>  Just recently a small scandal flared up on this.  Google is developing software used for the Project Maven military drone pilot project.  Presumably, in the future this could lead to the creation of fully autonomous weapons. <br><br><img src="https://habrastorage.org/webt/sr/wa/ei/srwaeiionmujk7cx1zj-fzf-s0g.jpeg"><br>  <a href="https://www.popmech.ru/weapon/232198-10-samykh-nepravdopodobnykh-tankov-zvyezdnykh-voyn/">A source</a> <br><br><img src="https://habrastorage.org/webt/8f/zx/1m/8fzx1mjghd0aq-e0yfrrsowwhuq.png" align="right">  So, at least 12 employees of Google resigned in protest, another 4000 signed a petition asking them to abandon the contract with the military.  More than 1,000 prominent scientists in the field of AI, ethics and information technology have written <a href="https://www.icrac.net/open-letter-in-support-of-google-employees-and-tech-workers/">an open letter</a> asking Google to stop working on the project and support an international treaty to ban autonomous weapons. <br><br><h1>  "Greedy" bias </h1><br>  But even if the authors of the machine learning algorithm and do not want to kill people and bring harm, they, nevertheless, often still want to benefit.  In other words, not all algorithms work for the good of society, many work for the benefit of the creators.  This can often be observed in the field of medicine - it is more important not to cure, but to recommend more treatment. <img src="https://habrastorage.org/webt/8f/zx/1m/8fzx1mjghd0aq-e0yfrrsowwhuq.png" align="right">  In general, if machine learning advises something paid - with high probability the algorithm is ‚Äúgreedy‚Äù. <br><br>  Well, and sometimes the society itself is not interested in the resulting algorithm being a model of morality.  For example, there is a compromise between the speed of traffic and mortality on the roads.  We could greatly reduce mortality if we limited the speed to 20 km / h, but then life in big cities would be difficult. <br><br><h1>  Ethics is only one of the parameters of the system. </h1><br><img src="https://habrastorage.org/webt/cr/ng/y6/crngy6nspkfqx_7uxxt_trxffnk.png" align="right">  Imagine, we are asking the algorithm to impose a country's budget in order to ‚Äúmaximize GDP / labor productivity / life expectancy.‚Äù  In the formulation of this problem there are no ethical restrictions and goals.  Why allocate money for orphanages / hospices / environmental protection, because it does not increase GDP (at least, directly)?  And it‚Äôs good if we only assign the budget to the algorithm, otherwise, in a broader formulation of the problem, it will be released that it is ‚Äúmore profitable‚Äù to kill the inoperative population right away in order to increase labor productivity. <br><br>  It turns out that ethical issues should be among the goals of the system initially. <br><br><h1>  Ethics are hard to describe formally. </h1><br>  There is one problem with ethics - it is difficult to formalize it.  Different countries have different ethics.  It changes with time.  For example, on issues such as LGBT rights and interracial / inter-caste marriages, opinions may change significantly over the decades.  Ethics may depend on the political climate. <br><img src="https://habrastorage.org/webt/oe/bj/x9/oebjx9ygsiqyfs-zap5diznne2i.png"><br><br><img src="https://habrastorage.org/webt/8f/zx/1m/8fzx1mjghd0aq-e0yfrrsowwhuq.png" align="right">  For example, in China, <a href="http://www.businessinsider.com/how-china-is-watching-its-citizens-in-a-modern-surveillance-state-2018-4">controlling the movement of citizens</a> with the help of surveillance cameras and face recognition is considered the norm.  In other countries, the attitude to this issue may be different and depend on the situation. <br><br><h1>  Machine learning affects people </h1><br>  Imagine a machine learning based system that advises you which movie to watch.  Based on your ratings for other films, and by comparing your tastes with the tastes of other users, the system can quite reliably recommend a film that you‚Äôll like very much. <img src="https://habrastorage.org/webt/8f/zx/1m/8fzx1mjghd0aq-e0yfrrsowwhuq.png" align="right"><br><br>  But at the same time, the system will eventually change your tastes and make them more narrow.  Without a system, you would occasionally watch both bad films and films of unusual genres.  And so that no movie - to the point.  As a result, we cease to be "film experts", and become only consumers of what they give.  Another interesting fact is that we do not even notice how the algorithms manipulate us. <br><br>  If you say that the impact of algorithms on people is even good, here is another example.  In China, the Social Rating System is being prepared for launch - a system for evaluating individual citizens or organizations for various parameters, the values ‚Äã‚Äãof which are obtained using mass monitoring tools and using big data analysis technology. <img src="https://habrastorage.org/webt/cr/ng/y6/crngy6nspkfqx_7uxxt_trxffnk.png" align="right">  If a person buys diapers - this is good, the rating grows.  If you spend money on video games - this is bad, the rating drops.  If communicates with a person with a low rating, it also falls. <br><br>  As a result, it turns out that thanks to the System, citizens consciously or subconsciously begin to behave differently.  Communicate less with unreliable citizens, buy more diapers, etc. <br><br><h1>  Algorithmic system error </h1><br>  Besides the fact that we sometimes do not know what we want from the algorithm, there is also a whole bunch of technical limitations. <br><br>  The algorithm absorbs the imperfections of the surrounding world. <img src="https://habrastorage.org/webt/8f/zx/1m/8fzx1mjghd0aq-e0yfrrsowwhuq.png" align="right">  If data from a company with racist politicians is used as a training sample for an algorithm for hiring employees, the algorithm will also be racist. <br><br>  Microsoft once taught a chat bot to chat on Twitter.  It <a href="https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist">had to be turned off in</a> less than a day, because the bot quickly mastered curses and racist remarks. <br><br><img src="https://habrastorage.org/webt/-o/vn/yb/-ovnybi6tui2yl-e3jewbkjlkae.png"><br><br>  In addition, the algorithm during training can not take into account some non-formalizable parameters.  For example, when calculating a recommendation to a defendant - whether or not to admit guilt on the basis of collected evidence, the algorithm finds it difficult to take into account what impression such a confession will make on a judge, because the impression and emotions are not recorded anywhere. <br><br><h1>  False correlations and "feedback loops" </h1><br>  False correlation - this is when it seems that the more firefighters in the city, the more often fires.  Or when it is obvious that the fewer pirates on Earth, the warmer the climate on the planet. <br><br><img src="https://habrastorage.org/webt/ch/d6/yy/chd6yybu-p8-kwd90abngmtlnm8.jpeg"><br><br>  So - people suspect that pirates and the climate are not directly related, and firefighters are not so simple, and Matmodel of machine learning simply learns and generalizes. <br><br><img src="https://habrastorage.org/webt/8f/zx/1m/8fzx1mjghd0aq-e0yfrrsowwhuq.png" align="right">  Famous example.  The program, which arranged for patients to take turns on the urgency of providing care, concluded that asthmatics with pneumonia need less help than people with pneumonia without asthma.  The program looked at the statistics and concluded that asthmatics do not die - why do they need priority?  But they do not actually die because such patients immediately receive the best care in medical institutions due to the very high risk. <br><br>  Worse than false correlations are only feedback loops.  The California crime prevention program proposed sending more police officers to the black neighborhoods based on the crime rate (number of recorded crimes).  And the more police cars in the field of visibility, the more often residents report crimes (just have someone to report).  As a result, crime is only increasing - it means that more police officers must be sent, and so on. <br><br>  In other words, if racial discrimination is a factor of arrest, feedback loops can reinforce and perpetuate racial discrimination in the police. <br><br><h1>  Who is to blame </h1><br>  In 2016, the Big Data Working Group under the Obama Administration released a <a href="https://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/2016_0504_data_discrimination.pdf">report</a> warning of ‚Äúpossible coding discrimination when making automated decisions‚Äù and postulating the ‚Äúprinciple of equal opportunities‚Äù. <br><br>  But it is easy to say something, but what to do? <br><br><img src="https://habrastorage.org/webt/8f/zx/1m/8fzx1mjghd0aq-e0yfrrsowwhuq.png" align="right">  First, the machine learning model is hard to test and fix.  For example, the Google Photo app recognized black-skinned people like gorillas.  And how to be?  If we read the usual programs in steps and learned how to test them, then in the case of machine learning, everything depends on the size of the control sample, and it cannot be infinite.  For three years, Google was <a href="https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai">unable to come up with anything better</a> than to turn off the recognition of gorillas, chimpanzees and monkeys at all, to prevent the error from recurring. <br><br>  Secondly, it is difficult for us to understand and explain machine learning solutions.  For example, the neural network somehow placed weights within itself so that the correct answers were obtained.  And why do they turn out exactly so and what to do to change the answer? <br><br><img src="https://habrastorage.org/webt/8f/zx/1m/8fzx1mjghd0aq-e0yfrrsowwhuq.png" align="right">  A 2015 survey found that women are much less likely than men to <a href="https://www.theguardian.com/technology/2015/jul/08/women-less-likely-ads-high-paid-jobs-google-study">see</a> high-paying job ads being shown by Google AdSense.  Amazon‚Äôs same-day delivery service <a href="https://www.geekwire.com/2016/amazon-same-day-delivery-black-neighborhoods/">was regularly unavailable</a> in black neighborhoods.  In both cases, company representatives found it difficult to explain such algorithm solutions. <br><br><h1>  It remains to make laws and rely on machine learning. </h1><br>  It turns out that there is no one to blame, it remains to make laws and postulate the "ethical laws of robotics."  Germany just recently, in May 2018, issued such a set of rules about unmanned vehicles.  Among other things, it recorded there: <br><ul><li>  Human safety is the highest priority compared to damage to animals or property. </li><li>  In the event of an imminent accident, there should be no discrimination, and it is unacceptable to distinguish between people by any factors. </li></ul><br>  But what is especially important in our context: <br>  Automatic driving systems become an <b>ethical imperative</b> if systems cause less accidents than human drivers. <br><br>  Obviously, we will rely more and more on machine learning - simply because it will generally cope better than people. <br><br><h1>  Machine learning can be poisoned </h1><br>  And here we come to no less misfortune than the bias of the algorithms - they can be manipulated. <br><br>  ML poisoning means that if someone takes part in the training of the model, then he can influence the decisions made by the model. <br><br>  For example, in a computer virus analysis laboratory, a matmodel processes on average a million new samples (clean and malicious files) every day. <img src="https://habrastorage.org/webt/8f/zx/1m/8fzx1mjghd0aq-e0yfrrsowwhuq.png" align="right">  The threat landscape is constantly changing, so changes in the model in the form of anti-virus database updates are delivered to anti-virus products on the user side. <br><br>  So, an attacker can constantly generate malicious files, very similar to some kind of clean, and send them to the laboratory.  The border between clean and malicious files will gradually fade, the model will ‚Äúdegrade‚Äù.  And in the end, the model can recognize the original clean file as malware - a false positive will result. <br><br>  And vice versa, if you ‚Äúspam‚Äù a self-learning spam filter with a ton of clean generated emails, you will eventually be able to create spam that passes through the filter. <br><br>  Therefore, in Kaspersky Lab, a <a href="https://www.kaspersky.ru/blog/multilayered-approach/15067/">multi-layered approach to protection</a> , we <a href="https://securelist.ru/five-myths-about-machine-learning-in-cybersecurity/29454/">do not rely</a> solely on machine learning. <br><br>  Another example is fictional.  Specially generated faces can be added to the facial recognition system to end up confusing you with someone else.  Do not think that this is impossible, take a look at the picture from the next section. <br><br><img src="https://habrastorage.org/webt/8f/zx/1m/8fzx1mjghd0aq-e0yfrrsowwhuq.png" align="right"><h1>  Hacking machine learning </h1><br>  Poisoning is an effect on the learning process.  But it is not necessary to participate in training in order to get the benefit - you can deceive a ready-made model if you know how it works. <br><br><img src="https://habrastorage.org/webt/78/tn/ea/78tneahwtmergijhapcs5cg5no4.png"><br><br><img src="https://habrastorage.org/webt/qq/hf/uv/qqhfuvjsdh5m9t8i8kd-l9wnxqo.png" align="right">  <i>Wearing specially painted glasses, the researchers <a href="https://www.theguardian.com/technology/2016/nov/03/how-funky-tortoiseshell-glasses-can-beat-facial-recognition">presented</a> themselves as other people - celebrities</i> <br><br>  This example has not yet been encountered by individuals in the ‚Äúwild‚Äù - precisely because no one has yet entrusted the machine to make important decisions based on facial recognition.  Without control from the person will be exactly as in the picture. <br><br>  Even where, seemingly, there is nothing difficult, the car is easy to deceive in a way unknown to the uninitiated. <br><br><img src="https://habrastorage.org/webt/pb/_6/1b/pb_61bjjv5nnw5kxwxwcbifkm20.png"><br>  <i>The first three characters are <a href="https://arxiv.org/pdf/1412.6572.pdf">recognized</a> as ‚ÄúSpeed ‚Äã‚Äãlimit 45‚Äù, and the last one as a STOP character.</i> <br><br><img src="https://habrastorage.org/webt/qq/hf/uv/qqhfuvjsdh5m9t8i8kd-l9wnxqo.png" align="right">  Moreover, in order for the Matmodel of machine learning to recognize surrender, it is not necessary to make significant changes, the <a href="https://www.kaspersky.ru/blog/ai-fails/18678/">minimum invisible</a> edits to a person are <a href="https://www.kaspersky.ru/blog/ai-fails/18678/">sufficient</a> . <br><br><img src="https://habrastorage.org/webt/_c/du/mj/_cdumjxqnp1ojmvp2i-orq-8uke.png"><br><br>  <i>If we add the minimum special noise to the panda on the left, then machine learning will be sure that <a href="https://arxiv.org/pdf/1312.6199.pdf">it is a</a> gibbon</i> <br><br><img src="https://habrastorage.org/webt/cr/ng/y6/crngy6nspkfqx_7uxxt_trxffnk.png" align="right">  As long as a person is smarter than most algorithms, he can deceive them.  Imagine that in the near future machine learning will analyze X-rays of suitcases at the airport and look for weapons.  A clever terrorist can put a special shape next to the pistol and thereby "neutralize" the pistol. <br><br>  Similarly, it will be possible to ‚Äúhack‚Äù the Chinese Social Rating System and become the most respected person in China. <br><br><h1>  Conclusion </h1><br>  Let's summarize what we managed to discuss. <br><img src="https://habrastorage.org/webt/p4/ca/et/p4caet8far6cj7xuvafv8e6qddq.png"><br><br><img src="https://habrastorage.org/webt/cr/ng/y6/crngy6nspkfqx_7uxxt_trxffnk.png" align="right"><ol><li>  There is no strong AI yet. </li><li>  We are relaxed. </li><li>  Machine learning will reduce the number of victims in critical areas. </li><li>  We will rely on machine learning more and more. </li><li>  We will have good intentions. </li><li>  We will even lay ethics into systems design. </li><li>  But ethics is hard formalized and different in different countries. </li><li>  Machine learning is full of bias for various reasons. </li><li>  We can not always explain the solutions of machine learning algorithms. </li><li>  Machine learning can be poisoned. </li><li>  And even "hack". </li><li>  An attacker can gain an advantage over other people. </li><li>  Machine learning has an impact on our lives. </li></ol><br>  And all this is the near future. </div><p>Source: <a href="https://habr.com/ru/post/421791/">https://habr.com/ru/post/421791/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../421781/index.html">Yandex has deleted both pirated links and links to the sites of the copyright holder</a></li>
<li><a href="../421783/index.html">Fun State Management Huex Framework</a></li>
<li><a href="../421785/index.html">California is on the verge of a complete abandonment of carbon in energy production</a></li>
<li><a href="../421787/index.html">Design project architecture, ships and javascript</a></li>
<li><a href="../421789/index.html">Make frontend ‚Äúbackend‚Äù again</a></li>
<li><a href="../421793/index.html">Looking for the best or how we chose the blockchain network for the project</a></li>
<li><a href="../421795/index.html">Data-driven decision on the example of choosing the color for painting walls</a></li>
<li><a href="../421797/index.html">Why do you need Splunk? Monitoring the work of the IT infrastructure</a></li>
<li><a href="../421799/index.html">How to get remotely in a company that does not take remote employees?</a></li>
<li><a href="../421801/index.html">The author of the BetterSlack extension withdraws it at the request of Slack lawyers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>