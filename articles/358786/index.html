<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Ship terabytes in barrels or SparkStreaming vs Spring + YARN + Java</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="As part of the project to integrate GridGain and storage based on Hadoop (HDFS + HBASE), we faced the challenge of obtaining and processing a substant...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Ship terabytes in barrels or SparkStreaming vs Spring + YARN + Java</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/webt/or/zl/yk/orzlykbil_xvjnn0womccnuybom.png"></div><br>  As part of the project to integrate GridGain and storage based on Hadoop (HDFS + HBASE), we faced the challenge of obtaining and processing a substantial amount of data, up to about 80 TB per day.  This is necessary for building storefronts and for recovering data deleted in GridGain after it is uploaded to our long-term storage.  In general terms, we can say that we transfer data between two distributed data processing systems using a distributed data transmission system.  Accordingly, we want to talk about the problems that our team encountered in the implementation of this task and how they were solved. <br><br>  Since the integration tool is Kafka (described in detail in the <a href="https://habr.com/company/sberbank/blog/353608/">article by</a> Mikhail Golovanov), the use of SparkStreaming looks like a natural and easy solution.  Easy, because you don‚Äôt need to worry too much about crashes, reconnects, commits, etc.  Spark is known as a fast alternative to the classic MapReduce, thanks to numerous optimizations.  You just need to tune in to the topic, process the batch and save it to a file, which was implemented.  However, during the development and testing, the instability of the data receiving module was noticed.  In order to eliminate the influence of potential errors in the code, the following experiment was performed.  All the message handling functionality was cut out and only direct storage was left immediately in avro: <br><a name="habracut"></a><br><pre><code class="java hljs">JavaRDD&lt;AvroWrapper&lt;GenericRecord&gt;&gt; map = rdd.map(messageTuple -&gt; { SeekableByteArrayInput sin = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> SeekableByteArrayInput(messageTuple.value()); DataFileReader dataFileReader = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> DataFileReader&lt;&gt;(sin, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> GenericDatumReader&lt;&gt;()); GenericRecord record = (GenericRecord) dataFileReader.next(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> AvroWrapper&lt;&gt;(record); }); Timestamp ts = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Timestamp(System.currentTimeMillis()); map.mapToPair(recordAvroWrapper -&gt; <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Tuple2&lt;AvroWrapper&lt;GenericRecord&gt;, NullWritable&gt;(recordAvroWrapper, NullWritable.get())) .saveAsHadoopFile(<span class="hljs-string"><span class="hljs-string">"/tmp/SSTest/"</span></span> + ts.getTime(), AvroWrapper.class, NullWritable.class, AvroOutputFormat.class, jobConf);</code> </pre> <br>  All tests took place on this stand: <br><br><img src="https://habrastorage.org/webt/_d/n8/ax/_dn8axoku3umnrp5rahubyrzfpa.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      As it turned out, everything works fine on a cluster free from other tasks, you can get pretty good speed.  However, it turned out that when working simultaneously with other applications, there are very large delays.  Moreover, problems arise even at ridiculous speeds, about 150 MB / s.  Sometimes the spark comes out of depression and catches up with the loss, but sometimes it happens like this: <br><br><img src="https://habrastorage.org/webt/y4/ob/n5/y4obn5pvys0jfgzciffou4qdycw.png"><br><br>  Here you can see that at a reception rate of about 1000 messages per second (input rate), after several drawdowns, the delay in starting the batch processing (scheduling delay) still returned to normal (the middle part of the graph).  However, at some point, the processing time went out of permissible limits and the soul of Sparke could not stand the earthly tests and rushed into the sky. <br><br>  It is clear that for the Indian guru this is the norm, but our PROM is not in the ashram, so this is not particularly acceptable.  In order to make sure that the problem is not in the function of storing data, you can use the Dataset wrapper - it seems like it is well optimized.  Therefore, we try this code: <br><br><pre> <code class="java hljs">JavaRDD&lt;Row&gt; rows = rdd.map(messageTuple -&gt; { <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> (SeekableByteArrayInput sin = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> SeekableByteArrayInput(messageTuple.value()); DataFileReader dataFileReader = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> DataFileReader&lt;&gt;(sin, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> GenericDatumReader&lt;&gt;())) { GenericRecord record = (GenericRecord) dataFileReader.next(); Object[] values = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Object[]{ record.get(<span class="hljs-string"><span class="hljs-string">"field_1"</span></span>), ‚Ä¶ record.get(<span class="hljs-string"><span class="hljs-string">"field_N"</span></span>)}; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> GenericRowWithSchema(values, getSparkSchema(ReflectData.get().getSchema(SnapshotContainer.class))); } }); StructType st = (StructType) SchemaConverters.toSqlType(schm).dataType(); Dataset&lt;Row&gt; batchDs = spark.createDataFrame(rows, st); Timestamp ts = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Timestamp(System.currentTimeMillis()); batchDs .write() .mode(SaveMode.Overwrite) .format(<span class="hljs-string"><span class="hljs-string">"com.databricks.spark.avro"</span></span>) .save(<span class="hljs-string"><span class="hljs-string">"/tmp/SSTestDF/"</span></span> + ts.getTime());</code> </pre> <br>  And we get exactly the same problems.  And if you run two versions at the same time, on different clusters, then problems arose only for the one that works on the more loaded one.  This meant that the problem was not the decision and the specifics of the data storage function.  Testing also showed that if we simultaneously read the same topic with which SS worked, using flume on the same cluster, the same data extraction slowdown was obtained: <br><br>  Topic1, Cluster1, SparkSreaming - slowdowns <br>  Topic2, Cluster1, Flume - slowdowns <br>  Topic2, Cluster2, SparkSreaming - no slowdowns <br><br>  In other words, the problem was precisely the background load on the cluster.  Thus, the task was to write an application that would work reliably in a highly loaded environment, and all this was complicated by the fact that the tests above do not contain any data processing logic at all.  Whereas the real process looks like this: <br><br><img src="https://habrastorage.org/webt/xh/0p/xg/xh0pxgd95vszof9mwtwapnbvmu0.png"><br><br>  The main difficulty here is the task of collecting data simultaneously from two topics (from one small data stream, and from the second large) and their join on the fly.  There was also a need to write data from one batch to different files at the same time.  In a Spark, this was implemented using the class to be serialized and calling its methods from the message receiving map.  Sometimes Spark fell, trying to read foul messages from the topic, and we started storing offsets in hbase.  At some point we began to look at the resulting monster with some kind of longing and soulful torment. <br><br>  Therefore, we decided to turn to the bright side of the force - a warm, tube-like java.  The blessing we have is an agile, and it is not at all necessary to <s>gnaw a cactus</s> to jump into a waterfall, when for some reason you don't want to <br><br><img src="https://habrastorage.org/webt/va/xg/nz/vaxgnz7hordjv-rdc96cbiml9ie.png"><br><br>  However, this requires solving the problem of distributed message reception from several nodes at once.  For this, the Spring for Apache Hadoop framework was selected, which allows you to run the required number of Yarn containers and execute your code inside each. <br><br>  The general logic of his work is as follows.  Run AppMaster, which is the coordinator of the containers YARN.  That is, it launches them, passing the necessary parameters to them for input, and tracks the status of execution.  In the event of a container crash (for example, due to OutOfMemory), it can restart it or shut down. <br><br>  Directly in the container and implemented the logic of the work and the processing of data.  Since YARN launches containers by distributing approximately equally at the cluster nodes, there are no bottlenecks for network traffic or disk access.  Each container clings to the selected partition and works only with it, it helps to avoid rebalancing of the consumer. <br><br>  Below is a greatly simplified description of the logic of the module, a more detailed description of what is happening under the hood of the spring, colleagues plan to do in a separate article.  The original example can be downloaded <a href="https://github.com/spring-guides/gs-yarn-basic">here</a> . <br><br>  So, to launch the wizard, the client module is used: <br><br><pre> <code class="java hljs"><span class="hljs-meta"><span class="hljs-meta">@EnableAutoConfiguration</span></span> <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ClientApplication</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">static</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">main</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(String[] args)</span></span></span><span class="hljs-function"> </span></span>{ ConfigurableApplicationContext applicationContext = SpringApplication.run(ClientApplication.class, args); YarnClient yarnClient = applicationContext.getBean(YarnClient.class); yarnClient.submitApplication(); } }</code> </pre> <br>  After the submit wizard is complete, the client quits.  Next, the CustomAppMaster class is written in application.yml <br><br><pre> <code class="hljs pgsql">spring: hadoop: fsUri: hdfs://namespace:port/ resourceManagerHost: hostname resources: - "file:/path/hdfs-site.xml" yarn: appName: <span class="hljs-keyword"><span class="hljs-keyword">some</span></span>-<span class="hljs-type"><span class="hljs-type">name</span></span> applicationDir: /<span class="hljs-type"><span class="hljs-type">path</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">to</span></span>/app/ appmaster: resource: memory: <span class="hljs-number"><span class="hljs-number">10240</span></span> virtualCores: <span class="hljs-number"><span class="hljs-number">1</span></span> appmaster-<span class="hljs-keyword"><span class="hljs-keyword">class</span></span>: enter.appmaster.CustomAppMaster containerCount: <span class="hljs-number"><span class="hljs-number">10</span></span> launchcontext: archiveFile: container.jar container: container-<span class="hljs-keyword"><span class="hljs-keyword">class</span></span>: enter.appmaster.FailingContextContainer</code> </pre><br>  It has the most interesting preLaunch function.  Here we manage containers and parameters passed to the input: <br><br><pre> <code class="java hljs"><span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> ContainerLaunchContext </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preLaunch</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(Container container, ContainerLaunchContext context)</span></span></span><span class="hljs-function"> </span></span>{ Integer attempt = <span class="hljs-number"><span class="hljs-number">1</span></span>; <span class="hljs-comment"><span class="hljs-comment">//    ContainerId containerId = container.getId(); ContainerId failedContainerId = failed.poll(); if (failedContainerId == null) { //      } else { //      (  ..) } Object assignedData = (failedContainerId != null ? getContainerAssign().getAssignedData(failedContainerId) : null); if (assignedData != null) { attempt = (Integer) assignedData; attempt += 1; } getContainerAssign().assign(containerId, attempt); Map&lt;String, String&gt; env = new HashMap&lt;String, String&gt;(context.getEnvironment()); env.put("some.param", "param1"); context.setEnvironment(env); return context; }</span></span></code> </pre><br>  And the crash handler: <br><br><pre> <code class="java hljs"><span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">protected</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">boolean</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onContainerFailed</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(ContainerStatus status)</span></span></span><span class="hljs-function"> </span></span>{ ContainerId containerId = status.getContainerId(); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (status.getExitStatus() &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>) { failed.add(containerId); getAllocator().allocateContainers(<span class="hljs-number"><span class="hljs-number">1</span></span>); } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>; }</code> </pre> <br>  In the ContainerApplication.java container class, the necessary bins are connected, for example: <br><br><pre> <code class="java hljs"><span class="hljs-meta"><span class="hljs-meta">@Bean</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> WorkClass </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">workClass</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> WorkClass(); }</code> </pre> <br>  In the working class, we use the @OnContainerStart annotation to specify the method that will be called automatically when the container is started: <br><br><pre> <code class="java hljs"><span class="hljs-meta"><span class="hljs-meta">@OnContainerStart</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">doWorkOnStart</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">throws</span></span></span><span class="hljs-function"> Exception </span></span>{ <span class="hljs-comment"><span class="hljs-comment">//       containerId DefaultYarnContainer yarnContainer = (DefaultYarnContainer) containerConfiguration.yarnContainer(); Map&lt;String, String&gt; environment = yarnContainer.getEnvironment(); ContainerId containerId = getContainerId(environment); //     String param = environment.get("some.param"); SimpleConsumer&lt;Serializable, Serializable&gt; simpleConsumer = new SimpleConsumer&lt;&gt;(); //   simpleConsumer.kafkaConsumer(param); }</span></span></code> </pre><br>  In reality, the logic of implementation is, of course, much more complicated.  In particular, there is an exchange of messages between the container and AppMaster through REST, allowing to coordinate the process of receiving data, etc. <br><br>  As a result, we received an application that needs to be tested in a loaded cluster.  To do this, during the day, during a high background load, we launched a trimmed version on SparkStreaming, which does nothing except save to a file, and at the same time the version of ‚Äúfull stuffing‚Äù on java.  Resources they were allocated the same, each 30 containers of 2 cores. <br><br><img src="https://habrastorage.org/webt/wn/fe/dy/wnfedygliohaajjtvzb-5p9kalo.png"><br><br>  Now it is interesting to conduct an experiment in pure conditions in order to understand the performance limit of a java solution.  For this, the download of 1.2 TB of data was started, 65 containers of 1 core and it was completed in 10 minutes: <br><br><img src="https://habrastorage.org/webt/it/0q/pr/it0qpr-wvwunf5aag5uyouwvg0k.png"><br><br>  Those.  speed was 2 GB / s.  The higher values ‚Äã‚Äãin the picture above are explained by the fact that the data replication factor on HDFS is equal to 3. CPUs of the servers of the data receiving cluster E5-2680v4 @ 2.40GHz.  The remaining parameters do not make much sense to give, because all the same the utilization of resources is significantly below 50%.  The current solution allows you to scale easily further, but it does not make sense, because  at the moment, the bottleneck is Kafka itself (or rather its network interfaces, there are only three brokers and at the same time triple replication for reliability). <br><br>  In fact, it should not seem that we have something against Spark in principle.  This is a very good tool in certain conditions and we also use it for further processing.  However, a high level of abstraction, which allows you to quickly and easily work with any data, has its price.  It always happens when something goes wrong.  We had the experience of Hbase patching and picking in the Hive code, however, this is not the most encouraging thing, in fact.  In the case of a Spark, of course, it is also possible to find some acceptable solution, but at the cost of quite a lot of effort.  And in our application, we can always quickly find the cause of the problems and correct, as well as implement very complex logic and this will work quickly.  In general, as the old Latin saying goes: <br><br><img src="https://habrastorage.org/webt/6z/xo/e4/6zxoe4_jdnkvvwvzthiblvddokq.png"></div><p>Source: <a href="https://habr.com/ru/post/358786/">https://habr.com/ru/post/358786/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../358776/index.html">Move to the cloud: how LDAP helps implement virtual PBX</a></li>
<li><a href="../358778/index.html">GeekBrains opens a set of free programmer training courses.</a></li>
<li><a href="../358780/index.html">Overview of popular AR frameworks</a></li>
<li><a href="../358782/index.html">Chinese government introduces artificial intelligence into high school curriculum</a></li>
<li><a href="../358784/index.html">Figma - we make design systematically</a></li>
<li><a href="../358788/index.html">Holidays in the world of telecommunications</a></li>
<li><a href="../358790/index.html">Finding the number of commissions that "draw" integer turnout values ‚Äã‚Äãin the presidential election of the Russian Federation in 2018</a></li>
<li><a href="../358794/index.html">Mitapa in May: blockchain in Moscow and testing in St. Petersburg</a></li>
<li><a href="../358796/index.html">Remote control of the heating system</a></li>
<li><a href="../358798/index.html">Inverse kinematics in two-dimensional space</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>