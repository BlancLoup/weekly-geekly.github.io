<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Search under the hood. Cloud indexing</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the last article, I talked about how a search engine can find out that a particular web page exists and save it to itself in the repository. But fi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Search under the hood. Cloud indexing</h1><div class="post__text post__text-html js-mediator-article"><p>  In the <a href="https://habrahabr.ru/post/345672/">last article,</a> I talked about how a search engine can find out that a particular web page exists and save it to itself in the repository.  But finding out that a webpage exists is just the beginning.  It is much more important in a split second to have time to find those pages that contain keywords entered by the user.  I will talk about how this works in today's article, illustrating my story with a ‚Äúlearning‚Äù implementation, which is nevertheless designed to be able to scale up to the size of the entire Internet indexing and take into account the current state of large data analysis technologies. </p><br><p><img src="https://habrastorage.org/webt/wl/5h/h9/wl5hh9jw_0fbq12wkxjzmhscbm4.png"></p><br><p>  At the same time, I was able to consider the main functions and methods of Apache Spark, so this article can also be viewed as a small tutorial on Spark. </p><a name="habracut"></a><br><h2 id="formulirovka-zadachi">  Task statement </h2><br><p>  A more formal formulation of the problem, which I will analyze today: there is a repository containing a set of web pages <a href="https://habrahabr.ru/post/345672/">downloaded from the Internet by a crawler</a> .  It is necessary to design a mechanism that allows for fractions of a second to provide links to all web pages from this repository, including <strong>all keywords</strong> contained in a user request.  This mechanism should be: </p><br><ol><li>  scalable by the amount of data - potentially we should be able to process the entire Internet; </li><li>  scalable by the number of requests per second: for ‚Äúadult‚Äù search engines, such as Yandex and Google, the number of search requests can reach <a href="http://www.internetlivestats.com/one-second/">tens of thousands of requests per second.</a> </li></ol><br><p>  Some of the important limitations of the task being analyzed today: </p><br><ol><li>  As part of this article, I will not try to organize the pages found.  The engine will return just a lot.  The task of ordering, or, more correctly, ranking, is a separate important task, which I will analyze in the following articles. </li><li>  My implementation will imply the presence of all the words from the search query.  Modern search engines allow you to correct typos, search for synonyms, etc., but in the end it all the same boils down to several queries ‚Äúby all‚Äù words and combining or intersecting their results. </li></ol><br><p>  Let's now analyze how to solve the problem within the limits of the set limits. </p><br><h3 id="invertirovannyy-indeks">  Invert Index </h3><br><p>  Consider the following data structure: a dictionary whose keys are words from our language, values ‚Äã‚Äã‚Äî sets of web pages where this word occurs: </p><br><p><img src="https://habrastorage.org/webt/ql/wl/rp/qlwlrpbkma55xjiwfg_z9wxolto.png"></p><br><p>  This data structure is called the inverted index, and it is the key to the search engine.  So key that, for example, Yandex is even named after her (yandex is nothing but <strong>y</strong> et <strong>a</strong> nother i <strong>ndex</strong> ). </p><br><p>  In reality, this dictionary will be much larger than in the above example: the number of elements in it will be equal to the number of different words on web pages, and the maximum size of the set for one element will be all web pages in the indexed part of the Internet. </p><br><p>  Suppose we were able to build such a data structure.  In this case, the search for web pages containing words from the query will occur as follows: </p><br><ol><li>  We split the query into words. </li><li>  For each word, go to the reverse index and extract a lot of web pages. </li><li>  The result is the intersection of all the sets extracted in clause 1. </li></ol><br><p>  For example, if we search all web pages for the query ‚Äúvisualization algorithm‚Äù and the reverse index corresponds to the one shown in the table, then the resulting set will contain only one web page - <strong>habr.ru/post/325422/</strong> , since it only contains at the intersection of the sets for the words "algorithm" and "visualization." </p><br><p>  In order to build such a data structure, you can use the MapReduce approach.  I have a <a href="https://habrahabr.ru/company/dca/blog/267361/">separate article</a> about this approach, but the basic idea is this: </p><br><ol><li>  At the first stage ( <strong>map step</strong> ), you can convert source objects (in our case, documents) into key-value pairs (the keys are words, and the values ‚Äã‚Äãare the URL of the document). </li><li>  Key-value pairs are automatically grouped by key ( <strong>shuffle step</strong> ). </li><li>  To process all values ‚Äã‚Äãfor a given key ( <strong>step reduce</strong> ).  In our case, save in reverse index. </li></ol><br><p>  Below in the implementation section, I will show how to implement the described algorithm using the popular open source tool for working with big data - apache spark. </p><br><h2 id="chut-chut-nlp">  A little nlp </h2><br><p>  The abbreviation <strong>NLP</strong> in the field of data analysis is understood to be computer <strong>processing of natural language</strong> (Natural Language Processing, not to be confused with pseudoscientific neuro-linguistic programming).  When working with a search engine, you cannot avoid even a distant collision with language processing, so we need some concepts and tools from this area.  As a library of working with natural language, I will use the popular library for python <a href="http://www.nltk.org/">NLTK</a> . </p><br><h3 id="tokenizaciya">  Tokenization </h3><br><p>  The first concept from the NLP domain that we need is tokenization.  When describing working with a reverse index, I used the <strong>word</strong> concept.  However, the word is not a very good term for a search engine developer, since web pages contain many different character sets that are not a word in the direct sense of the word (for example, masha545 or 31337).  Therefore, we will use tokens instead.  The NLTK library has a special module for selecting tokens: <a href="http://www.nltk.org/api/nltk.tokenize.html">nltk.tokenize</a> .  There are various ways to split text into tokens.  We will use the simplest way to select tokens - tokenization by regexp: </p><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#code from nltk.tokenize import RegexpTokenizer tokenizer = RegexpTokenizer(r'[-a-z0-9]+') text = "     ,    ." tokenizer.tokenize(text.lower()) #result ['', '', '', '', '', '', '', '', '', '']</span></span></code> </pre> <br><h3 id="lemmatizaciya">  Lemmatization </h3><br><p>  Many languages, especially Russian, are rich in word forms.  It is clear that when we search for the word "computer", we expect that there will be pages containing the word "computer", "computers" and so on.  To do this, all the tokens must be brought into the so-called "normal form".  This can be done with different tools.  For example, on github there is a library <a href="https://github.com/nlpub/pymystem3">pymystem</a> , which is a wrapper over a library developed by Yandex.  For simplicity, I will use the method of stemming - the rejection of insignificant endings - and use for this the Russian language stemmer, included in the nltk library: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#code from nltk.stem.snowball import RussianStemmer stemmer = RussianStemmer() tokens = ['', '', '', '', '', '', '', '', '', ''] stemmed_tokens = [stemmer.stem(token) for token in tokens] print(stemmed_tokens) #result ['', '', '', '', '', '', '', '', '', '']</span></span></code> </pre> <br><h3 id="otsechenie-neznachimyh-slov">  Cut off insignificant words </h3><br><p>  In the reverse index for each word we store the set of URL pages in which this word is published.  The problem is that some words (for example, the preposition "in") are found on almost every web page.  At the same time, the information content of the presence or absence of such words on the page is very small.  Therefore, in order not to store huge arrays in the index and not do extra work, we will simply ignore such words. </p><br><p>  To determine the set of words that will be ignored, for each word you can calculate the proportion of web pages on which this word occurs, and put a certain limit on this frequency.  Calculation of the frequency of words is a classic problem in the map-reduce paradigm, you can read more about this in my <a href="https://habrahabr.ru/company/dca/blog/267361/">articles</a> . </p><br><p><img src="https://habrastorage.org/webt/bt/82/sc/bt82sc3njlkgcyvjy6hvrkahrww.png"></p><br><p>  For my implementation, I calculated not just the word frequency, but some of its monotonous transformation - the <a href="https://nlp.stanford.edu/IR-book/html/htmledition/inverse-document-frequency-1.html">Inverse Document Frequency</a> (IDF), which we will later also use for ranking documents.  By the method of ‚Äúgaze‚Äù, I determined that the appropriate constant for cutting off words would be approximately equal to 1.12, between the words ‚Äúfor‚Äù and ‚Äúcode‚Äù (the word ‚Äúcode‚Äù is very often found in Habr√©). </p><br><h2 id="indeksiruem">  Indexing </h2><br><p><img src="https://habrastorage.org/webt/mg/bv/eh/mgbvehfn4guv28afo6iojsqxh6m.png"><br>  The architecture of my academic search engine </p><br><h3 id="apache-spark">  Apache spark </h3><br><p>  There are quite a few documents that I index - a few million.  To process them, you need tools for working with big data.  I chose <a href="https://spark.apache.org/">apache spark</a> , which is one of the most popular frameworks to date.  Since I use amazon web services for my implementation, I used the Spark distribution kit, which is part of the <a href="https://aws.amazon.com/emr/">elastic map reduce</a> .  Apache Spark has several options for presenting datasets.  One of the main ones - the so-called <strong>Resilient Distributed Dataset</strong> (RDD) - is essentially a distributed data array that can be processed in parallel.  I will use it for my implementation (although there are other APIs for working with sparks, which in some cases can be faster, see for example the <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">Dataframe API</a> ) </p><br><p>  Since the data in our case is stored on the Amazon object storage (S3), first we need to provide the necessary information for working with this storage: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#sc- spark context,       def init_aws_spark(sc, config): sc._jsc.hadoopConfiguration().set("fs.s3a.access.key", config.AWS_ACCESS_KEY) sc._jsc.hadoopConfiguration().set("fs.s3a.secret.key", config.AWS_SECRET_KEY)</span></span></code> </pre> <br><p>  Then you can create RDD from the data stored on S3 ( <a href="https://habrahabr.ru/post/345672/">which the crawler saved there</a> ), and at the same time parse documents from json-format: </p><br><pre> <code class="python hljs">rdd = sc.textFile(<span class="hljs-string"><span class="hljs-string">"s3a://minicrawl/habrahabr.ru/*"</span></span>,) jsons_rdd = rdd.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> doc: json.loads(doc))</code> </pre> <br><p>  Here we have applied one of the basic functions of Spark - map, which applies the function to all elements of the array, doing it in parallel on all nodes of the cluster. </p><br><p>  Next, we apply this function several times to preprocess text: </p><br><h4 id="ochistka-ot-html-razmetki">  Clearing HTML Markup </h4><br><p>  There is nothing particularly interesting here, I use the lxml library for parsing html and removing the markup: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> copy <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> lxml.etree <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> etree <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">stringify_children</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(node)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> str(node.tag).lower() <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> {<span class="hljs-string"><span class="hljs-string">'script'</span></span>, <span class="hljs-string"><span class="hljs-string">'style'</span></span>}: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [] <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> lxml.etree <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tostring parts = [node.text] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> element <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> node.getchildren(): parts += stringify_children(element) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-string"><span class="hljs-string">''</span></span>.join(filter(<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, parts)).lower() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_tree</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(html)</span></span></span><span class="hljs-function">:</span></span> parser = etree.HTMLParser() tree = etree.parse(StringIO(html), parser) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> tree.getroot() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">remove_tags</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(html)</span></span></span><span class="hljs-function">:</span></span> tree = get_tree(html) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> stringify_children(tree) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(doc)</span></span></span><span class="hljs-function">:</span></span> res = copy.deepcopy(doc) res[<span class="hljs-string"><span class="hljs-string">'html'</span></span>] = res[<span class="hljs-string"><span class="hljs-string">'text'</span></span>] res[<span class="hljs-string"><span class="hljs-string">'text'</span></span>] = remove_tags(res[<span class="hljs-string"><span class="hljs-string">'html'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> res clean_text_rdd = jsons_rdd.map(get_text).cache()</code> </pre> <br><h4 id="tokenizaciya-i-stemming">  Tokenization and stemming </h4><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.tokenize <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RegexpTokenizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.stem.snowball <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RussianStemmer <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tokenize</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(doc)</span></span></span><span class="hljs-function">:</span></span> tokenizer = RegexpTokenizer(<span class="hljs-string"><span class="hljs-string">r'[-a-z0-9]+'</span></span>) res = copy.deepcopy(doc) tokens = tokenizer.tokenize(res[<span class="hljs-string"><span class="hljs-string">'text'</span></span>]) res[<span class="hljs-string"><span class="hljs-string">'tokens'</span></span>] = list(filter(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: len(x) &lt; <span class="hljs-number"><span class="hljs-number">15</span></span>, tokens)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> res <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">stem</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(doc)</span></span></span><span class="hljs-function">:</span></span> stemmer = RussianStemmer() res = copy.deepcopy(doc) res[<span class="hljs-string"><span class="hljs-string">'stemmed'</span></span>] = [stemmer.stem(token) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> token <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> res[<span class="hljs-string"><span class="hljs-string">'tokens'</span></span>]] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> res stemmed_docs = clean_text_rdd.map(tokenize).map(stem).cache()</code> </pre><br><p>  The cache () function, called after the map () function, prompts you to cache this dataset.  If this is not done, with repeated use, the spark will count it again. </p><br><h3 id="filtraciya-vysokochastotnyh-slov-na-spark">  Filter high-frequency words on spark </h3><br><p>  As I wrote, we will filter words for which the IDF measure is less than 1.12.  To do this, we first need to calculate the frequency of all words.  <a href="https://habrahabr.ru/company/dca/blog/267361/">This is a straightforward classic big data analysis task</a> : </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_words</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(doc)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [(word, <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> set(doc[<span class="hljs-string"><span class="hljs-string">'stemmed'</span></span>])] word_counts = stemmed_docs.flatMap(get_words)\ .reduceByKey(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x, y: x+y)</code> </pre> <br><p>  It uses two interesting Spark functions: </p><br><ol><li>  <strong>flatMap</strong> - works like a map, but returns not one value for one value of the input dataset, but several.  In our case, we return a key-value pair (&lt;word&gt;, 1) for each word at least once in the document. </li><li>  <strong>reduceByKey</strong> - allows you to process all values ‚Äã‚Äãfor a single key.  In our case, sum up. </li></ol><br><p>  Next, we calculate the IDF for all tokens: </p><br><pre> <code class="python hljs">doc_count = stemmed_docs.count() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_idf</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(doc_count, doc_with_word_count)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> math.log(doc_count/doc_with_word_count) idf = word_counts.mapValues(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> word_count: get_idf(doc_count, word_count))</code> </pre> <br><p>  Get a list of high-frequency stop words: </p><br><pre> <code class="python hljs">idf_border = <span class="hljs-number"><span class="hljs-number">1.12</span></span> stop_words_list =idf.filter(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: x[<span class="hljs-number"><span class="hljs-number">1</span></span>] &lt; idf_border).keys().collect() stop_words = set(sop_words)</code> </pre> <br><p>  Spark functions are used here: </p><br><ul><li>  <strong>filter</strong> () - leaves in the dataset only elements that match a specific criterion; </li><li>  <strong>keys</strong> () - leaves only keys in dataset (there is a similar function values ‚Äã‚Äã(), which leaves only values); </li><li>  <strong>collect</strong> () - collects distributed data into a local list.  After this, it is no longer possible to perform Sparsk functions on it. </li></ul><br><p>  I got <strong>50 stop words</strong> , among which there are both obvious: " <strong>in</strong> ", " <strong>o</strong> ", " <strong>not</strong> ", and less obvious, but logical for habr: " <strong>habrabra</strong> ", " <strong>mobile</strong> ", " <strong>sandboxes</strong> ", " <strong>support</strong> "," <strong>registration</strong> ". </p><br><h3 id="postroenie-obratnogo-indeksa">  Reverse Index Building </h3><br><p>  The task to build a dataset of the word -&gt; set of URLs is very similar to the task of counting the number of documents in which the word occurs, with one difference: we will not add one every time, but add a new URL to the set. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">token_urls</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(doc)</span></span></span><span class="hljs-function">:</span></span> res = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> token <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> set(doc[<span class="hljs-string"><span class="hljs-string">'stemmed'</span></span>]): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> token <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> stop_words: res.append((token, doc[<span class="hljs-string"><span class="hljs-string">'url'</span></span>])) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> res index_rdd = stemmed_docs.flatMap(token_urls)\ .aggregateByKey(set(),\ <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x, y: x.union({y}), <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x, y: x.union(y))</code> </pre> <br><p>  Here, in addition to the previously used flatMap function, the <strong>aggregateByKey</strong> function is also used, which is very similar to reduceByKey, but takes three parameters: </p><br><ul><li>  empty battery object in which the result will accumulate; </li><li>  a function that adds one value to the battery; </li><li>  a function that can drain two batteries into one.  Values ‚Äã‚Äãfor one key can be aggregated in parallel, this function is needed to combine partially aggregated results. <br>  Then it remains only to maintain the reverse index.  In order to save it, any distributed key-value storage will be suitable for us.  I chose <a href="http://www.aerospike.com/">aerospike</a> - it is fast, well distributed.  Write a directly serialized url set to the value for the token: </li></ul><br><p>  In general, everything is simple: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pickle storage = LazyAerospike(config.AEROSPIKE_ADDRESS) results = index_rdd.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: storage.put(x[<span class="hljs-number"><span class="hljs-number">0</span></span>], pickle.dumps(x[<span class="hljs-number"><span class="hljs-number">1</span></span>]))).collect()</code> </pre> <br><p>  Here I use <a href="https://docs.python.org/3/library/pickle.html">pickle</a> - the standard Python method of serializing almost any objects.  I also use a small wrapper over the standard aerospike client, which allows you to initialize the connection to the database at the time of the first write or read.  This is necessary, since spark cannot parallelize the connection to the database across all nodes of the cluster; you have to reconnect every time. </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">LazyAerospike</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> aerospike <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, addr, namespace=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'test'</span></span></span></span><span class="hljs-function"><span class="hljs-params">, table=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'index'</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> self.addr = addr self.connection = <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> self.namespace = namespace self.table = table <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">check_connection</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> self.connection <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>: config = { <span class="hljs-string"><span class="hljs-string">'hosts'</span></span>: [ (self.addr, <span class="hljs-number"><span class="hljs-number">3000</span></span>) ] } self.connection = self.aerospike.Client(config).connect() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">_get_full_key</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, key)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (self.namespace, self.table, key) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">put</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, key, value)</span></span></span><span class="hljs-function">:</span></span> self.check_connection() key_full = self._get_full_key(key) self.connection.put(key_full, {<span class="hljs-string"><span class="hljs-string">'value'</span></span>: value}) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">True</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, key)</span></span></span><span class="hljs-function">:</span></span> self.check_connection() key_full = self._get_full_key(key) value = self.connection.get(key_full) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> value[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-string"><span class="hljs-string">'value'</span></span>]</code> </pre> <br><h3 id="api-dlya-izvlecheniya-dannyh">  API for data retrieval </h3><br><p>  It remains to write a function that will be executed during a user request.  Everything is simple with it: we split the request for tokens, extract sets of URLs for each token and intersect them: </p><br><pre> <code class="hljs python"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">index_get_urls</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(keyword)</span></span></span><span class="hljs-function">:</span></span> raw_urls = storage.get(keyword) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> pickle.loads(raw_urls) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">search</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(query)</span></span></span><span class="hljs-function">:</span></span> stemmer = RussianStemmer() tokenizer = RegexpTokenizer(<span class="hljs-string"><span class="hljs-string">r'[-a-z0-9]+'</span></span>) keywords_all = [stemmer.stem(token) \ <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> token <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tokenizer.tokenize(query)] keywords = list(filter(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> token: \ token <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> stop_words, keywords_all)) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(keywords) == <span class="hljs-number"><span class="hljs-number">0</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [] result_set= index_get_urls(keywords[<span class="hljs-number"><span class="hljs-number">0</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> keyword <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> keywords[<span class="hljs-number"><span class="hljs-number">1</span></span>:]: result_set=\ result_set.intersection(index_get_urls(keyword)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> result_set</code> </pre> <br><p>  We start and make sure that everything works as it should (here I run on a small sample): <br><img src="https://habrastorage.org/webt/qc/5m/oc/qc5mochs1elxnkmgeotqfpuat0s.png"></p><br><h2 id="zaklyuchenie">  Conclusion </h2><br><p>  This search engine, of course, is much more complex.  For example, I store sets very nonoptimally, it would be enough to store only their id-names instead of urls themselves.  Nevertheless, the architecture turned out to be distributed and could potentially work on a large part of the Internet and under heavy loads. </p><br><p>  In production, you most likely will not need to implement the search engine with your hands - it‚Äôs better to use ready-made solutions, such as <a href="https://www.elastic.co/">ElasticSearch</a> for example. </p><br><p>  However, understanding the basics of how the reverse index works can help to properly configure and use it, and it can be very useful for solving similar problems. </p><br><p>  In this article, I did not touch on the most interesting, in my opinion, part of the search - ranking.  He will be discussed in the following articles. </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/jmiIBTgWZn4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/349404/">https://habr.com/ru/post/349404/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../349394/index.html">The release of the first beta cross-platform XAML UI-toolkit Avalonia</a></li>
<li><a href="../349396/index.html">Configuring VoIP GSM Gateway Yeastar Neogate TGXXX Series for 3CX</a></li>
<li><a href="../349398/index.html">Safely speeding up the Erlang application using NIF on Rust</a></li>
<li><a href="../349400/index.html">FastTrack Training. "Network Basics". "Basics of switching or switches." Part one. Eddie Martin December 2012</a></li>
<li><a href="../349402/index.html">UWP game: Where to start</a></li>
<li><a href="../349406/index.html">The digest of fresh materials from the world of the frontend for the last week ‚Ññ302 (February 12 - 18, 2018)</a></li>
<li><a href="../349408/index.html">Yii 2.0.14</a></li>
<li><a href="../349412/index.html">Experience of transition of the project to phalcon from php 5.6 to 7.1</a></li>
<li><a href="../349414/index.html">The digest of interesting materials for the mobile developer # 241 (February 12 ‚Äî February 18)</a></li>
<li><a href="../349416/index.html">Proportional time management</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>