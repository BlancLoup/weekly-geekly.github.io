<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>History of predictions of transitions from 1 500 000 BC. till 1995</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This is an approximate transcript of a lecture on predicting transitions (branch prediction) to localhost, a new lecture series organized by RC . The ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>History of predictions of transitions from 1 500 000 BC. till 1995</h1><div class="post__text post__text-html js-mediator-article"> <i>This is an approximate transcript of a lecture on predicting transitions (branch prediction) to localhost, a new lecture series organized by <a href="https://www.recurse.com/apply">RC</a> .</i>  <i>The performance took place on August 22, 2017 at Two Sigma Ventures.</i> <br><br>  Who among you uses branching in your code?  Can you raise your hand if you use if statements or pattern matching? <br><br> <code>     </code> <br> <br>  Now I will not ask you to raise your hands.  But if I ask how many of you think that you understand the actions of the CPU when processing branching and the implications for performance, and how many of you can understand a modern scientific article on branch prediction, fewer people will raise their hands. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The purpose of my presentation is to explain how and why processors predict transitions, and then briefly explain the classic transition prediction algorithms that you can read about in modern articles so that you have a general understanding of the topic. <br><a name="habracut"></a><br>  Before discussing the prediction of transitions, let's talk about why processors do this at all.  To do this, you need to know a little about the work of the CPU. <br><br>  In this lecture you can think of a computer as a CPU plus some memory.  Instructions live in memory, and the CPU executes a sequence of instructions from memory, where the instructions themselves look like ‚Äúadd two numbers‚Äù, ‚Äútransfer a piece of data from memory to the processor‚Äù.  Usually, after executing one instruction, the CPU executes the instruction with the next address in order.  However, there are instructions such as "transitions" that allow you to change the address of the next instruction. <br><br>  Here is an abstract diagram of a CPU that executes some instructions.  Time is plotted on the horizontal axis, and differences between different instructions are on the vertical axis. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ea2/2d6/031/ea22d6031569bb2163fdf44b0508875f.png"><br><br>  Here we first execute instruction A, then instruction B, C and D. <br><br>  One way to design a CPU is to do all the work in turn.  There is nothing wrong with that;  Many old CPUs work like some modern, very cheap processors.  But if you want to make a faster CPU, then you need to turn it into a kind of pipeline.  To do this, you break the processor into two parts, so that the first half is on the frontend of working with instructions, and the second half is in the backend, like on a conveyor belt.  This is commonly referred to as a pipelined CPU (pipelined processor). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/eb8/513/9c0/eb85139c0122cffc09e5046e6df0c2c7.png"><br><br>  If you do this, then the instructions will be executed approximately as in the illustration.  After the first half of instruction A is executed, the processor starts working on the second half of instruction A simultaneously with the first half of instruction B. And when the second half of A is completed, the processor can simultaneously start the second half of B and the first half of C. In this diagram, you can see that the pipeline processor able to perform in a period of time twice as many instructions as a conventional processor, as shown earlier. <br><br>  There is no reason to split the CPU into just two parts.  We can divide it into three parts and get a threefold increase in productivity or four parts for a fourfold increase.  Although this is not quite true and in fact usually the increase will not be threefold for a three-stage conveyor or fourfold for a four-stage conveyor, because there are certain overhead costs when dividing the CPU into parts. <br><br>  One source of overhead is how branches are processed.  First of all, the CPU should receive instructions.  For this, he must know where she is.  For example, take the following code: <br><br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (x == <span class="hljs-number"><span class="hljs-number">0</span></span>) { // <span class="hljs-keyword"><span class="hljs-keyword">Do</span></span> stuff } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { // <span class="hljs-keyword"><span class="hljs-keyword">Do</span></span> other stuff (things) } // Whatever happens later</code> </pre> <br>  It can be translated into assembler: <br><br><pre> <code class="hljs sql">branch_if_not_equal x, 0, else_label // <span class="hljs-keyword"><span class="hljs-keyword">Do</span></span> <span class="hljs-keyword"><span class="hljs-keyword">stuff</span></span> <span class="hljs-keyword"><span class="hljs-keyword">goto</span></span> end_label else_label: // <span class="hljs-keyword"><span class="hljs-keyword">Do</span></span> things end_label: // whatever happens later</code> </pre> <br>  In this example, we compare x with zero.  If it is not equal to zero ( <code>if_not_equal</code> ), then we go to the <code>else_label</code> and execute the code in the else block.  If this comparison is not performed (that is, if x is zero), then we fail, execute the code in the if block, and then jump to <code>end_label</code> to avoid executing the code in the <code>else</code> block. <br><br>  For the pipeline, the specific sequence of instructions is problematic: <br><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">branch_if_not_equal</span></span> x, <span class="hljs-number"><span class="hljs-number">0</span></span>, else_label ???</code> </pre> <br>  The processor does not know what will happen next: <br><br><pre> <code class="hljs sql">branch_if_not_equal x, 0, else_label // <span class="hljs-keyword"><span class="hljs-keyword">Do</span></span> <span class="hljs-keyword"><span class="hljs-keyword">stuff</span></span></code> </pre> <br>  or <br><br><pre> <code class="hljs objectivec">branch_if_not_equal x, <span class="hljs-number"><span class="hljs-number">0</span></span>, else_label <span class="hljs-comment"><span class="hljs-comment">// Do things</span></span></code> </pre> <br>  He does not know this until the execution of the branch has been completed (or almost completed).  One of the first actions of the CPU should be to retrieve the instruction from the memory, and we do not even know what the instruction will be <code>???</code>  And we can not work with it until the previous instruction is almost complete. <br><br>  Earlier we said that we get almost threefold acceleration when using a three-stage conveyor, or almost 20-fold when using a 20-step.  It was assumed that on each cycle you can begin executing the new instruction, but in this case the two instructions are practically serialized. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0b4/04b/9ac/0b404b9ac763458415a895850660dc8c.png"><br><br>  One solution to the problem is to use the prediction of transitions.  When a branch appears, the CPU will assume whether this branch is taken or not. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cc7/429/f8f/cc7429f8f61b97ae484a815d0a1fb128.png"><br><br>  In this case, the processor predicts that the branch will not be busy, and begins the execution of the first half of the <code>stuff</code> , while simultaneously ending the execution of the second half of the branch.  If the prediction is correct, the CPU will execute the second half of the <code>stuff</code> and can start executing one more instruction, working with the second half of the <code>stuff</code> , as we saw in the first pipeline diagram. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4cc/23f/c7e/4cc23fc7e7c33fcc94061312788c63b3.png"><br><br>  If the prediction is erroneous, then after completion of the execution of the branch, the CPU will discard the results of <code>stuff.1</code> and will start executing the correct instructions instead of the incorrect ones.  Since, in the absence of transition prediction, we would stop the processor and do not execute any instructions, we do not lose anything in such a situation (at least at the approximation level with which we consider the situation). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/924/6b4/4e1/9246b44e1befccb5b799700c9f4934a5.png"><br><br>  What is the performance impact?  To evaluate it, you need to simulate performance and load.  For this lecture, we take a caricature model of a conveyor CPU, where non-branching is worked out in approximately one instruction per cycle, unpredicted or incorrectly predicted transitions take 20 cycles, and correctly predicted transitions take one cycle. <br><br>  If we take the most popular benchmark for integer loads on the SPECint "workstation", then the distribution can be 20% of branches and 80% of other operations.  Without a prediction of transitions, we expect that the ‚Äúaverage‚Äù instruction will take <code>branch_pct * 1 + non_branch_pct * 20 = 0.8 * 1 + 0.2 * 20 = 0.8 + 4 = 4.8</code> cycles.  With an ideal 100% accurate prediction of transitions, the average instruction will take <code>0.8 * 1 + 0.2 * 1 = 1</code> cycle, which means acceleration 4.8 times!  In other words, if we have a pipeline with a ‚Äúfine‚Äù of 20 cycles for an unpredictable transition, then we get almost fivefold costs compared to the ideal pipeline, only due to branch prediction. <br><br>  See what you can do about it.  Let's start with the simplest things, and gradually develop a better solution. <br><br><h3>  Take all the transitions </h3><br>  Instead of random predictions, we can look at all the branches of the execution of all programs.  In this case, we will see that taken and unmarked branches are not equally balanced - the branches taken are much more than the uncivilized ones.  One of the reasons are loop branches that are often executed. <br><br>  If we predict the taking of each branch, then we can reach the level of prediction accuracy of 70%, so we will only pay for incorrectly predicted 30% of transitions, which reduces the cost of the average instruction to <code>(0.8 + 0.7 * 0.2) * 1 + 0.3 * 0.2 * 20 = 0.94 + 1.2. = 2.14</code>  <code>(0.8 + 0.7 * 0.2) * 1 + 0.3 * 0.2 * 20 = 0.94 + 1.2. = 2.14</code> .  If we compare the mandatory prediction of all transitions with the lack of prediction and the ideal prediction, then the mandatory prediction plays most of the advantages of the ideal prediction, although this is a very simple algorithm. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/42d/e81/bb6/42de81bb605d9d8e4240eccd85d5232b.png"><br><br><h3>  We take transitions back, we do not take forward transitions (BTFNT) </h3><br>  Predicting the taking of all branches works well for cycles, but not for other branches.  If you look at the percentage of transitions carried forward in the program or back in the program (returning to the previous code), we will see that the transitions are performed more often than the transitions forward.  Therefore, you can try a predictor that predicts taking all transitions back and not taking all transitions forward (BTFNT, backwards taken forwards not taken).  If we implement this scheme at the hardware level, the compilers will adapt to us and begin to organize the code so that the branches with the greatest chances of execution become transitions back, and the branches with the least chances of execution become transitions forward. <br><br>  If you do this, you can achieve prediction accuracy of about 80%, which reduces the cost of executing instructions to <code>(0.8 + 0.8 * 0.2) * 1 + 0.2 * 0.2 * 20 = 0.96 + 0.8 = 1.76</code> cycles. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/330/a36/3b9/330a363b98c3898acf3be3366880c7f8.png"><br><br>  <b>Who uses:</b> <br><br><ul><li>  PPC 601 (1993): also uses compiler tips in the form of generated transitions. </li><li>  PPC 603 </li></ul><br><h3>  One bit </h3><br>  So far, we have considered schemes that do not save any state, that is, such schemes, where the predictor ignores the program execution history.  In the literature, they are called static transition prediction methods.  Their advantage is simplicity, but they do poorly with predicting transitions that change over time.  Here is an example of such a transition: <br><br><pre> <code class="hljs objectivec"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (flag) { <span class="hljs-comment"><span class="hljs-comment">// things }</span></span></code> </pre> <br>  During the execution of the program at one stage, the flag can be set and the transition is made, and at another stage there is no flag and the transition does not occur.  Static methods cannot predict branching like this well, so consider <i>dynamic</i> methods, where the prediction may vary depending on the execution history of the program. <br><br>  One of the simplest things is to predict on the basis of the last result, that is, to predict the transition if it took place the last time, and vice versa. <br><br>  Since assigning a bit to each transition is too much for reasonable storage, we will make a table for a certain number of transitions that have taken place recently.  For this lecture, assume that the absence of a transition corresponds to 0, and the capture of a branch is 1. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3a5/433/964/3a5433964cce0c970051e8aa2bb347f0.png"><br><br>  In our case, so that everything fits in the illustration, take a table with 64 entries.  This number of records allows us to index the table with 6 bits, so that we use the lower 6 bits of the jump address.  After the transition is completed, we update the state in the prediction table (highlighted below), and the next time in the index we use the already updated state. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9cc/e0a/723/9cce0a723d49e23483e04b50af636df6.png"><br><br>  Perhaps there will be an overlap and two branches from different places will point to the same address.  The scheme is not perfect, but it is a compromise between the speed of the table and its size. <br><br>  If you use a one-bit scheme, then we can achieve an accuracy of 85%, which corresponds to an average <code>(0.8 + 0.85 * 0.2) * 1 + 0.15 * 0.2 * 20 = 0.97 + 0.6 = 1.57</code> cycles per instruction. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/168/e88/f40/168e88f40797f037f82aa095df1b55da.png"><br><br>  <b>Who uses:</b> <br><br><ul><li>  DEC EV4 (1992) </li><li>  MIPS R8000 (1994) </li></ul><br><h3>  <a href="https://courses.cs.washington.edu/courses/cse590g/04sp/Smith-1981-A-Study-of-Branch-Prediction-Strategies.pdf">Two bits</a> </h3><br>  A one-bit scheme works well for templates of the form <code>TTTTTTTT‚Ä¶</code> or <code>NNNNNNN‚Ä¶</code> , but will give incorrect predictions for the branch flow, where most transitions occur, but one of them does not occur, <code>...TTTNTTT...</code>  This can be corrected by adding a second bit to each address and introducing a counter that implements saturation arithmetic.  For example, we will take a unit if the transition did not take place, and add a unit if it took place.  The binary results will be as follows: <br><br> <code>00: predict Not <br> 01: predict Not <br> 10: predict Taken <br> 11: predict Taken</code> <br> <br>  The ‚Äúsaturating‚Äù part of the counter means that if we count to 00, then we remain at this value.  In the same way, if we count to 11, then we stay on it.  This scheme is identical to one bit, but instead of one bit in the prediction table, we use two bits. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a7e/34f/326/a7e34f326273080b8d1d12d75042afb4.png"><br><br>  Compared to a single-bit scheme, a two-bit scheme can have half the number of records at the same size and cost (if we take into account only the cost of storage, but do not take into account the cost of the counter logic).  But even so, for any reasonable table size, a two-bit scheme provides better accuracy. <br><br>  Despite its simplicity, the scheme works quite successfully, and we can expect an increase in accuracy in the two-bit predictor to about 90%, which corresponds to 1.38 cycles per instruction. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a20/b95/2cf/a20b952cf372bbe73f6890a76e073fb8.png"><br><br>  It seems logical to generalize the scheme for an n-bit counter with saturation, but it turns out that increasing the bit depth has almost no effect on accuracy.  We did not discuss the price of the branch predictor, but the transition from two to three bits increases the size of the table by a factor of one and a half for insignificant gain.  In most cases it is not worth it.  The simplest and most common cases that we cannot predict well with a two-bit predictor are patterns like <code>NTNTNTNTNT...</code> or <code>NNTNNTNNT‚Ä¶</code> , but switching to more bits won't improve the accuracy in these cases either! <br><br>  <b>Who uses:</b> <br><br><ul><li>  LLNL S-1 (1977) </li><li>  CDC Cyber?  (early 80s) </li><li>  Burroughs B4900 (1982): the state is stored in the stream of instructions;  at the hardware level, the instruction is overwritten to update the transition state </li><li>  Intel Pentium (1993) </li><li>  PPC 604 (1994) </li><li>  DEC EV45 (1993) </li><li>  DEC EV5 (1995) </li><li>  PA 8000 (1996): actually implemented as a three-bit shift register with a majority of votes. </li></ul><br><h3>  <a href="http://www.seas.upenn.edu/~cis501/papers/two-level-branch-pred.pdf">Two-Level Adaptive Global Transition Prediction</a> (1991) </h3><br>  If you think about this code <br><br><pre> <code class="hljs objectivec"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; <span class="hljs-number"><span class="hljs-number">3</span></span>; ++i) { <span class="hljs-comment"><span class="hljs-comment">// code here. }</span></span></code> </pre> <br>  Then it generates a branch pattern like <code>TTTNTTTNTTTN...</code> <br><br>  If we know the three previous branch results, then we can predict the fourth: <br><br> <code>TTT:N <br> TTN:T <br> TNT:T <br> NTT:T</code> <br> <br>  The previous schemes used the branch address to place the index in a table that indicated whether or not a transition was more likely depending on the program execution history.  She says in which direction the branching will take place, but she is not able to guess that we are in the middle of a repeating pattern.  To fix this, we will keep the history of most recent transitions, as well as a table of predictions. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7fa/c98/ffb/7fac98ffbc179772e7ce6011a2d2d4ba.png"><br><br>  In this example, we associate the four bits of the transition history with two bits of the transition address to place the index in the prediction table.  As before, the source of the prediction is a two-bit counter with saturation.  Here we do not want to use only the conversion history for the index.  If you do this, then any two branches with the same history will refer to the same entry in the table.  In a real predictor, we would probably have a larger table and more bits for the jump address, but we have to use a six-bit index to fit the table into the slide. <br><br>  Below we see what value is updated during the transition. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8bb/962/9cc/8bb9629ccf4240b8a4bd065f14208de4.png"><br><br>  Bold highlighted upgradeable parts.  In this diagram, we shift the new bits of the transition history from right to left, updating the branch history.  As the history is updated, the lower bits of the index in the prediction table are also updated, so the next time we encounter this branch, we use a different value from the table to predict the transition, unlike previous schemes, where the index was fixed in the transition address. <br><br>  Since the global history is stored in this diagram, it will correctly predict patterns like <code>NTNTNTNT‚Ä¶</code> in internal cycles, but it may not always correctly predict high-level branching, because global history is clogged with information from other branches.  However, the trade-off here is that it is cheaper to keep global history than a table of local stories.  In addition, the use of a global history allows you to correctly predict correlated branches.  For example, we might have something like this: <br><br><pre> <code class="hljs objectivec"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> x &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>: x -= <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> y &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>: y -= <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> x * y &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>: foo()</code> </pre> <br>  If there is a transition along the first or second branch, the third one will definitely remain untapped. <br><br>  With this scheme, we can achieve an accuracy of 93%, which corresponds to 1.27 cycles per instruction. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e0b/756/7d6/e0b7567d62acde3af024ae5a5e01431f.png"><br><br>  <b>Who uses:</b> <br><br><ul><li>  Pentium MMX (1996): 4-Bit Global Transition History </li></ul><br><h3>  <a href="http://www.seas.upenn.edu/~cis501/papers/two-level-branch-pred.pdf">Two-level adaptive local transition prediction</a> (1992) </h3><br>  As mentioned above, the global history scheme has the problem that the history of local branches is clogged by other branches. <br><br>  Good local predictions can be achieved by storing the history of local branches separately. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/170/acb/121/170acb1218cf7180d209fa9aedf7b897.png"><br><br>  Instead of storing a single global history, we save a table of local histories, indexed by the address of the transition.  Such a scheme is identical to the global scheme that we have just considered, with the exception of storing the histories of several branches.  You can imagine a global history as a special case of a local history, where the number of stored stories is 1. <br><br>  With this scheme, we can achieve an accuracy of 94%, which corresponds to 1.23 cycles per instruction <br><br><img src="https://habrastorage.org/getpro/habr/post_images/51b/8ac/fe4/51b8acfe444d6a1367ebc4099db48a4d.png"><br><br>  <b>Who uses:</b> <br><br><ul><li>  Pentium Pro (1996): <a href="http://www.ece.uah.edu/~milenka/docs/milenkovic_WDDD02.pdf">4-bit history of local branching, the lower bits are used for the index</a> .  Please note that this issue is debatable, and Andrew Fogh states that PPro and subsequent processors use 4-bit global history. </li><li>  Pentium II (1997): the same as in PPro </li><li>  Pentium III (1999): the same as in PPro </li></ul><br><h3>  gshare </h3><br>  In the global two-tier scheme, you have to compromise: in the fixed-size prediction table, the bits must match either the transition history or the transition address.  We would like to allocate more bits for the transition history, because this will allow us to track the correlations at a greater distance, as well as to track more complex patterns.  But we also want to give more bits under the address to avoid mutual interference between unrelated branches. <br><br>  You can try to solve both problems by applying simultaneous hashing, and not the linkage of the transition history and the address of the transition.  This is one of the simplest and most reasonable things we can do, and the first candidate for the role of the mechanism for this operation comes <code><a href="https://en.wikipedia.org/wiki/XOR_gate">xor</a></code> .  This two-level adaptive scheme where we use <code>xor</code> is called <code>gshare</code> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f55/f6c/751/f55f6c7515cf97baec778d986b623e53.png"><br><br>  With this scheme, we can increase the accuracy to about 94%.  This is the same accuracy as in the local scheme, but gshare does not need to store a large table of local stories.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">That is, we get the same accuracy at lower cost, which is a significant improvement. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Who uses:</font></font></b> <br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> MIPS R12000 (1998): 2K records, 11 bits PC, 8 bits history </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> UltraSPARC-3 (2001): 16K records, 14 bits PC, 12 bits history </font></font></li></ul><br><h3> <a href="http://meseec.ce.rit.edu/eecc722-fall2006/papers/branch-prediction/5/agree_isca24.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">agree</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (1997)</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">One of the reasons for incorrect prediction of transitions is interference between different branches that refer to the same address. </font><font style="vertical-align: inherit;">There are many ways to reduce interference. </font><font style="vertical-align: inherit;">In fact, in this lecture we look at the schemes of the 90s precisely because since then a large number of noise suppression schemes have been proposed, and there are too many of them to meet the half hour.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's look at one scheme that gives a general idea of ‚Äã‚Äãhow interference elimination may look like - this is the ‚Äúagree‚Äù predictor. When a collision of two history-transition pairs occurs, the predictions either coincide or not. If they match, we call it neutral interference, and if not, then negative interference. The idea is that most of the branches have a strong bias in some way (that is, if we use a two-bit entry in the table of predictions, it is expected that most of the entries without interference most of the time will be equal to </font></font><code>00</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, or </font></font><code>11</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">instead of </font></font><code>01</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, or</font></font><code>10</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">). For each transition in the program, we will store one bit, which we call ‚Äúbias‚Äù. Instead of storing absolute transition predictions, the table will store information about whether or not the prediction matches the ‚Äúbias‚Äù bit.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/00d/1fb/cc1/00d1fbcc1d2c9f7587c8d8be51517eb6.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If you look at how this works, then the predictor is identical to the gshare predictor, with the exception of the change we made - the prediction now looks like agree / disagree (agree or disagree), not taken / not taken (transition made or not implemented) , and we have the ‚Äúbias‚Äù bit, which is indexed at the jump address, which gives us the material to make an agree / disagree decision. The authors of the original scientific article suggest using ‚Äúbias‚Äù directly, but other experts have put forward the idea of ‚Äã‚Äãdefining ‚Äúbias‚Äù by optimization based on profiling (essentially, when a working program sends data back to the compiler).</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Notice that if we make the transition and then go back to the same branch, we will use the same bit ‚Äúbias‚Äù because it is indexed by the transition address, but we will use another entry in the prediction table because it is indexed at the same time and the address of the transition, and the history of transitions. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/438/25a/180/43825a1806516cb291409695b6f6d17f.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If it seems strange to you that such a change matters, consider it with a specific example. Let's say we have two branches, branch A, on which the transition occurs with a probability of 90%, and branch B, on which the transition occurs with a probability of 10%. If we assume that the probabilities of transitions for each branch are independent of each other, then the probability of a disagree decision and negative interference is </font></font><code>P(A taken) * P(B not taken) + P(A not taken) + P(B taken) = (0.9 * 0.9) + (0.1 * 0.1) = 0.82</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If we use the agree scheme, we can repeat this calculation, but the probability of a disagree decision making for the two branches and negative interference is </font></font><code>P(A agree) * P(B disagree) + P(A disagree) * P(B agree) = P(A taken) * P(B taken) + P(A not taken) * P(B taken) = (0.9 * 0.1) + (0.1 * 0.9) = 0.18</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">In other words, in order for destructive interference to appear, one of the branches must disagree with ‚Äúbias‚Äù. </font><font style="vertical-align: inherit;">By definition, if we correctly identified this bit, then the probability of this event cannot be large. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">With this scheme, we can achieve an accuracy of 95%, which corresponds to 1.19 cycles per instruction. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/74f/cdb/9dc/74fcdb9dcb5439bdbda7cd337c2dd532.png"><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Who uses:</font></font></b> <br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> PA-RISC 8700 (2001) </font></font></li></ul><br><h3> <a href="http://www.hpl.hp.com/techreports/Compaq-DEC/WRL-TN-36.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hybrid</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (1993)</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As we have seen, local predictors can well predict some types of branching (for example, built-in loops), and global predictors do well with other types (for example, with some correlated branches). You can try to combine the advantages of both predictors and get a meta-predictor that predicts whether to use a local or global predictor. A simple way is to use the same scheme for a meta-predictor as in the above-described two-bit predictor, but instead of a prediction, </font></font><code>taken</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">or </font></font><code>not taken</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">it predicts a local or global predictor.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/b63/025/864/b630258644349fe799f832d1d967765f.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Also, as there are many schemes for eliminating interference, one of which is the aforementioned agree scheme, there are also many hybrid schemes. </font><font style="vertical-align: inherit;">We can use any two predictors, not only local and global, and the number of predictors is even more than two. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If we use global and local predictors, we can achieve an accuracy of 96%, which corresponds to a 1.15 cycle per instruction. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/f56/358/5f4/f563585f43d9165cf7d25b37e5744be4.png"><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Who uses:</font></font></b> <br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> DEC EV6 (1998): combination of local (1k records, 10 bits of history, 3 bits per counter) and global (4k records, 12 bits of history, 2 bits per counter) predictors </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> IBM POWER4 (2001): local (16k records) and gshare (16k records, 11 bits of history, xor with a jump address, 16k selector table) </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> IBM POWER5 (2004): a combination of bimodal (not covered here) and two-level adaptive predictors </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> IBM POWER7 (2010) </font></font></li></ul><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> What we missed </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In this lecture, we have not talked about many things. </font><font style="vertical-align: inherit;">As you might expect, the volume of the leaked material is much larger than the one we told you about. </font><font style="vertical-align: inherit;">I will briefly mention some of the missing topics with links, so you can read about them if you want to know more. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">One important thing we didn‚Äôt talk about was </font></font><a href="https://en.wikipedia.org/wiki/Branch_target_predictor"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">how to predict the destination of the transition</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Note that this needs to be done even for some unconditional transitions (that is, for transitions that do not need special prediction, because they occur in any case), because </font></font><a href="http://www.engr.uconn.edu/~zshi/course/cse5302/ref/bray91btb_tr.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(some) unconditional transitions have unknown goals</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Predicting the goal of a transition is so costly that some early CPUs had a transition prediction policy ‚Äúto always predict no transition‚Äù, because the target address of the transition is not needed if there is no transition! Constant prediction of the absence of transitions has low accuracy, but it is still higher than if you do not make predictions at all. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Among the predictors with reduced interference, which we have not discussed, </font></font><a href="https://people.eecs.berkeley.edu/~kubitron/courses/cs152-S04/handouts/papers/p4-lee.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">bi-mode</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="http://meseec.ce.rit.edu/eecc722-fall2002/papers/branch-prediction/7/michaud97trading.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">gskew</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><a href="http://web.eecs.umich.edu/~tnm/papers/yags.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">YAGS</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Briefly, bi-mode is similar to agree in trying to split transitions by direction, but it uses a different mechanism: we maintain several prediction tables, and the third predictor uses the transition address to predict which table to use for a particular transition combination. and transition history. Bi-mode seems to be more successful than agree, as it has gained wider use. In the case of gksew, we maintain at least three prediction tables and separate hashes for indexing each table. The idea is that if two transitions overlap each other in the index, then this only happens in one table, so that we can vote and the result from the other two tables overlap the potentially bad result from the overlapping table. I do not know how to very briefly describe the scheme YAGS :-).</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Since we did not talk about speed (delay), we did not talk about such a prediction strategy as using a small / fast predictor, the result of which can be covered by a slower and more accurate predictor. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Some modern CPUs have completely different conversion predictors; AMD Zen (2017) and AMD Bulldozer (2011) processors seem to use </font></font><a href="https://www.cs.utexas.edu/~lin/papers/hpca01.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">perceptron transition predictors</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Perceptrons are single-level neural networks. </font></font><br><br> <a href="https://hal.inria.fr/hal-01100647/document"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Allegedly</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , the Intel Haswell processor (2013) uses a variation of the </font></font><a href="http://www.irisa.fr/caps/people/seznec/JILP-COTTAGE.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TAGE predictor</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. TAGE means tagged geometric (TAgged Geometric) predictor with history size. If we study the predictors described by us and look at the actual execution of programs ‚Äî which transitions are predicted incorrectly there, then there is one big class among them: these are transitions that need a big story. A significant number of transitions require dozens or hundreds of bits of history, and some even need more than a thousand bits of conversion history. If we have a single predictor or even a hybrid one that combines several different predictors, it will be inefficient to store a thousand bits of history, since this reduces the quality of predictions for transitions that need a relatively small history (especially in relation to price), and most of such transitions. One of the ideas behind the TAGE predictor is thatto keep the geometric lengths of the story sizes, and then each transition will use the corresponding story. This is an explanation of a part of GE in the abbreviation. The TA part means that the transitions are tagged. We did not discuss this mechanism; the predictor uses it to determine which story corresponds to which transitions.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In modern CPUs, specialized predictors are often used. For example, the cycle predictor can accurately predict transitions in cycles where the general transition predictor is not able to store a reasonable history size for perfect prediction of each iteration of the cycle. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We didn‚Äôt talk at all about the trade-off between memory usage and better predictions. Changing the size of the table not only changes the effectiveness of the predictor, but also changes the balance of forces in which predictor is better compared to others. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We also did not talk at all about how different tasks affect the performance of predictors of different types. Their effectiveness depends not only on the size of the table, but also on which particular program is running.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We also discussed the cost of incorrectly predicting a transition as if it were a constant value, </font></font><a href="http://users.elis.ugent.be/~leeckhou/papers/ispass06-eyerman.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">but this was not the case</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , and in this respect the cost of instructions without transitions is also very different, depending on the type of task being performed. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I tried to avoid confusing terminology where possible, so if you read the literature, there is a slightly different terminology.</font></font><br><br><h3>  Conclusion </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We reviewed a number of classical predictors of transitions and very briefly discussed a couple of newer predictors. Some of the classic predictors we considered are still used in processors, and if I had an hour of time, not half an hour, we could discuss the most up-to-date predictors. I think many people have the idea that a processor is a mysterious and difficult to understand thing, but in my opinion, processors are actually easier to understand than software. I may not be objective, because I used to work with processors, but still it seems to me that this is not my bias, but this is something fundamental.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If you think about the complexity of the software, then the only limiting factor of this complexity will be your imagination. If you can imagine something in such detailed detail that it can be recorded, then you can do it. Of course, in some cases the limiting factor is something more practical (for example, the performance of large-scale applications), but I think most of us spend most of the time writing programs, where the limiting factor is the ability to create and manage complexity.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The hardware is noticeably different here, because there are forces that oppose complexity. Every piece of iron you implement costs money, so you want to use a minimum of equipment. In addition, for most iron, performance is important (whether it is absolute performance or productivity per dollar, or per watt, or other resource), and the increase in complexity slows down the iron, which limits productivity. Today you can buy a serial CPU for $ 300, which accelerates to 5 GHz. At 5 GHz, one unit of work is performed in one fifth of a nanosecond. For information, light travels about 30 centimeters per nanosecond. Another deterrent is that people get very upset if the CPU doesn't work perfectly all the time. Although </font></font><a href="https://danluu.com/cpu-bugs/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">processors have bugs</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, the number of bugs is much less than in almost any software, that is, the standards for checking / testing them are much higher. Increasing complexity makes testing and verification difficult. Since processors adhere to a higher standard of accuracy than </font></font><a href="https://danluu.com/everything-is-broken/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">most software</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , increasing the complexity adds too much testing / testing burden for the CPU. Thus, the same complication is much more expensive for hardware than for software, even without taking into account other factors that we discussed.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A side effect of these factors that counteract the complexity of the chip is that each ‚Äúhigh-level‚Äù function of a general-purpose processor is usually fairly conceptually simple to describe in half an hour or an hour lecture. </font><font style="vertical-align: inherit;">Processors are simpler than many programmers think! </font><font style="vertical-align: inherit;">By the way, I said ‚Äúhigh-level‚Äù to exclude all sorts of things like devices of transistors and circuitry, for understanding of which a certain low-level background is needed (physics or electronics).</font></font></div><p>Source: <a href="https://habr.com/ru/post/337000/">https://habr.com/ru/post/337000/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../336984/index.html">Position selection procedure</a></li>
<li><a href="../336988/index.html">Programming Contest: JSDash (Results)</a></li>
<li><a href="../336992/index.html">Mobile Applications Testing Cheat Sheet</a></li>
<li><a href="../336994/index.html">Getting rid of fragment state saving libraries using pure kotlin</a></li>
<li><a href="../336998/index.html">Fast rendering of ocean waves on mobile devices</a></li>
<li><a href="../337002/index.html">Kotlin aftertaste, part 2</a></li>
<li><a href="../337004/index.html">Andrei Ershov about the evolution of Future in Java and Scala on jug.msk.ru</a></li>
<li><a href="../337006/index.html">Microsoft Excel add-in development using Excel-DNA library</a></li>
<li><a href="../337008/index.html">Ansible Dynamic Inventory from DNS or how to waste time searching for standards</a></li>
<li><a href="../337010/index.html">Ideal OS: rebooting desktop operating systems (part 1)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>