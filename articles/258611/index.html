<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>An example of a vector implementation of a neural network using Python</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The article discusses the construction of neural networks (with regularization) with calculations mainly vector-based method of Python. The article is...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>An example of a vector implementation of a neural network using Python</h1><div class="post__text post__text-html js-mediator-article">  The article discusses the construction of neural networks (with regularization) with calculations mainly vector-based method of Python.  The article is close to the materials of the course Machine learning by Andrew Ng for quicker perception, but if you did not complete the course, nothing terrible, nothing specific is foreseen.  If you have always wanted to build your neural network with <s>preference and young lady</s> vectors and regularization, but something is holding you back, now is the time. <br><br>  This article is aimed at the practical implementation of neural networks, and it is assumed that the reader is familiar with the theory (so it will be omitted). <br><a name="habracut"></a><br>  The code is commented in English, but it is unlikely that someone will have difficulties, since the main comments are also given in the article in Russian. <br><br>  We will write code to search for optimal weights using scipy.optimize.fmin_cg, as well as independently, so the scripts will be universal. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <div class="spoiler">  <b class="spoiler_title">What vectors are we talking about and why are they needed?</b> <div class="spoiler_text">  Suppose a simple task is to add in pairs the elements of two one-dimensional arrays.  We can solve this problem in a loop with enumeration of all array values ‚Äã‚Äãor add two vectors.  Consider the following code. <br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> time A = np.random.rand(<span class="hljs-number"><span class="hljs-number">1000000</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-comment"><span class="hljs-comment">#    1 .   1     float B = np.random.rand(1000000, 1) # ---- C1 = np.empty((1000000, 1)) #    1 .   1     C2 = np.empty((1000000, 1)) #    1 .   1     start = time.time() for i in range(0, len(A)): C1[i] = A[i] * B[i] #     A, B        C print(time.time() - start) start = time.time() C2 = A + B #    print(time.time() - start) if (C1 == C2).all(): #       print('Equal!')</span></span></code> </pre> <br>  On a simple laptop, the cycle is processed in an average of 4 seconds.  40 mil  seconds  Vectors add up in 0.02 sec. <br>  Approximately the same difference in speed and with other arithmetic operations.  Not to mention the visibility of the code. <br></div></div><br>  We will immediately move on to practice, there are many articles on neural networks on Habr√©, <br><div class="spoiler">  <b class="spoiler_title">for example these</b> <div class="spoiler_text">  <a href="http://habrahabr.ru/post/143129/">Neural networks for dummies</a> <br>  <a href="http://habrahabr.ru/post/144881/">Neural networks for dummies 2</a> <br>  <a href="http://habrahabr.ru/post/198268/">Algorithm for learning a multilayer neural network using the back propagation error (Backpropagation)</a> <br><br></div></div><br>  <u>Software and dependencies</u> <br>  Python 3.4 (on 2.7 will also work with minor edits) <br>  Numpy <br>  SciPy (optional) <br><br>  For convenience, all functions of the neural network will be rendered into separate module network.py <br><br><h4>  <b>Networking</b> </h4><br>  The first thing you want to do is create a fully connected neural network.  In fact, this means that we need to initialize the Theta Œ∏ weights matrices with random values ‚Äã‚Äãfrom -1 to 1. <br><br>  Since we are creating functions inside the network class, the function will actually have one layers variable, which contains the number of layers and neurons in the created network in the list format.  For example [10, 5, 5, 1] ‚Äã‚Äãmeans a network with ten input neurons, two hidden layers of 5 neurons each and an output layer with one neuron. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">create</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, layers)</span></span></span><span class="hljs-function">:</span></span> theta=[<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, len(layers)): <span class="hljs-comment"><span class="hljs-comment"># for each layer from the first (skip zero layer!) theta.append(np.mat(np.random.uniform(-1, 1, (layers[i], layers[i-1]+1)))) # create nxM+1 matrix (+bias!) with random floats in range [-1; 1] nn={'theta':theta,'structure':layers} return nn</span></span></code> </pre><br>  Since the size of the variable layers is unlikely to exceed a few dozen elements even in the most complex neural network, in this case, apply a for loop. <br><br>  As a result, we want to get a list theta whose elements will be matrices with weights. <br><br><div class="spoiler">  <b class="spoiler_title">Why do we initialize theta [0] = 0?</b> <div class="spoiler_text"><img src="https://habrastorage.org/getpro/habr/post_images/a77/0d2/f7a/a770d2f7a78c6f7b97a78a32a2ab6a45.png" alt="image"><br>  Picture from materials of the course Machine Learning, with modifications <br><br>  In the field of neural networks, it is accepted to call the Œ∏1 matrix of weights from input data (input) to the first neural layer.  Since we do not have any layers up to the Input layer, there is no zero weight matrix either. <br></div></div><br><div class="spoiler">  <b class="spoiler_title">What is theta [1], theta [2], ..., theta [n]?</b> <div class="spoiler_text">  All matrices are compiled by one algorithm, so consider theta [1] for example.  Theta [1] is a matrix in which the number of rows is equal to the number of neurons in the first hidden layer, and the number of columns is equal to the column for the bias (offset) + the number of neurons in the input layer. <br><br>  That is, if we take the first row of the theta [1] matrix, then the zero element (read the zero column) will correspond to the weight of the bias, the other elements (columns) will correspond to the weights for communication with each element of the incoming layer. <br></div></div><br><div class="spoiler">  <b class="spoiler_title">What is Bias and why is it needed?</b> <div class="spoiler_text">  Bias is translated from English as ‚Äúoffset‚Äù and in fact this is what it means (always yours, Cap).  Better than <a href="http://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks">here</a> I can hardly say, so I‚Äôll just complete the translation. <br><br>  Bias is almost always useful because it allows <b>you</b> to <b>shift the activation function to the left or right</b> , which can be extremely important for successful learning. <br>  A simple example will help to understand.  Imagine the simplest neural network without a hidden layer, only one incoming and 1 outgoing neuron <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ef5/517/4e8/ef55174e813e40a3ccc3b2beaf961757.gif" alt="image"><br><br>  The output signal of the neural network is calculated by multiplying the weight of W0 by the signal X and applying the activation function (most often sigmoid) to the product. <br>  An example of the functions that we obtain for different values ‚Äã‚Äãof the weight W0 <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ede/140/790/ede140790c120e5d6f269c2f32dbfe90.png" alt="image"><br><br>  Changing the value of the weight W0 we change the slope of the curve, the degree of its steepness, this is convenient, but what if we want the outgoing signal to be 0 when X is 2?  Just changing the slope of the curve will not work - <b>it will not work to shift the entire curve to the right</b> . <br>  This is what Bias does.  If we add Bias to our neural network, like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a0a/6c1/b51/a0a6c1b514e3dbdc4556a60aeb64eb91.gif" alt="image"><br><br>  ... then the output of our neural network will be considered sig (w0 * x + w1 * 1.0).  Accordingly, our function will look like this when changing the weight of W1: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/994/8fd/509/9948fd5091b75121790c6608444b4889.png" alt="image"><br><br>  The weight of W1 equal to -5 will shift the curve to the right, therefore the output signal of the neural network will be equal to 0 with X equal to 2 <br></div></div><br><h4>  <b>Neural network outgoing signal calculation</b> </h4><br>  At this stage we need to calculate the output signal of the neural network using the weights initialized in the previous step. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">runAll</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, nn, X)</span></span></span><span class="hljs-function">:</span></span> z=[<span class="hljs-number"><span class="hljs-number">0</span></span>] m = len(X) a = [ copy.deepcopy(X) ] <span class="hljs-comment"><span class="hljs-comment"># a[0] is equal to the first input values logFunc = self.logisticFunctionVectorize() for i in range(1, len(nn['structure'])): # for each layer except the input a[i-1] = np.c_[ np.ones(m), a[i-1]]; # add bias column to the previous matrix of activation functions z.append(a[i-1]*nn['theta'][i].T) # for all neurons in current layer multiply corresponds neurons # in previous layers by the appropriate weights and sum the productions a.append(logFunc(z[i])) # apply activation function for each value nn['z'] = z nn['a'] = a return a[len(nn['structure'])-1]</span></span></code> </pre><br><div class="spoiler">  <b class="spoiler_title">More detailed description of variables</b> <div class="spoiler_text">  Matrix <b>X</b> format: rows - vectors of input values ‚Äã‚Äã(input values), columns - elements of vectors. <br><br>  <b>Z</b> - sheet with matrices of the sum of products of the activation function values ‚Äã‚Äãfrom the previous layer and the weights connecting them with the current layer by the neuron.  Incoming values ‚Äã‚Äãdo not need to use the activation function, they have no weights, so we skip z [0] and start with z [1] <br><br>  <b>a</b> - sheet with matrices of activation function values <br>  a [0] is a matrix containing a bias (a unit vector of dimension m * 1) and incoming vectors X, that is, its dimension is the number of rows in X * (1 + the number of columns in X).  Accordingly, a [1] contains the matrix value of activation functions in the first hidden layer, its dimensionality is the number of rows in X * (1 + the number of neurons in the first hidden layer) <br></div></div><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">logisticFunction</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x)</span></span></span><span class="hljs-function">:</span></span> a = <span class="hljs-number"><span class="hljs-number">1</span></span>/(<span class="hljs-number"><span class="hljs-number">1</span></span>+np.exp(-x)) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> a == <span class="hljs-number"><span class="hljs-number">1</span></span>: a = <span class="hljs-number"><span class="hljs-number">0.99999</span></span> <span class="hljs-comment"><span class="hljs-comment">#make smallest step to the direction of zero elif a == 0: a = 0.00001 # It is possible to use np.nextafter(0, 1) and #make smallest step to the direction of one, but sometimes this step is too small and other algorithms fail :) return a def logisticFunctionVectorize(self): return np.vectorize(self.logisticFunction)</span></span></code> </pre><br>  In short, using the np.vectorize command, the function can now take and read matrices of values.  For example, for each element of the 10x1 matrix, a logistic function will be calculated, and a matrix of values ‚Äã‚Äãwith a dimension of 10x1 will be returned. <br><br><div class="spoiler">  <b class="spoiler_title">What are these conditions in the logisticFunction function?</b> <div class="spoiler_text">  In the code above, one important pitfall is associated with rounding (here you have to run into the front).  Suppose that you are preparing a large network, many layers, many neurons, you initialize weights randomly and it turns out that the sum of products on the output layer for each neuron is very small, for example -40.  The logistic function from -40 will happily return you a unit. <br><br>  Next, we will need to calculate the error of our neural network and we will transfer this unit to calculate the logarithm from 1 - the output value [log (1-output)] is naturally the logarithm of the unit is not defined, but the error does not pop up, just our neural network will not train. <br></div></div><br>  From the important I want to note that we add a column of bias after we applied the activation function to the sum of the products.  This means that the bias is always equal to one, and not the logistic function of one (which is 0.73) <br><br><pre> <code class="python hljs">a[i<span class="hljs-number"><span class="hljs-number">-1</span></span>] = np.c_[ np.ones(m), a[i<span class="hljs-number"><span class="hljs-number">-1</span></span>]];</code> </pre><br>  In addition, in the final matrix of activation functions the bias is present in all layers except the output layer, it contains only the output signals of the neural network (respectively, the dimension of the matrix = number of examples * number of neurons in the output layer). <br><br>  For those who are not familiar with python, I note that the <i>runAll</i> function <i>does</i> not transfer copies of variables (for example, the neural network itself <i>nn</i> ) but references to them, so when we change the variable <i>nn ['z'] = z</i> we change our network nn despite by not passing the nn variable back. <br><br>  As a result, this function ( <i>runAll</i> ) will return to us the matrix of the output signals of the network (its dimensionality is the number of output neurons * 1) and will change the matrices <i>z</i> and <i>a</i> in the variable neural network. <br><br><h4>  <b>Neural network error</b> </h4><br>  The error of the output signal of the neural network with regularization is calculated by the following formula <br><img src="https://habrastorage.org/getpro/habr/post_images/392/44c/a8a/39244ca8a6c6aca4582f66758e7e20cb.png" alt="image"><br>  Picture taken from Machine Learning course materials. <br>  <i>m</i> is the number of examples, <i>K</i> is the number of output neurons of the neural network, <i>h0 (xi)</i> is the vector of output values ‚Äã‚Äãof the neural network, <i>Œ∏</i> is the weights matrix, where Œ∏ ^ 1 is the weights matrix for the first hidden layer, <i>lambda</i> is the regularization coefficient. <br><br>  If it seems to you rather scary and incomprehensible, it is normal :), in essence, it is decomposed into 2 components, with which we will work. <br><br>  A detailed and understandable explanation of the essence of this formula will stretch out, and I am not sure that it is necessary for such a prepared public, so for now let us omit it, but write, if necessary, add. <br><br><div class="spoiler">  <b class="spoiler_title">What is the essence of regularization?</b> <div class="spoiler_text">  The second line of the formula is responsible for regularization, the larger the regularization parameter, the greater the neural network error (since the sum of two positive numbers occurs in the whole formula), therefore in the process of learning, to reduce the error, it will be necessary to reduce the neural network weights, that is, a high regularization coefficient will keep the weights of the neural network small. <br></div></div><br>  Actually the error function will return us a single value in the float format, which will characterize how correctly our neural network calculates the output signal. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">costTotal</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, theta, nn, X, y, lamb)</span></span></span><span class="hljs-function">:</span></span> m = len(X) <span class="hljs-comment"><span class="hljs-comment">#following string is for fmin_cg computaton if type(theta) == np.ndarray: nn['theta'] = self.roll(theta, nn['structure']) y = np.matrix(copy.deepcopy(y)) hAll = self.runAll(nn, X) #feed forward to obtain output of neural network cost = self.cost(hAll, y) return cost/m+(lamb/(2*m))*self.regul(nn['theta']) #apply regularization</span></span></code> </pre><br>  The function returns a neural network error for a given matrix <i>X of</i> input parameters. <br>  Calculate the first part of the formula, the network error itself. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">cost</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, h, y)</span></span></span><span class="hljs-function">:</span></span> logH=np.log(h) log1H=np.log(<span class="hljs-number"><span class="hljs-number">1</span></span>-h) cost=<span class="hljs-number"><span class="hljs-number">-1</span></span>*yT*logH-(<span class="hljs-number"><span class="hljs-number">1</span></span>-yT)*log1H <span class="hljs-comment"><span class="hljs-comment">#transpose y for matrix multiplication return cost.sum(axis=0).sum(axis=1) # sum matrix of costs for each output neuron and input vector</span></span></code> </pre><br>  We consider regularization <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">regul</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, theta)</span></span></span><span class="hljs-function">:</span></span> reg=<span class="hljs-number"><span class="hljs-number">0</span></span> thetaLocal=copy.deepcopy(theta) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>,len(thetaLocal)): thetaLocal[i]=np.delete(thetaLocal[i],<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-comment"><span class="hljs-comment"># delete bias connection thetaLocal[i]=np.power(thetaLocal[i], 2) # square the values because they can be negative reg+=thetaLocal[i].sum(axis=0).sum(axis=1) # sum at first rows, than columns return reg</span></span></code> </pre><br>  We assume a cycle through the array with theta matrices, since it is assumed that we have a very limited number of layers, and the performance will not suffer much damage. <br><br>  We remove from the regularization the connection with the bias, since it may well be of great importance if we need to shift the logistics function along the X axis. <br><br><h4>  <b>Gradient calculation</b> </h4><br>  At this stage, we can create a neural network, calculate the output signal and error, now it is only necessary to calculate the gradient and implement the weights correction algorithm. <br><br>  At this step, we can not do without a cycle of incoming vectors and in order to speed up the calculation of the gradient a little, we carry out a maximum of operations before the cycle.  For example, in the previous steps, we specifically designed the <i>runAll</i> function so that it calculates a matrix of input values, and not vectors (strings) individually, at this stage we will calculate the output values ‚Äã‚Äãin advance, then we will access them in a loop.  According to experimental measurements, these features speed up the function by an additional 25% <br><br>  We use the reverse cycle through the layers of the neural network from the last to the first, since we need to calculate the error and transfer it back to the layer to calculate the next one, etc. <br><br>  The main difficulty is not to get confused in the indices of variables.  For example, in most documentation on neural networks for example a three-layer network (with one hidden layer), the delta of the output layer will have index 3, it is clear that in this case the sheet should consist of four elements, while the gradient sheet consists of 3 elements. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">backpropagation</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, theta, nn, X, y, lamb)</span></span></span><span class="hljs-function">:</span></span> layersNumb=len(nn[<span class="hljs-string"><span class="hljs-string">'structure'</span></span>]) thetaDelta = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*(layersNumb) m=len(X) <span class="hljs-comment"><span class="hljs-comment">#calculate matrix of outpit values for all input vectors X hLoc = copy.deepcopy(self.runAll(nn, X)) yLoc=np.matrix(y) thetaLoc = copy.deepcopy(nn['theta']) derFunct=np.vectorize(lambda x: (1/(1+np.exp(-x)))*(1-(1/(1+np.exp(-x)))) ) zLoc = copy.deepcopy(nn['z']) aLoc = copy.deepcopy(nn['a']) for n in range(0, len(X)): delta = [0]*(layersNumb+1) #fill list with zeros delta[len(delta)-1]=(hLoc[n].T-yLoc[n].T) #calculate delta of error of output layer for i in range(layersNumb-1, 0, -1): if i&gt;1: # we can not calculate delta[0] because we don't have theta[0] (and even we don't need it) z = zLoc[i-1][n] z = np.c_[ [[1]], z ] #add one for correct matrix multiplication delta[i]=np.multiply(thetaLoc[i].T*delta[i+1],derFunct(z).T) delta[i]=delta[i][1:] thetaDelta[i] = thetaDelta[i] + delta[i+1]*aLoc[i-1][n] for i in range(1, len(thetaDelta)): thetaDelta[i]=thetaDelta[i]/m thetaDelta[i][:,1:]=thetaDelta[i][:,1:]+thetaLoc[i][:,1:]*(lamb/m) #regularization if type(theta) == np.ndarray: return np.asarray(self.unroll(thetaDelta)).reshape(-1) # to work also with fmin_cg return thetaDelta</span></span></code> </pre><br>  The function returns a sheet whose elements are matrices whose dimensions coincide with the dimension of <i>theta</i> matrices. <br><br>  Moreover, this function is lambda, it is nothing but a derivative of the activation function (sigmoid), so if you want to replace the activation function, also change the derivative <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: (<span class="hljs-number"><span class="hljs-number">1</span></span>/(<span class="hljs-number"><span class="hljs-number">1</span></span>+np.exp(-x)))*(<span class="hljs-number"><span class="hljs-number">1</span></span>-(<span class="hljs-number"><span class="hljs-number">1</span></span>/(<span class="hljs-number"><span class="hljs-number">1</span></span>+np.exp(-x))))</code> </pre><br><h4>  <b>Testing</b> </h4><br>  Now we can test our neural network and even try to teach it something :) <br>  To begin with we will teach our network simple segmentation, all values ‚Äã‚Äãwithin [0; 5) is zero, [5; 9] is one <br><br><pre> <code class="python hljs">nn=nt.create([<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>]) lamb=<span class="hljs-number"><span class="hljs-number">0.3</span></span> cost=<span class="hljs-number"><span class="hljs-number">1</span></span> alf = <span class="hljs-number"><span class="hljs-number">0.2</span></span> xTrain = [[<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">1.9</span></span>], [<span class="hljs-number"><span class="hljs-number">2</span></span>], [<span class="hljs-number"><span class="hljs-number">3</span></span>], [<span class="hljs-number"><span class="hljs-number">3.31</span></span>], [<span class="hljs-number"><span class="hljs-number">4</span></span>], [<span class="hljs-number"><span class="hljs-number">4.7</span></span>], [<span class="hljs-number"><span class="hljs-number">5</span></span>], [<span class="hljs-number"><span class="hljs-number">5.1</span></span>], [<span class="hljs-number"><span class="hljs-number">6</span></span>], [<span class="hljs-number"><span class="hljs-number">7</span></span>], [<span class="hljs-number"><span class="hljs-number">8</span></span>], [<span class="hljs-number"><span class="hljs-number">9</span></span>]] yTrain = [[<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>]] xTest= [[<span class="hljs-number"><span class="hljs-number">0.4</span></span>], [<span class="hljs-number"><span class="hljs-number">1.51</span></span>], [<span class="hljs-number"><span class="hljs-number">2.6</span></span>], [<span class="hljs-number"><span class="hljs-number">3.23</span></span>], [<span class="hljs-number"><span class="hljs-number">4.87</span></span>], [<span class="hljs-number"><span class="hljs-number">5.78</span></span>], [<span class="hljs-number"><span class="hljs-number">6.334</span></span>], [<span class="hljs-number"><span class="hljs-number">7.667</span></span>], [<span class="hljs-number"><span class="hljs-number">8.22</span></span>], [<span class="hljs-number"><span class="hljs-number">9.1</span></span>]] yTest = [[<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>]] theta = nt.unroll(nn[<span class="hljs-string"><span class="hljs-string">'theta'</span></span>]) print(nt.runAll(nn, xTest)) theta = optimize.fmin_cg(nt.costTotal, fprime=nt.backpropagation, x0=theta, args=(nn, xTrain, yTrain, lamb), maxiter=<span class="hljs-number"><span class="hljs-number">200</span></span>) print(nt.runAll(nn, xTest))</code> </pre><br>  Result.  Neural network output before training, training and after training.  It is seen that after training, the first five values ‚Äã‚Äãare closer to zero, the second five are closer to one. <br><img src="https://habrastorage.org/getpro/habr/post_images/490/9cc/587/4909cc587e0fe9533647841fae97056f.png" alt="image"><br><br>  In the previous example, the training was controlled by the function <i>fmin_cg, but</i> now we will change <i>theta</i> (network weight) independently. <br>  Let's set a simple task, to distinguish an upward trend from a downward one.  Neural network input will be fed 4 numbers, if they are consistently increased, this is one, if they decrease, this is zero. <br><br><pre> <code class="python hljs">nn=nt.create([<span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>]) lamb=<span class="hljs-number"><span class="hljs-number">0.3</span></span> cost=<span class="hljs-number"><span class="hljs-number">1</span></span> alf = <span class="hljs-number"><span class="hljs-number">0.2</span></span> xTrain = [[<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2.3</span></span>, <span class="hljs-number"><span class="hljs-number">4.5</span></span>, <span class="hljs-number"><span class="hljs-number">5.3</span></span>], [<span class="hljs-number"><span class="hljs-number">1.1</span></span>, <span class="hljs-number"><span class="hljs-number">1.3</span></span>, <span class="hljs-number"><span class="hljs-number">2.4</span></span>, <span class="hljs-number"><span class="hljs-number">2.4</span></span>], [<span class="hljs-number"><span class="hljs-number">1.9</span></span>, <span class="hljs-number"><span class="hljs-number">1.7</span></span>, <span class="hljs-number"><span class="hljs-number">1.5</span></span>, <span class="hljs-number"><span class="hljs-number">1.3</span></span>], [<span class="hljs-number"><span class="hljs-number">2.3</span></span>, <span class="hljs-number"><span class="hljs-number">2.9</span></span>, <span class="hljs-number"><span class="hljs-number">3.3</span></span>, <span class="hljs-number"><span class="hljs-number">4.9</span></span>], [<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">5.2</span></span>, <span class="hljs-number"><span class="hljs-number">6.1</span></span>, <span class="hljs-number"><span class="hljs-number">8.2</span></span>], [<span class="hljs-number"><span class="hljs-number">3.31</span></span>, <span class="hljs-number"><span class="hljs-number">2.9</span></span>, <span class="hljs-number"><span class="hljs-number">2.4</span></span>, <span class="hljs-number"><span class="hljs-number">1.5</span></span>], [<span class="hljs-number"><span class="hljs-number">4.9</span></span>, <span class="hljs-number"><span class="hljs-number">5.7</span></span>, <span class="hljs-number"><span class="hljs-number">6.1</span></span>, <span class="hljs-number"><span class="hljs-number">6.3</span></span>], [<span class="hljs-number"><span class="hljs-number">4.85</span></span>, <span class="hljs-number"><span class="hljs-number">5.0</span></span>, <span class="hljs-number"><span class="hljs-number">7.2</span></span>, <span class="hljs-number"><span class="hljs-number">8.1</span></span>], [<span class="hljs-number"><span class="hljs-number">5.9</span></span>, <span class="hljs-number"><span class="hljs-number">5.3</span></span>, <span class="hljs-number"><span class="hljs-number">4.2</span></span>, <span class="hljs-number"><span class="hljs-number">3.3</span></span>], [<span class="hljs-number"><span class="hljs-number">7.7</span></span>, <span class="hljs-number"><span class="hljs-number">5.4</span></span>, <span class="hljs-number"><span class="hljs-number">4.3</span></span>, <span class="hljs-number"><span class="hljs-number">3.9</span></span>], [<span class="hljs-number"><span class="hljs-number">6.7</span></span>, <span class="hljs-number"><span class="hljs-number">5.3</span></span>, <span class="hljs-number"><span class="hljs-number">3.2</span></span>, <span class="hljs-number"><span class="hljs-number">1.4</span></span>], [<span class="hljs-number"><span class="hljs-number">7.1</span></span>, <span class="hljs-number"><span class="hljs-number">8.6</span></span>, <span class="hljs-number"><span class="hljs-number">9.1</span></span>, <span class="hljs-number"><span class="hljs-number">9.9</span></span>], [<span class="hljs-number"><span class="hljs-number">8.5</span></span>, <span class="hljs-number"><span class="hljs-number">7.4</span></span>, <span class="hljs-number"><span class="hljs-number">6.3</span></span>, <span class="hljs-number"><span class="hljs-number">4.1</span></span>], [<span class="hljs-number"><span class="hljs-number">9.8</span></span>, <span class="hljs-number"><span class="hljs-number">5.3</span></span>, <span class="hljs-number"><span class="hljs-number">3.1</span></span>, <span class="hljs-number"><span class="hljs-number">2.9</span></span>]] yTrain = [[<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>]] xTest= [[<span class="hljs-number"><span class="hljs-number">0.4</span></span>, <span class="hljs-number"><span class="hljs-number">1.9</span></span>, <span class="hljs-number"><span class="hljs-number">2.5</span></span>, <span class="hljs-number"><span class="hljs-number">3.1</span></span>], [<span class="hljs-number"><span class="hljs-number">1.51</span></span>, <span class="hljs-number"><span class="hljs-number">2.0</span></span>, <span class="hljs-number"><span class="hljs-number">2.4</span></span>, <span class="hljs-number"><span class="hljs-number">3.8</span></span>], [<span class="hljs-number"><span class="hljs-number">2.6</span></span>, <span class="hljs-number"><span class="hljs-number">5.1</span></span>, <span class="hljs-number"><span class="hljs-number">6.2</span></span>, <span class="hljs-number"><span class="hljs-number">7.2</span></span>], [<span class="hljs-number"><span class="hljs-number">3.23</span></span>, <span class="hljs-number"><span class="hljs-number">4.1</span></span>, <span class="hljs-number"><span class="hljs-number">4.3</span></span>, <span class="hljs-number"><span class="hljs-number">4.9</span></span>], [<span class="hljs-number"><span class="hljs-number">7.1</span></span>, <span class="hljs-number"><span class="hljs-number">7.6</span></span>, <span class="hljs-number"><span class="hljs-number">8.2</span></span>, <span class="hljs-number"><span class="hljs-number">9.3</span></span>], [<span class="hljs-number"><span class="hljs-number">5.78</span></span>, <span class="hljs-number"><span class="hljs-number">5.1</span></span>, <span class="hljs-number"><span class="hljs-number">4.5</span></span>, <span class="hljs-number"><span class="hljs-number">3.55</span></span>], [<span class="hljs-number"><span class="hljs-number">6.33</span></span>, <span class="hljs-number"><span class="hljs-number">4.8</span></span>, <span class="hljs-number"><span class="hljs-number">3.4</span></span>, <span class="hljs-number"><span class="hljs-number">2.5</span></span>], [<span class="hljs-number"><span class="hljs-number">7.67</span></span>, <span class="hljs-number"><span class="hljs-number">6.45</span></span>, <span class="hljs-number"><span class="hljs-number">5.8</span></span>, <span class="hljs-number"><span class="hljs-number">4.31</span></span>], [<span class="hljs-number"><span class="hljs-number">8.22</span></span>, <span class="hljs-number"><span class="hljs-number">6.32</span></span>, <span class="hljs-number"><span class="hljs-number">5.87</span></span>, <span class="hljs-number"><span class="hljs-number">3.59</span></span>], [<span class="hljs-number"><span class="hljs-number">9.1</span></span>, <span class="hljs-number"><span class="hljs-number">8.5</span></span>, <span class="hljs-number"><span class="hljs-number">7.7</span></span>, <span class="hljs-number"><span class="hljs-number">6.1</span></span>]] yTest = [[<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>]] <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> cost&gt;<span class="hljs-number"><span class="hljs-number">0</span></span>: cost=nt.costTotal(<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, nn, xTrain, yTrain, lamb) costTest=nt.costTotal(<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, nn, xTest, yTest, lamb) delta=nt.backpropagation(<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, nn, xTrain, yTrain, lamb) nn[<span class="hljs-string"><span class="hljs-string">'theta'</span></span>]=[nn[<span class="hljs-string"><span class="hljs-string">'theta'</span></span>][i]-alf*delta[i] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">0</span></span>,len(nn[<span class="hljs-string"><span class="hljs-string">'theta'</span></span>]))] print(<span class="hljs-string"><span class="hljs-string">'Train cost '</span></span>, cost[<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-string"><span class="hljs-string">'Test cost '</span></span>, costTest[<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>]) print(nt.runAll(nn, xTest))</code> </pre><br>  After 400 iterations (about 1 min.) For some reason, the last test case has the highest error (output of the neural network 0.13), most likely in this case it would help to add training data to improve the quality. <br><img src="https://habrastorage.org/getpro/habr/post_images/359/b59/975/359b599753118b58ee1e158cebd8e118.png" alt="image"><br><br>  In the cycle we change <i>theta</i> in order to achieve the maximum result.  It turns out that we are gliding to the local minimum of the function (and if we added a gradient, then we would go to the local maximum).  The variable <i>alf,</i> often called the ‚Äúlearning rate‚Äù, is responsible for how much we will change <i>theta</i> in each iteration.  Moreover, if you set the <i>alpha</i> parameter too large, the network error may even increase or jump up and down as the function will simply step over the local minimum. <br><br>  As you can see, the entire neural network consists of a single variable of the type dict, so it is easy to make it a secondary one, and save it in a simple text file and also restore it for future use. <br><br>  Perhaps the next post will be on how to speed up this code (and any other written in Python) using GPU computing. <br><br><div class="spoiler">  <b class="spoiler_title">Full listing of the module, use at your discretion</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> copy <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> rd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> theano.tensor <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> th <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">network</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># layers -list [5 10 10 5] - 5 input, 2 hidden # layers (10 neurons each), 5 output def create(self, layers): theta = [0] # for each layer from the first (skip zero layer!) for i in range(1, len(layers)): # create nxM+1 matrix (+bias!) with random floats in range [-1; 1] theta.append( np.mat(np.random.uniform(-1, 1, (layers[i], layers[i - 1] + 1)))) nn = {'theta': theta, 'structure': layers} return nn def runAll(self, nn, X): z = [0] m = len(X) a = [copy.deepcopy(X)] # a[0] is equal to the first input values logFunc = self.logisticFunctionVectorize() # for each layer except the input for i in range(1, len(nn['structure'])): # add bias column to the previous matrix of activation functions a[i - 1] = np.c_[np.ones(m), a[i - 1]] # for all neurons in current layer multiply corresponds neurons z.append(a[i - 1] * nn['theta'][i].T) # in previous layers by the appropriate weights and sum the # productions a.append(logFunc(z[i])) # apply activation function for each value nn['z'] = z nn['a'] = a return a[len(nn['structure']) - 1] def run(self, nn, input): z = [0] a = [] a.append(copy.deepcopy(input)) a[0] = np.matrix(a[0]).T # nx1 vector logFunc = self.logisticFunctionVectorize() for i in range(1, len(nn['structure'])): a[i - 1] = np.vstack(([1], a[i - 1])) z.append(nn['theta'][i] * a[i - 1]) a.append(logFunc(z[i])) nn['z'] = z nn['a'] = a return a[len(nn['structure']) - 1] def logisticFunction(self, x): a = 1 / (1 + np.exp(-x)) if a == 1: a = 0.99999 # make smallest step to the direction of zero elif a == 0: a = 0.00001 # It is possible to use np.nextafter(0, 1) and # make smallest step to the direction of one, but sometimes this step # is too small and other algorithms fail :) return a def logisticFunctionVectorize(self): return np.vectorize(self.logisticFunction) def costTotal(self, theta, nn, X, y, lamb): m = len(X) # following string is for fmin_cg computaton if type(theta) == np.ndarray: nn['theta'] = self.roll(theta, nn['structure']) y = np.matrix(copy.deepcopy(y)) # feed forward to obtain output of neural network hAll = self.runAll(nn, X) cost = self.cost(hAll, y) # apply regularization return cost / m + (lamb / (2 * m)) * self.regul(nn['theta']) def cost(self, h, y): logH = np.log(h) log1H = np.log(1 - h) # transpose y for matrix multiplication cost = -1 * yT * logH - (1 - yT) * log1H # sum matrix of costs for each output neuron and input vector return cost.sum(axis=0).sum(axis=1) def regul(self, theta): reg = 0 thetaLocal = copy.deepcopy(theta) for i in range(1, len(thetaLocal)): # delete bias connection thetaLocal[i] = np.delete(thetaLocal[i], 0, 1) # square the values because they can be negative thetaLocal[i] = np.power(thetaLocal[i], 2) # sum at first rows, than columns reg += thetaLocal[i].sum(axis=0).sum(axis=1) return reg def backpropagation(self, theta, nn, X, y, lamb): layersNumb = len(nn['structure']) thetaDelta = [0] * (layersNumb) m = len(X) # calculate matrix of outpit values for all input vectors X hLoc = copy.deepcopy(self.runAll(nn, X)) yLoc = np.matrix(y) thetaLoc = copy.deepcopy(nn['theta']) derFunct = np.vectorize( lambda x: (1 / (1 + np.exp(-x))) * (1 - (1 / (1 + np.exp(-x))))) zLoc = copy.deepcopy(nn['z']) aLoc = copy.deepcopy(nn['a']) for n in range(0, len(X)): delta = [0] * (layersNumb + 1) # fill list with zeros # calculate delta of error of output layer delta[len(delta) - 1] = (hLoc[n].T - yLoc[n].T) for i in range(layersNumb - 1, 0, -1): # we can not calculate delta[0] because we don't have theta[0] # (and even we don't need it) if i &gt; 1: z = zLoc[i - 1][n] # add one for correct matrix multiplication z = np.c_[[[1]], z] delta[i] = np.multiply( thetaLoc[i].T * delta[i + 1], derFunct(z).T) delta[i] = delta[i][1:] thetaDelta[i] = thetaDelta[i] + delta[i + 1] * aLoc[i - 1][n] for i in range(1, len(thetaDelta)): thetaDelta[i] = thetaDelta[i] / m thetaDelta[i][:, 1:] = thetaDelta[i][:, 1:] + \ thetaLoc[i][:, 1:] * (lamb / m) # regularization if type(theta) == np.ndarray: # to work also with fmin_cg return np.asarray(self.unroll(thetaDelta)).reshape(-1) return thetaDelta # create 1d array form lists like theta def unroll(self, arr): for i in range(0, len(arr)): arr[i] = np.matrix(arr[i]) if i == 0: res = (arr[i]).ravel().T else: res = np.vstack((res, (arr[i]).ravel().T)) res.shape = (1, len(res)) return res # roll back 1d array to list with matrices according to given structure def roll(self, arr, structure): rolled = [arr[0]] shift = 1 for i in range(1, len(structure)): temparr = copy.deepcopy( arr[shift:shift + structure[i] * (structure[i - 1] + 1)]) temparr.shape = (structure[i], structure[i - 1] + 1) rolled.append(np.matrix(temparr)) shift += structure[i] * (structure[i - 1] + 1) return rolled</span></span></code> </pre><br></div></div><br><br>  <b>UPD</b> <br>  Thanks to everyone who is a plus. </div><p>Source: <a href="https://habr.com/ru/post/258611/">https://habr.com/ru/post/258611/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../258599/index.html">Using Arduino as a component of Wolfram SystemModeler</a></li>
<li><a href="../258601/index.html">Unique TechTalk with Michael Monti Widenius</a></li>
<li><a href="../258603/index.html">We select the correct Microsoft Azure services</a></li>
<li><a href="../258605/index.html">Bash state machine</a></li>
<li><a href="../258607/index.html">Krovi: Big Data - as dream. 9th series: Why IBM was forced to buy "Alchemists" for $ 100 million</a></li>
<li><a href="../258613/index.html">Wing IDE Protection Study</a></li>
<li><a href="../258615/index.html">Pulse sensor device Part 2 - Sensors</a></li>
<li><a href="../258619/index.html">Homemade control unit for diesel engine</a></li>
<li><a href="../258621/index.html">DICOM Viewer from the inside. Functionality</a></li>
<li><a href="../258625/index.html">Top 10 non-cash countries of the world</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>