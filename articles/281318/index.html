<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Use an Intel RealSense camera with TouchDesigner. Part 2</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="¬© Rolf Arnold 
 The Intel RealSense Camera is a very useful tool for creating virtual reality and augmented reality projects. In the second part of th...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Use an Intel RealSense camera with TouchDesigner. Part 2</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/82c/71a/539/82c71a539e9d409e9cae8b84940cc12d.jpg"><br>  <sup>¬© Rolf Arnold</sup> <br>  The Intel RealSense Camera is a very useful tool for creating virtual reality and augmented reality projects.  In the second part of this article, you will learn how to use the Intel RealSense camera nodes in TouchDesigner to set up real-time rendering or projection for multi-screen systems, single screens, 180-degree (full-dome), and 360-degree virtual reality systems.  In addition, information from the Intel RealSense camera can be transferred to the Oculus Rift using the Oculus Rift TOP node in TouchDesigner. <br>  The second part will be devoted to the RealSense CHOP node in TouchDesigner. <br><a name="habracut"></a><br>  Access to the most important tracking functions of the RealSense <a href="https://software.intel.com/en-us/RealSense/F200Camera">F200</a> and <a href="https://software.intel.com/en-us/RealSense/R200Camera">R200</a> cameras, such as eye, finger and face tracking, is provided through the RealSense CHOP node in TouchDesigner.  These tracking functions are particularly interesting in real-time animation or when tracking animations in accordance with body movements and gestures of people.  This seems to me most useful for performances by dancers or musicians, where a high level of interactivity between live video, animation, graphics, sound and performance is required. <br><br>  To get TouchDesigner * (.toe) files associated with this article, click <a href="">here</a> .  A <a href="https://www.derivative.ca/088/Downloads/">free copy of TouchDesigner</a> is also available for non-commercial use.  It has full functionality, but the maximum resolution can not exceed 1280 x 1280. <br>  Again, with the support of the Intel RealSense camera, TouchDesigner * becomes even more versatile and powerful. <br><br>  <b>Note.</b>  Like the <a href="https://habrahabr.ru/company/intel/blog/280810/">first part of this article</a> , the second part is intended for users already familiar with TouchDesigner * and its interface.  If you do not have experience with TouchDesigner * and you are going to gradually understand this article, I recommend that you first review the documentation available here: <a href="http://www.derivative.ca/wiki088/index.php%3Ftitle%3DLearning_TouchDesigner">Exploring TouchDesigner</a> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>Note.</b>  When using an Intel RealSense camera for optimal results, consider distance.  On <a href="http://www.intel.com/support/peripherals/smartdevices/rs-camera/sb/CS-035553.htm%3Fwapkw%3Dintel%2Brealsense%2B3d%2Bcamera">this Intel web page, you can see</a> the range of all camera models and recommendations for using cameras. <br><br><h1>  <font color="#0071c5">Historical information</font> </h1><br>  All data provided by Intel RealSense cameras is very useful for creating virtual reality and augmented reality.  Some attempts to do what is being done with the Intel RealSense camera were back in the 80s.  <a href="https://en.wikipedia.org/wiki/Wired_glove">The hand-tracking technology</a> was developed in the 1980s in the form of a glove that transmits data; the authors of this invention are Jason Lanier and Thomas G. Zimmerman.  In 1987, the company Nintendo released the first manipulator to control games in the form of gloves, connected by wire to the game console Nintendo. <br><br>  The devices, the development of which led to the creation of Intel RealSense cameras, were originally intended for animation on performances: motion capture technologies were used to convert a person‚Äôs performance into mathematical data, that is, into digital information.  The capture of the movement has been used since the 70s in research projects at various universities, as well as in the army for training.  One of the first animated films created with motion capture was the animated video of <a href="https://www.youtube.com/watch%3Fv%3DeedXpclrKCc">Sexy Robot</a> , created in 1985 by Robert Abel and his colleagues.  In the Sexy Robot video, several technologies were used to obtain the information with which the digital model of the robot was created and animated.  First, a robot model was created.  It was measured from all sides, the information describing it was transferred to a digital form: the RealSense camera achieves similar results when shooting objects.  To calculate the movement on the actor, points were drawn, the movement of which was correlated with the movement of the digital ‚Äúskeleton‚Äù: a vector animation was created, with which the digital model was moved.  The RealSense camera has infrared imaging and an infrared laser projector that allows you to receive data for digital models and motion tracking.  The tracking capabilities of the Intel RealSense camera are quite advanced: you can even track eye movements. <br><br><h1>  <font color="#0071c5">Intel RealSense cameras</font> </h1><br>  Currently, there are two models of Intel RealSense cameras.  They perform similar functions, but in some ways they differ: this is the Intel RealSense F200 camera for which the exercises in this article are intended, and the Intel RealSense R200 camera. <br><br>  The Intel RealSense R200 camera has important advantages due to its compact size.  It is designed for installation on a tripod or on the back of the tablet.  Thus, the camera lens is not aimed at the user, but at the world around it, but thanks to the improved shooting capabilities, the field of vision of the camera covers a wider area.  In addition, this camera has improved depth measurement capabilities.  This camera is very interesting to use for augmented reality projects, since it supports the scene perception function, which allows you to add virtual objects to the real world footage scene.  You can also overlay virtual information on the captured image live.  Unlike the F200, the R200 does not support finger, hand and face tracking.  TouchDesigner supports both Intel RealSense camera models: both the F200 and R200. <br><br><h1>  <font color="#0071c5">Intel RealSense cameras in TouchDesigner</font> </h1><br>  TouchDesigner fits perfectly with the Intel RealSense camera: it supports direct communication between facial expressions and hand movements and the software interface.  TouchDesigner can directly use this tracking and position data.  TouchDesigner can also use depth, color, and infrared data transmitted by an Intel RealSense camera.  Intel RealSense cameras are very light and compact, especially the R200 model, which can easily be placed next to the performers imperceptible to the audience. <br><br>  Adam Berg, a researcher at Leviathan, who is working on a project to use the Intel RealSense camera with TouchDesigner to create interactive installations, states: ‚ÄúThanks to its compact size and simple design, the camera is great for interactive solutions.  The camera is not striking, and infrastructure requirements are simplified, since the camera does not require an external power source.  Also, we liked the low latency when creating the image depth.  TouchDesigner is a great platform for work (from creating the initial prototype to developing the final version).  It has built-in support for five cameras, high-performance multimedia playback, and convenient shader capabilities.  In addition, of course, excellent support should also be noted. ‚Äù <br><br><h1>  <font color="#0071c5">Using the Intel RealSense Camera in TouchDesigner</font> </h1><br>  In the second part, we look at the CHOP node in TouchDesigner for the Intel RealSense camera. <br><br><h3>  <font color="#0071c5">RealSense CHOP node</font> </h3><br>  The RealSense CHOP node manages 3D tracking and position data.  The CHOP node contains two types of information.  (1) Position in the real world (measured in meters, but accuracy can be increased to units of millimeters) is used to convert along the x, y and z axes.  Turns around the x, y, and z axes in RealSense CHOP are displayed as Euler angles along the x, y, and z axes in degrees.  (2) The RealSense CHOP node also receives the pixels of the input image and converts them to normalized UV coordinates.  This is useful for image tracking. <br>  The RealSense CHOP node has two configurable parameters: finger / face tracking and token tracking. <br><ul><li>  In the Finger / Face Tracking section, you can select the tracked items.  You can limit the list of monitored items to only one aspect, and then, by connecting the Select CHOP node to the RealSense CHOP node, restrict the list again in order to track only the movement of the eyebrow or eye. </li><li>  Tracking markers allows you to upload an image and track this element, wherever it is. </li></ul><br><h1>  <font color="#0071c5">Using the RealSense CHOP node in TouchDesigner</font> </h1><br><h3>  <font color="#0071c5">Demo 1: Using Tracking</font> </h3><br>  This is a simple first demonstration of the RealSense CHOP node shows how it can be connected to other nodes and used to track and create motion.  Again, note that for these demonstrations, TouchDesigner's extremely superficial knowledge is sufficient.  If you do not have experience with TouchDesigner * and you are going to gradually understand this article, I recommend that you first review the documentation available here: Exploring TouchDesigner. <br>  1. Create the nodes we need and arrange them in a horizontal row in the following order: Geo COMP node, RealSense CHOP node, Select CHOP node, Math CHOP node, Lag CHOP node, Out CHOP node, and Trail CHOP node. <br>  2. Connect the RealSense CHOP node to the Select CHOP node, the Select CHOP node to the Math CHOP node, the Math CHOP node to the Lag CHOP node, the Lag CHOP node to the Out CHOP Node node, and the Out CHOP node to the Trail CHOP node. <br>  3. Open the <b>Setup</b> parameters page of the RealSense CHOP node and make sure that the Hands World Position parameter is set to On.  Displays the location of the tracked arm joints in space.  Values ‚Äã‚Äãare in meters relative to the camera. <br>  4. On the Select CHOP <b>Select</b> Parameters page, set the <b>Channel Names</b> parameter to hand_r / wrist: tx, selecting it from the available values ‚Äã‚Äãin the drop-down list to the right of the parameter. <br>  5. In the <b>Rename From</b> parameter, enter hand_r / wrist: tx, then in the <b>Rename To parameter parameter,</b> enter x. <br><br><img src="https://habrastorage.org/files/933/621/ea4/933621ea40c94daaaf2c1bbc41e53fd9.jpg"><br>  <i>Figure 1. Channel selection from the RealSense CHOP node occurs in the Select CHOP node</i> <br><br>  6. In the <b>Range / To Range</b> parameter of the Math CHOP node, enter 0, 100. For a reduced range of movements, enter a number less than 100. <br>  7. Select the Geometry COMP node and make sure it is on the <b>Xform</b> options <b>page</b> .  Click the "+" button in the lower right corner of the Out CHOP node to enable browsing.  Drag the X channel onto the Translate X parameter of the Geometry COMP node and select <b>Export CHOP</b> from the drop-down menu. <br><br><img src="https://habrastorage.org/files/526/473/2e2/5264732e2834414a85a32488feb09107.jpg"><br>  <i>Figure 2. Here you add the animation obtained from RealSense CHOP</i> <br><br>  To render the geometry, you need the Camera COMP node, the Material node (MAT) (I used the MAT Wireframe), the Light COMP node and the Render TOP node.  Add these nodes to render this project. <br>  8. In the Camera <b>Comp</b> node on the <b>Xform</b> parameters <b>page,</b> set the Translate Z parameter to 10. This will allow you to better see the movement of the created geometry, as the camera shifts back along the Z axis. <br>  9. Swipe your hand in front of the camera and see how the geometric shape will move in the Render TOP node. <br><br><img src="https://habrastorage.org/files/4a9/221/2e1/4a92212e14914166840849ff70f9ac17.jpg"><br>  <i>Figure 3. How nodes are connected to each other.</i>  <i>Thanks to the Trail CHOP node at the end you can see the animation in graphical form</i> <br><br><img src="https://habrastorage.org/files/3ce/d75/d7b/3ced75d7be454f5ba25124e4e9ec997d.jpg"><br>  <i>Figure 4. Geometry COMP node x transform value was exported from channel x to Out CHOP node, which was transferred further along the chain from Select CHOP node</i> <br><br><h1>  <font color="#0071c5">Demo 2. Tracking the RealSense CHOP marker</font> </h1><br>  In this demonstration, we will use the marker tracking feature in RealSense CHOP to show how to use the image for tracking.  You will create an image, you will have two copies of it: a printed copy and a digital copy.  They must match exactly.  In this case, you can initially have a digital file and print it, or, having an image on paper, scan it for a digital version. <br>  1. Add a RealSense CHOP node to the scene. <br>  2. On the <b>Setup</b> Options page of the RealSense CHOP node, select the Mode value of <b>Marker Tracking</b> . <br>  3. Create a Movie File resource in TOP. <br>  4. On the <b>Play</b> settings page of the TOP node, in the <b>File</b> section, select and download a digital image for which you also have a printed version. <br>  5. Drag the Movie File to TOP on the <b>Setup</b> Settings page of the RealSense CHOP node and to the <b>Marker Image TOP</b> cell at the bottom of the page. <br>  6. Create the Geometry COMP, Camera COMP, Light COMP, and Render TOP nodes. <br>  7. As was done earlier in step 7 in demonstration 1, export the tx channel from RealSense CHOP and drag it to the <b>Translate X</b> parameter of the Geometry COMP node. <br>  8. Create a Reorder TOP node and connect it to the Render TOP node.  On the <b>Output Alpha</b> node's Reorder settings page, select <b>One</b> from the drop-down list. <br>  9. Place the printed image of the digital file in front of the Intel RealSense camera and move it.  The camera should track the movement and issue it to the Render TOP node.  The numbers in RealSense CHOP will also change. <br><br><img src="https://habrastorage.org/files/1b4/cfc/c06/1b4cfcc067b44db7a7404bf5f6398655.jpg"><br>  <i>Figure 5. This is a full mock marker demonstration layout.</i> <br><br><img src="https://habrastorage.org/files/2ad/66b/93a/2ad66b93aeb34d3199d031f5986ef8fb.jpg"><br>  <i>Figure 6. On the Geo COMP node settings page, the <b>tx</b> channel from the RealSense CHOP node is dragged to the <b>Translate x</b> parameter</i> <br><br><h1>  <font color="#0071c5">TouchDesigner eye tracking with RealSense CHOP</font> </h1><br>  On the TouchDesigner program palette in the <b>RealSense</b> section, <b>there</b> is an <b>eyeTracking</b> template that can be used to track the movement of the user's eyes.  This template uses the finger / face tracking of a RealSense CHOP node;  RealSense TOP node must be set to <b>Color</b> .  In the template, the green rectangles of WireFrame move in accordance with the movement of the human eye and are superimposed on the color image of the person in RealSense TOP.  Instead of open green rectangles, you can use any other geometric shapes or particles.  This is a very convenient template.  Here is an image with a template. <br><br><img src="https://habrastorage.org/files/a29/ebe/4bd/a29ebe4bda1f4015bcc8569caa3e6cd2.jpg"><br>  <i>Figure 7. Notice that the eyes are tracked even through the glasses.</i> <br><br><h1>  <font color="#0071c5">Demonstration 3, part 1. A simple way to configure full-domed rendering or virtual reality</font> </h1><br>  In this demonstration, we take a file and show how to present it in full-domed rendering and in 360-degree virtual reality.  I have already prepared such a file for download.  This is the file <i>chopRealSense_FullDome_VR_render.toe</i> . <br><br>  <b>Brief description of the process of creating this file</b> <br>  In this file I wanted to place geometric shapes (sphere, torus, cylinders and rectangles) in the scene.  So I created several SOP nodes for these various geometric shapes.  Each SOP node was attached to the Transform SOP node to move (transform) geometric shapes in different parts of the scene.  All SOP nodes are connected to one Merge SOP node.  The Merge SOP node is served by the Geometry COMP node. <br><br><img src="https://habrastorage.org/files/205/762/a96/205762a96d034ab693cca200d7f6bb78.jpg"><br>  <i>Figure 8. This is the first step in marking up geometric shapes placed in the scene in a downloadable file.</i> <br><br>  Then I created the Grid SOP node and the SOP ‚Äì DAT node.  The SOP ‚Äì DAT node was used to obtain an instance of the Geometry COMP so that more geometric shapes could be added to the scene.  I also created the Constant MAT node, selected green and turned on the <b>WireFrame</b> parameter on the <b>Common</b> page. <br><br><img src="https://habrastorage.org/files/8df/766/174/8df76617482746e9b61d63d434314f31.jpg"><br>  <i>Figure 9. SOP ‚Äì DAT node was created using the Grid SOP node</i> <br><br>  Then I created the RealSense CHOP node and connected it to the Select CHOP node, in which I selected the hand_r / wrist: tx channel for tracking and renamed it to x.  I connected Select CHOP with the Math CHOP node so that the range could be changed, and connected the Math CHOP with the Null CHOP node.  It is recommended to always terminate the chain with a Null or Out node so that it is more convenient to insert new filters into the chain.  Then I exported the x Channel from the Null CHOP node to the <b>Scale X</b> parameter of the Geometry COMP node.  This provides control over all movements of geometric shapes along the x-axis in the scene, when I spend my right hand in front of the Intel RealSense camera. <br><br><img src="https://habrastorage.org/files/089/735/e1a/089735e1ace04510abf0148c551c3558.jpg"><br>  <i>Figure 10. Tracking data from the RealSense CHOP node is used to create real-time animation and movement of geometric shapes along the x axis</i> <br><br>  To create a 180 ¬∞ full dome rendering from this file <br>  1. Create Render TOP, Camera COMP and Light COMP nodes. <br>  2. On the Render TOP node's Render TOP settings page, select <b>Cube Map</b> in the Render Mode drop-down menu. <br>  3. On the Common parameters page of the Render TOP node, set the Resolution parameter to a resolution with a 1: 1 aspect ratio, for example 4096 to 4096, to obtain 4K resolution. <br>  4. Create a Projection TOP node and connect the Render TOP node with it. <br>  5. On the Projection TOP Projection Options page, select <b>Fish-Eye</b> from the Output drop-down menu. <br>  6. (This is optional, in which case the file will have a black background.) Create a Reorder TOP node and on the Reorder settings page, in the right-click Output Alpha drop-down menu, select <b>One</b> . <br>  7. Now everything is ready either to directly perform the animation, or to export the movie file.  See instructions <a href="https://habrahabr.ru/company/intel/blog/280810/">in the first part of this article</a> .  You create a circular domed animation of the Fisheye type.  It will be a circle with a square. <br><br>  To use an alternative method, go back to step 2, and instead of choosing Cube Map from the Render Mode drop-down menu, select <b>Fish-Eye (180)</b> .  Now go to step 3, and also, if you wish, go to step 6. Now the animation is ready for launch or for export. <br><br>  <b>To create a 360-degree virtual reality from this file</b> <br>  1. Create Render TOP, Camera COMP and Light COMP nodes. <br>  2. On the Render TOP node's Render TOP settings page, select <b>Cube Map</b> in the <b>Render Mode</b> drop-down menu. <br>  3. On the <b>Common</b> parameters page of the Render TOP node, set the <b>Resolution</b> parameter to a resolution with a 1: 1 aspect ratio, for example 4096 to 4096, to obtain 4K resolution. <br>  4. Create a Projection TOP node and connect the Render TOP node with it. <br>  5. On the Projection TOP <b>Projection</b> Options page, select <b>Equirectangular</b> in the <b>Output</b> drop-down menu.  This will automatically set the aspect ratio to 2: 1. <br>  6. (This is optional, in which case the file will have a black background.) Create a Reorder TOP node, then on the Reorder settings page, in the right-click Output Alpha drop-down menu, select <b>One</b> . <br>  7. Now everything is ready either to directly perform the animation, or to export the movie file.  See instructions <a href="https://habrahabr.ru/company/intel/blog/280810/">in the first part of this article</a> .  When exporting a movie, you create a rectangular animation with a 2: 1 aspect ratio for viewing with virtual reality glasses. <br><br><img src="https://habrastorage.org/files/ba3/daf/944/ba3daf9449a448778e9f550de947a407.jpg"><br>  <i>Figure 11. Long orange Tube SOP nodes are added to the file.</i>  <i>You can add your own geometry to this file.</i> <br><br><h1>  <font color="#0071c5">Output to Oculus Rift * from TouchDesigner when using an Intel RealSense camera</font> </h1><br>  TouchDesigner has created several download templates.  They show how to set up your Oculus Rift in TouchDesigner.  One of these templates - <i>OculusRiftSimple.toe</i> - can be found in the archive.  To view the Oculus Rift, of course, the computer must be connected to the Oculus Rift.  Without Oculus Rift, you can create a file, view the images in the LeftEye Render TOP and RightEye Render TOP nodes and display them in the background of the scene.  I added Oculus Rift support to the file used in demo 3. Thus, the Intel RealSense camera animates the image I see in the Oculus Rift. <br><br><img src="https://habrastorage.org/files/a33/89b/dff/a3389bdff8734abd888e4f5687a3e848.jpg"><br>  <i>Figure 12. Here, the left eye and the right eye are displayed in the background.</i>  <i>Much of the animation for this scene is controlled by tracking from the Intel RealSense camera's CHOP node.</i>  <i>The file used to obtain this image can be downloaded by clicking on the button in the upper right corner of this article, chopRealSense_FullDome_VRRender_FinalArticle2_OculusRiftSetUp.toe</i> </div><p>Source: <a href="https://habr.com/ru/post/281318/">https://habr.com/ru/post/281318/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../281306/index.html">PHP Digest number 83 - interesting news, materials and tools (March 27 - April 10, 2016)</a></li>
<li><a href="../281308/index.html">The digest of interesting materials from the world of web development and IT for the last week ‚Ññ206 (April 4 - 10, 2016)</a></li>
<li><a href="../281312/index.html">Android Cuvettes, Part 3: SDK and RxJava (Final)</a></li>
<li><a href="../281314/index.html">Easla.com review</a></li>
<li><a href="../281316/index.html">Boolean expressions in C / C ++. How are the professionals mistaken</a></li>
<li><a href="../281320/index.html">Perspectives of the go language for the programmer</a></li>
<li><a href="../281322/index.html">Gaming simulators for the development of BrainApps cognitive brain functions with a cloud backend</a></li>
<li><a href="../281324/index.html">"Looming Clouds" will make you rebuild your IT platform</a></li>
<li><a href="../281330/index.html">PHP and Temporal Coupling</a></li>
<li><a href="../281332/index.html">Plugins in your pocket or penknife in the program</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>