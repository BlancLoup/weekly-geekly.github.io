<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Hadoop From Scratch</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This article will serve as a practical guide to building, initial configuration, and testing the health of Hadoop beginners administrators. We will an...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Hadoop From Scratch</h1><div class="post__text post__text-html js-mediator-article"><img width="300" align="right" src="https://habrastorage.org/files/bb9/7b3/b01/bb97b3b0113a4ab98d9ee6a9b53f54d9.png">  This article will serve as a practical guide to building, initial configuration, and testing the health of Hadoop beginners administrators.  We will analyze how to build Hadoop from source, configure, run and verify that everything works as it should.  In the article you will not find the theoretical part.  If you have not come across Hadoop before, you don‚Äôt know what parts it consists of and how they interact, here are a couple of useful links to official documentation: <br><br>  <a href="http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html">hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html</a> <br>  <a href="http://hadoop.apache.org/docs/r2.7.3/hadoop-yarn/hadoop-yarn-site/YARN.html">hadoop.apache.org/docs/r2.7.3/hadoop-yarn/hadoop-yarn-site/YARN.html</a> <br><br>  <b>Why not just use the finished distribution?</b> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      - Training.  Similar articles often begin with recommendations to download a virtual machine image with a Cloudera or HortonWorks distribution.  As a rule, a distribution kit is a complex ecosystem with a lot of components.  It will be difficult for a beginner to figure out where that is and how it all interacts.  Starting from scratch, we slightly reduce the threshold of entry, since we can consider the components one by one. <br><br>  - Functional tests and benchmarks.  There is a small lag between the release of a new version of the product, and the moment when it appears in the distribution.  If you need to test the new features of the version that has just appeared, you will not be able to use a ready-made distribution.  It will also be difficult to compare the performance of two versions of the same software, since in the ready-made distributions there is usually no opportunity to update the version of any one component, leaving everything else as it is. <br><br>  - Just for fun. <br><a name="habracut"></a><br>  <b>Why collect from sources?</b>  <b>After all, Hadoop binary builds are also available.</b> <br><br>  Part of the Hadoop code is written in C / C ++.  I don‚Äôt know on which system the development team does the builds, but the C-libraries that come with the Hadoop binary builds depend on the libc version, which is not in RHEL or Debian / Ubuntu.  The inoperability of the Hadoop C-libraries is generally not critical, but some features will not work without them. <br><br>  <b>Why re-describe everything that is already in the official documentation?</b> <br><br>  The article aims to save time.  The official documentation does not contain quickstart-instructions - do it and it will work.  If for one reason or another you need to collect the ‚Äúvanilla‚Äù Hadoop, but you don‚Äôt have time to do it through trial and error, you went to the address. <br><br><h3>  Assembly </h3><br>  For the assembly we will use CentOS 7. If you believe loudera, most clusters work on RHEL and derivatives (CentOS, Oracle Linux).  The 7th version is the most suitable, since its repositories already have the protobuf library of the required version.  If you want to use CentOS 6, you will need to build protobuf yourself. <br><br>  We will carry out assembly and other experiments with root privileges (in order not to complicate the article). <br><br>  Somewhere 95% of Hadoop code is written in Java.  For the assembly, we need Oracle JDK and Maven. <br><br>  Download the latest version of the JDK from the Oracle site and unzip it to / opt.  Also add the JAVA_HOME variable (used by Hadoop) and add / opt / java / bin to the PATH for the root user (for convenience): <br><br><pre><code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> ~ wget --no-check-certificate --no-cookies --header <span class="hljs-string"><span class="hljs-string">"Cookie: oraclelicense=accept-securebackup-cookie"</span></span> http://download.oracle.com/otn-pub/java/jdk/8u112-b15/jdk-8u112-linux-x64.tar.gz tar xvf ~/jdk-8u112-linux-x64.tar.gz mv ~/jdk1.8.0_112 /opt/java <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"PATH=\"/opt/java/bin:\$PATH\""</span></span> &gt;&gt; ~/.bashrc <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"export JAVA_HOME=\"/opt/java\""</span></span> &gt;&gt; ~/.bashrc</code> </pre> <br>  Install Maven.  It will be needed only at the assembly stage.  Therefore, we will install it in our home (after the end of the assembly, all files that remain in the home can be deleted). <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> ~ wget http://apache.rediris.es/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar xvf ~/apache-maven-3.3.9-bin.tar.gz mv ~/apache-maven-3.3.9 ~/maven <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"PATH=\"/root/maven/bin:\$PATH\""</span></span> &gt;&gt; ~/.bashrc <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> ~/.bashrc</code> </pre> <br>  Somewhere 4-5% of Hadoop code is written in C / C ++.  Install the compiler and other packages necessary for the assembly: <br><br><pre> <code class="bash hljs"> yum -y install gcc gcc-c++ autoconf automake libtool cmake</code> </pre> <br>  We will also need some third-party libraries: <br><br><pre> <code class="bash hljs">yum -y install zlib-devel openssl openssl-devel snappy snappy-devel bzip2 bzip2-devel protobuf protobuf-devel</code> </pre> <br>  The system is ready.  Download, build and install Hadoop in / opt: <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> ~ wget http://apache.rediris.es/hadoop/common/hadoop-2.7.3/hadoop-2.7.3-src.tar.gz tar -xvf ~/hadoop-2.7.3-src.tar.gz mv ~/hadoop-2.7.3-src ~/hadoop-src <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> ~/hadoop-src mvn package -Pdist,native -DskipTests -Dtar tar -C/opt -xvf ~/hadoop-src/hadoop-dist/target/hadoop-2.7.3.tar.gz mv /opt/hadoop-* /opt/hadoop <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"PATH=\"/opt/hadoop/bin:\$PATH\""</span></span> &gt;&gt; ~/.bashrc <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> ~/.bashrc</code> </pre> <br><h3>  Primary configuration </h3><br>  Hadoop has about a thousand parameters.  Fortunately, to run Hadoop and take some first steps in mastering, around 40 is enough, leaving the rest as default. <br><br>  Let's get started  If you remember, we installed Hadoop in / opt / hadoop.  All configuration files are in / opt / hadoop / etc / hadoop.  In total, you will need to edit 6 configuration files.  All configs below are in the form of commands.  In order for those who are trying to build their Hadoop on this article, could simply copy the commands to the console. <br><br>  First, we set the JAVA_HOME environment variable in the hadoop-env.sh and yarn-env.sh files.  So we will let all components know where java is installed, which they should use. <br><br><pre> <code class="bash hljs">sed -i <span class="hljs-string"><span class="hljs-string">'1iJAVA_HOME=/opt/java'</span></span> /opt/hadoop/etc/hadoop/hadoop-env.sh sed -i <span class="hljs-string"><span class="hljs-string">'1iJAVA_HOME=/opt/java'</span></span> /opt/hadoop/etc/hadoop/yarn-env.sh</code> </pre> <br>  Configure the URL for HDFS in the core-site.xml file.  It consists of the hdfs: // prefix, the name of the host on which the NameNode is running, and the port.  If you do not do this, Hadoop will not use the distributed file system, but will work from a local file system on your computer (default URL: file: ///). <br><br><pre> <code class="bash hljs">cat &lt;&lt; EOF &gt; /opt/hadoop/etc/hadoop/core-site.xml &lt;configuration&gt; &lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://localhost:8020&lt;/value&gt;&lt;/property&gt; &lt;/configuration&gt; EOF</code> </pre> <br>  In the hdfs-site.xml file, we configure 4 parameters.  The number of replicas is set to 1, since our ‚Äúcluster‚Äù consists of only one node.  We also configure the directories where they will store the NameNode, DataNode and SecondaryNameNode data. <br><br><pre> <code class="bash hljs">cat &lt;&lt; EOF &gt; /opt/hadoop/etc/hadoop/hdfs-site.xml &lt;configuration&gt; &lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;1&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;&lt;value&gt;/data/dfs/nn&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;&lt;value&gt;/data/dfs/dn&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;&lt;value&gt;/data/dfs/snn&lt;/value&gt;&lt;/property&gt; &lt;/configuration&gt; EOF</code> </pre> <br>  We have finished setting up HDFS.  It would be possible to run the NameNode and DataNode, and work with the file system.  But let's leave it for the next section.  We turn to the configuration YARN. <br><br><pre> <code class="bash hljs">cat &lt;&lt; EOF &gt; /opt/hadoop/etc/hadoop/yarn-site.xml &lt;configuration&gt; &lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&lt;value&gt;localhost&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;&lt;value&gt;4096&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;&lt;value&gt;4&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;&lt;value&gt;1024&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt;&lt;value&gt;1&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;&lt;value&gt;<span class="hljs-literal"><span class="hljs-literal">false</span></span>&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt;&lt;value&gt;/data/yarn&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt;&lt;value&gt;/data/yarn/<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;&lt;value&gt;<span class="hljs-literal"><span class="hljs-literal">true</span></span>&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/name&gt;&lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;&lt;/property&gt; &lt;/configuration&gt; EOF</code> </pre> <br>  There are quite a few parameters.  Let's go through them in order. <br><br>  The yarn.resourcemanager.hostname parameter indicates which host the ResourceManager service is running on. <br><br>  The parameters yarn.nodemanager.resource.memory-mb and yarn.nodemanager.resource.cpu-vcores are perhaps the most important.  In them, we tell the cluster how much memory and CPU cores each node can use in total to run containers. <br><br>  The parameters yarn.scheduler.maximum-allocation-mb and yarn.scheduler.maximum-allocation-vcores indicate how much memory and cores can be allocated to a single container.  It is easy to see that with this configuration in our ‚Äúcluster‚Äù consisting of one node, 4 containers can be launched simultaneously (with 1GB of memory each). <br><br>  The parameter yarn.nodemanager.vmem-check-enabled set to false disables checking the amount of virtual memory used.  As can be seen from the previous paragraph, not much memory is available for each container, and with such a configuration, any application will certainly increase the limit of available virtual memory. <br><br>  The yarn.nodemanager.local-dirs parameter specifies where the container temporary data will be stored (jar with application bytecode, configuration files, temporary data generated during execution, ...) <br><br>  The yarn.nodemanager.log-dirs parameter specifies where the logs of each task will be stored locally. <br><br>  The yarn.log-aggregation-enable parameter specifies to keep logs in HDFS.  After the application is completed, its logs from yarn.nodemanager.log-dirs of each node will be moved to HDFS (by default, to the / tmp / logs directory). <br><br>  The yarn.nodemanager.aux-services and yarn.nodemanager.aux-services.mapreduce_shuffle.class parameters specify the third-party shuffle service for the MapReduce framework. <br><br>  That's probably all for YARN.  I will also give the configuration for MapReduce (one of the possible frameworks for distributed computing).  Although it has recently lost its popularity due to the advent of Spark, it is still used a lot. <br><br><pre> <code class="bash hljs">cat &lt;&lt; EOF &gt; /opt/hadoop/etc/hadoop/mapred-site.xml &lt;configuration&gt; &lt;property&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;&lt;value&gt;localhost:10020&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;&lt;value&gt;localhost:19888&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;mapreduce.job.reduce.slowstart.completedmaps&lt;/name&gt;&lt;value&gt;0.8&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;yarn.app.mapreduce.am.resource.cpu-vcores&lt;/name&gt;&lt;value&gt;1&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt;&lt;value&gt;1024&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;yarn.app.mapreduce.am.command-opts&lt;/name&gt;&lt;value&gt;-Djava.net.preferIPv4Stack=<span class="hljs-literal"><span class="hljs-literal">true</span></span> -Xmx768m&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;mapreduce.map.cpu.vcores&lt;/name&gt;&lt;value&gt;1&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;&lt;value&gt;1024&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;mapreduce.map.java.opts&lt;/name&gt;&lt;value&gt;-Djava.net.preferIPv4Stack=<span class="hljs-literal"><span class="hljs-literal">true</span></span> -Xmx768m&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;mapreduce.reduce.cpu.vcores&lt;/name&gt;&lt;value&gt;1&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;&lt;value&gt;1024&lt;/value&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt;&lt;value&gt;-Djava.net.preferIPv4Stack=<span class="hljs-literal"><span class="hljs-literal">true</span></span> -Xmx768m&lt;/value&gt;&lt;/property&gt; &lt;/configuration&gt; EOF</code> </pre> <br>  The mapreduce.framework.name parameter indicates that we will run MapReduce tasks in YARN (the default value of local is used only for debugging ‚Äî all tasks are run in the same jvm on the same machine). <br><br>  The mapreduce.jobhistory.address and mapreduce.jobhistory.webapp.address parameters specify the name of the node on which the JobHistory service will run. <br><br>  The mapreduce.job.reduce.slowstart.completedmaps parameter instructs the reduce phase to occur no earlier than 80% of the map phase. <br><br>  The remaining parameters set the maximum possible values ‚Äã‚Äãof memory and CPU cores and jvm heap for mappers, reducers, and application masters.  As you can see, they should not exceed the corresponding values ‚Äã‚Äãfor YARN containers, which we defined in yarn-site.xml.  The values ‚Äã‚Äãof jvm heap are usually set to 75% of the parameters * .memory.mb. <br><br><h3>  Start </h3><br>  Create a / data directory in which HDFS data will be stored, as well as temporary files of YARN containers. <br><br><pre> <code class="bash hljs">mkdir /data</code> </pre> <br>  Format HDFS <br><br><pre> <code class="bash hljs">hadoop namenode -format</code> </pre> <br>  And finally, we will launch all services of our ‚Äúcluster‚Äù: <br><br><pre> <code class="bash hljs">/opt/hadoop/sbin/hadoop-daemon.sh start namenode /opt/hadoop/sbin/hadoop-daemon.sh start datanode /opt/hadoop/sbin/yarn-daemon.sh start resourcemanager /opt/hadoop/sbin/yarn-daemon.sh start nodemanager /opt/hadoop/sbin/mr-jobhistory-daemon.sh start historyserver</code> </pre> <br>  If everything went well (you can check the error messages in the logs in / opt / hadoop / logs), Hadoop is deployed and ready to go ... <br><br><h3>  Health check </h3><br>  Look at the hadoop directory structure: <br><br><pre> <code class="bash hljs">/opt/hadoop/ ‚îú‚îÄ‚îÄ bin ‚îú‚îÄ‚îÄ etc ‚îÇ ‚îî‚îÄ‚îÄ hadoop ‚îú‚îÄ‚îÄ include ‚îú‚îÄ‚îÄ lib ‚îÇ ‚îî‚îÄ‚îÄ native ‚îú‚îÄ‚îÄ libexec ‚îú‚îÄ‚îÄ logs ‚îú‚îÄ‚îÄ sbin ‚îî‚îÄ‚îÄ share ‚îú‚îÄ‚îÄ doc ‚îÇ ‚îî‚îÄ‚îÄ hadoop ‚îî‚îÄ‚îÄ hadoop ‚îú‚îÄ‚îÄ common ‚îú‚îÄ‚îÄ hdfs ‚îú‚îÄ‚îÄ httpfs ‚îú‚îÄ‚îÄ kms ‚îú‚îÄ‚îÄ mapreduce ‚îú‚îÄ‚îÄ tools ‚îî‚îÄ‚îÄ yarn</code> </pre> <br>  Hadoop itself (executable java-bytecode) is located in the share directory and is divided into components (hdfs, yarn, mapreduce, etc ...).  The lib directory contains libraries written in C. <br><br>  The assignment of other directories is intuitively clear: bin - command line utilities for working with Hadoop, sbin - startup scripts, etc - configs, logs - logs.  We are primarily interested in two utilities from the bin directory: hdfs and yarn. <br><br>  If you remember, we have already formatted HDFS and started all the necessary processes.  Let's see what we have in HDFS: <br><br><pre> <code class="bash hljs">hdfs dfs -ls -R / drwxrwx--- - root supergroup 0 2017-01-05 10:07 /tmp drwxrwx--- - root supergroup 0 2017-01-05 10:07 /tmp/hadoop-yarn drwxrwx--- - root supergroup 0 2017-01-05 10:07 /tmp/hadoop-yarn/staging drwxrwx--- - root supergroup 0 2017-01-05 10:07 /tmp/hadoop-yarn/staging/<span class="hljs-built_in"><span class="hljs-built_in">history</span></span> drwxrwx--- - root supergroup 0 2017-01-05 10:07 /tmp/hadoop-yarn/staging/<span class="hljs-built_in"><span class="hljs-built_in">history</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">done</span></span> drwxrwxrwt - root supergroup 0 2017-01-05 10:07 /tmp/hadoop-yarn/staging/<span class="hljs-built_in"><span class="hljs-built_in">history</span></span>/done_intermediate</code> </pre> <br>  Although we obviously did not create this directory structure, it was created by the JobHistory service (the last running daemon: mr-jobhistory-daemon.sh start historyserver). <br><br>  Let's see what is in the / data directory: <br><br><pre> <code class="bash hljs">/data/ ‚îú‚îÄ‚îÄ dfs ‚îÇ ‚îú‚îÄ‚îÄ dn ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ current ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ BP-1600342399-192.168.122.70-1483626613224 ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ current ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ finalized ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ rbw ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ VERSION ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ scanner.cursor ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ tmp ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ VERSION ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ in_use.lock ‚îÇ ‚îî‚îÄ‚îÄ nn ‚îÇ ‚îú‚îÄ‚îÄ current ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ edits_inprogress_0000000000000000001 ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ fsimage_0000000000000000000 ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ fsimage_0000000000000000000.md5 ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ seen_txid ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ VERSION ‚îÇ ‚îî‚îÄ‚îÄ in_use.lock ‚îî‚îÄ‚îÄ yarn ‚îú‚îÄ‚îÄ filecache ‚îú‚îÄ‚îÄ <span class="hljs-built_in"><span class="hljs-built_in">log</span></span> ‚îú‚îÄ‚îÄ nmPrivate ‚îî‚îÄ‚îÄ usercache</code> </pre> <br>  As you can see, in / data / dfs / nn the NameNode created the fsimage file and the first edit file.  In / data / dfs / dn DataNode created a directory for storing data blocks, but the data itself is not yet. <br><br>  Copy some file from a local file system to HDFS: <br><br><pre> <code class="bash hljs">hdfs dfs -put /var/<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>/messages /tmp/ hdfs dfs -ls /tmp/messages -rw-r--r-- 1 root supergroup 375974 2017-01-05 09:33 /tmp/messages</code> </pre> <br>  Look again at the contents of / data <br><br><pre> <code class="bash hljs">/data/dfs/dn ‚îú‚îÄ‚îÄ current ‚îÇ ‚îú‚îÄ‚îÄ BP-1600342399-192.168.122.70-1483626613224 ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ current ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ finalized ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ subdir0 ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ subdir0 ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ blk_1073741825 ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ blk_1073741825_1001.meta ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ rbw ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ VERSION ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ scanner.cursor ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ tmp ‚îÇ ‚îî‚îÄ‚îÄ VERSION ‚îî‚îÄ‚îÄ in_use.lock</code> </pre> <br>  Hooray!!!  The first block and its checksum appeared. <br><br>  Run some application to make sure that YARN works as expected.  For example, pi from hadoop-mapreduce-examples.jar package: <br><br><pre> <code class="bash hljs">yarn jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar pi 3 100000 ‚Ä¶ Job Finished <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 37.837 seconds Estimated value of Pi is 3.14168000000000000000</code> </pre> <br>  If you look at the contents of / data / yarn during the execution of the application, you can learn a lot of interesting things about how the YARN applications are executed: <br><br><pre> <code class="hljs ruby">/data/yarn/ ‚îú‚îÄ‚îÄ filecache ‚îú‚îÄ‚îÄ log ‚îÇ ‚îî‚îÄ‚îÄ application_1483628783579_0001 ‚îÇ ‚îú‚îÄ‚îÄ container_1483628783579_0001_01_000001 ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ stderr ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ stdout ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ syslog ‚îÇ ‚îú‚îÄ‚îÄ container_1483628783579_0001_01_000002 ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ stderr ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ stdout ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ syslog ‚îÇ ‚îú‚îÄ‚îÄ container_1483628783579_0001_01_000003 ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ stderr ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ stdout ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ syslog ‚îÇ ‚îî‚îÄ‚îÄ container_1483628783579_0001_01_000004 ‚îÇ ‚îú‚îÄ‚îÄ stderr ‚îÇ ‚îú‚îÄ‚îÄ stdout ‚îÇ ‚îî‚îÄ‚îÄ syslog ‚îú‚îÄ‚îÄ nmPrivate ‚îÇ ‚îî‚îÄ‚îÄ application_1483628783579_0001 ‚îÇ ‚îú‚îÄ‚îÄ container_1483628783579_0001_01_000001 ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ container_1483628783579_0001_01_000001.pid ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ container_1483628783579_0001_01_000001.tokens ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ launch_container.sh ‚îÇ ‚îú‚îÄ‚îÄ container_1483628783579_0001_01_000002 ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ container_1483628783579_0001_01_000002.pid ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ container_1483628783579_0001_01_000002.tokens ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ launch_container.sh ‚îÇ ‚îú‚îÄ‚îÄ container_1483628783579_0001_01_000003 ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ container_1483628783579_0001_01_000003.pid ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ container_1483628783579_0001_01_000003.tokens ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ launch_container.sh ‚îÇ ‚îî‚îÄ‚îÄ container_1483628783579_0001_01_000004 ‚îÇ ‚îú‚îÄ‚îÄ container_1483628783579_0001_01_000004.pid ‚îÇ ‚îú‚îÄ‚îÄ container_1483628783579_0001_01_000004.tokens ‚îÇ ‚îî‚îÄ‚îÄ launch_container.sh ‚îî‚îÄ‚îÄ usercache ‚îî‚îÄ‚îÄ root ‚îú‚îÄ‚îÄ appcache ‚îÇ ‚îî‚îÄ‚îÄ application_1483628783579_0001 ‚îÇ ‚îú‚îÄ‚îÄ container_1483628783579_0001_01_000001 ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ container_tokens ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ default_container_executor_session.sh ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ default_container_executor.sh ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ job.jar -&gt; <span class="hljs-regexp"><span class="hljs-regexp">/data/yarn</span></span><span class="hljs-regexp"><span class="hljs-regexp">/usercache/root</span></span><span class="hljs-regexp"><span class="hljs-regexp">/appcache/application</span></span>_1483628783579_0001/filecache/<span class="hljs-number"><span class="hljs-number">11</span></span>/job.jar ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ jobSubmitDir ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ job.split -&gt; <span class="hljs-regexp"><span class="hljs-regexp">/data/yarn</span></span><span class="hljs-regexp"><span class="hljs-regexp">/usercache/root</span></span><span class="hljs-regexp"><span class="hljs-regexp">/appcache/application</span></span>_1483628783579_0001/filecache/<span class="hljs-number"><span class="hljs-number">12</span></span>/job.split ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ job.splitmetainfo -&gt; <span class="hljs-regexp"><span class="hljs-regexp">/data/yarn</span></span><span class="hljs-regexp"><span class="hljs-regexp">/usercache/root</span></span><span class="hljs-regexp"><span class="hljs-regexp">/appcache/application</span></span>_1483628783579_0001/filecache/<span class="hljs-number"><span class="hljs-number">10</span></span>/job.splitmetainfo ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ job.xml -&gt; <span class="hljs-regexp"><span class="hljs-regexp">/data/yarn</span></span><span class="hljs-regexp"><span class="hljs-regexp">/usercache/root</span></span><span class="hljs-regexp"><span class="hljs-regexp">/appcache/application</span></span>_1483628783579_0001/filecache/<span class="hljs-number"><span class="hljs-number">13</span></span>/job.xml ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ launch_container.sh ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ tmp ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ Jetty_0_0_0_0_37883_mapreduce___<span class="hljs-number"><span class="hljs-number">_</span></span>.rposvq ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ webapp ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ webapps ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ mapreduce ‚îÇ ‚îú‚îÄ‚îÄ container_1483628783579_0001_01_000002 ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ container_tokens ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ default_container_executor_session.sh ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ default_container_executor.sh ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ job.jar -&gt; <span class="hljs-regexp"><span class="hljs-regexp">/data/yarn</span></span><span class="hljs-regexp"><span class="hljs-regexp">/usercache/root</span></span><span class="hljs-regexp"><span class="hljs-regexp">/appcache/application</span></span>_1483628783579_0001/filecache/<span class="hljs-number"><span class="hljs-number">11</span></span>/job.jar ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ job.xml ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ launch_container.sh ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ tmp ‚îÇ ‚îú‚îÄ‚îÄ container_1483628783579_0001_01_000003 ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ container_tokens ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ default_container_executor_session.sh ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ default_container_executor.sh ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ job.jar -&gt; <span class="hljs-regexp"><span class="hljs-regexp">/data/yarn</span></span><span class="hljs-regexp"><span class="hljs-regexp">/usercache/root</span></span><span class="hljs-regexp"><span class="hljs-regexp">/appcache/application</span></span>_1483628783579_0001/filecache/<span class="hljs-number"><span class="hljs-number">11</span></span>/job.jar ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ job.xml ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ launch_container.sh ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ tmp ‚îÇ ‚îú‚îÄ‚îÄ container_1483628783579_0001_01_000004 ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ container_tokens ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ default_container_executor_session.sh ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ default_container_executor.sh ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ job.jar -&gt; <span class="hljs-regexp"><span class="hljs-regexp">/data/yarn</span></span><span class="hljs-regexp"><span class="hljs-regexp">/usercache/root</span></span><span class="hljs-regexp"><span class="hljs-regexp">/appcache/application</span></span>_1483628783579_0001/filecache/<span class="hljs-number"><span class="hljs-number">11</span></span>/job.jar ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ job.xml ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ launch_container.sh ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ tmp ‚îÇ ‚îú‚îÄ‚îÄ filecache ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ <span class="hljs-number"><span class="hljs-number">10</span></span> ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ job.splitmetainfo ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ <span class="hljs-number"><span class="hljs-number">11</span></span> ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ job.jar ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ job.jar ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ <span class="hljs-number"><span class="hljs-number">12</span></span> ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ job.split ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ <span class="hljs-number"><span class="hljs-number">13</span></span> ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ job.xml ‚îÇ ‚îî‚îÄ‚îÄ work ‚îî‚îÄ‚îÄ filecache <span class="hljs-number"><span class="hljs-number">42</span></span> directories, <span class="hljs-number"><span class="hljs-number">50</span></span> files</code> </pre><br>  In particular, we see that the logs are written to / data / yarn / log (the yarn.nodemanager.log-dirs parameter from yarn-site.xml). <br><br>  At the end of the application / data / yarn comes to its original appearance: <br><br><pre> <code class="bash hljs">/data/yarn/ ‚îú‚îÄ‚îÄ filecache ‚îú‚îÄ‚îÄ <span class="hljs-built_in"><span class="hljs-built_in">log</span></span> ‚îú‚îÄ‚îÄ nmPrivate ‚îî‚îÄ‚îÄ usercache ‚îî‚îÄ‚îÄ root ‚îú‚îÄ‚îÄ appcache ‚îî‚îÄ‚îÄ filecache</code> </pre> <br>  If we look again at the contents of HDFS, we see that log aggregation is working (the logs of the just-executed application were moved from the local FS / data / yarn / log to HDFS / tmp / logs). <br><br>  We also see that the JobHistory service has saved information about our application in / tmp / hadoop-yarn / staging / history / done. <br><br><pre> <code class="bash hljs">hdfs dfs -ls -R / drwxrwx--- - root supergroup 0 2017-01-05 10:12 /tmp drwxrwx--- - root supergroup 0 2017-01-05 10:07 /tmp/hadoop-yarn drwxrwx--- - root supergroup 0 2017-01-05 10:12 /tmp/hadoop-yarn/staging drwxrwx--- - root supergroup 0 2017-01-05 10:07 /tmp/hadoop-yarn/staging/<span class="hljs-built_in"><span class="hljs-built_in">history</span></span> drwxrwx--- - root supergroup 0 2017-01-05 10:13 /tmp/hadoop-yarn/staging/<span class="hljs-built_in"><span class="hljs-built_in">history</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">done</span></span> drwxrwx--- - root supergroup 0 2017-01-05 10:13 /tmp/hadoop-yarn/staging/<span class="hljs-built_in"><span class="hljs-built_in">history</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">done</span></span>/2017 drwxrwx--- - root supergroup 0 2017-01-05 10:13 /tmp/hadoop-yarn/staging/<span class="hljs-built_in"><span class="hljs-built_in">history</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">done</span></span>/2017/01 drwxrwx--- - root supergroup 0 2017-01-05 10:13 /tmp/hadoop-yarn/staging/<span class="hljs-built_in"><span class="hljs-built_in">history</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">done</span></span>/2017/01/05 drwxrwx--- - root supergroup 0 2017-01-05 10:13 /tmp/hadoop-yarn/staging/<span class="hljs-built_in"><span class="hljs-built_in">history</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">done</span></span>/2017/01/05/000000 -rwxrwx--- 1 root supergroup 46338 2017-01-05 10:13 /tmp/hadoop-yarn/staging/<span class="hljs-built_in"><span class="hljs-built_in">history</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">done</span></span>/2017/01/05/000000/job_1483628783579_0001-1483629144632-root-QuasiMonteCarlo-1483629179995-3-1-SUCCEEDED-default-1483629156270.jhist -rwxrwx--- 1 root supergroup 117543 2017-01-05 10:13 /tmp/hadoop-yarn/staging/<span class="hljs-built_in"><span class="hljs-built_in">history</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">done</span></span>/2017/01/05/000000/job_1483628783579_0001_conf.xml drwxrwxrwt - root supergroup 0 2017-01-05 10:12 /tmp/hadoop-yarn/staging/<span class="hljs-built_in"><span class="hljs-built_in">history</span></span>/done_intermediate drwxrwx--- - root supergroup 0 2017-01-05 10:13 /tmp/hadoop-yarn/staging/<span class="hljs-built_in"><span class="hljs-built_in">history</span></span>/done_intermediate/root drwx------ - root supergroup 0 2017-01-05 10:12 /tmp/hadoop-yarn/staging/root drwx------ - root supergroup 0 2017-01-05 10:13 /tmp/hadoop-yarn/staging/root/.staging drwxrwxrwt - root supergroup 0 2017-01-05 10:12 /tmp/logs drwxrwx--- - root supergroup 0 2017-01-05 10:12 /tmp/logs/root drwxrwx--- - root supergroup 0 2017-01-05 10:12 /tmp/logs/root/logs drwxrwx--- - root supergroup 0 2017-01-05 10:13 /tmp/logs/root/logs/application_1483628783579_0001 -rw-r----- 1 root supergroup 65829 2017-01-05 10:13 /tmp/logs/root/logs/application_1483628783579_0001/master.local_37940 drwxr-xr-x - root supergroup 0 2017-01-05 10:12 /user drwxr-xr-x - root supergroup 0 2017-01-05 10:13 /user/root</code> </pre> <br><h3>  Testing in a distributed cluster </h3><br>  You may have noticed that so far I have taken the ‚Äúcluster‚Äù in quotes.  After all, everything works on the same machine.  Correct this unfortunate misunderstanding.  Test our Hadoop in a real distributed cluster. <br><br>  First of all, let's tweak the Hadoop configuration.  Currently, the host name in the Hadoop configuration is listed as localhost.  If you now just copy this configuration to other nodes, each node will try to find NameNode, ResourceManager, and JobHistory services on its host.  Therefore, we will define in advance the name of the host with these services and make changes to the configs. <br><br>  In my case, all the above master services (NameNode, ResourceManager, JobHistory) will run on master.local.  Replace localhost with master.local in the configuration: <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /opt/hadoop/etc/hadoop sed -i <span class="hljs-string"><span class="hljs-string">'s/localhost/master.local/'</span></span> core-site.xml hdfs-site.xml yarn-site.xml mapred-site.xml</code> </pre> <br>  Now I just clone the virtual machine on which I built two times to get two slaves.  On the slave nodes, you need to set a unique host name (in my case it is slave1.local and slave2.local).  Also on all three nodes of our cluster, we configure / etc / hosts so that each cluster machine can contact the others by the host name.  In my case it looks like this (the same content on all three machines): <br><br><pre> <code class="bash hljs">cat /etc/hosts ‚Ä¶ 192.168.122.70 master.local 192.168.122.59 slave1.local 192.168.122.217 slave2.local</code> </pre> <br>  Additionally, on the nodes slave1.local and slave2.local, you need to clear the contents of / data / dfs / dn <br><br><pre> <code class="bash hljs">rm -rf /data/dfs/dn/*</code> </pre> <br>  All is ready.  On master.local we start all services: <br><br><pre> <code class="bash hljs">/opt/hadoop/sbin/hadoop-daemon.sh start namenode /opt/hadoop/sbin/hadoop-daemon.sh start datanode /opt/hadoop/sbin/yarn-daemon.sh start resourcemanager /opt/hadoop/sbin/yarn-daemon.sh start nodemanager /opt/hadoop/sbin/mr-jobhistory-daemon.sh start historyserver</code> </pre> <br>  On slave1.local and slave2.local, we run only the DataNode and NodeManager: <br><br><pre> <code class="bash hljs">/opt/hadoop/sbin/hadoop-daemon.sh start datanode /opt/hadoop/sbin/yarn-daemon.sh start nodemanager</code> </pre> <br>  Let's check that our cluster now consists of three nodes. <br><br>  For HDFS, let's look at the output of the dfsadmin -report command and make sure that all three machines are included in the Live datanodes list: <br><br><pre> <code class="bash hljs">hdfs dfsadmin -report ... Live datanodes (3): ‚Ä¶ Name: 192.168.122.70:50010 (master.local) ... Name: 192.168.122.59:50010 (slave1.local) ... Name: 192.168.122.217:50010 (slave2.local)</code> </pre> <br>  Or go to the NameNode web page: <br><br>  <a href="http://master.local/">master.local</a> : 50070 / dfshealth.html # tab-datanode <br><br><img src="https://habrastorage.org/files/015/8f7/ae8/0158f7ae86304434be75c7d6e645a658.png"><br>  For YARN, let's look at the output of the node -list command: <br><br><pre> <code class="bash hljs">yarn node -list -all 17/01/06 06:17:52 INFO client.RMProxy: Connecting to ResourceManager at master.local/192.168.122.70:8032 Total Nodes:3 Node-Id Node-State Node-Http-Address Number-of-Running-Containers slave2.local:39694 RUNNING slave2.local:8042 0 slave1.local:36880 RUNNING slave1.local:8042 0 master.local:44373 RUNNING master.local:8042 0</code> </pre> <br>  Or go to the ResourceManager webpage <br><br>  <a href="http://master.local/">master.local</a> : 8088 / cluster / nodes <br><br><img src="https://habrastorage.org/files/7f6/595/657/7f65956572eb440dacb118920af0aac6.png"><br>  All nodes must be listed as RUNNING. <br><br>  Finally, make sure that the MapReduce applications that are started use resources on all three nodes.  Run the already familiar Pi application from hadoop-mapreduce-examples.jar: <br><br><pre> <code class="bash hljs">yarn jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar pi 30 1000</code> </pre> <br>  During the execution of the application, we once again see the output of yarn node -list -all: <br><br><pre> <code class="bash hljs">... Node-Id Node-State Node-Http-Address Number-of-Running-Containers slave2.local:39694 RUNNING slave2.local:8042 4 slave1.local:36880 RUNNING slave1.local:8042 4 master.local:44373 RUNNING master.local:8042 4</code> </pre> <br>  Number-of-Running-Containers - 4 on each node. <br><br>  We can also go to <a href="http://master.local/">master.local</a> : 8088 / cluster / nodes and see how many cores and memory are used by all applications in total at each node. <br><br><img src="https://habrastorage.org/files/e6b/49d/c14/e6b49dc1491c4740b9f5fbd7fd991d48.png"><br><br><h3>  Conclusion </h3><br>  We compiled Hadoop from source code, installed, configured and tested performance on a separate machine and in a distributed cluster.  If the topic is interesting to you, if you want to similarly collect other services from the Hadoop ecosystem, leave a link to the script, which I support for my own needs: <br><br>  <a href="https://github.com/hadoopfromscratch/hadoopfromscratch">github.com/hadoopfromscratch/hadoopfromscratch</a> <br><br>  With his help, you can install zookeeper, spark, hive, hbase, cassandra, flume.  If you find errors or inaccuracies, please write.  I would be very grateful. </div><p>Source: <a href="https://habr.com/ru/post/319048/">https://habr.com/ru/post/319048/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../319034/index.html">Infrastructure simple electronic signature. Part 2: Target System Modeling</a></li>
<li><a href="../319036/index.html">Comparison of lock-free algorithms - CAS and FAA on the example of JDK 7 and 8</a></li>
<li><a href="../319038/index.html">Isomorphic React Applications: Performance and Scaling</a></li>
<li><a href="../319040/index.html">Effective calculation of the field of view and the line of sight in games</a></li>
<li><a href="../319042/index.html">Recognize checks in Google Docs using the ABBYY OCR SDK</a></li>
<li><a href="../319050/index.html">Galois field on Scala</a></li>
<li><a href="../319052/index.html">NoSQL - briefly about the main thing</a></li>
<li><a href="../319054/index.html">How to start using SSD hardware encryption using the example of Samsung EVO 850 and sedutil</a></li>
<li><a href="../319056/index.html">FuseTools - a unique tool for prototyping and development</a></li>
<li><a href="../319058/index.html">IBM Expands Serverless OpenWhisk Platform</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>