<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to make binary classifier work a little bit better</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Disclaimer: A post is written based on this . I suspect that most readers are well aware of how the Naive Bayes classifier works, so I propose only a ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to make binary classifier work a little bit better</h1><div class="post__text post__text-html js-mediator-article"><h6>  <i><b>Disclaimer: A</b> post is written based on <a href="http://habrahabr.ru/post/120194/">this</a> .</i>  <i>I suspect that most readers are well aware of how the Naive Bayes classifier works, so I propose only a glimpse of what it says before going under the cat.</i> </h6><br>  Problem solving using machine learning algorithms has long been firmly established in our lives.  This happened for all understandable and objective reasons: cheaper, simpler, faster than explicitly coding the algorithm for solving each individual problem.  Usually, the ‚Äúblack boxes‚Äù of classifiers (it is unlikely that the same VC will offer you its corpus of marked names), which does not allow them to be fully managed. <br>  Here I would like to talk about how to try to achieve the ‚Äúbest‚Äù results of the binary classifier, what characteristics the binary classifier has, how to measure them, and how to determine that the result of the work has become ‚Äúbetter‚Äù. <br><a name="habracut"></a><br><h3>  Theory.  Short course </h3><h4>  1. Binary classification </h4>  Let be <img src="https://habrastorage.org/getpro/habr/post_images/19b/dd8/21e/19bdd821e3d32029e66bf63246c01108.png">  - many objects <img src="https://habrastorage.org/getpro/habr/post_images/745/637/711/745637711a25d149d283b377e95ada16.png">  - finite set of classes.  <b>Categorize object</b> - use mapping <img src="https://habrastorage.org/getpro/habr/post_images/64a/2da/d42/64a2dad425c6147b1af14e7ba0b98743.png">  .  When <img src="https://habrastorage.org/getpro/habr/post_images/82a/cf8/116/82acf8116a2888dd50756ea870df59f3.png">  , such a classification is called <b>binary</b> , because we have only 2 options at the output.  For simplicity, we will further assume that <img src="https://habrastorage.org/getpro/habr/post_images/ec0/33a/900/ec033a9003fd2a02aa0985ee0696a0cc.png">  , because absolutely any problem of binary classification can lead to this.  <b>Usually</b> , the result of mapping an object into a class is a real number, and if it is above a given <b>threshold</b> , then the object is classified as <b>positive</b> , and its class is 1. <br><br><h4>  2. Table of contingency binary classifier </h4>  Obviously, the predicted class of the object obtained by mapping <img src="https://habrastorage.org/getpro/habr/post_images/6c9/154/701/6c91547016a6258eb18222c57bb15066.png">  can either match in a real class or not.  Total 4 options in the amount.  This is very clearly demonstrated by this tablet: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/95e/5c3/6b5/95e5c36b5a14f60426b01fe2c639cccf.gif"></div><br>  If we know the quantitative values ‚Äã‚Äãof each of the assessments - we know all that is possible about this classifier and can dig further. <br>  (I deliberately do not use the terms of the type "error of the first kind" because it seems to me not obvious and unnecessary) <br>  We will continue to use the following notation: <br><ul><li><img src="https://habrastorage.org/getpro/habr/post_images/bec/0fe/57c/bec0fe57cc2fd896237996de84cc9c22.png">  - the number of True Positive results for this sample. </li><li><img src="https://habrastorage.org/getpro/habr/post_images/d92/b32/a20/d92b32a205e3fa1974f342bd30ba8936.png">  - the number of True Negative results for this sample. </li><li><img src="https://habrastorage.org/getpro/habr/post_images/481/5a7/12a/4815a712a20c85154840d73520e94da4.png">  - False Positive number of results for this sample. </li><li><img src="https://habrastorage.org/getpro/habr/post_images/cce/050/993/cce050993d18155925bdf641f4abad46.png">  - the number of False Negative results for this sample. </li></ul><br><h4>  3. Characteristics of a binary classifier </h4>  <b>Accuracy (precision)</b> - shows how many of the predicted positive objects turned out to be really positive. <br><img src="https://habrastorage.org/getpro/habr/post_images/5fe/916/e53/5fe916e53c7df86ded4c773d3b69a455.png"><br>  <b>Fullness (recall)</b> - shows how much of the total number of real positive objects, it was predicted as a positive class. <br><img src="https://habrastorage.org/getpro/habr/post_images/09c/5d4/672/09c5d46721206811f90acc5e5c977557.png"><br>  These two characteristics are essential for any binary classifier.  Which of the characteristics is more important - it all depends on the task.  For example, if we want to create a ‚Äúschool search engine,‚Äù then it will be in our interest to remove ‚Äúnon-childish‚Äù content from the issue, and here completeness is much more important than accuracy.  In the case of determining the gender of the name - we are more interested in accuracy than completeness. <br>  <b>F-measure (F-measure)</b> - a characteristic that allows you to give an estimate at the same time for accuracy and completeness. <br><img src="https://habrastorage.org/getpro/habr/post_images/9c4/f96/816/9c4f968165afda9d658b5524de5ab678.png"><br>  Coefficient <img src="https://habrastorage.org/getpro/habr/post_images/dbf/78a/f1c/dbf78af1ca7b5f3f2d94600591367bdc.png">  sets the balance of precision and completeness.  When <img src="https://habrastorage.org/getpro/habr/post_images/750/f8f/aa1/750f8faa10a90fa727f27d8d56becb08.png">  The F-measure gives equal weight to both characteristics.  This F-measure is called balanced, or <img src="https://habrastorage.org/getpro/habr/post_images/a9f/148/c70/a9f148c7097aeed158e153846a815ec3.png"><br><img src="https://habrastorage.org/getpro/habr/post_images/8de/037/c38/8de037c38cbc716bb6069930152ca898.png"><br>  <b>False Positive Rate (</b> <b><img src="https://habrastorage.org/getpro/habr/post_images/cde/9ca/616/cde9ca616f99d6529f31d87d6a1af776.png"></b>  <b>)</b> - shows how many of the total number of real negative objects turned out to be incorrectly predicted. <br><img src="https://habrastorage.org/getpro/habr/post_images/6dd/5b5/3cb/6dd5b53cb88f9140c3b34f90f31c73d6.png"><br><br><h4>  4. ROC curve and its AUC </h4>  <b>ROC curve</b> - a graph that allows you to assess the quality of the binary classification.  The graph shows the dependence of <b>TPR</b> (completeness) on <b>FPR</b> with varying threshold.  At the point (0,0), the threshold is minimal, and <b>TPR</b> and <b>FPR</b> are also minimal.  An ideal case for a classifier is the passage of a graph through a point (0,1).  Obviously, the graph of this function is always monotonically non-decreasing. <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/800/ecb/955/800ecb9550bc7dafae14dc9f811a0a36.png"></div><br>  <b>AUC (Area Under Curve)</b> - this term (the area under the graph) gives a quantitative characteristic of the ROC curve: the more the better.  AUC is equivalent to the probability that the classifier will assign a greater value to a randomly selected positive object than to a randomly chosen negative object.  That is why it was previously said that, as a <b>rule</b> , the positive class is rated higher than the negative. <br>  When <b>AUC = 0.5</b> , then this classifier is equal to random.  If <b>AUC &lt;0.5</b> , then you can simply flip the values ‚Äã‚Äãproduced by the classifier. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  Cross validation </h3>  There are many methods of cross validation (assessing the quality of a binary classifier), and this is a topic for a separate article.  Here I just want to consider one of the most popular methods in order to understand how this thing works at all, and why it is needed. <br>  Of course, it is possible to build a ROC curve for any sample.  However, the ROC curve constructed from the training set will be shifted left-up due to retraining.  To avoid this and get the most objective assessment, cross-validation is used. <br>  <b>K-fold cross validation</b> - the pool is split into <b>k</b> folds, then each fold is used for the test, while the remaining <b>k-1</b> folds are used for training.  The value of the parameter <b>k</b> can be arbitrary.  In this case, I used it to be 10. For the given gender classifier of the name, the following AUC results were obtained (get_features_simple is one significant letter, get_features_complex is 3 significant letters) <table><tbody><tr><td>  <b>fold</b> </td><td>  <b>get_features_simple</b> </td><td>  <b>get_features_complex</b> </td></tr><tr><td>  0 </td><td>  0.978011 </td><td>  0.962733 </td></tr><tr><td>  one </td><td>  0.96791 </td><td>  0.944097 </td></tr><tr><td>  2 </td><td>  0.963462 </td><td>  0.966129 </td></tr><tr><td>  3 </td><td>  0.966339 </td><td>  0.948452 </td></tr><tr><td>  four </td><td>  0.946586 </td><td>  0.945479 </td></tr><tr><td>  five </td><td>  0.949849 </td><td>  0.989648 </td></tr><tr><td>  6 </td><td>  0.959984 </td><td>  0.943266 </td></tr><tr><td>  7 </td><td>  0.979036 </td><td>  0.958863 </td></tr><tr><td>  eight </td><td>  0.986469 </td><td>  0.951975 </td></tr><tr><td>  9 </td><td>  0.962057 </td><td>  0.980921 </td></tr><tr><td>  <b>avg</b> </td><td>  0.9659703 </td><td>  0.9591563 </td></tr></tbody></table><br><h3>  Practice </h3><h4>  1. Preparation </h4>  The entire repository is <a href="https://github.com/achigin/OptimalThresholdFinder">here</a> . <br>  I took the same tagged file and <a href="https://raw.githubusercontent.com/achigin/OptimalThresholdFinder/master/names_learn_pool.txt">replaced</a> it with ‚Äúm‚Äù for 1, ‚Äúf‚Äù - 0. We‚Äôll use this pool for training, like the author of the previous article.  Armed with the first page of issuing a favorite search engine and awk, I <a href="https://raw.githubusercontent.com/achigin/OptimalThresholdFinder/master/names_test_pool.txt">rivet the</a> list of names that are not in the original.  This pool will be used for the test. <br>  Changed the classification function so that it returns the probabilities of positive and negative classes, and not logarithmic indicators. <br><div class="spoiler">  <b class="spoiler_title">Classification function</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">classify2</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(classifier, feats)</span></span></span><span class="hljs-function">:</span></span> classes, prob = classifier class_res = dict() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, item <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(classes.keys()): value = -log(classes[item]) + sum(-log(prob.get((item, feat), <span class="hljs-number"><span class="hljs-number">10</span></span>**(<span class="hljs-number"><span class="hljs-number">-7</span></span>))) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> feat <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> feats) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (item <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>): class_res[item] = value eps = <span class="hljs-number"><span class="hljs-number">709.0</span></span> posVal = <span class="hljs-string"><span class="hljs-string">'1'</span></span> negVal = <span class="hljs-string"><span class="hljs-string">'0'</span></span> posProb = negProb = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (abs(class_res[posVal] - class_res[negVal]) &lt; eps): posProb = <span class="hljs-number"><span class="hljs-number">1.0</span></span> / (<span class="hljs-number"><span class="hljs-number">1.0</span></span> + exp(class_res[posVal] - class_res[negVal])) negProb = <span class="hljs-number"><span class="hljs-number">1.0</span></span> / (<span class="hljs-number"><span class="hljs-number">1.0</span></span> + exp(class_res[negVal] - class_res[posVal])) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> class_res[posVal] &gt; class_res[negVal]: posProb = <span class="hljs-number"><span class="hljs-number">0.0</span></span> negProb = <span class="hljs-number"><span class="hljs-number">1.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: posProb = <span class="hljs-number"><span class="hljs-number">1.0</span></span> negProb = <span class="hljs-number"><span class="hljs-number">0.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str(posProb) + <span class="hljs-string"><span class="hljs-string">'\t'</span></span> + str(negProb)</code> </pre> </div></div><br><h4>  2. Implementation and use </h4>  As written in the title, my task was to make the binary classifier work better than it works by default.  In this case, we want to learn how to determine the gender of a name, better than the probability of Naive Bayes 0.5.  For this, this simplest utility was written.  It is written in C ++, because gcc is everywhere.  The implementation itself is nothing interesting, it seems to me.  With a key <b>-?</b>  or <b>--help</b> you can read the help, I tried to describe it in as much detail as possible. <br>  And now, in fact, what we were going to: the assessment of the classifier and its tuning.  At the output, <b>nbc.py</b> creates a sheet from the files with the results of the classification, I use them directly below.  For our purposes, we will clearly see the graphs of accuracy from the threshold, completeness from the threshold and F-measure from the threshold.  They can be built as follows: <br><pre> <code class="bash hljs">$ ./OptimalThresholdFinder -A 3 -P 1 &lt; names_test_pool.txt_simple -x thr -y prc -p plot_test_thr_prc_simple.txt $ ./OptimalThresholdFinder -A 3 -P 1 &lt; names_test_pool.txt_simple -x thr -y tpr -p plot_test_thr_tpr_simple.txt $ ./OptimalThresholdFinder -A 3 -P 1 &lt; names_test_pool.txt_simple -x thr -y fms -p plot_test_thr_fms_simple.txt $ ./OptimalThresholdFinder -A 3 -P 1 &lt; names_test_pool.txt_simple -x thr -y fms -p plot_test_thr_fms_0.7_simple.txt -a 0.7</code> </pre> <br>  For educational purposes, I made 2 F-measures from the threshold, with different weights.  The second weight was chosen 0.7, because in our problem we are more interested in accuracy than completeness.  The default graph is based on 10,000 different points, which is a lot for such simple data, but these are uninteresting optimization subtleties.  In the same way, by constructing graph data for get_features_complex, we obtain the following results: <br><div style="text-align:center;"><img src="//habrastorage.org/files/2e6/793/13c/2e679313c47d4e58ad8b85ad8d4a3778.png"></div><div style="text-align:center;"><img src="//habrastorage.org/files/feb/6be/f70/feb6bef70c7f4da79cf3f3b96d29f770.png"></div><br><div style="text-align:center;"><img src="//habrastorage.org/files/353/d21/887/353d2188745644e1a36349655b6e81e3.png"></div><div style="text-align:center;"><img src="//habrastorage.org/files/d53/2ae/283/d532ae2835464762be4e1767e83c30ed.png"></div><br>  From the graphs it becomes obvious that the classifier shows the best results by no means at the threshold of 0.5.  The graph of the F-measure clearly demonstrates that the "complex feature" gives the best result on all the variation of the threshold.  This is quite logical, given that it is also "complicated."  We obtain the threshold values ‚Äã‚Äãat which the F-measure reaches a maximum: <br><pre> <code class="bash hljs">$ ./OptimalThresholdFinder -A 3 -P 1 &lt; names_test_pool.txt_simple --target fms --argument thr --argval 0 Optimal threshold = 0.8 Target <span class="hljs-keyword"><span class="hljs-keyword">function</span></span> = 0.911937 Argument = 0.8 $ ./OptimalThresholdFinder -A 3 -P 1 &lt; names_test_pool.txt_complex --target fms --argument thr --argval 0 Optimal threshold = 0.716068 Target <span class="hljs-keyword"><span class="hljs-keyword">function</span></span> = 0.908738 Argument = 0.716068</code> </pre>  Agree, these values ‚Äã‚Äãare much better than those that turned out to be at the threshold of 0.5. <br><br><h3>  Conclusion </h3>  With such simple manipulations, we were able to choose the optimal Naive Bayes threshold to determine the gender of the name that works better than the default threshold.  This raises a reasonable question, how do we determine that it works ‚Äúbetter‚Äù?  I have repeatedly mentioned that accuracy is more important to us in this task than completeness, but the question of how important it is is very, very difficult, so a balanced F-measure was used to evaluate the classifier‚Äôs performance, which in this case can be an objective indicator quality. <br>  What is much more interesting, the results of the binary classifier based on the ‚Äúsimple‚Äù and ‚Äúcomplex‚Äù features turned out to be approximately the same, differing in the value of the optimal threshold. </div><p>Source: <a href="https://habr.com/ru/post/228963/">https://habr.com/ru/post/228963/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../228953/index.html">Spring: Transactional TaskExecutor Implementation</a></li>
<li><a href="../228955/index.html">Learning security: the most popular webinars</a></li>
<li><a href="../228957/index.html">Idris language overview</a></li>
<li><a href="../228959/index.html">Cost Optimization Yota: Attempt # 3</a></li>
<li><a href="../228961/index.html">Ubuntu and Debian Linux for advanced</a></li>
<li><a href="../228965/index.html">The story of the creation of the game</a></li>
<li><a href="../228967/index.html">Using FitNesse for .Net applications</a></li>
<li><a href="../228969/index.html">Radio-controlled walking robot on MG90</a></li>
<li><a href="../228971/index.html">Tor Relay in five minutes</a></li>
<li><a href="../228973/index.html">We unlock the bike computer VDO A4 to A8</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>