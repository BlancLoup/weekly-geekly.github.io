<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Structure from Motion - classic implementation</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="There is such an interesting task - the construction of a 3D structure on a set of images (photos) - Structure from Motion. How can it be solved? Afte...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Structure from Motion - classic implementation</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/cb6/62f/b34/cb662fb34c8b00f735a1b969c9b83862.gif"><br><br>  There is such an interesting task - the construction of a 3D structure on a set of images (photos) - Structure from Motion.  How can it be solved?  After some thought, this algorithm comes to mind.  We find the characteristic features (points) on all the images, compare them with each other and by triangulation we find their three-dimensional coordinates.  There really is a problem - the camera position is unknown when shooting.  Can you find them?  It seems possible.  Indeed, let us have <i>N</i> points on the frame and <i>M</i> frames.  Then the unknowns will be <i>3 * N</i> (three-dimensional coordinates of points) + <i>6 * (M - 1)</i> (coordinates of cameras (instead of 6 there may be a different number, but this does not change the essence)).  Equations we have <i>2 * M * N</i> (each point on each image has two coordinates).  It turns out that already for two images and 6 points the problem is solvable.  Under the cut is a description of the conceptual scheme for solving the SfM problem (if possible without formulas - but with links for thoughtful study). <br><a name="habracut"></a><br>  The plan is: <br><ol><li>  Find Key Points </li><li>  Find matches between points for consecutive pairs of images (although it can be found for all pairs of images) </li><li>  Filter false matches </li><li>  Solve the system of equations and find the three-dimensional structure along with the camera positions </li></ol><br><br>  The plan seems to be simple and straightforward.  And, of course, all the intrigue in the 3rd and 4th points. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h5>  1. Points </h5><br>  Finding points does not cause problems.  There are lots of methods.  The most famous is <a href="http://en.wikipedia.org/wiki/Scale-invariant_feature_transform">SIFT</a> .  He quite reliably finds the key points taking into account their scale and orientation (which is important when the camera moves arbitrarily).  There are several open-source implementations of this method. <br><br><h5>  2. Compliance </h5><br>  Each point must be accompanied by its description (descriptor).  If the descriptors of points on different images are close, then we can assume that the same physical object.  The simplest descriptor is a small neighborhood of a point.  But ideally, of course, the descriptor should be independent of the scale and orientation of the image.  In the SIFT method it is used such.  Then the search for matches is reduced to bypassing all the key points of one image and finding the closest descriptor on another image. <br><br><h5>  3. Filtering false matches. </h5><br>  It is clear that with any quality of the descriptor of false matches there will be many.  They need to filter.  Here we can distinguish two stages: <br><ol><li>  Geometry-independent filtering </li><li>  Epipolar restriction filtering </li></ol><br>  Geometry-independent filtering is described in the same SIFT method.  There are several simple filters, for example: the best fit from the first image to the second should match the best match from the second to the first. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/aa8/e22/ed9/aa8e22ed973f2e369857aa264a887b24.png" alt="image"><br><br>  <b>Epipolar geometry.</b>  <b><i>C</i> and <i>C '</i> are the optical centers of the cameras.</b>  <b>If the point <i>P</i> in the first image is designed in <i>m</i> .</b>  <b>Then, on the other image, its projection must be searched for on the straight line <i>l ' <sub>m</sub></i> .</b> <br><br>  The strongest filtering is, of course, using the <a href="http://en.wikipedia.org/wiki/Epipolar_geometry">epipolar constraint</a> .  Its geometric meaning is that for each point of one image its corresponding point on another image is on a certain line (which does not depend on the true three-dimensional coordinates of the point).  Mathematically, this property is expressed by the equation <i>m <sup>T</sup> Fm '= 0</i> .  Here <i>m = (u, v, 1) <sup>T</sup></i> are the <a href="http://ru.wikipedia.org/wiki/%25D0%259E%25D0%25B4%25D0%25BD%25D0%25BE%25D1%2580%25D0%25BE%25D0%25B4%25D0%25BD%25D1%258B%25D0%25B5_%25D0%25BA%25D0%25BE%25D0%25BE%25D1%2580%25D0%25B4%25D0%25B8%25D0%25BD%25D0%25B0%25D1%2582%25D1%258B">homogeneous coordinates of the</a> key point in the image, and <i>F</i> is the fundamental matrix.  Most importantly, this ratio does not depend on the three-dimensional structure of the observed scene. <br><br>  Since the fundamental matrix is ‚Äã‚Äãunknown to us, the following method can be used to filter out false matches: <br><br><ol><li>  Randomly select several points and calculate the fundamental matrix from them (the calculation method is described in [1], section 14.3.2). </li><li>  Let us calculate how many more points satisfy the condition <i>m <sup>T</sup> Fm '= 0</i> with a given accuracy. </li><li>  If the number of points is large enough, stop the cycle, otherwise go to point 1. </li></ol><br><br>  This simple algorithm is called <a href="http://en.wikipedia.org/wiki/Ransac">RANSAC</a> and is ideal for our task.  And yes - he is still quite cunning and you can suffer a lot with him too.  Well, or you can use <a href="http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html%3Fhighlight%3Dfundamental">OpenCV</a> . <br><br><h5>  4. Search for three-dimensional structure </h5><br>  It's time to finally figure out how the points in space are connected with the points on the image.  Everything is simple - a projective transformation.  Fortunately, the specifics of my task (observation in a telescope with a small field of view) made it possible to use the affine geometry of the camera.  Geometric parameters of the camera consist of two parts: <br><br><ul><li>  External parameters that determine the position of the camera in space (rotation matrix <i>R</i> 3x3 and shift vector <i>t</i> 3x1). </li><li>  Internal parameters that determine the design of the image on the CCD camera.  These include scaling along each of the axes, the angle between the axes, and higher order distortion factors.  For the case of square pixels and an affine camera, only one parameter remains - the scale <i>k</i> . </li></ul><br><br>  The observation equation (the relationship between the three-dimensional coordinates <i>(X, Y, Z) <sup>T of a</sup></i> point and its position in the image <i>(u, v) <sup>T</sup></i> ) can be written as: <br><br><img width="250" src="https://habrastorage.org/getpro/habr/post_images/782/131/d8e/782131d8e4bb21da577778d6d0a5f896.png"><br><br>  It is important to note here that the third row is ignored in affine geometry, and only two rows can be used from the rotation and shift matrices. <br><br>  It turns out that we need to select the external (and internal) parameters of all the cameras and the three-dimensional coordinates of the points so that this relationship is performed as precisely as possible for all the points. <br><br>  There are two ways to solve this problem.  The first is to try to solve this non-linear system with a bunch of non-linear methods ( <a href="http://en.wikipedia.org/wiki/Bundle_adjustment">bundle adjustment</a> ).  The main problem here is obvious - it is necessary to be able to solve very large systems of nonlinear equations (or rather, to minimize the nonlinear function in a multidimensional space) and to achieve their convergence to the correct solution.  Another way is to solve the problem analytically whenever possible.  So we will do. <br><br>  And here we are helped by the method of factorization.  It turns out that using a <a href="http://en.wikipedia.org/wiki/Singular_value_decomposition">singular decomposition</a> one can solve the problem for an affine camera for the case when all points are visible from all cameras (in practice this means ‚Äúfor each pair of images separately‚Äù).  A justification can be found in [1], section 18.2. <br><br><ol><li>  Shift all points on all images so that the center of mass is at the origin.  This allows you to completely eliminate the shift vector from the equations. </li><li>  We compose the observation matrix from the projection of all points found. <img width="150" src="https://habrastorage.org/getpro/habr/post_images/670/96f/1b8/67096f1b8137d662daae39b9d9502f63.png">  .  Then, having written all the matrixes of the cameras one under the other into the matrix <i>M</i> , and all three-dimensional points next to each other in the matrix <i>X,</i> we obtain the equation <i>W = MX</i> . </li><li>  Find the singular decomposition <i>W = UDV <sup>T.</sup></i>  Then the camera matrices are obtained from the first three columns of <i>U</i> multiplied by singular values.  The three-dimensional affine structure is obtained from the first three columns of the matrix <i>V.</i> </li></ol><br><br>  Too easy?  Yes - there is a catch.  The fact is that our solution was found up to an affine transformation, since  we ignored the structure of <a href="http://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B0%25D1%2582%25D1%2580%25D0%25B8%25D1%2586%25D0%25B0_%25D0%25BF%25D0%25BE%25D0%25B2%25D0%25BE%25D1%2580%25D0%25BE%25D1%2582%25D0%25B0">the rotation matrix</a> (and it is not arbitrary at all).  To correct the situation, we must take into account that the cameras are described by orthogonal matrices, or simply noting that <i>W = MX = MCC <sup>T</sup> X</i> for any orthogonal matrix <i>C</i> (for a description of the method, see [2], section 12.4.2.) <br><br>  The rotation matrix must satisfy the following conditions: <br><br><img width="100" src="https://habrastorage.org/getpro/habr/post_images/fe0/c44/951/fe0c449517667f2dc1744e536c30b7eb.png"><br>  where <i>m <sub>ij</sub></i> - matrix lines of each camera.  These restrictions can be used to search for a matrix <i>C</i> that reduces the affine structure to a Euclidean one: <br><br><img width="120" src="https://habrastorage.org/getpro/habr/post_images/903/16d/fa1/90316dfa16a442895702d4fd0d97e78b.png"><br><br>  This system of equations is nonlinear with respect to <i>C</i> , but linear with <i>D = CC <sup>T.</sup></i>  From <i>D, the</i> matrix <i>C is</i> obtained by taking the <a href="http://twt.mpei.ac.ru/math/LARB/Quadrf/LA_06030400.html">square root of the matrix</a> .  Multiplying the matrixes of the cameras on the right by <i>C</i> , and the three-dimensional structure on the left by <i>C <sup>T</sup></i> , we obtain the correct Euclidean structure, which also satisfies the observation matrix. <br><br>  And finally - the union of three-dimensional structures from different pairs of cameras.  There is a small nuance.  The positions of the cameras obtained by us are centered relative to the set of points, and for different pairs of cameras the set of points is different.  So you need to find on two pairs of cameras an intersecting set of points.  Then, to recalculate the coordinates, you only need to find the transformation between these sets of points. <br>  So we got a three-dimensional Euclidean (i.e. with preserving the angles and ratios of the lengths of segments) structure.  In the image at the beginning of the post - this is the structure for the Energy-Buran model (one of the original images below).  Such a structure is called sparse, because  There are relatively few points.  Getting out of this dense (dense) structure and stretching textures is another story. <br><br><img width="300" src="https://habrastorage.org/getpro/habr/post_images/87a/b8a/f34/87ab8af34372fb5f3eb619eae27c045c.jpg"><br><br><h5>  Literature </h5><br><ol><li>  R. Hartley, A. Zisserman, ‚ÄúMultiple View Geometry in Computer Vision‚Äù </li><li>  Forsyth, Pons.  ‚ÄúComputer vision.  Modern approach " </li></ol></div><p>Source: <a href="https://habr.com/ru/post/228525/">https://habr.com/ru/post/228525/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../228513/index.html">Venture investor Tim Draper became the winner of the auction for the sale of bitcoins seized from Silk Road</a></li>
<li><a href="../228515/index.html">Technology World Cup 2014</a></li>
<li><a href="../228519/index.html">Case: Clouds as a Tool for Optimizing IT Infrastructure</a></li>
<li><a href="../228521/index.html">A new version of the distribution has been released to create a pfSense 2.1.4 firewall.</a></li>
<li><a href="../228523/index.html">How to impose a theme for WordPress</a></li>
<li><a href="../228527/index.html">How does international roaming work?</a></li>
<li><a href="../228529/index.html">How does the CROC engineering service work - and what happens if, at 3 am, a cluster breaks somewhere far away</a></li>
<li><a href="../228531/index.html">DMA in general and in particular</a></li>
<li><a href="../228533/index.html">GameDev for quizzes - I brought you a bunch of useless facts and a bit of veterinary</a></li>
<li><a href="../228535/index.html">Month of use of the IBM cloud server: free opportunity to get acquainted with the Softlayer platform</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>