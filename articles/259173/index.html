<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Data Lake - from theory to practice. Tale about how we build ETL on Hadoop</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this article I want to tell about the next stage of development of DWH in Tinkoff Bank and about the transition from the paradigm of the classical ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Data Lake - from theory to practice. Tale about how we build ETL on Hadoop</h1><div class="post__text post__text-html js-mediator-article">  In this article I want to tell about the next stage of development of <b>DWH</b> in <b>Tinkoff Bank</b> and about the transition from the paradigm of the classical <b>DWH</b> to the paradigm of <b>Data Lake</b> . <br><br>  I want to start my story with such a funny picture: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/f0b/698/3ac/f0b6983accd746a0893e600958eaa978.jpeg"></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Yes, a few years ago the picture was relevant.  But now, with the development of technologies that are part of the Hadoop eco-system and the development of ETL platforms, it is legitimate to assert that ETL on Hadoop does not just exist, but that ETL on Hadoop has a great future.  Further in article I will tell about how we build ETL on Hadoop in Tinkoff Bank. <br><a name="habracut"></a><br><h1>  From task to implementation </h1><br>  Before DWH management a big task was set - to analyze the interests and behavior of the Internet visitors of the bank site.  DWH has two new data sources; big data is clickstream from the portal ( <a href="http://www.tinkoff.ru/">www.tinkoff.ru</a> ) and the RTB (Real-Time Bidding) platform of the bank.  Two sources generate a huge amount of textual semi-structured data, which of course does not suit the traditional DWH built in a bank on the massively parallel DBMS <a href="http://pivotal.io/big-data/pivotal-greenplum-database">Greenplum</a> .  The Hadoop cluster was deployed in the bank, based on the Cloudera distribution, which then formed the basis of the target data warehouse, or rather the data lake, for external data. <br><br><h1>  Lake building concept </h1><br>  It was important at the initial stages to think over and fixed the conceptual architecture, which will need to be followed during the modeling of new structures for data storage and data loading work.  We really didn‚Äôt want to turn our lake into a data swamp :) As in the classic DWH, we have identified the main conceptual data layers (see Fig. 1). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/055/cc2/980/055cc298078c4db9a09b95a457350909.jpg"></div><br>  <i>Fig.1 Concept</i> <br><br><ul><li>  RAW - a layer of raw data, here we load files, logs, archives.  Formats can be completely different: tsv, csv, xml, syslog, json, etc.  etc.; <br></li><li>  ODD - Operational Data Definition.  Here we load data in a format close to the relational.  The data here can be the result of preprocessing data from RAW before loading it into DDS; <br></li><li>  DDS - Detail Data Store.  Here we collect a consolidated detailed data model.  To store the data in this layer, we chose the concept of a <a href="http://en.wikipedia.org/wiki/Data_Vault_Modeling">Data Vault</a> ; <br></li><li>  MART - data marts.  Here we collect application data marts. <br></li></ul><br><br><h1>  Data Vault and how we prepare it </h1><br><br>  Why Data Vault?  This approach has its pluses and minuses. <br>  Pros: <br><ul><li>  Modeling flexibility <br></li><li>  Quick and easy development of ETL processes <br></li><li>  No data redundancy, and for big data this is a very important argument. <br></li></ul><br>  Minuses: <br><ul><li>  The main disadvantage for us was due to the storage environment (and more precisely, processing) of data and, as a consequence, the performance of join operations.  As it is known, Hive doesn‚Äôt like join operations very much, due to the fact that in the end everything turns into a slow map reduce. <br></li></ul><br><br>  After analyzing the trends in the development of Hadoop technologies, we decided to use this approach and, rolling up our sleeves, began to model the Data Vault for the above stated task. <br><br>  Actually, I want to tell a few concepts that we use.  For example, in downloading Internet user visits through pages, we do not save the visit URL every time.  We have allocated all URLs, in terms of the Data Vault, into a separate hub (see Figure 2).  This approach allows you to save space in HDFS and more flexibly work with URLs at the stage of loading and further processing of data. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/fd4/be8/539/fd4be8539b904de5bff88c7eaca26370.jpg"></div><br>  <i>Fig.2 Data Vault for visits</i> <br><br>  Another concept relates to the field of Internet user downloads.  We do not receive a single Internet user at the stage of loading into DDS, but load data in the context of source systems.  Thus, downloads to the Data Vault from different sources are independent of each other. <br><br>  It was important to immediately provide for the physical structure of data storage in Hadoop, i.e.  Immediately think over the DDL tables in Hive.  At this stage, we recorded two agreements: <br><ul><li>  Using Partitioning in HDFS; <br></li><li>  Distribution emulation by key in HDFS. <br></li></ul><br>  As a result, each object (table) Data Vault in its DDL contains: <br><br><pre><code class="sql hljs">PARTITIONED BY (ymd string, load_src string)</code> </pre> <br>  and <br><br><pre> <code class="sql hljs">CLUSTERED BY (l_visit_rk) INTO 64 BUCKETS</code> </pre> <br><br><h1>  ETL Rivers in Data Lake </h1><br>  That came to the most interesting.  The concept was thought out, modeling was carried out, data structures were created, now it would be good to fill all this with data. <br><br>  In order to provide a steady stream of data (files) to the RAW layer, we use <a href="https://flume.apache.org/">Apache Flume</a> .  To ensure fault tolerance and independence from the Hadoop cluster, we placed Flume on a separate server - we got a File Gate, as it were, in front of the Hadoop cluster.  Below is an example of setting up the Flume agent to transfer portal syslog: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># *** Clickstream PROD syslog source *** a3.sources = r1 r2 a3.channels = c1 a3.sinks = k1 a3.sources.r1.type = syslogtcp a3.sources.r2.type = syslogudp a3.sources.r1.port = 5141 a3.sources.r2.port = 5141 a3.sources.r1.host = 0.0.0.0 a3.sources.r2.host = 0.0.0.0 a3.sources.r1.channels = c1 a3.sources.r2.channels = c1 # channel a3.channels.c1.type = memory a3.channels.c1.capacity = 1000 # sink a3.sinks.k1.type = hdfs a3.sinks.k1.channel = c1 a3.sinks.k1.hdfs.path = /prod_raw/portal/clickstream/ymd=%Y-%m-%d a3.sinks.k1.hdfs.useLocalTimeStamp = true a3.sinks.k1.hdfs.filePrefix = clickstream a3.sinks.k1.hdfs.rollCount = 100000 a3.sinks.k1.hdfs.rollSize = 0 a3.sinks.k1.hdfs.rollInterval = 600 a3.sinks.k1.hdfs.idleTimeout = 0 a3.sinks.k1.hdfs.fileType = CompressedStream a3.sinks.k1.hdfs.codeC = bzip2 # *** END ***</span></span></code> </pre><br>  Thus, we got a stable data flow to the RAW layer.  Next, you need to decompose this data into a model, fill the Data Vault, well, in short, you need ETL on Hadoop. <br><br>  Drum roll, the lights go out, On the scene goes <a href="https://www.informatica.com/products/big-data/big-data-edition.html">Informatica Big Data Edition</a> .  I will not be in colors and talk a lot about this ETL tool, I will try to briefly and to the point. <br><br>  Lyrical digression.  I would like to note right away that the Informatica Platform (which includes the BDE) is not the familiar Informatica PowerCenter.  This is a fundamentally new data integration platform from Informatica corporation, to which all that great set of useful functions from the old and well-loved PowerCenter are now being transferred. <br><br>  Now in the case.  Informatica BDE allows you to quickly develop ETL procedures (mappings), the environment is very convenient and does not require long training.  Mapping is broadcast in HiveQL and performed on a Hadoop cluster, Informatica provides convenient monitoring, the sequence of starting ETL processes, the processing of branches and exceptions. <br><br>  For example, this is the mapping that fills the hub of the Internet users of our portal (see. Fig. 3). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/bca/ca5/304/bcaca5304f044a389d95cfd19776a863.jpg"></div><br>  <i>Fig.3 Mapping</i> <br><br>  The Informatica BDE optimizer translates this mapping into HiveQL and determines the execution steps itself (see Fig. 4). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/073/b2f/afc/073b2fafc3ad4f9a8fd2daaab712b629.jpg"></div><br>  <i>Fig.4 Execution Plan</i> <br><br>  Informatica BDE allows you to flexibly manage runtime parameters.  For example, we set up the following parameters: <br><br><pre> <code class="bash hljs">mapreduce.input.fileinputformat.split.minsize = 256000000 mapred.java.child.opts = -Xmx1g mapred.child.ulimit = 2 mapred.tasktracker.map.tasks.maximum = 100 mapred.tasktracker.reduce.tasks.maximum = 150 io.sort.mb = 100 hive.exec.dynamic.partition.mode = nonstrict hive.optimize.ppd = <span class="hljs-literal"><span class="hljs-literal">true</span></span> hive.exec.max.dynamic.partitions = 100000 hive.exec.max.dynamic.partitions.pernode = 10000</code> </pre><br>  Mappings can be streamed.  For example, we have data from separate source systems loaded in separate streams (see Fig. 5). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/90c/76f/e31/90c76fe3116345f2b7e10ecf78366baf.jpg"></div><br>  <i>Fig.5 Data Download Stream</i> <br><br>  Informatica BDE has a convenient administration and monitoring tool (see Fig. 6). <br><div style="text-align:center;"><img src="https://habrastorage.org/files/f66/2b7/1b6/f662b71b62b74050aa7b09411a78117f.jpg"></div><br>  <i>Fig.6 Monitoring of data flow execution</i> <br><br>  The advantages of Informatica BDE are the following: <br><br><ul><li>  Support for many Hadoop distributions: Cloudera, Hortonworks, MapR, PivotalHD, IBM Biginsights; <br></li><li>  Rapid implementation of new features developed in Hadoop into the product: support for new distributions, support for new versions of Hive, support for new data types in Hive, support for partitioned tables in Hive, support for new data storage formats; <br></li><li>  Rapid development of mappings; <br></li><li>  And one more very important argument in favor of Informatica is a very close cooperation and partnership with the market leader of Hadoop distributions, <a href="http://www.cloudera.com/content/cloudera/en/home.html">Cloudera</a> .  This argument allows you to determine the strategic choice in favor of these two platforms, if you decide to build Data Lake. <br></li></ul><br><br>  Among the disadvantages are the following: <br><br><ul><li>  One big, but not so significant, but still a drawback - it lacks all the many useful features that are in the old PowerCenter.  This is flexible work with variables and parameters both within the mapping and at the stage of interaction between workflow-&gt; mapping-&gt; workflow.  But, the new platform Informatica is developing and with each new version it becomes more convenient. <br></li></ul><br><br>  In general, the Informatica BDE tool has shown itself very well when working with Hadoop and we have very big plans for it in the ETL part of Hadoop.  I think soon we will write more substantive articles about the implementation of ETL on Hadoop on Informatica BDE. <br><br><h1>  results </h1><br>  The main result we obtained at this stage is a stable ETL that fills the DDS.  The result was obtained in two months by a team of two ETL developers and an architect.  Now we are daily running ETL on Hadoop ~ 100Gb text logs and we get about an order of magnitude less data in the Data Vault, on the basis of which data marts are collected.  Loading into the model occurs on the nightly schedule, the daily increment of data is loaded.  Download duration is ~ 2 hours.  With this data, performing Ad-hoc requests, analysts work through Hue and IPython. <br><br><h1>  Future plans </h1><br><ul><li>  Switching to CDH 5.4 (currently working on 5.2) and piloting Hive 0.14 and Hive on Spark technology; <br></li><li>  Update Informatica 9.6.1 Hotfix2 to Hotfix3.  And of course we are waiting for Informatica 10; <br></li><li>  Development of mappings that collect showcases for machine learning and data scientist; <br></li><li>  The development of ILM in Hadoop / HDFS. <br></li></ul></div><p>Source: <a href="https://habr.com/ru/post/259173/">https://habr.com/ru/post/259173/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../259163/index.html">DevConf 2015: Interview with the author - there will be no PHP6! Meet PHP7</a></li>
<li><a href="../259165/index.html">Making a Callback from the site using Askozia PBX</a></li>
<li><a href="../259167/index.html">XSD Design Patterns</a></li>
<li><a href="../259169/index.html">A way to make iptables write to your log and not duplicate to the system</a></li>
<li><a href="../259171/index.html">Level device in NES games</a></li>
<li><a href="../259175/index.html">Attackers use Linux / Moose to compromise Linux-embedded systems, part 1</a></li>
<li><a href="../259177/index.html">Hola VPN extension sells user traffic and contains remote code execution vulnerabilities</a></li>
<li><a href="../259181/index.html">Reuters: US plans to target Stuxnet to North Korea</a></li>
<li><a href="../259183/index.html">16 reasons why players leave your game</a></li>
<li><a href="../259185/index.html">Game server on Scala + Akka: Case study</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>