<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Sounding the past. A guide for historians to convert data into sound</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I'm sick of looking at the past. There are many guidelines for recreating the appearance of historical artifacts, but often we forget that this is a c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Sounding the past. A guide for historians to convert data into sound</h1><div class="post__text post__text-html js-mediator-article">  I'm sick of <i>looking</i> at the past.  There are many guidelines for recreating the appearance of historical artifacts, but often we forget that this is a creative act.  Perhaps we are too attached to our screens, we attach too much importance to our appearance.  Let's try to <b>hear</b> something from the past instead. <br><br>  The rich literature on archeoacoustics and sound landscapes helps to recreate the sound of a place <i>as it was</i> (for example, see the <a href="">Virtual Cathedral of St. Paul</a> or the <a href="https://jeffdveitch.wordpress.com/">work of Jeff Weitch on ancient Ostia</a> ).  But it is interesting to me to ‚Äúvoice‚Äù the data itself.  I want to define the syntax for representing data in the form of sound, so that these algorithms can be used in historical science.  Drucker said the <a href="http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html">famous phrase</a> that ‚Äúdata‚Äù is not really what is given, but rather what is captured, transformed, that is, 'capta'.  When dubbing data, I literally <i>reproduce the</i> past in the present.  Therefore, the assumptions and transformations of this data come to the fore.  The resulting sounds are ‚Äúdeformed performance‚Äù, which makes you hear the modern layers of history in a new way. <br><br>  I want to hear the meaning of the past, but I know that this is impossible.  However, when I hear an instrument, I can physically present a musician;  on the echoes and resonances can distinguish the physical space.  I feel the bass, can move in rhythm.  Music covers my body, all imagination.  Associations with previously heard sounds, music, and tones create a deep temporal experience, a system of embodied relationships between me and the past.  Visuality?  We have so long been visual representations of the past, that these grammars almost lost their artistic expressiveness and performative aspect. <br><a name="habracut"></a><br>  In this lesson you will learn how to create some noise from historical data.  <i>The value of</i> this noise, well ... depends on you.  Partially the point is to make your data unfamiliar again.  Translating, recoding, <a href="http://blog.taracopplestone.co.uk/making-things-photobashing-as-archaeological-remediation/">restoring</a> them, we begin to see the data elements that remained invisible by visual examination.  This deformation is consistent with the arguments made, for example, by Mark Sample about the <a href="http://www.samplereality.com/2012/05/02/notes-towards-a-deformed-humanities/">deformation of society</a> or Bethany Nowowiski about the <a href="http://nowviskie.org/2013/resistance-in-the-materials/">"resistance of materials"</a> .  Voiceover leads us from data to the ‚Äúcapt‚Äù, from social sciences to art, <a href="http://nooart.org/post/73353953758/temkin-glitchhumancomputerinteraction">from glitch to aesthetics</a> .  Let's see what it looks like. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1>  Content </h1><br><ul><li>  <a href="https://habr.com/ru/post/454318/">Targets and goals</a> <ul><li>  <a href="https://habr.com/ru/post/454318/">Instruments</a> </li><li>  <a href="https://habr.com/ru/post/454318/">Sample data</a> </li></ul></li><li>  <a href="https://habr.com/ru/post/454318/">A little introduction to sound</a> <ul><li>  <a href="https://habr.com/ru/post/454318/">Filling in the blanks</a> </li></ul></li><li>  <a href="https://habr.com/ru/post/454318/">Muscular algorithms</a> <ul><li>  <a href="https://habr.com/ru/post/454318/">Practice</a> </li></ul></li><li>  <a href="https://habr.com/ru/post/454318/">Briefly about configuring Python</a> </li><li>  <a href="https://habr.com/ru/post/454318/">MIDITime</a> <ul><li>  <a href="https://habr.com/ru/post/454318/">MIDITime installation</a> </li><li>  <a href="https://habr.com/ru/post/454318/">Practice</a> </li><li>  <a href="https://habr.com/ru/post/454318/">Upload your own data</a> </li></ul></li><li>  <a href="https://habr.com/ru/post/454318/">Sonic pi</a> <ul><li>  <a href="https://habr.com/ru/post/454318/">Practice</a> </li></ul></li><li>  <a href="https://habr.com/ru/post/454318/">Nihil novi sub sole</a> </li><li>  <a href="https://habr.com/ru/post/454318/">Conclusion</a> <ul><li>  <a href="https://habr.com/ru/post/454318/">Terms</a> </li></ul></li></ul><br><a name="1"></a><h1>  Targets and goals </h1><br>  In this lesson, I will discuss three ways to generate sound or music from your data. <br><br>  We will first use the free and open Musicalgorithms system developed by Jonathan Middleton.  In it we will get acquainted with key problems and terms.  Then take a small Python library to ‚Äútranslate‚Äù data onto an 88-key keyboard and bring some creativity to the work.  Finally, load the data into Sonic Pi's real-time audio and music processing software, for which many tutorials and reference resources are published. <br><br>  You will see how working with sounds moves us from a simple visualization into a really effective environment. <br><br><a name="2"></a><h3>  Instruments </h3><br><ul><li>  <a href="http://musicalgorithms.org/">Muscular algorithms</a> <br></li><li>  <a href="https://github.com/cirlabs/miditime">MIDITime</a> (I <a href="https://github.com/shawngraham/miditime">fork</a> ) <br></li><li>  <a href="http://sonic-pi.net/">Sonic pi</a> </li></ul><br><a name="3"></a><h3>  Sample data </h3><br><ul><li>  <a href="">Roman Empire Artifacts</a> <br></li><li>  <a href="">Excerpt from the thematic model diary of President John Adams</a> <br></li><li>  <a href="">An excerpt from the thematic model "Jesuit Relations"</a> </li></ul><br><a name="4"></a><h1>  A little introduction to sound </h1><br>  Sonification is a method of translating certain aspects of data into audio signals.  In general, a method can be called ‚Äúsounding‚Äù if it satisfies certain conditions.  These include reproducibility (other researchers can process the same data in the same way and get the same results) and what can be called ‚Äúintelligibility‚Äù or ‚Äúintelligibility‚Äù, that is, when significant elements of the source data are systematically reflected in the resulting sound (see <a href="http://www.icad.org/Proceedings/2008/Hermann2008.pdf">Hermann, 2008</a> ).  <a href="https://www.researchgate.net/publication/282504359_Listen_to_the_Sound_of_Data">Mark Last and Anna Usyskina (2015)</a> described a series of experiments to determine which analytical tasks can be performed on dubbing data.  Their <a href="https://www.researchgate.net/publication/282504359_Listen_to_the_Sound_of_Data">experimental results</a> showed that even untrained listeners (without formal learning to music) can discern the data aurally and make useful conclusions.  They found that listeners are able to perform common data mining tasks by ear, such as classification and clustering (in their experiments they broadcast basic scientific data on a scale of Western music). <br><br>  Last and Usyskina focused on time series.  According to their conclusions, time-series data is particularly well-suited for dubbing, since there are natural parallels here.  The music is consistent, it has a duration, and it develops over time;  likewise with the data of time series ( <a href="https://www.researchgate.net/publication/282504359_Listen_to_the_Sound_of_Data">Last, Usyskina 2015: p. 424</a> ).  It remains to compare the data with the corresponding audio outputs.  To combine aspects of data from different auditory measurements, such as height, variation form, and spacing (onset), the parameter mapping method is used in many applications.  The problem with this approach is that if there is no temporal relationship between the source data points (or, rather, a non-linear relationship), the resulting sound can be ‚Äúentangled‚Äù ( <a href="https://www.researchgate.net/publication/282504359_Listen_to_the_Sound_of_Data">2015: 422</a> ). <br><br><a name="5"></a><h3>  Filling in the blanks </h3><br>  Listening to the sound, a person fills the moments of silence with his expectations.  Consider a video where mp3 is converted to MIDI and back to mp3;  the music is ‚Äúflattened‚Äù, so that all the sound information is reproduced by one tool (the effect is similar to saving a web page as .txt, opening it in Word, and then re-saving it in .html format).  All sounds (including vocals) are converted to the corresponding note values, and then back to mp3. <br><br>  This is noise, but the meaning can be caught: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://player.vimeo.com/video/149070596" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  What's going on here?  If this song was known to you, you probably understood the real ‚Äúwords‚Äù.  But there are no words in the song!  If you have not heard it before, then it sounds like a meaningless cacophony (more examples on <a href="http://waxy.org/2015/12/if_drake_was_born_a_piano/"></a>  <a href="http://motherboard.vice.com/read/the-strange-acoustic-phenomenon-behind-these-wacked-out-versions-of-pop-songs">Andy Bayo site).</a>  <a href="http://motherboard.vice.com/read/the-strange-acoustic-phenomenon-behind-these-wacked-out-versions-of-pop-songs">This effect is sometimes called ‚Äúsound hallucination‚Äù</a> (auditory hallucination).  The example shows how in any data representation we can hear / see what, strictly speaking, no.  We fill the void with our own expectations. <br><br>  What does this mean for the story?  If we voice our data and begin to hear patterns in sound or weird outliers, then our cultural expectations about music (memories of similar fragments of music heard in certain contexts) will color our interpretation.  I would say that this is true for all perceptions of the past, but sounding is quite different from standard methods, so this self-awareness helps identify or express certain critical patterns in (data about) the past. <br><br>  Consider three tools for dubbing data and note how the choice of instrument affects the result, and how to solve this problem by rethinking the data in another tool.  In the end, sounding is not more objective than visualization, so the researcher must be ready to justify his choice and make this choice transparent and reproducible.  (So ‚Äã‚Äãthat no one would think that sounding and algorithmically generated music is something new, I send an interested reader to <a href="http://www.jstor.org/stable/734136">Hedges, 1978</a> ). <br><br>  Each section contains a conceptual introduction, and then a step-by-step guide using samples of archaeological or historical data. <br><br><h1>  Muscular algorithms </h1><br>  There is a wide range of tools for voicing data.  For example, packages for the popular <a href="https://cran.r-project.org/">statistical R environment</a> , such as <a href="https://cran.r-project.org/web/packages/playitbyr/index.html">playitbyR</a> and <a href="https://cran.r-project.org/web/packages/audiolyzR/index.html">AudiolyzR</a> .  But the first one is not supported in the current version of R (the last update was several years ago), and in order to make the second one work properly, a serious configuration of additional software is required. <br><br>  In contrast, the <a href="http://musicalgorithms.org/">Musical algorithms</a> website <a href="http://musicalgorithms.org/">is</a> quite simple to use, it has been running for more than ten years.  Although the source code has not been published, it is a long-term research project on computational music from Jonathan Middleton.  It is currently in the third major version (previous iterations are available on the Internet).  We start with Musicalalgorithms, because it allows us to quickly load and tune our data for the release of the presentation as MIDI files.  Before you begin, be sure to select the <a href="http://musicalgorithms.org/3.0/index.html">third version</a> . <br><br> <a href=""><img src="https://habrastorage.org/webt/wc/ug/vy/wcugvyis0m48fphh68iyksl2aak.png"></a> <br>  <i><font color="gray">Site Musicalgorithms as of February 2, 2016</font></i> <br><br>  Musicalgorithms produces a series of transformations with data.  In the example below (by default on the site), there is only one line of data, although it looks like several lines.  This pattern consists of fields, separated by commas, which are internally separated by spaces. <br><br><pre>  # Of Voices, Text Area Name, Text Area Data
 1, morphBox,
 , areaPitch1,2 7 1 8 2 8 1 8 2 8 4 5 9 0 4 5 2 3 5 3 6 0 2 8
 , dAreaMap1,2 7 1 8 2 8 1 8 2 8 4 5 9 0 4 5 2 3 3 3 6 0 2 8
 , mapArea1.20 69 11 78 20 78 11 78 20 78 40 49 88 1 40 49 20 30 49 30 59 1 20 78
 , dMapArea1,1 5 1 5 1 5 1 5 1 5 3 3 6 0 3 3 1 2 3 2 4 0 1 5
 , so_text_area1.20 69 11 78 20 78 11 78 20 78 40 49 88 1 40 49 20 30 49 30 59 1 20 78 </pre><br>  These figures represent the source data and their conversion.  Sharing a file allows another researcher to repeat the work or continue processing with other tools.  If you start from the beginning, then you need only the source data below (a list of data points): <br><br><pre>  # Of Voices, Text Area Name, Text Area Data
 1, morphBox,
 , areaPitch1,24 72 12 84 21 81 14 81 24 81 44 51 94 01 44 51 24 31 5 43 61 04 21 81 </pre><br>  For us, the key is the 'areaPitch1' field with input data, which are separated by spaces.  Other fields will be filled in during work with various Musicalgorithms settings.  In the above data (for example, 24 72 12 84, etc.), the values ‚Äã‚Äãrepresent the initial calculations of the number of inscriptions in British cities along the Roman road (later we will practice with other data). <br><br> <a href=""><img src="https://habrastorage.org/webt/zl/hh/pu/zlhhpua3byjlwhpt8bwv5fjgwqa.png"></a> <br>  <i><font color="gray">After loading data in the top menu bar, you can select various operations.</font></i>  <i><font color="gray">In the screenshot, hovering the mouse over information displays an explanation of what happens when you select a division operation to scale the data to the selected range of notes.</font></i> <br><br>  Now when viewing various tabs in the interface (duration, height translation, duration translation, scale parameters) various transformations are available.  In the ‚Äúpitch mapping‚Äù, there are a number of mathematical options for translating data to the full 88-key piano keyboard (in linear broadcasting, the <i>average</i> value is transmitted to the average C, that is, 40).  You can also choose the type of scale: minor or major and so on.  At this stage, after selecting various transformations, it is necessary to save the text file.  On the File ‚Üí Play tab, you can upload a midi file.  Your audio program should be able to play midi by default (often the notes of the piano are used by default).  More complex midi tools are assigned in mixer programs, such as GarageBand (Mac) or <a href="https://lmms.io/">LMMS</a> (Windows, Mac, Linux).  However, the use of GarageBand and LMMS is beyond the scope of this guide: a video tutorial on LMMS is available <a href="https://youtu.be/4dYxV3tqTUc">here</a> , and GarageBand tutorials are full on the Internet.  For example, a <a href="http://www.lynda.com/GarageBand-tutorials/Importing-audio-tracks/156620/164050-4.html">great guide</a> to Lynda.com. <br><br>  It happens that for the same points there are several columns of data.  Say, in our example from Britain, we want to voice also a calculation on the types of ceramics for the same cities.  Then you can reload the next row of data, transform and match, and create another MIDI file.  Since GarageBand and LMMS allow you to overlay voices, creating complex music sequences is available. <br><br> <a href=""><img src="https://habrastorage.org/webt/zl/hh/pu/zlhhpua3byjlwhpt8bwv5fjgwqa.png"></a> <br>  <i><font color="gray">Screenshot GarageBand, where midi-files are voiced themes from the diary of John Adams.</font></i>  <i><font color="gray">In the GarageBand (and LMMS) interface, each midi-file is dragged with the mouse to the appropriate place.</font></i>  <i><font color="gray">The toolkit of each midi file (i.e. tracks) is selected in the GarageBand menu.</font></i>  <i><font color="gray">Track labels are changed to reflect keywords in each topic.</font></i>  <i><font color="gray">The green area on the right is a visualization of notes on each track.</font></i>  <i><font color="gray">You can watch this interface in action and listen to music <a href="https://youtu.be/ikqRXtI3JeA">here.</a></font></i> <br><br>  What transformations to use?  If you have two columns of data, these are two voices.  Perhaps in our hypothetical data it makes sense to reproduce the first voice loudly as the main one: in the end, the inscriptions in a certain way ‚Äúspeak‚Äù with us (Roman inscriptions literally refer to passersby: ‚ÄúO you passing by ...‚Äù).  And pottery, perhaps, is a more modest artifact, which can be compared with the lower end of the scale or increase the duration of notes, reflecting its omnipresence among representatives of different classes in this region. <br><br>  <i>There is no single ‚Äúright‚Äù way to translate data to sound</i> , at least not yet.  But even in this simple example, we see how shades of meaning and interpretation appear in the data and their perception. <br><br>  What about time?  Historical data often has a specific date reference.  Thus, it is necessary to take into account the time interval between two data points.  This is where our next tool becomes useful if the data points correspond to each other in space.  We begin to move from sound (data points) to music (the relationship between the points). <br><br><h3>  Practice </h3><br>  In the first column <a href="">of the data set</a> - the number of Roman coins and the amount of other materials from the same cities.  Info taken from the British Museum's Portable Antiquities Scheme.  Processing this data may reveal some aspects of the economic situation along Watling Street, the main route through Roman Britain.  Data points are geographically located from northwest to southeast;  so as we play the sound, we hear movement in space.  Each note represents each stop on the way. <br><br><ol><li>  Open the <a href="">thesonification-roman-data.csv</a> in the spreadsheet.  Copy the first column into a text editor.  Delete line endings so that all data is in one line. <br></li><li>  Add the following information: <br><br><pre>  # Of Voices, Text Area Name, Text Area Data
 1, morphBox,
 , areaPitch1, </pre><br>  ... so your data follows immediately after the last comma (like <a href="">pltcm</a> ).  Save the file with a meaningful name, for example, <code>coinsounds1.csv</code> . <br></li><li>  Go to the website <a href="http://musicalgorithms.org/3.0/index.html">Musical algorithms</a> (third version) and click on the 'Load' button.  In the pop-up window, click the blue 'Load' button and select the file saved in the previous step.  The site will upload your materials and, if successful, will show a green check mark.  If this is not the case, make sure that the values ‚Äã‚Äãare separated by spaces and immediately follow the last comma in the code block.  You can try to download the <a href="">demo file from this guide</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f16/7d6/41b/f167d641bbcf6c3182cbed9f425b6d9d.png"><br>  <i><font color="gray">After clicking 'Load', this dialog box appears on the main screen.</font></i>  <i><font color="gray">Here click 'Load CSV File'.</font></i>  <i><font color="gray">Select your file, it will appear in the field.</font></i>  <i><font color="gray">Then click the 'Load' button below.</font></i> <br></li><li>  Click on 'Pitch Input', and see the values ‚Äã‚Äãof your data.  At the moment, do not select additional parameters on this page (thus, the default values ‚Äã‚Äãare applied). <br></li><li>  Click on 'Duration Input'.  <b>Do not select any options yet</b> .  These options will make a different transformation of your data with a change in the duration of each note.  Don't worry about these options yet, move on. <br></li><li>  Click 'Pitch Mapping'.  This is the most important choice, as it translates (that is, scales) your raw data to the keyboard keys.  Leave the <code>mapping</code> as 'division' (other parameters are modulo translation or logarithmic).  The <code>Range</code> parameter from 1 to 88 uses the full keyboard length of 88 keys;  thus, the lowest value will correspond to the deepest note on the piano, and the highest value will correspond to the highest note.  Instead, you can limit the music to a range around medium C, then enter a range of 25 to 60. The output will change as follows: <code>31,34,34,34,25,28,30,60,28,25,26,26,25,25,60,25,25,38,33,26,25,25,25</code> .  These are not your numbers, but notes on the keyboard. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/29d/052/d9e/29d052d9e1fb42221c3f30ab8712b4ce.png"><br>  <i><font color="gray">Click in the 'Range' field and specify 25. The values ‚Äã‚Äãbelow will change automatically.</font></i>  <i><font color="gray">In the 'to' field, set 60. If you switch to another field, the values ‚Äã‚Äãwill be updated.</font></i> <br></li><li>  Click 'Duration Mapping'.  As with the pitch broadcast, the program here takes the specified time range and uses various mathematical parameters to translate this range into notes.  If you hover over <code>i</code> , you will see which numbers correspond to whole notes, quarters, eighths, and so on.  For now, keep the default values. <br></li><li>  Click 'Scale Options'.  Here we begin to work with what corresponds in a certain sense to the ‚Äúemotional‚Äù aspect.  Usually the major scale is perceived as ‚Äújoyous‚Äù, and the minor scale as ‚Äúsad‚Äù;  For a detailed discussion of this topic, see <a href="http://www.ethanhein.com/wp/2010/scales-and-emotions/">here</a> .  For now, leave the 'scale by: major'.  Leave 'scale' as C. </li></ol><br>  So, we voiced one column of data!  Click 'Save', then 'Save CSV'. <br><br> <a href=""><img src="https://habrastorage.org/webt/bp/ny/7l/bpny7lu30o2ix52diruwtgsmq0y.png"></a> <br>  <i><font color="gray">Dialog box 'Save'</font></i> <br><br>  You will get something like this: <br><br><pre>  # Of Voices, Text Area Name, Text Area Data
 1, morphBox,
 , areaPitch 1.80 128 128 128 1 40 77 495 48 2 21 19 1 1 500 1 3 190 115 13 5 1 3
 , dAreaMap1,2 7 1 8 2 8 1 8 2 8 4 5 9 0 4 5 2 3 3 3 6 0 2
 , mapArea1.31 34 34 34 25 28 30 60 28 25 26 26 25 25 60 25 25 38 33 26 25 25 25
 , dMapArea1,1 5 1 5 1 5 1 5 1 5 3 3 6 0 3 3 1 2 3 2 4 0 1
 , so_text_area1,32 35 35 35 25 28 30 59 28 25 27 27 25 25 59 25 25 39 33 27 25 25 25 </pre><br>  The original data remained in the 'areaPitch1' field, and then the mappings follow.  The site allows you to generate in one MIDI file up to four voices simultaneously.  Depending on which instruments you wish to use later, you can choose to generate one MIDI file at a time.  Let's start the music: click 'Play'.  Here you choose the tempo and instrument.  You can listen to your data in the browser or save it as a MIDI file with the blue 'Save MIDI file' button. <br><br>  Let's go back to the beginning and load both data columns into this template: <br><br><pre>  # Of Voices, Text Area Name, Text Area Data
 2, morphBox,
 , areaPitch1,
 , areaPitch2, </pre><br> <a href=""><img src="https://habrastorage.org/webt/ap/9o/mj/ap9omjzg9k5zg7ja4mxxxtmrw08.png"></a> <br>  <i><font color="gray">Here we are on the page with the 'pitch input' parameters.</font></i>  <i><font color="gray">At the top of the window, enter two voices, now on any page with parameters two windows open for two voices.</font></i>  <i><font color="gray">As before, we load the data in CSV format, but the file needs to be formatted so that the values ‚Äã‚Äãof 'areaPitch1' and 'areaPitch2' are indicated there.</font></i>  <i><font color="gray">Data for the first voice will appear on the left, and the second - on the right.</font></i> <br><br>  If we have a few voices, what to bring to the fore?  Please note that with this approach, our voice acting does not take into account the distance between points in the real world.  If taken into account, it will greatly affect the result.  Of course, the distance is not necessarily tied to geography - it can be tied to time.  The following tool will allow you to clearly indicate this factor when dubbing. <br><br><a name="8"></a><h1>  Briefly about configuring Python </h1><br>  This section of the tutorial will require Python.  If you have not experimented with this language yet, you will have to spend some time <a href="https://programminghistorian.org/lessons/intro-to-bash">getting</a> to <a href="https://programminghistorian.org/lessons/intro-to-bash">know the command line</a> .  See also the <a href="https://programminghistorian.org/lessons/installing-python-modules-pip">module installation quick start guide</a> . <br><br>  On Macs, Python is already installed.  You can check: press COMMAND and space, enter <code>terminal</code> in the search window and click on the terminal application.  The <code>$ type python ‚Äîversion</code> will show which version of Python you have installed.  In the article we work with Python 2.7, the code was not checked in Python 3. <br><br>  Windows users need to install Python themselves: start from <a href="http://docs.python-guide.org/en/latest/starting/install/win/">this page</a> , although everything is a bit more complicated than what is written there.  First, you need to download the <code>.msi</code> file (Python 2.7).  Run the installer, it will be installed in a new directory, for example, <code>C:\Python27\</code> .  Then you need to register this directory in the paths, that is, tell Windows where to look for Python when you start the Python program.  There are several ways to do this.  Perhaps the easiest way to find <code>Powershell</code> on your computer is (enter 'powershell' in the Windows search box).  Open Powershell and in the command line, paste it in its entirety: <br><br><pre>  [Environment] :: SetEnvironmentVariable ("Path", "$ env: Path; C: \ Python27 \; C: \ Python27 \ Scripts \", "User") </pre><br>  If nothing happens by pressing Enter, then the command worked.  To test, open a command prompt (here are <a href="http://www.howtogeek.com/235101/10-ways-to-open-the-command-prompt-in-windows-10/">10 ways to do this</a> ) and type <code>python --version</code> .  A response should be displayed indicating <code>Python 2.7.10</code> or a similar version. <br><br>  The last piece of the puzzle is a program called <code>Pip</code> .  Mac users can install it with a command in the <code>sudo easy_install pip</code> terminal.  Windows users will have a little more difficult.  First, right-click and save the file by <a href="https://bootstrap.pypa.io/get-pip.py">this link</a> (if you simply click on the link, the code <i>get-pip.py</i> opens in the browser).  Keep it somewhere handy.  Open a command prompt in the directory where you saved <code>get-pip.py</code> .  Then type <code>python get-pip.py</code> on the command line. <br><br>  When you have Python code that you want to run, paste it into a text editor and save the file with the extension <code>.py</code> .  This is a text file, but the file extension tells the computer to use to interpret Python.  It is launched from the command line, where the name of the interpreter is first specified, and then the file name: <code>python my-cool-script.py</code> . <br><br><a name="9"></a><h1>  MIDITime </h1><br>  MIDITime is a Python package developed by <a href="https://www.revealnews.org/">Reveal News</a> (formerly known as the Center for Investigative Journalism).  Repository <a href="https://github.com/cirlabs/miditime">on Github</a> .  MIDITime is designed specifically for processing time series (that is, a sequence of observations collected over time). <br><br>  While Musicalgorithms has a more or less intuitive interface, here the advantage is open source.  More importantly, the previous tool does not know how to take into account data taking into account historical time.  MIDITime allows you to cluster information on this factor. <br><br>  Suppose we have a historical diary to which the <a href="https://programminghistorian.org/lessons/topic-modeling-and-mallet">thematic model has</a> been applied.  The resulting output may contain diary entries in the form of lines, and in the columns will be the percentage contribution of each topic.  In this case, <i>listening to the</i> values ‚Äã‚Äãwill help to understand such patterns of thinking from the diary, which can not be transferred as a graph.  Emissions or repetitive musical patterns that are not visible on the chart are immediately noticeable. <br><br><a name="10"></a><h3>  MIDITime installation </h3><br>  Installation with one <a href="https://programminghistorian.org/lessons/installing-python-modules-pip">pip</a> command: <br><br><pre> <code class="bash hljs">$ pip install miditime</code> </pre> <br>  for poppies; <br><br><pre> <code class="bash hljs">$ sudo pip install miditime</code> </pre> <br>  under Linux; <br><br><pre> <code class="bash hljs">&gt; python pip install miditime</code> </pre> <br>  under Windows (if the instruction does not work, you can try <a href="https://sites.google.com/site/pydatalog/python/pip-for-windows">this helper program</a> to install Pip). <br><br><a name="11"></a><h3>  Practice </h3><br>  Consider an example script.  Open a text editor, copy and paste this code: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/python from miditime.miditime import MIDITime # </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">NOTE:</span></span></span><span class="hljs-comment"> this import works at least as of v1.1.3; for older versions or forks of miditime, you may need to use # from miditime.MIDITime import MIDITime # Instantiate the class with a tempo (120bpm is the default) and an output file destination. mymidi = MIDITime(120, 'myfile.mid') # Create a list of notes. Each note is a list: [time, pitch, attack, duration] midinotes = [ [0, 60, 200, 3], #At 0 beats (the start), Middle C with attack 200, for 3 beats [10, 61, 200, 4] #At 10 beats (12 seconds from start), C#5 with attack 200, for 4 beats ] # Add a track with those notes mymidi.add_track(midinotes) # Output the .mid file mymidi.save_midi()</span></span></code> </pre> <br>  Save the script as <code>music1.py</code> .  In a terminal or command line, run it: <br><br><pre> <code class="bash hljs">$ python music1.py</code> </pre> <br>  A new file <code>myfile.mid</code> will be created in the <code>myfile.mid</code> .  You can open it for listening using Quicktime or Windows Media Player (and add tools to GarageBand or <a href="https://lmms.io/">LMMS there</a> ). <br><br>  <code>Music1.py</code> imports miditime (do not forget to install it before running the script: <code>pip install miditime</code> ).  Then sets the pace.  All notes are listed separately, where the first number is the start time of the playback, the pitch (i.e., the note itself!), How strongly or rhythmically the note (attack) is played and its duration.  Then the notes are recorded on the track, and the track itself is recorded in the file <code>myfile.mid</code> . <br><br>  Play with the script, add more notes.  Here are the notes for the song 'Baa Baa Black Sheep': <br><br><pre>  D, D, A, A, B, B, B, B, A
 Baa, Baa, black, sheep, have, you, any, wool? </pre><br>  Can you write instructions for the computer to play the melody (here's a <a href="http://www.electronics.dit.ie/staff/tscarff/Music_technology/midi/midi_note_numbers_for_octaves.htm">chart</a> to help)? <br><br>  <b>By the way</b> .  There is a special text file format for describing music called <a href="">ABC Notation</a> .  It is beyond the scope of this article, but a scoring script can be written, say, in spreadsheets, comparing the values ‚Äã‚Äãof notes in ABC notation (if you have ever used the IF - THEN construct in Excel, you have an idea how this is done) and then through sites <a href="http://trillian.mit.edu/~jc/music/abc/ABCcontrib.html">like this, the</a> ABC notation is converted to a .mid file. <br><br><a name="12"></a><h3>  Upload your own data </h3><br>  <a href="">This file</a> contains a selection of the thematic model diaries of John Adams for the <a href="http://themacroscope.org/">Macroscope</a> site.  Here only the strongest signals were left, rounding up the values ‚Äã‚Äãin the columns to two decimal places.  To insert this data into the Python script, you need to format it in a special way.  The hardest thing is the date field. <br><br>  <i>For this lesson, we leave the variable names and the rest unchanged from the script with an example.</i>  <i>The sample is designed to process earthquake data;</i>  <i>therefore, here ‚Äúmagnitude‚Äù can be represented as our ‚Äúcontribution of the theme‚Äù.</i> <br><br><pre>  my_data = [
     {'event_date': &lt;datetime object&gt;, 'magnitude': 3.4},
     {'event_date': &lt;datetime object&gt;, 'magnitude': 3.2},
     {'event_date': &lt;datetime object&gt;, 'magnitude': 3.6},
     {'event_date': &lt;datetime object&gt;, 'magnitude': 3.0},
     {'event_date': &lt;datetime object&gt;, 'magnitude': 5.6},
     {'event_date': &lt;datetime object&gt;, 'magnitude': 4.0}
 ] </pre><br>  For formatting data, you can use regular expressions, and even easier - spreadsheets.  Copy the element with the theme contribution value to the new sheet and leave the columns left and right.  In the example below, I placed it in column D, and then filled the rest: <br><br><div class="scrollable-table"><table><thead><tr><th></th><th>  A </th><th>  B </th><th>  C </th><th>  D </th><th>  E </th></tr></thead><tbody><tr><td>  one </td><td>  {'event_date': datetime </td><td>  (1753,6,8) </td><td>  , 'magnitude': </td><td>  0.0024499630 </td><td>  }, </td></tr><tr><td>  2 </td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>  3 </td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></div><br>  Then copy and paste the immutable elements by filling in the entire column.  The element with the date must be in the format (year, month, day).  After filling the table, it can be copied and pasted into a text editor, making it part of the <code>my_data</code> array, for example: <br><br><pre>  my_data = [
 {'event_date': datetime (1753,6,8), 'magnitude': 0.0024499630},
 {'event_date': datetime (1753,6,9), 'magnitude': 0.0035766320},
 {'event_date': datetime (1753,6,10), 'magnitude': 0.0022171550},
 {'event_date': datetime (1753,6,11), 'magnitude': 0.0033220150},
 {'event_date': datetime (1753,6,12), 'magnitude': 0.0046445900},
 {'event_date': datetime (1753,6,13), 'magnitude': 0.0035766320},
 {'event_date': datetime (1753,6,14), 'magnitude': 0.0042241550}
 ] </pre><br>  Note that there is no comma at the end of the last line. <br><br>  The final script will look something like this if you use the example from the Miditime page itself (the code snippets below are interrupted by comments, but they should be pasted together into a text editor as one file): <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> miditime.MIDITime <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MIDITime <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> datetime <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> datetime <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random mymidi = MIDITime(<span class="hljs-number"><span class="hljs-number">108</span></span>, <span class="hljs-string"><span class="hljs-string">'johnadams1.mid'</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br>  Values ‚Äã‚Äãafter MIDITime are set as <code>MIDITime(108, 'johnadams1.mid', 3, 4, 1)</code> , here: <br><br><ul><li>  beats per minute (108), <br></li><li>  output file ('johnadams1.mid'), <br></li><li>  the number of seconds in the music to represent one year in history (3 seconds per calendar year, so the diary entries for 50 years are scaled to a melody of 50 √ó 3 seconds, that is, two and a half minutes), <br></li><li>  the basic octave for music (the average C is usually represented as C5, so here 4 corresponds to an octave one lower than the reference), <br></li><li>  and the number of octaves to match the heights. </li></ul><br>  Now we transfer the data to the script by loading it into the array <code>my_data</code> : <br><br><pre> <code class="python hljs">my_data = [ {<span class="hljs-string"><span class="hljs-string">'event_date'</span></span>: datetime(<span class="hljs-number"><span class="hljs-number">1753</span></span>,<span class="hljs-number"><span class="hljs-number">6</span></span>,<span class="hljs-number"><span class="hljs-number">8</span></span>), <span class="hljs-string"><span class="hljs-string">'magnitude'</span></span>:<span class="hljs-number"><span class="hljs-number">0.0024499630</span></span>}, {<span class="hljs-string"><span class="hljs-string">'event_date'</span></span>: datetime(<span class="hljs-number"><span class="hljs-number">1753</span></span>,<span class="hljs-number"><span class="hljs-number">6</span></span>,<span class="hljs-number"><span class="hljs-number">9</span></span>), <span class="hljs-string"><span class="hljs-string">'magnitude'</span></span>:<span class="hljs-number"><span class="hljs-number">0.0035766320</span></span>},</code> </pre> <br>  ... here we insert all the data and do not forget to <b>delete the comma</b> at the end of the last line of <code>event_date</code> , and after the data put the terminating bracket on a separate line: <br><br><pre> <code class="python hljs">{<span class="hljs-string"><span class="hljs-string">'event_date'</span></span>: datetime(<span class="hljs-number"><span class="hljs-number">1753</span></span>,<span class="hljs-number"><span class="hljs-number">6</span></span>,<span class="hljs-number"><span class="hljs-number">14</span></span>), <span class="hljs-string"><span class="hljs-string">'magnitude'</span></span>:<span class="hljs-number"><span class="hljs-number">0.0042241550</span></span>} ]</code> </pre> <br>  then insert the timing: <br><br><pre> <code class="python hljs">my_data_epoched = [{<span class="hljs-string"><span class="hljs-string">'days_since_epoch'</span></span>: mymidi.days_since_epoch(d[<span class="hljs-string"><span class="hljs-string">'event_date'</span></span>]), <span class="hljs-string"><span class="hljs-string">'magnitude'</span></span>: d[<span class="hljs-string"><span class="hljs-string">'magnitude'</span></span>]} <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> d <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> my_data] my_data_timed = [{<span class="hljs-string"><span class="hljs-string">'beat'</span></span>: mymidi.beat(d[<span class="hljs-string"><span class="hljs-string">'days_since_epoch'</span></span>]), <span class="hljs-string"><span class="hljs-string">'magnitude'</span></span>: d[<span class="hljs-string"><span class="hljs-string">'magnitude'</span></span>]} <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> d <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> my_data_epoched] start_time = my_data_timed[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-string"><span class="hljs-string">'beat'</span></span>]</code> </pre> <br>  This code sets the timing between different diary entries;  if the diaries are close to each other in time, the corresponding notes will also be closer.  Finally, we determine how the data is compared to the height.  The initial values ‚Äã‚Äãare indicated in percents in the range from 0.01 (i.e. 1%) to 0.99 (99%), so <code>scale_pct</code> set between 0 and 1. If we have no percentages, then we use the lowest and the highest values.  Thus, we insert the following code: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">mag_to_pitch_tuned</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(magnitude)</span></span></span><span class="hljs-function">:</span></span> scale_pct = mymidi.linear_scale_pct(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, magnitude) <span class="hljs-comment"><span class="hljs-comment"># Pick a range of notes. This allows you to play in a key. c_major = ['C', 'C#', 'D', 'D#', 'E', 'E#', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B', 'B#'] #Find the note that matches your data point note = mymidi.scale_to_note(scale_pct, c_major) #Translate that note to a MIDI pitch midi_pitch = mymidi.note_to_midi_pitch(note) return midi_pitch note_list = [] for d in my_data_timed: note_list.append([ d['beat'] - start_time, mag_to_pitch_tuned(d['magnitude']), random.randint(0,200), # attack random.randint(1,4) # duration, in beats ])</span></span></code> </pre> <br>  and the last fragment to save the data to a file: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Add a track with those notes mymidi.add_track(midinotes) # Output the .mid file mymidi.save_midi()</span></span></code> </pre> <br>  Save this file with a new name and the extension <code>.py</code> . <br><br>  For each column in the source data we make a unique script and do not forget <b>to change the name of the output file</b> !  You can then upload individual midi files to GarageBand or LMMS for instrumentation.  Here is the complete <a href="https://www.youtube.com/watch%3Fv%3DikqRXtI3JeA">diary of John Adams</a> . <br><br><a name="13"></a><h1>  Sonic pi </h1><br>  Processing unique midi in GarageBand or another music editor means moving from simple voice over to the art of music.  This final section of the article is not a complete guide to using the <a href="http://sonic-pi.net/">Sonic Pi</a> , but rather an introduction to an environment that allows you to encode and play back data in the form of music in real time (see <a href="https://www.youtube.com/watch%3Fv%3DoW-3HVOeUQA">video</a> for an example of coding with real-time <a href="https://www.youtube.com/watch%3Fv%3DoW-3HVOeUQA">playback</a> ).  The tutorials built into the program will show how to use the computer as a musical instrument (you enter Ruby code into the built-in editor, and the interpreter loses the result immediately). <br><br>  Why do you need it?  As you can see from this tutorial, as data is played out, you begin to make decisions about how to translate data into sound.  These decisions reflect implicit or explicit decisions about which data is relevant.  There is a continuum of ‚Äúobjectivity,‚Äù if you will.  On the one hand, the historical data that has been voiced, on the other hand, the view of the past is as exciting and personal as any well-done public lecture.  Voicing allows you to actually hear the data stored in the documents: it is a kind of public story.  The musical performance of our data ... just imagine! <br><br>  Here I offer a code snippet for importing data, which is simply a list of values ‚Äã‚Äãsaved as csv.  Thanks to the librarian of George Washington University Laura Vrubel, who posted her experiments on dubbing library operations on <a href="https://gist.github.com/lwrubel">gist.github.com</a> . <br><br>  In <a href="">this sample</a> (thematic model generated from <a href="http://puffin.creighton.edu/jesuit/relations/">the Jesuit Relation</a> ) there are two themes.  The first line contains the headings 'topic1' and 'topic2'. <br><br><a name="14"></a><h3>  Practice </h3><br>  Follow the built-in guides of the Sonic Pi until you get familiar with the interface and features (all of these tutorials are collected <a href="https://gist.github.com/jwinder/e59be201082cca694df9">here</a> ; you can also listen to <a href="https://devchat.tv/ruby-rogues/215-rr-sonic-pi-with-sam-aaron">an interview</a> with Sam Aaron, the creator of Sonic Pi).  Then in the new buffer (editor window) copy the following code (again, separate fragments should be collected in one script): <br><br><pre> <code class="ruby hljs"><span class="hljs-keyword"><span class="hljs-keyword">require</span></span> <span class="hljs-string"><span class="hljs-string">'csv'</span></span> data = CSV.parse(File.read(<span class="hljs-string"><span class="hljs-string">"/path/to/your/directory/data.csv"</span></span>), {<span class="hljs-symbol"><span class="hljs-symbol">:headers</span></span> =&gt; <span class="hljs-literal"><span class="hljs-literal">true</span></span>, <span class="hljs-symbol"><span class="hljs-symbol">:header_converters</span></span> =&gt; <span class="hljs-symbol"><span class="hljs-symbol">:symbol</span></span>}) use_bpm <span class="hljs-number"><span class="hljs-number">100</span></span></code> </pre> <br>  Remember that <code>path/to/your/directory/</code> is the actual location of your data on the computer.  Make sure the file is really called <code>data.csv</code> , or edit this line in the code. <br><br>  Now let's load this data into the musical composition: <br><br><pre> <code class="ruby hljs"><span class="hljs-comment"><span class="hljs-comment">#this bit of code will run only once, unless you comment out the line with #'live_loop', and also comment out the final 'end' at the bottom # of this code block #'commenting out' means removing the # sign. # live_loop :jesuit do data.each do |line| topic1 = line[:topic1].to_f topic2 = line[:topic2].to_f use_synth :piano play topic1*100, attack: rand(0.5), decay: rand(1), amp: rand(0.25) use_synth :piano play topic2*100, attack: rand(0.5), decay: rand(1), amp: rand(0.25) sleep (0.5) end</span></span></code> </pre> <br>  The first few rows load data columns;  then we indicate which sample of sound we want to use (piano), and then indicate to play the first theme (topic1) according to the specified criteria: for the strength of the note playing (attack) a random value less than 0.5 is selected;  for decay (decay) - random value less than 1;  for amplitude, a random value of less than 0.25. <br><br>  See the line with multiplication by one hundred ( <code>*100</code> )?  It takes our data value (decimal) and turns it into an integer.  In this passage, the number is directly equivalent to a note.  If the lowest note is 88, and the highest is 1, then this approach is a bit problematic: we actually do not display any pitch here!  In this case, you can use Musicalgorithms to display the height, and then transfer these values ‚Äã‚Äãback to Sonic Pi.  In addition, since this code is more or less standard Ruby, you can apply the usual methods of data normalization, and then make a linear comparison of your values ‚Äã‚Äãwith a range of 1‚Äì88.  For a start, take a good look at <a href="https://github.com/stevelloyd/Learn-sonification-with-Sonic-Pi">Steve Lloyd's work</a> on dubbing weather data with Sonic Pi. <br><br>  And the last thing to note here: the value of 'rand' (random) allows you to add a bit of ‚Äúhumanity‚Äù to the music in terms of dynamics.  Do the same for topic2. <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">You can also specify the rhythm (beats per minute), loops, samples, and other effects that Sonic Pi supports. </font><font style="vertical-align: inherit;">The location of the code affects the playback: for example, if placed before the above data block, it will play first. </font><font style="vertical-align: inherit;">For example, if you </font></font><code>use_bpm 100</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">insert the following </font><font style="vertical-align: inherit;">after the line </font><font style="vertical-align: inherit;">:</font></font><br><br><pre> <code class="ruby hljs"><span class="hljs-comment"><span class="hljs-comment">#intro bit sleep 2 sample :ambi_choir, attack: 2, sustain: 4, rate: 0.25, release: 1 sleep 6</span></span></code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">... then get a little musical introduction. </font><font style="vertical-align: inherit;">The program waits 2 seconds, plays the sample 'ambi_choir', then waits another 6 seconds before playing our data. </font><font style="vertical-align: inherit;">If you want to add a little sinister drum throughout the melody, put the following bit (before your own data):</font></font><br><br><pre> <code class="ruby hljs"><span class="hljs-comment"><span class="hljs-comment">#bit that keeps going throughout the music live_loop :boom do with_fx :reverb, room: 0.5 do sample :bd_boom, rate: 1, amp: 1 end sleep 2 end</span></span></code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The code is pretty clear: a looped sample of 'bd_boom' with a reverberation sound effect at a certain speed. </font><font style="vertical-align: inherit;">Pause between cycles of 2 seconds. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As for ‚Äúreal-time coding,‚Äù this means that you can make changes to the code </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">while simultaneously playing back those changes</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Do not like what you hear? </font><font style="vertical-align: inherit;">Immediately change the code! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The study of Sonic Pi can be started from </font></font><a href="https://www.miskatonic.org/music/access2015/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">this seminar</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">See also the </font></font><a href="http://library.gwu.edu/scholarly-technology-group/posts/sound-library-work"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">report of Laura Vrubel</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> on attending the seminar, which also describes her work in this area and the work of her colleagues.</font></font><br><br><a name="15"></a><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Nothing new under the sun </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And I repeat again: it is not necessary to think that we, with our algorithmic approach, are at the forefront of science. In 1978, a </font></font><a href="http://www.jstor.org/stable/734136"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">scientific article</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> was published </font><font style="vertical-align: inherit;">on the 18th century ‚Äúmusical dice‚Äù, where bone rolls determined the recombination of previously written pieces of music. Robin Newman </font></font><a href="https://rbnrpi.wordpress.com/project-list/mozart-dice-generated-waltz-revisited-with-sonic-pi/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">studied and coded some of these games for Sonic Pi</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . For Newman musical notation uses a tool that can be described as Markdown + Pandoc, and for conversion into notes - </font></font><a href="http://www.lilypond.org/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lilypond</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . So all the themes on our blog </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The Programming Historian have a</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> long history!</font></font><br><br><a name="16"></a><h1>  Conclusion </h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">When dubbing, we see that our data often reflects not so much history as its interpretation in our performance. This is partly due to the novelty and artistic nature necessary to translate data into sound. But it also strongly distinguishes sound interpretation from traditional visualization. Maybe the generated sounds will never rise to the level of "music"; but if they help change our understanding of the past and influence others, then the effort is worth it. As Trevor Owens would say, ‚ÄúSounding is a </font></font><a href="http://www.trevorowens.org/2012/11/discovery-and-justification-are-different-notes-on-sciencing-the-humanities/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">discovery of a new, not a justification of the known</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .‚Äù</font></font><br><br><a name="17"></a><h1>  Terms </h1><br><ul><li> <b>MIDI</b> :    .      ,       (  ).        .  MIDI-    ,     . <br></li><li> <b>MP3</b> :     <i></i>   ,      . <br></li><li> <b></b> :    ( C  . .) <br></li><li> <b></b> :      <br></li><li> <b></b> :     ( , ,   . .) <br></li><li> <b>   </b> :          <br></li><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Amplitude</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : roughly speaking, the volume of the note</font></font></li></ul></div><p>Source: <a href="https://habr.com/ru/post/454318/">https://habr.com/ru/post/454318/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../45431/index.html">Ventures: funds, incubators and business angels</a></li>
<li><a href="../454310/index.html">Generalized closure in Rust</a></li>
<li><a href="../454312/index.html">How to use the jQuery library with the Angular framework (when you really need it)</a></li>
<li><a href="../454314/index.html">10 principles of object-oriented programming that every developer should know</a></li>
<li><a href="../454316/index.html">In the footsteps of the industrial ninja: we invite you to participate in an online competition for industrial safety</a></li>
<li><a href="../454320/index.html">Labor Market Analysts and Data Scientists</a></li>
<li><a href="../454322/index.html">There is an opinion: the DANE technology for browsers failed</a></li>
<li><a href="../454324/index.html">Pointers in Python: what‚Äôs the point?</a></li>
<li><a href="../454326/index.html">In addition to Moore - who else formulated the laws of scaling computing systems</a></li>
<li><a href="../45433/index.html">YouTube has become widescreen.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>