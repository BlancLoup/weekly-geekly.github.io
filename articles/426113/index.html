<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Google News and Leo Tolstoy: Visualizing Vector Representations of Words with t-SNE</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Each of us perceives the texts in their own way, whether it is news on the Internet, poetry or classic novels. The same applies to algorithms and meth...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Google News and Leo Tolstoy: Visualizing Vector Representations of Words with t-SNE</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/6c/ux/7m/6cux7mvmp3phb8d8efjwmqrb_yc.gif"><br><br>  Each of us perceives the texts in their own way, whether it is news on the Internet, poetry or classic novels.  The same applies to algorithms and methods of machine learning, which, as a rule, perceive texts in mathematical form, in the form of a multidimensional vector space. <br><br>  The article is devoted to the visualization of multidimensional vector representations of words using t-SNE of the calculated Word2Vec.  Visualization will allow you to more fully understand how Word2Vec works and how to interpret the relationship between word vectors before using them further in neural networks and other machine learning algorithms.  The article focuses on visualization, further research and data analysis are not considered.  We use Google News articles and L.N.'s classic works as a data source.  Tolstoy.  The code will be written in Python in Jupyter Notebook. <br><a name="habracut"></a><br><h1>  T-distributed Stochastic Neighbor Embedding </h1><br>  T-SNE is a machine learning algorithm for data visualization based on the nonlinear dimension reduction method, which is described in detail in the original article [1] and on <a href="https://habr.com/post/267041/">Habr√©</a> .  The basic t-SNE principle of operation is to reduce pairwise distances between points while maintaining their relative position.  In other words, the algorithm maps multidimensional data to a space of a lower dimension, while maintaining the structure of the neighborhood of points. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1>  Vector representations of words and Word2Vec </h1><br>  First of all, we need to present the words in vector form.  For this task, I chose the Word2Vec distribution semantics utility, which is designed to display the semantic meaning of words in a vector space.  Word2Vec finds relationships between words according to the assumption that semantically close words occur in similar contexts.  More information about Word2Vec can be found in the original article [2], as well as <a href="https://habr.com/company/ods/blog/329410/">here</a> and <a href="https://habr.com/post/249215/">here</a> . <br><br>  As input, we will take articles from Google News and the novels of L.N.  Tolstoy.  In the first case, we will use the veterans trained by Google News (about 100 billion words), published by Google <a href="https://code.google.com/archive/p/word2vec/">on the project page</a> . <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gensim model = gensim.models.KeyedVectors.load_word2vec_format(<span class="hljs-string"><span class="hljs-string">'GoogleNews-vectors-negative300.bin'</span></span>, binary=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br>  In addition to the pre-trained vectors using the Gensim library [3], we will train another model on the texts of L.N.  Tolstoy.  Since Word2Vec accepts an array of sentences as input, we use the pre-trained Punkt Sentence Tokenizer model from the NLTK package to automatically break the text into sentences.  The model for the Russian language can be downloaded <a href="https://github.com/mhq/train_punkt">from here</a> . <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> codecs <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(text)</span></span></span><span class="hljs-function">:</span></span> text = re.sub(<span class="hljs-string"><span class="hljs-string">'[^a-zA-Z--1-9]+'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) text = re.sub(<span class="hljs-string"><span class="hljs-string">' +'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text.strip() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">prepare_for_w2v</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename_from, filename_to, lang)</span></span></span><span class="hljs-function">:</span></span> raw_text = codecs.open(filename_from, <span class="hljs-string"><span class="hljs-string">"r"</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'windows-1251'</span></span>).read() <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename_to, <span class="hljs-string"><span class="hljs-string">'w'</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> sentence <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> nltk.sent_tokenize(raw_text, lang): print(preprocess_text(sentence.lower()), file=f)</code> </pre><br>  Next, using the Gensim library, we will train the Word2Vec model with the following parameters: <br><br><ul><li>  <i>size = 200</i> is the dimension of the attribute space; </li><li>  <i>window = 5</i> - the number of words from the context that the algorithm analyzes; </li><li>  <i>min_count = 5</i> - the word must occur at least five times for the model to take it into account. </li></ul><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> multiprocessing <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> gensim.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Word2Vec <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train_word2vec</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> data = gensim.models.word2vec.LineSentence(filename) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Word2Vec(data, size=<span class="hljs-number"><span class="hljs-number">200</span></span>, window=<span class="hljs-number"><span class="hljs-number">5</span></span>, min_count=<span class="hljs-number"><span class="hljs-number">5</span></span>, workers=multiprocessing.cpu_count())</code> </pre><br><h1>  Visualizing vector representations of words using t-SNE </h1><br>  T-SNE is extremely useful for visualizing the similarities between objects in a multidimensional space.  With the increase in the amount of data, it becomes more and more difficult to build a visual graph, therefore, in practice, similar words are combined into groups for further visualization.  Take, for example, a few words from a dictionary of a pre-Google2 Word2Vec model. <br><br><pre> <code class="python hljs">keys = [<span class="hljs-string"><span class="hljs-string">'Paris'</span></span>, <span class="hljs-string"><span class="hljs-string">'Python'</span></span>, <span class="hljs-string"><span class="hljs-string">'Sunday'</span></span>, <span class="hljs-string"><span class="hljs-string">'Tolstoy'</span></span>, <span class="hljs-string"><span class="hljs-string">'Twitter'</span></span>, <span class="hljs-string"><span class="hljs-string">'bachelor'</span></span>, <span class="hljs-string"><span class="hljs-string">'delivery'</span></span>, <span class="hljs-string"><span class="hljs-string">'election'</span></span>, <span class="hljs-string"><span class="hljs-string">'expensive'</span></span>, <span class="hljs-string"><span class="hljs-string">'experience'</span></span>, <span class="hljs-string"><span class="hljs-string">'financial'</span></span>, <span class="hljs-string"><span class="hljs-string">'food'</span></span>, <span class="hljs-string"><span class="hljs-string">'iOS'</span></span>, <span class="hljs-string"><span class="hljs-string">'peace'</span></span>, <span class="hljs-string"><span class="hljs-string">'release'</span></span>, <span class="hljs-string"><span class="hljs-string">'war'</span></span>] embedding_clusters = [] word_clusters = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> keys: embeddings = [] words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> similar_word, _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> model.most_similar(word, topn=<span class="hljs-number"><span class="hljs-number">30</span></span>): words.append(similar_word) embeddings.append(model[similar_word]) embedding_clusters.append(embeddings) word_clusters.append(words)</code> </pre> <br><img src="https://habrastorage.org/webt/uc/1k/o6/uc1ko6efgx_d-wnbwolgdabifl8.gif"><br>  <i>Figure 1. Groups of similar words from Google News with different preplexity parameter values.</i> <br><br>  Next, go to the most remarkable fragment of the article - the configuration of t-SNE.  Here, first of all, attention should be paid to the following hyperparameters: <br><br><ul><li>  <i>n_components</i> - the number of components, i.e., the dimension of the value space; </li><li>  <i>perplexity</i> is perplexion, the value of which in t-SNE can be equated to the effective number of neighbors.  It is related to the number of nearest neighbors, which is used in other models that study on the basis of manifolds (see picture above).  Its value is recommended [1] to set in the range of 5-50; </li><li>  <i>init</i> is the type of initial initialization of vectors. </li></ul><br><pre> <code class="python hljs">tsne_model_en_2d = TSNE(perplexity=<span class="hljs-number"><span class="hljs-number">15</span></span>, n_components=<span class="hljs-number"><span class="hljs-number">2</span></span>, init=<span class="hljs-string"><span class="hljs-string">'pca'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">3500</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">32</span></span>) embedding_clusters = np.array(embedding_clusters) n, m, k = embedding_clusters.shape embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, <span class="hljs-number"><span class="hljs-number">2</span></span>)</code> </pre> <br>  Below is a script for constructing two-dimensional graphics using Matplotlib, one of the most popular libraries for visualizing data in Python. <br><br><img src="https://habrastorage.org/webt/34/9y/7h/349y7hxuanvvttqfxkpb48j4n-q.png"><br>  <i>Figure 2. Groups of similar words from Google News (preplexity = 15).</i> <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.manifold <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> TSNE <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.cm <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> cm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np % matplotlib inline <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tsne_plot_similar_words</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(labels, embedding_clusters, word_clusters, a=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.7</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">16</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) colors = cm.rainbow(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, len(labels))) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> label, embeddings, words, color <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(labels, embedding_clusters, word_clusters, colors): x = embeddings[:,<span class="hljs-number"><span class="hljs-number">0</span></span>] y = embeddings[:,<span class="hljs-number"><span class="hljs-number">1</span></span>] plt.scatter(x, y, c=color, alpha=a, label=label) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(words): plt.annotate(word, alpha=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, xy=(x[i], y[i]), xytext=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), textcoords=<span class="hljs-string"><span class="hljs-string">'offset points'</span></span>, ha=<span class="hljs-string"><span class="hljs-string">'right'</span></span>, va=<span class="hljs-string"><span class="hljs-string">'bottom'</span></span>, size=<span class="hljs-number"><span class="hljs-number">8</span></span>) plt.legend(loc=<span class="hljs-number"><span class="hljs-number">4</span></span>) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) plt.savefig(<span class="hljs-string"><span class="hljs-string">"f/.png"</span></span>, format=<span class="hljs-string"><span class="hljs-string">'png'</span></span>, dpi=<span class="hljs-number"><span class="hljs-number">150</span></span>, bbox_inches=<span class="hljs-string"><span class="hljs-string">'tight'</span></span>) plt.show() tsne_plot_similar_words(keys, embeddings_en_2d, word_clusters)</code> </pre> <br>  Sometimes it is necessary to build not separate clusters of words, but the entire dictionary.  For this purpose, let's analyze the ‚ÄúAnna Karenina‚Äù, the great history of passion, treason, tragedy and redemption. <br><br><pre> <code class="python hljs">prepare_for_w2v(<span class="hljs-string"><span class="hljs-string">'data/Anna Karenina by Leo Tolstoy (ru).txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'train_anna_karenina_ru.txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'russian'</span></span>) model_ak = train_word2vec(<span class="hljs-string"><span class="hljs-string">'train_anna_karenina_ru.txt'</span></span>) words = [] embeddings = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> list(model_ak.wv.vocab): embeddings.append(model_ak.wv[word]) words.append(word) tsne_ak_2d = TSNE(n_components=<span class="hljs-number"><span class="hljs-number">2</span></span>, init=<span class="hljs-string"><span class="hljs-string">'pca'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">3500</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">32</span></span>) embeddings_ak_2d = tsne_ak_2d.fit_transform(embeddings)</code> </pre><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tsne_plot_2d</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(label, embeddings, words=[], a=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">16</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) colors = cm.rainbow(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) x = embeddings[:,<span class="hljs-number"><span class="hljs-number">0</span></span>] y = embeddings[:,<span class="hljs-number"><span class="hljs-number">1</span></span>] plt.scatter(x, y, c=colors, alpha=a, label=label) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(words): plt.annotate(word, alpha=<span class="hljs-number"><span class="hljs-number">0.3</span></span>, xy=(x[i], y[i]), xytext=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), textcoords=<span class="hljs-string"><span class="hljs-string">'offset points'</span></span>, ha=<span class="hljs-string"><span class="hljs-string">'right'</span></span>, va=<span class="hljs-string"><span class="hljs-string">'bottom'</span></span>, size=<span class="hljs-number"><span class="hljs-number">10</span></span>) plt.legend(loc=<span class="hljs-number"><span class="hljs-number">4</span></span>) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) plt.savefig(<span class="hljs-string"><span class="hljs-string">"hhh.png"</span></span>, format=<span class="hljs-string"><span class="hljs-string">'png'</span></span>, dpi=<span class="hljs-number"><span class="hljs-number">150</span></span>, bbox_inches=<span class="hljs-string"><span class="hljs-string">'tight'</span></span>) plt.show() tsne_plot_2d(<span class="hljs-string"><span class="hljs-string">'Anna Karenina by Leo Tolstoy'</span></span>, embeddings_ak_2d, a=<span class="hljs-number"><span class="hljs-number">0.1</span></span>)</code> </pre> <br><img src="https://habrastorage.org/webt/j5/hn/82/j5hn8285nih2kwlop_badd2lzlk.png"><br><br><img src="https://habrastorage.org/webt/x6/jc/i7/x6jci7vka7-efczouqxpiqueisq.png"><br>  <i>Figure 3. Visualization of the dictionary of the Word2Vec-model trained in the novel ‚ÄúAnna Karenina‚Äù.</i> <br><br>  The picture can become even more informative if we use three-dimensional space.  Take a look at War and Peace, one of the main novels of world literature. <br><br><pre> <code class="python hljs">prepare_for_w2v(<span class="hljs-string"><span class="hljs-string">'data/War and Peace by Leo Tolstoy (ru).txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'train_war_and_peace_ru.txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'russian'</span></span>) model_wp = train_word2vec(<span class="hljs-string"><span class="hljs-string">'train_war_and_peace_ru.txt'</span></span>) words_wp = [] embeddings_wp = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> list(model_wp.wv.vocab): embeddings_wp.append(model_wp.wv[word]) words_wp.append(word) tsne_wp_3d = TSNE(perplexity=<span class="hljs-number"><span class="hljs-number">30</span></span>, n_components=<span class="hljs-number"><span class="hljs-number">3</span></span>, init=<span class="hljs-string"><span class="hljs-string">'pca'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">3500</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">12</span></span>) embeddings_wp_3d = tsne_wp_3d.fit_transform(embeddings_wp)</code> </pre><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mpl_toolkits.mplot3d <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Axes3D <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tsne_plot_3d</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(title, label, embeddings, a=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> fig = plt.figure() ax = Axes3D(fig) colors = cm.rainbow(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) plt.scatter(embeddings[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], embeddings[:, <span class="hljs-number"><span class="hljs-number">1</span></span>], embeddings[:, <span class="hljs-number"><span class="hljs-number">2</span></span>], c=colors, alpha=a, label=label) plt.legend(loc=<span class="hljs-number"><span class="hljs-number">4</span></span>) plt.title(title) plt.show() tsne_plot_3d(<span class="hljs-string"><span class="hljs-string">'Visualizing Embeddings using t-SNE'</span></span>, <span class="hljs-string"><span class="hljs-string">'War and Peace'</span></span>, embeddings_wp_3d, a=<span class="hljs-number"><span class="hljs-number">0.1</span></span>)</code> </pre> <br><img src="https://habrastorage.org/webt/ch/jm/os/chjmos082qn6ktw9afdeavbz6_a.png"><br>  <i>Figure 4. Visualization of the dictionary of the Word2Vec-model, trained in the novel ‚ÄúWar and Peace‚Äù.</i> <br><br><h1>  Sources </h1><br>  The code is available on <a href="https://github.com/sismetanin/word2vec-tsne">github</a> .  There you can also find the code for rendering animations. <br><br><h1>  Sources </h1><br><ol><li>  Maaten L., Hinton G. Visualizing data using t-SNE // Journal of machine learning learning.  - 2008. - V. 9. - p. 2579-2605. </li><li>  Representations of Words and Phrases and their Compositionality // <i>Advances in Neural Information Processing Systems</i> .  - 2013. - p. 3111-3119. </li><li>  Rehurek R., Sojka P. Software framework for topic modeling with large-scale corpora // InLearning of the LREC 2010 Workshop on New Challenges for NLP Frameworks.  - 2010. </li></ol></div><p>Source: <a href="https://habr.com/ru/post/426113/">https://habr.com/ru/post/426113/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../426099/index.html">Typegram news</a></li>
<li><a href="../426101/index.html">Data integrity in microservice architecture - how to ensure it without distributed transactions and rigid connectivity</a></li>
<li><a href="../426103/index.html">Webinar "Do you need Kubernetes" October 15 at 19:00</a></li>
<li><a href="../426105/index.html">My catch for the week</a></li>
<li><a href="../426111/index.html">Google is going to launch a censored search service in China</a></li>
<li><a href="../426115/index.html">Practice using the actor model in the back end platform of Quake Champions</a></li>
<li><a href="../426117/index.html">Ombudsman proposed to block ads with potentially dangerous children's products</a></li>
<li><a href="../426119/index.html">Antiquities: Cryptonomicon Iron</a></li>
<li><a href="../426121/index.html">MC.exe (Message compiler), rc.exe, link.exe to generate .dll for EventMessageFile</a></li>
<li><a href="../426123/index.html">Learn OpenGL. Lesson 6.1. PBR or Physically-correct rendering. Theory</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>