<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>GPU, hexagonal accelerators and linear algebra</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="All these words are much more connected with mobile development than it seems at first glance: hexagonal accelerators are already helping to train neu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>GPU, hexagonal accelerators and linear algebra</h1><div class="post__text post__text-html js-mediator-article">  All these words are much more connected with mobile development than it seems at first glance: hexagonal accelerators are already helping to train neural networks on mobile devices;  algebra and matan come in handy to get a job at Apple;  And GPU programming not only allows you to speed up applications, but also teaches you to see the essence of things. <br><br>  In any case, the head of mobile development Prisma <b>Andrei Volodin</b> says so.  And also about how ideas flow into mobile development from GameDev, what distinguishes paradigms, why Android doesn‚Äôt have native blur - yes, there‚Äôs a lot more productive, AppsCast has been released.  Under the <a href="http://appsconf.ru/moscow/2019">cut, let's</a> talk about Andrew's report on <a href="http://appsconf.ru/moscow/2019">AppsConf</a> without spoilers. <br><br><img src="https://habrastorage.org/webt/0q/af/fk/0qaffkk1onyogn5werp4i1_rcxg.jpeg"><br><a name="habracut"></a><br>  <i><a href="https://soundcloud.com/appscast/episode-4-gpu-geksagonalnye-uskoriteli-i-lineynaya-algebra-andrey-volodin">AppsCast</a> is a podcast dedicated to the AppsConf mobile conference.</i>  <i>Each issue is a new guest.</i>  <i>Each guest is the speaker of the conference, with whom we discuss his report and speak on related topics.</i>  <i>The podcast is conducted by members of the AppsConf program committee Alexey Kudryavtsev and Daniil Popov.</i> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <strong>Alexey Kudryavtsev:</strong> Hello everyone!  Andrew, please tell us about your experience. <br><br>  <b>Andrey Volodin</b> : We at Prisma are developing products that are mainly related to photo and video processing.  Our flagship app is Prisma.  Now we are doing another Lensa application for Facetune-like functionality. <br><br>  I lead mobile development, but I'm a gaming trainer.  I have the whole core part, I write the GPU pipelines for all these applications.  I develop core frameworks so that the algorithms and neurons that the R &amp; D team has developed run on mobile devices in realtime.  In short, to kill server computing and all that. <br><br>  <b>Alexey Kudryavtsev:</b> It doesn‚Äôt sound like an ordinary iOS development. <br><br>  <b>Andrei Volodin:</b> Yes, I have this specificity - I write every day on Swift, but at the same time it is very far from what is considered to be an iOS development. <br><br>  <b>Daniil Popov:</b> You mentioned the GPU pipelines, is that all about? <br><br>  <b>Andrei Volodin:</b> When you make photo editors, you also need to adjust the architecture and decompose the logic, because the application has different tools.  For example, in Lensa there is a bokeh tool that blurs the background with the help of a neuron, there is a retouching tool that makes a person more beautiful.  It is necessary that all this work more efficiently on the GPU.  Moreover, it is advisable not to transfer data between the processor and the video card each time, but to build in advance a set of operations, perform them in one run, and show the final result to the user. <br><br>  GPU pipelines are ‚Äúsmall lumps‚Äù from which the instructions for the video card are collected.  Then she does all this very quickly and efficiently, and you take the result at a time, and not after each tool.  I am committed to ensuring that our GPU pipelines are as fast as possible, efficient and generally exist. <br><br>  <b>Alexey Kudryavtsev:</b> Tell me, how did you come to this?  A regular iOS developer starts with riveting and molds, then walks somewhere on the API and is happy.  How did it happen that you are doing something completely different? <br><br>  <b>Andrei Volodin:</b> For the most part, this is a coincidence.  Before I got a job, I made games for iOS.  It was always interesting to me, but I understood that in Russia there is no particular place to develop in this direction.  It so happened that we found each other with Prisma.  They needed an iOS developer who knows how to write on Swift and at the same time knows the GPU, in particular, Metal, which then only came out, and I fit that description exactly. <br><br>  I responded to the vacancy, we had a synergy, and now for the third year I am going deeper and deeper into this thing.  If something goes wrong now, then I already have to figure out all these Viper and MVVM - I do not even know how it stands for - from the very beginning. <br><br><h2>  What does GPU Engineer do? </h2><br>  <b>Daniil Popov:</b> GPU Engineer is written on your AppsConf <a href="https://appsconf.ru/moscow/2019/abstracts/4986">profile</a> .  What does the GPU Engineer do most of the work day, except for drinking coffee? <br><br>  <b>Andrei Volodin:</b> It should be mentioned here, what is fundamentally different from the processor GPU.  The processor performs operations as it were, sequentially.  Even the multithreading that we have is often fake: the processor stops and switches to make small pieces of different tasks, and performs them a little bit in slices.  GPU works exactly the opposite way.  There are n processors that are truly working in parallel, and there is parallelism between processes and parallelism inside the GPU. <br><br>  My main job, in addition to trivial things like optimizing working with memory and organizing code reuse, is that I port algorithms that are written for the CPU on the video card so that they parallel.  This is not always a trivial task, because there are very efficient algorithms that are fully tied to the sequential execution of instructions.  My job is to come up with, for example, an approximation for such an algorithm, which does maybe not exactly the same, but you cannot distinguish visually the result.  So we can get the acceleration 100 times, slightly sacrificing quality. <br><br>  I also do porting neurons.  By the way, we will soon make a major open source release.  Even before Core ML appeared, we had our own analogue, and we finally matured to put it in Open Source.  Its paradigm is slightly different from Core ML.  I, among other things, develop its core part. <br><br>  In general, I do everything around Computer Vision algorithms and calculations. <br><br>  <b>Alexey Kudryavtsev:</b> An interesting announcement. <br><br>  <b>Andrei Volodin:</b> It is not a secret, we will not announce it with some kind of fanfare, it will just be possible to see an example of the frameworks that are used inside Prisma. <br><br><h2>  Why optimize for GPU </h2><br>  <b>Alexey Kudryavtsev:</b> Tell me, please, why optimize algorithms for the GPU in general.  It may seem that it is enough to add cores to the processor or to optimize the algorithm.  Why GPU? <br><br>  <b>Andrei Volodin:</b> Working on a GPU can accelerate algorithms tremendously.  For example, we have neurons that will run for 30 s on the Samsung S10 CPU, and there will be 1 frame on the GPU, that is, 1/60 s.  It incredibly changes the user experience.  There is no perpetual loading screen, you can see the result of the algorithm on the video stream, or turn the slider and immediately see the effects. <br><br><blockquote>  The point is not at all that we are too cool to write on the CPU, so let's rewrite everything on the GPU.  Using a GPU has a transparent goal - to speed up the work. </blockquote><br>  <b>Alexey Kudryavtsev:</b> GPU handles operations similar to each other well in parallel.  Do you have such operations and why can you achieve such success? <br><br>  <b>Andrei Volodin:</b> Yes, the main difficulty is not to code, but to create algorithms that are well-placed on the GPU.  This is not always trivial.  It happens that you figure out how to do everything cool, but you need too many synchronization points to do this.  For example, you write everything in one property, and this is a clear sign that it will be badly paralleled.  If you write a lot in one place, then all threads will need to synchronize for this.  Our task is to approximate the algorithms so that they are well parallel. <br><br>  <b>Alexey Kudryavtsev:</b> For me, as a mobile developer, it sounds like rocket science. <br><br>  <b>Andrei Volodin:</b> In fact, it is not so difficult.  For me, rocket science is VIPER. <br><br><h2>  Third chip </h2><br>  <b>Daniil Popov:</b> It seems that at the last Google I / O conference they announced a piece of hardware for TensorFlow and other things.  When at last the third chip appears in the mobile phones, TPU or what will it be called, who will also do all the ML magic on the device? <br><br>  <b>Andrei Volodin:</b> We have this thing, it is connected via USB, and you can drive neurons from Google on it.  This is already the case with Huawei, we even wrote software for their hexagonal accelerators, so that the segmentation neurons quickly run on the P20. <br><br>  I must say that they actually already exist on the iPhone.  For example, the latest iPhone XS has a co-processor called the NPU (Neural Processing Unit), but so far only Apple has access to it.  This coprocessor now cuts the GPU into the iPhone.  Some Core ML models use NPU and due to this work faster than bare Metal. <br><br>  This is significant, given that in addition to the very low inference of the neuron, Core ML requires many additional actions.  First, you need to convert the input data into the Core ML format, it will process them, then return it in its format ‚Äî you need to convert it back, and only then show it to the user.  It all takes quite some time.  We write overhead free pipelines that work from the beginning to the end on the GPU, while the Core ML models are faster due to this hardware process. <br><br><blockquote>  Most likely, a framework for working with NPU will be shown at WWDC in June. </blockquote><br>  That is, as you said, the device is already there, just the developers can not yet use them to the full.  My hypothesis is that companies do not yet understand how to do this neatly in the form of a framework.  Or just do not want to give up to have a market advantage. <br><br>  <b>Alexey Kudryavtsev:</b> With the fingerprint scanner, the same thing was in the IPhone, as I recall. <br><br>  <b>Andrei Volodin:</b> Even now it‚Äôs not that super-affordable.  You can use it at the top level, but you cannot get the imprint itself.  You can simply ask Apple to allow the user to use it.  It's still not that full access to the scanner itself. <br><br><h2>  Hexagonal Accelerators </h2><br>  <b>Daniil Popov:</b> You mentioned the term hexagonal accelerators.  I think not everyone knows what it is. <br><br>  <b>Andrei Volodin:</b> This is just a feature of the hardware piece of hardware that Huawei uses.  I must say, it is quite tricky.  Few people know, but in some Huawei these processors are, but are not used, because they have a hardware bug.  Huawei released them, and then they found a problem, now in some phones there are special chips that are dead weight.  In the latest versions everything is already working. <br><br>  In programming, there is the SIMD (Single Instruction, Multiple Data) paradigm, when the same instructions are executed in parallel on different data.  The chip is designed so that it can process some operation in parallel on several data streams simultaneously.  In particular, hexagonal means that 6 elements are parallel. <br><br>  <b>Alexey Kudryavtsev:</b> I thought that the GPU works just like this: vectorizes the task and performs the same operation on different data.  What's the Difference? <br><br>  <b>Andrei Volodin</b> : GPU more general purpose.  Despite the fact that programming for the GPU is rather low-level, it is rather high-level with respect to working with co-processors.  For programming on a GPU, a C-like language is used.  On iOS, the code is still then compiled using LLVM into machine instructions.  And these things for coprocessors are often written directly hardcore - in assembly language, on machine instructions.  Therefore, there the increase in productivity is much more noticeable, because they are sharpened for specific operations.  They can not count anything at all, but you can count only what they were originally intended for. <br><br>  <b>Alexey Kudryavtsev:</b> And what are they usually designed for? <br><br>  <b>Andrei Volodin:</b> Now mainly for the most common operations in neural networks: convolution - convolution or some kind of intermediate activation.  They have pre-wired functionality that works super-fast.  So they are much faster on some tasks than GPUs, but in all the others they simply don‚Äôt apply. <br><br>  <b>Alexey Kudryavtsev:</b> It looks like DSP processors, which were once used for audio, and all the plug-ins and effects worked on them very quickly.  Special expensive pieces of iron were sold, but then the processors grew, and now we record and process podcasts directly on laptops. <br><br>  <b>Andrei Volodin:</b> Yes, about the same. <br><br><h2>  GPU is not only for graphics </h2><br>  <b>Daniil Popov:</b> I understand correctly that now on the GPU you can process data that is not related to graphics directly?  It turns out that the GPU loses its original purpose. <br><br>  <b>Andrei Volodin:</b> Exactly.  I talk about this quite often at conferences.  The first were NVidia, which presented CUDA.  This is a technology that makes GPGPU (General-purpose computing on graphics processing units) easier.  You can write on it on the superset of C ++ algorithms that are parallelized on the GPU. <br><br>  But people have done it before.  For example, craftsmen on OpenGL or on even older DirectX simply wrote data into the texture ‚Äî each pixel was interpreted as data: the first 4 bytes into the first pixel, and the second 4 bytes into the second pixel.  We processed the textures, then back the data from the texture was extracted and interpreted.  It was very crutch and difficult.  Now video cards support general purpose logic.  You can feed any buffer to the GPU, describe your structures, even the hierarchy of structures in which they will refer to each other, calculate something and return to the processor. <br><br>  <b>Daniel Popov:</b> That is, we can say that the GPU is now Data PU. <br><br>  <b>Andrei Volodin:</b> Yes, graphics on the GPU are sometimes processed less than general calculations. <br><br>  <b>Alexey Kudryavtsev: The</b> architecture of the CPU and the GPU is different in essence, and you can be considered both there and there. <br><br>  <b>Andrei Volodin</b> : Indeed, in something the CPU is faster, in something the GPU.  Not to say that the GPU is always faster. <br><br>  <b>Daniil Popov:</b> As far as I remember, if the task is to calculate something very different, then on the CPU it can be much faster. <br><br>  <b>Andrei Volodin: It also</b> depends on the amount of data.  There is always an overhead for transferring data from the CPU to the GPU and back.  If you consider, for example, a million elements, then using a GPU is usually justified.  But calculating a thousand items on a CPU can be faster than simply copying them onto a video card.  Therefore, you should always choose a task. <br><br>  By the way, Core ML does it.  Core ML is able in runtime, according to Apple, to choose where to calculate faster: on the processor or on the video card.  I do not know if it works in reality, but they declare that they are. <br><br><h2>  Hardcore GPU Engineer knowledge for a mobile developer </h2><br>  <b>Alexey Kudryavtsev:</b> Let's go back to mobile development.  You are a GPU Engineer, you have a lot of hardcore knowledge.  How can this knowledge be applied to a mobile developer?  For example, what do you see in UIKit that others do not see? <br><br>  <b>Andrei Volodin:</b> I will <a href="https://appsconf.ru/moscow/2019/abstracts/4986">talk</a> about it in detail at AppsConf.  You can apply a lot where.  When I see, for example, how the UIKit API works, I can immediately understand why and why it is done.  Observing a drop in performance when rendering some views, I can understand the reason, because I know how the rendering is written inside.  I understand that in order to display the effects that the Gaussian blur actually does over the frame buffer, you must first cache the entire texture, apply a heavy blur operation to it, return the result, finish rendering the other views, and only then show it on the screen.  All this must fit in 1/60 of a second, otherwise it will slow down. <br><br>  It is absolutely clear to me why this is a long time, but for my colleagues it is not clear.  That is why I want to share the design techniques that we often use in GameDev, and my insights on how I look at the problems and try to solve them.  It will be an experiment, but I think that should be interesting. <br><br><h2>  Why there is no native blur in Android </h2><br>  <b>Daniil Popov:</b> You mentioned the blur, and I had a question that worries, I think, all Android developers: why iOS has native blur, and Android doesn't. <br><br>  <b>Andrei Volodin:</b> I think this is because of the architecture.  Apple platforms use Tiled Shading rendering architecture.  With this approach, not the whole frame is rendered, but small tiles ‚Äî small squares, parts of the screen.  This allows you to optimize the performance of the algorithm, because the main performance gains with the use of the GPU is the efficient use of the cache.  On iOS, the frame is often rendered in such a way that it doesn't take up memory at all.  For example, on iPhone 7 Plus, the resolution is 1920 * 1080, which is about 2 million pixels.  Multiply by 4 bytes per channel, it turns out in the region of 20 megabytes per frame.  20 MB to simply store the system frame buffer. <br><br>  The Tiled Shading approach allows you to split this buffer into small pieces and render it slightly.  So the number of cache accesses is greatly increased, because in order to make a blur, you need to read the already drawn pixels and count the Gaussian distribution on them.  If you read across the frame, the cash rate will be very low, because each thread will read different places.  But if you read small pieces, then the cash rate will be very high, and the performance will also be high. <br><br>  It seems to me that the lack of native blur in Android is connected precisely with the peculiarities of the architecture.  Although, maybe this is a grocery solution. <br><br>  <b>Daniil Popov:</b> In Android, there is a RenderScript for it, but there you need to mix, draw, lay with your hands.  This is much more complicated than one checkbox in iOS. <br><br>  <b>Andrei Volodin:</b> Most likely, the performance is also lower. <br><br>  <b>Daniil Popov:</b> Yes, in order to satisfy the designer's wishes, we have to downscale the picture, blur it, and then upscale it back in order to save something. <br><br>  <b>Andrei Volodin:</b> By the way, using this you can do different tricks.  The Gaussian distribution is a blurred circle.  Gauss sigma depends on the number of pixels you want them to collect.  Often, as an optimization, you can downscale a picture and slightly narrow the sigma, and when you return the original scale, there will be no difference, because the sigma directly depends on the size of the picture.  This trick we often use inside to speed up the blur. <br><br>  <b>Daniil Popov:</b> Nevertheless, RenderScript in Android does not allow making a radius greater than 30. <br><br>  <b>Andrei Volodin:</b> Actually, a radius of 30 is a lot.  Again, I understand that it is very expensive to assemble 30 pixels using a GPU on each stream. <br><br><h2>  What is similar to mobile development and GameDev </h2><br>  <b>Alexey Kudryavtsev:</b> In the theses to your report, you say that mobile development and GameDev have a lot in common.  Tell me a little, what exactly? <br><br>  <b>Andrei Volodin:</b> UIKit architecture is very similar to game engines, and the old ones.  Modern went towards Entity Component System, this will also be in the report.  In UIKit it also comes, there are articles in which they write how to design views on components.  But it came up with GameDev, the first time the Component System was used in the game Thief in '98. <br><br>  Fundamentally, for example, Cocos2d, on which I worked for a long time, and the ideas that were used in the first implementation are very similar.  Both Scene graph is used there, the scene tree, when each node has sub nodes, they are rendered using the accumulation of affine transformations, which are specifically called CGAffineTransform on iOS.  These are simply 4 * 4 matrices that are multiplied together to change the coordinate system.  Animation is made about the same everywhere. <br><br>  Both in game engines and in UIKit everything is built on time interpolation.  We just interpolate some values ‚Äã‚Äã- be it colors or positions between frames.  The optimizations are all the same: in GameDev, it‚Äôs customary not to do too much work, and UIKit uses setNeedsLayout, layoutIfNeeded. <br><br>  I keep these parallels for myself constantly - between what I once did and between what I see in the Apple framework.  About this and tell on <a href="https://appsconf.ru/moscow/2019/">AppsConf</a> . <br><br>  <b>Daniil Popov:</b> Indeed, Cocos2d API is similar to iOS (for UI).  Do you think the developers were inspired by each other's work or did it just work out architecturally? <br><br>  <b>Andrey Volodin:</b> I think that they were inspired by something.  Cocos2d appeared in 2008-2009, then UIKit was not the UIKit that we know now.  It seems to me that some tricks were repeated there in order to make it more comfortable for people to work so that they could draw parallels. <br><br>  It's funny that the rocker swung: the Cocos2d core-team originally borrowed Apple's ideas a little, and then Apple completely copied Cocos2d, right down to all architectural solutions.  SpriteKit is essentially a complete copy of all the ideas that appeared in Cocos2d.  In this sense, Apple has taken its credit. <br><br>  <b>Alexey Kudryavtsev:</b> It seems to me that the same tricks as in UIKit in 2009 were still on MacOS, which has existed since ancient times.  There the same setNeedsLayout, layoutIfNeeded is, affine transformations. <br><br>  <b>Andrei Volodin:</b> Of course, but GameDev exists even longer than MacOS. <br><br>  <b>Alexey Kudryavtsev:</b> Do not argue! <br><br>  <b>Andrei Volodin:</b> Therefore, I do not compare Cocos2d with Apple frameworks, but rather consider in principle the paradigms that originated in GameDev.  It was in GameDev that people understood for the first time that inheritance is bad.  When the whole world admired the PLO, GameDev already began to think that inheritance brings problems, and came up with components.  Mobile development, as an industry, has come to this only now. <br><br>  <b>Alexey Kudryavtsev:</b> It seems that Alan Kay understood a long time ago that inheritance is bad. <br><br>  <b>Andrei Volodin</b> : Yes, but on the whole, you will agree that just a few years ago, everyone said that the PLO is cool.  And now there is Protocol-Oriented Programming in Swift, a functional, and everyone is coming up with something new.  In GameDev, these moods have appeared for quite some time. <br><br>  <b>Alexey Kudryavtsev:</b> I will make a remark: Alan Kay is the same person who invented the PLO.  He said that he did not invent inheritance, but only sending messages, and in general he was misunderstood. <br><br><h2>  Differences between mobile development and GameDev </h2><br>  <b>Alexey Kudryavtsev:</b> Tell me now about the differences: how are GameDev and mobile development radically different, and what can we not use from GameDev? <br><br>  <b>Andrei Volodin:</b> It seems to me that the fundamental difference is that product development is as lazy as possible.  We are trying to write code according to the principle ‚Äúuntil they ask, I will not get up‚Äù.  Until the callback works, we will not do anything.  Even rendering in product development is lazy: not the whole frame is redrawn, but only those parts that have changed. <br><br>  GameDev-development in this sense is merciless.  Everything is done for each frame: 30 or 60 times per second the whole scene is redrawn from scratch, every frame, every object is updated, every frame is simulated by physics.  A lot of things happen, and this changes the paradigm very much.  You begin to live inside one frame - I have an entire part of the report devoted to this.  You need all-all-all fit in 1/60 or 1/30 seconds.  Therefore, you begin to be clever, to do the maximum number of preliminary calculations, parallelization, while the GPU renders a frame, to prepare the next on the CPU.  That is why the battery from games is discharged much faster than from conventional applications. <br><br>  <b>Alexey Kudryavtsev:</b> And why in games you can't do everything too lazy? <br><br>  <b>Andrei Volodin:</b> The concept of games does not allow well.  Some games could definitely benefit from this, for example, Tetris, in which there are few dynamics and only some parts change.  But overall, the game is a very complex thing.  Even when the character is just standing, he, for example, is swaying - some kind of animation occurs, there is some logic, physics is calculated.  From the savings can get more damage, because each frame is so changed that reuse fragments becomes almost impossible. <br><br>  In addition, there are hardware restrictions.  For example, the GPU works better with the float type, and not with double, because of this, the accuracy is much lower.  Therefore, for example, if you redraw only part of the screen, noticeable artifacts may occur.  On the CPU, the accuracy is high, because there everything is rendered in double precision, you can use beautiful fonts and neat curves, but on the GPU there will still be some approximation. <br><br>  The combination of these factors leads to the fact that each frame requires heavy calculations, updating all objects is actually drawing from scratch. <br><br><h2>  Classic development is much closer to GameDev than you think. </h2><br>  <b>Daniil Popov:</b> I want to discuss a provocative statement from your future report that ‚Äúclassical development is much closer to GameDev than you think.‚Äù  I immediately remembered a series of <a href="https://habr.com/ru/post/358704/">articles</a> about crutches in games that were intended to speed up development when deadlines were running out.  For these articles, it seems that GameDev is a crutch on a crutch for the sake of optimizations.  In the usual development now everyone is obsessed with architecture, beautiful code.  I can not relate this to GameDev. <br><br>  <b>Andrei Volodin:</b> Of course, enterprise companies do not do that, but in GameDev indie, this is about it.  But specifically this thesis about the other.  I often notice that developers use many of the concepts that are used in GameDev, but do not even understand it. <br><br>  For example, affine transformations.  Few can clearly say that this is just a multiplication of 4 * 4 matrices.  More often, the CGAffineTransform is an opaque data structure in which something is stored and it is not clear how the view causes the view to scale. <br><br><blockquote>  In the report I will try to show the other side of what we use every day, but at the same time, maybe, we do not fully understand. </blockquote><br><h2>  About the benefits of mathematics </h2><br>  <b>Alexey Kudryavtsev:</b> How can a mobile developer come to this understanding?  How to figure out what is under the hood of rendering in UIKit, how affine transformations are arranged inside, and not to be scared once again?  I understand that this is a matrix, but I cannot say what exactly the figure is responsible for what.  Where to gather information in order not to be afraid and understand? <br><br>  <b>Andrei Volodin:</b> The most obvious advice is to start doing a pet project. <br><br>  The main thing about this is to say: all the concepts of mobile GPU development are absolutely similar to those on the desktop.  iOS GPU programming is not fundamentally different from what is in the desktop environment.  Therefore, if for iOS there is a lack of material on the topic, then you can always read something for NVidia or AMD solutions and be inspired by them.  Ideologically, they are exactly the same.  The API is a little different, but it's usually clear how to shift existing practices from desktop programming to mobile. <br><br>  <b>Alexey Kudryavtsev:</b> When you use an API, for example, the Cocos2d or Unity game engine, you don‚Äôt understand everything early - you just pull some methods.  How exactly to begin to understand, and where it is better to see what is better to read, so that it can be shifted to UIKit? <br><br>  <b>Andrei Volodin:</b> Cocos2d - Open Source project and well written.  I‚Äôm not very objective, because I‚Äôve put a hand on it, but it seems to me that there is a pretty good code that can be read and inspired.  It is written in a not very modern objective-C, but there are detailed comments on many difficult places. <br><br>  But when I talk about the pet project, I‚Äôm not talking more about high-level projects like making a game, but about writing an API that does, for example, the glitch effect.  You know, there are popular APIs that make a VHS effect.  And not on the processor, but on the GPU.  This is a relatively simple task that can be done over the weekend.  But it is not so easy, if you never tried it.  When I did this for the first time, I learned amazing things: ‚ÄúThat's how contrast and saturation work on Instagram, or lightroom presets!‚Äù It turns out that these are just shaders that multiply 4 numbers or raise to a power - that's all. <br><br><blockquote>  Directly tears down the tower from how simple it is. </blockquote><br>  You use it every day and take it for granted - it works, but you don‚Äôt understand how.  Then you start doing it yourself, and it becomes at the same time cool from the fact that you are doing something supposedly complicated, but it is also funny that in reality it is so simple that it is even funny. <br><br>  <b>Daniil Popov:</b> Anyway, it seems to me that we need some kind of mathematical basis.  For example, in Cocos2d, some shaders are literally 5 lines of code, and you sit and look at them like a ram at the gate, and you just don‚Äôt understand what is written there.  Probably, it‚Äôs not easy to dive into the language of shaders without knowing mathematics, basic concepts, etc. <br><br>  <b>Andrei Volodin: I</b> agree about mathematics.  Without basic knowledge of linear algebra it will be difficult, first you have to figure it out.  But at the same time, if you had a linear algebra course at the university, and you at least roughly represent at the first year level what a scalar product and its geometric meaning, what is a vector product and its geometric meaning, what is its own vector, normal matrix, as matrix multiplication works, it will be quite easy to understand. <br><br>  <b>Daniil Popov:</b> Often computer science students whine that they don‚Äôt need physics and mathematics.  Probably, many now hardly remember how matrix multiplication works. <br><br>  <b>Andrei Volodin:</b> For me, this is a sore subject.  I was the same, arrogantly yelled, why do I need a functional analysis and the like.  But I have a valuable life experience when I was interviewed at Apple, on the ARKit team.  At the interview, there was such a huge amount of mathematics that I later thanked myself for going to couples.  If it were not for the background that I received at the university, I would never have answered these questions, and would never have understood how it works. <br><br>  Now, when I myself teach at the university or come on an open day, I always say: ‚ÄúFriends, you will have time to sit in your IDE, please go to Linal, to Matan and in general understand what it is.  In the era of machine learning, this will definitely come in handy. ‚Äù <br><br>  <b>Daniil Popov:</b> The most important thing - was the interview? <br><br>  <b>Andrei Volodin:</b> Yes, of course, and only due to the fact that I had a mathematical background. <br><br>  <b>Alexey Kudryavtsev:</b> Now you know why to teach matan, and where you can get after that. <br><br>  <b>Andrei Volodin:</b> For example, without an understanding of affine transformations and knowledge of what the normal is, you cannot go far in VR.  Even when you create a Project Template in Xcode, everything is already multiplied there, there are vector artworks, something is transposed.    ,       . <br><br> <b> :</b>       . <br><br><h2>  </h2><br> <b> :</b>  -  ,      GameDev  GPU. <br><br> <b> :</b>    .   - ,      ,   ,  .   ,         ,   ,   ,  ,      UI: ,  , runtime Objective-C ‚Äî  ,    ,     .     .      ,     : ,  ‚Äî , X  Y, ! <br><br><blockquote>    ,  ,  - ,  GameDev  GPU- ‚Äî   . </blockquote><br> <b>        ,   .          <a href="https://appsconf.ru/moscow/2019/">AppsConf</a> 22  23     .</b> </div><p>Source: <a href="https://habr.com/ru/post/448222/">https://habr.com/ru/post/448222/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../448208/index.html">20, 100, 3, 19 - InoThings in numbers</a></li>
<li><a href="../448210/index.html">A satellite cannon, a blaster and a sun-walker: funny and paradoxical projects</a></li>
<li><a href="../448212/index.html">Chinese AIBUS protocol and laboratory chemical reactor</a></li>
<li><a href="../448218/index.html">New MFP security level: imageRUNNER ADVANCE III</a></li>
<li><a href="../448220/index.html">GLTF and GLB format basics, part 1</a></li>
<li><a href="../448228/index.html">Types of modeling. Basics of Sculpting, Retopology and Sweep</a></li>
<li><a href="../448230/index.html">Manage Business Continuity with ClearView</a></li>
<li><a href="../448234/index.html">AI and MO: some trends and trends</a></li>
<li><a href="../448236/index.html">Facebook's Terragraph technology moves from testing to commercial use</a></li>
<li><a href="../448238/index.html">Transistor Story: Groping in the Dark</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>