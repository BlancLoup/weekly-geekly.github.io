<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Solving the problem of binary classification in the XGboost machine learning package</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This article will discuss the problem of binary classification of objects and its implementation in one of the most productive machine learning packag...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Solving the problem of binary classification in the XGboost machine learning package</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/files/e44/dfb/7ce/e44dfb7cea384064851ace7a27a68b77.JPG" alt="image"></p><br><p>  This article will discuss the problem of binary classification of objects and its implementation in one of the most productive machine learning packages "R" - "XGboost" (Extreme Gradient Boosting). <br>  In real life, we often encounter a class of problems where the object of prediction is a nominative variable with two gradations, when we need to predict the outcome of a certain event or make decisions in binary expression based on the data model.  For example, if we assess the situation on the market and our goal is to make an unequivocal decision whether it makes sense to invest in a specific instrument at a given time, whether the buyer will buy the product under investigation or not, whether the borrower will pay the loan or leave the employee in the near future etc. <a name="habracut"></a></p><br><p>  In the general case, <em>binary classification</em> is used to predict the probability of occurrence of a certain event by the values ‚Äã‚Äãof a set of features.  To do this, we introduce the so-called dependent variable (outcome of the event), which takes only one of two values ‚Äã‚Äã(0 or 1) and a set of independent variables (also called characteristics, predictors, or regressors). </p><br><p>  At once I will make a reservation that in "R" there are several linear functions for solving similar problems, such as "glm" from the standard package of functions, but here we consider a more advanced version of the binary classification implemented in the "XGboost" package.  This model, the multiple winner of Kaggle competitions, is based on building binary decision trees capable of supporting multi-stream data processing.  About the features of the implementation of the family of models "Gradient Boosting" you can read here: </p><br><p>  <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/</a> <br>  <a href="https://en.wikipedia.org/wiki/Gradient_boosting">https://en.wikipedia.org/wiki/Gradient_boosting</a> </p><br><p>  Let's take a test data set (Train) and build a model to predict the survival of passengers during a crash: </p><br><pre><code class="python hljs">data(agaricus.train, package=<span class="hljs-string"><span class="hljs-string">'xgboost'</span></span>) data(agaricus.test, package=<span class="hljs-string"><span class="hljs-string">'xgboost'</span></span>) train &lt;- agaricus.train test &lt;- agaricus.test</code> </pre> <br><p>  If, after a transformation, the matrix contains many zeros, then such an array of data must first be converted into a sparse matrix - in this form, the data will take up much less space, and accordingly the processing time will be much shorter.  Here the Matrix library will help us today the latest available version 1.2-6 contains a set of functions for converting to dgCMatrix on a column basis. <br>  In the case when the already compacted matrix (sparse matrix) after all transformations does not fit in the RAM, in such cases use the special program ‚ÄúVowpal Wabbit‚Äù.  This is an external program that can handle datasets of any size, reading from many files or databases.  ‚ÄúVowpal Wabbit‚Äù is an optimized parallel machine learning platform developed for distributed computing by ‚ÄúYahoo!‚Äù You can read about it in some detail at these links: </p><br><p>  <a href="https://habrahabr.ru/company/mlclass/blog/248779/">https://habrahabr.ru/company/mlclass/blog/248779/</a> <br>  <a href="https://en.wikipedia.org/wiki/Vowpal_Wabbit">https://en.wikipedia.org/wiki/Vowpal_Wabbit</a> </p><br><p>  The use of sparse matrices allows us to construct a model using text variables with their preliminary transformation. <br>  So, to build the predictor matrix, first load the necessary libraries: </p><br><pre> <code class="python hljs"> library(xgboost) library(Matrix) library(DiagrammeR)</code> </pre> <br><p>  When converting to the matrix, all categorical variables will be transposed, respectively, the function with the standard booster will include their values ‚Äã‚Äãin the model.  The first thing to do is to remove variables with unique values ‚Äã‚Äãfrom the data set, such as ‚ÄúPassenger ID‚Äù, ‚ÄúName‚Äù and ‚ÄúTicket Number‚Äù.  We carry out the same actions with a test dataset, which will be used to calculate predicted outcomes.  For clarity, I downloaded data from local files that I downloaded in the corresponding dataset Kaggle.  For the model, it will need the following table columns: </p><br><pre> <code class="python hljs"> input.train &lt;- train[, c(<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>,<span class="hljs-number"><span class="hljs-number">6</span></span>,<span class="hljs-number"><span class="hljs-number">7</span></span>,<span class="hljs-number"><span class="hljs-number">8</span></span>,<span class="hljs-number"><span class="hljs-number">10</span></span>,<span class="hljs-number"><span class="hljs-number">11</span></span>,<span class="hljs-number"><span class="hljs-number">12</span></span>)] input.test &lt;- test[, c(<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>,<span class="hljs-number"><span class="hljs-number">6</span></span>,<span class="hljs-number"><span class="hljs-number">7</span></span>,<span class="hljs-number"><span class="hljs-number">9</span></span>,<span class="hljs-number"><span class="hljs-number">10</span></span>,<span class="hljs-number"><span class="hljs-number">11</span></span>)]</code> </pre> <br><p><img src="https://habrastorage.org/files/a1a/b20/565/a1ab20565f1b41a7ab8deccbaba00fc3.JPG" alt="image"><br>  separately form a vector of known outcomes for learning the model </p><br><pre> <code class="python hljs"> train.lable &lt;- train$Survived</code> </pre> <br><p>  Now it is necessary to perform data conversion so that statistically significant variables are taken into account when training the model into accounting.  Perform the following conversions: <br>  Replace variables containing categorical data with numeric values.  It should be borne in mind that ordered categories, such as 'good', 'normal', 'bad' can be replaced by 0,1,2.  Non-ordered data with relatively low selectivity, such as 'gender' or 'Country Name', can be left unchanged as factors; after being converted into a matrix, they are transposed into the appropriate number of columns with zeros and ones.  For numeric variables, all unassigned and missing values ‚Äã‚Äãmust be processed.  There are at least three options: you can replace them with 1, 0, or a more acceptable option would be to replace them with an average value in the column of this variable. <br>  When using the ‚ÄúXGboost‚Äù package with a standard booster (gbtree), variable scaling can be avoided, unlike other linear methods such as ‚Äúglm‚Äù or ‚Äúxgboost‚Äù with a linear booster (gblinear). </p><br><p>  Basic package information can be found at the following links: </p><br><p>  <a href="https://github.com/dmlc/xgboost">https://github.com/dmlc/xgboost</a> <br>  <a href="https://cran.r-project.org/web/packages/xgboost/xgboost.pdf">https://cran.r-project.org/web/packages/xgboost/xgboost.pdf</a> </p><br><p>  Returning to our code, as a result we got a table of the following format: </p><br><p><img src="https://hsto.org/files/e19/3f4/acb/e193f4acbf6840eabad3359bbb4aa3de.JPG" alt="image"></p><br><p>  next, replace all missing entries with the arithmetic average value of the predictor column </p><br><pre> <code class="python hljs"> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(inp.column)</span></span></span><span class="hljs-class"> %</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">in</span></span></span><span class="hljs-class">% </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">c</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(</span></span><span class="hljs-string"><span class="hljs-class"><span class="hljs-params"><span class="hljs-string">'numeric'</span></span></span></span><span class="hljs-class"><span class="hljs-params">, </span></span><span class="hljs-string"><span class="hljs-class"><span class="hljs-params"><span class="hljs-string">'integer'</span></span></span></span><span class="hljs-class"><span class="hljs-params">)</span></span></span><span class="hljs-class">) { </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">inp</span></span></span><span class="hljs-class">.</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">table</span></span></span><span class="hljs-class">[</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">is</span></span></span><span class="hljs-class">.</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">na</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(inp.column)</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">i</span></span></span><span class="hljs-class">] &lt;- </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">mean</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(inp.column, na.rm=TRUE)</span></span></span></span></code> </pre> <br><p>  after preprocessing, we do the conversion to "dgCMatrix": </p><br><pre> <code class="python hljs"> sparse.model.matrix(~., inp.table)</code> </pre> <br><p>  It makes sense to create a separate function for preprocessing predictors and converting to sparse.model.matrix format, for example, the variant with the "for" cycle is given below.  To optimize performance, you can vectorize an expression using the "apply" function. </p><br><pre> <code class="python hljs"> spr.matrix.conversion &lt;- function(inp.table) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>:ncol(inp.table)) { inp.column &lt;- inp.table [ ,i] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(inp.column)</span></span></span><span class="hljs-class"> == '</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">character</span></span></span><span class="hljs-class">') { </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">inp</span></span></span><span class="hljs-class">.</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">table</span></span></span><span class="hljs-class"> [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">is</span></span></span><span class="hljs-class">.</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">na</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(inp.column)</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">i</span></span></span><span class="hljs-class">] &lt;- '</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">NA</span></span></span><span class="hljs-class">' </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">inp</span></span></span><span class="hljs-class">.</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">table</span></span></span><span class="hljs-class"> [, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">i</span></span></span><span class="hljs-class">] &lt;- </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">as</span></span></span><span class="hljs-class">.</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">factor</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(inp.table [, i])</span></span></span><span class="hljs-class"> } </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">else</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">if</span></span></span><span class="hljs-class"> </span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(class</span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params"><span class="hljs-params">(inp.column)</span></span></span></span><span class="hljs-class"><span class="hljs-params"> %in% c</span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-string"><span class="hljs-class"><span class="hljs-params"><span class="hljs-params"><span class="hljs-string">'numeric'</span></span></span></span></span><span class="hljs-class"><span class="hljs-params"><span class="hljs-params">, </span></span></span><span class="hljs-string"><span class="hljs-class"><span class="hljs-params"><span class="hljs-params"><span class="hljs-string">'integer'</span></span></span></span></span><span class="hljs-class"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-class"><span class="hljs-params">)</span></span></span><span class="hljs-class"> { </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">inp</span></span></span><span class="hljs-class">.</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">table</span></span></span><span class="hljs-class"> [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">is</span></span></span><span class="hljs-class">.</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">na</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(inp.column)</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">i</span></span></span><span class="hljs-class">] &lt;- </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">mean</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(inp.column, na.rm=TRUE)</span></span></span><span class="hljs-class"> } } </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">return</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(sparse.model.matrix</span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params"><span class="hljs-params">(~.,inp.table)</span></span></span></span><span class="hljs-class"><span class="hljs-params">)</span></span></span><span class="hljs-class"> }</span></span></code> </pre> <br><p>  Then we use our function and convert the actual and test tables into sparse matrices: </p><br><pre> <code class="python hljs"> sparse.train &lt;- preprocess(train) sparse.test &lt;- preprocess(test)</code> </pre> <br><p><img src="https://hsto.org/files/882/fa2/9fa/882fa29faafe4bd4b0e2c0a2e88040fb.JPG" alt="image"></p><br><p>  To build a model, we need two data sets: the data matrix that we have just created and the vector of actual outcomes with a binary value (0.1). <br>  The "xgboost" function is the most convenient to use.  The ‚ÄúXGBoost‚Äù implements a standard booster based on binary decision trees. <br>  To use ‚ÄúXGboost‚Äù, we must choose one of three parameters: general parameters, booster parameters and destination parameters: <br>  ‚Ä¢ General parameters - we define which booster will be used, linear or standard. <br>  The remaining parameters of the booster depend on which booster we chose in the first step: <br>  ‚Ä¢ Parameters of learning tasks - we determine the purpose and scenario of learning <br>  ‚Ä¢ Command line parameters - used to determine the command line mode when using ‚Äúxgboost.‚Äù. </p><br><p>  General view of the ‚Äúxgboost‚Äù function that we use: </p><br><pre> <code class="python hljs"> xgboost(data = NULL, label = NULL, missing = NULL, params = list(), nrounds, verbose = <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">print</span></span>.every.n = <span class="hljs-number"><span class="hljs-number">1L</span></span>, early.stop.round = NULL, maximize = NULL, ...)</code> </pre> <br><p>  "data" - the data matrix format ("matrix", "dgCMatrix", local data file or "xgb.DMatrix".) <br>  "label" is the vector of the dependent variable.  If this field was a component of the original parameter table, then it should be excluded before processing and converting it to a matrix, in order to avoid the transitivity of the links. <br>  "nrounds" is the number of constructed decision trees in the final model. <br>  "objective" - ‚Äã‚Äãthrough this parameter we transfer the tasks and assignments of training the model.  For logistic regression, there are 2 options: <br>  "reg: logistic" is a logistic regression with a continuous value from 0 to 1; <br>  "binary: logistic" is a logistic regression with a binary prediction value.  For this parameter, you can set a specific threshold for the transition from 0 to 1. By default, this value is 0.5. <br>  Details on the model parameterization can be found at this link. </p><br><p>  <a href="http://xgboost/parameter.md%2520at%2520master%2520%25C2%25B7%2520dmlc/xgboost%2520%25C2%25B7%2520GitHub">http: //xgboost/parameter.md% 20at% 20master% 2020% 20dmlc / xgboost% 20% 5GitHub</a> <br>  Now we are starting to create and train the ‚ÄúXGBoost‚Äù model: </p><br><pre> <code class="python hljs"> set.seed(<span class="hljs-number"><span class="hljs-number">1</span></span>) xgb.model &lt;- xgboost(data=sparse.train, label=train$Survived, nrounds=<span class="hljs-number"><span class="hljs-number">100</span></span>, objective=<span class="hljs-string"><span class="hljs-string">'reg:logistic'</span></span>)</code> </pre> <br><p>  if desired, you can extract the tree structure using the function xgb.model.dt.tree (model = xgb).  Next, use the standard function ‚Äúpredict‚Äù to form a predictive vector: </p><br><pre> <code class="python hljs"> prediction &lt;- predict(xgb.model, sparse.test)</code> </pre> <br><p>  and finally, save the data in a readable format </p><br><pre> <code class="python hljs"> solution &lt;- data.frame(prediction = round(prediction, digits = <span class="hljs-number"><span class="hljs-number">0</span></span>), test) write.csv(solution, <span class="hljs-string"><span class="hljs-string">'solution.csv'</span></span>, row.names=FALSE, quote=FALSE)</code> </pre> <br><p>  adding a vector of predicted outcomes, we get the following table: </p><br><p><img src="https://hsto.org/files/d70/2c4/954/d702c49544f54441869328465b067dbc.JPG" alt="image"></p><br><p>  Now we‚Äôll go back and briefly look at the model we just created.  To display the decision trees, you can use the functions "xgb.model.dt.tree" and "xgb.plot.tree".  So, the last function will give us a list of selected trees with a model fit factor: </p><br><p><img src="https://hsto.org/files/a1c/2b9/8a3/a1c2b98a348d4aa2a8dff7719fa09813.JPG" alt="image"></p><br><p>  Using the xgb.plot.tree function, we will also see a graphical representation of trees, although it should be noted that in the current version, it is far from the best way implemented in this function and is of little use.  Therefore, for clarity, I had to manually reproduce an elementary decision tree based on the standard Train data model. </p><br><p><img src="https://habrastorage.org/files/b79/3ed/b81/b793edb81703474ab71aada96e43b9f1.JPG" alt="image"></p><br><p>  Testing the statistical significance of variables in the model will tell us how to optimize the matrix of predictors for learning the XGB model.  It is best to use the xgb.plot.importance function in which we pass an aggregated parameter importance table. </p><br><pre> <code class="python hljs"> importance_frame &lt;- xgb.importance(sparse.train@Dimnames[[<span class="hljs-number"><span class="hljs-number">2</span></span>]], model = xgb) xgb.plot.importance(importance_frame)</code> </pre> <br><p><img src="https://hsto.org/files/ba5/67b/b7c/ba567bb7c94345d19116c7ebe8f06ed8.JPG" alt="image"></p><br><p>  So, we have considered one of the possible implementations of logistic regression based on the ‚Äúxgboost‚Äù function package with a standard booster.  At the moment, I recommend using the ‚ÄúXGboost‚Äù package as the most advanced group of machine learning models.  Currently, predictive models based on the logic of ‚ÄúXGboost‚Äù are widely used in financial and market forecasting, marketing, and many other areas of applied analytics and machine intelligence. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/307150/">https://habr.com/ru/post/307150/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../307136/index.html">About clouds and sensors: connecting Intel NUC and Genuino 101 to the IBM Watson IoT platform</a></li>
<li><a href="../307138/index.html">We charge Xcode</a></li>
<li><a href="../307140/index.html">Do it yourself web service with asynchronous queues and parallel execution</a></li>
<li><a href="../307142/index.html">Configuring public preview VNET Peering in Azure</a></li>
<li><a href="../307148/index.html">We write microservice on KoaJS 2 in the style of ES2017. Part II: Minimalistic REST</a></li>
<li><a href="../307152/index.html">Install Sailfish OS on your smartphone (for example, Nexus 5)</a></li>
<li><a href="../307154/index.html">Machine learning with TMVA. Reader models</a></li>
<li><a href="../307156/index.html">NIST: SMS can not be used as an authentication tool.</a></li>
<li><a href="../307158/index.html">A new study by the Association of the semiconductor industry: "After 5 years, Moore's law will cease to operate"</a></li>
<li><a href="../307160/index.html">Apple introduces a reward program for identifying vulnerabilities in their products.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>