<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Key QA Process Indicators</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Within Quality Assurance, various metrics and indicators of product quality and development process can and should be used. Metrics can be divided int...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Key QA Process Indicators</h1><div class="post__text post__text-html js-mediator-article">  Within Quality Assurance, various metrics and indicators of product quality and development process can and should be used.  Metrics can be divided into groups according to the parameters on the basis of which they are calculated, according to the stages of the development life cycle, in which they are applied, according to the goals and objectives, according to the stakeholders for which they are intended.  This list goes on and on. <br><br>  In this article I decided to gather together and consider the most basic, in my opinion, groups of criteria and measures for the QA process.  And in each group I will list only the most important and illustrative, again, in my opinion, metrics as well as analyze what they are necessary for, in what situations are useful and how to use them. <br><br><img src="https://habrastorage.org/webt/59/dd/ff/59ddff1098ba4142852527.png"><br><a name="habracut"></a><br><h2>  What should be the metrics? </h2><br>  By itself, a metric in the context of software is a numerical expression of a property, the quality of the product itself or the process of its development.  In other words, this is what we can use to measure, compare and evaluate software. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Now literally a couple of comments about the values ‚Äã‚Äãand properties of metrics: <br><br><ul><li>  The main goal of any metric is to improve the development process and the software itself.  The metric allows you to see at what point on the way to the goals we are at the moment, approach them or are removed, whether success criteria are achieved. </li><li>  Metrics should not exist for the sake of the measurement process itself.  It is necessary to use only those metrics that really have practical value and will affect the further development of the product or process optimization.  A simple rule follows from this: first you need to define zones for change / improvement, and then decide how to evaluate them. </li></ul><br><h2>  Primary Metric Groups for QA </h2><br>  It is theoretically possible to invent your own characteristic, formula or indicator for practically every, even the most insignificant action, stage or status of the QA process.  You can take into account each artifact, all transitions of defects by status, calculate the number of tests in the set.  However, the most important question that you should immediately ask yourself is when there is a desire to measure something: ‚ÄúWhy is this information needed, how can it be used?‚Äù.  When forming a set of metrics, one should make a start from goals, plans for improving processes and product. <br><br>  So, in this article we will not consider the usual quantitative indicators of the progress of testing, which are used in most reports and statuses.  Instead, we will analyze which areas, artifacts and areas of development from the point of view of Quality Assurance, we must measure and control in order to assess the quality of work performed.  Analysis and optimization of these points are extremely important for effective interaction with stakeholders ( <a href="https://doitsmartly.ru/all-articles/sw-testing/120-stakeholders-for-qa.html">https://doitsmartly.ru/all-articles/sw-testing/120-stakeholders-for-qa.html</a> ) and software development in general: <br><br>  1. Requirements for the software being developed. <br>  Absolutely, we must understand that we are developing and testing, and the degree of this understanding must be able to be assessed.  Potential risks or missed problems at the specification level can lead to the most serious and expensive errors. <br><br>  2. The quality of the developed product. <br>  Everything is obvious: you must be able to assess the quality of development and software in order to make predictions and assess risks.  It is important to understand how high-quality and reliable the product is, relying not only on the presence or absence of errors found, but just predicting whether there are many potential problems. <br><br>  3. Capabilities of the QA team. <br>  It is also simple here: in order to manage the testing process, plan work and predict deadlines, it is always necessary to have not only the actual status of the tasks, but also the capabilities of the QA team. <br><br>  4. The quality of the testing team. <br>  In addition to the quality of the product itself, it is necessary to measure the effectiveness of the QA process itself and the testing team.  To constantly optimize and improve the quality of work, you need to know where we are now, what allows us to move forward and what is thrown back. <br><br>  5. Feedback and product satisfaction. <br>  The latter is an area, but, of course, not in importance, but in the opinion of the stakeholders of the process, consumers of our services, users of the product.  It is very important to be able to measure the overall degree of satisfaction with the product, highlight trends and draw appropriate conclusions.  Properly selected metrics for this group will allow you to identify possible problems in time and promptly apply feedback to improve processes. <br><br>  Next, we consider exactly which metrics are included in each group, analyze how exactly they can be evaluated.  For each group, I will give a few examples of possible metrics and describe their meaning.  These and some other QA process indicators are discussed in more detail in my article ‚ÄúThe Most Important QA Metrics‚Äù ( <a href="https://doitsmartly.ru/all-articles/sw-testing/133-the-most-important-metrics-in-qa.html">https://doitsmartly.ru/all-articles/sw-testing/133-the-most-important-metrics-in-qa. html</a> ). <br><br><h2>  Group 1 - Requirements for software being developed </h2><br>  This group of metrics will allow you to assess how much we have worked out the requirements (user story) for software, identify vulnerabilities and the most complex, potentially problematic software features, and understand where special control is required. <br><br>  <b>1. Test coverage requirements</b> <br>  "Total number of tests" / "Total number of requirements" <br><br>  The purpose of the metric: to identify weaknesses in the test coverage, highlight the risks. <br><br><ul><li>  This metric will work only if the requirements are well decomposed into equivalent ones.  Otherwise, it will turn into an indicator of the presence or absence of tests for each of the requirements. </li><li>  For requirements where the coefficient is equal to or close to 0, the possibility of adding tests should be considered. </li></ul><br>  <b>2. The degree of interconnectedness of requirements</b> <br>  AVERAGE (‚ÄúNumber of requirements related to requirement No. 1‚Äù / ‚ÄúTotal number of requirements -1‚Äù, ..., ‚ÄúNumber of requirements related to requirement No.n‚Äù / ‚ÄúTotal number of requirements -1‚Äù) <br><br>  The metric is calculated as the number of links of each requirement with the remaining requirements.  It uses the average value for all requirements. <br><br>  Purpose of the metric: to provide a basis for assessing the timing of testing and taking into account possible risks.  Knowing the degree of mutual influence of requirements on each other, you can, for example, plan additional time and cases for end-to-end testing, work out regression tests, look towards integration, etc. <br><br><ul><li>  From my own experience, I can say that the acceptable degree of connectedness does not exceed 0.2-0.3.  Otherwise, the refinement of one of the requirements will lead to a chain of processing, and therefore possible errors in a significant part of the product. </li></ul><br>  <b>3. The coefficient of stability requirements</b> <br>  ‚ÄúNumber of changes in existing requirements‚Äù / ‚ÄúTotal number of requirements implemented per iteration, including new ones‚Äù <br><br>  Purpose of the metric: to show how many requirements already implemented have to be redone from release to release when developing new features.  Also, the metric gives an idea of ‚Äã‚Äãhow easily the system's functionality is scaled, new features are added. <br><br><ul><li>  For this metric, the coefficient must be at least less than 0.5.  In this case, we introduce new features in 2 times more than rework existing ones.  Otherwise, the team focuses more on redoing previously released features, rather than creating new values ‚Äã‚Äãfor the business. </li></ul><br><h2>  Group 2 - Quality of the developed product </h2><br>  This group of metrics allows you to evaluate and compare from release to release both the quality of the software and the quality of the development itself. <br><br>  <b>1. The density of defects</b> <br>  ‚ÄúNumber of defects in a separate module‚Äù / ‚ÄúTotal number of defects in software‚Äù <br><br>  It is calculated as the proportion of defects in their total number falling on a separate module within an iteration or release. <br>  Purpose of the metric: highlight which part of the software is the most problematic.  This information will help in assessing and planning work with this module, as well as in risk analysis. <br><br><ul><li>  This metric will help draw our attention to the problem module \ system \ functionality, where the proportion of defects is above average. </li></ul><br>  <b>2. Regression coefficient</b> <br>  "The number of defects in the old functionality" / "The total number of defects, including the new functionality" <br><br>  The purpose of the metric is: to show what the team‚Äôs efforts are going to do - whether it is engaged in more development and debugging of new features or spends most of the time on fixes in already existing parts of the software. <br><br><ul><li>  For example, if the regression coefficient is greater than 0.5, this means that more than half of the time we spend on restoring previously functioning software functions.  This situation needs to be corrected. </li></ul><br>  <b>3. The ratio of re-open defects</b> <br>  "The number of re-detected defects" / "The total number of errors, including previously corrected and new ones" <br><br>  The purpose of the metric: to assess the quality of the development and correction of defects, as well as the complexity of the product or a separate module. <br><br><ul><li>  The closer the coefficient value is to 0, the less the old errors recur. </li><li>  At the same time, if the value is greater than 0.2-0.3, this may indicate either the technical complexity of the module, or problems in the architecture, or poor-quality error correction. </li></ul><br><h2>  Group 3 - Opportunities and effectiveness of the QA team </h2><br>  The main task of this group of metrics is to express in numbers what the testing team is capable of.  These indicators can and should be calculated and compared on a regular basis, analyzing trends, observing how certain changes influence the work of a team. <br><br>  <b>1. Speed ‚Äã‚Äãof operation of the QA command</b> <br>  "The number of story points for N iterations)" / "N" <br><br>  It is calculated as the ratio of realized story points \ requirements \ user stories for several iterations \ sprints to the number of selected iterations. <br>  Purpose of the metric: to express numerically the possibilities, the speed of the team‚Äôs work for the further planning of the scope of work and analysis of development trends.  The metric allows you to monitor the speed of QA, to monitor which internal or external influences on the team affect this speed. <br><br>  <b>2. The average lifetime of the defect</b> <br>  "The total time to correct defects found" / "Number of defects" <br><br>  The total time during which the defects found during the iteration or release were open to the sum of the defects. <br><br>  Purpose of the metric: show how much time on average it takes to work with one defect: its registration, correction and reproduction.  This indicator will allow us to estimate the time required for testing, to highlight areas of software with which the greatest difficulties arise. <br><br><ul><li>  The lifetime of a defect is the entire time from its registration to closure minus all possible work interruptions.  By showing defects with the longest correction time, the metric will allow revealing particularly complex and problematic modules or the ‚Äúweak link‚Äù in the development team. </li></ul><br><h2>  Group 4 - Quality of the testing team </h2><br>  The task of this set of metrics is to assess how well testers perform their tasks, determine the level of competence and maturity of the QA team.  With such a set of indicators, you can compare a team with it at different time intervals or with other, external testing groups. <br><br>  <b>1. The effectiveness of tests and test kits</b> <br>  "The number of errors found" / "The number of cases in the test set" <br><br>  Purpose of the metric: to show how many errors on average our cases can detect.  This metric reflects the quality of the test design and helps to monitor the trend of its change. <br><br><ul><li>  The ‚Äúkilling rate‚Äù of tests allows you to monitor the effectiveness of each of the test kits, as it changes over time.  This will make it possible to supplement them with ‚Äúfresh‚Äù tests in time. </li></ul><br>  <b>2. The ratio of errors missed on the product</b> <br>  "The number of errors detected after the release of the release" / "The total number of errors detected before and after the release" <br><br>  Purpose of the metric: to demonstrate the quality of testing and the efficiency of error detection - what proportion of defects was filtered, and what percentage went to the productive level. <br><br><ul><li>  Of course, the allowable percentage of missed errors will depend on many factors, however, if it is&gt; 0.1, then there is clearly a problem, because in this case, every tenth defect has fallen on the productive side and has led to software failures among users. </li></ul><br>  <b>3. Real time QA team work</b> <br>  "Time spent on target QA activity" / "Total working hours of the team" <br><br>  The ratio of time spent by the team directly on target QA activities to total hours. <br><br>  Purpose of the metric: firstly, to increase the planning accuracy, and secondly, to monitor and manage the team‚Äôs performance. <br><br><ul><li> Target activities may include: analysis, design, evaluation, testing, work meetings, and much more, non-targeted - simple due to blockers, communication problems, inaccessibility of resources, etc. </li><li>  Of course, this metric will never be equal to 1. Practice shows that for effective commands, its value can be 0.5-0.6. </li></ul><br>  <b>4. Accuracy of time estimation for areas \ types \ types of work</b> <br>  "Estimated working time" / "Actual working time" <br><br>  Metric Assignment: allows you to use the correction factor for subsequent evaluations. <br><br><ul><li>  The degree of accuracy of the assessment can be determined for the whole team or individual testers, for the entire system or for individual software modules. </li></ul><br><h2>  Group 5 - Feedback and User Satisfaction </h2><br>  And finally, a group of metrics showing how the product was accepted by end users, how well it met their expectations.  At the same time, as part of evaluating user interaction, not only feedback about the software itself is important to us.  Another significant task of this group of metrics is to show whether users are satisfied with the process of interaction with the IT team in general and QA in particular. <br><br>  <b>1. User satisfaction with IT services</b> <br>  A regular survey of user satisfaction with IT services with scoring. <br><br>  Purpose of the metric: to show whether users trust the IT team, whether they understand how and why their work is organized, how much this work meets expectations. <br><br><ul><li>  A metric can serve as an indicator of the need to focus on optimizing the process or to make it clearer and more transparent for users. </li><li>  The satisfaction rate can be calculated based on the results of the survey on the results of software delivery.  For this you need to collect all the estimates and calculate the average score. </li></ul><br>  <b>2. User satisfaction with the product</b> <br>  Regular user survey on how satisfied they are with the quality of the product. <br>  Purpose of the metric: to determine how the product being developed meets user expectations, whether we are moving in that direction, whether we determine the importance of the features correctly and choose solutions. <br><br><ul><li>  To calculate this metric, we also conduct a user survey and calculate the average score.  By calculating this indicator on a regular basis, you can follow the trend of user satisfaction. </li></ul><br>  <b>3. Stakeholder Involvement</b> <br>  The number of initiatives and proposals for improving the process and product received during the iteration (release) by stakeholders. <br><br>  Purpose of the metric: to determine the degree of participation of external stakeholders (business, infrastructure, users, support, etc.) in the work on the product.  Having such a metric in your hands, you can navigate where you want to get feedback, so that one day you will not encounter problems and misunderstandings. <br><br>  Pavel Novikov, <br>  Program manager <br>  <a href="https://doitsmartly.ru/">https://doitsmartly.ru/</a> </div><p>Source: <a href="https://habr.com/ru/post/340192/">https://habr.com/ru/post/340192/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../340182/index.html">Critical vulnerabilities detected in WPA2 - Key Reinstallation Attacks (KRACK)</a></li>
<li><a href="../340184/index.html">Introduction to Neural Network Architectures</a></li>
<li><a href="../340186/index.html">How to create a competitive advantage and value of a technological product</a></li>
<li><a href="../340188/index.html">Our cloud-based JS is now ES2017, and this reduces the code by several times.</a></li>
<li><a href="../340190/index.html">Russian-speaking chat bot Boltoon: create a virtual companion</a></li>
<li><a href="../340194/index.html">A tricky javascript question asked at Google and Amazon interviews</a></li>
<li><a href="../340196/index.html">ScadaPy application modbus protocol</a></li>
<li><a href="../340198/index.html">Kali Linux: security test questions</a></li>
<li><a href="../340200/index.html">Steve Wozniak opened an online university Woz U</a></li>
<li><a href="../340202/index.html">Love static code analysis</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>