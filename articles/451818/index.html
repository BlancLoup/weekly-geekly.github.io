<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Rook or not Rook - that is the question</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Earlier this month, on May 3, a major release of the ‚Äúmanagement system for distributed data warehouses in Kubernetes‚Äù - Rook 1.0.0 was announced. Mor...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Rook or not Rook - that is the question</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/pp/71/45/pp7145_00rhowbftvzhds-h7bkk.png"><br><br>  Earlier this month, on May 3, a major release of the ‚Äúmanagement system for distributed data warehouses in Kubernetes‚Äù - <a href="https://blog.rook.io/rook-v1-0-a-major-milestone-689ca4c75508"><b>Rook 1.0.0</b></a> was announced.  More than a year ago we <a href="https://habr.com/ru/company/flant/blog/348044/">published a</a> general review of the Rook.  At the same time, we were asked to tell about the experience of its <b>use in practice</b> - and now, in time for such a significant milestone in the history of the project, we are happy to share our accumulated impressions. <br><br>  In short, Rook is a set of <a href="https://habr.com/ru/company/flant/blog/326414/">operators</a> for Kubernetes that fully take control of deployment, management, automatic recovery of storage solutions such as Ceph, EdgeFS, Minio, Cassandra, CockroachDB. <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      At the moment, the most developed (and <a href="https://github.com/rook/rook">only</a> in a <b>stable</b> stage) solution is the <a href="">rook-ceph-operator</a> . <br><br>  <i><b>Note</b> : Among the significant changes in the release of Rook 1.0.0 related to Ceph are Ceph Nautilus support and the ability to use NFS for CephFS or RGW buckets.</i>  <i>Of the others, ‚Äúripening‚Äù support for EdgeFS to beta level stands out.</i> <br><br>  So, in this article we: <br><br><ul><li>  answer the question of what advantages we see in using Rook to deploy Ceph in the Kubernetes cluster; </li><li>  share experiences and impressions of using Rook in production; </li><li>  We will tell why we say ‚ÄúYes!‚Äù to Rook, and about our plans for him. </li></ul><br>  Let's start with the general concepts and theories. <br><br><h2>  ‚ÄúI have the advantage of one Ladu!‚Äù (Unknown chess player) </h2><br><img src="https://habrastorage.org/webt/qb/su/7j/qbsu7jt4qtrayeg5vg21-wilmrw.png"><br><br>  One of the main advantages of Rook is that the interaction with data warehouses is carried out through the mechanisms of Kubernetes.  This means that you no longer need to copy the commands to configure Ceph from a piece of paper to the console. <br><br>  <i>- Want to deploy in a CephFS cluster?</i>  <i>Just write the YAML file!</i> <i><br></i>  <i>- What?</i>  <i>Do you want to expand the object store with the S3 API?</i>  <i>Just write the second YAML file!</i> <br><br>  Rook created by all the rules of a typical operator.  Interaction with it takes place with the help of <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/">CRD (Custom Resource Definitions)</a> , in which we describe the characteristics of Ceph entities we need <i>(since this is the only stable implementation, by default the article will talk about Ceph unless explicitly stated otherwise)</i> .  According to the set parameters, the operator will automatically execute the commands necessary for setting up. <br><br>  Let's take a concrete look at the example of creating the Object Store, or rather, <code>CephObjectStoreUser</code> . <br><br><pre> <code class="plaintext hljs">apiVersion: ceph.rook.io/v1 kind: CephObjectStore metadata: name: {{ .Values.s3.crdName }} namespace: kube-rook spec: metadataPool: failureDomain: host replicated: size: 3 dataPool: failureDomain: host erasureCoded: dataChunks: 2 codingChunks: 1 gateway: type: s3 sslCertificateRef: port: 80 securePort: instances: 1 allNodes: false --- apiVersion: ceph.rook.io/v1 kind: CephObjectStoreUser metadata: name: {{ .Values.s3.crdName }} namespace: kube-rook spec: store: {{ .Values.s3.crdName }} displayName: {{ .Values.s3.username }}</code> </pre> <br>  The parameters listed in the listing are fairly standard and hardly need comments, but you should pay particular attention to those that are highlighted in template variables. <br><br>  The general scheme of work is reduced to the fact that through the YAML file we ‚Äúorder‚Äù resources, for which the operator executes the necessary commands and returns to us the ‚Äúnot very real‚Äù secret with which we can continue to work <i>(see below)</i> .  And from the variables listed above, the command and the name of the secret will be composed. <br><br>  What is this team?  When creating a user for object storage, the Rook operator inside the pod will do the following: <br><br><pre> <code class="plaintext hljs">radosgw-admin user create --uid="rook-user" --display-name="{{ .Values.s3.username }}"</code> </pre> <br>  The result of this command is the JSON structure: <br><br><pre> <code class="json hljs">{ <span class="hljs-attr"><span class="hljs-attr">"user_id"</span></span>: <span class="hljs-string"><span class="hljs-string">"rook-user"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"display_name"</span></span>: <span class="hljs-string"><span class="hljs-string">"{{ .Values.s3.username }}"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"keys"</span></span>: [ { <span class="hljs-attr"><span class="hljs-attr">"user"</span></span>: <span class="hljs-string"><span class="hljs-string">"rook-user"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"access_key"</span></span>: <span class="hljs-string"><span class="hljs-string">"NRWGT19TWMYOB1YDBV1Y"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"secret_key"</span></span>: <span class="hljs-string"><span class="hljs-string">"gr1VEGIV7rxcP3xvXDFCo4UDwwl2YoNrmtRlIAty"</span></span> } ], ... }</code> </pre> <br>  <code>Keys</code> is what future applications will need to access object storage via the S3 API.  The Rook operator kindly selects them and adds them to your namespace as a secret with the name <code>rook-ceph-object-user-{{ $.Values.s3.crdName }}-{{ $.Values.s3.username }}</code> . <br><br>  To use the data from this secret, it is enough to add them to the container as environment variables.  As an example, I will give a template for Job, in which we automatically create buckets for each user environment: <br><br><pre> <code class="plaintext hljs">{{- range $bucket := $.Values.s3.bucketNames }} apiVersion: batch/v1 kind: Job metadata: name: create-{{ $bucket }}-bucket-job annotations: "helm.sh/hook": post-install "helm.sh/hook-weight": "2" spec: template: metadata: name: create-{{ $bucket }}-bucket-job spec: restartPolicy: Never initContainers: - name: waitdns image: alpine:3.6 command: ["/bin/sh", "-c", "while ! getent ahostsv4 rook-ceph-rgw-{{ $.Values.s3.crdName }}; do sleep 1; done" ] - name: config image: rook/ceph:v1.0.0 command: ["/bin/sh", "-c"] args: ["s3cmd --configure --access_key=$(ACCESS-KEY) --secret_key=$(SECRET-KEY) -s --no-ssl --dump-config | tee /config/.s3cfg"] volumeMounts: - name: config mountPath: /config env: - name: ACCESS-KEY valueFrom: secretKeyRef: name: rook-ceph-object-user-{{ $.Values.s3.crdName }}-{{ $.Values.s3.username }} key: AccessKey - name: SECRET-KEY valueFrom: secretKeyRef: name: rook-ceph-object-user-{{ $.Values.s3.crdName }}-{{ $.Values.s3.username }} key: SecretKey containers: - name: create-bucket image: rook/ceph:v1.0.0 command: - "s3cmd" - "mb" - "--host=rook-ceph-rgw-{{ $.Values.s3.crdName }}" - "--host-bucket= " - "s3://{{ $bucket }}" ports: - name: s3-no-sll containerPort: 80 volumeMounts: - name: config mountPath: /root volumes: - name: config emptyDir: {} --- {{- end }}</code> </pre> <br>  All the actions listed in this Job were performed without going beyond Kubernetes.  The structures described in the YAML files are folded into a Git repository and reused many times.  In this we see a huge plus for DevOps engineers and the CI / CD process as a whole. <br><br><h2>  With Rook and Rados in joy </h2><br>  Using the Ceph + RBD bundle imposes certain restrictions on mounting volumes to pods. <br><br>  In particular, the namespace must have a secret to access Ceph in order for stateful applications to function.  Normally, if you have 2-3 environments in your namespaces: you can go and copy the secret manually.  But what to do if for each feature for developers a separate environment with its own namespace is created? <br><br>  We solved this problem with the help of a <a href="https://github.com/flant/shell-operator">shell-operator</a> , which automatically copied secrets to new namespaces (an example of such a hook is described in <a href="https://habr.com/ru/company/flant/blog/447442/">this article</a> ). <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#! /bin/bash if [[ $1 == ‚Äú--config‚Äù ]]; then cat &lt;&lt;EOF {"onKubernetesEvent":[ {"name": "OnNewNamespace", "kind": "namespace", "event": ["add"] } ]} EOF else NAMESPACE=$(kubectl get namespace -o json | jq '.items | max_by( .metadata.creationTimestamp ) | .metadata.name') kubectl -n ${CEPH_SECRET_NAMESPACE} get secret ${CEPH_SECRET_NAME} -o json | jq ".metadata.namespace=\"${NAMESPACE}\"" | kubectl apply -f - fi</span></span></code> </pre> <br>  However, when using Rook, this problem simply does not exist.  The mounting process takes place with the help of our own drivers based on <a href="">Flexvolume</a> or <a href="https://habr.com/ru/company/flant/blog/424211/">CSI</a> (while in the beta stage) and therefore does not require secrets. <br><br>  Rook automatically solves many problems, which pushes us to use it in new projects. <br><br><h2>  Siege Rook </h2><br>  We conclude the practical part of the unfolding of Rook and Ceph for the possibility of conducting their own experiments.  In order to take this impregnable tower by storm it was easier, the developers prepared the Helm-package.  Let's download it: <br><br><pre> <code class="bash hljs">$ helm fetch rook-master/rook-ceph --untar --version 1.0.0</code> </pre> <br>  There are many different settings in the <code>rook-ceph/values.yaml</code> file.  The most important thing is to specify tolerations for agents and searches.  For what you can use the taints / tolerations mechanism, we described in detail in <a href="https://habr.com/ru/company/flant/blog/432748/">this article</a> . <br><br>  In short, we do not want the pods with the client application to be located on the same nodes as the disks for data storage.  The reason is simple: so the work of the Rook agents will not affect the application itself. <br><br>  So, open the <code>rook-ceph/values.yaml</code> favorite editor and add the following block to the end: <br><br><pre> <code class="plaintext hljs">discover: toleration: NoExecute tolerationKey: node-role/storage agent: toleration: NoExecute tolerationKey: node-role/storage mountSecurityMode: Any</code> </pre> <br>  For each node reserved for data storage, add the corresponding taint: <br><br><pre> <code class="bash hljs">$ kubectl taint node <span class="hljs-variable"><span class="hljs-variable">${NODE_NAME}</span></span> node-role/storage=<span class="hljs-string"><span class="hljs-string">""</span></span>:NoExecute</code> </pre> <br>  After that set the helm-chart with the command: <br><br><pre> <code class="bash hljs">$ helm install --namespace <span class="hljs-variable"><span class="hljs-variable">${ROOK_NAMESPACE}</span></span> ./rook-ceph</code> </pre> <br>  Now you need to create a cluster and specify the location of the <a href="http://docs.ceph.com/docs/mimic/man/8/ceph-osd/">OSD</a> : <br><br><pre> <code class="plaintext hljs">apiVersion: ceph.rook.io/v1 kind: CephCluster metadata: clusterName: "ceph" finalizers: - cephcluster.ceph.rook.io generation: 1 name: rook-ceph spec: cephVersion: image: ceph/ceph:v13 dashboard: enabled: true dataDirHostPath: /var/lib/rook/osd mon: allowMultiplePerNode: false count: 3 network: hostNetwork: true rbdMirroring: workers: 1 placement: all: tolerations: - key: node-role/storage operator: Exists storage: useAllNodes: false useAllDevices: false config: osdsPerDevice: "1" storeType: filestore resources: limits: memory: "1024Mi" requests: memory: "1024Mi" nodes: - name: host-1 directories: - path: "/mnt/osd" - name: host-2 directories: - path: "/mnt/osd" - name: host-3 directories: - path: "/mnt/osd"</code> </pre> <br>  Check the status of Ceph - expect to see <code>HEALTH_OK</code> : <br><br><pre> <code class="bash hljs">$ kubectl -n <span class="hljs-variable"><span class="hljs-variable">${ROOK_NAMESPACE}</span></span> <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> $(kubectl -n <span class="hljs-variable"><span class="hljs-variable">${ROOK_NAMESPACE}</span></span> get pod -l app=rook-ceph-operator -o name -o jsonpath=<span class="hljs-string"><span class="hljs-string">'{.items[0].metadata.name}'</span></span>) -- ceph -s</code> </pre> <br>  At the same time, we will check that the pods with the client application do not fall on the nodes reserved for Ceph: <br><br><pre> <code class="bash hljs">$ kubectl -n <span class="hljs-variable"><span class="hljs-variable">${APPLICATION_NAMESPACE}</span></span> get pods -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName</code> </pre> <br>  Further optional components are customized.  More information about them is specified in the <a href="https://github.com/rook/rook/tree/release-1.0/cluster/examples/kubernetes/ceph">documentation</a> .  For administration, we strongly recommend installing a dashboard and toolbox. <br><br><h2>  Rook'i-hooks: Is Rook enough for everything? </h2><br>  As you can see, the development of Rook is in full swing.  But there are still problems that do not allow us to completely abandon the manual configuration of Ceph: <br><br><ul><li>  No Rook driver <a href="https://github.com/rook/rook/issues/1659">can</a> export metrics for using mounted blocks, which deprives us of monitoring. </li><li>  Flexvolume and CSI <a href="https://github.com/rook/rook/issues/1169">cannot</a> resize volumes (unlike RBD), so Rook loses a useful (and sometimes critical!) Tool. </li><li>  Rook is still not as flexible as regular Ceph.  If we want to configure the CephFS metadata pool to be stored on the SSD, and the data itself on the HDD, we will need to manually register separate groups of devices in CRUSH maps. </li><li>  Despite the fact that the rook-ceph-operator is considered stable, at the moment there are certain problems when upgrading Ceph from version 13 to 14. </li></ul><br><h2>  findings </h2><br>  <i>‚ÄúNow Rook is closed from the outside world with pawns, but we believe that one day she will play a decisive role in the game!‚Äù (The quotation was invented specifically for this article)</i> <br><br>  The Rook project, undoubtedly, won our hearts - we believe that [with all its pluses and minuses] it definitely deserves your attention. <br><br>  Our future plans boil down to making the rook-ceph a module for an <a href="https://github.com/flant/addon-operator">addon-operator</a> , which will make its use in our many Kubernetes clusters even easier and more convenient. <br><br><h2>  PS </h2><br>  Read also in our blog: <br><br><ul><li>  ‚Äú <a href="https://habr.com/ru/company/flant/blog/348044/">Rook is a‚Äú self-service ‚Äùdata store for Kubernetes</a> ‚Äù; </li><li>  ‚Äú <a href="https://habr.com/ru/company/flant/blog/329666/">Create a permanent repository with Ceph based provisioning in Kubernetes</a> ‚Äù; </li><li>  " <a href="https://habr.com/ru/company/flant/blog/431500/">Databases and Kubernetes (review and video of the report)</a> "; </li><li>  ‚Äú <a href="https://habr.com/ru/company/flant/blog/447442/">Introducing a shell-operator: creating operators for Kubernetes is even easier</a> ‚Äù; </li><li>  " <a href="https://habr.com/ru/company/flant/blog/326414/">Operators for Kubernetes: how to run stateful applications</a> ." </li></ul></div><p>Source: <a href="https://habr.com/ru/post/451818/">https://habr.com/ru/post/451818/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../451802/index.html">.Net Community Raiffeisenbank invites you to meet UPD Broadcast</a></li>
<li><a href="../451806/index.html">iOS Digest 5 (April 27 - May 16)</a></li>
<li><a href="../45181/index.html">Free alternative</a></li>
<li><a href="../451812/index.html">Now, good developers are measured by views and subscribers - and this is bad</a></li>
<li><a href="../451814/index.html">RBKmoney Payments under the hood - the infrastructure of the payment platform</a></li>
<li><a href="../451820/index.html">Thematic Habramitap # 1: Backend Development</a></li>
<li><a href="../451826/index.html">Marketing sales boosting absurd: proven cases</a></li>
<li><a href="../451828/index.html">The main secret of Google I / O 2019, which is not to learn from the Internet</a></li>
<li><a href="../451830/index.html">Briefly with the implementation of the AES 128 ECB</a></li>
<li><a href="../451832/index.html">How to understand when a proxy is lying: verification of the physical locations of network proxies using an active geolocation algorithm</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>