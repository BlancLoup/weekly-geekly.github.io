<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Handwriting recognition using Python and scikit</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hey. Surely many are interested in methods of machine learning and solving various problems that are not solved by conventional approaches. Recently, ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Handwriting recognition using Python and scikit</h1><div class="post__text post__text-html js-mediator-article">  Hey.  Surely many are interested in methods of machine learning and solving various problems that are not solved by conventional approaches.  Recently, I was fortunate enough to attend the Data Mining course organized as part of the <a href="http://www.gamechangers.ru/">GameChangers</a> program.  The first homework was to make a <a href="http://www.kaggle.com/">Kaggle</a> submit - solve the problem of <a href="http://www.kaggle.com/c/digit-recognizer">Digit Recognizer</a> . <br><a name="habracut"></a><br><h5>  Briefly about the data </h5><br>  The data for training is a csv-table, in the first column of which are the numerical values ‚Äã‚Äãof written numbers, in the rest - 784 saturation values ‚Äã‚Äãof individual pixels (pictures are black and white).  Test data - the same table, but without answers. <br><br><h5>  Methods </h5><br><ul><li>  Random forest </li><li>  kNN method </li><li>  SVM with quadratic core </li><li>  Cubic SVM </li><li>  Method Ensemble </li></ul><br><br>  All calculations were performed in Python using the <a href="http://scikit-learn.org/dev/index.html">scikit</a> library, links to distributions and tutorials are attached below. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      I deliberately do not talk here about optimizations that can and should have been done on the algorithms used, because otherwise the post about homework will grow to the abstract. <br><br><h5>  Random forest </h5><br><h6>  The idea of ‚Äã‚Äãthe algorithm </h6><br>  Random Forest uses an ensemble of decision trees.  In itself, the decision tree does not provide sufficient accuracy for this task, but is very fast.  The RF algorithm teaches k decision trees on parameters randomly selected for each tree (in our case, the parameters are the brightness of individual pixels), after which each test is voted on among the trained ensemble.  The construction of this algorithm is based on the idea that if you aggregate data from a large number of different weak algorithms, reducing them into a single answer, the result is likely to be better than that of a single powerful algorithm. <br><br><h6>  Implementation </h6><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> savetxt, loadtxt <span class="hljs-comment"><span class="hljs-comment"># - csv- from sklearn.ensemble import RandomForestClassifier #      from sklearn.externals import joblib #   #         dataset = loadtxt(open('train.csv', 'r'), dtype='f8', delimiter=',', skiprows=1) joblib.dump(dataset, 'training_set.pkl') dataset = joblib.load('training_set.pkl') #     test = loadtxt(open('test.csv', 'r'), dtype='f8', delimiter=',', skiprows=1) joblib.dump(test, 'test_set.pkl') test = joblib.load('test_set.pkl') # n_estimators -    , n_jobs -  ,     forest = RandomForestClassifier(n_estimators = 1000, n_jobs = 4) #        target = [x[0] for x in dataset] # train = [x[1:] for x in dataset] forest.fit(train, target) #  # predict()    (   numpy),    -  1x28000,      csv- savetxt('answer.csv', forest.predict(test), delimiter=',', fmt='%d')</span></span></code> </pre> <br><br><h6>  Summary </h6><br>  On four cores, the training + solution took an hour and a half; this is the slowest algorithm of those considered.  However, this is the second most accurate algorithm, which, when submitted to Kaggle, has more than 96% accuracy. <br>  There is a problem of optimal selection of the number of trees: for this task several runs were performed with different values ‚Äã‚Äãof n_estimators, 1000 gave the best result. <br><br><h5>  kNN - method </h5><br><h6>  The idea of ‚Äã‚Äãthe algorithm </h6><br>  One of the fastest classification algorithms.  In short: all entities used in training are pushed into a metric space of a dimension equal to the number of parameters.  After that, when classifying a single vector, k vectors from the training set closest to the studied one are considered. <br>  The main question when using this algorithm is the choice of the number k.  When k = 1, the algorithm loses stability and begins to behave inadequately when noise appears, when k is close to the number of vectors of the training set, the accuracy becomes redundant and the algorithm degenerates. <br><br>  I started the algorithm with different k and finally settled on the initially recommended k = 10 <br><br><h6>  Implementation </h6><br>  This is all similar to the previous source, moreover, the dumps obtained during the work of Random Forest are used. <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> neighbors <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.externals <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> joblib <span class="hljs-comment"><span class="hljs-comment">#    ,      dataset = joblib.load('training_set.pkl') target = [x[0] for x in dataset] #    train = [x[1:] for x in dataset] # - clf = neighbors.KNeighborsClassifier(n_neighbors = 10, weights='uniform') clf.fit(train, target) # # test = joblib.load('test_set.pkl') savetxt('knn_answer.csv', clf.predict(test), delimiter=',', fmt='%d')</span></span></code> </pre><br><br><h6>  Summary </h6><br>  The kNN also showed accuracy above 96% with shorter running times. <br><br><h5>  SVM </h5><br>  SVM (Support Vector Machine) is one of the most universal classification methods, distinguished by speed and high reliability.  Looking ahead, I will say that with its use, the highest accuracy among all the algorithms used was 97.7% <br><br><h6>  The idea of ‚Äã‚Äãthe algorithm </h6><br>  SVM classifies vectors located in multidimensional space, similar to that used in kNN, separated from the hyperplane, having dimensions n-1, where n is the dimension of the original space. <br><br><h6>  Kernels </h6><br><br>  The SVM method has one of the key hyperparameters, called the kernel.  The scikit library supports all major cores used: linear, radial and polynomial.  I will give the statistics of tests on small samples of tagged data: <br><br>  Accuracy with a linear core = 0.9131 <br>  Accuracy with a radial core = 0.1265 <br>  Accuracy with a quadratic polynomial = 0.9635 <br>  Accuracy with a cubic polynomial = 0.9595 <br><br>  After these tests, I dropped the options with the linear and radial cores and drove the algorithm on the largest possible data set using the quadratic and cubic cores.  Quadratic gave greater accuracy, using it, in the end and got the final solution. <br><br><h6>  Release </h6><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.externals <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> joblib <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> savetxt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> svm <span class="hljs-comment"><span class="hljs-comment">#       dataset = joblib.load('training_set.pkl') target = [x[0] for x in dataset] train = [x[1:] for x in dataset] # .  kernel   , degree -   .  , ,  3. clf_poly2 = svm.SVC(kernel = "poly", degree = 2) clf_poly2.fit(train, target) # test = joblib.load('test_set.pkl') #  savetxt('svm_answer.csv', clf_poly2.predict(test), delimiter=',', fmt='%d')</span></span></code> </pre><br><br><h6>  Summary </h6><br>  It works quite quickly, with any core overtakes Random Forest, the accuracy of the normalized data is excellent. <br><br><h5>  Method Ensemble </h5><br>  At the courses we were given an additional task - to see how the accuracy of the solution would change when using several algorithms at the same time. <br>  I wanted to cluster the training sample on superpixels and see what happens, but I didn‚Äôt meet the deadline, I had to pass a simpler option - the answers given by the RF, SVM and kNN algorithms were read, and then a direct vote was taken.  If opinions were divided, preference was given to SVM, as a bit more accurate.  However, this ensemble gave a half-percent solution worse than what was obtained on pure SVM, so I did not succeed in improving the solution. <br><br><h5>  Links </h5><br>  <a href="http://scikit-learn.org/stable/">Library used</a> <br>  <a href="http://www.machinelearning.ru/wiki">Wiki with a detailed description of the algorithms</a> <br>  <a href="http://scikit-learn.org/dev/modules/generated/sklearn.ensemble.RandomForestClassifier.html">Documentation for the class implementing Random Forest</a> <br>  <a href="http://scikit-learn.org/dev/modules/generated/sklearn.svm.SVC.html">SVM description with a small example</a> <br>  <a href="http://scikit-learn.org/stable/modules/svm.html">More information on SVM</a> <br>  <a href="http://scikit-learn.org/dev/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">All about kNN</a> <br><br>  That's all.  In the next post we will discuss methods for optimizing the parameters of the algorithms themselves. </div><p>Source: <a href="https://habr.com/ru/post/171723/">https://habr.com/ru/post/171723/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../171709/index.html">Combining responsive layout and templates for mobile</a></li>
<li><a href="../171711/index.html">Introduction to libuniset - library for creating ACS</a></li>
<li><a href="../171713/index.html">Review of Ubuntu Touch for tablets</a></li>
<li><a href="../171719/index.html">How to increase the IQ of the network button without interfering with the operator‚Äôs brain</a></li>
<li><a href="../171721/index.html">Russian startups at the 2013 London Web Summit</a></li>
<li><a href="../171725/index.html">Non-standard cost-saving: how we learned to repair the Nortel 1120 office IP phones by ourselves</a></li>
<li><a href="../171727/index.html">Lean. Part 1. Kanban board in a new way.</a></li>
<li><a href="../171729/index.html">CeBIT 2013: SMI Augmented Eyewear Goggles</a></li>
<li><a href="../171731/index.html">JavaScript Prototypes for C / C ++ / C # / Java Programmers</a></li>
<li><a href="../171733/index.html">Nirvana for Testers - Nerrvana</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>