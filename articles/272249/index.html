<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Proxmox 4.0: Root partition on ZFS RAID1 or how to increase fault tolerance if the server has only 2 disks</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The task turned up for me - to run Proxmox and several virtual locks on a server with just 2 disks. At the same time, it was necessary to ensure, at l...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Proxmox 4.0: Root partition on ZFS RAID1 or how to increase fault tolerance if the server has only 2 disks</h1><div class="post__text post__text-html js-mediator-article">  The task turned up for me - to run Proxmox and several virtual locks on a server with just 2 disks.  At the same time, it was necessary to ensure, at least, some reliability and ease of correcting the problems associated with the failure of one of the disks.  Further in the note is a detailed description of testing solutions on the stand. <br><a name="habracut"></a><br><h4>  Introductory </h4><br>  I believe that the reader of this note can independently install Proxmox on the node and will not consider installing and configuring the hypervisor itself.  Consider only the settings relating to ZFS RAID1 and testing the situation of failure of one of the disks. <br>  The hardware on which the project was to be deployed was the Supermicro node, apparently performed by 2 node in 1U with a pseudo-raid integrated into the Intel chipset which is not supported in Proxmox.  In this regard, we will try to test the solution offered "out of the box" in version 4.0.  Even if you kill me, I don‚Äôt remember if this was an installation option in Proxmox 3.6, maybe it was, but it wasn‚Äôt stored in the memory due to the lack of demand for such a configuration.  In the test rack, we found a similar server and I began to check the solution provided by the guys from Proxmox Server Solutions. <br><br><h4>  Installation </h4><br>  As I warned, I will not show the installation completely, I‚Äôll only focus on the important points. <br><br>  Choose zfs RAID1: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/files/356/43b/3fa/35643b3fa54142cdb8f175103e337a6e.jpg" alt="image"><br><br>  Server test and no subscription to a commercial repository.  In /etc/apt/sources.list we connect the free: <br><br><pre><code class="bash hljs">deb http://download.proxmox.com/debian jessie pve-no-subscription</code> </pre> <br>  In /etc/apt/sources.list.d/pve-enterprise.list we comment out the commercial one. <br><br>  Well, suddenly forget: <br><br><pre> <code class="bash hljs">root@pve1:~<span class="hljs-comment"><span class="hljs-comment"># apt-get update &amp;&amp; apt-get upgrade</span></span></code> </pre><br>  We look that to us the installer on disks cut (I provide only a part of the output): <br><br><pre> <code class="bash hljs">root@pve1:~<span class="hljs-comment"><span class="hljs-comment"># fdisk -l /dev/sd* Disk /dev/sda: 149.1 GiB, 160041885696 bytes, 312581808 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: gpt Disk identifier: 758FA29C-4F49-4315-BA0C-F3CCC921FA01 Device Start End Sectors Size Type /dev/sda1 34 2047 2014 1007K BIOS boot /dev/sda2 2048 312565389 312563342 149G Solaris /usr &amp; Apple ZFS /dev/sda9 312565390 312581774 16385 8M Solaris reserved 1 Disk /dev/sdb: 149.1 GiB, 160041885696 bytes, 312581808 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: gpt Disk identifier: 3CD4B489-A51D-4354-8018-B1391F52B08D Device Start End Sectors Size Type /dev/sdb1 34 2047 2014 1007K BIOS boot /dev/sdb2 2048 312565389 312563342 149G Solaris /usr &amp; Apple ZFS /dev/sdb9 312565390 312581774 16385 8M Solaris reserved 1</span></span></code> </pre><br>  Look at our array: <br><br><pre> <code class="bash hljs">root@pve1:~<span class="hljs-comment"><span class="hljs-comment"># zpool status rpool pool: rpool state: ONLINE scan: none requested config: NAME STATE READ WRITE CKSUM rpool ONLINE 0 0 0 mirror-0 ONLINE 0 0 0 sda2 ONLINE 0 0 0 sdb2 ONLINE 0 0 0 errors: No known data errors</span></span></code> </pre><br>  By default, the Proxmox installer installed a bootloader on both partitions - great! <br><br><h4>  Testing </h4><br>  We simulate a hard disk failure as follows: <br>  - turn off the server; <br>  - pull out one of the baskets; <br>  - turn on the server. <br><br>  The server loads perfectly on any of the remaining disks, the array works in the DEGRADED mode and kindly prompts which disk we need to change and how to do it: <br><br><pre> <code class="bash hljs">root@pve1:~<span class="hljs-comment"><span class="hljs-comment"># zpool status rpool pool: rpool state: DEGRADED status: One or more devices could not be used because the label is missing or invalid. Sufficient replicas exist for the pool to continue functioning in a degraded state. action: Replace the device using 'zpool replace'. see: http://zfsonlinux.org/msg/ZFS-8000-4J scan: none requested config: NAME STATE READ WRITE CKSUM rpool DEGRADED 0 0 0 mirror-0 DEGRADED 0 0 0 14981255989033513363 FAULTED 0 0 0 was /dev/sda2 sda2 ONLINE 0 0 0 errors: No known data errors</span></span></code> </pre><br>  If you put the recovered disk back in place, it perfectly ‚Äúrises‚Äù back into the mirror: <br><br><pre> <code class="bash hljs">root@pve1:~<span class="hljs-comment"><span class="hljs-comment"># zpool status rpool pool: rpool state: ONLINE status: One or more devices has experienced an unrecoverable error. An attempt was made to correct the error. Applications are unaffected. action: Determine if the device needs to be replaced, and clear the errors using 'zpool clear' or replace the device with 'zpool replace'. see: http://zfsonlinux.org/msg/ZFS-8000-9P scan: resilvered 1.29M in 0h0m with 0 errors on Wed Dec 2 08:37:46 2015 config: NAME STATE READ WRITE CKSUM rpool ONLINE 0 0 0 mirror-0 ONLINE 0 0 0 sda2 ONLINE 0 0 3 sdb2 ONLINE 0 0 0 errors: No known data errors</span></span></code> </pre><br>  We stage the replacement of a disk with a new one.  I just took another basket with the same disk from the old server.  Put the basket on the hot for greater likelihood: <br><br><pre> <code class="bash hljs">root@pve1:~<span class="hljs-comment"><span class="hljs-comment"># fdisk -l /dev/sdb Disk /dev/sdb: 149.1 GiB, 160041885696 bytes, 312581808 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: gpt Disk identifier: 38BE38AC-00D9-4680-88FC-0876378526BC Device Start End Sectors Size Type /dev/sdb1 40 409639 409600 200M EFI System /dev/sdb2 411648 312580095 312168448 148.9G Microsoft basic data</span></span></code> </pre><br>  Conditionally faulty disk is <b>/ dev / sdb</b> and taking into account the same capacity and geometry we copy the 1: 1 partition table from the working disk <b>/ dev / sda</b> <br><br><pre> <code class="bash hljs">root@pve1:~<span class="hljs-comment"><span class="hljs-comment"># sgdisk -R /dev/sdb /dev/sda The operation has completed successfully.</span></span></code> </pre><br>  Generating unique UUIDs for / dev / sdb <br><pre> <code class="bash hljs">root@pve1:~<span class="hljs-comment"><span class="hljs-comment"># sgdisk -G /dev/sdb The operation has completed successfully.</span></span></code> </pre><br>  Put the bootloader on the replaced disk and update GRUB: <br><br><pre> <code class="bash hljs">root@pve1:~<span class="hljs-comment"><span class="hljs-comment"># grub-install --recheck /dev/sdb Installing for i386-pc platform. Installation finished. No error reported. root@pve1:~# update-grub Generating grub configuration file ... Found linux image: /boot/vmlinuz-4.2.3-2-pve Found initrd image: /boot/initrd.img-4.2.3-2-pve Found linux image: /boot/vmlinuz-4.2.2-1-pve Found initrd image: /boot/initrd.img-4.2.2-1-pve Found memtest86+ image: /ROOT/pve-1@/boot/memtest86+.bin Found memtest86+ multiboot image: /ROOT/pve-1@/boot/memtest86+_multiboot.bin done root@pve1:~# update-initramfs -u update-initramfs: Generating /boot/initrd.img-4.2.3-2-pve</span></span></code> </pre><br>  It remains only to replace the failed disk in the array with a newly installed one, but one problem emerges here, generated by the method of addressing disks in the array used in the installer.  Namely, the disks are included in the array by the physical address and the <i>zpool replace rpool / dev / sdb2 command</i> will show us this figure: <br><br><pre> <code class="bash hljs">root@pve1:~<span class="hljs-comment"><span class="hljs-comment"># zpool replace rpool /dev/sdb2 cannot replace /dev/sdb2 with /dev/sdb2: /dev/sdb2 is busy</span></span></code> </pre><br>  What is absolutely logical, you can not change the failed disk to <b>/ dev / sdb2</b> because the failed disk is <b>/ dev / sdb2</b> , and why should we repeat the installer flaw?  We tie the disk by UUID, I have already forgotten the time when the disks were nailed like / dev / sdXX - our UUID is all: <br><br><pre> <code class="bash hljs">root@pve1:~<span class="hljs-comment"><span class="hljs-comment"># zpool replace rpool /dev/disk/by-partuuid/cf590df4-72b7-4cfc-a965-001ffe56d0c9 Make sure to wait until resilver is done before rebooting.</span></span></code> </pre><br>  We were warned about the need to wait for the end of synchronization before rebooting.  Check the status of the array: <br><br><pre> <code class="bash hljs">root@pve1:~<span class="hljs-comment"><span class="hljs-comment"># zpool status rpool pool: rpool state: ONLINE status: One or more devices is currently being resilvered. The pool will continue to function, possibly in a degraded state. action: Wait for the resilver to complete. scan: resilver in progress since Wed Dec 2 18:07:01 2015 92.8M scanned out of 920M at 8.44M/s, 0h1m to go 92.5M resilvered, 10.09% done config: NAME STATE READ WRITE CKSUM rpool ONLINE 0 0 0 mirror-0 ONLINE 0 0 0 sda2 ONLINE 0 0 0 cf590df4-72b7-4cfc-a965-001ffe56d0c9 ONLINE 0 0 0 (resilvering) errors: No known data errors</span></span></code> </pre><br>  For the general order, we include sda2 in the array using the UUID: <br><br><pre> <code class="bash hljs">root@pve1:~<span class="hljs-comment"><span class="hljs-comment"># zpool detach rpool /dev/sda2 root@pve1:~# zpool attach rpool /dev/disk/by-partuuid/cf590df4-72b7-4cfc-a965-001ffe56d0c9 /dev/disk/by-partuuid/8263d908-e9a8-4ace-b01e-0044fa519037 Make sure to wait until resilver is done before rebooting.</span></span></code> </pre><br>  While I copied the previous 2 commands from the console to the editor, the array was already synchronized: <br><br><pre> <code class="bash hljs">root@pve1:~<span class="hljs-comment"><span class="hljs-comment"># zpool status rpool pool: rpool state: ONLINE scan: resilvered 920M in 0h1m with 0 errors on Wed Dec 2 18:36:37 2015 config: NAME STATE READ WRITE CKSUM rpool ONLINE 0 0 0 mirror-0 ONLINE 0 0 0 cf590df4-72b7-4cfc-a965-001ffe56d0c9 ONLINE 0 0 0 8263d908-e9a8-4ace-b01e-0044fa519037 ONLINE 0 0 0 errors: No known data errors</span></span></code> </pre><br><h4>  Conclusion </h4><br>  When there is no hardware Raid controller, it is quite convenient to apply the placement of the root partition on the available out of the box Proxmox 4.0 zfs RAID1.  Of course, there is always the option of transferring / boot and the root to mirrors created by mdadm, which has also been used repeatedly by me and still does not work on several servers, but the considered option is simpler and is offered by the developers out of the box. <br><br><h4>  Addition </h4><br>  There were a few questions, until the hands reached only the autoexpand health check.  Test drives for 160Gb were replaced with 500Gb. <br><br>  After replacing the first disk: <br><pre> <code class="bash hljs">root@testve1:~<span class="hljs-comment"><span class="hljs-comment"># zpool list NAME SIZE ALLOC FREE EXPANDSZ FRAG CAP DEDUP HEALTH ALTROOT rpool 149G 69.3G 79.7G 317G 28% 46% 1.00x ONLINE -</span></span></code> </pre><br><br>  After replacing the second: <br><pre> <code class="bash hljs">root@testve1:~<span class="hljs-comment"><span class="hljs-comment"># zpool list NAME SIZE ALLOC FREE EXPANDSZ FRAG CAP DEDUP HEALTH ALTROOT rpool 465G 69.3G 396G - 9% 14% 1.00x ONLINE -</span></span></code> </pre><br><br>  All manipulations happen online, without rebooting the server and stopping the virtual machines. <br><br>  I made instructions for the classic installation of Proxmox on soft raid1, there are many such instructions, but the 4th version has its own small details.  Who cares - <a href="http://habrahabr.ru/post/273391/">we read</a> </div><p>Source: <a href="https://habr.com/ru/post/272249/">https://habr.com/ru/post/272249/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../272237/index.html">Master class by Boris Wolfson. Agile basics</a></li>
<li><a href="../272239/index.html">Bug with GPU and Canvas in Google Chrome under Windows</a></li>
<li><a href="../272241/index.html">Elasticweb - the first adaptive virtual hosting</a></li>
<li><a href="../272243/index.html">The most dangerous feature in the C / C ++ world</a></li>
<li><a href="../272245/index.html">Sparrow - perl framework for testing and monitoring web applications</a></li>
<li><a href="../272251/index.html">Alternative to the IEEE754 standard</a></li>
<li><a href="../272253/index.html">Let's Encrypt go public beta: HTTPS everywhere, to everyone, from now on and forever free</a></li>
<li><a href="../272255/index.html">The news called for the road: a super-fast, energy-efficient optical coprocessor for big data</a></li>
<li><a href="../272257/index.html">Access tables from C extensions for Postgres</a></li>
<li><a href="../272259/index.html">Law enforcement authorities dismantled the Dorkbot botnet</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>