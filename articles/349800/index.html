<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Depth training with reinforcements does not work yet</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="About the author . Alex Irpan is a developer from the Brain Robotics group at Google, before that he worked in the laboratory of Berkeley Artificial I...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Depth training with reinforcements does not work yet</h1><div class="post__text post__text-html js-mediator-article">  <a href="https://www.alexirpan.com/about/">About the author</a> .  Alex Irpan is a developer from the Brain Robotics group at Google, before that he worked in the laboratory of Berkeley Artificial Intelligence Research (BAIR). <br><br>  <i>Here are mainly cited articles from Berkeley, Google Brain, DeepMind and OpenAI over the past few years, because their work is most noticeable from my point of view.</i>  <i>Almost certainly I missed something from older literature and from other organizations, so I apologize - I am just one person in the end.</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ee9/625/517/ee96255172175888c7c273e689b71b0b.jpg"></div><br><h1>  Introduction </h1><br>  Once on Facebook, I stated the following. <br><blockquote>  <i><font color="gray">When someone asks if learning with reinforcement (RL) can solve their problem, I immediately reply that they cannot.</font></i>  <i><font color="gray">I think that this is true at least in 70% of cases.</font></i> </blockquote>  Depth training with reinforcement is accompanied by a lot of hype.  And for good reasons!  Reinforced learning (RL) is an incredibly common paradigm.  In principle, a reliable and high-performance RL system should be perfect in everything.  The merging of this paradigm with the empirical power of deep learning is evident in itself.  Deep RL is what most looks like a strong AI, and it's a kind of dream that feeds billions of dollars in funding. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Unfortunately, in reality, this thing does not work yet. <br><br>  But I believe that she will shoot.  If I didn‚Äôt believe, I wouldn‚Äôt cook in this thread.  But there are a lot of problems ahead, many of which are fundamentally complex.  Beautiful demo trained agents hide all the blood, sweat and tears that spilled in the process of their creation. <br><a name="habracut"></a><br>  Several times I have seen people seduced by the latest results.  They first tried in-depth RL and always underestimated the difficulties.  Without a doubt, this ‚Äúmodel problem‚Äù is not as simple as it seems.  And without a doubt, this area broke them several times before they learned to set realistic expectations in their research. <br><br>  There is no personal error.  Here is a system problem.  Easy to draw a story around a positive result.  Try to do it with a negative.  The problem is that researchers often get exactly a negative result.  In a sense, such results are even more important than positive ones. <br><br>  In this article I will explain why deep RL does not work.  I will give examples of when it still works and how to achieve more reliable work in the future, in my opinion.  I do this not to make people stop working on deep-seated RL, but because it is easier to make progress if everyone understands the existence of problems.  It is easier to reach agreement, if you really talk about problems, and not over and over again stumble on the same rake separately from each other. <br><br>  I would like to see more research on deep RL.  To come here new people.  And so they know what they are getting involved in. <br><br>  Before continuing, let me make a few comments. <br><br><ul><li>  Here are cited several scientific articles.  I usually cite convincing negative examples and keep silent about positive ones.  <b>This does not mean that I do not like scientific work</b> .  They are all good - worth reading if you have time. </li><li>  I use the terms ‚Äúreinforcement learning‚Äù and ‚Äúin-depth reinforcement learning‚Äù as synonyms, because in my daily work, RL always implies deep-seated RL.  <b>Empirical behavior of depth learning with reinforcement, rather than training with reinforcement as a whole, is criticized here</b> .  In the cited articles, the work of an agent with a deep neural network is usually described.  Although empirically criticism <i>may</i> also apply to linear RL (linear RL) or tabular RL (tabular RL), I am not sure that this criticism can be extended to smaller tasks.  The buzz around deep RL is due to the fact that RL is presented as a solution for large, complex, multidimensional environments where a good approximation function is needed.  It is with this hype, in particular, we need to understand. </li><li>  The article is structured to move from pessimism to optimism.  I know that it is a bit long, but I will be very grateful if you take the time to read it in its entirety before answering. </li></ul><br>  Without further ado, here are some of the cases where the deep-seated RL fails. <br><br><h1>  Depth reinforcement training can be terribly ineffective </h1><br>  The most famous benchmark for in-depth training with reinforcements is Atari games.  As shown in the now famous Deep Q-Networks (DQN) article, if you combine Q-Learning with reasonable-sized neural networks and some optimization tricks, you can achieve or exceed human performance in several Atari games. <br><br>  Atari games play at 60 frames per second.  Can you just figure out how many frames you need to process the best DQN to show the result as a person? <br><br>  The answer depends on the game, so take a look at a recent article by Deepmind - <a href="https://arxiv.org/abs/1710.02298">Rainbow DQN (Hessel et al, 2017)</a> .  It shows how some consistent improvements to the original DQN architecture improve the result, and a combination of all the improvements as efficiently as possible.  The neural network is superior to human performance in over 40 of the 57 Atari games.  The results are shown in this convenient graph. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/917/09e/b68/91709eb6826b82f803fdf3c634321592.png"><br><br>  On the vertical axis, the ‚Äúmedian median result normalized by the human‚Äù is plotted.  it is calculated by training 57 DQN neural networks, one for each Atari game, with normalizing the result of each agent when the human result is taken as 100%, and then calculating the average median result for 57 games.  RainbowDQN exceeds 100% after processing 18 <i>million</i> frames.  This corresponds to about 83 hours of play, plus training time, no matter how long it takes.  This is a lot of time for simple Atari games that most people grasp in a couple of minutes. <br><br>  Consider that 18 million frames is actually a very good result, because the previous record belonged to the <a href="https://arxiv.org/pdf/1707.06887.pdf">Distributional DQN</a> system <a href="https://arxiv.org/pdf/1707.06887.pdf">(Bellemare et al, 2017)</a> , which required 70 million frames to achieve a result of 100%, that is, about four times longer.  As for <a href="https://www.nature.com/articles/nature14236">Nature DQN (Mnih et al, 2015)</a> , it never reaches 100% of the median result at all, even after 200 million frames. <br><br>  Cognitive distortion ‚Äúplanning error‚Äù says that completing a task usually takes longer than you thought.  In reinforcement training there is a planning error of its own - training usually requires more samples than you thought. <br><br>  The problem is not limited to Atari games.  The second most popular test is MuJoCo benchmarks, a set of tasks in the MuJoCo physics engine.  In these tasks, the position and speed of each hinge in the simulation of a certain robot are usually given at the entrance.  Although there is no need to solve the problem of vision, RL systems are required for learning from <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1"><span class="MJXp-msubsup" id="MJXp-Span-2"><span class="MJXp-mn" id="MJXp-Span-3" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">10 </font></font></span><span class="MJXp-mn MJXp-script" id="MJXp-Span-4" style="vertical-align: 0.5em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">5</font></font></span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.379ex" height="2.419ex" viewBox="0 -935.7 1454.9 1041.5" role="img" focusable="false" style="vertical-align: -0.246ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/349800/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgVpqt6QfWA1L1UeIL0BglCSw7-XA#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/349800/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgVpqt6QfWA1L1UeIL0BglCSw7-XA#MJMAIN-30" x="500" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/349800/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgVpqt6QfWA1L1UeIL0BglCSw7-XA#MJMAIN-35" x="1415" y="557"></use></g></svg></span><script type="math/tex" id="MathJax-Element-1"> 10 ^ 5 </script>  before <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-5"><span class="MJXp-msubsup" id="MJXp-Span-6"><span class="MJXp-mn" id="MJXp-Span-7" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">10 </font></font></span><span class="MJXp-mn MJXp-script" id="MJXp-Span-8" style="vertical-align: 0.5em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7</font></font></span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.379ex" height="2.419ex" viewBox="0 -935.7 1454.9 1041.5" role="img" focusable="false" style="vertical-align: -0.246ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/349800/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgVpqt6QfWA1L1UeIL0BglCSw7-XA#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/349800/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgVpqt6QfWA1L1UeIL0BglCSw7-XA#MJMAIN-30" x="500" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/349800/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgVpqt6QfWA1L1UeIL0BglCSw7-XA#MJMAIN-37" x="1415" y="557"></use></g></svg></span><script type="math/tex" id="MathJax-Element-2"> 10 ^ 7 </script>  steps, depending on the task.  This is an incredible amount to control in such a simple environment. <br><br>  <a href="https://arxiv.org/abs/1707.02286">The DeepMind Parkour article (Heess et al, 2017)</a> , illustrated below, has been trained using 64 workers for more than 100 hours.  The article does not specify what a "worker" is, but I guess it means a single processor. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/hx_bgoTF7bs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  This is a <i>super result</i> .  When he first came out, I was surprised that the deep-seated RL was able to learn such gaits on the run. <br><br>  But it took 6400 hours of CPU time, which is a bit disappointing.  Not that I was counting on less time ... it's just sad that in simple skills, deep RL is still an order of magnitude less than the level of training that could be useful in practice. <br><br>  There is an obvious counter-argument here: what if you just ignore the effectiveness of training?  There are certain environments that make it easy to generate experience.  For example, games.  But for any environment where this is <i>impossible</i> , RL faces enormous challenges.  Unfortunately, most environments fall into this category. <br><br><h1>  If you only care about the final performance, then many problems are better solved by other methods. </h1><br>  When looking for solutions to any problem, one usually has to find a compromise in achieving different goals.  You can focus on a really good solution to this particular problem, or you can focus on maximizing your contribution to the overall research.  The best problems are those where a good contribution to research is required to get a good solution.  But in reality it is difficult to find problems that meet these criteria. <br><br>  Purely in demonstrating maximum efficiency, the in-depth RL shows not very impressive results, because it is constantly superior to other methods.  Here is a video with MuJoCo robots, which are controlled by interactive trajectory optimization.  Correct actions are calculated almost in real time, online, without offline learning.  Yes, and everything works on the equipment of 2012 ( <a href="https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf">Tassa et al, IROS 2012</a> ). <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/uRVAX_sFT24" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  I think this job can be compared with the article DeepMind on parkour.  What is the difference? <br><br>  The difference is that here the authors apply control with predictive models, working with a real model of the earthly world (physics engine).  There are no such models in RL, which makes work very difficult.  On the other hand, if planning an action based on a model improves the result so much, then why suffer with tricky training on RL rules? <br><br>  Similarly, you can easily surpass the DQN neural network in Atari with a ready-made Monte Carlo tree search (MCTS) solution.  Here are the main indicators from the work of <a href="https://papers.nips.cc/paper/5421-deep-learning-for-real-time-atari-game-play-using-offline-monte-carlo-tree-search-planning">Guo et al, NIPS 2014</a> .  The authors compare the results of the trained DQN with the results of the UCT agent (this is the standard version of the modern MCTS). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a0f/d12/cd9/a0fd12cd99514b6f090bd71aec4c5e0a.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/b59/52a/d8d/b5952ad8d6d8eca97669f4d6c6c313a1.png"><br><br>  Again, this is an unfair comparison, because DQN does not search, and the MCTS does exactly the search on the real model of terrestrial physics (Atari emulator).  But in some situations you do not care, here is an honest or unfair comparison.  Sometimes you just need everything to work (if you need a full UCT assessment, see the appendix of the original research article <a href="http://www.marcgbellemare.info/static/publications/bellemare13arcade.pdf">Arcade Learning Environment (Bellemare et al, JAIR 2013)</a> ). <br><br>  Reinforced learning is theoretically appropriate for everything, including environments with an unknown model of the world.  However, such versatility is costly: it is difficult to use some specific information that could help in learning.  Because of this, we have to use a lot of samples in order to learn things that could be simply hard-coded initially. <br><br>  Experience shows that, with the exception of rare cases, algorithms sharpened for specific tasks work faster and better than reinforcement learning.  This is not a problem if you develop deep RL for the deepest RL, but personally I‚Äôm upset about comparing the effectiveness of RL c ... well, with anything else.  One of the reasons why I liked AlphaGo so much is because it was an unequivocal victory for deep-seated RL, and this does not happen very often. <br><br>  Because of all this, it is more difficult for people to explain why my tasks are so cool, complex and interesting, because they often have no context or experience to evaluate, <i>why</i> are they so difficult.  There is a certain difference between what people think about the possibilities of deep-seated RL - and its real possibilities.  Now I work in the field of robotics.  Consider a company that comes to mind for most people if you mention robotics: Boston Dynamics. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/fRj34o4hN4I" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  This thing doesn't use reinforcement training.  I met people several times who thought that RL was used here, but no.  If you look for published scientific papers from a group of developers, you will find articles with references <a href="https://dspace.mit.edu/openaccess-disseminate/1721.1/110533">to time-varying linear-quadratic regulators, quadratic programming solvers, and convex optimization</a> .  In other words, they use mostly classical methods of robotics.  It turns out that these classic techniques work fine, if properly applied. <br><br><h1>  Reinforcement training usually requires a reward function. </h1><br>  Reinforcement training assumes the existence of a reward function.  Usually, it is either originally, or manually configured offline and remains unchanged during training.  I say ‚Äúusually‚Äù because there are exceptions, such as simulation training or the reverse RL (when the reward function is restored after the fact), but in most RL options they use reward as an oracle. <br><br>  It is important to note that in order for the RL to work properly, the reward function must cover <i>exactly</i> what we need.  And I mean <i>exactly</i> .  RL is annoyingly prone to overfit, which leads to unexpected consequences.  That is why Atari is such a good benchmark.  There is not only easy to get a lot of samples, but every game has a clear goal - the number of points, so you never have to worry about finding the reward function.  And you know that everyone else has the same function. <br><br>  The popularity of the tasks MuJoCo due to the same reasons.  Since they work in the simulation, you have complete information about the state of the object, which greatly simplifies the creation of the reward function. <br><br>  In the Reacher task, you control a two-segment hand that is connected to a central point, and the goal is to move the end of the hand to a given target.  See below for an example of successful learning. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/BkhSKqc8vSA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Since all coordinates are known, the reward can be defined as the distance from the end of the arm to the target, plus a short time to move.  In principle, in the real world, you can conduct the same experience if you have enough sensors to accurately measure the coordinates.  But depending on what the system should do, it can be difficult to determine a reasonable reward. <br><br>  The reward function itself would not be a big problem if not ... <br><br><h1>  The difficulty of developing a reward function </h1><br>  Making the reward function is not that difficult.  Difficulties arise when you try to create a function that will encourage proper behavior, and at the same time, the system will retain learnability. <br><br>  In HalfCheetah, we have a two-legged robot bounded by a vertical plane, which means, that is, it can only move forward or backward. <br><br> <a href=""><img src="https://habrastorage.org/webt/h9/dc/hf/h9dchfesck8yxwcnxibb5n0pdkq.png"></a> <br><br>  The goal is to learn to trot.  Reward - HalfCheetah speed ( <a href="">video</a> ). <br><br>  This is a <i>smooth</i> or shaped (shaped) reward, that is, it increases with the approach to the final goal.  In contrast to the <i>sparse</i> (sparse) reward, which is given only upon reaching the final state of the goal, and is absent in other states.  Smooth growth of remuneration is often much easier to learn, because it provides positive feedback, even if the training did not provide a complete solution to the problem. <br><br>  Unfortunately, a smooth growth reward can be biassed.  As already mentioned, this causes unexpected and undesirable behavior.  A good example is the boating race from the <a href="https://blog.openai.com/faulty-reward-functions/">OpenAI blog article</a> .  The intended goal is to reach the finish line.  You can submit a reward as +1 for ending the race at a given time, and a reward of 0 otherwise. <br><br>  The reward function gives points for crossing checkpoints and collecting bonuses that allow you to get to the finish line faster.  As it turned out, the collection of bonuses gives more points than the end of the race. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/tlOIHko8ySg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  To be honest, at first this publication annoyed me a little.  Not because it is wrong!  And because it seemed to me that she demonstrates the obvious things.  Of course, reinforcement training will give a strange result when the reward is incorrectly determined!  It seemed to me that the publication attaches unreasonably great importance to this particular case. <br><br>  But then I started writing this article and I realized that the most convincing example of a wrongly defined reward is the very same boat racing video.  Since then, it has been used in several presentations on this topic, which has drawn attention to the problem.  So okay, I‚Äôm reluctant to admit that it was a good blog post. <br><br>  Algorithms RL fall into a black hole, if they have more or less to guess about the world around them.  The most universal category of modelless RL is almost like a black box optimization.  Such systems are only allowed to assume that they are in the MDP (Markov decision-making process) - and nothing more.  The agent is simply told that you get +1 for this, but you don‚Äôt get for this, but you have to learn everything else yourself.  As with the black box optimization, the problem is that any behavior that gives +1 is considered good, even if the reward is received in the wrong way. <br><br>  The classic example is not from the realm of RL - when someone applied genetic algorithms for chip design and got a circuit in which <a href="https://en.wikipedia.org/wiki/Evolvable_hardware">one unconnected logic gate was needed for the final design</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cb1/492/23a/cb149223a5dec3a7d3c9051d63a0334e.png"><br>  <i><font color="gray">Gray elements are necessary for the correct operation of the circuit, including the element in the upper left corner, although it is not connected to anything.</font></i>  <i><font color="gray">From the article <a href="http://citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.50.9691%26rep%3Drep1%26type%3Dpdf">‚ÄúAn Evolved Circuit, Intrinsic in Silicon, Entwined with Physics‚Äù</a></font></i> <br><br>  Or a more recent example - here is the <a href="https://www.salesforce.com/products/einstein/ai-research/tl-dr-reinforced-model-abstractive-summarization/">publication in the 2017 Salesforce blog</a> .  Their goal was to write a summary for the text.  The basic model was trained with the teacher, then it was evaluated by an automated metric called ROUGE.  ROUGE is a non-differentiable reward, but RL can work with such.  So they tried to apply RL to optimize ROUGE directly.  This gives a high ROUGE (hooray!), But not really very good lyrics.  Here is an example. <br><br><blockquote>  <i>Button was deprived of his 100th race for McLaren after the ERS did not let him start.</i>  <i>So ended a bad weekend for the British.</i>  <i>Button ahead in qualifying.</i>  <i>Finished ahead of Nico Rosberg in Bahrain.</i>  <i>Lewis Hamilton</i>  <i>In 11 races .. Race.</i>  <i>To lead 2000 laps .. V ... I. - <a href="https://arxiv.org/abs/1705.04304">Paulus et al, 2017</a></i> </blockquote><br>  And although the RL model showed the maximum ROUGE result ... <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d78/d17/d85/d78d17d851b470c97d9a37e98b12accd.png"><br><br>  ... they finally decided to use a different model for the resume. <br><br>  Another funny example.  This is from an article by <a href="https://arxiv.org/abs/1704.03073">Popov et al, 2017</a> , also known as the ‚Äúarticle on folding Lego designer.‚Äù  The authors use the distributed version of DDPG to learn how to capture.  The goal is to grab the red cube and put it on the blue one. <br><br>  They made her work, but faced an interesting case of failure.  The initial lifting movement is rewarded based on the height of the red block lifting.  This is determined by the z-coordinate of the bottom face of the cube.  In one of the options for failure, the model learned to turn the red cube upside down, rather than lift it. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/8QnD8ZM0YCo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  It is clear that this behavior was not intended.  But RL doesn't care.  From the point of view of reinforcement training, she received a reward for turning the cube - therefore she will continue to turn the cube. <br><br>  One way to solve this problem is to make the reward sparse, giving it only after connecting the cubes.  Sometimes it works because the rare reward is trainable.  But often this is not the case, since the absence of positive reinforcement complicates things too much. <br><br>  Another solution to the problem is the careful formation of remuneration, the addition of new remuneration conditions and the adjustment of coefficients for existing conditions until the RL algorithm begins to demonstrate the desired behavior during training.  Yes, <i>it is possible</i> to overcome the RL on this front, but such a struggle does not bring satisfaction.  Sometimes it is necessary, but I never felt that I had learned something in the process. <br><br>  For reference, here is one of the functions of remuneration from the article on folding designer Lego. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cea/4f1/c40/cea4f1c405600c81096b8c953d37de50.png"><br><br>  I do not know how much time the guys spent on the development of this function, but by the number of members and different coefficients, I would say ‚Äúa lot‚Äù. <br><br>  In conversations with other RL researchers, I heard several stories about the original behavior of models with improperly established rewards. <br><br><ul><li>  A colleague teaches the agent to navigate the room.  The episode ends if the agent is out of bounds, but in this case no penalty is imposed.  Upon completion of the training, the agent learned the behavior of the suicide, because it was very easy to get a negative reward, and a positive one was too difficult, so a quick death with a result of 0 was preferable to a long life with a high risk of a negative result. </li><li>  A friend taught the robot arm to move in the direction of a certain point above the table.  It turns out that the point was determined <i>relative to the table</i> , and the table was not tied to anything.  The model learned to knock very hard on the table, causing it to fall, which moved the target point - and that turned out to be next to the hand. </li><li>  The researcher talked about the use of RL for teaching a robotic arm simulator to take a hammer and hammer a nail.  Initially, the reward was determined by how far the nail entered the hole.  Instead of taking a hammer, the robot hammered a nail into his own limbs.  Then they added a reward to encourage the robot to take the hammer.  As a result, the learned strategy for the robot began to take the hammer ... and throw the tool into the nail, and not use it in the normal way. </li></ul><br>  True, all these are stories from the lips of others; I personally have not seen videos with such behavior.  But none of these stories seems to me impossible.  I burned on the RL too many times to believe the opposite. <br><br>  I know people who love telling stories about <a href="https://en.wikipedia.org/wiki/Instrumental_convergence">clip optimizers</a> .  Okay, I understand, honestly.  But in truth, I am sick of listening to these stories, because they always talk about some kind of superhuman disoriented strong AI as a real story.  Why invent, if there are so many real stories that happen every day. <br><br><h1>  Even with a good reward, it is difficult to avoid a local optimum. </h1><br>  The previous examples of RL are often called "reward hacks."<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As for me, this is a smart, non-standard solution that brings more reward than the intended solution from the designer of the problem. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Khaki rewards are exceptions. </font><font style="vertical-align: inherit;">Cases of incorrect local optima are much more common, as they arise from an incorrect compromise between exploration and exploitation (exploration ‚Äì exploitation). </font></font><br><br> <a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Here is one of my favorite videos</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Here the </font></font><a href="https://arxiv.org/abs/1603.00748"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">normalized benefit function is</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> implemented </font><font style="vertical-align: inherit;">, which is learned in the HalfCheetah environment. </font><font style="vertical-align: inherit;">From the perspective of an outside observer, this is very, </font><i><font style="vertical-align: inherit;">very</font></i></font><br><br> <a href=""><img src="https://habrastorage.org/webt/h9/dc/hf/h9dchfesck8yxwcnxibb5n0pdkq.png"></a> <br><br><font style="vertical-align: inherit;"></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">stupid </font><font style="vertical-align: inherit;">But we call it stupid just because we look from the side and have a lot of knowledge that moving on legs is better than lying on your back. </font><font style="vertical-align: inherit;">RL does not know this! </font><font style="vertical-align: inherit;">He sees a state vector, sends action vectors, and sees that he receives some positive reward.</font></font> That's all. <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Here is the most plausible explanation that I can think of about what happened during the training. </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> In the process of random research, the model found that falling forward is more beneficial than staying motionless. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The model often did this to ‚Äúflash‚Äù this behavior and begin to fall continuously. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> After falling forward, the model found out that if you apply enough effort, you can do a back flip, which gives a little more reward. </font></font></li><li>      ‚Äî  ,    ,      ¬´¬ª   . </li><li>     ,     ‚Äî       ¬´ ¬ª    ,   ?      . </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This is very funny, but obviously not what we want from the robot. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Here is another unfortunate example, this time surrounded by Reacher ( </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">video</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) </font><font style="vertical-align: inherit;">In this run, random initial weights, as a rule, gave strongly positive or strongly negative values ‚Äã‚Äãfor actions. Because of this, most actions were performed with the maximum or minimum acceleration. In fact, it is very easy to unwind the model: just give a high amount of force to each hinge. When the robot has unwound, it is already difficult to get out of this state in some understandable way: in order to stop the uncontrolled rotation, several steps of reconnaissance should be taken. Of course, this is possible. But this did not happen in this run.</font></font><br><br> <a href=""><img src="https://habrastorage.org/webt/fd/db/mx/fddbmxo99tsxafisrm_sugkemzs.png"></a> <br><br><font style="vertical-align: inherit;"></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In both cases, we see the classic problem of exploration ‚Äì exploitation, which since time immemorial has pursued reinforcement training. Your data flow from current rules. If the current rules provide for extensive intelligence, you will receive unnecessary data and learn nothing. You exploit too much - and "flush" non-optimal behavior.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There are several intuitively pleasant ideas on this subject - internal motives and curiosity, intelligence on the basis of calculation, etc. Many of these approaches were first proposed in the 1980s or earlier, and some were later revised for deep learning models. But as far as I know, no approach works stably in all environments. Sometimes it helps, sometimes it doesn't. It would be nice if some intelligence trick worked everywhere, but I doubt that in the foreseeable future they will find a silver bullet of this caliber. Not because no one is trying, but because exploration-exploitation is very, very, very, very difficult. Quote from the </font></font><a href="https://en.wikipedia.org/wiki/Multi-armed_bandit"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">article about the multi-armed gangster from Wikipedia</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br><br><blockquote> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">For the first time in history, this problem was studied by scientists from the Allied countries of the Second World War. </font><font style="vertical-align: inherit;">It turned out to be so intractable that, according to Peter Whittle, it was offered to throw it to the Germans, so that the German scientists would also spend their time on it.</font></font></i> </blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(Source: </font></font><a href="http://citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.57.1916%26rep%3Drep1%26type%3Dpdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Q-Learning for Bandit Problems, Duff 1995</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I see the in-depth RL as a demon who specifically misunderstands your reward and is actively looking for the laziest way to achieve a local optimum. </font><font style="vertical-align: inherit;">A little funny, but it turned out to be a really productive mindset.</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Even if in-depth RL works, he can retrain to strange behavior. </font></font></h1><br><blockquote> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In-depth training is popular because it is the only area of ‚Äã‚Äãmachine learning where it is socially acceptable to study on a test suite.</font></font></i> </blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">( </font></font><a href="https://twitter.com/jacobandreas/status/924356906344267776"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Source</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) The </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">positive side of learning with reinforcement is that if you want to achieve a good result in a specific environment, you can retrain like crazy. The disadvantage is that if you need to expand the model to any other environment, it will probably work poorly because of the crazy re-learning. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DQN networks cope with many Atari games, because all the training of each model is focused on the only goal - to achieve maximum results in a single game. The final model cannot be expanded to other games, because it was not taught so. You can set up a trained DQN for the new Atari game (see </font></font><a href="https://arxiv.org/abs/1606.04671"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Progressive Neural Networks (Rusu et al, 2016)</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">), but there is no guarantee that such a transfer will take place, and usually no one expects it. This is not the wild success that people see on the pre-trained ImageNet signs. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To warn some obvious comments: yes, in principle, learning in a wide range of environments can solve some problems. In some cases, such an extension of the model's action occurs by itself. An example is navigation, where you can try random locations of targets and use universal functions for generalization. (see </font></font><a href="http://proceedings.mlr.press/v37/schaul15.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Universal Value Function Approximators, Schaul et al, ICML 2015</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">). I find this work very promising, and later I will give more examples from this work. But I think that the possibilities for generalizing deep RL are not yet so great as to cope with a diverse set of tasks. The perception has become much better, but the deep RL is still ahead when ‚ÄúImageNet for management‚Äù will appear. OpenAI Universe tried to light this fire, but from what I heard, the task was too difficult, so they achieved little. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">While there is no such moment for the generalization of models, we are stuck with surprisingly narrow models in terms of coverage. As an example (and as an excuse to laugh at my own work) take a look at the </font><a href="https://arxiv.org/abs/1711.02301"><font style="vertical-align: inherit;">Can Deep RL Solve Erdos-Selfridge-Spencer Games</font></a><font style="vertical-align: inherit;"> article </font></font><a href="https://arxiv.org/abs/1711.02301"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">? (Raghu et al, 2017)</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. We studied a combinatorial game for two players, where there is a solution in an analytical form for an optimal game. In one of the first experiments, we recorded the behavior of player 1, and then trained player 2 with the help of RL. In this case, you can consider the actions of the player 1 part of the environment. Teaching player 2 against optimal player 1, we showed that RL is able to show high results. But when we applied the same rules against non-optimal player 1, then the effectiveness of player 2 fell because it did not apply to non-optimal opponents. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The authors of the article </font></font><a href="https://arxiv.org/abs/1711.00832"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lanctot et al, NIPS 2017</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Got a similar result. </font><font style="vertical-align: inherit;">Here two agents play laser tag. </font><font style="vertical-align: inherit;">Agents are trained through multi-agent training with reinforcements. </font><font style="vertical-align: inherit;">To test the generalization, the training was started with five random starting points (sid). </font><font style="vertical-align: inherit;">Here is a video of agents who were taught to play against each other.</font></font><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/8vXpdHuoQH8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As you can see, they learned to get close and shoot each other. </font><font style="vertical-align: inherit;">Then the authors took player 1 from one experiment - and brought him to player 2 from another experiment. </font><font style="vertical-align: inherit;">If the learned rules are generalized, then we should see similar behavior. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Spoiler: we will not see him.</font></font><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/jOjwOkCM_i8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This seems to be a common problem with multi-agent RL. When agents are trained against each other, a kind of joint evolution takes place. Agents are trained to really fight well with each other, but when they are sent against a player with whom they have not met before, the effectiveness decreases. I want to note that the only difference between these videos is random sit. The same learning algorithm, the same hyperparameters. The difference in behavior is purely due to the random nature of the initial conditions. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nevertheless, there are some impressive results obtained in an environment with independent play against each other - they seem to contradict the general thesis. The OpenAI blog has a </font></font><a href="https://blog.openai.com/competitive-self-play/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">nice post about some of their work in this area.</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Self-play is also an important part of AlphaGo and AlphaZero. </font><font style="vertical-align: inherit;">My intuitive guess is that if agents learn at the same pace, they can constantly compete and accelerate each other‚Äôs learning, but if one of them learns much faster than the other, then he takes advantage of the weakness of a weak player and retrains. </font><font style="vertical-align: inherit;">As you move from symmetrical self-play to general multi-agent settings, it becomes much more difficult to make sure that learning is going at the same speed.</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Even without generalization, it may be that the final results are unstable and difficult to reproduce. </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Almost every machine learning algorithm has hyper parameters that affect the behavior of the learning system. Often they are selected manually or by random search. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Teaching with the teacher is stable. Fixed dataset, true data check. If you slightly change the hyperparameters, the operation will not change much. Not all hyperparameters work well, but over the years many empirical tricks have been found, so many hyperparameters show signs of life during training. These signs of life are very important: they say that you are on the right track, doing something sensible - and you need to spend more time. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Currently, deep RL is not stable at all, which is very annoying in the research process.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">When I started working in Google Brain, I almost immediately started working on the implementation of the algorithm from the above article, </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Normalized Advantage Function</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (NAF). I thought it would take only two or three weeks. I had several trump cards: a certain acquaintance with Teano (which is well tolerated by TensorFlow), some experience with deep-seated RL, and also the lead author of an article on NAF trained at Brain, so I could pester him with questions. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the end, it took me six weeks to reproduce the results, due to several bugs in the software. The question is, why are these bugs hiding for so long?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To answer this question, consider the simplest continuous control problem in the OpenAI Gym: the Pendulum task. In this task, the pendulum is fixed at a certain point and gravity acts on it. At the entrance of the three-dimensional state. The action space is one-dimensional: this is the moment of force that is attached to the pendulum. The goal is to balance the pendulum in exactly vertical position. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This is a small problem, and it becomes even easier thanks to a well-defined reward. The reward depends on the angle of the pendulum. Actions that bring the pendulum closer to the vertical position, not only give reward, they </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">increase</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> it. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Here is a </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">video of a model</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> that is </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">almost</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">works. Although it does not bring the pendulum into a precisely vertical position, it produces an exact moment of force to compensate for gravity. </font><font style="vertical-align: inherit;">But the performance graph after correcting all errors. Each line is a reward curve from one of ten independent runs. The same hyperparameters, the difference is only in a random starting point. </font><font style="vertical-align: inherit;">Seven out of ten runs worked well. Three did not pass. </font><i><font style="vertical-align: inherit;">A failure rate of 30% is considered working</font></i><font style="vertical-align: inherit;"> . Here is another plot from the published work </font><a href="https://arxiv.org/abs/1605.09674"><font style="vertical-align: inherit;">‚ÄúVariational Information Maximizing Exploration‚Äù (Houthooft et al, NIPS 2016)</font></a><font style="vertical-align: inherit;"> . Wednesday - HalfCheetah. The award was made sparse, although the details are not too important. On the y-axis, episodic reward, on the x-axis, the number of time intervals, and the algorithm used is TRPO.</font></font><br><br> <a href=""><img src="https://habrastorage.org/webt/wf/sn/yu/wfsnyuqnmfbdgufwddmrb72frc0.png"></a> <br><br><font style="vertical-align: inherit;"></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/9c8/4f6/245/9c84f6245bb0ebb035aa881066f511b0.png"><br><br><font style="vertical-align: inherit;"></font><i><font style="vertical-align: inherit;"></font></i><font style="vertical-align: inherit;"></font><a href="https://arxiv.org/abs/1605.09674"><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/e9d/581/277/e9d5812774c2d76e35b741a7263b1eb6.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The dark line is the median performance of ten random sids, and the shaded area is the coverage from the 25th to the 75th percentile. Don't get me wrong, this chart seems to be a good argument for VIME. But on the other hand, the 25th percentile line is really close to zero. This means that about 25% do not work simply because of a random starting point. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">See, there is also a variance in learning with the teacher, but not so bad. If my learning code with the teacher could not cope with 30% of the runs with random sidami, then I would be quite sure that there was some kind of error when loading data or training. If my training code with reinforcements does not do better than the random one, then I have no idea whether this is a bug or bad hyperparameters, or if I‚Äôm just unlucky.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/bee/552/42f/bee55242f3636086cbdb999e784cc82f.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This is an illustration from the article </font></font><a href="http://ai.stanford.edu/~zayd/why-is-machine-learning-hard.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúWhy is it so‚Äú difficult ‚Äùto work with machine learning?‚Äù</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The main thesis is that machine learning adds extra dimensions to the failure space, which exponentially increases the number of failure options. Deep RL adds another dimension: randomness. And the only way to solve the problem of chance is to clear out the noise. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If your learning algorithm has an inefficient sample and is unstable, it greatly reduces the productivity of your research.</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Maybe he will need only 1 million steps. But when you multiply this by five random input values, and then multiply by the scatter of hyper parameters, you will see an exponential increase in the calculations needed to effectively test the hypothesis.</font></font><br><br><blockquote> <i>If this becomes easier for you, then I have been doing this for some time - and spent about the last 6 weeks to remove from zero the gradients of the model, which work 50% of cases on a bunch of RL tasks.</i>  <i>And I have a cluster of GPUs and a few friends with whom I have lunch every day - and they have been working in this field for the past few years.</i> <i><br><br></i>  <i>In addition, information from the field of training with the teacher about the correct design of convolutional neural networks does not seem to extend to the area of ‚Äã‚Äãstudy with reinforcement, because here the main limitations are credit distribution / bitrate control, and not the lack of effective performance.</i>  <i>Your ResNet, batchnorm and very deep networks do not work here.</i> <i><br><br></i>  <i>[Teaching with the teacher] wants to work.</i>  <i>Even if you mess up something, you usually get some non-random result.</i>  <i>RL needs to be made to work.</i>  <i>If you mess up something or don't set up something well enough, you will almost certainly get rules that are worse than random ones.</i>  <i>And even if everything is fine tuned, bad results will be in 30% of cases.</i>  <i>Why?</i>  <i>Just because.</i> <i><br><br></i>  <i>In short, your problems are more likely due to the complexity of the work of deep-seated RL, than the complexity of ‚Äúdesigning neural networks‚Äù.</i>  - <a href="https://news.ycombinator.com/item%3Fid%3D13519044">Comment on Hacker News by Andrei Karpati (Andrej Karpathy) when he worked at OpenAI</a> </blockquote><br>  The instability of random seeds is like a canary in a coal mine.  If simple randomness leads to such a strong gap between runs, then imagine what difference will arise from a real change in the code. <br><br>  Fortunately, we do not need to conduct this thought experiment, because it has already been carried out and described in the article <a href="https://arxiv.org/abs/1709.06560">‚ÄúDeep Reinforcement Learning That Matters‚Äù (Henderson et al, AAAI 2018)</a> .  Here are some conclusions: <br><br><ul><li>  Multiplying remuneration by a constant can lead to a significant performance difference. </li><li>  Five random seeders (a common metric in the reports) may not be enough to show meaningful results, since careful selection may result in non-overlapping confidence intervals. </li><li>  Different implementations of the same algorithm have different performance for the same task, even with the same hyperparameters. </li></ul><br>  My theory is that RL is very sensitive both to initialization and to the dynamics of the learning process, because data is always collected on the Internet and the only controllable parameter is only the amount of remuneration.  Models that randomly encounter good learning examples work much better.  If the model does not see good examples, then it may not learn anything at all, as it becomes more and more convinced that any deviations are unsuccessful. <br><br><h1>  But what about all the great achievements of deep RL? </h1><br>  Of course, in-depth training with reinforcements has achieved some excellent results.  DQN is no longer a novelty, but at one time it was an absolutely <i>crazy</i> discovery.  The same model learns directly from the pixels, without adjusting for each game individually.  Both AlphaGo and AlphaZero also remain very impressive achievements. <br><br>  But apart from these successes, it is difficult to find cases where the deep RL has practical value for the real world. <br><br>  I tried to think about how to use deep RL in the real world for practical tasks - and this is surprisingly difficult.  I thought that I would find some use in recommendation systems, but in my opinion, <a href="https://en.wikipedia.org/wiki/Collaborative_filtering">collaborative filtering</a> and <a href="https://research.yahoo.com/publications/5863/contextual-bandit-approach-personalized-news-article-recommendation">contextual gangsters</a> still dominate there. <br><br>  The best thing that I finally found was two Google projects: a <a href="https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/">reduction in energy consumption in data centers</a> and the recently announced <a href="https://cloud.google.com/automl/">AutoML Vision</a> project.  Jack Clark from OpenAI <a href="https://twitter.com/jackclarkSF/status/919584404472602624">addressed the readers on Twitter with a similar question - and came to the same conclusion</a> .  (Last year's tweet, before the AutoML announcement). <br><br>  I know that Audi is doing something interesting with the in-depth RL, because at NIPS they showed a small model of an unmanned racing car and said that the deep-down system RL was developed for it.  I know that skillful work is being done with <a href="https://arxiv.org/abs/1706.04972">optimizing the placement of devices for large tensor graphs (Mirhoseini et al, ICML 2017)</a> .  Salesforce has a text summarization model that works when RL is used carefully enough.  Financial companies are probably experimenting with RL while you are reading this article, but so far there is no evidence of this.  (Of course, financial companies have reasons to hide their methods of playing in the market, so we may never get solid evidence).  Facebook works great with in-depth RL for chatbots and speech.  Every Internet company in history has probably ever thought about introducing RL into its advertising model, but if someone actually implemented RL, then it is silent about it. <br><br>  So, in my opinion, either in-depth RL is still a topic of academic research, not a reliable technology for widespread use, or it really can be made to work effectively - and the people who succeeded in it did not disclose the information.  I think the first option is more likely. <br><br>  If you came to me with the problem of image classification, I would advise the pre-trained ImageNet models - they will probably do an excellent job.  We live in a world where <a href="https://habrahabr.ru/post/331740/">filmmakers from the Silicon Valley TV series are jokingly making a real AI application for recognizing hot dogs</a> .  I can not say the same successes deep RL. <br><br><h1>  Given these limitations, when to apply deep RL? </h1><br>  This is a priori tough question.  The problem is that you are trying to use the same RL approach for different environments.  It is quite natural that it will not always work. <br><br>  In view of the above, it is possible to draw some conclusions from the existing achievements of reinforced learning.  These projects, where in-depth RL is either learning some kind of qualitatively impressive behavior, or learning better than previous systems in this area (although these are very subjective criteria). <br><br>  Here is my list for now. <br><br><ul><li>  The projects mentioned in the previous sections: DQN, AlphaGo, AlphaZero, parkour-bot, reduced power consumption in data centers and AutoML with Neural Architecture Search. </li><li>  <a href="https://blog.openai.com/dota-2/">Bot OpenAI Dota 2 1v1 Shadow Fiend, who beat the best professionals with simplified settings of the match</a> . </li><li>  <a href="https://arxiv.org/abs/1702.06230">Bot Super Smash Brothers Melee</a> , able to beat professional players in <a href="https://www.youtube.com/watch%3Fv%3DdXJUlqBsZtE">1v1 Falcon in the same way</a> .  (Firoiu et al, 2017). </li></ul><br>  (A brief digression: machine learning recently beat professional players in no limit Texas Hold'em. The program uses both <a href="https://www.ijcai.org/proceedings/2017/0772.pdf">Libratus (Brown et al, IJCAI 2017)</a> and <a href="https://arxiv.org/abs/1701.01724">DeepStack (Moravƒç√≠k et al, 2017) systems</a> . I spoke with several people who thought they were working here. deep RL. Both systems are very cool, but they do not use in-depth training with reinforcements. They use an algorithm of counterfactual minimization of regret and a competent iterative solution of subgame). <br><br>  From this list, you can isolate common properties that facilitate learning.  None of these properties listed below are <i>required</i> for training, but the more they are present - the better the result will be, definitely. <br><br><ul><li>  <b>Easy generation of virtually unlimited experience</b> .  Here the benefits are obvious.  The more data you have, the easier the training.  This applies to Atari, Go, Chess, Shogi, and Parkour Bot.  Probably, this also applies to the data center power supply project, because <a href="https://googleblog.blogspot.com/2014/05/better-data-centers-through-machine.html">in previous work (Gao, 2014)</a> it was shown that neural networks are able to predict energy efficiency with high accuracy.  This is exactly the simulation model you would use for learning the RL system. <br><br>  Perhaps the principle applies to the work on Dota 2 and SSBM, but this depends on the maximum game speed and the number of processors. </li><li>  <b>The task is simplified to a simpler form</b> .  One of the most common mistakes in the deep RL - too ambitious plans and dreams.  Training with reinforcements is capable of anything!  But this does not mean that you need to take on everything at once. <br><br>  The OpenAI Dota 2 bot only works at the beginning of the game, only Shadow Fiend vs. Shadow Fiend, only 1x1, with certain settings, with a fixed position and type of buildings, and also probably refers to the <a href="https://developer.valvesoftware.com/wiki/Dota_2_Workshop_Tools/Scripting/API">Dota 2 API</a> in order not to solve graphics processing tasks.  The SSBM bot shows superhuman performance, but only in the 1x1 game, only with Captain Falcon, only on Battlefield with endless match time. <br><br>  This is not a mockery of any of the bots.  Indeed, why solve a difficult problem if you don‚Äôt even know if simple is solved?  The general approach in this area is to first obtain a minimal proof of the concept, and later to generalize it.  OpenAI is expanding its work on Dota 2. Work continues on expanding the SSBM bot <a href="https://github.com/vladfi1/phillip">to other characters</a> . </li><li>  <b>There is a way to learn in an independent game</b> .  This is the way AlphaGo, AlphaZero, Dota 2 Shadow Fiend and SSBM Falcon bots work.  I should note that by independent game I mean competitive game, although both players can be controlled by one agent.  Apparently, this setting gives the most stable result. </li><li>  <b>There is a clear way to determine the right reward for learning</b> .  In two-player games, this is +1 for a win and ‚àí1 for a loss.  The <a href="https://openreview.net/forum%3Fid%3Dr1Ue8Hcxg">original Neural Architecture Search article from Zoph et al, ICLR 2017</a> was validation accuracy of the trained model.  Every time you set a smooth reward, you enter a chance to learn a non-optimal policy that optimizes the model for the wrong purpose. <br><br>  If you want to read more about how to make the right reward, try searching for the phrase [ <a href="https://en.wikipedia.org/wiki/Scoring_rule">correct scoring rule</a> ].  <a href="https://terrytao.wordpress.com/2016/06/01/how-to-assign-partial-credit-on-an-exam-of-true-false-questions/">This blog post by Terrence Tao</a> provides an accessible example. <br><br>  As for learning, I have no advice, except to try and see what works and what does not. </li><li>  <b>If you need to define a continuous reward, then at least it should be rich</b> .  In Dota 2, a reward can be awarded for the last hits (for each monster killed by any player) and health (triggered after each precise attack or use of a skill).  These signals come quickly and often.  The SSBM bot can be rewarded for the damage done and received, with a signal after each successful attack.  The shorter the delay between action and effect, the faster the feedback loop closes - and the easier it is for the reinforcement system to find the path to maximum reward. </li></ul><br><br><h1>  Example: Neural Architecture Search </h1><br>  You can combine several principles and analyze the success of Neural Architecture Search.  According to the initial <a href="https://arxiv.org/abs/1611.01578">version of ICLR 2017</a> , after 12800 samples, the deep RL is able to design the best of its kind neural networks architecture.  Admittedly, in each example it was necessary to train the neural network to convergence, but it is still very effective in terms of the number of samples. <br><br>  As mentioned above, the reward is validation accuracy.  This is a very rich reward signal - if a change in the structure of a neural network increases the accuracy from just 70% to 71%, RL will still benefit from it.  In addition, there is evidence that hyperparameters in depth learning are close to linearly independent.  (This was empirically shown in <a href="https://arxiv.org/abs/1706.00764">Hyperparameter Optimization: A Spectral Approach (Hazan et al, 2017)</a> - my resume is <a href="https://www.alexirpan.com/2017/06/27/hyperparam-spectral.html">here</a> if you <a href="https://arxiv.org/abs/1706.00764">'re</a> interested).  The NAS does not specifically configure the hyperparameters, but I think it is quite reasonable that the design decisions of the neural network are made this way.  This is good news for learning, given the strong correlation between the solution and performance.  Finally, here is not only a rich reward, but here is exactly what is important to us when training models. <br><br>  Together, these reasons make it possible to understand why a NAS requires ‚Äúonly‚Äù about 12,800 trained networks to determine the best neural network, compared with the millions of examples needed in other environments.  From different sides, everything plays in favor of RL. <br><br>  In general, such success stories are still the exception, not the rule.  Many puzzle pieces must be folded correctly so that learning with reinforcements works convincingly, and even in this case it will not be easy. <br><br>  In general, now deep RL can not be called a technology that works out of the box. <br><br><h1>  A look into the future </h1><br>  There is an old saying that every researcher learns how to hate his area of ‚Äã‚Äãresearch.  The joke is that researchers still continue to do this, because they like problems too much. <br><br>  This is about how I feel about deep learning with reinforcement.  Despite all this, I am absolutely sure that we need to try to apply RL on various problems, including those where it is most likely to be ineffective.  But how else can we improve RL? <br><br>  I see no reason why in-depth RL will not work in the future if technology is given time to improve.  Some very interesting things will start to happen when the deep RL becomes reliable enough for widespread use.  The question is how to achieve this. <br><br>  Below I have listed some plausible options for the development of the future.  If additional research is needed for development in this area, then references to relevant scientific articles in these areas are indicated. <br><br>  <b>Local optima are good enough</b> .  It would be arrogant to say that people themselves are globally optimal in everything.  I would say that we are slightly better than other species optimized to create civilization.  In the same vein, the RL decision does not necessarily have to strive for global optima if its local optima exceed the basic level of a person. <br><br>  <b>Iron will solve everything</b> .  I know people who believe that the most important thing for creating AI is to simply increase the speed of iron.  Personally, I am skeptical that iron will solve all problems, but it will certainly make an important contribution.  The faster everything works, the less you worry about the inefficiency of the samples and the easier it is to brute force through the problems of intelligence. <br><br>  <b>Add more learning signals</b> .  Sparse rewards are hard to grasp, because there is very little information about what gives effect.  It is possible that we will either generate positive rewards in the form of hallucinations ( <a href="https://arxiv.org/abs/1707.01495">Hindsight Experience Replay, Andrychowicz et al, NIPS 2017</a> ), or define auxiliary tasks ( <a href="https://arxiv.org/abs/1611.05397">UNREAL, Jaderberg et al, NIPS 2016</a> ), or build a good model of the world, starting with self-controlled learning .  Add cherries to the cake, so to speak. <br><br>  <b>Model training will increase sample efficiency</b> .  Here's how I describe the RL based on the model: ‚ÄúEveryone wants to do it, but few know how.‚Äù  In principle, a good model fixes a bunch of problems.  As can be seen in the example of AlphaGo, the presence of a model in principle greatly facilitates the search for a good solution.  Good models of the world are not bad transferred to new tasks, and the introduction of a model of the world allows you to imagine a new experience.  In my experience, model-based solutions also require fewer samples. <br><br>  But learning a good model is difficult.  I got the impression that low-dimensional state models sometimes work, but image models are usually too difficult.  But if they become simpler, some interesting things can happen. <br><br>  <a href="http://citeseerx.ist.psu.edu/viewdoc/download%3Bjsessionid%3D711FEF6BA26BBF98C28BC111B26F8761%3Fdoi%3D10.1.1.48.6005%26rep%3Drep1%26type%3Dpdf">Dyna (Sutton, 1991)</a> and <a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Applications_files/dyna2.pdf">Dyna-2 (Silver et al., ICML 2008)</a> are classic works in this area.  As examples of work where model-based training is combined with deep-seated networks, I would recommend several recent articles from the Berkeley Robotics Laboratory: <br><br><ul><li>  <a href="http://bair.berkeley.edu/blog/2017/11/30/model-based-rl/">Neural Network Dynamics for Model-Based Deep RL with Model-Free Fine-Tuning (Nagabandi et al, 2017</a> ; </li><li>  <a href="https://arxiv.org/abs/1710.05268">Self-Supervised Visual Planning with Temporal Skip Connections (Ebert et al, CoRL 2017)</a> ; </li><li>  <a href="https://arxiv.org/abs/1703.03078">Combining Model-Based Model-Free Model for Reject Forwarding (Chebotar et al, ICML 2017)</a> ; </li><li>  <a href="http://rll.berkeley.edu/dsae/dsae.pdf">Deep Spatial Autoencoders for Visuomotor Learning (Finn et al, ICRA 2016)</a> ; </li><li>  <a href="http://jmlr.org/papers/v17/15-522.html">Guided Policy Search (Levine et al, JMLR 2016)</a> . </li></ul><br>  <b>Using reinforcement training is simply a fine tuning step</b> .  The first article on AlphaGo began training with a teacher followed by fine tuning of the RL.  This is a good option because it speeds up the initial learning, using a faster, but less powerful way.  This method worked in another context - see <a href="https://arxiv.org/abs/1611.02796">Sequence Tutor (Jaques et al, ICML 2017)</a> .  You can consider it as the beginning of the RL process with a reasonable, rather than random, prior distribution of probabilities (prior), where another system is involved in creating this ‚Äúprior‚Äù. <br><br>  <b>Remuneration functions can become learners</b> .  Machine learning promises that, based on data, one can learn to build things of higher quality than those designed by man.  If it is so difficult to choose the remuneration function, then why not use machine learning for this task?  Simulation training and the inverse of RL - these rich areas have shown that reward functions can be implicitly defined by evidence or ratings from a person. <br><br>  The most famous research papers on reverse RL and simulation training are <a href="http://ai.stanford.edu/~ang/papers/icml00-irl.pdf">Algorithms for Inverse Reinforcement Learning (Ng and Russell, ICML 2000)</a> , <a href="http://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf">Apprenticeship Learning through Inverse Reinforcement Learning (Abbeel and Ng, ICML 2004)</a> and <a href="https://www.cs.cmu.edu/~sross1/publications/Ross-AIStats11-NoRegret.pdf">DAgger (Ross, Gordon, and Bagnell, AISTATS 2011)</a> . <br><br>  Of the recent works that extend these ideas to the field of in-depth learning - <a href="https://arxiv.org/abs/1603.00448">Guided Cost Learning (Finn et al, ICML 2016)</a> , <a href="https://arxiv.org/abs/1704.06888">Time-Constrastive Networks (Sermanet et al, 2017)</a> and <a href="https://blog.openai.com/deep-reinforcement-learning-from-human-preferences/">Learning From Human Preferences (Christiano et al, NIPS 2017)</a> .  In particular, the last of the listed articles shows that the reward derived from the ratings put by people actually turned out to be better (better-shaped) for training than the original hard-coded reward - and this is a good practical result. <br><br>  From long-term work where deep learning is not used, I liked the articles <a href="https://arxiv.org/abs/1711.02827">Inverse Reward Design (Hadfield-Menell et al, NIPS 2017)</a> and <a href="http://proceedings.mlr.press/v78/bajcsy17a/bajcsy17a.pdf">Learning Robot Objectives from Physical Human Interaction (Bajcsy et al, CoRL 2017)</a> . <br><br>  <b>Transfer training</b> .  Transferring training promises that you can use the knowledge from previous tasks to speed up learning new tasks.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I am quite sure that this is the future when learning becomes reliable enough to solve disparate tasks. It is difficult to transfer training, if you cannot study at all, and if there are tasks A and B, it is difficult to predict whether transfer of training from task A to task B will occur. In my experience, here is either a very obvious answer or completely incomprehensible. And even in the most obvious cases, a non-trivial approach is required. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Recent work in this area is </font></font><a href="http://proceedings.mlr.press/v37/schaul15.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Universal Value Function Approximators (Schaul et al, ICML 2015)</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://arxiv.org/abs/1707.04175"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Distral (Whye Teh et al, NIPS 2017)</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><a href="https://deepmind.com/blog/enabling-continual-learning-in-neural-networks/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Overcoming Catastrophic Forgetting (Kirkpatrick et al, PNAS 2017)</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . From older works, see </font></font><a href="http://citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.297.6455%26rep%3Drep1%26type%3Dpdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Horde (Sutton et al, AAMAS 2011)</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">For example, robotics shows good progress in transferring training from simulators to the real world (from simulating a task to a real task). See </font></font><a href="https://blog.openai.com/spam-detection-in-the-physical-world/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Domain Randomization (Tobin et al, IROS 2017)</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://arxiv.org/abs/1610.04286"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sim-to-Real Robot Learning with Progressive Nets (Rusu et al, CoRL 2017)</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><a href="https://research.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GraspGAN (Bousmalis et al, 2017)</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . (Disclaimer: I worked on GraspGAN). </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Good priors can greatly reduce training time.</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. This is closely related to some of the preceding paragraphs. On the one hand, the transfer of training is to use past experience to create good a priori probability distributions (priors) for other tasks. RL algorithms are designed to work on any Markov decision-making process - and this is where the problem with generalization arises. If we believe that our solutions work well only in a narrow sector of environments, then we should be able to use a common structure to effectively solve these environments.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pietr Abbil in his speeches likes to note that the deep RL needs to be given only such tasks as we solve in the real world. I agree that this makes a lot of sense. There must be a certain real world prior (real-world prior) that allows us to quickly learn new real-world problems at the expense of slower learning of unrealistic tasks, and this is a perfectly acceptable compromise.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The difficulty is that such a prior world is very difficult to design. However, I think there is a good chance that this is still possible. Personally, I am delighted with the recent work on meta-learning, because it provides a way to generate reasonable priors from the data. For example, if I want to use RL to navigate the warehouse, it would be curious to first use meta-learning to teach good navigation in general, and then fine-tune this priority for the particular warehouse where the robot will move. This is very similar to the future, and the question is whether meta-learning will get there or not. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">For a summary of recent learning-to-learn work, see </font></font><a href="http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">this publication from BAIR (Berkeley AI Research)</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">More complex environments, paradoxically, may be easier.</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. One of the main conclusions of the DeepMind article on parkour bot is that if you complicate a task very much by adding several variations of tasks, then in reality you simplify learning, because the rules cannot be retrained in one situation without losing productivity in all other parameters . We saw something similar in articles on domain randomization (domain randomization) and even in ImageNet: models trained in ImageNet can expand to other environments much better than those trained in CIFAR-100. As I said before, maybe we only need to create some ‚ÄúImageNet for management‚Äù in order to go to a much more universal RL. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There are many options. </font></font><a href="https://github.com/openai/gym"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenAI Gym is the</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> most popular environment, but there is also the </font></font><a href="https://github.com/mgbellemare/Arcade-Learning-Environment"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Arcade Learning Environment</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ,</font></font><a href="https://github.com/openai/roboschool"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Roboschool</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://github.com/deepmind/lab"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DeepMind Lab</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://github.com/deepmind/dm_control"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DeepMind Control Suite</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><a href="https://github.com/facebookresearch/ELF"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ELF</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Finally, although it is a shame from an academic point of view, the empirical problems of deep RL may not matter from a practical point of view. As a hypothetical example, imagine that a financial company uses deep RL. They train a sales agent on past US stock market data using three random sid. In real A / B testing, the first seed brings 2% less revenue, the second works with average profitability, and the third brings 2% more. In this hypothetical version, reproducibility does not matter - you simply unfold a model whose yield is 2% higher and you rejoice. Similarly, it does not matter that a sales agent can work well only in the United States: if it expands badly on the world market, simply do not use it there.There is a big difference between an extraordinary system and a reproducible extraordinary system. Perhaps you should focus on the first one.</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Where are we now </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In many ways, I am annoyed by the current state of the deep RL. </font><font style="vertical-align: inherit;">And yet it attracts such a strong interest from researchers, which I have not seen in any other field. </font><font style="vertical-align: inherit;">My feelings are best expressed by the phrase Andrew Eun referred to in his speech </font></font><a href="https://www.youtube.com/watch%3Fv%3DF1ka6a13S9I"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nuts and Bolts of Applying Deep Learning</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : strong pessimism in the short term, balanced by even stronger long-term optimism. </font><font style="vertical-align: inherit;">Now the deep RL is a bit chaotic, but I still believe in the future. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">However, if someone asks me again if training with reinforcements (RL) can solve their problem, I will still immediately answer that no - they cannot. </font><font style="vertical-align: inherit;">But I will ask you to repeat this question in a few years. </font><font style="vertical-align: inherit;">By that time, maybe everything will work out.</font></font></div><p>Source: <a href="https://habr.com/ru/post/349800/">https://habr.com/ru/post/349800/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../349790/index.html">Announcement Dart 2.0: Optimized for customer development</a></li>
<li><a href="../349792/index.html">Network layer in iOS application</a></li>
<li><a href="../349794/index.html">Magento Meetup Kharkov - Videos and Presentations</a></li>
<li><a href="../349796/index.html">Why I don't like DevOps (and modern software)</a></li>
<li><a href="../349798/index.html">Automate the removal of forgotten transactions</a></li>
<li><a href="../349802/index.html">Multi-stage (multi-stage builds) builds in Docker</a></li>
<li><a href="../349804/index.html">From a series of conversations with colleagues or a grain of experience: DC Edge design</a></li>
<li><a href="../349808/index.html">How we do AB-DOC</a></li>
<li><a href="../349810/index.html">Hyperapp for Refugees with React / Redux</a></li>
<li><a href="../349812/index.html">Segregated Witness for Dummies</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>