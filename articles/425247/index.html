<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Crowdsourcing Testing</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Regression testing is a very important part of working on product quality. And the more products and the faster they develop, the more effort it requi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Crowdsourcing Testing</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/ip/m_/g0/ipm_g0xsqi2usuqwicmpt9usxjg.jpeg"><br><br>  Regression testing is a very important part of working on product quality.  And the more products and the faster they develop, the more effort it requires. <br><br>  In Yandex, we learned how to scale up the tasks of manual testing for most products using assessors - remote employees working part-time on a piece-rate basis, and now hundreds of assessors are taking part in testing Yandex products, in addition to full-time testers. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In this post it is told: <br><br><ul><li>  How did you manage to make the tasks of manual testing as formalized as possible and train them with hundreds of remote employees; </li><li>  How did you manage to put the process on industrial rails, provide testing in various environments, withstand SLA in speed and quality; </li><li>  What difficulties they faced and how they were solved (and some have not yet decided); </li><li>  How did testing by testors contribute to the development of Yandex products, how did it affect the frequency of releases and the number of skipped bugs. </li></ul><a name="habracut"></a><br>  The text is based on the transcript of the <a href="https://heisenbug-piter.ru/talks/2018/spb/6vm8srckjooygusg6quem8/">report by</a> <b>Olga Megorskaya</b> from our May conference Heisenbug 2018 Piter: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/1yq9aModCyo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  <i>From the day of the report, some numbers had changed, in such cases we indicated actual data in brackets.</i>  <i>Further, it comes from the first person:</i> <br><br>  Today we will talk about the use of crowdsourcing techniques for scaling manual testing tasks. <br><br>  I have a rather strange job title: the head of the department of expert assessments.  I will try to tell by examples what I do.  In Yandex, I have two main responsibility vectors: <br><br><img src="https://habrastorage.org/webt/0r/i6/hu/0ri6hudtbemee0isdg0-lnzecng.jpeg"><br><br>  On the one hand, this is all that is associated with crowdsourcing.  I am responsible for our crowdsourcing Yandex.Tolok platform. <br><br>  And on the other - the teams, which, if you try to give a universal definition, can be attributed to the "massive non-working vacancies."  There are many things that go in there, including one of our recent projects: manual testing with the help of crowd, which we call ‚Äútesting by assessors‚Äù. <br><br>  My main activity in Yandex is that I bring together the left and right columns from the image and try to optimize the tasks and processes in mass production using crowdsourcing.  And today we will talk just about it on the example of testing tasks. <br><br><h2>  What is crowdsourcing? </h2><br>  Let's start with what crowdsourcing is.  It can be said that this is a replacement of the expertise of one particular specialist for the so-called ‚Äúwisdom of the crowd‚Äù in cases where the expertise of a specialist is either very expensive or difficult to scale. <br><br>  Crowdsourcing is actively used in various areas is not the first year.  For example, NASA loves crowdsourcing projects very much.  There, with the help of the ‚Äúcrowd‚Äù, they explore and discover new objects in the galaxy.  It seems that this is a very difficult task, but with the help of crowdsourcing it comes down to quite simple.  There is a <a href="https://www.zooniverse.org/projects/marckuchner/backyard-worlds-planet-9">special site</a> on which they spread hundreds of thousands of photos taken by space telescopes and ask anyone who wants to search for certain objects there.  And when a lot of people found it suspiciously similar to the object they need, then higher-level specialists are connected and they begin to investigate it. <br><br>  Generally speaking, crowdsourcing is such a method when we take some big high-level task and divide it into many simple and homogeneous subtasks into which many independent performers gather.  Each of the performers can solve one or more of these small puzzles, and together they ultimately work on one big common cause and collect a great result for a high-level task. <br><br><h2>  Crowdsourcing in Yandex </h2><br>  We have already begun to develop our crowdsourcing system for several years.  Initially, it was used for tasks related to machine learning: to collect training data, to configure neural networks, search algorithms, and so on. <br><br><img src="https://habrastorage.org/webt/tr/y2/xx/try2xx-snzmr6jvcoabugadlime.jpeg"><br><br>  How does our crowdsourcing ecosystem work?  First, we have <a href="https://toloka.yandex.ru/">Yandex.Toloka</a> .  This is an open crowdsourcing platform on which anyone can register either as a customer (place their tasks, set a price for them and collect data), or as a performer (find interesting tasks, perform them and receive a small reward).  Toloku we launched a few years ago.  Now we have more than a million registered performers (we call them Tolokers), and every day in the system about 17,000 people perform tasks. <br><br>  Since we initially created Toloka with an eye on tasks related to machine learning, it has traditionally been the case that most of the tasks that tolokers perform are tasks that are very simple and trivial for man to do, but it is still rather difficult for the algorithm.  For example, look at a photo and say whether there is adult content on it or not, or listen to an audio recording and decipher what you heard. <br><br>  Toloka is a very powerful tool in terms of performance and the amount of data it helps to collect, but rather non-trivial to use.  The people in the picture are wearing yellow balaclavas, because all the performers in Toloka are anonymous and unknown to customers.  And managing these thousands of Anonymus, making them do exactly what you need is not an easy task at all.  Therefore, not all the tasks that we have, we are still able to solve with the help of such an absolutely ‚Äúwild‚Äù crowd.  Although we are striving for this, I will say more about this later. <br><br>  Therefore, for higher-level tasks, we have the next level of performers.  These are the people we call assessors.  The word ‚Äúassessors‚Äù itself may be a little strange.  It came from the word ‚Äúassessment‚Äù, that is, ‚Äúassessment‚Äù, because we initially used assessors to collect subjective assessments of the quality of search results.  This data was then used as the target for machine learning search ranking functions.  Much time has passed since then, assessors began to perform many very different other tasks, so now this is already a nominal word: the tasks have changed, but the word has remained. <br><br>  In fact, our assessors are full-time employees of Yandex, but working part-time and completely remotely.  These are guys who work on their own equipment.  We interact with them only remotely: we remotely select them, remotely train them, remotely work with them and, if necessary, remotely dismiss them.  We never intersect in person with most of them.  They work on any schedule convenient for themselves, day or night: they have minimum rates equivalent to about 10-15 hours a week, and they can work out this time as they see fit.  Assessors perform a variety of tasks: they are associated with search, and technical support, and with some low-level translations, and with testing, which we will continue to talk about. <br><br>  As a rule, no matter what task we take, the most talented people who do it better, who are interested in this particular task, always stand out from the group of assessors who perform it.  We single them out, endow them with a loud title of super-assessors, and these guys are already performing higher-level functions as curators: they check the quality of other people‚Äôs work, advise them, support them, and so on. <br><br>  And only at the very top of our pyramid do we have the first full-time employee who sits in the office and controls these processes.  We have far fewer people who are much more advanced and have strong technical and managerial skills, literally ones.  Such a system allows us to come to the fact that these units of ‚Äúhigh-level‚Äù people build pipelines and manage production chains, in which dozens, hundreds and even thousands of people are involved. <br><br>  By itself, this scheme is not new, even Genghis Khan successfully applied it.  She has some interesting properties that we try to use.  The first property is quite understandable - such a scheme is very easily scaled.  If some task needs to be suddenly started to do more, then we do not need to look for additional space in the office in order to put a person somewhere.  We generally have very little to think about: just pour more money, hire more performers with this money, and more talented guys in academic caps will surely grow out of these performers, and the whole system will scale further. <br>  The second property (and it was surprising for me) - such a pyramid is very well replicated, regardless of the subject area in which to apply it.  This also applies to the area we are going to talk about today - the tasks of manual testing. <br><br><h2>  Crowd testing </h2><br>  When we started the process of testing with the help of crowde, the biggest problem was the lack of a positive reference.  There was no experience that we could refer to and say: ‚ÄúWell, these guys have done so, they are already testing with the help of a crowd in a very similar pattern to us, and everything is good there, it means that everything will be fine with us.‚Äù  Therefore, we had to rely only on our personal experience, which was separated from the testing domain and was more associated with setting up similar production processes, but in other areas. <br><br>  Therefore, we had to do what we can do.  What can we do?  In essence, decompose one task into tasks of different levels of complexity and scatter them across the floors of our pyramid.  Let's see what we did. <br><br>  First, we looked at the tasks that our testers in Yandex do, and asked them to conditionally scatter these tasks at different levels of complexity.  This is the "hospital average": <br><br><img src="https://habrastorage.org/webt/-o/ms/_f/-oms_fh-u-y4kd-ozicqyoobcio.jpeg"><br><br>  They estimated that only 57% of their time is spent on complex high-level tasks, and somewhere around 20% is spent on a very low-level routine that everyone wants to get rid of, and on tasks a little more complicated, which can also be delegated.  Encouraged by these figures, which show that almost half of the work can be transferred somewhere, we began to build testing with the help of crowds. <br><br>  What goals did we set for ourselves? <br><br><ul><li>  To make testing to cease to be a bottleneck, which it periodically appeared in production processes when the release is ready, but it waits until it passes testing. </li><li>  Unload our cool, very smart, high-level specialists - full-time testers - from the routine, taking them really interesting and higher-level tasks. </li><li>  Increase the variety of environments in which we test products. </li><li>  To learn how to handle peak loads, because our testers said that they often have uneven loads.  Even if, on average, the team copes with the tasks, when a peak occurs, then it has to be raked for a very long time. </li><li>  Since we in Yandex still spent quite noticeable money on outsourcing testing in some projects, we thought that we would like to get a little more results for the money we spend, to optimize our outsourcing expenses. </li></ul><br>  I want to emphasize that among these goals there is no task to replace testers with crowds, to somehow harm them, and so on.  All we wanted to do was to help the testing teams, freeing them from the low-level routine load. <br><br>  Let's see what we got in the end.  I‚Äôll immediately say that the main testing tasks are now performed not by the lowest level of the ‚Äúpyramid‚Äù, but by the assessors, but by the assessors, our staff members.  Then we will discuss mainly about them, except for the very end. <br><br>  Now assessors carry out regression testing tasks and pass all kinds of polls like ‚Äúlook at this application and leave your feedback‚Äù.  At the full-fledged task of regression, we now have about 300 people qualified ( <i>note: since the report was 500</i> ).  But this figure is conditional, because the system that we have built works for an arbitrary number of people: as many as we need.  Now our production needs are covered by approximately as many people.  This does not mean that at each moment of time they are all ready and ready to perform the task: since assessors work in a flexible schedule, at every moment of time 100-150 people are ready to connect.  But just a pool of artists like this.  And simple tasks, like polls, when you just need to collect unformalized feedback from users, we have a lot more people going through: hundreds and thousands assessors participate in such surveys. <br><br>  Since these are people who work on their own equipment, each assessor has his own personal devices.  This, by default, the desktop and some kind of mobile device.  Accordingly, we test our products on personal assessors devices.  But it is clear that they do not have all possible devices, so if we need testing in some rare environment, we use remote access through the device farm. <br><br>  Now crowd testing is already used as a standard production process of about 40 ( <i>approx .: now 60</i> ) of Yandex services and commands: this is Mail, and Disk, and Browser (mobile and desktop), and Maps, and Search, and many, many Who.  This is curious.  When we set plans for the end of the third quarter in the fall of 2017, we had an ambitious goal: to attract at least somehow, ‚Äúat least with deception, even with bribery,‚Äù at least five teams that would use our testing processes with the help of crowds.  And we very strongly persuaded everyone to say: ‚ÄúYes, do not be afraid, come on, try it!‚Äù But after just a few months, we had dozens of teams. <br><br>  And now we are solving another problem: how to manage to connect more and more new teams that want to join these processes.  So we can assume that now it is a standard practice in Yandex, which flies very well. <br><br>  What have we got in terms of performance indicators?  Now we are doing about 3,000 regression testing cases per day ( <i>note: as of October 2018, already 7,000 cases</i> ).  Test runs, depending on the size, run from several hours to (peak) 2 days.  Most of the passes in a few hours, within a day.  The introduction of such a system has allowed us to reduce the cost by about 30% compared with the period when we used outsourcing.  This allowed teams to be released much more often, on average, somewhere several times, because releases began to take place with the speed that is available to the development, and not to the one that is available for testing, when it sometimes became a bottleneck. <br><br>  Now I will try to tell you how we even built a production process that allowed us to come to this scheme. <br><br><h2>  Infrastructure </h2><br>  Let's start with the technical infrastructure.  Those of you who have seen Toloka as a platform, imagine what its interface looks like: you can come into the system, choose tasks that interest you and carry them out.  For internal employees, we have an internal instance of Toloki, in which we, among other things, distribute various types of tasks for our assessors. <br><br><img src="https://habrastorage.org/webt/13/79/ra/1379rap2naab6uybwpdga_fi4wq.jpeg"><br><br>  The picture shows what this interface looks like.  Here you can see the tasks available to the assessor: there are several testing tasks and several tasks of a different type that the assessor from this example can also perform.  And here the person comes, sees the tasks available to him at the moment, clicks "Proceed", receives test cases for analysis and begins to perform them. <br><br>  An important part of our infrastructure is farms.  Not all devices are on hand, so the task is, in fact, a couple: a test case and the environment in which it needs to be checked.  When a person presses the ‚ÄúProceed‚Äù button, the system checks if he has an environment in which to conduct a test.  If there is, then the person simply takes the task and tests it on a personal device.  If not, we send it via remote access to the farm. <br><br><img src="https://habrastorage.org/webt/pf/nv/dg/pfnvdgufrdlouvdstugsj9pw3rm.jpeg"><br><br>  The picture shows how it looks, on the example of a mobile farm.  So a person remotely connects to a mobile phone that lies in our office on the farm.  For Android, we use OpenSTF open source solutions.  For iOS, there are no good solutions - to such an extent that we have already made our own (but we will tell about it in detail some time next), because we could not find either the open source or anything that would make sense to buy.  It is clear that the farm is useful in cases where we do not have people who have the right devices.  And another important advantage of it is that the farm has a very high utilization rate: whenever and whatever person comes, we can send it to the farm at any time.  This is better than handing out devices personally, because devices handed out to a person are available for work only when that person is ready to work. <br><br>  We talked a little about how this is implemented for our assessors from a technical point of view, and now the most interesting part for me: the principles of how we organized this production in general. <br><br><h1>  Crowd Production Principles </h1><br>  For me in this project it was interesting that the subject area seems to be very specific, but all the principles of production organization are fairly universal: the same ones used in organizing mass production in other subject areas. <br><br><h2>  1. Formalization </h2><br>  The first principle (not the most important, but one of the most important, one of our ‚Äúwhales, elephants and turtles‚Äù) is the formalization of tasks.  I think you all know it by yourself.  Almost any task is the easiest to do yourself.  It‚Äôs a little harder to explain to your colleague who is sitting next to you in a room so that he can do exactly what you need.  And the task to make so that hundreds of performers whom you have never seen, who work remotely, at any arbitrary time, did exactly what you expect from them - this task is several orders of magnitude more complex and implies a rather high threshold for entering to start doing this at all.  In the context of testing tasks, the task with us, of course, is a test case that needs to be passed and processed. <br><br>  And what should be the test cases in order to be able to use them in such a task as testing with crowds? <br><br>  Firstly - and it turned out that this is not at all a matter of course - there should be test cases in general.  There were such cases when teams came to us who wanted to be connected to testing by assessors, we said: ‚ÄúGreat, bring your test cases, we will pass them!‚Äù At that moment the customer was sad, left, not even always returned.  After several such appeals, we realized that, probably, help was needed in this place.  Because if a tester from a team of a service himself regularly tests his services, he doesn‚Äôt really need complete, well-described test cases.  And if we want to delegate this task to a large number of performers, then we simply cannot do without it. <br><br>  But even in those cases where there were generally test cases, they were almost always understood only by those people who are very deeply immersed in service.  And all other people who are out of context, it was very difficult to understand what is happening here and what needs to be done.  Therefore, it was important to rework the test cases so that they were understandable to a person who was not immersed in the context. <br><br>  One last thing: if we reduce the task of testing to the strictly formal passage of concrete cases, it is very important to ensure that these cases are constantly updated, updated and replenished. <br><br>  I will give a few examples. <br><br><img src="https://habrastorage.org/webt/g6/gu/qd/g6guqdw45hxtterffonf7uwhn8y.jpeg"><br><br>  The picture above, for example, shows a good case from our native Toloka service, in which you need to check the correctness of the work of the artist‚Äôs profile.  Here everything is broken down in steps.  There is every step that needs to be done.  There is an expectation of what should happen at every step.  Such a case will be clear to anyone. <br><br><img src="https://habrastorage.org/webt/cm/te/ze/cmtezebrziqy0qmsek3bk_amhoo.jpeg"><br><br>  And here is an example of a not so successful case.  In general, it is not clear what is happening.  The description seems to be there, but in reality - what is it that you want from me?  This kind of cases - it does not immediately, very bad pass. <br><br>  How did we build the process of formalization of test cases, so that, firstly, we generally had them, constantly appeared and replenished, and, secondly, that they were clear enough for assessors? <br><br>  Through trial and error, we came to this scheme: <br><br><img src="https://habrastorage.org/webt/nr/yo/kw/nryokwvzfcqdwqqav-nj9iirmeg.jpeg"><br><br>  Our customer, that is, some kind of service or team, comes and in an arbitrarily arbitrary form, convenient for him, describes those test cases that he needs. <br><br>  After that comes our clever assessor, who looks at this freely formulated text and translates it into well-formalized and detailed test cases.  Why is it important that this assessor?  Because he himself was in the shoes of those people who pass test cases from completely different areas, and he understands how detailed a test case should be for colleagues to understand. <br><br>  After that, we run around the case: give assignments to assignors and collect feedback.  The process is organized in such a way that if a person does not understand what is required of him in the test case, he skips it.  As a rule, after the first time there is a fairly large percentage of omissions.  All the same, no matter how well we at the previous stage formalize test cases, it is never impossible to guess what would be incomprehensible to people.  Therefore, the first run is almost always a test version, its most important functions are to collect feedback.  After we collected feedback, received comments from assessors, found out that they understood and what was not, we rewrite, append test cases again.  And after several iterations we get cool, very clearly formulated test cases that are clear to everyone. <br><br>  Such an imposing order has an interesting side effect.  Firstly, it turned out that for very many teams this is generally a killer feature.  Everyone comes to us and says: ‚ÄúAnd what, can you really write test cases for me?‚Äù This is the most important thing, by which we attract our customers.  The second effect, unexpected for me - the order in the test cases has other delayed effects.  For example, we have technical writers who write user documentation, and it is much easier for them to write on the basis of such well-sorted and understandable test cases.  Previously, they had to distract the service to figure out what needs to be described, and now you can use our clear and cool test cases. <br><br>  I will give an example. <br><br><img src="https://habrastorage.org/webt/3w/fv/6q/3wfv6qb7fwsbvytn1kwdayjiufk.jpeg"><br><br>  This is how the test case looked like before it went through our meat grinder: a very short, not full description field, it says ‚Äúwell, look at the screenshot in the app‚Äù and that's it. <br><br><img src="https://habrastorage.org/webt/rc/q2/qg/rcq2qgb-0lj8vjrkpmjucgavkx8.jpeg"><br><br>  This is how it began to look after it was rewritten - steps and expectations were added at each of the steps.  So much better already.  It is much more pleasant to work with such a test case. <br><br><h2>  2. Scalable learning </h2><br>  The next task is my favorite, the most, I think, creative in this whole thing.  This is a scalable learning task.  In order for us to operate with such numbers - ‚Äúhere we have 200 assessors, here there are 1,000, and here there are 17,000 talkers in general every day‚Äù - it is important to be able to train people quickly and scalably. <br><br><img src="https://habrastorage.org/webt/jf/xl/iw/jfxliw5afoimerteq6h6auvxqew.jpeg"><br>  It is very important to come to such a system, when you spend no more time on training an arbitrary number of people than on training one specific specialist.  This, for example, is what we encountered when working with outsourcing.  The specialists are very cool, but in order to immerse them in the context of the work, the service took quite a lot of time, and at the output we still get one person who is six months immersed in the context.  And this is such a very non-scalable scheme.  It turns out that each next person needs to be submerged in the task context for another six months.  And it was necessary to expand this bottleneck. <br><br>  For any mass vacancies, not only in testing, we recruit people through several channels.  For testing, we do this.  First, we attract people who are in principle interested in testing tasks, guys somewhere in junior-testers, for them this is a good start, immersion in the subject area.  But there are still a limited number of such people on the market, and we need to have no restrictions on hiring people, so that we never rest on the number of performers. <br><br>  Therefore, in addition to searching testers specifically for these tasks, we are recruiting a group of people who simply responded to the common position of the assessor.  We are promoting something like this: whatever people you take, if there are a lot of them, you can arrange the process in such a way as to select the most capable of them and direct them to the solution of the task that you are pursuing.  In the context of testing, we build training in such a way that it is possible for arbitrary people who do not even know anything about testing, to train at least the minimal elements so that they begin to understand something.  Thanks to this approach, we never rest on the lack of performers and the number of people who work on these tasks for us, it all comes down to the question of the amount of money we are willing to spend on it. <br><br>  I do not know how often you come into contact with this topic, but in all sorts of popular science articles, especially about machine learning and neural networks, they often write that machine learning is very similar to human learning.  We show the child 10 cards with the image of a ball, and for the 11th time he will understand and say: ‚ÄúOh!  This is a ball! ‚ÄùIn fact, computer vision and any other machine learning technology also works. <br><br>  I want to talk about the reverse situation: the training of people can be built according to the same formal scheme as the training of machines.  What do we need for this?  We need a training set - a set of pre-marked examples in which a person will be trained.  We need a control set on which we can check whether he has studied well or not.  As in machine learning, you need a test set on which we understand how our function works at all.  And we need a formal metric that will measure the quality of the work performed.  It is on these principles that we have built training for the simplest regression testing tasks. <br><img src="https://habrastorage.org/webt/6h/l4/6o/6hl46oj6tgo7yc8hcja5jq2xiku.jpeg"><br><br>  The picture shows how this training looks like in us.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It consists of several parts. Firstly, there is a theory, then practice and then an exam, in which we check, the person understood the essence of the problem or did not understand. </font></font><br><br><img src="https://habrastorage.org/webt/50/n9/fe/50n9fejzu-q_mkmxk5xn0-1p1vk.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's start with the theory. It is clear that for any task that the assessor performs, we have a large, spreading, full-fledged instruction with a large number of examples, where everything is analyzed in great detail. But nobody reads it.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Therefore, in order to verify that theoretical knowledge really settled in a person‚Äôs head, we always give access to instructions, but after that we use what we conditionally call ‚Äútheoretical test‚Äù. This is a test in which we preload important questions and correct answers for us. Questions may be the most stupid. I think that for you it will be comical examples, but for people who are faced with the tasks of testing for the first time in their lives, these are not at all obvious things. For example: ‚ÄúIf I met several bugs, do I need to get several tickets ‚Äî one for each bug ‚Äî or dump everything in one pile?‚Äù Or: ‚ÄúWhat if I want to take a screenshot, but the screenshot doesn't work for me?‚Äù</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">These can be very different, arbitrarily low-level questions, and it is important for us that a person works on their own at the stage of learning the theory. Therefore, a theoretical test consists of questions of this type: ‚ÄúI found several bugs, do I have one ticket or several?‚Äù If a person chooses the wrong answer, he gets a red die that says: ‚ÄúNo, wait, the right answer here is another, turn to this Attention". Even if a person has not read the instructions, he cannot pass this test.</font></font><br><br><img src="https://habrastorage.org/webt/ft/1-/yn/ft1-yn0qgmhdl9bveswgr9usbdk.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The next moment is practice. How to make so that people who did not know anything at all about testing and did not respond specifically to the tester's vacancy understand what to do next? Here we come to the very training set. I think that you will immediately find a large number of bugs that are in this picture. This is the learning task for the assessor: here is a screenshot in front of you, find all the bugs on it. What is wrong here? Calculator sticks out.</font></font> What else?<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Layout went. </font></font><br><br><img src="https://habrastorage.org/webt/jc/if/ql/jcifqlxkuvfrze5zmseogun0htc.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Or here is a more complex example, "with an asterisk". The mailbox of the main recipient is open, I am the one to whom this letter has been sent. Here I see such a picture in front of me. What is the bug here? The biggest problem here is that the hidden copy is shown, and I, as the recipient of the letter, see who it was in the hidden copy. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Having passed a couple of dozens of such examples, even a person who is infinitely far from testing, is already beginning to understand what it is and what is required of him further when passing test cases. The practical part is a set of examples, the bugs in which we already know; we ask the person to find them and at the end show him: ‚ÄúLook, the bug was here,‚Äù so that he correlates his guesses with our correct answers.</font></font><br><br><img src="https://habrastorage.org/webt/gj/wm/aa/gjwmaaklsarkqe2hdr7dyb3g_m8.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And the last part is what we call the exam. We have a special test build, the bugs of which we already know, and we ask the person to go through it. Here we no longer show him the correct and incorrect answer choices, but simply see what he could find. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The beauty of this system and its scalability lies in the fact that all these processes occur completely autonomously, without the participation of the manager. We run as many people as we like: everyone who wants to read the instructions, everyone who wants to undergo a theoretical test, everyone who wants to go through the practice - all this happens automatically by pressing the button, but we don‚Äôt care at all.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The last part - the exam - is also passed by everyone who wants, and then, finally, we begin to look at them carefully. Since this is a test build and we know in advance all its bugs, we can automatically determine what percentage of the bugs were found by humans. If it is very low, then we don‚Äôt look any further, we write an automatic beating: ‚ÄúThank you very much for your efforts!‚Äù - and do not give this person access to combat missions. If we see that almost all the bugs have been found, then at that moment a person is connected who is looking at how correctly issued tickets, how well everything is done correctly according to the procedure, in terms of our instructions.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If we see that a person has independently mastered both theory and practice and has passed the exam well, then we let such people into our production processes. This scheme is good because it does not depend on how many people we pass through it. If we need more people, we just fill in more people at the entrance and get more at the exit. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This is a cool system, but, naturally, it would be naive to believe that after that you can already have a ready tester. Even the guys who have successfully passed our training, there are many questions with which they need to quickly help. And here we are faced with a lot of unexpected problems for us.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">People ask a lot of questions. </font><font style="vertical-align: inherit;">Moreover, these questions can be so strange that you would never have thought that the answers to these questions should be added to the instruction, described in a test or something like that. </font><font style="vertical-align: inherit;">If you think this is a normal situation. </font><font style="vertical-align: inherit;">Each of us is with you, when he finds himself in an unknown area, with a small probability, but he will ask some question that seems to the expert silly. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Here, the situation is aggravated by the fact that we have several hundred of these people, and even if everyone has a specific chance of asking a stupid question is low, the total is: ‚ÄúA-aa! </font><font style="vertical-align: inherit;">Oh god</font></font> What's happening?<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It floods us! ‚Äù </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sometimes questions seem strange. </font><font style="vertical-align: inherit;">For example, a person writes: ‚ÄúI don‚Äôt understand what it means to‚Äú tap into Undo ‚Äù.‚Äù </font><font style="vertical-align: inherit;">They say to him: ‚ÄúFriend! </font><font style="vertical-align: inherit;">This is the same as pressing the "Cancel" button. </font><font style="vertical-align: inherit;">He: ‚ÄúOh!</font></font> Thank!<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Now I understand everything. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Or another person says: "It seems everything is fine, but something is broken, I can not understand whether this is a mistake or not." But in a minute he himself understands where he got - in the task of testing; probably a broken photo is not very normal. Here he understood, and ok. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Or here is an interesting example that really plunged us into the abyss of research for a long time. A person comes and says: </font></font><br><br><img src="https://habrastorage.org/webt/tz/kk/q1/tzkkq1y0ynrbks8ak5agljalxoi.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Everyone does not understand what is happening, where he is from - we tried so hard, described test cases - until we find out that he has some special browser extension that translates from Russian to English, but then from English to Russian, and in the end it turns out some kind of heresy.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In fact, there are many such questions, the study of each of them takes some non-zero time. And at some point, our customers ‚Äî the services of the Yandex team who used testing by the assessors ‚Äî began to tear their hair out and say: ‚ÄúListen, we would spend much less time if we would test it all ourselves than sit in these chats and answer to these strange questions. ‚Äù </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Therefore, we have come to a two-level chat system. There is a conditional fludilka, where our assessors communicate with their curators, with these ‚Äúguys in hats‚Äù - here 90% of issues are solved. And only the most important and complex issues are escalated into a dedicated chat room in which the service team sits. This made life easier for all teams, everyone sighed calmly.</font></font><br><br><img src="https://habrastorage.org/webt/-e/2b/nb/-e2bnb0e9pjncoojaka0vzqvpsc.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">These horrors, which I am talking about, are not so terrible. The good news is that all these processes converge very quickly. Any first launch is always very bad. In the picture above, 6 consecutive launches of the same regression are seen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Look at how much time the staff spent on answering questions for the first time, when the assessors did not understand what they were talking about and what they wanted from them. </font><font style="vertical-align: inherit;">They found few bugs, they got a lot of tickets about anything. </font><font style="vertical-align: inherit;">Therefore, the first time is horror-horror-horror, the second time is horror-horror, and by the third time, 80 percent of all processes converge. </font><font style="vertical-align: inherit;">And then there is a cool process: the assessors get used to the new task, and after each launch we collect feedback, we supplement test cases, we analyze something. </font><font style="vertical-align: inherit;">And it turns out a cool factory that works by pressing a button and does not require any participation of a full-time specialist.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 3. Quality control </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> A very important point, without which all this will not work, is quality control. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Assessors work for us by piecework: all their tasks are very clearly regulated and quantified, each unit of work has its own standard tariff, and they receive payment for the number of units completed. Toloka works in the same way, and generally any crowd. This system has many advantages, it is very flexible, but it also has its drawbacks. In the system with piecework wages, any performer will try to optimize his work - to spend as little time and effort on the task in order to get much more money per unit of time. Therefore, any such system built on crowdsourcing is guaranteed to work with the minimum quality that you allow it. If you do not control the quality in any way, it will fall as low as it can fall.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The good news is that you can fight it, you can control it. If we are able to quantify the quality of work, then the task comes down to a rather simple one. This is in theory. In practice, it is not so simple at all, especially in testing tasks. Because testing, unlike many other mass problems that we solved with the help of assessors, deals with rare events, and all sorts of statistics work quite poorly there. It is very difficult to understand how often a person actually finds bugs, if in principle there are very few bugs. Therefore, we have to pervert and use several quality control methods at once, which together will give us a certain picture of the quality with which the performer works.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The first is a check in the overlap. ‚ÄúOverlapping‚Äù means that we assign each task to several people. We do this in a natural way, because each test case needs to be tested in several environments. Thus, it turns out that the same test case was tested in environments A, B and C. We have three results from three people - passing the same test case. Further we look, whether results have dispersed.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sometimes it happens that in one environment a bug was found, in the other two it was not found. Maybe it really is, or maybe someone‚Äôs mistake: either one person found an extra bug, or those two faked and didn‚Äôt find something. In any case, this is a suspicious case. If we are faced with such a thing, then we send to an additional double-check in order to be sure and verify who was right and who was wrong. Such a scheme allows us to catch people who, for example, got extra tickets in places where they were not needed, or missed where they were needed. At the same time, we look at how correctly the ticket was entered, whether everything is according to the procedure: whether screenshots are added, if necessary, if a clear description is added, and so on.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In addition to this - and this is especially true of the correctness of registration of tickets - it is pleasant and convenient to automatically control some things that, on the one hand, seem to be trifles, but, on the other hand, quite strongly affect the workflow. Therefore, we automatically check if there is an application to the ticket, whether screenshots have been added, whether there are comments on the ticket, or if it was just closed without looking at how much time was spent on it in order to detect suspicious cases. Here you can invent many different heuristics and apply them. The process is almost endless.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">An overlap check is a good thing, but it gives a slightly biased assessment, because we only check controversial cases. Sometimes you want to make an honest random inspection. To do this, we use the control runs. At the training stage, we had specially assembled test assemblies, in which we know in advance where there are bugs and where not. We use similar launches for quality control and check how many bugs a person found and missed how many. This is a cool way, it gives the most complete picture of the world. But it is quite expensive to use: while we are still assembling a new test assembly ... We use this approach quite rarely, every few months.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The last important point: even if we have already done everything, we need to analyze why the bugs were missed. </font><font style="vertical-align: inherit;">We check if it was possible to find this bug by test case steps. </font><font style="vertical-align: inherit;">If it was possible, and the person missed, then it means that the person is a burdock, and you need to have some effect on him. </font><font style="vertical-align: inherit;">And if this case was not, then you need to somehow complement, update the test cases. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">All the quality metrics we end up with in a single rating assessors, which affects their careers and fate in our system. </font><font style="vertical-align: inherit;">The higher the rating of a person, the more he receives more complex tasks and claims for bonuses. </font><font style="vertical-align: inherit;">The lower the assessor rating, the greater the likelihood of being dismissed. </font><font style="vertical-align: inherit;">When a person works steadily with a low rating, we end up with him.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 4. Delegation </font></font></h2><br>  The very last of the pillars of our pyramidal scalable scheme, which we want to talk about, is the task of delegation. <br><br><img src="https://habrastorage.org/webt/b5/fp/ih/b5fpiheifvw6ykupkph6climm1u.jpeg"><br><br>  I will remind once again how our pyramid looks for manual testing tasks.  We have ‚Äúhigh-level‚Äù people - they are full-time testers, representatives of the service team, who make up the training programs for the service they need for testing, form a strategy for what needs to be tested, write primary test cases in free form. <br><br>  Next, we have the most talented assessors who translate test cases from free form to formalized ones, help other assessors, support them in chatikah, and carry out rechecking and selective quality control. <br>  Further there is a cloud of our numerous performers who regress in steps. <br><br>  Next we have Toloka, about which we have not forgotten.  Now we are at the stage of experiments: we understand that the simplest cases can be given to testing in impersonal crowd in Toloka.  It will be much cheaper and faster, because there are even more performers.  But while we are in the process of building this system.  Now we give only the simplest, but I hope that in a few months we will come to the conclusion that we will delegate more to it. <br><br><img src="https://habrastorage.org/webt/jp/bl/hy/jpblhytlteo1wzry6b_ichrprlu.jpeg"><br><br>  It is very important to follow the correct development of this pyramid.  First of all (such questions are often asked to me, so I want to answer them proactively), crowdsourcing is not a rejection of the labor of high-level specialists in favor of crowding, but a scaling tool.  We cannot abandon the top of this pyramid, our ‚Äúhead‚Äù, we can only add more hands to this system with the help of crowdsourcing, thus really very easily scaling it almost for free. <br><br>  Secondly, this is not rocket science, but it must be constantly remembered: this whole story functions well and correctly, if the most complex tasks for this level are solved at its every level.  Roughly speaking, if the same can be done at several levels of the pyramid, it must be done at its lowest level.  This is not a static story, but a dynamic one.  We begin by saying that only ‚Äúhigh-level‚Äù people can do some tasks, gradually work out the process and lower these tasks below, scaling and cheapening the whole process. <br><br>  And I quite often hear such a remark: ‚ÄúWhy bother to make this garden in general, it‚Äôs better to just automate everything and spend energy on it.‚Äù  But crowdsourcing is not a replacement for automation, it is a parallel thing.  We do this not in place of automation, but in addition to it.  Such a system just allows us to free up working hands that could be engaged in automation, on the one hand, and, on the other hand, to formalize the process well enough, which then will be much easier to automate. <br><br><img src="https://habrastorage.org/webt/5z/xt/lr/5zxtlretcxlmrz2-vu9iowexhes.jpeg"><br>  Finally, let me remind you once again how our entire history looks like.  We begin by getting free test cases.  We run them several times through assessors, collect feedback, clarify them.  After that we get cool, already licked test cases.  In parallel with this, we recruit many, many people, conduct them through an automatic learning system and at the output we get only those who could cope with all the stages on their own and understood what we wanted from him.  We get trained crowd.  He works with us on formalized test cases, and we control its quality: we constantly recheck, analyze cases with missing bugs in order to improve our processes. <br><br>  And such a system works for us, flies.  I don‚Äôt know if my story will be useful to someone right now from a practical point of view, but I hope that it will allow us to think a little bit more widely and assume that some tasks can be solved in this way.  Because - and we are faced with this very often - some of you could already catch yourself thinking: ‚ÄúWell, maybe it works somewhere, but definitely not for me.  I have such difficult tasks that it‚Äôs not about me at all. ‚Äù  But our experience suggests that virtually any tasks from virtually any subject areas, if competently decomposed, formalized, and built into a clear process, can be at least partially scaled with the help of crowds. <br><br><blockquote>  If you like this report with <b>Heisenbug 2018 Piter</b> , pay attention: <b>Moscow's Heisenbug</b> will take place on December 6-7, there will also be many interesting things there, and descriptions of many reports can already be seen on <a href="https://heisenbug-moscow.ru/">the conference website</a> . </blockquote></div><p>Source: <a href="https://habr.com/ru/post/425247/">https://habr.com/ru/post/425247/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../425235/index.html">A little more about graphs, or how to detect dependencies between your applications.</a></li>
<li><a href="../425237/index.html">Time measurement with nanosecond precision</a></li>
<li><a href="../425241/index.html">Developer 20 years later: Vasily Lebedev about ICRE, education, his book and programming</a></li>
<li><a href="../425243/index.html">The john willis handbook</a></li>
<li><a href="../425245/index.html">Preview RamblerFront & # 6</a></li>
<li><a href="../425249/index.html">How is familiarity with the LLP at ITMO University: the course "Low-level programming"</a></li>
<li><a href="../425251/index.html">LoJax: the first known UEFI rootkit used in a malicious campaign</a></li>
<li><a href="../425253/index.html">Making a machine learning project in Python. Part 1</a></li>
<li><a href="../425255/index.html">Broo lossless compression algorithm and delta encoding, compared with Xdelta3. Home project development</a></li>
<li><a href="../425259/index.html">Backing up your site with git and a makefile</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>