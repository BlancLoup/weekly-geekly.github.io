<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Docker: flexible network without NAT for all occasions</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Time does not stand on the spot, and the beloved Docker from all versions has a new functionality from version to version. It happens that when you re...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Docker: flexible network without NAT for all occasions</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/eb1/180/eea/eb1180eeabd24f02af31e48cba60d44e.jpg" alt="image"><br><br>  Time does not stand on the spot, and the beloved Docker from all versions has a new functionality from version to version.  It happens that when you read Changelog for a new version, you see there something that can come in handy and make some things better than there are at the moment. <br><br>  So it was in my case.  I want to note that many of the tasks that have to be done, I do according to the principle of keep it simple.  That is almost always, if you can use simple tools and steps to solve the problem, I will choose this way.  I understand that a simple or complex step or tool is a subjective assessment, but because  we work in a team, then such criteria may be appropriate when choosing tools: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  Is the tool used in infrastructure? </li><li>  If something new is required, then is it possible to use what is already there? </li><li>  How much service maintenance (update, restart) will differ from other services? </li><li>  &lt;...&gt; </li></ul><br>  This article will focus on the network aspect of Docker.  I'll tell you about everything in order, but I want to note that this time I will not say "we use the host network, avoiding the use of NAT in every way". <br><a name="habracut"></a><cut><br>  How Docker works with the network can be found <a href="https://docs.docker.com/engine/userguide/networking/dockernetworks/">here</a> .  Highlight the main points: <br><br><ul><li>  default bridge network; </li><li>  host network; </li><li>  user defined network. </li></ul><br>  As I said earlier in some of my public speaking, we need maximum network performance for our containers.  If we talk about production, we do not use NAT for containers. <br><br>  For a long time (yes, why hide it - to this day), we use the <b>--net = host</b> parameter to launch containers, thereby obtaining a ‚Äúnative‚Äù eth inside the container.  Yes, in this case, one benefit - isolation - we, of course, lose ... But looking at the pros and cons in our particular case, we intentionally came to this decision, because  there were no tasks to isolate the network between running applications within the same host.  I want to remind you that I am writing about a specific Docker application site - in Badoo. <br><br>  What we know about our services: <br><br><ul><li>  we have a map where services are hosted on servers; </li><li>  we have a map of ports for each service and its type (or types, if there are many); </li><li>  we have an agreement that the ports should be unique. </li></ul><br>  Based on the foregoing, we guarantee the following: <br><br><ul><li>  if we run several services on the same machine with <b>--net = host</b> , then we will not get the intersection of the ports: everything will start and will work; </li><li>  if one of the eth-interfaces becomes not enough for us, then we will physically connect one more and by means of, for example, DNS, spread the load between them. </li></ul><br>  Everything is good, then why did I have to change something? <br><br>  It was evening, there was nothing to do ... Earlier it was said that we continue to transfer our services to containers.  If you follow this scenario, as is usually the case, the most difficult ones remain for later.  The reasons for this can be mass: <br><br><ul><li>  service is critical; </li><li>  lack of sufficient experience to offer the most transparent and quick service transfer to the container; </li><li>  You can "add something else to your taste." </li></ul><br>  So.  There was (and is) we have one such service, which has long been written.  It still works fine to this day, but it has several drawbacks: <br><br><ul><li>  it works on the same core (yes, it happens); </li><li>  filling the first space, it is worth noting that you can start several service instances and use <b>taskset / - cpuset-cpus</b> ; </li><li>  the service ‚Äúheavily‚Äù uses the network, and it also requires a large number of ports for outgoing connections. </li></ul><br>  This is how the service started before: <br><br><ul><li>  on the machine where the service was planned to raise, it was necessary to add an additional IP address (or even several) - <b>ip a add</b> (here you can immediately point out the many disadvantages of this approach that we know about); </li><li>  it is worth remembering the above, in order not to get, for example, 2 identical addresses on different machines; </li><li>  in the configuration of the daemon, it was worth pointing out at what address it works, just in order not to ‚Äúeat‚Äù all the ports of a neighbor or host system. </li></ul><br>  How could solve the problem, if it was too lazy to invent new methods: <br><br><ul><li>  leave everything as it is, but wrap in a container; </li><li>  bring up all the same additional IP addresses on dockerhost; </li><li>  Bind the application to a specific address. </li></ul><br>  How did I decide to approach the task?  Initially, of course, it all looked like an experiment, but why hide it - it was an experiment.  It seemed to me that the MACVLAN technology, which at that time was noted in Docker as Experimental (version 1.11.2), is suitable for this service, but in version 1.12 everything is already available in the main functionality. <br><br>  MACVLAN is essentially a Linux switch, which is based on static matching of MAC and VLAN.  It uses unicast filtering, not promiscuous mode.  MACVLAN can work in private mode, VEPA, bridge, passthru.  MACVLAN is a reverse VLAN in Linux.  This technology allows you to take one real interface and make on its basis several virtual ones with different MAC addresses. <br><br>  Also, the IPVLAN technology ( <a href="https://www.kernel.org/doc/Documentation/networking/ipvlan.txt">https://www.kernel.org/doc/Documentation/networking/ipvlan.txt</a> ) has recently appeared.  The main difference from MACVLAN is that IPVLAN can work in L3 mode.  In this article I will consider the use of MACVLAN (in bridge mode), because: <br><br><ul><li>  there is no restriction in 1 MAC address from one link on the active network equipment; </li><li>  the number of containers on the host will not be so large that it can lead to mac capacity exceeding.  Over time, this moment in our country can, of course, change; </li><li>  L3 is not needed at this stage. </li></ul><br>  A little more about MACVLAN vs IPVLAN can be found at <a href="http://hicu.be/macvlan-vs-ipvlan">http://hicu.be/macvlan-vs-ipvlan</a> . <br>  Here you can read the theory and how it works in Docker: <a href="">https://github.com/docker/docker/blob/master/experimental/vlan-networks.md</a> . <br><br>  The theory is great, but even there we see that overhead is the place to be.  You can and should look at the comparative tests of the MACVLAN bandwidth on the Internet (for example, <a href="http://comp.photo777.org/docker-network-performance/">http://comp.photo777.org/docker-network-performance/</a> and <a href="http://delaat.net/rp/2014-2015/p92/report.pdf">http://delaat.net/rp/2014-2015/p92/report. pdf</a> ), but also an integral part of the experiment is to conduct the test in their laboratory conditions.  To believe a word is good, but ‚Äúto touch it‚Äù and draw conclusions yourself is interesting and necessary. <br><br><h6>  So let's go! </h6><br>  In order to check if MACVLAN is working in Docker, we need to include support for experimental features in the latter. <br>  If this functionality is not included in the build, then you will see the following error messages in the logs: <br><br><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># docker network create -d macvlan --subnet=1.1.1.0/24 --gateway=1.1.1.1 -o parent=eth0 cppbig_vlan Error response from daemon: plugin not found</span></span></code> </pre> <br>  And in the process logs will be the following: <br><br><pre> <code class="hljs pgsql">docker[<span class="hljs-number"><span class="hljs-number">2012</span></span>]: <span class="hljs-type"><span class="hljs-type">time</span></span>="2016-08-04T11:44:44.095241242Z" <span class="hljs-keyword"><span class="hljs-keyword">level</span></span>=<span class="hljs-built_in"><span class="hljs-built_in">warning</span></span> msg="Unable to locate plugin: macvlan, retrying in 1s" docker[<span class="hljs-number"><span class="hljs-number">2012</span></span>]: <span class="hljs-type"><span class="hljs-type">time</span></span>="2016-08-04T11:44:45.095489283Z" <span class="hljs-keyword"><span class="hljs-keyword">level</span></span>=<span class="hljs-built_in"><span class="hljs-built_in">warning</span></span> msg="Unable to locate plugin: macvlan, retrying in 2s" docker[<span class="hljs-number"><span class="hljs-number">2012</span></span>]: <span class="hljs-type"><span class="hljs-type">time</span></span>="2016-08-04T11:44:47.095750785Z" <span class="hljs-keyword"><span class="hljs-keyword">level</span></span>=<span class="hljs-built_in"><span class="hljs-built_in">warning</span></span> msg="Unable to locate plugin: macvlan, retrying in 4s" docker[<span class="hljs-number"><span class="hljs-number">2012</span></span>]: <span class="hljs-type"><span class="hljs-type">time</span></span>="2016-08-04T11:44:51.095970433Z" <span class="hljs-keyword"><span class="hljs-keyword">level</span></span>=<span class="hljs-built_in"><span class="hljs-built_in">warning</span></span> msg="Unable to locate plugin: macvlan, retrying in 8s" docker[<span class="hljs-number"><span class="hljs-number">2012</span></span>]: <span class="hljs-type"><span class="hljs-type">time</span></span>="2016-08-04T11:44:59.096197565Z" <span class="hljs-keyword"><span class="hljs-keyword">level</span></span>=error msg="Handler for POST /v1.23/networks/create returned error: plugin not found"</code> </pre> <br>  If you see such messages, then Docker‚Äôs MACVLAN support is not enabled. <br><br>  The test was symbolic, using <b>iperf</b> .  For each option, I started first 1 client, then 8 in parallel.  There were 2 options: <br><br><ul><li>  <b>--net = host</b> ; </li><li>  <b>--net = macvlan</b> . </li></ul><br><div class="spoiler">  <b class="spoiler_title">Look at the test details</b> <div class="spoiler_text">  We start the server: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># docker run -it --net=host --name=iperf_w_host_net --entrypoint=/bin/bash dockerio.badoo.com/itops/sle_12_base:latest # iperf3 -s -p 12345 ----------------------------------------------------------- Server listening on 12345 -----------------------------------------------------------</span></span></code> </pre><br>  We start the client: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># iperf3 -c 1.1.1.2 -p 12345 -t 30</span></span></code> </pre><br>  On the server we get the result: <br> <code>- - - - - - - - - - - - - - - - - - - - - - - - - <br> [ ID] Interval Transfer Bandwidth <br> [ 5] 0.00-30.04 sec 0.00 Bytes 0.00 bits/sec sender <br> [ 5] 0.00-30.04 sec 2.45 GBytes 702 Mbits/sec receiver <br></code> <br>  On the client: <br> <code>- - - - - - - - - - - - - - - - - - - - - - - - - <br> [ ID] Interval Transfer Bandwidth Retr <br> [ 4] 0.00-30.00 sec 2.46 GBytes 703 Mbits/sec 0 sender <br> [ 4] 0.00-30.00 sec 2.45 GBytes 703 Mbits/sec receiver <br></code> <br><br>  We are launching 8 clients in parallel: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># iperf3 -c 1.1.1.2 -p 12345 -t 30 -P 8</span></span></code> </pre><br>  On the server we get the result: <br> <code>- - - - - - - - - - - - - - - - - - - - - - - - - <br> [ ID] Interval Transfer Bandwidth <br> [ 5] 0.00-30.03 sec 0.00 Bytes 0.00 bits/sec sender <br> [ 5] 0.00-30.03 sec 314 MBytes 87.7 Mbits/sec receiver <br> [ 7] 0.00-30.03 sec 0.00 Bytes 0.00 bits/sec sender <br> [ 7] 0.00-30.03 sec 328 MBytes 91.5 Mbits/sec receiver <br> [ 9] 0.00-30.03 sec 0.00 Bytes 0.00 bits/sec sender <br> [ 9] 0.00-30.03 sec 305 MBytes 85.2 Mbits/sec receiver <br> [ 11] 0.00-30.03 sec 0.00 Bytes 0.00 bits/sec sender <br> [ 11] 0.00-30.03 sec 312 MBytes 87.3 Mbits/sec receiver <br> [ 13] 0.00-30.03 sec 0.00 Bytes 0.00 bits/sec sender <br> [ 13] 0.00-30.03 sec 316 MBytes 88.3 Mbits/sec receiver <br> [ 15] 0.00-30.03 sec 0.00 Bytes 0.00 bits/sec sender <br> [ 15] 0.00-30.03 sec 310 MBytes 86.7 Mbits/sec receiver <br> [ 17] 0.00-30.03 sec 0.00 Bytes 0.00 bits/sec sender <br> [ 17] 0.00-30.03 sec 313 MBytes 87.5 Mbits/sec receiver <br> [ 19] 0.00-30.03 sec 0.00 Bytes 0.00 bits/sec sender <br> [ 19] 0.00-30.03 sec 321 MBytes 89.7 Mbits/sec receiver <br> [SUM] 0.00-30.03 sec 0.00 Bytes 0.00 bits/sec sender <br> [SUM] 0.00-30.03 sec 2.46 GBytes 704 Mbits/sec receiver <br></code> <br>  On the client: <br> <code>- - - - - - - - - - - - - - - - - - - - - - - - - <br> [ ID] Interval Transfer Bandwidth Retr <br> [ 4] 0.00-30.00 sec 315 MBytes 88.1 Mbits/sec 0 sender <br> [ 4] 0.00-30.00 sec 314 MBytes 87.8 Mbits/sec receiver <br> [ 6] 0.00-30.00 sec 330 MBytes 92.3 Mbits/sec 0 sender <br> [ 6] 0.00-30.00 sec 328 MBytes 91.6 Mbits/sec receiver <br> [ 8] 0.00-30.00 sec 306 MBytes 85.6 Mbits/sec 0 sender <br> [ 8] 0.00-30.00 sec 305 MBytes 85.3 Mbits/sec receiver <br> [ 10] 0.00-30.00 sec 313 MBytes 87.5 Mbits/sec 0 sender <br> [ 10] 0.00-30.00 sec 312 MBytes 87.4 Mbits/sec receiver <br> [ 12] 0.00-30.00 sec 317 MBytes 88.8 Mbits/sec 0 sender <br> [ 12] 0.00-30.00 sec 316 MBytes 88.4 Mbits/sec receiver <br> [ 14] 0.00-30.00 sec 312 MBytes 87.1 Mbits/sec 0 sender <br> [ 14] 0.00-30.00 sec 310 MBytes 86.8 Mbits/sec receiver <br> [ 16] 0.00-30.00 sec 314 MBytes 87.9 Mbits/sec 0 sender <br> [ 16] 0.00-30.00 sec 313 MBytes 87.6 Mbits/sec receiver <br> [ 18] 0.00-30.00 sec 322 MBytes 90.2 Mbits/sec 0 sender <br> [ 18] 0.00-30.00 sec 321 MBytes 89.8 Mbits/sec receiver <br> [SUM] 0.00-30.00 sec 2.47 GBytes 707 Mbits/sec 0 sender <br> [SUM] 0.00-30.00 sec 2.46 GBytes 705 Mbits/sec receiver <br></code> <br><br>  2. Start the server using MACVLAN: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># docker run -it --net=cppbig_vlan --name=iperf_w_macvlan_net --ip=1.1.1.202 --entrypoint=/bin/bash dockerio.badoo.com/itops/sle_12_base:latest # iperf3 -s -p 12345 ----------------------------------------------------------- Server listening on 12345 -----------------------------------------------------------</span></span></code> </pre><br>  We start the client: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># iperf3 -c 1.1.1.202 -p 12345 -t 30</span></span></code> </pre><br>  On the server we get the result: <br> <code>- - - - - - - - - - - - - - - - - - - - - - - - - <br> [ ID] Interval Transfer Bandwidth <br> [ 5] 0.00-30.04 sec 0.00 Bytes 0.00 bits/sec sender <br> [ 5] 0.00-30.04 sec 2.45 GBytes 701 Mbits/sec receiver <br></code> <br>  On the client: <br> <code>- - - - - - - - - - - - - - - - - - - - - - - - - <br> [ ID] Interval Transfer Bandwidth Retr <br> [ 4] 0.00-30.00 sec 2.46 GBytes 703 Mbits/sec 0 sender <br> [ 4] 0.00-30.00 sec 2.45 GBytes 702 Mbits/sec receiver <br></code> <br><br>  We are launching 8 clients in parallel: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># iperf3 -c 1.1.1.202 -p 12345 -t 30 -P 8</span></span></code> </pre><br>  On the server we get the result: <br> <code>- - - - - - - - - - - - - - - - - - - - - - - - - <br> [ ID] Interval Transfer Bandwidth <br> [ 5] 0.00-30.03 sec 0.00 Bytes 0.00 bits/sec sender <br> [ 5] 0.00-30.03 sec 306 MBytes 85.4 Mbits/sec receiver <br> [ 7] 0.00-30.03 sec 0.00 Bytes 0.00 bits/sec sender <br> [ 7] 0.00-30.03 sec 319 MBytes 89.1 Mbits/sec receiver <br> [ 9] 0.00-30.03 sec 0.00 Bytes 0.00 bits/sec sender <br> [ 9] 0.00-30.03 sec 307 MBytes 85.8 Mbits/sec receiver <br> [ 11] 0.00-30.03 sec 0.00 Bytes 0.00 bits/sec sender <br> [ 11] 0.00-30.03 sec 311 MBytes 87.0 Mbits/sec receiver <br> [ 13] 0.00-30.03 sec 0.00 Bytes 0.00 bits/sec sender <br> [ 13] 0.00-30.03 sec 317 MBytes 88.6 Mbits/sec receiver <br> [ 15] 0.00-30.03 sec 0.00 Bytes 0.00 bits/sec sender <br> [ 15] 0.00-30.03 sec 322 MBytes 90.1 Mbits/sec receiver <br> [ 17] 0.00-30.03 sec 0.00 Bytes 0.00 bits/sec sender <br> [ 17] 0.00-30.03 sec 313 MBytes 87.5 Mbits/sec receiver <br> [ 19] 0.00-30.03 sec 0.00 Bytes 0.00 bits/sec sender <br> [ 19] 0.00-30.03 sec 310 MBytes 86.7 Mbits/sec receiver <br> [SUM] 0.00-30.03 sec 0.00 Bytes 0.00 bits/sec sender <br> [SUM] 0.00-30.03 sec 2.45 GBytes 700 Mbits/sec receiver <br></code> <br>  On the client: <br> <code>- - - - - - - - - - - - - - - - - - - - - - - - - <br> [ ID] Interval Transfer Bandwidth Retr <br> [ 4] 0.00-30.00 sec 307 MBytes 85.8 Mbits/sec 0 sender <br> [ 4] 0.00-30.00 sec 306 MBytes 85.5 Mbits/sec receiver <br> [ 6] 0.00-30.00 sec 320 MBytes 89.6 Mbits/sec 0 sender <br> [ 6] 0.00-30.00 sec 319 MBytes 89.2 Mbits/sec receiver <br> [ 8] 0.00-30.00 sec 308 MBytes 86.2 Mbits/sec 0 sender <br> [ 8] 0.00-30.00 sec 307 MBytes 85.9 Mbits/sec receiver <br> [ 10] 0.00-30.00 sec 313 MBytes 87.5 Mbits/sec 0 sender <br> [ 10] 0.00-30.00 sec 311 MBytes 87.1 Mbits/sec receiver <br> [ 12] 0.00-30.00 sec 318 MBytes 89.0 Mbits/sec 0 sender <br> [ 12] 0.00-30.00 sec 317 MBytes 88.6 Mbits/sec receiver <br> [ 14] 0.00-30.00 sec 324 MBytes 90.5 Mbits/sec 0 sender <br> [ 14] 0.00-30.00 sec 322 MBytes 90.2 Mbits/sec receiver <br> [ 16] 0.00-30.00 sec 314 MBytes 87.9 Mbits/sec 0 sender <br> [ 16] 0.00-30.00 sec 313 MBytes 87.6 Mbits/sec receiver <br> [ 18] 0.00-30.00 sec 311 MBytes 87.1 Mbits/sec 0 sender <br> [ 18] 0.00-30.00 sec 310 MBytes 86.8 Mbits/sec receiver <br> [SUM] 0.00-30.00 sec 2.46 GBytes 704 Mbits/sec 0 sender <br> [SUM] 0.00-30.00 sec 2.45 GBytes 701 Mbits/sec receiver <br></code> <br></div></div><br>  As can be seen from the results, there is overhead, but in this case it can be considered not critical. <br><br>  Limitations of technology by design: the availability of the container from the host and the availability of the host from the container is missing.  We need this functionality because: <br><br><ul><li>  a part of the service availability checks is checked by Zabbix ‚Äúhelpers‚Äù that are performed on the host where the service is running; </li><li>  there is a need to use caching DNS, which is located on the host system.  In our case, this is Unbound; </li><li>  there is a need to use access to some other services running on the host system. </li><li>  These are just some of the reasons why we need access to the ‚Äúhost &lt;==&gt; container‚Äù.  It is impossible to take and remake the architecture of such nodes overnight. </li></ul><br>  Options for overcoming this limitation: <br><br><ol><li>  Use two or more physical links on the machine.  This allows interaction through a neighboring interface.  For example, take eth1 and give it specifically for MACVLAN, and continue to use eth0 on the host system.  The option is certainly not bad, but it entails the need to keep the same number of links on all the machines where we plan to launch such services.  Implement it is expensive, not fast and not always possible. <br><br></li><li>  Use another additional IP address on the host system, hang it on the virtual MACVLAN interface, which you need to raise on the host system.  This is about as difficult from the point of view of support (‚Äúnot to forget / not to forget‚Äù), as the previous sentence is time.  And, since earlier I said that our service itself requires an additional address, then in the end, to start such a service, we need: <br><br><ul><li>  the address for the host host's main interface (1); </li><li>  address for the service (2); </li><li>  address for the virtual interface through which we will interact with the service (3). </li></ul><br>  In this case, it turns out that we need too many IP addresses, which, by and large, will be used a little.  In addition to the excessive expenditure of IP addresses, it is also worth remembering that we will need to support static routes through this very virtual interface to the container.  This is not an insurmountable complexity, but the complication of the system as a whole is a fact. <br><br>  The attentive reader will ask the question: ‚ÄúWhy the address on the main interface and on the MACVLAN interface, if you can give the address of the main interface to the virtual one?‚Äù In this case, we will leave our system without addresses on the real interfaces, but I‚Äôm not ready for that step yet. <br><br>  In the previous two variants it was assumed that the addresses of all interfaces belong to the same network.  As it is easy to imagine, even with 100 servers in such a subnet, if we have three addresses, then we will not get into <b>/ 24</b> anymore. <br><br></li><li>  Service IP.  The idea is that we make a separate subnet for services.  What it looks like: <br><br><ul><li>  we start to send ‚Äútagged‚Äù traffic to the server; </li><li>  native VLAN is left as the main network for dockerhost (eth0); </li><li>  we lifted the virtual interface with 802q, without the IP address on a host to the system; </li><li>  use for the service IP-address from the service network. </li></ul></li></ol><br>  Consider, as has already become clear, we will point three.  In order for everything to work, we need to do a few things: <br><br><ul><li>  in order to send "tagged" traffic to the interface, who do we need?  That's right, networkers!  We ask them to switch the access port to the port to which we are sending 2 VLANs; <br><br></li><li>  raise an additional interface on the host: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat /etc/sysconfig/network/ifcfg-vlan8 BOOTPROTO='static' STARTMODE='auto' VLAN='yes' ETHERDEVICE='eth0' # ip -d link show vlan8 31: vlan8@eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether e4:11:5b:ea:b6:30 brd ff:ff:ff:ff:ff:ff promiscuity 1 vlan protocol 802.1Q id 8 &lt;REORDER_HDR&gt;</span></span></code> </pre><br></li><li>  get a MACVLAN network in Docker <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># docker network create -d macvlan --subnet=1.1.2.0/24 --gateway=1.1.2.1 -o parent=vlan8 c_services</span></span></code> </pre><br></li><li>  make sure the Docker network appears: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># docker network ls | grep c_services a791089219e0 c_services macvlan</span></span></code> </pre> </li></ul><br>  Everything is done, all is well.  Then I decided to look at the general graphics on the host (or to be more precise, a colleague turned my attention to this).  Here is a picture we saw there: <br><br><img src="https://habrastorage.org/files/ac4/9eb/b85/ac49ebb85a8d474ea4da40818072f70a.png" alt="image"><br><br>  Yes, you can see the use of conntrack on the host. <br><br>  How so?  Well, do not need the same conntrack for MACVLAN ?!  As it was already in the evening, I decided to test even the most incredible theories.  In confirmation of my theoretical knowledge, connection tracking was not really needed.  Without it, everything continued to work.  Unloading modules, somehow tied to the conntrack, was impossible only at the time of launch of my container.  The ideas left me, and I went home, deciding that the morning of the evening was wiser. <br><br>  The next day, I was once again convinced of the accuracy of this statement.  So, I decided to use the ‚Äúclumsy‚Äù method so that Docker could not load nf_conntrack.  At first, I just renamed it (since blacklist is ignored when loading a module via modprobe), and then I started my container again.  The container, as expected, rose and felt great, but in the log I saw messages that four rules could not be added to iptables.  It turns out that conntrack is needed?  Here are the rules that did not want to be added: <br><br><pre> <code class="bash hljs">-t nat -A OUTPUT -d 127.0.0.11 -p udp --dport 53 -j DNAT --to-destination 127.0.0.11:35373 -t nat -A POSTROUTING -s 127.0.0.11 -p udp --sport 35373 -j SNAT --to-source :53 -t nat -A OUTPUT -d 127.0.0.11 -p tcp --dport 53 -j DNAT --to-destination 127.0.0.11:41214 -t nat -A POSTROUTING -s 127.0.0.11 -p tcp --sport 41214 -j SNAT --to-source :53</code> </pre><br>  Port 53?  There is a job associated with the "resolver".  And then I, to my surprise, find out about the embedded DNS server.  Well, albeit built-in, but is it possible to turn it off somehow with options?  No you can not :) <br><br>  Then I tried to return the module, start the service, correct the rules from iptables and unload the modules ... But it was not there.  By picking up modinfo, I found out which module depends on which one, and which one pulls someone along.  When creating a network, Docker forcibly makes <b>modprobe xt_nat</b> , which, in turn, depends on <b>nf_conntrack</b> , here is the confirmation: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># modinfo xt_nat filename: /lib/modules/4.4.0-3.1-default/kernel/net/netfilter/xt_nat.ko alias: ip6t_DNAT alias: ip6t_SNAT alias: ipt_DNAT alias: ipt_SNAT author: Patrick McHardy &lt;kaber@trash.net&gt; license: GPL srcversion: 9982FF46CE7467C8F2361B5 depends: x_tables,nf_nat intree: Y vermagic: 4.4.0-3.1-default SMP preempt mod_unload modversions</span></span></code> </pre><br>  As I said, everything works without these modules.  Accordingly, we can conclude that in our case they are not needed.  The question remains: why, nevertheless, are they needed?  I was not lazy and looked in 2 places: <br><br><ul><li>  on Docker issues; </li><li>  in source code. </li></ul><br>  And what did I find there?  True: for any user defined network Docker makes modprobe.  We look at the code and see 2 points of interest to us: <br><br><pre> <code class="go hljs"> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> out, err := exec.Command(<span class="hljs-string"><span class="hljs-string">"modprobe"</span></span>, <span class="hljs-string"><span class="hljs-string">"-va"</span></span>, <span class="hljs-string"><span class="hljs-string">"nf_nat"</span></span>).CombinedOutput(); err != <span class="hljs-literal"><span class="hljs-literal">nil</span></span> { logrus.Warnf(<span class="hljs-string"><span class="hljs-string">"Running modprobe nf_nat failed with message: `%s`, error: %v"</span></span>, strings.TrimSpace(<span class="hljs-keyword"><span class="hljs-keyword">string</span></span>(out)), err) } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> out, err := exec.Command(<span class="hljs-string"><span class="hljs-string">"modprobe"</span></span>, <span class="hljs-string"><span class="hljs-string">"-va"</span></span>, <span class="hljs-string"><span class="hljs-string">"xt_conntrack"</span></span>).CombinedOutput(); err != <span class="hljs-literal"><span class="hljs-literal">nil</span></span> { logrus.Warnf(<span class="hljs-string"><span class="hljs-string">"Running modprobe xt_conntrack failed with message: `%s`, error: %v"</span></span>, strings.TrimSpace(<span class="hljs-keyword"><span class="hljs-keyword">string</span></span>(out)), err) }</code> </pre><br>  And here's another: <br><br><pre> <code class="go hljs"> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> err := r.setupIPTable(); err != <span class="hljs-literal"><span class="hljs-literal">nil</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> fmt.Errorf(<span class="hljs-string"><span class="hljs-string">"setting up IP table rules failed: %v"</span></span>, err) }</code> </pre><br>  Making a patch, or rather, throwing out all unnecessary :) Making a new Docker build. <br><br>  We look.  Everything is OK, everything works. <br><br>  At this stage, we can assume that our entire circuit in the laboratory is working, it remains to do just a little - to attach it to our service.  Well, back to the service and look at its overall architecture: <br><br><img src="https://habrastorage.org/files/880/3a6/909/8803a6909fdb4ac2a987088e0d24ba0d.png" alt="image"><br>  Explanation of how it works: <br><br><ul><li>  (1 and 6) the mobile client establishes a connection with a certain URL, followed by a balancer; </li><li>  (2) the balancer selects the required instance of our service and allows you to establish a client-service connection; </li><li>  (3 and 4) then our service proxies requests from the client to the cluster with the code, but also through the balancer in the form of nginx.  This is where we returned to our request that nginx should be on the same machine as the service.  At the moment there is also a limitation in that it should be on the host, and not in the container (this, by the way, would solve the problem immediately).  We will not discuss the reasons for this requirement in this article, but accept it as a condition. </li><li>  (5) each instance of our service has a specific id that the code needs to understand through which instance to respond to the client. </li></ul><br>  In the first approximation, nothing prevents us from collecting an image with our service and launching it already in the container, but there is one BUT.  It so happened that for those services that need interaction with the external balancer, we have a certain set of static routes, for example, like this: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ip r default via 1.1.2.254 dev eth0 10.0.0.0/8 via 1.1.2.1 dev eth0 1.1.2.0/24 dev eth0 proto kernel scope link src 1.1.2.14 192.168.0.0/16 via 1.1.2.1 dev eth0</span></span></code> </pre><br>  Those.  everything that needs to go to or from our internal networks goes through .1, and the rest through .254. <br>  Why is this a problem in our case?  Because when you run the container in its routes, we see the following: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ip r default via 1.1.2.1 dev eth0 1.1.2.0/24 dev eth0 proto kernel scope link src 1.1.2.14</span></span></code> </pre><br>  Attempting to change the routes inside the container will not lead to anything, because  it is not privileged ( <b>--priveleged</b> ).  It remains to change the routes by hand after starting the container from the host (there is a big misconception here, but more on that - further).  Here are two options: <br><br><ul><li>  do it with your hands, using the container namespace; </li><li>  take pipework <a href="https://github.com/jpetazzo/pipework">https://github.com/jpetazzo/pipework</a> and do the same, but with it. </li></ul><br>  I will say right away that you can live with this, but there are dangers like a student: ‚Äúyou can forget, score or drink‚Äù :) <br><br>  Striving for the ideal, we made all routes through default gw for this service network, and the complexity of the routing was shifted to the network department.  Everything.  More precisely, I thought that everything ... <br><br>  It seemed to me at that time - the solution is excellent.  If everything worked exactly as I expected, it would have been so, but this would not have been the end of it.  A little later, it became clear that with such a scheme we get asymmetric routing for those networks that have a route through our LTM.  To make it a little more clear, I will try to show what subnets we can have. <br><br><ol><li>  A network that has only 1 default gw and no external balancer. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/291/96e/33c/29196e33c748455aae7f554db54501b7.png" alt="image"></div><br></li><li>  A network with more than one GW: for example, an external request balancer.  The difficulty is that we do not drive internal traffic through it. <br><div style="text-align:center;"><img src="https://habrastorage.org/files/842/03e/c8e/84203ec8ef3744289e0f5038bf197195.png" alt="image"></div></li></ol><br>  After talking with networkers, we made the following conclusions: <br><br><ul><li>  they are not ready to take responsibility and monitor all networks in which the routing will be exactly like that; </li><li>  we, for our part, are not ready to support static routes for all such networks on servers </li></ul><br>  It turned out that if we wanted to do something simple, we got the problem out of the blue, and if we hadn‚Äôt thought about possible difficulties right away, it would have rather sad consequences. <br><br>  I always say that you should not forget about ideas that came to mind before, but were rejected.  We returned to the idea of ‚Äã‚Äãusing static routes inside the container. <br><br> ,   ,        : <br><br><ul><li>  ; </li><li>  IP  ; </li><li>        ; </li><li>         ( ,      ). </li></ul><br>      ( <b>--privileged</b> )     .      Linux capabilities,        .       <a href="https://docs.docker.com/engine/reference/run/"></a> .       <b>NET_ADMIN</b> . <br><br>    ,        ,   ,  . <br>  ,    Dockerfile     . <br><br> Dockerfile: <br><br><pre> <code class="bash hljs">FROM dockerio.badoo.com/itops/sle_12_base:latest MAINTAINER <span class="hljs-comment"><span class="hljs-comment">#MAINTEINER# RUN /usr/bin/zypper -q -n in iproute2 RUN groupadd -g 1001 wwwaccess RUN mkdir -p /local/SERVICE/{var,conf} COPY get_configs.sh /local/SERVICE/ COPY config.cfg /local/SERVICE/ ADD SERVICE-CERTS/ /local/SERVICE-CERTS/ ADD SERVICE/bin/SERVICE-BINARY-${DVERSION} /local/SERVICE/bin/ ADD SERVICE/conf/ /local/SERVICE/conf/ COPY routes.sh /etc/cont-init.d/00-routes.sh COPY env.sh /etc/cont-init.d/01-env.sh COPY finish.sh /etc/cont-finish.d/00-finish.sh COPY run /etc/services.d/SERVICE/ COPY finish /etc/services.d/SERVICE/ RUN touch /tmp/fresh_container ENTRYPOINT ["/init"]</span></span></code> </pre><br>      : <br><br><ul><li>   s6 overlay  supervisor  ; </li><li>    iproute,     ; </li><li>     ,       ( /etc/cont-init.d/),    ,       ,   ,    (/etc/cont-finish.d/); </li><li>    /tmp/fresh_container  ,  ,        .     ,      ; </li></ul><br>  : <br><br><ol><li> <strong>get_configs.sh</strong> ‚Äî  ,  ,            ,    ,   ,     ,     .       <a href="https://tech.badoo.com/ru/presentation/193/monitor-avtomatiziruj-docker/">Docker Meetup</a> ; <br><br></li><li> <strong>routes.sh</strong> ‚Äî ,     : <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/with-contenv sh if [ ! -x /usr/sbin/ip ];then echo -e "\e[31mCan't execute /usr/sbin/ip\e[0m"; [ $(pgrep s6-svscan) ] &amp;&amp; s6-svscanctl -t /var/run/s6/services exit 1; else LTMGW=$(/usr/sbin/ip r show | /usr/bin/grep default | /usr/bin/awk {'print $3'} | /usr/bin/awk -F \. {'print $1"."$2"."$3".254"'}) DEFGW=$(/usr/sbin/ip r show | /usr/bin/grep default | /usr/bin/awk {'print $3'} | /usr/bin/awk -F \. {'print $1"."$2"."$3".1"'}) /usr/sbin/ip r replace default via ${LTMGW} /usr/sbin/ip r add 192.168.0.0/16 via 10.10.8.1 dev eth0 /usr/sbin/ip r add 10.0.0.0/8 via 10.10.8.1 dev eth0 echo -e "\e[32mAll job with routes done:\e[0m\n$(/usr/sbin/ip r show)" fi</span></span></code> </pre><br></li><li> <strong>env.sh</strong> ‚Äî ,      ;      1     : <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/with-contenv sh if [ ! -z "${ISTEST}" ];then exit 0;fi if [ ! -n "${SERVICETYPE}" ];then echo -e "\e[31mPlease set SERVICE type\e[0m"; [ $(pgrep s6-svscan) ] &amp;&amp; s6-svscanctl -t /var/run/s6/services exit 1; fi bash /local/SERVICE/get_configs.sh || exit 1 echo -e "\e[32mSERVICE ${SERVICETYPE} is running\e[0m"</span></span></code> </pre><br></li><li> <strong>finish.sh</strong> ‚Äî ,    pid-   .     (  ),     ,    ,    pid- :) <br><br></li><li> <strong>run</strong> ‚Äî  ,    : <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/with-contenv bash exec /local/SERVICE/bin/SERVICE-${DVERSION} -l /local/SERVICE/var/mobile-${SERVICETYPE}.log -P /local/SERVICE/var/mobile-${SERVICETYPE}.pid -c /local/SERVICE/conf/SERVICE.conf -v ${VERBOSITY}</span></span></code> </pre><br></li><li> <strong>finish</strong> ‚Äî ,     ,     : <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh [ $(pgrep s6-svscan) ] &amp;&amp; s6-svscanctl -t /var/run/s6/services</span></span></code> </pre> <br></li></ol><br>        : <br><br><pre> <code class="bash hljs">docker run -d --net=c_services --ip=1.1.2.17 --name=SERVICE-INSTANCE16 -h SERVICE-INSTANCE16.local --<span class="hljs-built_in"><span class="hljs-built_in">cap</span></span>-add=NET_ADMIN --add-host=<span class="hljs-string"><span class="hljs-string">'nginx.localhost:1.1.1.17'</span></span> -e SERVICETYPE=INSTANCE16_eu1 -e HOST_IP=1.1.1.17 --volumes-from=badoo_loop dockerio.badoo.com/cteam/SERVICE:2.30.0_994</code> </pre><br>          .  ,  ,  MACVLAN/IPVLAN      ,      . <br><br>  <a href="https://habrahabr.ru/users/banuchka/" class="user_link">banuchka</a>  <br> Site Reliability Engineer, Badoo </cut></div><p>Source: <a href="https://habr.com/ru/post/308402/">https://habr.com/ru/post/308402/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../308388/index.html">How to keep healthy relationships by being a developer</a></li>
<li><a href="../308392/index.html">Back to basics or a step forward? HPE StoreVirtual VSA</a></li>
<li><a href="../308394/index.html">Programs become more important than iron</a></li>
<li><a href="../308398/index.html">How we wrote a chat for the application of the bank "Opening"</a></li>
<li><a href="../308400/index.html">Step-by-step instructions for setting up LXD on Ubuntu 16.04</a></li>
<li><a href="../308406/index.html">New course on Stepic platform: "Advanced C / C ++ Programming"</a></li>
<li><a href="../308410/index.html">JavaScript Performance, databases and searches for the "silver bullet": video TOP-5 reports of HolyJS 2016</a></li>
<li><a href="../308412/index.html">SAP F & R: Order Creation Process. Part 1</a></li>
<li><a href="../308418/index.html">We work with a hybrid cloud: VMware vCloud Connector, part 2</a></li>
<li><a href="../308420/index.html">Ways of integration with 1C</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>