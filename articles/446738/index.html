<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The basics of natural language processing for text</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Natural language processing is currently not used unless in very conservative industries. In most technological solutions, the recognition and process...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The basics of natural language processing for text</h1><div class="post__text post__text-html js-mediator-article">  Natural language processing is currently not used unless in very conservative industries.  In most technological solutions, the recognition and processing of ‚Äúhuman‚Äù languages ‚Äã‚Äãhas long been implemented: that is why the usual IVR with hard-coded response options is gradually becoming a thing of the past, chatbots begin to communicate more adequately without a live operator, filters in the mail work with a bang, etc.  How does the recognition of recorded speech, that is, text?  Or rather, ask what is the basis of modern recognition and processing techniques?  Our today's adapted translation responds well to this - under the cat you will find a longrid that closes the gaps in the basics of NLP.  Enjoy reading! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nw/vz/qn/nwvzqnbpjgc_ndxnjt7eixdynro.jpeg"></div><br><a name="habracut"></a><br><h2>  What is Natural Language Processing? </h2><br>  Natural Language Processing (hereinafter - NLP) - natural language processing is a subsection of computer science and AI, dedicated to how computers analyze natural (human) languages.  NLP allows the use of machine learning algorithms for text and speech. <br><br>  For example, we can use NLP to create systems like speech recognition, document summarization, machine translation, spam detection, named entity recognition, question answers, autocomplete, predictive text input, etc. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Today, many of us have speech recognition smartphones ‚Äî they use NLP to understand our speech.  Also, many people use laptops with speech recognition built into the OS. <br><br><h2>  Examples </h2><br><h3>  Cortana </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ui/aa/hy/uiaahycfbatakz2q9dl0fqfhr-y.png"></div><br><br>  On Windows, there is a virtual assistant, Cortana, that recognizes speech.  With Cortana, you can create reminders, open applications, send letters, play games, check the weather, etc. <br><br><h3>  Siri </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ew/h7/9p/ewh79pl_rjkufl6seqyih-c_u4c.jpeg"></div><br>  Siri is an assistant for Apple's OS: iOS, watchOS, macOS, HomePod and tvOS.  Many functions also work through voice control: call / write to someone, send a letter, set a timer, take a photo, etc. <br><br><h3>  Gmail </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ec/rw/ii/ecrwii6nml6c6uvxn8vensihku0.gif"></div><br><br>  A well-known mail service can detect spam so that it does not get into your inbox. <br><br><h3>  Dialogflow </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mq/vg/iy/mqvgiyi8zv1dwfvwsqj6lvcfzsm.png"></div><br>  A platform from Google that allows you to create NLP bots.  For example, you can make a bot to order a pizza <a href="https://habr.com/ru/company/Voximplant/blog/412663/">that does not need an old-fashioned IVR to take your order</a> . <br><br><hr><br><h2>  NLTK Python Library </h2><br>  NLTK (Natural Language Toolkit) is the leading platform for creating NLP programs in Python.  It has easy-to-use interfaces for many <a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25BE%25D1%2580%25D0%25BF%25D1%2583%25D1%2581_%25D1%2582%25D0%25B5%25D0%25BA%25D1%2581%25D1%2582%25D0%25BE%25D0%25B2">language cases</a> , as well as word processing libraries for classifying, tokenizing, <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D1%2582%25D0%25B5%25D0%25BC%25D0%25BC%25D0%25B8%25D0%25BD%25D0%25B3">stemming</a> , <a href="https://ru.wikipedia.org/wiki/%25D0%25A7%25D0%25B0%25D1%2581%25D1%2582%25D0%25B5%25D1%2580%25D0%25B5%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2580%25D0%25B0%25D0%25B7%25D0%25BC%25D0%25B5%25D1%2582%25D0%25BA%25D0%25B0">marking</a> , filtering, and <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B5%25D0%25BC%25D0%25B0%25D0%25BD%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B8%25D0%25B9_%25D0%25BC%25D0%25B5%25D1%2585%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B7%25D0%25BC_%25D1%2580%25D0%25B0%25D1%2581%25D1%2581%25D1%2583%25D0%25B6%25D0%25B4%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B9">semantic reasoning</a> .  Well, this is also a free open-source project that is being developed with the help of the community. <br>  We will use this tool to show the basics of NLP.  For all the following examples, I assume that the NLTK has already been imported;  You can do this with the <code>import nltk</code> <br><br><h2>  Basics of NLP for text </h2><br>  In this article we will cover topics: <br><br><ol><li>  Tokenization by suggestions. </li><li>  Tokenization by words. </li><li>  <a href="https://ru.wikipedia.org/wiki/%25D0%259B%25D0%25B5%25D0%25BC%25D0%25BC%25D0%25B0%25D1%2582%25D0%25B8%25D0%25B7%25D0%25B0%25D1%2586%25D0%25B8%25D1%258F">Lemmatization</a> and <a href="https://ru.wikipedia.org/wiki/%25D0%259B%25D0%25B5%25D0%25BC%25D0%25BC%25D0%25B0%25D1%2582%25D0%25B8%25D0%25B7%25D0%25B0%25D1%2586%25D0%25B8%25D1%258F">stemming of the</a> text. </li><li>  Stop words. </li><li>  Regular expressions. </li><li>  <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">Bag of words</a> . </li><li>  <a href="https://ru.wikipedia.org/wiki/TF-IDF">TF-IDF</a> . </li></ol><br><h3>  1. Tokenization by suggestions </h3><br>  Tokenization (sometimes segmentation) by sentences is the process of splitting the written language into component sentences.  The idea looks pretty simple.  In English and some other languages, we can isolate a sentence every time we find a certain punctuation mark - a full stop. <br><br>  But even in English, this task is not trivial, since the dot is also used in abbreviations.  The abbreviation table can help a lot during word processing to avoid misallocating sentences.  In most cases, libraries are used for this, so you may not particularly worry about implementation details. <br><br>  <b>Example:</b> <br><br>  Take a small text about the backgammon board game: <br><br><pre> <code class="plaintext hljs">Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.</code> </pre> <br>  To make offers tokenization using NLTK, you can use the <code>nltk.sent_tokenize</code> method <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/39237759c087ac4151b3c06d4e566747.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-a9a1cf2ca01efd362bfa52312712ae94.css"><div id="gist92859547" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-sentence-tokenization-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-sentence-tokenization-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-sentence-tokenization-py-LC1" class="blob-code blob-code-inner js-file-line">text <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.<span class="pl-pds">"</span></span></td>
      </tr>
      <tr>
        <td id="file-sentence-tokenization-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-sentence-tokenization-py-LC2" class="blob-code blob-code-inner js-file-line">sentences <span class="pl-k">=</span> nltk.sent_tokenize(text)</td>
      </tr>
      <tr>
        <td id="file-sentence-tokenization-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-sentence-tokenization-py-LC3" class="blob-code blob-code-inner js-file-line"><span class="pl-k">for</span> sentence <span class="pl-k">in</span> sentences:</td>
      </tr>
      <tr>
        <td id="file-sentence-tokenization-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-sentence-tokenization-py-LC4" class="blob-code blob-code-inner js-file-line">    <span class="pl-c1">print</span>(sentence)</td>
      </tr>
      <tr>
        <td id="file-sentence-tokenization-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-sentence-tokenization-py-LC5" class="blob-code blob-code-inner js-file-line">    <span class="pl-c1">print</span>()</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">sentence tokenization.py</a>
        hosted with ‚ù§ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  At the output we get 3 separate sentences: <br><br><pre> <code class="plaintext hljs">Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.</code> </pre> <br><h3>  2. Tokenization by words </h3><br>  Tokenization (sometimes segmentation) by words is the process of dividing sentences into word-components.  In English and many other languages ‚Äã‚Äãthat use one or another version of the Latin alphabet, a space is a good word delimiter. <br><br>  However, problems may arise if we use only a space ‚Äî in English, compound nouns are written differently and sometimes with a space.  And here again the libraries help us. <br><br>  <b>Example:</b> <br><br>  Let's take the sentences from the previous example and apply the <code>nltk.word_tokenize</code> method to them <code>nltk.word_tokenize</code> <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/7befd293c570afd70158e954270fc98d.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-a9a1cf2ca01efd362bfa52312712ae94.css"><div id="gist92859647" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-word-tokenization-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-word-tokenization-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-word-tokenization-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-k">for</span> sentence <span class="pl-k">in</span> sentences:</td>
      </tr>
      <tr>
        <td id="file-word-tokenization-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-word-tokenization-py-LC2" class="blob-code blob-code-inner js-file-line">    words <span class="pl-k">=</span> nltk.word_tokenize(sentence)</td>
      </tr>
      <tr>
        <td id="file-word-tokenization-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-word-tokenization-py-LC3" class="blob-code blob-code-inner js-file-line">    <span class="pl-c1">print</span>(words)</td>
      </tr>
      <tr>
        <td id="file-word-tokenization-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-word-tokenization-py-LC4" class="blob-code blob-code-inner js-file-line">    <span class="pl-c1">print</span>()</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">word tokenization.py</a>
        hosted with ‚ù§ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Conclusion: <br><br><pre> <code class="plaintext hljs">['Backgammon', 'is', 'one', 'of', 'the', 'oldest', 'known', 'board', 'games', '.'] ['Its', 'history', 'can', 'be', 'traced', 'back', 'nearly', '5,000', 'years', 'to', 'archeological', 'discoveries', 'in', 'the', 'Middle', 'East', '.'] ['It', 'is', 'a', 'two', 'player', 'game', 'where', 'each', 'player', 'has', 'fifteen', 'checkers', 'which', 'move', 'between', 'twenty-four', 'points', 'according', 'to', 'the', 'roll', 'of', 'two', 'dice', '.']</code> </pre> <br><h3>  3. Lemmatization and stemming of text </h3><br>  Typically, the texts contain different grammatical forms of the same word, and the same root words may also occur.  Lemmatization and stemming aim to bring all occurring word forms to one, normal vocabulary form. <br><br>  <b>Examples:</b> <br><br>  Reduction of different word forms to one: <br><br><pre> <code class="plaintext hljs">dog, dogs, dog's, dogs' =&gt; dog</code> </pre> <br>  The same, but already applied to the whole sentence: <br><br><pre> <code class="plaintext hljs">the boy's dogs are different sizes =&gt; the boy dog be differ size</code> </pre> <br>  Lemmatization and stemming are special cases of normalization and they differ. <br><br>  Stemming is a rough heuristic process that cuts off the ‚Äúextra‚Äù from the root of words, often this leads to the loss of word-building suffixes. <br><br>  Lemmatization is a more subtle process that uses vocabulary and morphological analysis to eventually bring the word to its canonical form - the lemma. <br><br>  The difference is that a stemmer (a specific implementation of the algorithm of stemming - comment of the translator) operates without a knowledge of the context and, accordingly, does not understand the difference between words that have different meanings depending on the part of speech.  However, stimmers have their advantages: they are easier to implement and they work faster.  Plus, lower ‚Äúaccuracy‚Äù may not matter in some cases. <br><br>  <b>Examples:</b> <br><br><ol><li>  The word good is a lemma for the word better.  Stemmer will not see this connection, because here you need to check with the dictionary. </li><li>  The word play is the basic form of the word playing.  Both stemming and lemmatization can do this. </li><li>  The word meeting can be both the normal form of the noun and the verb form to meet, depending on the context.  Unlike stemming, lemmatization will try to choose the right lemma based on context. </li></ol><br>  Now that we know the difference, let's look at an example: <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/8c69db03e92337c9bc9a612361c9bcfb.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-a9a1cf2ca01efd362bfa52312712ae94.css"><div id="gist92909693" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-stemming-vs-lemmatization-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-stemming-vs-lemmatization-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-stemming-vs-lemmatization-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-k">from</span> nltk.stem <span class="pl-k">import</span> PorterStemmer, WordNetLemmatizer</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-stemming-vs-lemmatization-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-k">from</span> nltk.corpus <span class="pl-k">import</span> wordnet</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-stemming-vs-lemmatization-py-LC3" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-stemming-vs-lemmatization-py-LC4" class="blob-code blob-code-inner js-file-line"><span class="pl-k">def</span> <span class="pl-en">compare_stemmer_and_lemmatizer</span>(<span class="pl-smi">stemmer</span>, <span class="pl-smi">lemmatizer</span>, <span class="pl-smi">word</span>, <span class="pl-smi">pos</span>):</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-stemming-vs-lemmatization-py-LC5" class="blob-code blob-code-inner js-file-line">    <span class="pl-s"><span class="pl-pds">"""</span></span></td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L6" class="blob-num js-line-number" data-line-number="6"></td>
        <td id="file-stemming-vs-lemmatization-py-LC6" class="blob-code blob-code-inner js-file-line"><span class="pl-s">    Print the results of stemmind and lemmitization using the passed stemmer, lemmatizer, word and pos (part of speech)</span></td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L7" class="blob-num js-line-number" data-line-number="7"></td>
        <td id="file-stemming-vs-lemmatization-py-LC7" class="blob-code blob-code-inner js-file-line"><span class="pl-s">    <span class="pl-pds">"""</span></span></td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L8" class="blob-num js-line-number" data-line-number="8"></td>
        <td id="file-stemming-vs-lemmatization-py-LC8" class="blob-code blob-code-inner js-file-line">    <span class="pl-c1">print</span>(<span class="pl-s"><span class="pl-pds">"</span>Stemmer:<span class="pl-pds">"</span></span>, stemmer.stem(word))</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L9" class="blob-num js-line-number" data-line-number="9"></td>
        <td id="file-stemming-vs-lemmatization-py-LC9" class="blob-code blob-code-inner js-file-line">    <span class="pl-c1">print</span>(<span class="pl-s"><span class="pl-pds">"</span>Lemmatizer:<span class="pl-pds">"</span></span>, lemmatizer.lemmatize(word, pos))</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L10" class="blob-num js-line-number" data-line-number="10"></td>
        <td id="file-stemming-vs-lemmatization-py-LC10" class="blob-code blob-code-inner js-file-line">    <span class="pl-c1">print</span>()</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L11" class="blob-num js-line-number" data-line-number="11"></td>
        <td id="file-stemming-vs-lemmatization-py-LC11" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L12" class="blob-num js-line-number" data-line-number="12"></td>
        <td id="file-stemming-vs-lemmatization-py-LC12" class="blob-code blob-code-inner js-file-line">lemmatizer <span class="pl-k">=</span> WordNetLemmatizer()</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L13" class="blob-num js-line-number" data-line-number="13"></td>
        <td id="file-stemming-vs-lemmatization-py-LC13" class="blob-code blob-code-inner js-file-line">stemmer <span class="pl-k">=</span> PorterStemmer()</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L14" class="blob-num js-line-number" data-line-number="14"></td>
        <td id="file-stemming-vs-lemmatization-py-LC14" class="blob-code blob-code-inner js-file-line">compare_stemmer_and_lemmatizer(stemmer, lemmatizer, <span class="pl-v">word</span> <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>seen<span class="pl-pds">"</span></span>, <span class="pl-v">pos</span> <span class="pl-k">=</span> wordnet.<span class="pl-c1">VERB</span>)</td>
      </tr>
      <tr>
        <td id="file-stemming-vs-lemmatization-py-L15" class="blob-num js-line-number" data-line-number="15"></td>
        <td id="file-stemming-vs-lemmatization-py-LC15" class="blob-code blob-code-inner js-file-line">compare_stemmer_and_lemmatizer(stemmer, lemmatizer, <span class="pl-v">word</span> <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>drove<span class="pl-pds">"</span></span>, <span class="pl-v">pos</span> <span class="pl-k">=</span> wordnet.<span class="pl-c1">VERB</span>)</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">stemming vs lemmatization.py</a>
        hosted with ‚ù§ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Conclusion: <br><br><pre> <code class="plaintext hljs">Stemmer: seen Lemmatizer: see Stemmer: drove Lemmatizer: drive</code> </pre> <br><h3>  4. Stop words </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xr/fq/ju/xrfqju0nbugayjd8cnkvlcbiuwa.png"></div><br><br>  Stop words are words that are discarded from the text before / after text processing.  When we apply machine learning to texts, such words can add a lot of noise, so it‚Äôs necessary to get rid of irrelevant words. <br><br>  Stop words are usually understood by articles, interjections, unions, etc., which do not carry meaning.  It should be understood that there is no universal list of stop words, it all depends on the specific case. <br><br>  NLTK has a predefined list of stop words.  Before first use, you will need to download it: <code>nltk.download(‚Äústopwords‚Äù)</code> .  After downloading, you can import the <code>stopwords</code> package and look at the words themselves: <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/7d2e8f81219656f6d2e82933c6994cfe.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-a9a1cf2ca01efd362bfa52312712ae94.css"><div id="gist92916250" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-stop-words-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-stop-words-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-stop-words-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-k">from</span> nltk.corpus <span class="pl-k">import</span> stopwords</td>
      </tr>
      <tr>
        <td id="file-stop-words-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-stop-words-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-c1">print</span>(stopwords.words(<span class="pl-s"><span class="pl-pds">"</span>english<span class="pl-pds">"</span></span>))</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">stop words.py</a>
        hosted with ‚ù§ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Conclusion: <br><br><pre> <code class="plaintext hljs">['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]</code> </pre> <br>  Consider how you can remove the stop words from the sentence: <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/b1c69457cc0d8eab7b3661533725485a.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-a9a1cf2ca01efd362bfa52312712ae94.css"><div id="gist92916498" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-stop-words-example-1-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-stop-words-example-1-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-stop-words-example-1-py-LC1" class="blob-code blob-code-inner js-file-line">stop_words <span class="pl-k">=</span> <span class="pl-c1">set</span>(stopwords.words(<span class="pl-s"><span class="pl-pds">"</span>english<span class="pl-pds">"</span></span>))</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-1-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-stop-words-example-1-py-LC2" class="blob-code blob-code-inner js-file-line">sentence <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Backgammon is one of the oldest known board games.<span class="pl-pds">"</span></span></td>
      </tr>
      <tr>
        <td id="file-stop-words-example-1-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-stop-words-example-1-py-LC3" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-1-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-stop-words-example-1-py-LC4" class="blob-code blob-code-inner js-file-line">words <span class="pl-k">=</span> nltk.word_tokenize(sentence)</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-1-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-stop-words-example-1-py-LC5" class="blob-code blob-code-inner js-file-line">without_stop_words <span class="pl-k">=</span> [word <span class="pl-k">for</span> word <span class="pl-k">in</span> words <span class="pl-k">if</span> <span class="pl-k">not</span> word <span class="pl-k">in</span> stop_words]</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-1-py-L6" class="blob-num js-line-number" data-line-number="6"></td>
        <td id="file-stop-words-example-1-py-LC6" class="blob-code blob-code-inner js-file-line"><span class="pl-c1">print</span>(without_stop_words)</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">stop words example 1.py</a>
        hosted with ‚ù§ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Conclusion: <br><br><pre> <code class="plaintext hljs">['Backgammon', 'one', 'oldest', 'known', 'board', 'games', '.']</code> </pre> <br>  If you are not familiar with list comprehensions, you can learn more <a href="https://towardsdatascience.com/python-basics-list-comprehensions-631278f22c40">here</a> .  Here is another way to achieve the same result: <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/bbfab6573e886bd122aba972048d54cb.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-a9a1cf2ca01efd362bfa52312712ae94.css"><div id="gist92916520" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-stop-words-example-2-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-stop-words-example-2-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-stop-words-example-2-py-LC1" class="blob-code blob-code-inner js-file-line">stop_words <span class="pl-k">=</span> <span class="pl-c1">set</span>(stopwords.words(<span class="pl-s"><span class="pl-pds">"</span>english<span class="pl-pds">"</span></span>))</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-stop-words-example-2-py-LC2" class="blob-code blob-code-inner js-file-line">sentence <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Backgammon is one of the oldest known board games.<span class="pl-pds">"</span></span></td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-stop-words-example-2-py-LC3" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-stop-words-example-2-py-LC4" class="blob-code blob-code-inner js-file-line">words <span class="pl-k">=</span> nltk.word_tokenize(sentence)</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-stop-words-example-2-py-LC5" class="blob-code blob-code-inner js-file-line">without_stop_words <span class="pl-k">=</span> []</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L6" class="blob-num js-line-number" data-line-number="6"></td>
        <td id="file-stop-words-example-2-py-LC6" class="blob-code blob-code-inner js-file-line"><span class="pl-k">for</span> word <span class="pl-k">in</span> words:</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L7" class="blob-num js-line-number" data-line-number="7"></td>
        <td id="file-stop-words-example-2-py-LC7" class="blob-code blob-code-inner js-file-line">    <span class="pl-k">if</span> word <span class="pl-k">not</span> <span class="pl-k">in</span> stop_words:</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L8" class="blob-num js-line-number" data-line-number="8"></td>
        <td id="file-stop-words-example-2-py-LC8" class="blob-code blob-code-inner js-file-line">        without_stop_words.append(word)</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L9" class="blob-num js-line-number" data-line-number="9"></td>
        <td id="file-stop-words-example-2-py-LC9" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-stop-words-example-2-py-L10" class="blob-num js-line-number" data-line-number="10"></td>
        <td id="file-stop-words-example-2-py-LC10" class="blob-code blob-code-inner js-file-line"><span class="pl-c1">print</span>(without_stop_words)</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">stop words example 2.py</a>
        hosted with ‚ù§ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  However, remember that list comprehensions are faster because they are optimized - the interpreter identifies the predictive pattern during the loop. <br><br>  You may ask why we converted the list into a <a href="https://docs.python.org/3/tutorial/datastructures.html">multitude</a> .  A set is an abstract data type that can store unique values ‚Äã‚Äãin an undefined order.  Search by set is much faster than search by list.  For a small number of words it does not matter, but if we are talking about a large number of words, then it is strongly recommended to use sets.  If you want to learn a little more about the time for performing various operations, look at <a href="http://bigocheatsheet.com/">this wonderful cheat sheet</a> . <br><br><h3>  5. Regular expressions. </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hn/mm/jx/hnmmjxjubpvt7p1uc-lv-t-auhi.jpeg"></div><br>  A regular expression (regular, regexp, regex) is a sequence of characters that defines a search pattern.  For example: <br><br><ul><li>  .  - any character except newline; </li><li>  \ w - one word; </li><li>  \ d is one digit; </li><li>  \ s - one space; </li><li>  \ W - one is not a word; </li><li>  \ D is one Netsifra; </li><li>  \ S - one bad; </li><li>  [abc] - finds any of the specified characters match any of a, b, or c; </li><li>  [^ abc] - finds any character except the specified ones; </li><li>  [ag] - finds a character in the range from a to g. </li></ul><br>  Excerpt from the <a href="https://docs.python.org/3/library/re.html%3Fhighlight%3Dregex">Python documentation</a> : <br><blockquote>  A regular expression uses a backslash <code>(\)</code> to indicate special forms or to allow the use of special characters.  This is contrary to the use of the backslash in Python: for example, to literally mark a backslash, you need to write <code>'\\\\'</code> as a pattern for the search, because the regular expression should look like <code>\\</code> , where each backslash should be escaped. <br><br>  The solution is to use raw string notation for search patterns;  backslashes will not be processed in a special way if used with the <code>'r'</code> prefix.  Thus, <code>r‚Äù\n‚Äù</code> is a string with two characters <code>('\'  'n')</code> , and <code>‚Äú\n‚Äù</code> is a string with one character (newline). <br></blockquote>  We can use regulars to further filter our text.  For example, you can remove all characters that are not words.  In many cases, punctuation is not needed and is easy to remove with the help of regulars. <br><br>  The <b>re</b> module in Python represents regular expression operations.  We can use the <b>re.sub</b> function to replace everything that matches the search pattern with the specified string.  This is how you can replace all non-words with spaces: <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/a9a29588061a8c9bb8aeb28140a69f89.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-a9a1cf2ca01efd362bfa52312712ae94.css"><div id="gist92925716" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-regex-substitute-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-regex-substitute-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-regex-substitute-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-k">import</span> re</td>
      </tr>
      <tr>
        <td id="file-regex-substitute-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-regex-substitute-py-LC2" class="blob-code blob-code-inner js-file-line">sentence <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>The development of snowboarding was inspired by skateboarding, sledding, surfing and skiing.<span class="pl-pds">"</span></span></td>
      </tr>
      <tr>
        <td id="file-regex-substitute-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-regex-substitute-py-LC3" class="blob-code blob-code-inner js-file-line">pattern <span class="pl-k">=</span> <span class="pl-sr"><span class="pl-k">r</span><span class="pl-pds">"</span>[<span class="pl-k">^</span><span class="pl-c1">\w</span>]<span class="pl-pds">"</span></span></td>
      </tr>
      <tr>
        <td id="file-regex-substitute-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-regex-substitute-py-LC4" class="blob-code blob-code-inner js-file-line"><span class="pl-c1">print</span>(re.sub(pattern, <span class="pl-s"><span class="pl-pds">"</span> <span class="pl-pds">"</span></span>, sentence))</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">regex substitute.py</a>
        hosted with ‚ù§ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Conclusion: <br><br><pre> <code class="plaintext hljs">'The development of snowboarding was inspired by skateboarding sledding surfing and skiing '</code> </pre> <br>  Regulars are a powerful tool, with its help you can create much more complex patterns.  If you want to learn more about regular expressions, then I can recommend these 2 web applications: <a href="https://regexr.com/">regex</a> , <a href="https://regex101.com/">regex101</a> . <br><br><h3>  6. A bag of words </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/oh/va/sp/ohvaspwwyxr6mjgfz1w4vq8znku.png"></div><br>  Machine learning algorithms cannot directly work with raw text, therefore it is necessary to convert text into sets of numbers (vectors).  This is called <a href="https://ru.wikipedia.org/wiki/%25D0%2592%25D1%258B%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%25BF%25D1%2580%25D0%25B8%25D0%25B7%25D0%25BD%25D0%25B0%25D0%25BA%25D0%25BE%25D0%25B2">feature extraction</a> . <br><br>  A word bag is a popular and simple feature extraction technique used when working with text.  It describes the occurrences of each word in the text. <br><br>  To use the model, we need: <br><br><ol><li>  Define a dictionary of known words (tokens). </li><li>  Select the degree of presence of famous words. </li></ol><br>  Any information on the order or structure of words is ignored.  That is why it is called a BAG of words.  This model tries to understand whether a familiar word occurs in a document, but does not know exactly where it occurs. <br><br>  Intuition suggests that <b>similar documents</b> have <b>similar content</b> .  Also, thanks to the content, we can learn something about the meaning of the document. <br><br>  <b>Example:</b> <br>  Consider the steps of creating this model.  We use only 4 sentences to understand how the model works.  In real life you will encounter large amounts of data. <br><br><h4>  1. Load the data </h4><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tr/xz/w9/trxzw9m1s7psallg0ulf6wepnsu.png"></div><br>  Imagine that this is our data and we want to load it as an array: <br><br><pre> <code class="plaintext hljs">I like this movie, it's funny. I hate this movie. This was awesome! I like it. Nice one. I love it.</code> </pre> <br>  To do this, it is enough to read the file and divide by rows: <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/15abcc02fefb2782ba78ac695d4dda59.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-a9a1cf2ca01efd362bfa52312712ae94.css"><div id="gist93035402" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-read-the-movie-reviews-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-read-the-movie-reviews-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-read-the-movie-reviews-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-k">with</span> <span class="pl-c1">open</span>(<span class="pl-s"><span class="pl-pds">"</span>simple movie reviews.txt<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>r<span class="pl-pds">"</span></span>) <span class="pl-k">as</span> <span class="pl-v">file</span>:</td>
      </tr>
      <tr>
        <td id="file-read-the-movie-reviews-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-read-the-movie-reviews-py-LC2" class="blob-code blob-code-inner js-file-line">    documents <span class="pl-k">=</span> <span class="pl-v">file</span>.read().splitlines()</td>
      </tr>
      <tr>
        <td id="file-read-the-movie-reviews-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-read-the-movie-reviews-py-LC3" class="blob-code blob-code-inner js-file-line">    </td>
      </tr>
      <tr>
        <td id="file-read-the-movie-reviews-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-read-the-movie-reviews-py-LC4" class="blob-code blob-code-inner js-file-line"><span class="pl-c1">print</span>(documents)</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">read the movie reviews.py</a>
        hosted with ‚ù§ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Conclusion: <br><br><pre> <code class="plaintext hljs">["I like this movie, it's funny.", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']</code> </pre> <br><br><h4>  2. Define a dictionary </h4><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lk/vg/7p/lkvg7pwbgfcd130zn66zx_qhjxq.png"></div><br>  We collect all unique words from 4 loaded sentences, ignoring case, punctuation and single-character tokens.  This will be our dictionary (famous words). <br><br>  To create a dictionary, you can use the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">CountVectorizer</a> class from the sklearn library.  Go to the next step. <br><br><h4>  3. Create document vectors </h4><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/4q/jj/hh/4qjjhhlyqga--r6eh9kkery5jsy.png"></div><br>  Next, we need to evaluate the words in the document.  At this step, our goal is to turn raw text into a set of numbers.  After that, we use these sets as input to a machine learning model.  The simplest method of scoring is to mark the presence of words, that is, to put 1 if there is a word and 0 in its absence. <br><br>  Now we can create a bag of words using the aforementioned class CountVectorizer. <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/155e97ad22862a340d941a63e43295d9.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-a9a1cf2ca01efd362bfa52312712ae94.css"><div id="gist93036006" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-simple-bag-of-words-example-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-simple-bag-of-words-example-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-simple-bag-of-words-example-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> Import the libraries we need</span></td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-simple-bag-of-words-example-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-k">from</span> sklearn.feature_extraction.text <span class="pl-k">import</span> CountVectorizer</td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-simple-bag-of-words-example-py-LC3" class="blob-code blob-code-inner js-file-line"><span class="pl-k">import</span> pandas <span class="pl-k">as</span> pd</td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-simple-bag-of-words-example-py-LC4" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-simple-bag-of-words-example-py-LC5" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> Step 2. Design the Vocabulary</span></td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L6" class="blob-num js-line-number" data-line-number="6"></td>
        <td id="file-simple-bag-of-words-example-py-LC6" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> The default token pattern removes tokens of a single character. That's why we don't have the "I" and "s" tokens in the output</span></td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L7" class="blob-num js-line-number" data-line-number="7"></td>
        <td id="file-simple-bag-of-words-example-py-LC7" class="blob-code blob-code-inner js-file-line">count_vectorizer <span class="pl-k">=</span> CountVectorizer()</td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L8" class="blob-num js-line-number" data-line-number="8"></td>
        <td id="file-simple-bag-of-words-example-py-LC8" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L9" class="blob-num js-line-number" data-line-number="9"></td>
        <td id="file-simple-bag-of-words-example-py-LC9" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> Step 3. Create the Bag-of-Words Model</span></td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L10" class="blob-num js-line-number" data-line-number="10"></td>
        <td id="file-simple-bag-of-words-example-py-LC10" class="blob-code blob-code-inner js-file-line">bag_of_words <span class="pl-k">=</span> count_vectorizer.fit_transform(documents)</td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L11" class="blob-num js-line-number" data-line-number="11"></td>
        <td id="file-simple-bag-of-words-example-py-LC11" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L12" class="blob-num js-line-number" data-line-number="12"></td>
        <td id="file-simple-bag-of-words-example-py-LC12" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> Show the Bag-of-Words Model as a pandas DataFrame</span></td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L13" class="blob-num js-line-number" data-line-number="13"></td>
        <td id="file-simple-bag-of-words-example-py-LC13" class="blob-code blob-code-inner js-file-line">feature_names <span class="pl-k">=</span> count_vectorizer.get_feature_names()</td>
      </tr>
      <tr>
        <td id="file-simple-bag-of-words-example-py-L14" class="blob-num js-line-number" data-line-number="14"></td>
        <td id="file-simple-bag-of-words-example-py-LC14" class="blob-code blob-code-inner js-file-line">pd.DataFrame(bag_of_words.toarray(), <span class="pl-v">columns</span> <span class="pl-k">=</span> feature_names)</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">simple bag-of-words example.py</a>
        hosted with ‚ù§ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Conclusion: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ee/pf/-g/eepf-gw0it8d6a_fwcoew7bwbns.png"></div><br>  These are our suggestions.  Now we see how the word bag model works. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/l3/ep/95/l3ep95r4s_rdnvjfmfy7ebcwuag.png"></div><br><h3>  Just a few words about the bag of words </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/k4/mo/s1/k4mos1faome-c00hjo84v7nt7no.png"></div><br>  The complexity of this model is how to define a dictionary and how to count the occurrence of words. <br><br>  When the size of the dictionary increases, the vector of the document also grows.  In the example above, the length of the vector is equal to the number of known words. <br><br>  In some cases, we can have an incredibly large amount of data and then the vector can consist of thousands or millions of elements.  Moreover, each document can contain only a small part of the words from the dictionary. <br><br>  As a result, there will be a lot of zeros in the vector representation.  Vectors with a large number of zeros are called sparse vectors, they require more memory and computational resources. <br><br>  However, we can reduce the number of known words when we use this model to reduce the requirements for computing resources.  To do this, you can use the same techniques that we have already considered before creating a bag of words: <br><br><ul><li>  ignore word case; </li><li>  ignoring punctuation; </li><li>  discarding stop words; </li><li>  reduction of words to their basic forms (lemmatization and stemming); </li><li>  correction of incorrectly written words. </li></ul><br>  Another more complex way to create a dictionary is to use grouped words.  This will change the size of the dictionary and give the bag more words about the document.  This approach is called the " <a href="https://ru.wikipedia.org/wiki/N-%25D0%25B3%25D1%2580%25D0%25B0%25D0%25BC%25D0%25BC%25D0%25B0">N-gram</a> ." <br><br>  An n-gram is a sequence of any entities (words, letters, numbers, numbers, etc.).  In the context of language cases, an N-gram is usually understood as a sequence of words.  A unigram is one word, a bigram is a sequence of two words, a trigram is three words, and so on.  The number N indicates how many grouped words are included in the N-gram.  Not all possible N-grams fall into the model, but only those that appear in the body. <br><br>  <b>Example:</b> <br><br>  Consider this sentence: <br><br><pre> <code class="plaintext hljs">The office building is open today</code> </pre> <br>  Here are his bigrams: <br><br><ul><li>  the office </li><li>  office building </li><li>  building is </li><li>  is open </li><li>  open today </li></ul><br>  As you can see, the bag of bigrams is a more effective approach than a bag of words. <br><br>  <b>Scoring of words</b> <br><br>  When a dictionary is created, the presence of words should be evaluated.  We have already considered a simple, binary approach (1 - there is a word, 0 - no word). <br><br>  There are other methods: <br><br><ol><li>  Amount.  It is calculated how many times each word occurs in the document. </li><li>  Frequency.  It is calculated how often each word occurs in the text (in relation to the total number of words). </li></ol><br><br><h3>  7. TF-IDF </h3><br>  Frequency scoring has a problem: the words with the highest frequency, respectively, have the highest rating.  These words may not have as much <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D1%2581%25D1%2581%25D1%2582%25D0%25BE%25D1%258F%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%259A%25D1%2583%25D0%25BB%25D1%258C%25D0%25B1%25D0%25B0%25D0%25BA%25D0%25B0_%25E2%2580%2594_%25D0%259B%25D0%25B5%25D0%25B9%25D0%25B1%25D0%25BB%25D0%25B5%25D1%2580%25D0%25B0">informational gain</a> for a model as in less frequent words.  One way to remedy the situation is to lower the word grade, which is often found <b>in all similar documents</b> .  This is called <a href="https://ru.wikipedia.org/wiki/TF-IDF">TF-IDF</a> . <br><br>  TF-IDF (short for term frequency - inverse document frequency) is a statistical measure for assessing the importance of a word in a document that is part of a collection or corpus. <br><br>  TF-IDF scoring increases in proportion to the frequency with which a word appears in a document, but this is offset by the number of documents containing that word. <br><br>  The scoring formula for word X in document Y is: <br><br><img src="https://habrastorage.org/webt/_3/bb/xo/_3bbxoimlox11_am3gzyequcjic.png"><br>  <font color="grey">Formula TF-IDF.</font>  <font color="grey">Source: <a href="http://filotechnologia.blogspot.com/2014/01/a-simple-java-class-for-tfidf-scoring.html">filotechnologia.blogspot.com/2014/01/a-simple-java-class-for-tfidf-scoring.html</a></font> <br><br>  TF (term frequency) is the ratio of the number of occurrences of a word to the total number of words in a document. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ai/p0/wk/aip0wkqcynj8q1cxwxlufspqqds.png"></div><br>  IDF (inverse document frequency) is the inverse of the frequency with which a word occurs in collection documents. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6j/xd/32/6jxd32ydlpkmixkjw6hdgmp6f6m.png"></div><br>  As a result, you can calculate TF-IDF for the word <b>term</b> as follows: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hl/tp/n0/hltpn0vg_gdo8bn1pfimbvu60no.png"></div><br>  <b>Example:</b> <br><br>  You can use the <b>TfidfVectorizer</b> class from the sklearn library to calculate the TF-IDF.  Let's do this with the same messages that we used in the bag example. <br><br><pre> <code class="plaintext hljs">I like this movie, it's funny. I hate this movie. This was awesome! I like it. Nice one. I love it.</code> </pre> <br>  Code: <br><br><div class="oembed"><script type="text/javascript" src="https://gist.github.com/c84cfc6fef2dc131236a9fa5c72de3c9.js"></script><link rel="stylesheet" href="https://github.githubassets.com/assets/gist-embed-a9a1cf2ca01efd362bfa52312712ae94.css"><div id="gist93050848" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-tf-idf-example-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-tf-idf-example-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-tf-idf-example-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-k">from</span> sklearn.feature_extraction.text <span class="pl-k">import</span> TfidfVectorizer</td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-tf-idf-example-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-k">import</span> pandas <span class="pl-k">as</span> pd</td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-tf-idf-example-py-LC3" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-tf-idf-example-py-LC4" class="blob-code blob-code-inner js-file-line">tfidf_vectorizer <span class="pl-k">=</span> TfidfVectorizer()</td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-tf-idf-example-py-LC5" class="blob-code blob-code-inner js-file-line">values <span class="pl-k">=</span> tfidf_vectorizer.fit_transform(documents)</td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L6" class="blob-num js-line-number" data-line-number="6"></td>
        <td id="file-tf-idf-example-py-LC6" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L7" class="blob-num js-line-number" data-line-number="7"></td>
        <td id="file-tf-idf-example-py-LC7" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> Show the Model as a pandas DataFrame</span></td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L8" class="blob-num js-line-number" data-line-number="8"></td>
        <td id="file-tf-idf-example-py-LC8" class="blob-code blob-code-inner js-file-line">feature_names <span class="pl-k">=</span> tfidf_vectorizer.get_feature_names()</td>
      </tr>
      <tr>
        <td id="file-tf-idf-example-py-L9" class="blob-num js-line-number" data-line-number="9"></td>
        <td id="file-tf-idf-example-py-LC9" class="blob-code blob-code-inner js-file-line">pd.DataFrame(values.toarray(), <span class="pl-v">columns</span> <span class="pl-k">=</span> feature_names)</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="" style="float:right">view raw</a>
        <a href="">tf-idf example.py</a>
        hosted with ‚ù§ by <a href="">GitHub</a>
      </div>
    </div>
</div>
</div><br>  Conclusion: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ad/vj/kt/advjktxg44hyhjj63m27igwssiu.png"></div><br><h2>  Conclusion </h2><br>  In this article, the basics of NLP for the text were disassembled, namely: <br><br><ul><li>  NLP allows the use of machine learning algorithms for text and speech; </li><li>  NLTK (Natural Language Toolkit) - the leading platform for creating NLP-programs in Python; </li><li>  sentence tokenization is the process of splitting a written language into component sentences; </li><li>  word tokenization is the process of splitting sentences into word components; </li><li>  Lemmatization and stemming aim to bring all occurring word forms to one, normal vocabulary form; </li><li>  stop words are words that are discarded from the text before / after text processing; </li><li>  A regular expression (regular, regexp, regex) is a sequence of characters that defines a search pattern; </li><li>  A bag of words is a popular and simple feature extraction technique used when working with text.  It describes the occurrences of each word in the text. </li></ul><br>  Fine!  Now, knowing the basics of feature extraction, you can use features as input to machine learning algorithms. <br><br>  If you want to see all the concepts described in one big example, then <a href="https://github.com/Ventsislav-Yordanov/Blog-Examples/blob/master/Intro%2520to%2520NLP%2520-%2520Cleaning%2520Review%2520Texts%2520Example/Cleaning%2520Review%2520Texts%2520Example.ipynb">you are here</a> . </div><p>Source: <a href="https://habr.com/ru/post/446738/">https://habr.com/ru/post/446738/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../446718/index.html">Review of the most interesting reports CodeFest 2019: the version of True Engineering</a></li>
<li><a href="../446722/index.html">Testing will show: how to prepare for the implementation of Cisco ISE and understand what features of the system you need</a></li>
<li><a href="../446728/index.html">How does the received power from the wireless charging, depending on the location of the phone</a></li>
<li><a href="../446730/index.html">Backend section on DUMP: Serverless, Postgres and Go, .NET Core, GraphQL and more</a></li>
<li><a href="../446732/index.html">Feropods will not help: research and mathematical modeling of ant lion larvae pits</a></li>
<li><a href="../446740/index.html">Using Python to generate reports in a single company</a></li>
<li><a href="../446742/index.html">Top 3D Expo 2019 Topics: ‚ÄúAnisprinting - a new generation composite construction technology‚Äù, Fedor Antonov</a></li>
<li><a href="../446744/index.html">VR with neural interfaces - total immersion in virtual reality</a></li>
<li><a href="../446746/index.html">A UBS employee overheard a Eurostar neighbor conversation and found out about a $ 15 billion deal. Now he and the bank will be fined</a></li>
<li><a href="../446750/index.html">Lead from the bottom: IT giants began to actively build their own underwater backbone networks</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>