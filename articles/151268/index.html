<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Flashcache: first experience</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The disk subsystem is often a bottleneck in server performance, forcing companies to invest heavily in fast disks and specialized solutions. Currently...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Flashcache: first experience</h1><div class="post__text post__text-html js-mediator-article"> The disk subsystem is often a bottleneck in server performance, forcing companies to invest heavily in fast disks and specialized solutions.  Currently, SSDs are becoming increasingly popular, but they are still too expensive compared to traditional hard drives.  Nevertheless, there are technologies designed to combine the speed of SSD with the amount of HDD.  These are caching technologies when the amount of disk cache on an SSD is gigabytes, not megabytes of the HDD cache or controller. <br><br>  One such technology is flashcache, developed by Facebook for use with its databases, and which is now distributed open source.  I have long been eyeing her.  Finally, I had the opportunity to test it when I decided to put an SSD-drive in my home computer as a system disk. <br><br>  And before putting the SSD in the home computer, I connected it to the server, which just turned out to be free for testing.  Next, I will describe the process of installing flashcache on CentOS 6.3 and give the results of some tests. <br><a name="habracut"></a><br>  There is a server with 4 Western Digital Caviar Black WD1002FAEX SATA drives combined in a software RAID10, 2 Xeon E5-2620 processors and 8 GB of RAM.  The choice of solid state drive fell on the OCZ Vertex-3 with a capacity of 90 GB.  SSD is connected to a 6-gigabit SATA port, hard drives - to 3-gigabit ports.  The SSD is defined as <code>/dev/sda</code> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      For the experiments, I left unallocated a large area of ‚Äã‚Äãdisk space on a RAID device <code>/dev/md3</code> , in which I created an LVM volume group named <code>vg1</code> , and a <code>vg1</code> logical volume LVM named <code>lv1</code> : <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># lvcreate -L 100G -n lv1 vg1 # mkfs -t ext4 /dev/vg1/lv1</span></span></code> </pre><br><br>  On this volume I tested flashcache. <br><br><h4>  Flashcache installation </h4><br>  Recently, the installation of flashcache has become elementary: there are now binary packages in the elrepo repository.  Before that, it was necessary to compile utilities and kernel module from source. <br>  Let's connect the elrepo repository: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># rpm -Uvh http://elrepo.reloumirrors.net/elrepo/el6/x86_64/RPMS/elrepo-release-6-4.el6.elrepo.noarch.rpm</span></span></code> </pre><br><br>  Flashcache consists of a kernel module and control utilities.  The entire installation is reduced to one codeman: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># yum -y install kmod-flashcache flashcache-utils</span></span></code> </pre><br><br>  Flashcache can work in one of three modes: <br><br><ul><li>  <code>writethrough</code> - read and write operations are saved in the cache, and the disk is written immediately.  In this mode, data integrity is guaranteed. </li><li>  <code>writearound</code> is the same as the previous one, except that only reads are saved in the cache. </li><li>  <code>writeback</code> is the fastest mode because read and write operations are saved in the cache, but the data is flushed to the disk in the background after some time.  This mode is not so safe from the point of view of data integrity, as there is a risk that data will not be written to disk in case of a sudden server failure or loss of power. </li></ul><br><br>  There are three utilities for managing the bundle: <code>flashcache_create</code> , <code>flashcache_load</code> and <code>flashcache_destroy</code> .  The first is used to create a caching device, the other two are needed to work only in writeback mode to load an existing caching device and to delete it, respectively. <br><br>  <code>flashcache_create</code> has the following basic parameters: <br><ul><li>  <code>-p</code> - caching mode.  Required.  It can take the values <code>thru</code> , <code>around</code> and <code>back</code> to enable the writethrough, writearound and writeback modes, respectively. </li><li>  <code>-s</code> - cache size.  If this parameter is not specified, the entire SSD disk will be used for the cache. </li><li>  <code>-b</code> - block size.  The default is 4KB.  Optimal for most uses. </li></ul><br><br>  Create a cache. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># flashcache_create -p thru cachedev /dev/sda /dev/vg1/lv1 cachedev cachedev, ssd_devname /dev/sda, disk_devname /dev/vg1/lv1 cache mode WRITE_THROUGH block_size 8, cache_size 0 Flashcache metadata will use 335MB of your 7842MB main memory</span></span></code> </pre><br><br>  This command creates a caching device with the name <code>cachedev</code> , operating in writethrough mode on the SSD <code>/dev/sda</code> for the block device <code>/dev/vg1/lv1</code> . <br><br>  As a result, the <code>/dev/mapper/cachedev</code> device should appear, and the dmsetup status command should display statistics for various cache operations: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># dmsetup status vg1-lv1: 0 209715200 linear cachedev: 0 3463845888 flashcache stats: reads(142), writes(0) read hits(50), read hit percent(35) write hits(0) write hit percent(0) replacement(0), write replacement(0) write invalidates(0), read invalidates(0) pending enqueues(0), pending inval(0) no room(0) disk reads(92), disk writes(0) ssd reads(50) ssd writes(92) uncached reads(0), uncached writes(0), uncached IO requeue(0) uncached sequential reads(0), uncached sequential writes(0) pid_adds(0), pid_dels(0), pid_drops(0) pid_expiry(0)</span></span></code> </pre><br><br>  Now you can mount the section and start testing.  It is necessary to pay attention to the moment that it is necessary to mount not the partition itself, but the caching device: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># mount /dev/mapper/cachedev /lv1/</span></span></code> </pre><br><br>  That's all: now disk operations in the <code>/lv1/</code> directory will be cached on the SSD. <br><br>  I will describe one nuance that can be useful when working not with a separate LVM volume, but with a group of volumes.  The cache can be created on a block device containing the LVM group, in my case it is <code>/dev/md3</code> : <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># flashcache_create -p thru cachedev /dev/sda /dev/md3</span></span></code> </pre><br><br>  But in order for it to work, you need to change the LVM configuration settings that are responsible for finding volumes.  To do this, install the filter in the <code>/etc/lvm/lvm.conf</code> file: <br> <code>filter = [ "r/md3/" ]</code> <br>  or restrict LVM search only to the <code>/dev/mapper</code> directory: <br> <code>scan = [ "/dev/mapper" ]</code> <br> <br>  After saving changes, you need to report this LVM subsystem: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># vgchange -ay</span></span></code> </pre><br><br><h4>  Testing </h4><br>  I conducted three types of tests: <br><br><ul><li>  iozone utility testing in various caching modes; </li><li>  sequential read with <code>dd</code> in 1 and 4 threads; </li><li>  reading a set of files of various sizes taken from backups of real sites. </li></ul><br><br><h5>  Iozone testing </h5><br>  The utility is in the rpmforge repository. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># rpm -Uvh http://apt.sw.be/redhat/el6/en/x86_64/rpmforge/RPMS/rpmforge-release-0.5.2-2.el6.rf.x86_64.rpm # yum -y install iozone</span></span></code> </pre><br><br>  The test was run as follows: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cd /lv1/ # iozone -a -i0 -i1 -i2 -s8G -r64k</span></span></code> </pre><br><br><img src="https://habrastorage.org/storage2/33a/f97/22e/33af9722e9b7f3da77d35439c599fe77.png" alt="iozone"><br><br>  The diagram shows a slight loss of performance when the cache is turned on during sequential write and read operations, but in random read operations it gives a multiple increase in performance.  The <code>writeback</code> mode goes into the lead as well on write operations. <br><br><h5>  Testing dd </h5><br>  The sequential read test was run by the following commands. <br>  In one thread: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># dd if=/dev/vg1/lv1 of=/dev/null bs=1M count=1024 iflag=direct</span></span></code> </pre><br><br>  In 4 threads: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># dd if=/dev/vg1/lv1 of=/dev/null bs=1M count=1024 iflag=direct skip=1024 &amp; dd if=/dev/vg1/lv1 of=/dev/null bs=1M count=1024 iflag=direct skip=2048 &amp; dd if=/dev/vg1/lv1 of=/dev/null bs=1M count=1024 iflag=direct skip=3072 &amp; dd if=/dev/vg1/lv1 of=/dev/null bs=1M count=1024 iflag=direct skip=4096</span></span></code> </pre><br><br>  Before each launch, the following command was executed to clear the RAM cache: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># echo 3 &gt; /proc/sys/vm/drop_caches</span></span></code> </pre><br><br><img src="https://habrastorage.org/storage2/d81/0bc/0e2/d810bc0e2d80bf5d4bc7cbe63f22219f.png" alt="dd"><br><br>  Cached multi-threaded reading shows a speed close to the maximum speed of the SSD-drive, however, when you first start, the speed is lower than without a cache.  This is most likely due to the fact that, along with the return of the data stream, the system has to write information to the solid-state drive.  However, when you restart, all information is already recorded on the SSD and is given at the maximum speed. <br><br><h5>  Test reading various files </h5><br>  Finally, a small test of the time of reading a large number of files of various sizes.  I took dozens of real sites from backups, unpacked them into the <code>/lv1/sites/</code> directory.  The total amount of files was about 20 GB, and the number - about 760 thousand. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cd /lv1/sites/ # echo 3 &gt; /proc/sys/vm/drop_caches # time find . -type f -print0 | xargs -0 cat &gt;/dev/null</span></span></code> </pre><br><br>  The last command searches all the files in the current directory and reads them, noting the total execution time. <br><br><img src="https://habrastorage.org/storage2/9e6/c3f/5b4/9e6c3f5b4be00dbdf912f31675db5a2c.png" alt="sites"><br><br>  In the diagram, we see the expected performance loss "in the first reading", which was about 25%, but when restarted, the command was executed more than 3 times, faster than the operation without a cache. <br><br><h4>  Cache off </h4><br>  Before removing the caching device, you must unmount the partition: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># umount /lv1/</span></span></code> </pre><br><br>  Or, if you worked with a volume group, then disable LVM: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># vgchange -an</span></span></code> </pre><br><br>  Then delete the cache: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># dmsetup remove cachedev</span></span></code> </pre><br><br><h4>  Conclusion </h4><br>  I have not touched the flashcache tweaks that can be adjusted using sysctl.  Among them, the choice of the algorithm for writing to the FIFO or LRU cache, the caching threshold for sequential access, etc. I think that with these settings you can squeeze out a little more performance gain if you adapt them to the necessary working conditions. <br><br>  The technology has shown itself in the best way mainly in random reading operations, demonstrating a multiple increase in performance.  But in real conditions on servers, random input / output operations are most often used.  Therefore, by minimal investment, it became possible to combine the speed of solid-state drives with the volume of traditional hard drives, and the presence of packages in the repositories makes installation simple and fast. <br><br>  In preparing this article, I used the official documentation from the developers, which can be found <a href="https://github.com/facebook/flashcache/blob/master/doc/flashcache-sa-guide.txt">here</a> . <br><br>  That's all.  I hope this information will be useful to someone.  I would welcome comments, comments and recommendations.  It would be particularly interesting to learn the experience of using flashcache on combat servers, and, in particular, the nuances of using writeback mode. </div><p>Source: <a href="https://habr.com/ru/post/151268/">https://habr.com/ru/post/151268/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../151262/index.html">Minor repair and calibration of APC SmartUPS 700</a></li>
<li><a href="../151263/index.html">Achieve excellence</a></li>
<li><a href="../151264/index.html">System Center 2012 SP1 Beta is available for download.</a></li>
<li><a href="../151265/index.html">How to organize the subscription process on your site</a></li>
<li><a href="../151267/index.html">Happy programmer!</a></li>
<li><a href="../151269/index.html">Performance: Flash vs JavaScript</a></li>
<li><a href="../151270/index.html">Weekdays technical writer: from CIS to Strugatsky</a></li>
<li><a href="../151273/index.html">How to identify all monitors and their resolutions</a></li>
<li><a href="../151274/index.html">Nokia Lumia 920 and Nokia Lumia 820 Reviews</a></li>
<li><a href="../151277/index.html">Zombies - the key to success, or the story of creating the application. Article Maxim Selyaev</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>