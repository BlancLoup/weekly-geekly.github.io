<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Automatic Age Assessment System for Face Images</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="annotation 
 People are the most important tracking objects in video surveillance systems. However, tracking a person does not in itself provide suffi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Automatic Age Assessment System for Face Images</h1><div class="post__text post__text-html js-mediator-article"> <b>annotation</b> <br>  People are the most important tracking objects in video surveillance systems.  However, tracking a person does not in itself provide sufficient information about his motives, intentions, desires, etc.  In this paper, we present a new and reliable system for automatic age estimation using computer vision technologies.  It uses global features of the face, obtained by combining <a href="https://ru.wikipedia.org/wiki/%25D0%25A4%25D0%25B8%25D0%25BB%25D1%258C%25D1%2582%25D1%2580_%25D0%2593%25D0%25B0%25D0%25B1%25D0%25BE%25D1%2580%25D0%25B0">Gabor wavelets</a> and preserving the orthogonality of local projections ( <a href="http://www.cad.zju.edu.cn/home/dengcai/Publication/Conference/f33-cai.pdf">Orthogonal Locality Preserving Projections</a> , OLPP).  In addition, the system is able to estimate age using real-time images.  This means that the proposed system has a greater potential compared to other semi-automatic systems.  The results obtained in the process of applying the proposed approach may provide a clearer understanding of the algorithms in the field of age estimation required for developing applications that are relevant for real application. <br>  <i>Keywords:</i> Gabor wavelets, face image, age estimation, <a href="http://en.wikipedia.org/wiki/Support_vector_machine">Support Vector Machine</a> (SVM). <br><a name="habracut"></a><br>  <b>1. Introduction</b> <br>  The image of a human face contains abundant information about a person, including facial features, emotions, gender, age, etc. In general, an image of a person‚Äôs face can be viewed as a complex signal consisting of many facial properties, such as: skin color, geometric features of facial features .  These attributes play an important role in real-life face image analysis applications.  In such applications, various properties (attributes) evaluated from the captured face image can be used for further reaction (actions) of the system.  Age, in particular, is one of the most important attributes.  For example, users may require an age-dependent interactive computer system, or a system that can estimate age to provide access control or a system to gather intelligence.  Automatic age estimation using facial image analysis involves a huge number of real-world applications. <br>  The automatic age assessment system consists of two parts: the detection of a person in an image and the actual age assessment.  It is rather difficult to detect faces in an image, because the detection results strongly depend on many conditions: environment, movement, lighting, orientation of faces in space, expression of emotions.  These factors can lead to distortions in color, brightness, shadows and contours of images.  For this reason, Viola and Jones proposed their famous face detection system in 2004. The Viola-Jones classifier uses the <a href="https://ru.wikipedia.org/wiki/AdaBoost">AdaBoost algorithm</a> at each node of the classifier cascade to train a high degree of face detection by lowering the number of ignored faces of the entire cascade.  This algorithm has the following features: 1) uses the <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D1%2580%25D0%25B8%25D0%25B7%25D0%25BD%25D0%25B0%25D0%25BA%25D0%25B8_%25D0%25A5%25D0%25B0%25D0%25B0%25D1%2580%25D0%25B0">signs of the Haar</a> - comparing the differences of the sums of intensities of pixels in two rectangular areas with threshold values;  2) the use of an integral image to accelerate the calculation of the sums of pixels in a rectangular area or a rectangular area rotated at an angle of 45 degrees;  3) the AdaBoost algorithm uses statistical boosting to create binary (face - not face) classification nodes, characterized by a good probability of face detection and a small probability of face skip;  4) nodes of weak classifiers are organized in a cascade in order to filter out non-persons' images at the initial stage of the algorithm‚Äôs operation (i.e., the first levels of the cascade allow more errors of incorrect classification, but they work faster than subsequent levels of the cascade classifier).  A person is classified as a person only if it passes through all levels of the cascade classifier. <br>  Although automatic detection of faces in an image is a mature technique involving many applications, estimating the age from a face image is still a difficult task.  This is because the aging process is expressed differently not only among different races, but also within the race.  This process is mostly personal.  In addition, it is also determined by the influence of external factors: lifestyle (proper nutrition, sports), area of ‚Äã‚Äãresidence, weather conditions.  Therefore, the problem of sustainable age assessment is an open problem. <br>  In general, there are three categories of feature extraction methods for estimating a person‚Äôs age in the literature.  The first category is statistical approaches.  Xin Geng et al. [2, 3] proposed AGing pattErn Subspace (AGES), a method for automatic age estimation.  The idea of ‚Äã‚Äãthis approach is to model the pattern (pattern) of aging, which is determined by the sequence of personal images of aging face.  This model is constructed by studying the subspace for such an <a href="https://ru.wikipedia.org/wiki/EM-%25D0%25B0%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC">EM-algorithm of</a> iterative learning of the principal component method ( <a href="http://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis</a> , PCA).  In other papers [4, 5], Guodong Guo and others compare three typical methods for reducing the dimensions of the feature space and various embedding methods such as: PCA, <a href="https://www.cs.nyu.edu/~roweis/lle/">Locally Linear Embedding</a> , LLE, orthogonal <a href="http://www.cad.zju.edu.cn/home/dengcai/Publication/Conference/f33-cai.pdf">Locality Preserving Projections</a> , OLPP).  According to the distribution of data in the OLPP subspace, they offer a locally tuned sustainable regression (LARR) method for learning and predicting a person's age.  LARR uses support vector regression (SVR) regression for coarse prediction and determines local settings within a small limited range of ages, centered relative to the result, using the support vector <a href="http://en.wikipedia.org/wiki/Support_vector_machine">machine</a> (SVM). <br>  The second category of methods includes an approach based on the <a href="http://en.wikipedia.org/wiki/Active_appearance_model">Active Appearance Model</a> (AAM).  Using the appearance model is the most intuitive method among all the facial image analysis methods. <br>  Young H. Kwon et al. [6] used visual age features to construct an anthropometric model.  Primary features are eyes, nose, mouth and chin.  The relationships of these features were calculated to distinguish between different age categories.  When analyzing secondary features, a wrinkle map was used to control the detection and measurement of wrinkles.  Jun-Da Txia et al. [7] proposed an age-based estimation method based on the <a href="http://en.wikipedia.org/wiki/Active_appearance_model">active appearance model</a> (AAM) for extracting regions of age-specific features.  Each face requires the calculation of 28 specific points and is divided into 10 wrinkle regions.  Shuicheng Yan et al. [8] used a path-based appearance model called Patch-Kernel.  This method is designed to determine the Kullback-Leibler distance between models that are derived from the global <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3DEM_%25D0%25B0%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC_(%25D0%25BF%25D1%2580%25D0%25B8%25D0%25BC%25D0%25B5%25D1%2580)">Gaussian mixture model</a> (GMM) using the maximum a posteriori probability ( <a href="http://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation">Maximum a Posteriori</a> , MAP) of any two images.  The ability to classify was then enhanced by a process of weak learning, called synchronization of intermodal similarity.  Nuclear regression is used at the end for age estimation. <br>  The third category of methods uses a frequency based approach.  In image processing and pattern recognition, frequency domain analysis is one of the most popular methods for extracting image features.  Guodong Guo et al. [9] investigated the ‚Äúbiological‚Äù features of an image ( <a href="http://www.cs.tau.ac.il/~wolf/papers/meyerswolf2007.pdf">biologically inspired features</a> , BIF) to assess the age of people in an image.  Unlike previous works [4, 5], Guo modeled a person's face using Gabor filters [10].  Gabor filters are linear filters used in image processing to highlight the boundaries of objects inside an image.  The frequency and orientation of the Gabor filter views is similar to human vision and is well suited for texturing and solving the problem of discrimination. <br>  Our proposed system uses the cascade AdaBoost for learning to detect individuals, and gets the age estimate by applying the Gabor and OLPP wavelets.  This article consists of the following sections.  The first includes a description of the face detection system: histogram alignment, feature selection, cascade classifier, trained by AdaBoost, and the algorithm for clustering regions of the face image.  The second section: the age estimation process involves extracting features using Gabor wavelets, screening features and selecting the best, age classification.  At the end of the article, the simulation results are presented and conclusions are drawn. <br>  This article proposes a fully automatic age estimation system using Gabor wavelets to represent the aging process.  The system we offer has 4 main modules: 1) face detection;  2) analysis based on Gabor wavelets;  3) OLPP reduction;  4) classification by the method of support vectors.  The input image can come from the camera or be read from the file.  A face image is selected from the original image using a face detector using the approach indicated in [12].  Then the image is scaled to have a size of 64 * 64 pixels.  Then, using 40 cores of Gabor wavelets, features are extracted and the OLPP reduction is applied to them.  At the end, the age estimate is started using the trained SVM classifier. <br>  The rest of the article is organized as follows: Section 2 describes the face detection subsystem using AdaBoost.  Section 3 describes the algorithm for estimating age and includes: texture analysis of Gabor wavelets, OLPP reduction, and SVM classification.  Section 4 presents the experimental results.  Section 5 draws conclusions on the proposed system. <br><img src="https://habrastorage.org/files/15c/2e9/544/15c2e95448a74f51aa63c8c473eabe9d.jpg"><br>  Figure 1. System Overview <br><br>  <b>2. Face Detection</b> <br>  Figure 1 shows the architecture of the automatic age estimation system proposed in our work.  The entire system consists of a face detection subsystem, whose task is to detect areas of the faces in the image and the age estimation subsystem.  Scanning windows of various sizes are used to search for faces in an image, since  an object can capture images at different distances from the camera.  There are a total of 12 scale scan levels, and the image size changes from 24 * 24 with a scale factor of 1.25.  Depending on the lighting conditions in which the capture of images takes place, there may be various variations in the brightness of the images.  The image can be more accurately recognized (more precisely, the face on the image) after normalizing its brightness. <br><br>  <b>2.1.</b>  <b>Illumination normalization</b> <br>  Illumination normalization is based on the histogram equalization method.  The primary task of fitting a histogram is to convert the original histogram H (l) to the target histogram G (l).  The target histogram G (l) is selected as an image histogram close to the average histogram for the database of persons.  Select the target image and histogram G (l) as shown in Figure 2 (a).  Images before and after normalization are shown in Figures 2 (b) -. <br><img src="https://habrastorage.org/files/213/be7/871/213be787158542629c632a04a59755ed.jpg"><br>  Figure 2. Illumination normalization.  (a) Target Image.  (b) Input Images.  (c) Normalized images 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Input images that are too dark or too light are normalized according to the histogram of the target image.  The histograms H (l) are converted into histograms G (l) as follows: <br><img src="https://habrastorage.org/files/a94/f13/157/a94f1315760f494fb3ae9fadfd864b64.jpg"><br>  Where <img src="https://habrastorage.org/files/11a/d89/40d/11ad8940d2c14c118a66be932a787b68.jpg">  and <img src="https://habrastorage.org/files/3a0/123/371/3a01233716ac40ed88196ba1fedef696.jpg">  - direct and inverse mapping of histograms H (l) and G (l) into histograms of homogeneous (uniform) distributions. <br><br>  <b>2.2 Selection of features</b> <br>  We chose four rectangular signs of Haar as shown in Figure 3 [13]. <br><img src="https://habrastorage.org/files/101/dfb/c9e/101dfbc9e93b454999085d5918d847bb.jpg"><br>  Figure 3. Four types of rectangular features <br><br>  It is permissible to use a composition of rectangles of different brightness to represent the light and dark regions of the image.  Features are defined as follows: <br><img src="https://habrastorage.org/files/9ec/be6/7b8/9ecbe67b8dba4365836f8676890dbf65.jpg"><br>  where (x, y) denotes the center of the relative coordinate system of a rectangular feature in the scanning window.  The importance of w and h denotes the relative width and height of the rectangular features, respectively.  Type - the type of the rectangular feature, <img src="https://habrastorage.org/files/cf3/97c/912/cf397c91219442bf91bf41cd69dabefa.jpg">  - the difference of the sums of pixels in the light and dark areas. <br>  A rectangular feature that can effectively separate faces and non-faces is considered as a weak classifier: <br><img src="https://habrastorage.org/files/2d6/c4e/d58/2d6c4ed58559484eb06ae26fd5dd9ba2.jpg"><br><img src="https://habrastorage.org/files/1db/a6d/3b0/1dba6d3b04314e798fc371088ce225d0.jpg"><br>  Weak classifier <img src="https://habrastorage.org/files/2d6/c4e/d58/2d6c4ed58559484eb06ae26fd5dd9ba2.jpg">  used to determine whether the current part of the image is a face or not a face based on the calculation of the rectangular feature, threshold q and polarity (direction of inequality) p.  For each weak classifier, the optimal threshold is chosen so as to minimize the error of incorrect classification.  The threshold is selected through training on a sample of 4,000 face images and 59,000 non-face images.  Figures 4 (a) - (b) are examples from databases of individuals and not individuals.  In this procedure, we calculate the distribution of each feature. <img src="https://habrastorage.org/files/095/0c3/660/0950c366082b46be8a49ecb99a61ff8e.jpg">  for each image in the database and select the threshold, which has the maximum discriminative ability (ie, splits the image into two classes better than others). <br><img src="https://habrastorage.org/files/1c3/73d/50e/1c373d50ecaa4ec096f0f9869b274467.jpg"><br>  Figure 4. Database of persons (a) and non-persons (b) <br><br>  Although each rectangular feature is computed very efficiently, the computation of all combinations is very computationally expensive.  For example, for the smallest sliding window (24 * 24), the full set of features is 160,000. <br>  The AdaBoost algorithm combines a set of weak classifiers to form a strong classifier.  Although a strong classifier is effective for face detection applications, it works for a rather long time.  The structure of cascade classifiers, which improves detection ability and reduces computation time, was proposed by Viola and Jones [14].  Based on this idea, our cascade AdaBoost forms a strong classifier.  In the first step, if the image from the sliding window is classified as a face, then we proceed to step 2, in the other case, the image is discarded.  A similar process is performed for all steps.  The number of steps should be sufficient to achieve a good degree of recognition and at the same time, should minimize the computation time.  For example, if at each step the probability of face detection is 0.99, the 10-step classifier will reach a probability of 0.9 (since 0.9 ~ = 0.99 ^ 10).  Although achieving this probability may sound like a very difficult task, it can be done easily, since each step should have a false-positive recognition error value of only about 30%. <br>  The procedure of the AdaBoost algorithm can be described as follows: if m and l are the numbers of individuals and not individuals, respectively, and j is the sum of non-individuals and individuals.  The initial weights w_ (i, j) for the i-th step can be defined as <img src="https://habrastorage.org/files/cf9/2b5/45c/cf92b545c26e41bc8b213abe6fb6f028.jpg">  .  The normalized weighted error of a weak classifier can be expressed as follows: <br><img src="https://habrastorage.org/files/c3d/1a7/077/c3d1a7077cf944cc82ace149960bb947.jpg"><br>  Weights are updated by the formula (5) in each iteration.  If the object is classified correctly, then <img src="https://habrastorage.org/files/e24/e9a/f18/e24e9af188a14e4c84276b7c2b237d6e.jpg">  in other cases, ej = 1. <br><img src="https://habrastorage.org/files/7c4/f5c/642/7c4f5c642df44845b1a1197fc402d056.jpg"><br>  The final classifier for the i-th step is defined below: <br><img src="https://habrastorage.org/files/7c3/0b6/c52/7c30b6c52aec421e9b89bdf560b6363e.jpg"><br>  Where <img src="https://habrastorage.org/files/bc2/b49/c21/bc2b49c21fbe4e86b51887da95d33b61.jpg"><br><br>  <b>2.3 Area-based clustering</b> <br>  A face detector typically finds more than one face, even if it is one on the image (as shown in Figure 5). <br><img src="https://habrastorage.org/files/505/4f3/318/5054f3318f1243f3a27cbc4748407c83.jpg"><br>  Figure 5. The results of the face detector <br><br>  Therefore, domain-based clustering is used to solve this problem.  The proposed method consists of two levels of clustering - local and global clustering.  Local clustering is used to cluster the blocks at one scale and form a simple filter to determine the number of image blocks within the clusters.  If the number of blocks in a cluster is more than one, then this cluster is marked as probably containing a person, otherwise the cluster is rejected.  The local clustering method also has the following rule for deciding about cluster marking: <br><img src="https://habrastorage.org/files/76f/094/707/76f094707b1a409fb8e5782e67619830.jpg"><br>  In formula (7), the percentage of overlap (x, y) denotes the distance between two detected candidate regions and is equal to the distance between the centers of these regions.  Equality <img src="https://habrastorage.org/files/9d8/2e8/9f1/9d82e89f143546a9b9098332adcf75d2.jpg">  means that x and y are in the same cluster and these areas almost completely overlap each other <br>  Figure 6 shows several possible cases of overlapping areas. <br><img src="https://habrastorage.org/files/556/2f5/59b/5562f559b2214c35a2dacc650f2ae7d7.jpg"><br>  Figure 6. Overlapping regions and block center distances <br><br>  In Figure 6 (a), two blocks fall into one cluster.  In Figure 6 (b), two blocks fall into different clusters, since  the distance between their centers is greater than the threshold.  For special cases, as shown in Figure 6 (c), all blocks are considered as candidates, but most of them are false faces.  Therefore, in this paper, for practical applications, we choose only one block that satisfies equation (7) rather than several blocks.  In the end, global clustering will use the blocks obtained at the stage of local clustering, and the label of the front region corresponds to the average size of all the available blocks.  Some results of the whole clustering process based on the choice of regions for the local and global levels are shown in Figure 7. From the right image in Figure 7, in fact, only one block will be accurately classified as a facial region as a result of applying local and global clustering (even if more than 5 facial candidates obtained for the image, which includes only 5 persons). <br><img src="https://habrastorage.org/files/73e/92d/ec7/73e92dec730f4c9a9a9c4bcb9236d305.jpg"><br>  Figure 7. Clustering results.  (a) Results of clustering at the local level.  (b) Clustering results at the global level <br><br>  <b>3. Age assessment</b> <br>        ,    :   ,      .       ,      -      .     2D           ,    -     .  , ,        ,        .  , Donato   [15]  ,           .                ,    ,    . <br><br> <b>3.1      </b> <br>   <img src="https://habrastorage.org/files/ae8/786/30d/ae878630d324491c8d170852014e4871.jpg">      [16]: <br><img src="https://habrastorage.org/files/56c/323/e16/56c323e16eb44b87999460dafb9fc038.jpg"><br>  Where <img src="https://habrastorage.org/files/878/fe0/cea/878fe0ceae5c472e83b3a1e6f6f18ad9.jpg">  and <img src="https://habrastorage.org/files/4db/ef1/294/4dbef12949ee4297921547224ebbb8df.jpg">      , <img src="https://habrastorage.org/files/63d/a64/827/63da648279ef4a6baeea759b5e5ee4cc.jpg">    ,    <img src="https://habrastorage.org/files/3bc/310/bb6/3bc310bb6af84756935847b0ea6d653e.jpg">  : <br><img src="https://habrastorage.org/files/66a/800/d67/66a800d679dd4ea7869d195f59226d60.jpg"><br>  Where <img src="https://habrastorage.org/files/3b5/4a7/c5f/3b54a7c5f24b4320825506718604b146.jpg">  and <img src="https://habrastorage.org/files/660/82b/8d2/66082b8d2e274c948498d5fd3f4a6ac7.jpg"> ‚Äî  ,  f ‚Äî       .  ,     (8)  ,          ‚Äî  ,          <img src="https://habrastorage.org/files/9a0/459/ad6/9a0459ad638c4878be75bfd8f9d12b10.jpg">   ‚Äî       ,         (9)    ,       .   ‚Äî        . <br>   ,        , <img src="https://habrastorage.org/files/923/468/844/9234688446524e13a2f58858ec17be3c.jpg">   , <img src="https://habrastorage.org/files/685/2d1/96f/6852d196fdeb41919548a0b59bc4b81a.jpg">   8       5     8 ,       : <img src="https://habrastorage.org/files/cba/dfe/70d/cbadfe70dc354887ac3d05b32b9a66c2.jpg"><img src="https://habrastorage.org/files/e3e/632/dea/e3e632dea0ab484198ab1e28b7609f8e.jpg"><br><img src="https://habrastorage.org/files/a4b/7e9/fb2/a4b7e9fb20254865aece33a8311ead35.jpg"><br>  8.    <br><br>      ‚Äî       ,   (8).  Let be <img src="https://habrastorage.org/files/7b4/072/8c8/7b40728c8b5e4cff8960c31e50d7542e.jpg"> ‚Äî    .    I  <img src="https://habrastorage.org/files/8f8/9fd/992/8f89fd992c604fdf9b8b429c5e0236da.jpg">  : <br><img src="https://habrastorage.org/files/36c/0e2/f4a/36c0e2f4a4b0492ba248d97cc378b7ea.jpg"><br>  Where <img src="https://habrastorage.org/files/5b4/129/706/5b4129706eac4077a5f8a76622a10bcb.jpg">  *  <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B2%25D1%2591%25D1%2580%25D1%2582%25D0%25BA%25D0%25B0"> </a> (). <br>   , <a href="https://ru.wikipedia.org/wiki/%25D0%2591%25D1%258B%25D1%2581%25D1%2582%25D1%2580%25D0%25BE%25D0%25B5_%25D0%25BF%25D1%2580%25D0%25B5%25D0%25BE%25D0%25B1%25D1%2580%25D0%25B0%25D0%25B7%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%25A4%25D1%2583%25D1%2580%25D1%258C%25D0%25B5">  </a> ()      .  (11)  (12) ‚Äî    . <br><img src="https://habrastorage.org/files/a01/2af/00d/a012af00dadf4af68c8f37599d9595f3.jpg"><br>  Where <img src="https://habrastorage.org/files/9f3/fa9/c4d/9f3fa9c4dcda49f38b11ce72b1f81925.jpg">  and <img src="https://habrastorage.org/files/b53/5a0/165/b535a01654c1477a8e5289881a331537.jpg">        . <br><img src="https://habrastorage.org/files/29b/bce/0e6/29bbce0e643841f9aea6f4f6e72929bd.jpg"><br>  9.      40    <br><br>  9        .     9,         .      ,     .  ,   <img src="https://habrastorage.org/files/200/0fe/311/2000fe311ba84757a0d1843d76a828a3.jpg">    . <br><br> <b>3.2     </b> <br>  ,                 [19, 20].  ,    ,     ,      ‚Äî      .  3  : ()     (, Parallel Dimension Reduction Scheme, PDRS):               10.        ,       . (b)     (, <a href="http%253A%252F%252Fieeexplore.ieee.org%252Fxpls%252Fabs_all.jsp%253Farnumber%253D5302340">Ensemble Dimension Reduction Scheme</a> , EDRS):  ‚Äî   ,    .     11,       ,         . ()     (, <a href="http%253A%252F%252Fieeexplore.ieee.org%252Fxpls%252Fabs_all.jsp%253Farnumber%253D5302340">Multi-channel Dimension Reduction</a> , MDRS). Xiaodong Li   [21]    2009.     12,                 .  [21] Xiaodong Li  .  ,    ,      . <br><img src="https://habrastorage.org/files/3fe/fea/909/3fefea9093f14d818eda6d197964231f.jpg"><br>  10.     <br><br><img src="https://habrastorage.org/files/f8f/195/73a/f8f19573a7f447e6b035b4739db87be6.jpg"><br>  11.     <br><br><img src="https://habrastorage.org/files/90b/d12/870/90bd12870f7346a6964f535dc4c73759.jpg"><br>  12.     <br><br>         k-  (KNN).      ,  ¬´ ¬ª,   40 .       KNN       40 .            .    ‚Äî     .       . FG-NET    [22]   .    1002    (  )     ,    .    82   ( )    0  69 .         (, <a href="http://en.wikipedia.org/wiki/Mean_absolute_error">mean absolute error</a> , MAE)       .           .     : <br><img src="https://habrastorage.org/files/980/125/a61/980125a614e54189a96a721b52ac0677.jpg"><br>  Where <img src="https://habrastorage.org/files/356/186/af9/356186af90e44430881cc5b6e87109f4.jpg"> ‚Äî      k,  <img src="https://habrastorage.org/files/7df/d1a/b27/7dfd1ab27131434f9c42e1730ac9b41a.jpg"> ‚Äî  . N ‚Äî    .   1      .   ,  . <br>  1.       <br><img src="https://habrastorage.org/files/d97/266/e0b/d97266e0ba0f4fd4a1a2f30425e31673.jpg"><br><br> <b>3.3  </b> <br>      ,       .            .          : () <a href="http://en.wikipedia.org/wiki/Linear_discriminant_analysis">  </a> ()   ,    ,        ,    [23]. (b)    (LPP)  ,    ,      [24]. OLPP       LPP     [25].                  ,   KNN       .       LPP  OLPP    .   2       . OLPP         . <br><br>  2.       <br><img src="https://habrastorage.org/files/452/5bc/6f5/4525bc6f51f043979512c3a1cffd44d8.jpg"><br><br> <b>3.4  </b> <br>        -   .         .                       .        .,   . [25-27].  1   11                    .            .    1002    (  )     ,    .    82   ( )    0  69 .       43    (    2).  ,           KNN. <br><br> <b>4.  </b> <br>    FG-NET      [20].       1002    (  )     ,    .    82   ( )    0  69 .   13        . <br><img src="https://habrastorage.org/files/301/e17/559/301e1755900e47068fc809b50ce3ad4d.jpg"><br>  13.     FG-NET  <br><br>      ,          ,    2.   ,      ,       ,     . ,         . <br>         64*64 ,      256  .      ( <a href="http://en.wikipedia.org/wiki/Radial_basis_function_kernel">Radial basis function kernel</a> , RBF) ,    c = 0,5   g = 0.0078125.       ,     . <br>           :    ()    ().           .     [2-10].   : <br><img src="https://habrastorage.org/files/f5a/3c6/dd3/f5a3c6dd37774a3fb140dc23bc4f7abf.jpg"><br>  Where <img src="https://habrastorage.org/files/486/2f3/d53/4862f3d53be440ce99b998cfb228b393.jpg"> ‚Äî   ,           j. <br>  3   .       ,      FG-NET.  -OLSS,        8.43  5.71   KNN   ,   ,       .     16%      AGES [2].   3,  ,  LARR [4]   BIF [9]      : 5.07  4.77,  . <br><br>  3.      <br><img src="https://habrastorage.org/files/e10/a96/a64/e10a96a64b6844169f6db24997962c50.jpg"><br><br>   ,   ‚Äî      . LARR   AAM  FG-NET    ,            .       ,         . , LARR        .   BIF  ,   ,  .    ,    BIF .    ,   10.32.  ,  BIF        .     , BIF     .          12-15   . <br>      14.  Gabor-OLPP   ,  WAS  ,   .  AGES   GAbor-OLPP      ,     Gabor-OLPP,     . <br><img src="https://habrastorage.org/files/8b4/470/4cd/8b44704cdfcd4f2da74116f7a96b85f3.jpg"><br>  14.      <br><br> <b>5. </b> <br>              .      ,  ,             .           ,       . <br>                  .     ,     .       ,      ,   . ,         .         . OLPP          . <br><br> <b>6. </b> <br>          : 100‚ÄêEC-17‚ÄêA‚Äê02‚ÄêS1‚Äê032,  , ,       : NSC‚Äê100‚Äê2218-E‚Äê009‚Äê023. <br><br>  <b>Literature</b> <br> [1] Paul V, Jones MJ (2004) Robust Real‚ÄêTime Face Detection. International Journal of Computer Vision 57(2), 137‚Äê154 <br> [2] Geng X, Zhou Z‚ÄêH, Zhang Y, Li G, Dai H. (2006) Learning from facial aging patterns for automatic age estimation, In ACM Conf. on Multimedia, pages 307‚Äì 316 <br> [3] Geng X, Zhou Z‚ÄêH, Smith‚ÄêMiles K. (2007) Automatic age estimation based on facial aging patterns. IEEE Trans. on PAMI, 29(12): 2234‚Äì2240 <br> [4] Guo G, Fu Y, Dyer, CR, Huang, TS (2008) Image‚ÄêBased Human Age Estimation by Manifold Learning and Locally Adjusted Robust Regression. IEEE Trans. on Image Processing, 17(7): 1178‚Äê1188 <br> [5] Guo G, Fu Y, Huang TS and Dyer, CR (2008) Locally Adjusted Robust Regression for Human Age Estimation. IEEE Workshop on Applications of Computer Vision, pages 1‚Äê6,. <br> [6] Kwon Y, Lobo N. (1999) Age classification from facial images. Computer Vision and Image Understanding, 74(1): 1‚Äì21 <br> [7] Txia J‚ÄêD and Huang C‚ÄêL. (2009) Age Estimation Using AAM and Local Facial Features. Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing, pages 885‚Äê888 <br> [8] Yan S‚ÄêC, Zhou X and Liu M. Hasegawa‚ÄêJohnson, M., Huang, TS (2008) Regression from patch‚Äêkernel. IEEE Conference on CVPR, pages 1‚Äê8 <br> [9] Guo G, Mu G, Fu Y and Huang TS (2009) Human age estimation using bio‚Äêinspired features. IEEE Conference on CVPR, pages 112‚Äê119. <br> [10] Serre T, Wolf L, Bileschi S, Riesenhuber M and Poggio T. (2007) ‚ÄúRobust Object Recognition with Cortex‚ÄêLike Mechanisms. IEEE Trans. on PAMI, 29(3): 411‚Äì426 <br> [11] Lin C‚ÄêT, Siana L, Shou Y‚ÄêW, Yang C‚ÄêT (2010) Multiclient Identification System using Adaptive <br> Probabilistic Model. EURASIP Journal on Advances in Signal Processing.  Vol.  2010 <br> [12] Paul V and Jones MJ (2004) Robust Real‚ÄêTime Face Detection. International Journal of Computer Vision 57(2), 137‚Äê154 <br> [13] Papageorgiou C. P, Oren M and Poggio T. (1998) A general framework for object detection. in <br> Proceedings of the 6th IEEE International Conference on Computer Vision, pp. 555‚Äì562 <br> [14] Viola P and Jones MJ (2004) Robust real‚Äêtime face detection. International Journal of Computer Vision, vol. 57, no. 2, pp. 137‚Äì154 <br> [15] Donato G, Bartlett MS, Hager JC, Ekman P and Sejnowski TJ (1999) Classifying facial actions. IEEE Trans. Pattern Anal. Machine Intell., vol. 21, pp. 974‚Äì 989 <br> [16] Wiskott L, Fellous J, Kruger N and Malsburg C. (1997) Face recognition by elastic bunch graph matching. IEEE Transactions on Pattern <a href="http://cityadspix.com/tsclick-GCQRILZK-SLZKVXTQ%3F%26sa%3D1017%26sa1%3D%26sa2%3D%26sa3%3D%26sa4%3D%26sa5%3D%26bt%3D20%26pt%3D9%26lt%3D2%26tl%3D1%26sa%3D1017%26sa1%3D%26sa2%3D%26sa3%3D%26sa4%3D%26sa5%3D%26sa%3D1017%26sa1%3D%26sa2%3D%26sa3%3D%26sa4%3D%26sa5%3D%26im%3DMTc1NS0wLTE0MjI4NjAwMzgtMTY5ODc5MjI%253D%26fid%3DNDQ2Mjk3NDU1%26kw%3DAnalysis" title="mba.ru">Analysis</a> and Machine Intelligence, vol. 19, pp. 775‚Äì779 <br> [17] Liu C and Wechsler H. (2002) Gabor feature based classification using enhanced fisher linear discriminant model for face recognition. IEEE Transactions on Image Processing, vol. 11, pp. 467‚Äì 476 <br> [18] Liu C. (2004) Gabor‚Äêbased kernel PCA with fractional power polynomial models for face recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 26, pp. 572‚Äì581. <br> [19] Belhumeur PN, Hespanha JP and Kriegman DJ (1997).  ∫Eigenfaces vs. Fisherfaces: Recognition using class specific linear projection. ∫ IEEE Transactions on Pattern Analysis and Machine Intelligence 19(7): 711‚Äê 720. <br> [20] Duda RO, Hart PE, and Stork DG (2000) Pattern Classification, 2nd ed. New York: Wiley Interscience <br> [21] Li X, Fei S and Zhang T. (2009) Novel Dimension Reduction Method of Gabor Feature and Its Application to Face Recognition. International Congress on Image and Signal Processing, 2009. CISP  π09. 2nd, Page(s): 1‚Äê5 <br> [22] The FG‚ÄêNET Aging Database [Online]. Available: <a href="http://www.fgnet.rsunit.com/">www.fgnet.rsunit.com</a> <br> [23] He X‚ÄêF, Yan S‚ÄêC, Hu Y‚ÄêX, Niyogi P and Zhang H‚ÄêJ. (2005) Face recognition using Laplacianfaces. IEEE Transactions on Pattern <a href="http://cityadspix.com/tsclick-GCQRILZK-SLZKVXTQ%3F%26sa%3D1017%26sa1%3D%26sa2%3D%26sa3%3D%26sa4%3D%26sa5%3D%26bt%3D20%26pt%3D9%26lt%3D2%26tl%3D1%26sa%3D1017%26sa1%3D%26sa2%3D%26sa3%3D%26sa4%3D%26sa5%3D%26sa%3D1017%26sa1%3D%26sa2%3D%26sa3%3D%26sa4%3D%26sa5%3D%26sa%3D1017%26sa1%3D%26sa2%3D%26sa3%3D%26sa4%3D%26sa5%3D%26im%3DMTc1NS0wLTE0MjI4NjAwMzgtMTE2NzE5MTU%253D%26fid%3DNDQ2Mjk3NDU1%26kw%3DAnalysis" title="mba.ru">Analysis</a> and Machine Intelligence 27(3): 328‚Äê340. <br> [24] Cai D, He X‚ÄêF, Han J‚ÄêW and Zhang H‚ÄêJ. (2006) Orthogonal Laplacianfaces for Face Recognition. IEEE Transactions on Image Processing 15(11): 3608‚Äê 3614. <br> [25] Mercier G and Lennon M. (2003) Support vector machines for hyperspectral image classification with spectral‚Äêbased kernels. in Proc. IGARSS, Toulouse, France, July 21‚Äì25. <br> [26] Abe S. (2005) Support Vector Machines for Pattern Classification. London: Springer‚ÄêVerlag London Limited. <br> [27] Wang L. (2005) Support Vector Machines: Theory and Applications. New York: Springer, Berlin. <br> [28] Lanitis A, Draganova C and Christodoulou C. (2004) Comparing different classifiers for automatic age estimation. IEEE Trans. Syst., Man, Cybern. B, Cybern., vol. 34, no. 1, pp. 621‚Äì628 </div><p>Source: <a href="https://habr.com/ru/post/248991/">https://habr.com/ru/post/248991/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../248979/index.html">OpenMP Regions Analysis with Intel¬Æ VTune ‚Ñ¢ Amplifier XE</a></li>
<li><a href="../248981/index.html">Dynamic do-it-yourself java code compilation</a></li>
<li><a href="../248983/index.html">How-to: Automate accounting tasks hosting provider</a></li>
<li><a href="../248985/index.html">CSS Auditing: Style Sheets Shouldn't Be Horrible</a></li>
<li><a href="../248987/index.html">CTB-Locker - a new modification of the FileCoder cryptographer</a></li>
<li><a href="../248993/index.html">Map of the latest photos of all people from these cities VK using VKScript</a></li>
<li><a href="../248995/index.html">Guide to the Car Tutorial (Unity3d) part 3 of 3</a></li>
<li><a href="../248997/index.html">Details about the new Microsoft rendering engine for Project Spartan</a></li>
<li><a href="../248999/index.html">Generating fake data for your javascript application using faker</a></li>
<li><a href="../249001/index.html">New Flash Player vulnerabilities are exploited in-the-wild</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>