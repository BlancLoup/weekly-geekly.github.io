<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>On the verge of augmented reality: what to prepare for developers (part 3 of 3)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The third and final part of the trilogy transcript (see part 1 and part 2 ) of the Augmented Reality report. 

 It will be about image processing as a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>On the verge of augmented reality: what to prepare for developers (part 3 of 3)</h1><div class="post__text post__text-html js-mediator-article"><img align="right" src="https://habrastorage.org/storage/habraeffect/fb/63/fb63cef1a719dca3af666b6db130bb62.png"><br>  The third and final part of the trilogy transcript (see <a href="http://habrahabr.ru/blogs/augmented_reality/118123/">part 1</a> and <a href="http://habrahabr.ru/blogs/augmented_reality/118192/">part 2</a> ) <a href="http://addconf.ru/event.sdf/ru/add_2010/authors/112/149">of the Augmented Reality</a> report. <br><br>  It will be about image processing as applied to augmented reality: <br><ul><li>  detection of markers and tags; </li><li>  multi-camera marker motion capture systems; </li><li>  structured lighting; </li><li>  Z-sensors (in particular, Kinect); </li><li>  using posture databases; </li><li>  pure optical motion capture systems. </li></ul><br>  And for a snack - a subjective view of the future of augmented reality, which will clarify the meaning of the picture with the dog. <br><a name="habracut"></a><br><br><h4>  Markers </h4><br>  Much more interesting and innovative is the processing of video images that we receive from cameras.  The simplest is to use markers. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/303/7bf/1fb/3037bf1fb502e6689776e654a149e915.jpg"></div><br>  For example, as we see on the <a href="">Sony Move</a> , there is a glowing ball.  This is a pretty cool thing, because if we know the actual diameter of the marker (D), we know the parameters of our camera, as it is called, the viewing angle (Field-Of-View = FOV), and its resolution, in width (W) and height ( H), and knowing the size in the pixels of the marker's projection onto the image (d), we can estimate the distance to our real object (L), and in general, we can estimate its position in the camera coordinate system. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/6d5/1c9/2aa/6d51c92aa701940445816b1de6103751.jpg"></div><br>  This is all done by absolutely simple and stupid formulas, well, if we have a viewing angle of 75 ¬∞, 640 √ó 480, and a 5 cm ball, and we see it as 20 pixels, then it means there is a meter and a half to it.  On this elementary principle, the Sony Move tracking works.  There is an accelerometer, there is a gyroscope, and there is a magnetic compass, thanks to which we can see the corners, but we still wonder where it is in space. <br><br>  There is a completely simple camera, the Sony Playstation Eye, by the way, is a good camera.  It is relatively inexpensive, and at the same time it can shoot with high frequency and low distortion (geometric). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/e6d/861/bb9/e6d861bb9bdab075ae3a6c550484472a.jpg"></div><br>  And in general, based on this, we can understand where we have something, and to do just such an augmented reality.  Here, in this case, the dude got his hands instead of the Sony Move, because by the markers we understand where they are, and we can scale the hand, depending on whether it is closer or further, and in the corners we understand how to orient this hand: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/fc3/418/3c3/fc34183c32d5ba634f6f1697607eba26.jpg"></div><br><blockquote>  High frequency is how much? </blockquote><br>  60 frames per second, she even 77 can produce, but this is some kind of extreme mode. <br><br><h4>  How to detect markers on image </h4><br>  How to find a marker on the image?  Well, the dumbest way is on the doorstep.  Those.  you have some kind of picture, this is some kind of signal, and you just know that the markers, they are just the brightest in the color we need, and just cut them off at the thresholds.  In fact, most of the algorithms work in this way.  There are more cunning detectors, where they enter every gauss inside, and the like, but they are quite expensive, and what you see in real-time, most likely this is a simple threshold, maybe with some sort of bells and whistles. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/7ce/b11/0b7/7ceb110b7d5ad60ec349115839b2a547.jpg"></div><br>  For example, some such tricky markers are often used in augmented reality, ala some 2D barcodes, only simpler.  Here they are detected, and here, on top of them you can draw some figure: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/216/233/e51/216233e51e7a63aeab296341cd70b91f.jpg"></div><br>  How do they do it?  In <a href="http://studierstube.icg.tu-graz.ac.at/handheld_ar/artoolkitplus.php">fact,</a> there is also the original picture, then it is cut off on the threshold, the contours are cleared, and then with this binarized picture you can already apply a grid, understand what is painted, what is not, and put a pattern: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/416/455/016/416455016e14fe31d861b1a0c572fb56.jpg"></div><br>  All this magic in image processing is, often, a mixture of some completely stupid algorithms, and most importantly, to pick up this mixture, the main innovations in this. <br><br>  But there are more cunning algorithms, now the computing power is growing, it is becoming available. <br><br>  If you have N cameras, and there are M markers, then you can generally do triangulation to calculate the position of each marker in space, understanding how it is projected onto each of the planes, and on the basis of this, add something more complicated. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/568/eec/2d3/568eec2d3da27a25c8808553271182fc.jpg"></div><br>  For example, of such dinosaurs, the fact that it actively exploits this system, this idea, is <a href="http://www.vicon.com/">VICON</a> .  It costs about a hundred thousand dollars.  A person is dressed in a special suit, covered with a bunch of markers, a large number of cameras are placed, and then, strictly speaking, based on the position of these markers, you can try to reconstruct these skeletons. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/58f/8c5/092/58f8c509201049bfb4c87625ede5f9d6.jpg"></div><br>  Reconstruct how the human skeleton moves, based on the fact that it tracks this marker.  In the video, by the way, it‚Äôs quite dark, and you don‚Äôt see the markers yourself, because they are illuminated with infrared light, and there are infrared detectors in the cameras. <br><br><iframe width="420" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/fsYKosFwBkU%3Ffeature%3Doembed&amp;xid=25657,15700023,15700186,15700190,15700253,15700256,15700259&amp;usg=ALkJrhhgiH-xb0U6dkgtazMeOn7X3dIUbA" frameborder="0" allowfullscreen=""></iframe><br><br>  The cameras themselves, by the way, are simply driven to the computer by the coordinates of the marker, because with so many cameras to drive the image to the computer, no matter what modern tires are now ... therefore the dumbest algorithm ‚Äî a threshold, a special chip ‚Äî is sewn into the camera itself It transmits, roughly speaking (x, y), the coordinates of the marker image, and then the computer threshes. <br><br>  But there is still enough tricky mathematics to restore the position of the skeleton on the markers, it is still quite difficult.  Here they are selling it for a hundred thousand dollars, they actually have only one serious competitor among industrial marker systems, this is <a href="http://www.naturalpoint.com/optitrack/">OptiTrack</a> .  They sell something from six thousand dollars, but to work normally - at least 10 thousand.  But there they have a lot of restrictions - one actor, and the like. <br><br>  This is about a market for you to understand, which is still expensive. <br><br>  Total: it is difficult to process images, so you can get out: use markers - cut off on the threshold, and that‚Äôs it, we already have a dot, and with dots we can sort it out somehow. <br><br><h4>  Structured light </h4><br>  Accordingly, the following is such a perversion in order to simplify the processing of images, this is to take and direct some structured light to our object.  In this case, it is in the visible range, cameras in the visible range are working, you can put something smaller.  Based on this, shooting with several cameras ... we have a certain such structure in the image that is not very difficult to detect, and based on how it is displaced in one image relative to another, you can again reconstruct the three-dimensional model due to triangulation. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/01a/68a/dce/01a68adce78bc04377e7e73705d60bf7.jpg"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/7e7/239/42a/7e723942a27747da31982c58dbbfc0b2.jpg"></div><br>  About ten years ago I was doing this a little bit; I still have scanned Lenin's face somewhere on a laptop.  A terrible thing. <br><br>  As a matter of fact, <a href="http://www.microsoft.com/surface/en/us/default.aspx">Microsoft Surface</a> works on this principle until such a purely development-research, until it goes into consumer goods, and maybe it will not work, because it is quite expensive <sup><a href="https://habr.com/ru/post/118243/">1)</a></sup> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/49a/f21/99e/49af2199ec79a8747b6c29e80ff66f74.jpg"></div><br>  There are four or even more infrared cameras downstairs, an infrared structured backlight, and accordingly, based on this, she understands what they put on this Surface, what fingers they touch on it, and so on.  Those.  these are quite such cool and intuitive interfaces. <br><br><iframe width="420" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/6VfpVYYQzHs%3Ffeature%3Doembed&amp;xid=25657,15700023,15700186,15700190,15700253,15700256,15700259&amp;usg=ALkJrhjwLjuInDciyFIaJToEhnj6YsV-HQ" frameborder="0" allowfullscreen=""></iframe><br><br>  I like most of all that when a phone is detected it can be uploaded to it.  It is a cool application of augmented reality, very physically. <br><br><h4>  Z-cameras / Z-sensors </h4><br>  Next - Z-cameras, Z-sensors.  This, of course, is Microsoft Kinect, and I promised to tell you more about it. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/ec0/0e3/e55/ec00e3e552490cb07aac1d7dcc068bbd.jpg"></div><br>  But actually, Kinect, formerly called <a href="http://xbox360.ign.com/articles/109/1096876p1.html">Project Natal</a> , its roots are the Z-sensor and <a href="http://en.wikipedia.org/wiki/ZCam">Z-cam</a> from 3DV, and <a href="http://venturebeat.com/2009/02/21/sources-confirm-microsoft-is-buying-3dv-systems/">Microsoft bought them</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/052/591/34e/05259134e581fd2590ce4efd3470fd87.jpg"></div><br>  And the progenitor of this sensor is alive here.  But if someone sees on the laptop, I‚Äôm coming up here, and here - the depth map, but the Z-sensor itself. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/736/cc8/08a/736cc808a14cfb8101715ec4e87e55ef.jpg"></div><br><br>  How exactly does this sensor work?  In fact, it is like a laser range finder.  A beam of light is sent there, it is reflected from the object and it comes back with some picosecond delays, and we learned to measure them in some way.  This is essentially the time of passage of the light signal to the object and back: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/d20/e55/cb2/d20e55cb27bc545e3dbff7e53f7cfe9b.jpg"></div><br>  This I recorded at home with a video, the depth map looks like: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/a82/5d4/18c/a825d418c8a9afb6a0f1ff87d606fe5a.jpg"></div><br>  And plus to this, it removes the usual RGB image, so that you can somehow analyze it further.  You understand that this is a lot of good data, depth, distance, it is much easier to work with this than just with RGB, you can guess gestures and the like. <br><br>  But Kinect, how is it even different.  It seems that despite the fact that they bought it, they built it all on a different principle <sup><a href="https://habr.com/ru/post/118243/">2)</a></sup> .  Because they have two cameras that determine the depth, and one - which is RGB <sup><a href="https://habr.com/ru/post/118243/">3)</a></sup> .  And it seems that they provide some kind of structured illumination in the infrared range of the object, and they take it off with two cameras, and some rather complicated chip cheats it, in terms of the correlation of these images with the backlight, and the depth to different points.  Those.  They essentially changed the technology. <br><br>  Another interesting thing with Kinect is that they used a database of certain poses for recognition ... after all, the task remains to recognize what position the person is in at the moment, and they need it in real-time, because this is all for games.  They used some kind of database, and this is all very difficult, very secret.  <sup><a href="https://habr.com/ru/post/118243/">four)</a></sup> <br><br>  There are some <a href="http://people.csail.mit.edu/rywang/handtracking/">developments at MIT</a> that do something similar for the hand, <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/296/6c4/9de/2966c49de00d962874876da3b83eb189.jpg"></div><br>  on which they put on a special colored glove, you can order and print out some drawing on a white lycra glove, and then they also approximately restore the position of the hand: <ul><li>  somehow reduce the picture </li><li>  calculate some of its characteristics, </li><li>  and looking for the nearest database </li><li>  and having found, they have a ready position. </li></ul><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/991/bc2/841/991bc28419ac5c85d1e3b02696c634a0.jpg"></div><br>  This is how it works, by the way for what you can use your hands.  Enough such a fun development.  For now, here‚Äôs a lot of boiling down to the fact that you need to apply a trick - hang a marker, put something special on.  You see how they came up with, why all this can be applied - op. <br><br><iframe width="420" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/VnG_9yvEDU8%3Ffeature%3Doembed&amp;xid=25657,15700023,15700186,15700190,15700253,15700256,15700259&amp;usg=ALkJrhiJu4jlsq8XAFVd2xky6QheF5WtJQ" frameborder="0" allowfullscreen=""></iframe><br><br>  Well, then it is their fantasies that they have not washed their hands, but in a glove they take and wash.  Well done, with humor dudes. <br><br><h4>  True Optical: several cameras </h4><br>  Still, you can try to tense up and do it purely optically, without tricks, put a bunch of cameras, cut in more complicated algorithms, there is such a commercial system, again for Motion Capture, for capturing human movements, it is called <a href="http://www.organicmotion.com/">Organic Motion</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/ae3/0e5/a19/ae30e5a191c64ad062a808db8e883c2b.jpg"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/4d7/75b/ec0/4d775bec06720822e065280b3a433f92.jpg"></div><br>  There is some kind of clever algorithms, there they slash a person to restore his position, i.e.  all this is a little barbarous, all this looks, here is a video, this is Andrew Garlic, the author of the idea and the general director, he approached and talked to us at the exhibition, such a completely ordinary and sociable uncle American. <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/8ihmjpxkrYI%3Ffeature%3Doembed&amp;xid=25657,15700023,15700186,15700190,15700253,15700256,15700259&amp;usg=ALkJrhimSjVG6L6WKmXj2QpAqIEfOpwKkw" frameborder="0" allowfullscreen=""></iframe><br><br>  Here he explains how his system works, he has a lot of cameras arranged, a lot, and you see a special background.  As a matter of fact, right in real time, he shoots it and you have a character on the screen that is animated, ... like this. <br><br>  It is worth all this, the price, from 60 thousand dollars, for such a system, it is with equipment, turnkey. <br><br>  We are also trying to work on this niche, we also have marker-free capture of movements, but we have such that we want to make it completely accessible at all, with ordinary cameras, we use the PlayStation ourselves, with ordinary computers.  And if anyone is interested, I can then show the program, we can‚Äôt remove it here, didn‚Äôt bring it, and then, we don‚Äôt have real-time processing, but offline, it‚Äôs not so interesting, you don‚Äôt live looking. <br><br><iframe width="420" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/7ssb0ZN1MSA%3Ffeature%3Doembed&amp;xid=25657,15700023,15700186,15700190,15700253,15700256,15700259&amp;usg=ALkJrhhmgTpj9vHZqy9O0OCSWOqYIfLxFQ" frameborder="0" allowfullscreen=""></iframe><br><br>  Here is an example, one of our users, with the help of our Mockup, made this kind of action movie: <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/cAyeE28Vowc%3Ffeature%3Doembed&amp;xid=25657,15700023,15700186,15700190,15700253,15700256,15700259&amp;usg=ALkJrhhOStt1qWI5QW6mf43CHElmHtyoEw" frameborder="0" allowfullscreen=""></iframe><br><br>  The people already use this, although we still need to refine and refine <sup><a href="https://habr.com/ru/post/118243/">5)</a></sup> .  But this is much better than just sitting and trying to draw all this animation with your hands.  Here is such a thing. <br><br><h4>  True Optical: one moving camera </h4><br>  Well, and finally, I actually complete - truly optical.  There were a lot of cameras, now imagine that you have one moving camera.  Those.  over time, you see the same objects from different points of view.  Based on this, you can get information about these objects. <br><br><iframe width="420" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/Y9HMn6bd-v8%3Ffeature%3Doembed&amp;xid=25657,15700023,15700186,15700190,15700253,15700256,15700259&amp;usg=ALkJrhhKGpKjNEDlxU5NBZqSS2hwcBQlbg" frameborder="0" allowfullscreen=""></iframe><br><br>  Here is a live program that tries to track some points on the image, and on this basis, it finds some planes in the picture, in a real three-dimensional world and then you can put some characters on it, everything is quite funny, this is a live demonstration Moreover, all of this is <a href="http://www.robots.ox.ac.uk/~gk/PTAM/">available in source code</a> .  The only thing is that there are no ready-made c-build examples, so you need to complete the quest, and be able to assemble it all.  I could not under Windows, although it is argued that it is possible.  True, I did not try long, but if someone is very interested, then you can play. <br><br><h4>  Future </h4><br>  About the future.  Be sure to talk about money in the presentation. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/3e5/02b/085/3e502b08560e02b58f151a29d11b4e68.jpg"></div><br>  They assume a sharp increase in all kinds of money, although this is just ... not so many millions of dollars, and the research is so plush. <br><br>  But actually there are three problems on the way to this. <br><br><h5>  Robustness </h5><br>  The first is robustness, i.e.  really reliable.  All that you see and try to use, it is actually buggy, and as if unreliable. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/feb/2a8/ee2/feb2a8ee24897bdb5a2bdc2de04e2dbd.jpg"></div><br>  Robustness is like a doll, i.e.  in spite of the fact that you are making noise, in spite of the fact that something is imperfect there, it still continues to work.  While this is not there, these technologies are still very fragile, they should be used very carefully. <br><br><h5>  Quick response </h5><br>  The second is a quick response.  Because if we perform some action, and we see the response to it after some time, cognitive dissonance arises in our head.  By the way, this is the main problem of Microsoft Kinect, because they have a very noticeable lag between the action and the display in the game, and therefore they have all the games very plush - not hardcore <sup><a href="https://habr.com/ru/post/118243/">6)</a></sup> .  Those.  in such hardcore games, this is still not applicable, but Sony Move - it is applicable, there is almost no lag, it is there in the region of ten milliseconds. <br><br><h5>  Helpfulness </h5><br>  And then the usefulness of actually this all.  While many of these applications are of very dubious usefulness, like a fifth leg dog.  For example, on Android there is such an application, it measures the distance to an object on the floor, knowing the height, i.e.  there you must enter the height at which the device is located, there is such a crosshair in the camera, you point to a point on the floor, and it shows you the distance.  We can calculate the angle, we figured it out, mastered the Kalman filter or alpha-beta, we have entered it and then according to the sine or cosine theorem, as you like, we get the actual distance. <br><br>  Why this, I do not know.  So while with this there are some problems. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/geektimes/post_images/3b6/61c/ec5/3b661cec5445d90345adca7b74395db0.jpg"></div><br><br><h5>  Future is now </h5><br>  Well, what the future might look like, how BMW sees it for itself, in general, it's cool to show BMW as a future: <br><br><iframe width="420" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/P9KPJlA5yds%3Ffeature%3Doembed&amp;xid=25657,15700023,15700186,15700190,15700253,15700256,15700259&amp;usg=ALkJrhgpLe3NCuzQO6zEfWZesFFpB4IPbQ" frameborder="0" allowfullscreen=""></iframe><br><br>  Technician of the future.  Those.  Someone came with his BMW to the Armenians-service, it‚Äôs known, they just came from the village yesterday, they don‚Äôt know how and what is arranged in the car, so they put on special glasses, she draws them ‚ÄúDude, here you need to unscrew the two screws‚Äù .  So he said ‚ÄúNext step‚Äù, she disassembled the Armenian accent, and showed ‚Äúnow take it off‚Äù.  So you can disassemble, it is unclear whether it is also possible to assemble everything back, in general, the future sees BMW as well. <br><br>  This is how the AVATAR film was shot (I heard the statement that it was much more interesting to watch how Avatar was shot than to watch Avatar itself): <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/1wK1Ixr-UmM%3Ffeature%3Doembed&amp;xid=25657,15700023,15700186,15700190,15700253,15700256,15700259&amp;usg=ALkJrhhtxLr-PmpZDRCkqUq1yGhsKCzsDg" frameborder="0" allowfullscreen=""></iframe><br><br>  Here they are in special costumes, with special markers, in addition, they have black dots on their faces, we must also cover the facial animation, which I haven‚Äôt mentioned yet.  There are black dots on the face, and right here the camera is hanging on the bracket.  Accordingly, they immediately capture and facial animation, and the movement of the whole thing. <br><br>  And they rode there on real horses, instead of some mythical school, which are there on Pandora.  In general, if you type ‚ÄúAVATAR MOCKUP‚Äù on YouTube, then you will find a bunch of the same video, ten times filled with different people and organizations.  Here, you can get accustomed - put black dots on it, these are also markers.  In Hollywood, they do not soar. <br><br>  As I understand it, there was a lot of innovation there, highlighting the face so that it was bright, and they even learned how to track eye movements.  Here (the scene with the dragon at the cliff) - do you think this was all drawn on the computer?  Rendered it yes, but the actors had to fly. <br><br>  Also, I once was shocked by the way the matrix was removed there - it turned out that they were hung there on the ropes so that they ran along the walls.  So the actors still have to sweat. <br><br><hr><br><br>  That's all, SEE YOU IN AR! <br><br><hr><br><br><h4>  Notes </h4><br><a name="Note1"></a>  <sup><b>1)</b></sup> In the six months since the report, Microsoft has managed to announce a new version - <a href="http://venturebeat.com/2011/01/09/up-close-with-microsofts-next-generation-surface-touchscreen-tables-video-interview/">Surface 2.0</a> , working on new principles - PixelSense.  And in the near future <a href="http://www.microsoft.com/surface/en/us/purchaseprocess.aspx">is expected to begin sales of the</a> device SUR400 from Samsung. <br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/PwnMCvhPzLM%3Ffeature%3Doembed&amp;xid=25657,15700023,15700186,15700190,15700253,15700256,15700259&amp;usg=ALkJrhh4KNk7Eodnr0t6oOKbRU-jA0JV5A" frameborder="0" allowfullscreen=""></iframe><br><br><a name="Note2"></a>  <sup><b>2)</b></sup> As soon as the Kinects went on sale, they were <a href="http://www.ifixit.com/Teardown/Microsoft-Kinect-Teardown/4066/">immediately disassembled</a> and they found out that there is a chip from <a href="http://www.primesense.com/">PrimeSense</a> .  This is also an Israeli company (like 3DV), but they did not sell out to Microsoft, so now their chip stands in <a href="http://www.tomshardware.com/news/Motion-Sensing-Kinect-PrimeSense-WAVI-Xtion-Xbox-360,11874.html">ASUS Xtion</a> . <br><br><a name="Note3"></a>  <sup><b>3)</b></sup> In fact, there is one infrared camera, one RGB camera and an infrared laser for structured backlighting.  Hello again structured light!  This is how a room with a working Kinect through infrared glasses looks like: <br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/X02ArLyReSM&amp;xid=25657,15700023,15700186,15700190,15700253,15700256,15700259&amp;usg=ALkJrhhUrcnzofvzTVGB0_3YftDRuCZJ3g" frameborder="0" allowfullscreen=""></iframe><br><br><a name="Note4"></a>  <sup><b>4)</b></sup> Not so long ago, MS Research published an article on this topic: <a href="http://research.microsoft.com/pubs/145347/BodyPartRecognition.pdf">Real-Time Human Pose Recognition in Parts from Single Depth Images</a> .  Interestingly, the official <a href="http://www.engadget.com/photos/microsofts-kinect-patent-application/">patent</a> describes a slightly different approach. <br><br><a name="Note5"></a>  <sup><b>5)</b></sup> During this time, they released a version with Kinect support and made a lot of improvements: <br><iframe width="420" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/GCu8KTrC4sc%3Ffeature%3Doembed&amp;xid=25657,15700023,15700186,15700190,15700253,15700256,15700259&amp;usg=ALkJrhhGUvD9G9OU_Zty9n-aX6Iu07QmFA" frameborder="0" allowfullscreen=""></iframe><br><br><a name="Note6"></a>  <sup><b>6)</b></sup> Less than a month ago, MS <a href="http://kinectforums.net/content/87-microsoft-working-new-kinect-algorithms.html">announced</a> that a new version of the tracking algorithm of the human body is being prepared, which has significantly less time delays. </div><p>Source: <a href="https://habr.com/ru/post/118243/">https://habr.com/ru/post/118243/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../118236/index.html">Just about the Incredible. Video review HTC Incredible S</a></li>
<li><a href="../118237/index.html">DIY mini framework</a></li>
<li><a href="../118239/index.html">Opposition HP and Oracle. Continuation</a></li>
<li><a href="../118241/index.html">How Apple Influenced Web Design</a></li>
<li><a href="../118242/index.html">Create an application for Windows Phone 7 from start to finish. Part 7, the anniversary. Adding images and icons</a></li>
<li><a href="../118248/index.html">Apple has published the official position on the Consolidated.db file</a></li>
<li><a href="../118249/index.html">Sales reports on the Android Market</a></li>
<li><a href="../118251/index.html">Algorithms: search for chemical compounds, the task of ranking and analysis of genomes</a></li>
<li><a href="../118252/index.html">Google Chrome has learned to clean flash cookies</a></li>
<li><a href="../118257/index.html">Reasoning about the sense of work of a project manager and how to formulate requirements for this vacancy</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>