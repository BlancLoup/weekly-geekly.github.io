<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Saw - Won. How is the capture of items from the robot Tod Bot</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi Habr! Here we go again! In response to the many skeptics who often met on our way, we continue to develop the project ‚ÄúRob Tod Bot‚Äù. This post is a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Saw - Won. How is the capture of items from the robot Tod Bot</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/cfb/689/f00/cfb689f0053918055cd9b91eb0e3b33d.jpg"><br>  Hi Habr!  Here we go again!  In response to the many skeptics who often met on our way, we continue to develop the project ‚ÄúRob Tod Bot‚Äù.  This post is a continuation of the acquaintance with the <a href="http://habrahabr.ru/company/tod/blog/216681/">MoveIt module</a> as a manipulator control tool. <br>  First of all, I would like to say that we managed to achieve significant results in the task of capturing and moving objects by means of a manipulator, as well as in the recognition of objects, but first things first. <br><a name="habracut"></a><br><br><h4>  It is a little theory about capture in MoveIt </h4><br>  The capture of an object can be represented in the form of a conveyor consisting of several stages, in which the complete trajectory ready for execution is calculated: starting from the initial position of the manipulator and directly raising the object.  These calculations are based on the following data: <br><ul><li>  Scene planning, which provides a tool Planning Scene Monitor </li><li>  Object identifier to capture </li><li>  Pose capture brush for this object </li></ul><br>  The latter in turn contains the following data: <br><ul><li>  The position and orientation of the "brush" manipulator </li><li>  Expected probability of successful capture for this pose </li><li>  The preliminary approach of the manipulator, which is defined as the direction of the vector - the minimum / desired distance of the approach </li><li>  The indent of the manipulator after the capture, which is defined as the direction of the vector - the minimum indent distance </li><li>  Maximum grip force </li></ul><br>  Before the system starts the conveyor, it is necessary to generate possible options for the position of the brush to capture our object.  In our case, we make the assumption that all captured objects have a rectangular shape.  Accordingly, with the capture in the form of two fingers, we have only two paired planes for reliable gripping of the subject, not counting the top and bottom, on which the subject stands.  According to this, capture positions are generated in the form of hemispheres for one pair of planes and another. <br><img src="https://habrastorage.org/getpro/habr/post_images/5e1/208/41d/5e120841da16e5da5acab83c84e89ca3.jpg"><br><br>  Among the obtained set of possible poses, we must weed out those poses that do not satisfy the shape of our grip / brush, and then pass the remaining trajectories to achieve these poses in the pipeline for further planning. <br>  In the conveyor raising the subject, there are three main points: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/0c3/7f6/04e/0c37f604e05cff19ba75b1a5825deebb.jpg"><br>  1- Initial position;  2- Pre-locking position;  3 - Position capture; <br><br>  During the execution of the pipeline, individual paths will be added to the final plan for raising the object.  If the capture has successfully passed all the stages, then only the plan can be executed.  The conveyor algorithm in general looks like this: <br><ol><li>  The trajectory is planned from the initial position to the pre-grip point.  If we draw an analogy with the landing of the aircraft on the runway, it will be a landing approach. </li><li>  All objects of the environment are initially included in the collision matrix, we wrote about it <a href="http://habrahabr.ru/company/tod/blog/216681/">here</a> .  In order for our capture to succeed, collision checking is disabled.  Then the capture opens. </li><li>  The trajectory of the approach of the manipulator to the object from the pre-grip point to the capture point is calculated. </li><li>  Capture closes. </li><li>  The captured object is still represented as an object of collision so only a difference that it is now part of the capture and is taken into account when planning the trajectory. </li><li>  Then, an indentation trajectory is generated from the position of capture to the point of preload to detach the object from the surface and fix the result of raising the object. </li></ol><br>  The constructed plan, containing all the necessary trajectories, can now be executed. <br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/xz042wbjtaw%3Ffeature%3Doembed&amp;xid=17259,15700022,15700186,15700191,15700253&amp;usg=ALkJrhgQcS4Zb2EGO3XlOl6XDwhiTmcBVA" frameborder="0" allowfullscreen=""></iframe><br><br><h4>  That has not yet said </h4><br>  At the entrance of our experiments, we decided to add our hand to the original four degrees of freedom two more.  In the video and photo they are shown in red.  This is due to the fact that in the case of using a gripper in the form of a fork or an anthropomorphic brush, good flexibility of the manipulator is necessary.  By the way, if you use a vacuum sucker as a grip, then everything is somewhat simplified and 4 degrees of freedom can be enough, since  only one plane is used for capturing. <br>  In fact, the ability to perform capture largely depends on the generation of capture positions: the more and more diverse the positions are generated, the easier it will be to find the optimal one.  Although all of this has a downside: the more positions, the longer it will take to process them.  In our case, we first generated 10, 34 positions, then 68, and then 136. The best option that suits us is 34 positions.  With a minimal number of positions, the manipulator is quite difficult to get into the generated posture, as a rule, the manipulator simply physically cannot reach it: unable to turn out like this, too short, too long, etc.  At 34 there are from 2 to 5 positions satisfying all conditions. <br><br><h4>  Object Recognition </h4><br>  For these purposes, we decided to use the ROS node tabletop_object_detector.  It was implemented by scientists at the University of British Columbia and has already managed to recommend itself.  Although, in my opinion, the choice of the system should depend directly on the conditions in which you intend to apply the recognition and those objects that need to be identified.  In our case, the recognition is carried out in the form of objects, and if you need to distinguish a jar of cucumbers from a can of tomatoes, then this method will not work.  To identify objects using these depth camera data obtained from the Kinect. <br>  Before recognizing, you first need to train the system - to create a 3D model of the desired object. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ce9/983/d0b/ce9983d0b109e429b311d5b0ecf43538.png"><br>  3D model of the pack Pringles <br><br>  After that, the system compares the received data with the models available in the database. <br>  The recognition result looks like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/79b/1f0/575/79b1f057541c1667e835d7384c0dac46.jpg"><br><br>  As one would expect, the speed of searching for objects directly depends on the power of the machine at which the data is being processed.  We used a laptop with intel core 2 duo 1.8ghz and 3Gb RAM.  In this case, the identification of objects took about 1.5 - 2 seconds. <br>  And of course, being able to select and identify objects from the environment, now I want to take and move them.  The next step is to combine the tasks of recognition and control of the manipulator on a real robot. </div><p>Source: <a href="https://habr.com/ru/post/224765/">https://habr.com/ru/post/224765/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../224751/index.html">Some interesting and useful things for web developer # 18</a></li>
<li><a href="../224753/index.html">Dvorak or how to turn life into pain</a></li>
<li><a href="../224755/index.html">Illustration of the operation of logic gates using cables and balances</a></li>
<li><a href="../224757/index.html">How to turn everything upside down with the help of 50 Lumia smartphones</a></li>
<li><a href="../224761/index.html">Case that makes the space around the smartphone touch</a></li>
<li><a href="../224767/index.html">Do not cash in on customer errors</a></li>
<li><a href="../224771/index.html">I want to! Or the world of advertising upside down</a></li>
<li><a href="../224773/index.html">Google's XSS-game</a></li>
<li><a href="../224775/index.html">To be in 3d or not to be? How I scanned and printed myself on a 3d printer</a></li>
<li><a href="../224777/index.html">Simple Science - Digest of Experiences # 33</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>