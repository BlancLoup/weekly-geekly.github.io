<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Preparation of IT infrastructure of a foreign bank for moving information systems to Russia</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Probably, everyone has already heard that in accordance with the Federal Law of July 21, 2014 No. 242-FZ ‚ÄúOn Amendments to Certain Legislative Acts of...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Preparation of IT infrastructure of a foreign bank for moving information systems to Russia</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/web/f91/37b/efd/f9137befdf6249b8a5873f920a7b2b7d.jpg"><br><br><p>  Probably, everyone has already heard that in accordance with the Federal Law of July 21, 2014 No. 242-FZ ‚ÄúOn Amendments to Certain Legislative Acts of the Russian Federation Regarding the Clarification of the Procedure for Processing Personal Data in Information and Telecommunication Networks‚Äù that came into force, storage and processing of personal data in Russia.  The topic, of course, touched almost all foreign financial organizations represented in our country.  The wheel spun and by the will of fate we won the execution of a project of a foreign bank to create an IT infrastructure for the migration of its information systems (IS) to Russia.  Sorry, the contract includes NDA, therefore we cannot name the bank.  But we can tell you how we implemented all this, what solution we proposed, architecture, SPD, which vendors - in general, we pass all our experience below. </p><br><a name="habracut"></a><br><h2 id="nemnogo-podrobnee-o-predposylkah-zadachi">  A little more about the assumptions of the problem </h2><br><p>  Personal data is any information relating to an individual or determined on the basis of this information.  This includes a wide range of data in information systems.  Practical any information about customers in one form or another can be attributed to personal data. </p><br><p>  This topic is relevant for foreign representative offices and subsidiaries of foreign organizations operating in Russia.  Representative offices and subsidiaries require integration to exchange data with the information systems of the parent company.  Often they use the IT infrastructure of the parent company, which is consolidated in several data centers around the world.  In these data centers, almost like in the internal hosting, there are information systems for many of the company's representative offices in various countries of the whole region. </p><br><p>  So, on the one hand, in order to comply with the Law FZ-242, foreign companies faced the issue of transferring information systems processing personal data to the territory of the Russian Federation.  At the same time, business-critical integrated circuits operate on the territory of these companies, which contain the most important data for the customer and ensure the operation of business-critical processes.  Clients tried to keep such systems abroad for the sake of consolidation and providing protection against notorious risks in Russia, such as raider seizures and unexpected inspections of various agencies or the ‚Äúdischarge‚Äù of confidential information to competitors. </p><br><p>  In particular, the issue of the smooth operation of these IP arises very acutely for banks.  Banks provide not only a variety of different services in 24x7x365 mode to private and corporate clients, but must also provide a large amount of reporting to the inspection bodies on a daily, weekly, monthly, quarterly basis.  In the case of idle IP, banks suffer both direct losses - penalties for not filing reports, and indirect ones - for example, reputational losses caused by loss of loyalty and outflow of customers. </p><br><p>  Next, we turn to a specific task that we had to solve for a foreign bank in view of the effective Federal Law. </p><br><h2 id="zadacha-banka">  Bank task </h2><br><p>  So, the task that the bank has set for us can be formulated like this - <strong>to create the necessary IT infrastructure for the operation of portable business-critical information systems with specified performance and reliability indicators</strong> . </p><br><h2 id="trebovaniya">  Requirements </h2><br><p>  <u>1. A new IT infrastructure for portable information systems should have ensured business continuity, both in the case of local failures and in the event of a disaster</u> .  In other words, it was necessary to ensure high availability and disaster recovery. </p><br><img src="https://habrastorage.org/web/82f/205/967/82f205967473410c8c1f08e163adbbbd.png"><br><br><p>  According to indicators: </p><br><p>  RPO (Recovery point objective - how much data will be lost during disaster recovery) - in our case, the customer wanted to get recovery RPO = 0 in case of local failures, and RPO aspiring to 0 in case of a disaster in the main data center.  In other words, in the event of a local failure, there should be no data loss, and in the case of a global failure, data loss should be minimal. </p><br><p>  RTO (Recovery time objective - time for which it is possible to restore the IT system) - the customer wanted recovery time &lt;= 1 hour in case of the worst local failure and RTO &lt;= 2 hours in case of a disaster in the main data center. </p><br><p>  Technical solutions to ensure such parameters are not very cheap and "bite", but the downtime in the work of a subsidiary of a major international bank for 1 day can amount to losses of millions of dollars, which speaks for itself. </p><br><p>  <u>2. Sufficient performance of information systems on new equipment is not worse than before the move</u> .  This should have been ensured from all points of view - for example, as computing resources, and in terms of storage. </p><br><p>  Part of the customer's IT systems worked on the IBM Power platform.  For this platform, we collected statistics on the use of resources, taking into account the average and peak loads.  When sizing, it is important to know how much the indicators may deviate from the average value during the day, week or year, so that the IT systems maintain their performance even in the worst case of the maximum load in case of peak loads, for example, when closing a quarter. </p><br><p>  You can also calculate the performance in some conventional synthetic indicators, such as the number of IOPS with a specific unit and the ratio of read to write.  These metrics we took into account when sizing new equipment. </p><br><p>  It is true to say that it is more interesting for the business representatives from the customer to see more realistic IP indicators, such as the time taken for typical operations to be performed before and after migration.  These indicators were measured on the old platform before the beginning of the migration and were used as reference when the project was handed over to the client.  The task was to ensure that in the new system, with a limited budget, these indicators did not deteriorate at least, and there was also a margin for productivity growth. </p><br><p>  <u>3. Efficient investment of money and efficient use of equipment</u> .  The customer set the task from a business point of view to ensure that requirements No. 1 and No. 2 are met in the minimum budget.  At the same time, unlike many projects of Russian customers, not only the cost of the initial investment - CAPEX, but also OPEX - the cost of supporting and supporting the solution for 5 years was taken into account. </p><br><h2 id="reshenie">  Decision </h2><br><h3 id="ucody-i-rezervirovanieu">  <u>Data Centers and Reservations</u> </h3><br><p>  When a customer speaks of continuity in the case of both local and global failures, a backup data center is needed.  If something in the main data center burns down, then after a while the system will be able to resume work, turning on a new place. </p><br><p>  In our case, the IT infrastructure should have been ready no later than 2 hours after the failure.  Therefore, the cold reserve in the form of an alternative platform, at best, with empty servers did not suit us. </p><br><p>  Accordingly, either a ‚Äúwarm‚Äù reserve or a ‚Äúhot‚Äù one was required.  Customers, like normal foreigners, made demands that there be at least 100 km between data centers to exclude any influence (a power outage or, for example, a global catastrophe in Moscow).  From an economic point of view, synchronous replication was not advisable, since it would have required significant investments in the channel between the data center, the traffic on which was supposed to be encrypted.  From a technical point of view, delays at such a distance between data centers could already begin to affect the speed of information systems, so the option with asynchronous replication between data centers was chosen. </p><br><h3 id="uzhelezo-i-po-pod-risc-sistemy-u">  <u>Hardware and software for RISC systems</u> </h3><br><p>  For information systems running on the RISC architecture, E870 servers were chosen on the IBM Power platform.  They are designed to accommodate business critical loads with the highest level of availability and have a full set of RAS functionality (Reliability, Availability and Serviceability). </p><br><p>  These servers are virtualized at the hardware level using IBM PowerVM, and virtual server partitions (LPARs) are created in them.  In LPAR, processor cores are allocated to perform the load.  They can be singled out in LPAR as a monopoly - without re-signing resources, or in a common pool for sharing by a pool of virtual servers.  You can limit the marginal resource consumption from the shared pool of virtual servers on top in peak modes.  The architecture of the Power subsystems is shown in the figure below. </p><br><img src="https://habrastorage.org/web/2ad/67a/1b2/2ad67a1b2d6145e99a9dadec7087c3ac.png"><br><p>  <em>Fig.</em>  <em>IBM Power Subsystem Architecture</em> </p><br><p>  Any servers, even such reliable ones as the IBM Power E870, where almost everything has been dubbed, can refuse.  Therefore, high availability (HA) software is used to protect against server failure.  In our case, the most suitable cluster software - Veritas Infoscale.  This software has a significant advantage over solutions with simple HA.  It allows you to simultaneously make a local cluster (HA) both between servers on the same site and between sites (DR).  As a result, the customer will be insured against local failure and failure of the entire main data center. </p><br><p>  Veritas Infoscale allows you to organize 3-way replication of data.  This is when the data is duplicated in 2 places on the same site and at the same time there is continuous IP replication in the RTC.  Technically, it would have been possible to make it more simple and cheap, but a significant advantage of Veritas Infoscale software is that if one of the replicas on the local site fails, it will not be necessary to manually reconfigure replication.  As a result, customer data remains permanently protected, even in the event of a local failure. </p><br><p>  The block diagram of the created target architecture of a disaster-proof solution for a bank based on two clusters with external logical volumes is shown in the figure below. </p><br><img src="https://habrastorage.org/web/cf2/546/3e4/cf25463e455d45df823c614f1e671940.png"><br><p>  <em>Fig.</em>  <em>Structural diagram of the created target architecture of a disaster-proof solution</em> </p><br><p> E870 servers are expensive.  On these servers, unlike simpler servers, activations of processor cores are licensed.  In view of this, it was tempting to take simpler S824-type machines, but they have less reliability and, most importantly, less vertical scalability.  The client has one functional task, the execution of which now could take the whole of such an S824 server.  At first, the server would work, but after a couple of years, the performance would not be enough. </p><br><p>  However, in IBM High Power servers (including the E870), you can maximize the use of processor and memory activations by integrating them into a common Enterprise Pool.  Activations from the pool can be used on any of the servers in the pool.  To optimize the cost of the solution on the backup servers, you can purchase fewer activations compared to the main one, which we did.  At the same time, in the event of a failure on these servers, it will be possible to use the entire volume of pool activations. </p><br><h3 id="uzhelezo-i-po-pod-arhitekturu-x86-bankau">  <u>Hardware and software for x86 bank architecture</u> </h3><br><p>  For information systems running on x86 / VMware, a solution was chosen on HP Proliant Bl460 Gen9 server blades and VMware vSphere Enterprise Plus virtualization software.  The architecture of the server virtualization subsystem is shown in the figure below. </p><br><img src="https://habrastorage.org/web/b66/1a0/699/b661a0699fd341068b06a75dff7ac5ad.png"><br><p>  <em>Fig.</em>  <em>Server Virtualization Subsystem Architecture</em> </p><br><p>  To protect against the failure of one host, VMware High Availability cluster technology was used, which allows restarting the failed host virtual machines to others. </p><br><p>  Images and virtual machine data are stored on multiple storage systems.  Business-critical data is duplicated on at least 2 storage systems and presented to hosts through the EMC VPLEX storage virtualizer.  This eliminates one storage system as a point of failure.  Failures on 2-controller storage systems are usually rare, but they happen.  A battery of one controller's cache memory may fail, causing the cache to shut down and significant performance degradation. </p><br><p>  Data is transferred through the storage network over Fiber Channel using 2 factories for redundancy from logical and physical failures.  Factories between the ECC and the RTSOD are not united due to the significant distance between them (more than 600 km). </p><br><p>  To protect against catastrophe in the SLC, a proven solution based on VMware SRM and replication using dedicated devices ‚Äî EMC RecoverPoint ‚Äî has been applied.  They duplicate all the operations of the output from the MLC in the RRC in the asynchronous mode.  In the case of sufficient bandwidth, this replication gives an RPO close to 0. RecoverPoint devices allow you to compress traffic between sites and transmit only unique blocks, which reduces the channel requirements. </p><br><p>  In addition, they also allow you to roll back data volumes to a specific point in time, which provides protection against logical failures.  If a logical failure occurs at a certain point in time, the administrator has the opportunity to roll back to the state before the failure. </p><br><p>  Now many banks are thinking or are already creating their backup data center (or RCOD).  However, this may not be enough, since data centers in Moscow may have a common infrastructure.  Both data centers can be powered through one substation, a fan-shaped shutdown of substations can occur, optical routes to the data center can be approached through one point ... </p><br><p>  The advantage of the EMC VPLEX + EMC RecoverPoint + VMware vSphere HA solution complex we have created allows you to provide protection against failures based on 3 data centers - two closely spaced and one located relatively far in case of a disaster.  This allows the bank to receive synchronous replication with zero data loss in case of failure in one data center, as well as protection from global disasters. </p><br><p>  In our project we have implemented 2 platforms - OTsOD and RCOD.  But we placed 2 sets of equipment in the ECC.  It turns out, as 2 DPC in one.  In the normal case, the payload works on both, but in the event of a failure, the productive load can work on one set.  This allows you to provide the highest demands on the availability of information systems in various cases of failures. </p><br><h1 id="reshenie-v-chasti-spd">  Solution in part of the SAP </h1><br><p>  So, in terms of SPD, we needed to build a resilient solution for three data centers to ensure maximum availability of the IP. </p><br><p>  By the beginning of the project, the customer already had two data centers with Cisco network equipment.  Nexus switches were used, including FEX, as well as DMVPN with a branch network.  Naturally, this determined the preservation of the vendor when upgrading the network. </p><br><h2 id="arhitektura">  Architecture </h2><br><p>  In general, the architecture in the main data center we came out classic: </p><br><p>  ‚Ä¢ as the core of the Nexus 5672; <br>  ‚Ä¢ as access switches of Nexus 2000 series; <br>  ‚Ä¢ Catalyst 2960X was used for management ports. </p><br><p>  In the WAN and Internet segments: </p><br><p>  ‚Ä¢ a pair of ASR 1001X routers for connecting to operators with L3VPN clouds; <br>  ‚Ä¢ a pair of ASR 1001X routers for organizing DMVPN and QOS functions; <br>  ‚Ä¢ ISR4431 routers to connect to the Internet; <br>  ‚Ä¢ PaloAlto firewalls for communication with offices; <br>  ‚Ä¢ Checkpoint firewalls to connect to the Internet. </p><br><p>  In the backup data center, everything is the same except for the reused models Nexus 5548UP and ASR1001.  You also need to mention the Out of band (OOB) Internet channels in each data center with separate firewalls.  The schemes there are the most classic, so here they are not even attached. </p><br><h2 id="svyaz-s-setyu-internet">  Internet connection </h2><br><p>  With the connection to the Internet, a more interesting solution came out.  The customer had only one PI / 24 network.  It was necessary to: </p><br><p>  ‚Ä¢ announce the PI network only from the main data center (CSC) while it is alive (both on the Internet and inside the PDS); <br>  ‚Ä¢ PI network should move to the RCHD in case of the entire data center failure; <br>  ‚Ä¢ The PI network must move in case of double failure of one type of equipment in the DCNC: core, Internet channels, routers, firewalls or WAN switches; <br>  ‚Ä¢ PI network should not move with double failure of similar equipment in the backup data center; <br>  ‚Ä¢ PI network should not move in case of double channel failures between data centers; <br>  ‚Ä¢ The availability of Internet channels should be checked on several subnets from the Internet (for example, 8.8.8.0/24). </p><br><p>  Thus, the availability of ECCs from the RTCS had to be checked through the internal network and via the Internet simultaneously.  Checking the availability of the Internet with us is carried out using Cisco IP SLA.  Naturally, the conditional announcement of the PI network via BGP in the direction of the operators, as well as the conditional announcement in the local network of the DPCU are used in the SLC and RODS. </p><br><p>  Below is a logical diagram: </p><br><img src="https://habrastorage.org/web/a8c/354/67e/a8c35467ebc54db8a13dbebf0321640d.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2 id="obnaruzhennye-interesnye-detali-s-oborudovaniem-nexus">  Discovered interesting details with Nexus equipment </h2><br><p>  As already noted, the project used Nexus 5672UP switches.  When using them, 2 bugs were detected, which Cisco plans to transform into features :) </p><br><p>  The first is copper breakout DAC cables 40G to 4x10G.  In our case, these cables were compatible with all equipment, but they periodically flapped.  It is clear that downtime is small, but it happened quite often.  The cables themselves are shown below: </p><br><img src="https://habrastorage.org/web/2a9/1b9/5aa/2a91b95aa29b47a4912739a742fee298.jpg"><br><p>  In the end, after long tests, copper was replaced by optics, and all problems disappeared.  Optical cables, which have been replaced below: </p><br><img src="https://habrastorage.org/web/385/07d/98a/38507d98ad8e4375a3eb9860cab7d07d.jpg"><br><p>  It seems that Cisco is planning to correct the documentation so far - exclude these cables from the list of compatible ones.  So it is better not to save money and buy only optical DAC cables. <br>  About the Nexus 7000 series there is such a <a href="http://www.cisco.com/c/en/us/td/docs/switches/datacenter/nexus7000/sw/release/notes/cisco_nexus7000_release_notes_8x.html">document</a> .  The key phrase "passive copper optic cables are not supported on the non-EDC ports".  It turns out that all passive QSFP copper may not work correctly on the Nexus 5600 series. </p><br><p>  The second is Microsoft SLB.  Everyone remembers the mode in which static ARP and MAC entries are configured on the network without IGMP.  Moreover, if you do not specify static MAC records, then the packets will flood throughout the VLAN.  So, now the last statement will not be true for the Nexus 5672UP.  In order for the data to be transmitted, it will be necessary to specify static mac records without fail.  Under this case, Cisco also plans to change the documentation. </p><br><h2 id="predlozhennoe-reshenie-v-chasti-spd---svyaz-mezhdu-cod-dci-i-ofisov-wan">  The proposed solution for the PDS - the connection between the data center (DCI) and offices (WAN) </h2><br><p>  When upgrading, the connection between the Main Data Center and the DR Data Center, as well as between the data center and branches should be transferred to encryption with GOST algorithms.  There was enough L3 connectivity between the DSC and RTSOD. </p><br><p>  Cisco networking equipment does not need to know anything about cryptographic gateways, so we built GRE / mGRE on top of encrypted tunnels.  Thus, the dynamic routing protocol EIGRP remains to work. </p><br><p>  Crypto-gateways in both data centers should build a VPN tunnel between themselves, as well as to branch offices.  S-Terra was chosen as the crypto-gateway.  One of the reasons was a very similar setup on Cisco.  In principle, this option of working with S-Terra is described on their website, so nothing complicated usually arises. </p><br><h2 id="vremya-vosstanovleniya-seti-posle-sboev">  Network recovery time after failures </h2><br><p>  As for the recovery time in case of failures for the network, then, as is known, it depends on the technologies and protocols used, as well as the redundancy scheme of the equipment and channels. </p><br><p>  All network equipment and channels were reserved according to the 1 + 1 scheme. For redundancy at the data link level, popular LAP and VPC things with a sub-second time to recover from a failure were used. </p><br><p>  In this project, the customer has already used EIGRP protocols in the local network, between the branches and BGP to interact with the operators.  In the case of a local network, EIGRP does convergence with subsecond timers when a failure is detected.  Compared to OSPF, EIGRP makes it much easier to configure for such results.  You only need to fulfill one of two conditions for a backup route ‚Äî equal cost multi path (ECMP) or feasible successor.  In OSPF, this will require tyunit many timers. </p><br><p>  In general, implicit hardware or cable failures are almost impossible in a local network.  Therefore, the total convergence time will be less than 1 second.  If we talk about the WAN segment and the entire distributed network (communication between the branches and the data center), then the maximum recovery time for failures will be 5 seconds. </p><br><p>  In the case of communication with the Internet, the recovery time is obtained, of course, more - up to 1 minute.  The customer already had its own PI network.  As part of the upgrade, BGP began to accept the entire routing table.  Actually, this is necessary in order to protect against some failures inside Internet providers.  So, we rule out a situation where the BGP peer is visible, but then everything is bad with the connection. </p><br><p>  In order to achieve Internet recovery in 1 minute in case of equipment or channel failure, it is necessary to negotiate with the operators to reduce keepalive and hold timers.  In practice, we show good keepalive results in 3 seconds and hold in 10 seconds.  Flaps at such values ‚Äã‚Äãnever occurs, although at 2 and 7 seconds, respectively, they are already observed.  True, not all operators are ready to support such values ‚Äã‚Äãby default, but it is possible to agree.  :) </p><br><p>  That's all.  Customer satisfied.  If you have questions about the decision, we will be happy to answer. </p><br><p>  <strong><u>The authors of the material:</u></strong> </p><br><p>  <strong>Artem Burdin, Design Engineer of the Computing Systems Department of the Competence Center for Computing Complexes of Technoserv Company</strong> </p><br><p>  <strong>Mikhail Sheronkin, Head of Corporate Networks, Competence Center for Network Technologies, Tekhnoserv Company</strong> </p></div><p>Source: <a href="https://habr.com/ru/post/332982/">https://habr.com/ru/post/332982/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../332970/index.html">OAuth Authorization for Xamarin Applications</a></li>
<li><a href="../332974/index.html">Import () from the webpack will soon master JS + CSS, but how you can use it now</a></li>
<li><a href="../332976/index.html">Released the first drive on 64-layer 3D TLC NAND from Intel</a></li>
<li><a href="../332978/index.html">How I ordered the mobile app</a></li>
<li><a href="../332980/index.html">SYSMON Dashboards for InterSystems Cach√©, Ensemble and HealthShare performance monitoring</a></li>
<li><a href="../332984/index.html">Post mortem Super Meat Boy</a></li>
<li><a href="../332988/index.html">Yandex.Taxi and Uber will go together</a></li>
<li><a href="../332990/index.html">Linux Foundation Unveils Free Kubernetes Introductory Online Course</a></li>
<li><a href="../332992/index.html">7 ways to improve the design of the site</a></li>
<li><a href="../332994/index.html">Testing in Openshift: An Introduction</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>