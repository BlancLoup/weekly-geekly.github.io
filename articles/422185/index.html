<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How much data for learning model (not) similar to the test sample?</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Consider one of the scenarios in which your machine learning model may be useless. 

 There is a saying: "Do not compare apples with oranges . " But w...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How much data for learning model (not) similar to the test sample?</h1><div class="post__text post__text-html js-mediator-article">  Consider one of the scenarios in which your machine learning model may be useless. <br><br>  There is a saying: <i>"Do not compare apples with oranges</i> . <i>"</i>  But what if you need to compare one set of apples with oranges with another, but the distribution of fruits in two sets is different?  Can you work with the data?  And how will you do it? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lt/0y/oi/lt0yoiaq5fua0l1gfpguoigfxfa.png"></div><a name="habracut"></a><br>  In real-life cases, this situation is ubiquitous.  When developing machine learning models, we are faced with a situation where our model works well with the training sample, but the quality of the model drops sharply on test data. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      And this is not about retraining.  Suppose that we have built a model that gives an excellent result for cross-validation, but shows a poor result on the test.  So in the test sample there is information that we do not take into account. <br><br>  Imagine a situation in which we predict customer behavior in the store.  If the training and test sample looks like the image below, this is a clear problem: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/vk/ki/j2/vkkij2n5rlshttroh8mqfse-zf4.png"></div><br>  <i>In this example, the model is trained on the data with the average value of the sign ‚Äúcustomer age‚Äù lower than the average value of the same sign on the test.</i>  <i>In the process of learning, the model has never ‚Äúseen‚Äù the larger values ‚Äã‚Äãof the sign ‚Äúage‚Äù.</i>  <i>If age is an important feature for a model, then one should not expect good results on the test sample.</i> <br><br>  In this text we will talk about "naive" approaches that allow to identify such phenomena and try to eliminate them. <br><br><h3>  Covariant shift </h3><br>  We give a more accurate definition of this concept.  <b>Covariance</b> refers to the values ‚Äã‚Äãof features, and by <b>covariant shift</b> is meant the situation when the distributions of feature values ‚Äã‚Äãin the training and test samples have different characteristics (parameters). <br><br>  In real problems with a large number of variables, the covariant shift is difficult to detect.  The article discusses the method of identifying and also taking into account the covariant shift in the data. <br><br><img src="https://habrastorage.org/webt/no/ta/lj/notaljh1r3c8nimkt45_lp3yyco.png"><br><br><h3>  main idea </h3><br>  <i>If there is a shift in the data, then when mixing two samples, we will be able to build a classifier that can determine whether an object belongs to a training or test sample.</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pw/v-/42/pwv-42li1_tl9jqyghnph6czcsg.png"></div><br>  Let's understand why this is so.  Let us return to the example with customers, where age was a ‚Äúshifted‚Äù sign of a training and test sample.  If we take a classifier (for example, on the basis of a random forest) and try to divide the mixed sample into training and test, then age will be a very important feature for such a classification. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/j1/29/wf/j129wfqmbguam_2axs1h6liyt4e.png"></div><br><h3>  Implementation </h3><br>  Let's try to apply the described idea to a real dataset.  Use <a href="https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data">dataset</a> from Kaggle competition. <br><br><h4>  Step 1: Data Preparation </h4><br>  First of all, let's perform a series of standard steps: clean, fill in the blanks, perform label encoding for categorical features.  For the dataset in question, the step was not required, so let's skip its description. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-comment"><span class="hljs-comment">#  train  test train = pd.read_csv('train.csv',low_memory=True) test = pd.read_csv('test.csv',low_memory=True)</span></span></code> </pre> <br><h4>  Step 2: Add Data Source Indicator </h4><br>  To both parts of the dataset - training and test - it is necessary to add a new indicator indicator.  For the training sample with the value "1", for the test, respectively, "0". <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    ,   test,   train test['is_train'] = 0 train['is_train'] = 1</span></span></code> </pre><br><h4>  Step 3: Combine Training and Test Samples </h4><br>  Now it is necessary to combine two datasets.  Because the training dataset contains a column of target values ‚Äã‚Äã'target', which is not in the test dataset, this column must be deleted. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  train, test df_combine = pd.concat([train, test], axis=0, ignore_index=True) #  target df_combine = df_combine.drop('target', axis =1) y = df_combine['is_train'].values #  x = df_combine.drop('is_train', axis=1).values #  tst, trn = test.values, train.values</span></span></code> </pre><br><h4>  Step 4: building and testing the classifier </h4><br>  For classification purposes, we will use the Random Forest Classifier, which we will configure to predict the labels of the data source in the combined dataset.  You can use any other classifier. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.ensemble <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RandomForestClassifier <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np rfc = RandomForestClassifier(n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>, max_depth=<span class="hljs-number"><span class="hljs-number">5</span></span>, min_samples_leaf = <span class="hljs-number"><span class="hljs-number">5</span></span>) predictions = np.zeros(y.shape) <span class="hljs-comment"><span class="hljs-comment">#    </span></span></code> </pre><br>  We use a stratified randomized split into 4 folds.  This way we will keep the ratio of the 'is_train' tags in each fold as in the original merged sample.  For each splitting, we will train the classifier on most of the splitting and predict the class label for the smaller deferred part. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> StratifiedKFold, cross_val_score skf = StratifiedKFold(n_splits=<span class="hljs-number"><span class="hljs-number">4</span></span>, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">100</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> fold, (train_idx, test_idx) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(skf.split(x, y)): X_train, X_test = x[train_idx], x[test_idx] y_train, y_test = y[train_idx], y[test_idx] rfc.fit(X_train, y_train) probs = rfc.predict_proba(X_test)[:, <span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-comment"><span class="hljs-comment">#   predictions[test_idx] = probs</span></span></code> </pre><br><h4>  Step 5: Interpret Results </h4><br>  We calculate the value of the ROC AUC metric for our classifier.  Based on this value, we conclude how well our classifier detects a covariant shift in the data. <br><br>  <i>If the classifier with well separates objects into training and test datasets, then the value of the ROC AUC metric should be significantly greater than 0.5, ideally close to 1. This picture indicates a strong covariant shift in the data.</i> <br><br>  Find the ROC AUC value: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> roc_auc_score print(<span class="hljs-string"><span class="hljs-string">'ROC-AUC:'</span></span>, roc_auc_score(y_true=y, y_score=predictions)) <span class="hljs-comment"><span class="hljs-comment"># ROC-AUC: 0.49974692698385287</span></span></code> </pre><br>  The resulting value is close to 0.5.  This means that our quality classifier is the same as the random predictor of tags.  There is no evidence of a covariant shift in the data. <br><br>  Since it‚Äôs taken from Kaggle, the result is pretty predictable.  As in other machine learning competitions, the data are carefully checked to ensure that there are no shifts. <br><br>  But such an approach can be applied in other data science problems to check for the presence of a covariant shift just before the start of the solution. <br><br><h2>  Further steps </h2><br>  So, either we are seeing a covariant shift, or not.  What to do to improve the quality of the model on the test? <br><br><ol><li>  Remove offset features </li><li>  Use object importance weights based on density coefficient estimate </li></ol><br><h3>  Removing displaced features: </h3><br>  <i><b>Note: the</b> method is applicable if there is a covariant shift in the data.</i> <br><br><ul><li>  Extract the importance of signs from the Random Forest Classifier classifier, which we have built and trained earlier. </li><li>  The most important features are those that are shifted and cause a shift in the data. </li><li>  Starting with the most important ones, delete by one feature, build the target model and look at its quality.  Collect all signs for which the quality of the model is not reduced. </li><li>  Discard the collected signs from the data and build the final model. </li></ul><br><img src="https://habrastorage.org/webt/oa/ga/08/oaga08t43dtyoazhcor8bk3mb8w.png"><br>  <i>This algorithm will remove the signs from the red basket on the chart.</i> <br><br><h3>  Using object importance weights based on density coefficient estimation </h3><br>  <i><b>Note: the</b> method is applicable regardless of whether there is a covariant shift in the data.</i> <br><br>  Let's look at the predictions we received in the previous section.  For each object, the prediction contains the probability that this object belongs to the training set for our classifier. <br><br><pre> <code class="python hljs">predictions[:<span class="hljs-number"><span class="hljs-number">10</span></span>] <span class="hljs-comment"><span class="hljs-comment">#array([0.39743827 ...</span></span></code> </pre><br>  For example, for the first object, our Random Forest Classifier considers that it belongs to the training set with a probability of 0.397.  Let's call this value <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="8.87ex" height="2.66ex" viewBox="0 -832 3819 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-50" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMAIN-28" x="751" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-74" x="1141" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-72" x="1502" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-61" x="1954" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-69" x="2483" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-6E" x="2829" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMAIN-29" x="3429" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>P</mi><mo stretchy="false">(</mo><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-1"> P (train) </script>  .  Or you can say that the probability of belonging to the test data is 0.603.  Similarly, let's call the probability <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="7.408ex" height="2.66ex" viewBox="0 -832 3189.5 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-50" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMAIN-28" x="751" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-74" x="1141" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-65" x="1502" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-73" x="1969" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-74" x="2438" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMAIN-29" x="2800" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>P</mi><mo stretchy="false">(</mo><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-2"> P (test) </script>  . <br><br>  Now a little trick: for each object of the training dataset, we calculate the coefficient <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="26.185ex" height="2.66ex" viewBox="0 -832 11274.1 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-77" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMAIN-3D" x="994" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-66" x="2300" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-72" x="2851" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-61" x="3302" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-63" x="3832" y="0"></use><g transform="translate(4265,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-50" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMAIN-28" x="751" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-74" x="1141" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-65" x="1502" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-73" x="1969" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-74" x="2438" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMAIN-29" x="2800" y="0"></use></g><g transform="translate(7455,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-50" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMAIN-28" x="751" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-74" x="1141" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-72" x="1502" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-61" x="1954" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-69" x="2483" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-6E" x="2829" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMAIN-29" x="3429" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>w</mi><mo>=</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mi>P</mi><mo stretchy="false">(</mo><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi><mo stretchy="false">)</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mi>P</mi><mo stretchy="false">(</mo><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi><mo stretchy="false">)</mo></mrow></math></span></span><script type="math/tex" id="MathJax-Element-3"> w = \ frac {P (test)} {P (train)} </script>  . <br><br>  Coefficient <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.664ex" height="1.455ex" viewBox="0 -520.7 716.5 626.5" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-77" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>w</mi></math></span></span><script type="math/tex" id="MathJax-Element-4"> w </script>  tells us how close the object from the training set is to the test data.  Main thought: <br><br>  <i>We can use</i> <math> </math><i><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.664ex" height="1.455ex" viewBox="0 -520.7 716.5 626.5" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422185/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhWeqJl5V3A6FocIho_IH_O1jyZxg#MJMATHI-77" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>w</mi></math></span></span><script type="math/tex" id="MathJax-Element-5"> w </script></i>   <i>as weights in any of the models, in order to increase the weight of those observations that look similar to the test sample.</i>  <i>Intuitively, this makes sense, since our model will be more data-oriented as in the test set.</i> <br><br>  These weights can be calculated using the code: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">20</span></span>,<span class="hljs-number"><span class="hljs-number">10</span></span>)) predictions_train = predictions[len(tst):] weights = (<span class="hljs-number"><span class="hljs-number">1.</span></span>/predictions_train) - <span class="hljs-number"><span class="hljs-number">1.</span></span> weights /= np.mean(weights) <span class="hljs-comment"><span class="hljs-comment">#  plt.xlabel('  w') plt.ylabel(' ') sns.distplot(weights, kde=False)</span></span></code> </pre><br>  The resulting coefficients can be transferred to the model, for example, as follows: <br><br><pre> <code class="python hljs">rfc = RandomForestClassifier(n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>,max_depth=<span class="hljs-number"><span class="hljs-number">5</span></span>) m.fit(X_train, y_train, sample_weight=weights)</code> </pre><br><br><img src="https://habrastorage.org/webt/_z/nw/rn/_znwrn70amqpw4xosahu1hgj6oa.png"><br><br>  A couple of words about the resulting histogram: <br><br><ul><li>  Larger weights correspond to observations more similar to the test sample. </li><li>  Almost 70% of the objects from the training sample have a weight close to 1, and, therefore, are in a subspace that is equally similar to the training and the test sample.  This corresponds to the AUC value that we calculated earlier. </li></ul><br><h2>  Conclusion </h2><br>  We hope that this post will help you in identifying the "covariant shift" in the data and in dealing with it. <br><br><h2>  Links </h2><br>  [1] Shimodaira, H. (2000).  The log-likelihood function.  Journal of Statistical Planning and Inference, 90, 227‚Äì244. <br>  [2] Bickel, S. et al.  (2009).  Discriminative Learning Under Covariate Shift.  Journal of Machine Learning Research, 10, 2137‚Äì2155 <br>  [3] <a href="https://github.com/erlendd/covariate-shift-adaption">github.com/erlendd/covariate-shift-adaption</a> <br>  [4] <a href="https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data">Link to dataset used</a> <br><br>  PS Notebook with code from the article can be viewed <a href="https://github.com/hakeydotom/Covariate-shift-prediction/">here</a> . </div><p>Source: <a href="https://habr.com/ru/post/422185/">https://habr.com/ru/post/422185/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../422175/index.html">Environmental pollution reduces human cognitive abilities</a></li>
<li><a href="../422177/index.html">Why the Arduino is so slow and what can be done about it</a></li>
<li><a href="../422179/index.html">From clouds to earth: how to create a production-grade Kubernetes in any conditions</a></li>
<li><a href="../422181/index.html">How to collect corporate services on one online platform: the story of MegaFon.Business</a></li>
<li><a href="../422183/index.html">And what could have been cheaper?</a></li>
<li><a href="../422187/index.html">Sberseasons: how I spent this summer</a></li>
<li><a href="../422189/index.html">The truth about introducing an intranet portal</a></li>
<li><a href="../422191/index.html">How I created a profitable Android text recognition application</a></li>
<li><a href="../422195/index.html">The use of ACS in mining</a></li>
<li><a href="../422197/index.html">About the relay we put in a word</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>