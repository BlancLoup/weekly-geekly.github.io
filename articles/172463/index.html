<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Recognizing gender in images and videos</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This article presents a gender recognition algorithm with an accuracy of 93.1% [1] . The article does not require any prior knowledge in image process...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Recognizing gender in images and videos</h1><div class="post__text post__text-html js-mediator-article">  This article presents a gender recognition algorithm with an accuracy of 93.1% <a href="https://habr.com/ru/post/172463/">[1]</a> .  The article does not require any prior knowledge in image processing or machine learning.  After reading the article, the reader will be able to execute the considered algorithm on his own. <br><br><iframe src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://player.vimeo.com/video/58789839&amp;xid=17259,15700023,15700186,15700191,15700253,15700255&amp;usg=ALkJrhhRBkeoRpOOWElxENqIvBOrLW9gRA" width="560" height="315" frameborder="0" title="FaceCept Technology in Action. Sex Recognition" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe><br><a name="habracut"></a><br><h4>  Introduction </h4><br>  Looking at the person, we usually accurately determine the sex.  Sometimes we make mistakes, but often in such cases it is really difficult to do. <br><br>  Often we use some context.  For example, for the author of an article, the color of clothes is sometimes the most significant sign when trying to identify the sex of a child up to 1-2 years. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Let us turn to the problem of determining the sex of the face.  We want to refer each face image to either a class of men or a class of women.  We need a mechanism that will decide which class the photo belongs to.  In such a formulation, the problem looks like a typical machine learning task: there is a set of labeled examples of two classes: M and J. ‚ÄúMarked‚Äù means that each photo is mapped to the floor of the person depicted on it.  So we need some classifier. <br><br>  However, on the basis of what the floor classifier will decide which class to include the image in?  Looking for some characteristics.  Each image is a matrix of a certain size. <nobr><img src="https://habrastorage.org/storage2/70c/f59/669/70cf5966996388c8bbfcc98686a45681.png"></nobr>  <nobr>whose</nobr> elements encode the brightness of each pixel.  Obviously, you can use image pixels directly.  This approach often gives good results, but it is unstable to changes in lighting and the presence of noise in the image. <br><br>  Another approach is to use some pixel ratios.  Developed a huge number of such relationships or characteristics (features).  The study of the applicability of a particular type of characteristic is a key stage in many computer vision tasks. <br><br><h4>  Specifications </h4><br>  To solve the problem of gender recognition, the authors of the article Boosting Sex Identification Performance <a href="https://habr.com/ru/post/172463/">[1]</a> propose to use the set of simple features described below. <br><br>  For simplicity, we number all pixels in the image: <img src="https://habrastorage.org/storage2/4bf/411/8d9/4bf4118d935861c2757fd91cf00dfe93.png">  - means the brightness of a pixel with a serial number <nobr><img src="https://habrastorage.org/storage2/d04/1d4/09c/d041d409c8558fa539af036ba3286e3e.png"></nobr>  <nobr>.</nobr>  Consider the following set of characteristics <nobr>(</nobr> <nobr><img src="https://habrastorage.org/storage2/c03/395/cde/c03395cdeff980210f236a8651e4f8e7.png"></nobr>  <nobr>):</nobr> <br><br><ol><li><img src="https://habrastorage.org/storage2/aa5/b39/02f/aa5b3902f9482adff825acc018b923d5.png"></li><li><img src="https://habrastorage.org/storage2/8e3/9c6/a26/8e39c6a26c22465bdc0f6afd47496e79.png"></li><li><img src="https://habrastorage.org/storage2/6f6/e2b/75b/6f6e2b75b7d2075ea9cfbfb33ed7f878.png"></li><li><img src="https://habrastorage.org/storage2/778/9bc/e6e/7789bce6e4084516bc86cd95257a55a6.png"></li><li><img src="https://habrastorage.org/storage2/b4e/0e4/77b/b4e0e477b671eb7297f1b4c002029685.png"></li></ol><br>  In other words, we compose all possible pairs of pixels in the image, calculate their difference and apply the rules described above.  Each comparison gives a binary characteristic.  For example, for <img src="https://habrastorage.org/storage2/e0e/3e2/f2e/e0e3e2f2e19a8db21dabc42077105280.png"><img src="https://habrastorage.org/storage2/05f/58c/29c/05f58c29cff1fc23121e0574d06a7631.png">  we have: <br><br><ol><li><img src="https://habrastorage.org/storage2/b75/51d/9fc/b7551d9fc23812a96540277cfd90f3bb.png">  - false (0) </li><li><img src="https://habrastorage.org/storage2/3ca/d63/f04/3cad63f04e1f09b8e21fa516d38d7435.png">  - false (0) </li><li><img src="https://habrastorage.org/storage2/61a/98e/4a4/61a98e4a4aafb218092cb7c3827db462.png">  - false (0) </li><li><img src="https://habrastorage.org/storage2/e26/49c/291/e2649c291764352904cccd8b850fb67c.png">  - true (1) </li><li><img src="https://habrastorage.org/storage2/2d9/387/c00/2d9387c0018c4755d3133c445b3d4fb0.png">  - true (1) </li></ol><br>  Thus, we have a vector of characteristics <img src="https://habrastorage.org/storage2/f6d/26e/441/f6d26e441aea502c2ae6f9d129ca7b66.png">  .  To these five characteristics are added their inverse characteristics - <img src="https://habrastorage.org/storage2/eeb/489/b69/eeb489b69ba3d96fbc701f6cbb75f0bc.png">  .  Combining these vectors, we get the following set of binary characteristics for <img src="https://habrastorage.org/storage2/39c/a5c/36e/39ca5c36e30c20afcdc444695e2e3633.png">  and <img src="https://habrastorage.org/storage2/14d/66d/972/14d66d972a4cc4959cc7868dd3e5be51.png">  : <img src="https://habrastorage.org/storage2/b62/a37/823/b62a378233a1100ea3c2531886154a65.png">  .  The reader may have a logical question: why should the data that is obtained by negating the data that already exist in it be entered into the training sample?  Why do we need this redundancy?  Good question!  The answer to it will become clear later when creating a classifier. <br><br>  Thus, for each pair of pixels we have 10 characteristics; for the image <img src="https://habrastorage.org/storage2/893/6d8/768/8936d87687cb84cb219170d8f65843ea.png">  pixels we get <img src="https://habrastorage.org/storage2/02c/3eb/4f0/02c3eb4f0335f362cf4d3c09a587acea.png">  characteristics.  Once again, we note that the characteristics in our context are just pixel differences with the rules described above. <br><br>  Obviously, not all characteristics should be used for gender recognition.  Moreover, processing such a large number of numbers may be inefficient.  Surely, among this set of characteristics, there is a small part of those that encode gender differences better than others.  So how to choose such characteristics? <br><br>  This problem is effectively solved using the AdaBoost algorithm <a href="https://habr.com/ru/post/172463/">[2]</a> <a href="https://habr.com/ru/post/172463/">[3]</a> , the goal of which is to select a small set of valuable characteristics from a very large initial array. <br><br><h4>  Data preparation and classifier training </h4><br>  Let's start with the preparation of data.  For training and testing, we will use the FERET database <a href="https://habr.com/ru/post/172463/">[4]</a> <a href="https://habr.com/ru/post/172463/">[5]</a> .  It contains various photos of 994 people (591 men, 403 women), and, for each person, there are several photos taken in different poses. <br><br><img src="https://habrastorage.org/storage2/695/ed4/c41/695ed4c41d98d046587d4c765c5a7d16.jpg"><br><br>  On each photo you need to find the eyes, cut out the area of ‚Äã‚Äãinterest to us and reduce it to size. <nobr><img src="https://habrastorage.org/storage2/893/6d8/768/8936d87687cb84cb219170d8f65843ea.png"></nobr>  so that the result is similar to the following pattern: <br><br><img src="https://habrastorage.org/storage2/ef3/ff7/37a/ef3ff737a9f4fe22f81ee5af76a25786.jpg"><br><br>  Fortunately, the creators of FERET marked the main points on the face manually, and the eyes among them.  However, in a real recognition system, the eyes are arranged with a special algorithm, which is usually less accurate than manual placement.  Therefore, for cutting out areas of interest at the stage of learning to recognize sex, it is recommended to use the same eye placement algorithm that will be in the recognition system.  In this case, the classifier ‚Äúlearns‚Äù to take into account the alignment error of the eyes by the algorithm. <br><br>  As a result of pre-processing, we have two sets of pictures: <br><br><img src="https://habrastorage.org/storage2/72d/de6/95b/72dde695b5f03b418a651d0afb2ee805.jpg"><br><br>  Note that for clarity, each person in the figure above has dimensions <img src="https://habrastorage.org/storage2/b7f/ed3/461/b7fed3461fed3665a4244f6111c05c31.png">  pixels, in fact, in the considered approach images are used <img src="https://habrastorage.org/storage2/893/6d8/768/8936d87687cb84cb219170d8f65843ea.png">  pixels  The training set will be used to train the classifier, as the name suggests.  The test sample is ‚Äúunknown‚Äù to the classifier, so it can be used to objectively measure the accuracy indicators. <br><br>  Therefore, it is necessary <br><br><ul><li>  For each photo, compile a vector of binary characteristics (weak classifiers) containing <img src="https://habrastorage.org/storage2/644/a08/8d8/644a088d88205df6c3079aafad3fa0ba.png">  items </li><li>  ‚ÄúPack‚Äù such vectors of all photos in two matrices: <img src="https://habrastorage.org/storage2/0b0/368/fa9/0b0368fa93b553c5451a2f0f3f96b173.png">  and <img src="https://habrastorage.org/storage2/926/a5a/27d/926a5a27d6aeed95fcc8b9ec62142893.png"></li><li>  Create the corresponding output vectors in which <img src="https://habrastorage.org/storage2/bca/e6e/a96/bcae6ea9679478eafde56dbef361937a.png">  the element takes the value 1 if <img src="https://habrastorage.org/storage2/baa/258/5c2/baa2585c2feb47a415f5350e733ed3db.png">  photo belongs to a man, otherwise 0 </li></ul><br>  The figure below shows the matrix of input and output data.  Such a pair must be composed for both the training set and the test sample. <br><br>  Now we will try to understand the learning algorithm ‚Äúin words‚Äù.  A little below is the formal scheme of the AdaBoost algorithm. <br><br>  Recall that our task is to select such characteristics that can best divide the classes M and J. Suppose that we have already analyzed <img src="https://habrastorage.org/storage2/81b/df0/d2e/81bdf0d2e5028de7315599c09185db58.png">  such characteristics in order, and proceed to the analysis <nobr><img src="https://habrastorage.org/storage2/3b3/f5d/4ec/3b3f5d4ec880f7614dcadfdb40b52df1.png"></nobr>  <nobr>:</nobr> <br><br><img src="https://habrastorage.org/storage2/c66/51f/509/c6651f50938fda04d63c93c19df3a1c4.jpg"><br><br>  Meanings <img src="https://habrastorage.org/storage2/3b3/f5d/4ec/3b3f5d4ec880f7614dcadfdb40b52df1.png">  characteristics for the first three teaching examples <img src="https://habrastorage.org/storage2/df3/d21/eb3/df3d21eb3b96960fb450639c9bf9ba1e.png">  the value of the label vector for these examples <img src="https://habrastorage.org/storage2/55e/600/1ec/55e6001ec05310235d1c6b657c2535f6.png">  .  If you accept the values <img src="https://habrastorage.org/storage2/3b3/f5d/4ec/3b3f5d4ec880f7614dcadfdb40b52df1.png">  characteristics as a gender prediction, the error vector in this case will be <img src="https://habrastorage.org/storage2/5db/56d/e7d/5db56de7da7f3dbdfdaae588b9e1c408.png">  .  If we consider the characteristic <img src="https://habrastorage.org/storage2/382/0f4/225/3820f42259008bf95a6fd46264f9b31d.png">  then <img src="https://habrastorage.org/storage2/0a5/f7c/69b/0a5f7c69b1a9e5eba392d573fa471571.png">  .  Thus, choosing between characteristics <img src="https://habrastorage.org/storage2/760/b89/7d4/760b897d4cb559de34a9c7f13d394a42.png">  and <img src="https://habrastorage.org/storage2/382/0f4/225/3820f42259008bf95a6fd46264f9b31d.png">  , should prefer the characteristic <img src="https://habrastorage.org/storage2/382/0f4/225/3820f42259008bf95a6fd46264f9b31d.png">  , because <img src="https://habrastorage.org/storage2/b26/139/d11/b26139d11ab06a44cc122dbc0d4ad029.png">  .  After going through all the characteristics, we will find <i>T</i> best, which we will use for classification. <br><br>  And now the formal learning procedure. <br><br>  Teaching examples: <img src="https://habrastorage.org/storage2/513/70f/c2e/51370fc2e80d6c43ad2832ce21bc1254.png">  where <img src="https://habrastorage.org/storage2/d9e/875/1dc/d9e8751dca42bea83c1159fdf6a607b4.png">  - image, <img src="https://habrastorage.org/storage2/12a/668/43f/12a66843fedfad23cf4c543ae6ac048c.png">  in the case of the female, <img src="https://habrastorage.org/storage2/277/c53/31f/277c5331fd80c9d47d11872fdda8e1e3.png">  in the case of masculine. <br><br>  Initializing the weights vector <img src="https://habrastorage.org/storage2/e54/4f7/a77/e544f7a7744eec25e82ad4d5ea16312a.png">  : <br><br><ul><li><img src="https://habrastorage.org/storage2/ef8/85f/177/ef885f1778ba09f676aa30919e6879a8.png">  if the i-i picture is female </li><li><img src="https://habrastorage.org/storage2/d36/ba7/564/d36ba7564e513cc67a8b4ef30ea8a6f1.png">  if i-i picture is male </li></ul><br>  Where <img src="https://habrastorage.org/storage2/e6a/9f2/15d/e6a9f215de10519ba8ada2908554a79a.png">  and <img src="https://habrastorage.org/storage2/7d7/48d/388/7d748d388f8767f624bed064d779a831.png">  the number of examples of female and male, respectively. <br><br>  For <img src="https://habrastorage.org/storage2/86f/d85/423/86fd8542306b702b1ec4fbd27bfff450.png">  (where <i>T</i> is the number of characteristics we want to choose) <br><br><ol><li>  Normalization of scales, so that <img src="https://habrastorage.org/storage2/353/46f/597/35346f597e47be72bbe2f46af05d4c00.png">  . </li><li>  For each weak classifier, <img src="https://habrastorage.org/storage2/9ef/ea4/ca3/9efea4ca36a7130a9517e7a6f6ccedb9.png">  , we consider the classification error taking into account weights: <img src="https://habrastorage.org/storage2/a66/642/5a8/a666425a8b5475c36ffa2be0cb84c5e0.png">  .  For each example, the error takes on the value either 0 if the classification is correct or <img src="https://habrastorage.org/storage2/acf/fcd/5e3/acffcd5e3b0d9067fe6df69125fd09a4.png">  in case of wrong.  Because <img src="https://habrastorage.org/storage2/353/46f/597/35346f597e47be72bbe2f46af05d4c00.png">  then <img src="https://habrastorage.org/storage2/d6f/dd4/061/d6fdd4061024fb3a881e5bc3283ad016.png">  . </li><li>  Choosing a classifier <img src="https://habrastorage.org/storage2/416/e10/00e/416e1000ec8b74698bb9579f2aa8a6c3.png">  with the least error <img src="https://habrastorage.org/storage2/1a7/a98/50c/1a7a9850c2f09888b79eca555f1b0bd0.png">  and see how it classifies the data. </li><li>  Update weights. <br><br>  If the example is classified incorrectly (i.e. <img src="https://habrastorage.org/storage2/bf9/b5f/47e/bf9b5f47e65b003eda85807923fe49c3.png">  ), <br><br><img src="https://habrastorage.org/storage2/b55/24e/aad/b5524eaad455e4457aba588b03bf54fd.png">  . <br><br>  Otherwise <br><br><img src="https://habrastorage.org/storage2/030/9f5/e87/0309f5e876fa5aa852c98961fe6e4de1.png">  , <br><br>  Where <br><br><img src="https://habrastorage.org/storage2/f29/333/325/f29333325438da15794f38666ea459b8.png">  . <br></li></ol><br>  The resulting classifier is represented by the formula: <br><br><img src="https://habrastorage.org/storage2/f91/f0c/b2c/f91f0cb2ccf703b4a69990c0395cf652.png"><br><br>  Let's try to estimate the range of values ‚Äã‚Äãthat can take <img src="https://habrastorage.org/storage2/b42/1aa/219/b421aa2197737af35f5befec68c96ce8.png">  .  We have already noted that <img src="https://habrastorage.org/storage2/0e2/7f2/1b7/0e27f21b73ed711074eb9a7c64a8c517.png">  takes values ‚Äã‚Äãon a segment <img src="https://habrastorage.org/storage2/7b2/2e0/7f1/7b22e07f1c3b42863ff58e9a2a71b1d7.png">  .  Now suppose that <img src="https://habrastorage.org/storage2/e47/e54/420/e47e5442064e12170865cc6e947182b4.png">  for some classifier <img src="https://habrastorage.org/storage2/00c/6f5/b08/00c6f5b08b8b2db9b3b619f0b93a1b75.png">  .  Obviously, in this case <img src="https://habrastorage.org/storage2/c14/090/631/c1409063172c7044033e12a196b68509.png">  .  However, will such a classifier be chosen by the algorithm described above?  Obviously, it will not, because there will always be such a classifier in the training sample <img src="https://habrastorage.org/storage2/74f/943/8b8/74f9438b8955f9127d63ab3c3b2e49e4.png">  obtained by using the negation operation <img src="https://habrastorage.org/storage2/b38/59c/4d8/b3859c4d8da96545a8f7d0e835fa27be.png">  , for which <img src="https://habrastorage.org/storage2/e76/b11/c70/e76b11c70f697019a73776b07aac86dc.png">  and <img src="https://habrastorage.org/storage2/678/00f/a8f/67800fa8f442aea2eee3147db3f623e7.png">  .  It is easy to see that if the example was classified correctly, then its weight is reduced by multiplying by <img src="https://habrastorage.org/storage2/334/40d/0db/33440d0dba6854320329f4ec636b70b3.png">  , and thus, examples that are classified incorrectly (complex examples) gain more weight. <br><br>  Now back to the question of redundancy, which we asked earlier.  Presence in the sample of classifiers obtained by negation <img src="https://habrastorage.org/storage2/b38/59c/4d8/b3859c4d8da96545a8f7d0e835fa27be.png">  , limits <img src="https://habrastorage.org/storage2/b42/1aa/219/b421aa2197737af35f5befec68c96ce8.png">  on the segment <img src="https://habrastorage.org/storage2/269/1da/291/2691da2917e7be828938e5c42b41b6bb.png">  and allows weights of complex examples to grow. <br><br>  Given the size of the input matrices, the choice of the first 1000 characteristics of the standard AdaBoost algorithm takes about 10 hours (Intel Core i7).  Standard, then, running on all the characteristics.  There are versions of the algorithm that work with random characteristics from the entire set, but their description is beyond the scope of this article.  A training time of 10 hours is an acceptable result for this task, as this is a one-time operation. <br><br>  Note that the algorithm results in pairs of pixel indices to be compared, one of the five comparison rules and the value <img src="https://habrastorage.org/storage2/b42/1aa/219/b421aa2197737af35f5befec68c96ce8.png">  for each pair.  In fact, for the analysis of the photograph, it is only necessary to calculate the differences, apply the selected rule and find a solution using the formula for <img src="https://habrastorage.org/storage2/285/38c/e57/28538ce579f54ce73c3297b7ff6792b0.png">  .  Therefore, this algorithm is absolutely not demanding of resources and does not slow down the recognition system in which it is used. <br><br><h4>  results </h4><br>  Now the fun part!  What characteristics chose AdaBoost ?!  Below are the first 50 features that AdaBoost has chosen.  Red and white pixels are <img src="https://habrastorage.org/storage2/4bf/411/8d9/4bf4118d935861c2757fd91cf00dfe93.png">  and <img src="https://habrastorage.org/storage2/184/8db/539/1848db539d9c887b48ac513ae38b808f.png">  .  When analyzing pixels around the edges of an image, you must remember that the faces of many people do not fit into the frame.  Therefore, edge pixels can, in some way, encode the size of a face. <br><br><img src="https://habrastorage.org/storage2/f81/c59/54e/f81c5954ef29380927a62681bef78932.jpg"><br><br>  Consider the dependence of the classification accuracy on the number of used characteristics. <br><img src="https://habrastorage.org/storage2/eba/59f/05d/eba59f05d179c69f0a4fb8a47436d488.png"><br>  Note that the graph was obtained using data from the FERET database, which AdaBoost ‚Äúdid not see‚Äù at the training stage.  Maximum accuracy of 93.1% is achieved using 911 characteristics.  The authors of this approach report an accuracy of 94.3%, which is very close to the obtained indicator.  The difference of 1.2% may occur due to different splitting into training and test samples.  Also in this article, an own algorithm was used to search for eyes, the accuracy of which differs from the accuracy of the human arrangement of eyes in the FERET database. <br><br>  However, what if the classifier learned to recognize only the base on which it was trained ?!  A similar effect occurs when a student, preparing for an exam, simply learns formulas, tasks, examples, without understanding the essence.  Such a student is able to solve only those tasks that he has already seen, and is not able to extend his knowledge to new problems.  This effect in machine learning is called overfitting (retraining) and is a serious problem in recognition.  The opposite effect is called generalization. <br><br>  To test the ability to generalize, we will use another database of individuals - Bosphorus Database <a href="http://boshporus1/">[6]</a> <a href="http://boshporus2/">[7]</a> .  It consists of photos of 105 people.  The base contains up to 35 different facial expressions for each person. <br><br>  The graph below is similar to the graph presented a little higher, with the only difference that is obtained using the Bosphorus Database (that is, the possibility of becoming acquainted with the test sample is excluded). <br><img src="https://habrastorage.org/storage2/698/e9e/b4c/698e9eb4c25433de05ce71dc68ee9b0d.png"><br>  The test sample includes 1300 photographs (727 M and 573 F).  Maximum accuracy of 91% is achieved with 954 specifications.  Note that the mark of 90% is achieved already with 100 characteristics. <br><br>  These two graphics, as well as the videos that are present in the article, demonstrate the high accuracy of this approach on the data that he "did not see" at the training stage.  That is why it is considered the state-of-the-art algorithm for gender recognition. <br><br>  The following video shows the work of the algorithm on the fragments of TV shows.  To achieve this accuracy, ‚Äúsmoothing‚Äù of results over time is used: each face is observed for 19 frames, each of which determines the gender.  The result is the gender that has been encountered the most times in the last 19 frames. <br><br><iframe src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://player.vimeo.com/video/58832249&amp;xid=17259,15700023,15700186,15700191,15700253,15700255&amp;usg=ALkJrhgmUc6ym0QhEIXPJzHHYwK1wtR9Gw" width="560" height="315" frameborder="0" title="FaceCept Technology in Action. Gender Recognition" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe><br><br><h4>  Links </h4><br><ol><li>  S. Baluja, and H. Rowley, <a name="gender-recognition"></a>  <a href="http://www.cs.cmu.edu/afs/cs/usr/har/www/ijcv2007-sex.pdf">Boosting Sex Identification Performance</a> , International Journal of Computer Vision, v.  71 i.  1, January 2007 </li><li>  Y. Freund and R. Shapire, <a name="adaboost1"></a>  <a href="http://www.face-rec.org/algorithms/Boosting-Ensemble/decision-theoretic_generalization.pdf">‚ÄúA decision-theoretic generalization of on-line learning and an application to boosting‚Äù</a> , Journal of Computer and System Sciences, 1996 pp.  119-139 </li><li>  Y. Freund and R. Shapire, <a name="adaboost2"></a>  <a href="http://www.site.uottawa.ca/~stan/csi5387/boost-tut-ppr.pdf">‚ÄúA Short Introduction to Boosting‚Äù</a> , Journal of Japanese Society for Artificial Intelligence, 14 (5): 771-780, September, 1999 </li><li>  PJ Phillips, H. Moon, SA Rizvi and PJ Rauss, <a name="feret1"></a>  <a href="http://www.nist.gov/humanid/feret/doc/FERET_PAMI_Oct_2000.pdf">"The FERET evaluation methodology for face recognition algorithms,"</a> IEEE Trans.  Pattern Analysis and Machine Intelligence, Vol.  22, pp.  1090-1104, 2000 </li><li><a name="feret2"></a>  <a href="http://www.itl.nist.gov/iad/humanid/feret/feret_master.html">http://www.itl.nist.gov/iad/humanid/feret/feret_master.html</a> </li><li><a name="boshporus1"></a>  <a href="http://bosphorus.ee.boun.edu.tr/Home.aspx">Bosphorus Database</a> </li><li><a name="boshporus2"></a>  A. Savran, B. Sankur, MT Bilge, ‚ÄúComparative Evaluation of the Universe‚Äù, Pattern Recognition, Vol.  45, Issue 2, p767-782, 2012. <br></li></ol></div><p>Source: <a href="https://habr.com/ru/post/172463/">https://habr.com/ru/post/172463/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../172405/index.html">Radio86RK - Soviet homemade computer</a></li>
<li><a href="../172409/index.html">Life difficulties of independent developers</a></li>
<li><a href="../172413/index.html">Poll. Do you align code with columns?</a></li>
<li><a href="../172417/index.html">Recommender system: useful text mining tasks</a></li>
<li><a href="../172419/index.html">Creating a video broadcast on JS</a></li>
<li><a href="../172465/index.html">Boeing 787 and 500 Gb or in brief about the means of objective control</a></li>
<li><a href="../172467/index.html">Intel division has released an Android version optimized for Intel chips (with UEFI support)</a></li>
<li><a href="../172469/index.html">Shooting Time Lapse video for Android</a></li>
<li><a href="../172473/index.html">Distributed rendering</a></li>
<li><a href="../172475/index.html">Check input parameters or indirect references to BASH</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>