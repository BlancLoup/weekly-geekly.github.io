<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Segmentation Fault (computer memory allocation)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="When I make a mistake in the code, it usually results in the message ‚Äúsegmentation fault‚Äù, often shortened to ‚Äúsegfault‚Äù. And then my colleagues and m...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Segmentation Fault (computer memory allocation)</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/225/4da/58f/2254da58f07d4d69b4733304c85989e2.jpg"><br><br>  When I make a mistake in the code, it usually results in the message ‚Äúsegmentation fault‚Äù, often shortened to ‚Äúsegfault‚Äù.  And then my colleagues and management come to me: ‚ÄúHa!  We have a segfault for you to fix! ‚Äù-‚Äú Well, yes, it‚Äôs my fault, ‚ÄùI usually reply.  But how many of you know what the segmentation fault actually means? <br><br>  To answer this question, we need to go back to the distant 1960s.  I want to explain how a computer works, or more precisely, how memory is accessed in modern computers.  This will help you understand where this strange error message comes from. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      All the information below is the basics of computer architecture.  And without need, I will not go deep into this area.  Also, I will apply the well-known terminology to everyone, so that my post will be clear to everyone who is not entirely on ‚Äúyou‚Äù with computer technology.  If you want to study the issue of working with memory in more detail, you can refer to the numerous available literature.  And at the same time do not forget to delve into the source code of the kernel of some OS, for example, Linux.  I will not present here the history of computing technology, some things will not be covered, and some are greatly simplified. <br><a name="habracut"></a><br><h3>  A bit of history </h3><br>  Once computers were very large, weighed tons, while having one processor and memory of about 16 KB.  Such a monster cost about $ 150,000 and could perform only one task at a time: only one process was performed at a time.  The architecture of memory in those days can be schematically represented as follows: <br><br><img src="https://habrastorage.org/files/bfe/bb1/36e/bfebb136e70248acbb61051288c35d5d.png"><br><br>  That is, the OS accounted for, say, a quarter of all available memory, and the rest of the volume was given to user tasks.  At that time, the role of the OS was to simply control the hardware using interrupts of the CPU.  So OSes needed memory for themselves, for copying data from devices and for working with them ( <a href="https://ru.wikipedia.org/wiki/PIO">PIO mode</a> ).  To display data on the screen, it was necessary to use part of the main memory, because the video subsystem either did not have its own RAM, or it had read kilobytes.  And the program itself was executed in the area of ‚Äã‚Äãmemory that goes right after the OS, and solved its tasks. <br><br><h3>  Sharing resources </h3><br>  The main problem was that the device, worth $ 150,000, was single-tasking and spent whole days processing several kilobytes of data. <br><br>  Due to the exorbitant cost, few could afford to purchase several computers at once to process several tasks at the same time.  Therefore, people began to look for ways to share access to computing resources of a single computer.  This is the era of multitasking.  Please note that in those days no one even thought about multiprocessor computers.  So how can you make a computer with one CPU do several different tasks? <br><br>  The solution was to use the task scheduler (scheduling): while one process was interrupted, waiting for the completion of I / O operations, the CPU could execute another process.  I‚Äôm not going to touch the task scheduler here anymore, this is too broad a topic, unrelated to memory. <br><br>  If the computer is able to perform several tasks in turn, then the memory allocation will look like this: <br><br><img src="https://habrastorage.org/files/3f0/95a/c50/3f095ac509e14d7b8b172edc23f318d7.png"><br><br>  Tasks A and B are stored in memory, since copying them to disk and back is too expensive.  And as the processor performs a particular task, it accesses the memory for the appropriate data.  But there is a problem. <br><br>  When one programmer writes code to perform task B, he must know the boundaries of the allocated memory segments.  Suppose task B occupies a segment from 10 to 12 Kb in memory, then each memory address must be hard-coded within these limits.  But if the computer performs three tasks at once, the memory will be divided into more segments, and therefore the segment for task B may be shifted.  Then the program code will have to be rewritten so that it can operate with a smaller memory size, and also change all pointers. <br><br>  Here another problem emerges: what if task B addresses the memory segment allocated for task A?  This can easily happen, because when working with memory pointers it is enough to make a small mistake, and the program will access a completely different address, violating the integrity of the data of another process.  At the same time, task A can work with very important data from the point of view of security.  There is no way to prevent B from invading memory A. Finally, due to a programmer‚Äôs error, task B can overwrite the OS memory (in this case, from 0 to 4 KB). <br><br><h3>  Address space </h3><br>  To be able to safely perform several tasks stored in memory, we need help from the OS and hardware.  In particular, the address space.  This is a kind of memory abstraction allocated by the OS for some process.  Today it is a fundamental concept that is used everywhere.  At least, in ALL civilian computers, this approach is adopted, and the military can have its own secrets.  Personal computers, smartphones, TVs, game consoles, smart watches, ATMs - poke into any device, and it turns out that the memory allocation in it is carried out according to the code-stack-heap (code-stack-heap) principle. <br><br>  The address space contains everything you need to perform the process: <br><br><ul><li>  Machine instructions to be executed by the CPU. </li><li>  The data with which these machine instructions will work. </li></ul><br>  Schematically, the address space is divided as follows: <br><br><img src="https://habrastorage.org/files/821/413/655/8214136557724a4babfdb7fccf95a419.png"><br><br><ul><li>  A stack is a memory area in which a program stores information about called functions, their arguments, and each local variable in functions.  The size of the area may vary as the program runs.  When calling functions, the stack grows, and when completed, it decreases. </li><li> A heap is a memory area in which a program can do whatever it wants.  The size of the area may vary.  The programmer has the opportunity to use part of the heap memory with the help of the <code>malloc()</code> function, and then this memory area is increased.  Returning resources is done using <code>free()</code> , after which the heap is reduced. </li><li>  The code segment (code) is the area of ‚Äã‚Äãmemory in which the machine instructions of the compiled program are stored.  They are generated by the compiler, but can also be written manually.  Please note that this memory area can also be divided into three parts (text, data and BSS).  This memory area has a fixed size determined by the compiler.  In our example, let it be 1 KB. </li></ul><br>  Since the stack and the heap can vary in size, they are placed in opposite parts of the common address space.  The directions for changing their sizes are shown by arrows.  The OS‚Äôs responsibility is to ensure that these areas do not overlap each other. <br><br><h3>  Memory virtualization </h3><br>  Suppose task A has at its disposal all the available user memory.  And then there is a problem B. How to be?  The solution was found in <b>virtualization</b> . <br><br>  Let me remind you of one of the previous illustrations, when A and B are in memory at the same time: <br><br><img src="https://habrastorage.org/files/cd4/d9c/d3d/cd4d9cd3db824bd79f40ee681e015c20.png"><br><br>  Suppose A is trying to access memory in its own address space, for example, using the index 11 Kb.  It may even be her own stack.  In this case, the OS needs to figure out how not to load the index 1500, because in fact it can point to the area of ‚Äã‚Äãtask B. <br><br>  In fact, the address space that each program considers its memory is <b>virtual memory</b> .  <i>Fake</i>  And in the memory area of ‚Äã‚Äãtask A, the 11 Kb index will be a fake address.  That is, the address of virtual memory. <br><br>  <b>Each program running on a computer works with fake (virtual) memory</b> .  With the help of some chips, the OS is deceiving the process when it accesses any area of ‚Äã‚Äãmemory.  Thanks to virtualization, no process can access memory that does not belong to it: task A will not fit into the memory of task B or the OS itself.  At the same time, at the user level, everything is absolutely transparent, thanks to the extensive and complex code of the OS kernel. <br><br>  Thus, each memory access is controlled by the operating system.  And this should be done very effectively, so as not to slow down the work of various running programs.  Efficiency is ensured by hardware, primarily the CPU and some components like the <a href="https://ru.wikipedia.org/wiki/%25D0%2591%25D0%25BB%25D0%25BE%25D0%25BA_%25D1%2583%25D0%25BF%25D1%2580%25D0%25B0%25D0%25B2%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D1%258F_%25D0%25BF%25D0%25B0%25D0%25BC%25D1%258F%25D1%2582%25D1%258C%25D1%258E">MMU</a> .  The latter appeared as a separate chip in the early 1970s, and today MMUs are embedded directly into the processor and are mandatory used by the operating systems. <br><br>  Here is a small C program that shows how to work with memory addresses: <br><br><pre> <code class="cs hljs"><span class="hljs-meta"><span class="hljs-meta">#include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; int main(int argc, char **argv) { int v = 3; printf("Code is at %p \n", (void *)main); printf("Stack is at %p \n", (void *)&amp;v); printf("Heap is at %p \n", malloc(8)); return 0; }</span></span></code> </pre><br>  On my LP64 X86_64 machine, it shows the following result: <br><br> <code>Code is at 0x40054c</code> <br> <code>Stack is at 0x7ffe60a1465c</code> <br> <code>Heap is at 0x1ecf010</code> <br> <br>  As I described, the code segment first goes, then the heap, and then the stack.  But all these three addresses are fake.  The physical memory at 0x7ffe60a1465c does not store an integer variable with a value of 3. Never forget that all user programs manipulate virtual addresses, and only at the kernel level or hardware drivers are allowed to use physical memory addresses. <br><br><h3>  Call forwarding </h3><br>  Redirection (translation, translation, address translation) is the term for the process of matching a virtual address to a physical one.  The MMU module does this.  For each executing process, the OS should keep in mind that all virtual addresses match the physical ones.  And this is quite an easy task.  In fact, the OS has to manage the memory of each user process with each call.  Thus, it transforms the nightmarish reality of physical memory into a useful, powerful and easy-to-use abstraction. <br><br>  Let's take a closer look. <br><br>  When the process starts, the OS reserves a fixed amount of physical memory for it, let it be 16 KB.  The starting address of this address space is stored in a special variable <code>base</code> .  And in the <code>bounds</code> variable, the size of the allocated memory is recorded, in our example, 16 KB.  These two values ‚Äã‚Äãare written to each process table - PCB ( <a href="https://en.wikipedia.org/wiki/Process_control_block">Process Control Block</a> ). <br><br>  So this is the virtual address space: <br><br><img src="https://habrastorage.org/files/821/413/655/8214136557724a4babfdb7fccf95a419.png"><br><br>  And this is his physical image: <br><br><img src="https://habrastorage.org/files/d02/cb8/93e/d02cb893e4854879bb3bbbb3bb62523d.png"><br><br>  The OS decides to allocate a range of physical addresses from 4 to 20 Kb, that is, the <code>base</code> value is 4 Kb, and the <code>bounds</code> value is 4 + 16 = 20 Kb.  When a process is queued for execution (CPU time is allocated to it), the OS reads the values ‚Äã‚Äãof both variables from the PCB and copies them to special CPU registers.  Then the process starts and tries to access, say, a virtual address of 2 KB (in its heap).  To this address, the CPU adds the <code>base</code> value received from the OS.  Therefore, the physical address will be 2 + 4 = 6 Kb. <br><br>  <b>Physical address = virtual address + base</b> <br><br>  If the resulting physical address (6 KB) gets out of the boundaries of the selected area (4-20 KB), this means that the process is trying to access memory that does not belong to it.  The CPU then generates an exception and reports the OS, which handles the exception.  In this case, the system usually signals a violation to the process: <a href="https://ru.wikipedia.org/wiki/SIGSEGV">SIGSEGV</a> , Segmentation Fault.  This signal by default interrupts the execution of the process (this can be customized). <br><br><h3>  Memory reallocation </h3><br>  If task A is excluded from the execution queue, then it is even better.  This means that the scheduler was asked to perform another task (say, B).  While B is running, the OS can reallocate all the physical space of task A. During the execution of the user process, the OS often loses control of the processor.  But when the process makes a system call, the processor returns to the control of the OS.  Prior to this system call, the OS can do anything with memory, including completely redistributing the address space of the process to another physical partition. <br><br>  In our example, this is quite simple: the OS moves the 16-kilobyte area to another free space of a suitable size and simply updates the values ‚Äã‚Äãof the base and bounds variables for task A. When the processor returns to its execution, the redirection process still works, but the physical address space is already has changed. <br><br>  From the point of view of task A, nothing changes, its own address space is still located in the range of 0‚Äì16 Kb.  At the same time, the OS and the MMU fully control every access to the memory task.  That is, the programmer manipulates the virtual area of ‚Äã‚Äã0-16 KB, and the MMU takes on the mapping to the physical addresses. <br><br>  After redistribution, the memory image will look like this: <br><br><img src="https://habrastorage.org/files/2f8/f91/fa9/2f8f91fa9ea742afb5df2e9ab48a54f0.png"><br><br>  The programmer no longer needs to worry about which memory addresses his program will work with, no need to worry about conflicts.  OS in conjunction with the MMU relieve him of all these concerns. <br><br><h3>  Memory segmentation </h3><br>  In previous chapters, we addressed the issues of redirection and memory redistribution.  However, our memory model has several drawbacks: <br><br><ul><li>  We assume that each virtual address space has a size of 16 KB.  It has nothing to do with reality. </li><li>  The OS has to maintain a list of 16 KB of free ranges of physical memory in order to allocate them for new processes to be launched or to redistribute the currently selected areas.  How can you effectively do all this without degrading the performance of the entire system? </li><li>  We allocate 16 Kb each process, but it‚Äôs not a fact that each of them will use the entire selected area.  So we just lose a lot of memory from scratch.  This is called internal fragmentation ‚Äî memory is reserved but not used. </li></ul><br><br>  To solve some of these problems, let's consider a more complex memory organization system - segmentation.  Its meaning is simple: the ‚Äúbase and bounds‚Äù principle extends to all three memory segments - a heap, a code segment and a stack, and for each process, instead of considering the memory image as a single unique entity. <br><br>  As a result, we no longer lose memory between the stack and the heap: <br><br><img src="https://habrastorage.org/files/1af/3ac/4e9/1af3ac4e95554b3ea80ca50a598be4b8.png"><br><br>  As you can see, the free space in the virtual memory of task A is no longer located in physical memory.  And memory is now used much more efficiently.  The OS now has to memorize three <code>base</code> and <code>bounds</code> pairs for each task, one for each segment.  MMU, as before, is engaged in redirection, but it already operates with three <code>base <br></code> <code>base <br></code>  and three <code>bounds</code> . <br><br>  Suppose that for task heap A, the <code>base</code> parameter is 126 Kb, and bounds is 2 Kb.  Let task A refer to a virtual address of 3 Kb (in a heap).  Then the physical address is defined as 3 - 2 Kb (beginning of the heap) = 1 Kb + 126 Kb (shift) = 127 Kb.  This is less than 128, which means there will be no handling error. <br><br><h3>  Segment sharing </h3><br>  Segmentation of physical memory not only does not allow virtual memory to eat off physical memory, but also makes it possible to share physical segments using virtual address spaces of different processes. <br><br>  If you run task A twice, they will have the same code segment: the same machine instructions are executed in both tasks.  At the same time, each task will have its own stack and heap, since they operate on different data sets. <br><br><img src="https://habrastorage.org/files/62a/802/ce8/62a802ce8fef4d7fa288a2347acee5af.png"><br><br>  At the same time, both processes do not suspect that they share their memory with someone.  This approach was made possible by the introduction of segment protection bits (segment protection bits). <br><br>  For each physical segment of the OS being created, it registers the <code>bounds</code> value, which is used by the MMU for subsequent redirection.  But at the same time, the so-called permission flag is also registered. <br><br>  Since the code itself cannot be modified, all code segments are created with RX flags.  This means that the process can load this area of ‚Äã‚Äãmemory for later execution, but no one can write to it.  The other two segments, heap and stack, have RW flags, that is, the process can read and write to these two of its segments, but the code cannot be executed from them.  This is done to ensure security, so that an attacker could not damage the heap or stack by incorporating his code into them to gain root rights.  This was not always the case, and hardware support is required for the high efficiency of this solution.  In Intel processors, this is called ‚Äú <a href="https://ru.wikipedia.org/wiki/NX_bit">NX bit</a> ‚Äù. <br><br>  Flags can be changed during the program execution, for this purpose <a href="http://man7.org/linux/man-pages/man2/mprotect.2.html">mprotect () is used</a> . <br><br>  Under Linux, all these memory segments can be viewed using the <i>/ proc / {pid} / maps</i> or <i>/ usr / bin / pmap utilities</i> . <br><br>  Here is an example in PHP: <br><br><pre> <code class="php hljs">$ pmap -x <span class="hljs-number"><span class="hljs-number">31329</span></span> <span class="hljs-number"><span class="hljs-number">0000000000400000</span></span> <span class="hljs-number"><span class="hljs-number">10300</span></span> <span class="hljs-number"><span class="hljs-number">2004</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> rx-- php <span class="hljs-number"><span class="hljs-number">000000000100e000</span></span> <span class="hljs-number"><span class="hljs-number">832</span></span> <span class="hljs-number"><span class="hljs-number">460</span></span> <span class="hljs-number"><span class="hljs-number">76</span></span> rw--- php <span class="hljs-number"><span class="hljs-number">00000000010</span></span>de000 <span class="hljs-number"><span class="hljs-number">148</span></span> <span class="hljs-number"><span class="hljs-number">72</span></span> <span class="hljs-number"><span class="hljs-number">72</span></span> rw--- [ anon ] <span class="hljs-number"><span class="hljs-number">000000000197</span></span>a000 <span class="hljs-number"><span class="hljs-number">2784</span></span> <span class="hljs-number"><span class="hljs-number">2696</span></span> <span class="hljs-number"><span class="hljs-number">2696</span></span> rw--- [ anon ] <span class="hljs-number"><span class="hljs-number">00007</span></span>ff772bc4000 <span class="hljs-number"><span class="hljs-number">12</span></span> <span class="hljs-number"><span class="hljs-number">12</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> rx-- libuuid.so<span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span> <span class="hljs-number"><span class="hljs-number">00007</span></span>ff772bc7000 <span class="hljs-number"><span class="hljs-number">1020</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> ----- libuuid.so<span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span> <span class="hljs-number"><span class="hljs-number">00007</span></span>ff772cc6000 <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span> rw--- libuuid.so<span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span> ... ...</code> </pre><br>  It has all the necessary details regarding the memory allocation.  Virtual addresses, permissions for each memory area are displayed.  Each shared object (.so) is located in the address space in the form of several parts (usually code and data).  Code segments are executable and are shared in physical memory by all processes that have placed a similar shared object in their address space. <br><br>  Shared Objects is one of the biggest advantages of Unix and Linux systems, providing memory savings. <br><br>  Also, using the <a href="http://man7.org/linux/man-pages/man2/mmap.2.html">mmap ()</a> system call, you can create a shared area that is converted to a shared physical segment.  Then each region will have an index s, meaning shared. <br><br><h3>  Segmentation constraints </h3><br>  So, the segmentation solved the problem of unused virtual memory.  If it is not used, it is not placed in physical memory due to the use of segments that correspond to the amount of memory used. <br><br>  But this is not entirely true. <br><br>  Let's say the process requested 16 kb from the heap.  Most likely, the OS will create a segment of the appropriate size in physical memory.  If the user then releases 2 KB from them, then the OS will have to reduce the size of the segment to 14 KB.  But what if the programmer then asks for another 30 KB from the heap?  Then the previous segment should be increased more than twice, and is it possible to do this?  Perhaps, it is already surrounded by other segments that do not allow it to increase.  Then the OS will have to look for 30 KB of free space and redistribute the segment. <br><br><img src="https://habrastorage.org/files/085/7c6/502/0857c6502c244c8c9a2adad7779d2cff.png"><br><br>  The main drawback of the segments is that because of them the physical memory is very fragmented, as the segments grow and shrink as user processes request and free memory.  And the OS has to maintain a list of free sites and manage them. <br><br>  Fragmentation can lead to the fact that some process will request a memory size that will be larger than any of the free sections.  And in this case, the OS will have to refuse the process of allocating memory, even if the <b>total</b> amount of free areas will be significantly larger. <br><br>  The OS may try to place the data more compactly, combining all the free areas into one big chunk, which can later be used for the needs of new processes and redistribution. <br><br><img src="https://habrastorage.org/files/b2c/a4b/382/b2ca4b38231f478385fbc5eb7e26be78.png"><br><br>  But such optimization algorithms heavily load the processor, and yet its power is needed to perform user processes.  If the OS begins to reorganize the physical memory, the system becomes inaccessible. <br><br>  So the segmentation of memory entails a lot of problems associated with memory management and multitasking.  It is necessary to somehow improve the segmentation capabilities and correct the deficiencies.  This is achieved through another approach - virtual memory pages. <br><br><h3>  Memory pagination </h3><br>  As mentioned above, the main disadvantage of segmentation is that the segments very often change their size, and this leads to memory fragmentation, which may cause a situation when the OS does not allocate the necessary memory for processes.  This problem is solved with the help of pages: each location that the kernel makes in physical memory has a fixed size.  That is, pages are areas of physical memory of a fixed size, nothing more.  This greatly simplifies the task of managing free volume and eliminates fragmentation. <br><br>  Let's look at an example: a 16 KB virtual address space is paginated. <br><br><img src="https://habrastorage.org/files/1c4/a21/aaf/1c4a21aaf63d43098d5598bdfb03f604.png"><br><br>  We are not talking about a heap, stack, or code segment.  Just divide the memory into pieces of 4 KB.  Then we do the same with physical memory: <br><br><img src="https://habrastorage.org/files/3fa/03c/b15/3fa03cb156714373aa7cae6c60897cf3.png"><br><br>  The OS stores the process page table (process page table), which presents the relationship between the process virtual memory page and the physical memory page (page frame). <br><br><img src="https://habrastorage.org/files/190/b9e/41b/190b9e41ba3643d88bc36a95564ef211.png"><br><br>  Now we have got rid of the problem of searching for free space: the page frame is either used or not (unused).  Unlike the kernel, it is easier to find a sufficient number of pages to fulfill the process request for memory allocation. <br><br>  A page is the smallest and indivisible unit of memory that the OS can operate on. <br><br>  Each process has its own page table, in which redirection is presented.  Here not the values ‚Äã‚Äãof the borders of the region are used, but the number of the virtual page (VPN, virtual page number) and shift (offset). <br><br>  Example: the size of the virtual space is 16 KB, therefore, we need 14 bits to describe the addresses (2 <sup>14</sup> = 16 KB).  The page size is 4 Kb, which means we need 4 Kb (16/4) to select the desired page: <br><br><img src="https://habrastorage.org/files/d46/e20/c93/d46e20c937fb4ead85963b80f58bdef1.png"><br><br>  When a process wants to use, for example, the address 9438 (outside the boundaries of 16,384), it requests in binary code 10.0100.1101.1110: <br><br><img src="https://habrastorage.org/files/af7/073/923/af70739235064f0b95a390bd98564d1f.png"><br><br>  This is the 1246th byte in the virtual page number 2 ("0100.1101.1110" -th byte in the "10" -th page).  Now the OS simply needs to refer to the process page table to find this page number 2. In our example, it corresponds to an eight-thousandth byte of physical memory.  Therefore, the virtual address 9438 corresponds to the physical address 9442 (8000 + offset 1246). <br><br>  As already mentioned, each process has only one page table, since each process has its own redirection, as in the case of segments.  But where exactly are all these tables stored?  Probably, in physical memory, where else can it be? <br><br>  If the page tables themselves are stored in memory, then you need to access memory in order to receive a VPN.  Then the number of calls to it is doubled: first, we retrieve the number of the desired page from memory, and then we turn to the data itself stored in this page.  And if the speed of access to memory is small, then the situation looks pretty sad. <br><br><h3>  Fast forwarding buffer (TLB, Translation-lookaside Buffer) </h3><br>  Using pages as the primary tool for maintaining virtual memory can lead to severe performance degradation.  Splitting the address space into small pieces (pages) requires storing a large amount of data on the placement of pages.  And since this data is stored in memory, then each time the process accesses the memory, one more, additional access is performed. <br><br>  Equipment maintenance is used again to maintain performance.  As with segmentation, we use hardware methods to help the kernel to efficiently perform redirects.  To do this, use the TLB, which is part of the MMU, and is a simple cache for some VPN redirects. TLB        ,      . <br><br>  MMU      ,     VPN    TLB,        VPN.  ,    .  ,  MMU     ,       ,     TLB,       . <br><br>   ,      ,      .  ,     ,   ,   TLB   .          .     - .       . , Linux   ¬´¬ª   2    4 . <br><br>     ,    .       ,     TLB     ,     .      (spacial locality efficiency): ,       ,       ,    TLB     . <br><br>  , TLB       ASID (Address Space Identifier,   ).    PID, <a href="https://ru.wikipedia.org/wiki/%25D0%2598%25D0%25B4%25D0%25B5%25D0%25BD%25D1%2582%25D0%25B8%25D1%2584%25D0%25B8%25D0%25BA%25D0%25B0%25D1%2582%25D0%25BE%25D1%2580_%25D0%25BF%25D1%2580%25D0%25BE%25D1%2586%25D0%25B5%25D1%2581%25D1%2581%25D0%25B0"> </a> .  ,     ,   ASID,  TLB       ,        . <br><br>  :        ,      TLB. ,        .    ,     .  86-    4 ,      .       ,     ,    (¬´ ¬ª, dirty bit),   (protection bit),   (reference bit)  ..      ,      SIGSEGV,     ‚Äúsegmentation fault‚Äù,         . <br><br>             ,   .  ,      ,  ,   (page eviction),    ¬´¬ª (        ,             ). <br><br><h3>  Conclusion </h3><br>   ,     ‚Äúsegmentation fault‚Äù.            .       ,    MMU  .     , ‚Äî     ,        (   read only-), ‚Äî       SIGSEGV,          ‚Äúsegmentation fault‚Äù.  -     ‚ÄúGeneral protection fault‚Äù.      Linux  86/64-,   <a href="">   </a> ,   ‚Äî  <a href="">SIGSEGV</a> .   ,      <a href=""></a> .          ,    ,     . </div><p>Source: <a href="https://habr.com/ru/post/277759/">https://habr.com/ru/post/277759/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../277743/index.html">IP PBX Askozia and Push Notifications in Telegram</a></li>
<li><a href="../277747/index.html">Anatomy of ransomware for Android, part 1</a></li>
<li><a href="../277751/index.html">How our techies in the army served</a></li>
<li><a href="../277753/index.html">How to color the Chrome tab</a></li>
<li><a href="../277757/index.html">Convenient conversion of enumerations (enum) to string in C ++</a></li>
<li><a href="../277761/index.html">The digest of interesting materials for the mobile # 141 developer (February 15-23)</a></li>
<li><a href="../277763/index.html">Manage enterprise iOS devices using OS X Server, as well as distributing applications within the company</a></li>
<li><a href="../277765/index.html">Distributing iOS apps by reference in a corporate environment using the Microsoft technology stack</a></li>
<li><a href="../277771/index.html">Open Android courses StudyJams throughout Russia</a></li>
<li><a href="../277773/index.html">10 theses indie development, which led to success</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>