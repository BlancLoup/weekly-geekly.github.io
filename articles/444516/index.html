<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Wolfenstein 3D: ray tracing with WebGL1</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="After the appearance of Nvidia RTX graphics cards last summer, ray tracing has once again gained its former popularity. Over the past few months, my T...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Wolfenstein 3D: ray tracing with WebGL1</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/885/df3/33c/885df333c0da45fcbb909c01c5da9648.png" alt="image"></div><br>  After the appearance of Nvidia RTX graphics cards last summer, ray tracing has once again gained its former popularity.  Over the past few months, my Twitter feed has filled an endless stream of graphics comparisons with RTX turned on and off. <br><br>  After admiring such a number of beautiful images, I wanted to independently try to combine the classic forward renderer with the ray tracer. <br><br>  Suffering <a href="https://en.wikipedia.org/wiki/Not_invented_here">from the rejection syndrome of someone else's development</a> , I as a result created my own rendering engine based on WebGL1.  You can play with the Wolfenstein 3D level rendering demo with spheres (which I used due to ray tracing) <a href="https://reindernijhoff.net/wolfrt">here</a> . <br><a name="habracut"></a><br><h3>  Prototype </h3><br>  I started this project by creating a prototype, trying to recreate <a href="http://www.4a-games.com.mt/4a-dna/2018/9/19/the-tech-behind-metro-exodus-geforce-rtx-real-time-ray-traced-global-illumination">global illumination with ray tracing from Metro Exodus</a> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/262/61a/447/26261a447cd19758715a3cbde356eeb7.png"></div><br>  <i>First prototype showing diffused global illumination (Diffuse GI)</i> <br><br>  The prototype is based on a forward renderer that renders the entire geometry of the scene.  The shader used for rasterization of the geometry not only calculates direct illumination, but also emits random rays from the surface of the rendered geometry for accumulation of indirect rays of reflection from non-shiny surfaces (Diffuse GI) using the tracer. <br><br>  In the image above you can see how all the spheres are correctly illuminated only by indirect illumination (rays of light are reflected from the wall behind the camera).  The light source itself is covered by a brown wall on the left side of the image. <br><br><h3>  Wolfenstein 3d </h3><br>  The prototype uses a very simple scene.  It has only one light source and only a few spheres and cubes are rendered.  Because of this, the ray tracing code in the shader is very simple.  A rough loop through the intersection check, in which the beam is tested for intersection with all the cubes and spheres in the scene, is still fast enough for the program to perform in real time. <br><br>  After creating this prototype, I wanted to do something more complicated by adding more geometry and lots of light sources to the scene. <br><br>  The problem with a more complex environment is that I still need to be able to trace the rays in the scene in real time.  Normally, the structure of the <a href="https://en.wikipedia.org/wiki/Bounding_volume_hierarchy">bounding volume hierarchy</a> (BVH) would be used to speed up the ray tracing process, but my decision to create this project on WebGL1 did not allow this: WebGL1 cannot load 16-bit data into texture and cannot use binary operations in a shader.  This complicates the preliminary calculation and application of BVH in WebGL1 shaders. <br><br>  That is why I decided to use Wolfenstein 3D for this demo.  In 2013, I created a single <a href="https://www.shadertoy.com/view/4sfGWX">fragmentary WebGL shader</a> in <a href="https://www.shadertoy.com/">Shadertoy</a> , which not only renders Wolfenstein-like levels, but also procedurally creates all the necessary textures.  From my experience with this shader, I knew that the grid-based construction of Wolfenstein levels can also be used as a fast and simple acceleration structure, and that ray tracing along this structure will be very fast. <br><br>  Below is a screenshot of the demo, and in full screen mode you can play it here: <a href="https://reindernijhoff.net/wolfrt">https://reindernijhoff.net/wolfrt</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/eu/gl/fp/euglfpjgl9503fy-_hjdwmr5hdi.png"></div><br><h3>  Short description </h3><br>  The demo uses a hybrid rendering engine.  To render all polygons in a frame, it uses traditional rasterization, and then combines the result with shadows, diffuse GI and reflections created by ray tracing. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8f7/fd8/bb9/8f7fd8bb9324905edf5200fedb28bb7a.png"></div><br>  <i>Shadows</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/524/5b5/6ba/5245b56ba3a1feb02a9401857d625514.png"></div><br>  <i>Diffuse gi</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/885/df3/33c/885df333c0da45fcbb909c01c5da9648.png"></div><br>  <i>Reflections</i> <br><br><h3>  Proactive rendering </h3><br>  Wolfenstein maps can be fully encoded into a 64 √ó 64 two-dimensional grid.  The map used in the demo is based on the <a href="https://wolfenstein.fandom.com/wiki/Episode_1/Floor_1">first level of</a> Wolfenstein 3D <a href="https://wolfenstein.fandom.com/wiki/Episode_1/Floor_1">episode 1</a> . <br><br>  At launch, all geometry is created that is necessary for the advance rendering rendering.  A wall mesh is generated from the map data.  There are also floor and ceiling planes, separate meshes for lighting sources, doors and randomly-spaced spheres. <br><br>  All textures used for walls and doors are packed into a single texture atlas, so all walls can be drawn in one draw call. <br><br><h4>  Shadows and lighting </h4><br>  Direct illumination is computed in the shader used for the pre-render rendering.  Each fragment can be illuminated (maximum) by four different sources.  To know which sources can influence the fragment in the shader, when you run the demo, the search texture is precomputed.  This search texture is 64 by 128 and encodes the positions of the 4 closest light sources for each position in the map grid. <br><br><pre><code class="cpp hljs">varying vec3 vWorldPos; varying vec3 vNormal; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">main</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">void</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function"> </span></span>{ vec3 ro = vWorldPos; vec3 normal = normalize(vNormal); vec3 light = vec3(<span class="hljs-number"><span class="hljs-number">0</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i=<span class="hljs-number"><span class="hljs-number">0</span></span>; i&lt;LIGHTS_ENCODED_IN_MAP; i++) { light += sampleLight(i, ro, normal); }</code> </pre> <br>  To obtain soft shadows for each fragment and light source, a random position in the light source is sampled.  Using the ray-tracing code in the shader (see below, ‚ÄúRay-tracing‚Äù), a shadow beam is emitted to the sampling point to determine the visibility of the light source. <br><br>  After adding (auxiliary) reflections (see the ‚ÄúReflection‚Äù section below), the diffuse GI is added to the calculated fragment color by performing a search in the Diffuse GI Render Target (see below). <br><br><h3>  Ray tracing </h3><br>  Although in the prototype the ray-tracing code for the diffuse GI was combined with the pre-emptive shader, I decided to separate them in the demo. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/58b/8fd/3ab/58b8fd3abd38a86da42e1454df7109de.png"></div><br>  I divided them by performing the second drawing of the whole geometry into a separate render target (Diffuse GI Render Target) using another shader that only emits random rays to collect the diffuse GI (see below the section ‚ÄúDiffuse GI‚Äù).  The illumination collected in this render target is added to the direct illumination calculated in the aisle of the anticipating render. <br><br>  By separating the preemptive pass and diffuse GI, we can emit less than one diffuse GI beam per screen pixel.  This can be done by reducing the Buffer Scale (by moving the slider in the parameters in the upper right corner of the screen). <br><br>  For example, if the Buffer Scale is 0.5, then only one beam will be emitted for every four screen pixels.  This gives a huge performance boost.  Using the same UI in the upper right corner of the screen, you can also change the number of samples per pixel in the render target (SPP) and the number of reflections of the beam. <br><br><h4>  Emit beam </h4><br>  To be able to emit rays into the scene, all level geometry must be in a format that the ray tracer can use in the shader.  The Wolfenstein level is encoded with a 64 √ó 64 grid, so it is easy enough to encode all the data into a single 64 √ó 64 texture: <br><br><ul><li>  In the red channel of the texture, all the objects in the corresponding cell of the <em>x, y</em> grid of the map are encoded.  If the value of the red channel is zero, then there are no objects in the cell, otherwise, it is occupied by a wall (values ‚Äã‚Äãfrom 1 to 64), a door, a light source or a sphere that need to be checked for intersection. </li><li>  If a level grid cell occupies a sphere, then the green, blue and alpha channel are used to encode the radius and relative <em>x</em> and <em>y</em> coordinates of the sphere inside the grid cell. </li></ul><br>  Emitting a beam in a scene is performed by traversing the texture with the following code: <br><br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">bool</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">worldHit</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n vec3 ro,in vec3 rd,in </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> t_min, in </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> t_max, inout vec3 recPos, inout vec3 recNormal, inout vec3 recColor)</span></span></span><span class="hljs-function"> </span></span>{ vec3 pos = <span class="hljs-built_in"><span class="hljs-built_in">floor</span></span>(ro); vec3 ri = <span class="hljs-number"><span class="hljs-number">1.0</span></span>/rd; vec3 rs = sign(rd); vec3 dis = (pos-ro + <span class="hljs-number"><span class="hljs-number">0.5</span></span> + rs*<span class="hljs-number"><span class="hljs-number">0.5</span></span>) * ri; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>( <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i=<span class="hljs-number"><span class="hljs-number">0</span></span>; i&lt;MAXSTEPS; i++ ) { vec3 mm = step(dis.xyz, dis.zyx); dis += mm * rs * ri; pos += mm * rs; vec4 mapType = texture2D(_MapTexture, pos.xz * (<span class="hljs-number"><span class="hljs-number">1.</span></span> / <span class="hljs-number"><span class="hljs-number">64.</span></span>)); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (isWall(mapType)) { ... <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-literal"><span class="hljs-literal">true</span></span>; } } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-literal"><span class="hljs-literal">false</span></span>; }</code> </pre> <br>  A similar mesh tracing code can be found in this <a href="https://www.shadertoy.com/view/4sfGWX">Wolfenstein shader</a> on Shadertoy. <br><br>  After calculating the intersection point with the wall or door (using <a href="https://www.shadertoy.com/view/ld23DV">the intersection parallelogram test</a> ), a search in the same texture atlas that was used to prefetch the rendering gives us the albedo intersection points.  Spheres have a color that is procedurally determined based on their <em>x, y</em> coordinates in the grid and <a href="https://www.shadertoy.com/view/ll2GD3">the color gradient function</a> . <br><br>  Everything is a little more difficult with the doors, because they are moving.  So that the representation of the scene in the CPU (used to render meshes in the proactive rendering pass) would be the same as the representation of the scene in the GPU (used for ray tracing), all doors move automatically and deterministically based on the distance from the camera to the door. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3d7/ded/058/3d7ded058d2f62fcf298c9b68b568a69.png"></div><br><br><h4>  Diffuse gi </h4><br>  The diffuse global illumination (diffuse GI) is calculated by emitting rays in the shader, which is used to draw all the geometry in the Diffuse GI Render Target.  The direction of these rays depends on the normal to the surface, determined by sampling the cosine-weighted hemisphere. <br><br>  Having the direction of the ray <em>rd</em> and the starting point <em>ro</em> , the reflected light can be calculated using the following cycle: <br><br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-function">vec3 </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getBounceCol</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(in vec3 ro, in vec3 rd, in vec3 col)</span></span></span><span class="hljs-function"> </span></span>{ vec3 emitted = vec3(<span class="hljs-number"><span class="hljs-number">0</span></span>); vec3 recPos, recNormal, recColor; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i=<span class="hljs-number"><span class="hljs-number">0</span></span>; i&lt;MAX_RECURSION; i++) { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (worldHit(ro, rd, <span class="hljs-number"><span class="hljs-number">0.001</span></span>, <span class="hljs-number"><span class="hljs-number">20.</span></span>, recPos, recNormal, recColor)) { <span class="hljs-comment"><span class="hljs-comment">// if (isLightHit) { // direct light sampling code // return vec3(0); // } col *= recColor; for (int i=0; i&lt;2; i++) { emitted += col * sampleLight(i, recPos, recNormal); } } else { return emitted; } rd = cosWeightedRandomHemisphereDirection(recNormal); ro = recPos; } return emitted; }</span></span></code> </pre> <br>  In order to reduce noise, direct light sampling is added to the loop.  This is similar to the technique used in my shader <a href="https://www.shadertoy.com/view/3dfGR2">Yet another Cornell Box</a> on Shadertoy. <br><br><h4>  Reflection </h4><br>  Due to the possibility of tracing the scene by rays in a shader, it is very easy to add reflections.  In my demo, reflections are added by calling the same <em>getBounceCol</em> method shown above using the reflected camera beam: <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">ifdef</span></span></span><span class="hljs-meta"> REFLECTION col = mix(col, getReflectionCol(ro, reflect(normalize(vWorldPos - _CamPos), normal), albedo), .15); #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">endif</span></span></span></span></code> </pre> <br>  Reflections are added in the proactive rendering pass, therefore one reflection ray will always be emitted onto one screen pixel. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a88/e84/d6a/a88e84d6a2d0209234d6edaf02fcc441.png"></div><br><h3>  Temporal anti-aliasing </h3><br>  As for the soft shadows in the preemptive rendering aisle, and in the diffuse GI approximation, approximately one sample per pixel is used, the end result is extremely noisy.  To reduce the amount of noise, temporal anti-aliasing (TAA) is used, implemented on the basis of Playdead's TAA: <a href="http://twvideo01.ubm-us.net/o1/vault/gdc2016/Presentations/Pedersen_LasseJonFuglsang_TemporalReprojectionAntiAliasing.pdf">Temporal Reprojection Anti-Aliasing in INSIDE</a> . <br><br><h4>  Re-projection </h4><br>  The idea behind TAA is quite simple: TAA calculates one subpixel per frame, and then averages its values ‚Äã‚Äãwith a correlating pixel from the previous frame. <br><br>  To know where the current pixel was in the previous frame, the fragment position is re-projected using the model-view-projection matrix of the previous frame. <br><br><h4>  Dropping Samples and Neighborhood Restriction </h4><br>  In some cases, the sample saved from the past is invalid, for example, when the camera has moved in such a way that the fragment of the current frame in the previous frame was closed by the geometry.  To discard such invalid samples, a neighborhood constraint is used.  I chose the most simple type of restriction: <br><br><pre> <code class="cpp hljs">vec3 history = texture2D(_History, uvOld ).rgb; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">float</span></span> x = <span class="hljs-number"><span class="hljs-number">-1.</span></span>; x &lt;= <span class="hljs-number"><span class="hljs-number">1.</span></span>; x+=<span class="hljs-number"><span class="hljs-number">1.</span></span>) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">float</span></span> y = <span class="hljs-number"><span class="hljs-number">-1.</span></span>; y &lt;= <span class="hljs-number"><span class="hljs-number">1.</span></span>; y+=<span class="hljs-number"><span class="hljs-number">1.</span></span>) { vec3 n = texture2D(_New, vUV + vec2(x,y) / _Resolution).rgb; mx = max(n, mx); mn = min(n, mn); } } vec3 history_clamped = clamp(history, mn, mx);</code> </pre> <br>  I also tried to use the constraint method based on the bounding parallelogram, but I didn‚Äôt see much difference with my decision.  It probably happened because there are many identical dark colors in the scene from the demo and there are almost no moving objects. <br><br><h4>  Camera shake </h4><br>  To obtain anti-aliasing, the camera in each frame oscillates due to the use of (pseudo) random sub-pixel offset.  This is implemented by changing the projection matrix: <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">this</span></span>._projectionMatrix[<span class="hljs-number"><span class="hljs-number">2</span></span> * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">0</span></span>] += (<span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.getHaltonSequence(frame % <span class="hljs-number"><span class="hljs-number">51</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) - <span class="hljs-number"><span class="hljs-number">.5</span></span>) / renderWidth; <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>._projectionMatrix[<span class="hljs-number"><span class="hljs-number">2</span></span> * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>] += (<span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.getHaltonSequence(frame % <span class="hljs-number"><span class="hljs-number">41</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>) - <span class="hljs-number"><span class="hljs-number">.5</span></span>) / renderHeight;</code> </pre> <br><h3>  Noise </h3><br>  Noise is the basis of the algorithms used to calculate diffuse GI and soft shadows.  Using <a href="https://github.com/Atrix256/SampleZoo">good noise</a> greatly affects image quality, while poor noise creates artifacts or slows down the convergence of images. <br><br>  I'm afraid that the white noise used in this demo is not very good. <br><br>  Probably the use of good noise is the most important aspect of improving the quality of the image in this demo.  For example, you can use <a href="https://blog.demofox.org/2018/08/12/not-all-blue-noise-is-created-equal/">blue noise</a> . <br><br>  I conducted experiments with noise based on the golden section, but they were not crowned with particular success.  For now, Dave Hoskins' reputed <a href="https://www.shadertoy.com/view/4djSRW">Hash without Sine is used</a> : <br><br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-function">vec2 </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">hash2</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ vec3 p3 = fract(vec3(g_seed += <span class="hljs-number"><span class="hljs-number">0.1</span></span>) * HASHSCALE3); p3 += dot(p3, p3.yzx + <span class="hljs-number"><span class="hljs-number">19.19</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> fract((p3.xx+p3.yz)*p3.zy); }</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a81/338/42a/a8133842a43266dad1b4e9ee50c59228.png"></div><br><h3>  Noise reduction </h3><br>  Even with TAA enabled, a lot of noise is still visible in the demo.  It is especially difficult to render the ceiling, because it is illuminated only by indirect lighting.  It does not simplify the situation and the fact that the ceiling is a large flat surface filled with solid color: if it had a texture or geometric details, the noise would have become less noticeable. <br><br>  I didn‚Äôt want to spend a lot of time on this part of the demo, so I tried to apply only one noise reduction filter: Median3x3 <a href="http://casual-effects.com/research/McGuire2008Median/index.html">Morgan McGuire and Kyle Whitson</a> .  Unfortunately, this filter does not work well with ‚Äúpixel art‚Äù graphics of wall textures: it removes all the details away and rounds the corners of the pixels of nearby walls. <br><br>  In another experiment, I applied the same filter to the Diffuse GI Render Target.  Although he lowered the noise a little, at the same time almost without changing the details of the wall textures, I decided that this improvement was not worth the extra milliseconds spent. <br><br><h3>  Demo </h3><br>  <a href="https://reindernijhoff.net/wolfrt">The demo can be played here</a> . </div><p>Source: <a href="https://habr.com/ru/post/444516/">https://habr.com/ru/post/444516/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../444500/index.html">March 29, Moscow - Backend Stories 3.0</a></li>
<li><a href="../444502/index.html">Is quantum cryptography reliable?</a></li>
<li><a href="../444504/index.html">Using the local package directory in Python now</a></li>
<li><a href="../444508/index.html">How we made PHP 7 twice as fast as PHP 5</a></li>
<li><a href="../444514/index.html">Webinar "Machine Learning Security: Natural Problems of Artificial Intelligence"</a></li>
<li><a href="../444518/index.html">Towards a fundamental theory of consciousness</a></li>
<li><a href="../444520/index.html">2. Check Point Getting Started R80.20. Solution Architecture</a></li>
<li><a href="../444522/index.html">Apocalypse is canceled</a></li>
<li><a href="../444524/index.html">Lambda: from C ++ 11 to C ++ 20. Part 1</a></li>
<li><a href="../444526/index.html">DOTS stack: C ++ & C #</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>