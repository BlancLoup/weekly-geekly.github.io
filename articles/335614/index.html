<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Log it: the method of the logarithmic derivative in machine learning</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The technique in question ‚Äî the method of the logarithmic derivative ‚Äî helps us do all sorts of things using the main property of the derivative of th...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Log it: the method of the logarithmic derivative in machine learning</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a40/deb/ac2/a40debac242a37b3f77ef29ffc3060c3.jpg"></div><br>  The technique in question ‚Äî the method of the logarithmic derivative ‚Äî helps us do all sorts of things using the main property of the derivative of the logarithm.  Best of all, this method has proven itself in solving stochastic optimization problems that we investigated <a href="http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/">earlier</a> .  Thanks to its application, we found a new way to obtain stochastic gradient estimates.  We start with an example of using reception to determine the evaluation function. <br><br>  Pretty mathematically. <br><a name="habracut"></a><br><h3>  Evaluation function (score function) </h3><br>  The method of the logarithmic derivative is the application of the rule for the gradient to the parameters <i>Œ∏ of the</i> logarithm of the function <i>p (x; Œ∏)</i> : <br><p><math> </math> $$ display $$ logŒ∏ log p (x; Œ∏) = ‚àáŒ∏p (x; Œ∏) / p (x; Œ∏) $$ display $$ </p><br>  The application of the method turns out to be successful when the function <i>p (x; Œ∏)</i> is <a href="https://en.wikipedia.org/wiki/Likelihood_function">a likelihood function</a> , that is, the function f with parameters <i>Œ∏</i> is the probability of a random variable <i>x</i> .  In this particular case, the function <p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2207;</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x3B8;</mo></mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x3B8;</mo></mrow><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="12.396ex" height="2.66ex" viewBox="0 -832 5337.2 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMAIN-2207" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMATHI-3B8" x="833" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMATHI-6C" x="1303" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMATHI-6F" x="1601" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMATHI-67" x="2087" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMATHI-70" x="2567" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMAIN-28" x="3071" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMATHI-78" x="3460" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMAIN-3B" x="4033" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMATHI-3B8" x="4478" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMAIN-29" x="4947" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow class="MJX-TeXAtom-ORD"><mo>‚àá</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo>Œ∏</mo></mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo>;</mo><mrow class="MJX-TeXAtom-ORD"><mo>Œ∏</mo></mrow><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"> ‚àáŒ∏logp (x; Œ∏) </script></p>  is called <a href="https://en.wikipedia.org/wiki/Score_(statistics)">evaluative</a> , and the right-hand side is a relation of evaluations.  The evaluation function has a number of useful properties: <br><cut></cut><br>  <b>Basic operation for maximum likelihood estimation</b> .  <a href="https://en.wikipedia.org/wiki/Maximum_likelihood">Maximum likelihood</a> is one of the main principles of machine learning used in <a href="https://en.wikipedia.org/wiki/Generalized_linear_model">generalized linear regression</a> , in- <a href="https://en.wikipedia.org/wiki/Deep_learning">depth training</a> , <a href="https://en.wikipedia.org/wiki/Kernel_method">nuclear machines</a> , dimension reduction and <a href="https://en.wikipedia.org/wiki/Higher-order_singular_value_decomposition">tensor decompositions</a> , etc.  Evaluation is needed in all these tasks. <br><br>  <b>The expectation score is zero</b> .  Our first use of the logarithmic derivative method will show this: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/web/b8a/109/232/b8a109232954451496efab40ba19fae1.png"><br><br>  In the first line we applied the derived logarithm, and in the second line we changed the order of differentiation and integration.  This identity detects the type of probabilistic flexibility we need: it allows you to subtract any term from a zero-expectation estimate, and the change will not affect the expected result (see below: control variables). <br><br>  <b>The variance of the estimate is the Fisher information used to determine <a href="https://en.wikipedia.org/wiki/Cram%25C3%25A9r%25E2%2580%2593Rao_bound">the Cramer-Rao lower bound</a> .</b> <br><br><img src="https://habrastorage.org/web/bd1/b30/74f/bd1b3074f99e41adb4c4418cbf2e0f69.png"><br><br>  Now we can move in one boundary from the gradients of the logarithmic probability to the gradients of probability and back.  However, the main evil character of today's post - the complex expected gradient from <a href="http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/">Method 4</a> - arises again.  We can use our new power (the evaluation function) and find another clever evaluation solution for this class of problems. <br><br><h3>  Evaluation function evaluation unit </h3><br>  Our task is to calculate the gradient of the mathematical expectation of the function <i>f</i> : <br><br><img src="https://habrastorage.org/web/8d5/34c/4c6/8d534c4c6d8b4756b8e6967898c4b7d8.png"><br><br>  This is a frequently encountered machine learning problem, which is necessary for subsequent computation in <a href="http://arxiv.org/pdf/1401.4082v3.pdf">variational derivation</a> , the function of values ‚Äã‚Äãand the teaching of policies (strategies) in the <a href="http://www.scholarpedia.org/article/Policy_gradient_methods">learning process with reinforcement</a> , the pricing of derivatives in <a href="http://www.columbia.edu/~mnb2/broadie/Assets/bg_ms_1996.pdf">financial engineering</a> , <a href="http://citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.50.6751%26rep%3Drep1%26type%3Dpdf">inventory accounting</a> in operations research, etc. <br><br>  It is difficult to calculate this gradient, since the integral is usually unknown, and the parameters <i>Œ∏</i> , with respect to which we calculate the gradient, have the distribution <i>p (z; Œ∏)</i> .  In addition, we may need to calculate this gradient for a non-differentiable function.  Using the logarithmic derivative method and the properties of the estimation function, we can calculate this gradient in a more convenient way: <br><br><img src="https://habrastorage.org/web/8e4/53a/83d/8e453a83db4b479781cf591c02ea0658.png"><br><br>  We derive this expression and consider its consequences for our optimization problem.  For this purpose, we will use another frequently encountered method - the method of probability identity, according to which we multiply our expressions by 1 - the number formed by dividing the probability density by itself.  Combining this method with the method of the logarithmic derivative, we get the evaluation unit of the evaluation function of the gradient: <br><br><img src="https://habrastorage.org/web/053/9b4/7dc/0539b47dc342416d8e6f12d65da71e8d.png"><br><br>  In these four lines, we performed many operations.  In the first line, we replaced the derivative with an integral.  In the second, we applied our probabilistic method, which allowed us to form the coefficient of estimation.  Using the logarithmic derivative, we replaced this coefficient with the gradient of the logarithmic probability in the third row.  This gives us the desired stochastic estimate in the fourth line, which we calculated by the <a href="https://en.wikipedia.org/wiki/Monte_Carlo_integration">Monte Carlo</a> method, taking a sample from <i>p (z)</i> and then calculating the weighted gradient term. <br><br>  This is an <a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">unbiased</a> gradient <a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">estimate</a> .  Our assumptions in this process were simple: <br><br><ul><li>  The replacement of integration by differentiation is valid.  This is difficult to show in general terms, but the replacement itself usually causes no problems.  We can confirm the correctness of this technique, referring to <a href="https://en.wikipedia.org/wiki/Leibniz_integral_rule">the Leibniz formula</a> and the <a href="http://www.jstor.org/stable/2632893%3Fseq%3D1">analysis</a> given here [ <a href="http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/">1</a> ]. </li><li>  The function <i>f (z)</i> should not be differentiable.  So we can evaluate it or observe changes in values ‚Äã‚Äãfor a given <i>z</i> . </li><li>  Obtaining samples from the distribution of <i>p (z) is</i> not difficult, since it is necessary to estimate the integral by the Monte Carlo method. </li></ul><br>  Like many of our other techniques, the path we have chosen here has already been covered in many other research areas, and each area already has its own term, a history of method development and a range of tasks related to the formulation of the problem.  Here are some examples: <br><br>  1. <b>Assessment function evaluation unit (Score function estimator)</b> <br><br>  Our conclusion allowed us to convert the waiting gradient to the expectation of the evaluation function <p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2207;</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x3B8;</mo></mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo>;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x3B8;</mo></mrow><mo stretchy=&quot;false&quot;>)</mo><mo>,</mo></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="12.801ex" height="2.66ex" viewBox="0 -832 5511.7 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMAIN-2207" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMATHI-3B8" x="833" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMATHI-6C" x="1303" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMATHI-6F" x="1601" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMATHI-67" x="2087" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMATHI-70" x="2567" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMAIN-28" x="3071" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMATHI-7A" x="3460" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMAIN-3B" x="3929" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMATHI-3B8" x="4374" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMAIN-29" x="4843" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/335614/&amp;xid=17259,1500004,15700022,15700043,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj7ozSdgj-8X2SHWMGBTneGMRpg#MJMAIN-2C" x="5233" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow class="MJX-TeXAtom-ORD"><mo>‚àá</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo>Œ∏</mo></mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>p</mi><mo stretchy="false">(</mo><mi>z</mi><mo>;</mo><mrow class="MJX-TeXAtom-ORD"><mo>Œ∏</mo></mrow><mo stretchy="false">)</mo><mo>,</mo></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-3"> ‚àáŒ∏logp (z; Œ∏), </script></p>  which makes it convenient for designating such estimates as blocks of the evaluation function [ <a href="http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/">2</a> ].  This is a fairly common term, and I prefer to use it. <br><br>  Many insightful observations and a number of historically important developments on the topic are presented in the paper entitled " <a href="http://core.ac.uk/download/pdf/6635823.pdf">Optimization and sensitivity analysis of computer simulation models using the evaluation function method</a> ". <br><br>  2. <b>Likelihood ratio methods</b> <br><br>  One of the main popularizers of this class of evaluation was <a href="http://web.stanford.edu/~glynn/">P.V.</a>  <a href="http://web.stanford.edu/~glynn/">Glynn</a> [ <a href="http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/">3</a> ].  He interprets the attitude of evaluation <p><math> </math> $$ display $$ ‚àáŒ∏p (z; Œ∏) / p (z; Œ∏) $$ display $$ </p>  as likelihood ratio and describes evaluation units as likelihood ratio methods.  I believe that this is a somewhat non-standard use of the term "likelihood ratio".  It usually represents the ratio of the same function with different parameters, and not the ratio of different functions with the same parameter.  Therefore, I prefer the term ‚ÄúEvaluation function evaluation unit‚Äù. <br><br>  Many authors, and in particular, for example, Michael Foo [ <a href="http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/">4</a> ], speak about methods of likelihood ratio and estimation (LR / SF).  The most important work on the topic: "The <a href="http://web.stanford.edu/~glynn/papers/1990/G90a.pdf">estimated ratio of the likelihood coefficient for stochastic systems</a> ", where Glynn explains in detail the most important properties of dispersion, and Michael Fu's <a href="http://dspace.utamu.ac.ug:8080/xmlui/bitstream/handle/123456789/165/%255BShane_G._Henderson,_Barry_L._Nelson%255D_Handbooks_in(BookFi.org).pdf%3Fsequence%3D1">Grade Evaluation</a> . <br><br>  3. <b>Automated variation output</b> <br><br>  <a href="http://shakirm.com/papers/VITutorial.pdf">Variational derivation</a> transforms difficult-to-solve integrals arising in <a href="https://en.wikipedia.org/wiki/Bayesian_inference">Bayesian analysis</a> into problems for stochastic optimization.  It is not surprising that in this area there are also evaluation units for estimating functions known under different terms: <a href="http://www.cs.columbia.edu/~blei/papers/PaisleyBleiJordan2012a.pdf">variational stochastic search</a> , <a href="">automatic variational inference</a> , <a href="">variational inference on the black box model</a> , <a href="">neural variational inference</a> ‚Äî these are just a few of which I know. <br><br>  4. <b>Remuneration and strategy gradients</b> <br><br>  To solve <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> problems <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">,</a> we can compare the function <i>f</i> with the reward received from the environment, the distribution of <i>p (z; Œ∏)</i> with the policy, and the evaluation with the <a href="http://www.scholarpedia.org/article/Policy_gradient_methods">gradient of the policy</a> or, in some cases, with the characteristic possible choice (characteristic eligibility). <br><br>  Intuitively, this can be explained as follows: any policy gradients that correspond to high remuneration receive large weights due to reinforcement from the evaluation unit.  Consequently, the evaluation unit is called <a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf">REINFORCE</a> (REINFORCEMENT) [ <a href="http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/">5</a> ] and this generalization at the moment forms the <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf">strategy (policy) gradient theorem</a> . <br><br><h3>  Control variables </h3><br>  To do this Monte Carlo estimation effectively, we need to make its variance as small as possible.  Otherwise, there will be no benefit from the gradient.  To achieve greater control of dispersion is possible through the use of a modified estimate: <br><br><img src="https://habrastorage.org/web/131/5b9/66e/1315b966e7ac4ec0acc888e5379808b4.png"><br><br>  where the new term Œª is called the <a href="http://users.iems.northwestern.edu/~nelsonb/Publications/Nelson5.pdf">control variable</a> , widely used to reduce the variance in Monte-Carlo estimates.  The control variable is any additional term added to the estimate that has a zero mean value.  Such variables can be entered at any stage, as they do not affect expectations.  Control variables affect variance.  The main problem with the use of estimates is the choice of the reference variable.  The easiest way is to use a constant parameter, but there are other methods: reasonable sampling schemes (for example, <a href="https://en.wikipedia.org/wiki/Antithetic_variates">antithetical</a> or <a href="https://en.wikipedia.org/wiki/Stratified_sampling">stratified</a> ), delta methods or adaptive constant parameters.  One of the most comprehensive discussions of the sources of dispersion and methods for reducing dispersion is given in this book by P. <a href="https://books.google.se/books/about/Monte_Carlo_Methods_in_Financial_Enginee.html%3Fhl%3Dsv%26id%3De9GWUsQkPNMC">Glaserman</a> [ <a href="http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/">6</a> ] <br><br><h3>  Groups of stochastic evaluation systems </h3><br>  Compare today's estimate with the one we derived using the linear derivative <a href="http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/">method</a> in <a href="http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/">method 4</a> . <br><br><img src="https://habrastorage.org/web/9c8/ab7/44e/9c8ab744e2f34f88a70ca94ed45cb5a2.png"><br><br>  Here we have two effective approaches.  We can: <br><br><ul><li>  differentiate the function <i>f</i> using linear derivatives if the function is differentiable, or </li><li>  Differentiate the density of <i>p (z)</i> using the estimation function. </li></ul><br><br>  There are other ways to create stochastic gradient estimates, but these two are the most common.  They can be easily combined, which allows us to use the most suitable estimate (providing the lowest variance) at different stages of our calculations.  This problem can be solved, in particular, by using a <a href="http://arxiv.org/abs/1506.05254">stochastic computational graph</a> . <br><br><h3>  Conclusion </h3><br>  The method of the logarithmic derivative allows us to alternate the normalized probability and logarithmically probability representations of distributions.  It constitutes the basis for the determination of the evaluation function, and serves as a method for the most plausible evaluation and important asymptotic analysis, which have formed most of our statistical knowledge.  It is important to note that this method can be used to assess the general-purpose gradient in solving stochastic optimization problems that underlie many important machine learning problems that we currently face.  Monte Carlo gradient estimates are necessary for the genuinely universal machine learning to which we aspire.  For this, it is important to understand the patterns and methods underlying such an assessment. <br><br><hr><br><div class="spoiler">  <b class="spoiler_title">Links</b> <div class="spoiler_text">  [1] Note on the relationship between estimators, Management Science, 1995 <br>  [2] Kleijnen, PCR, Recycling, European Journal of Operational Research, 1996 <br>  [3] Peter W Glynn, Likelyhood ratio, Communications of the ACM, 1990 <br>  [4] Michael C Fu, Gradient estimation, Handbooks in operations research and management science, 2006 <br>  [5] Ronald J Williams, Simple gradient-following algorithms for connectionist, reinforcement learning, Machine learning, 1992 <br>  [6] Monte Carlo methods in financial engineering, 2003 </div></div></div><p>Source: <a href="https://habr.com/ru/post/335614/">https://habr.com/ru/post/335614/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../335602/index.html">The hunt for malicious npm-packages</a></li>
<li><a href="../335606/index.html">Useful books on the development of mobile games on Android and iOS</a></li>
<li><a href="../335608/index.html">In a section: the news aggregator on Android with backend. Configuration Management System (Puppet)</a></li>
<li><a href="../335610/index.html">World Machine + UE4: Full Workflow</a></li>
<li><a href="../335612/index.html">How to conduct a non-ideal interview of a tester and why there is no ideal</a></li>
<li><a href="../335616/index.html">Graphic Designers for the Mind and Soul: 20 New Lessons</a></li>
<li><a href="../335618/index.html">Why backup? We have the same RAID</a></li>
<li><a href="../335620/index.html">Calling managed code from unmanaged</a></li>
<li><a href="../335622/index.html">Due date as a component of responsibility in the development process</a></li>
<li><a href="../335626/index.html">How we participated in the first Ligaltekh Hakatone of the CIS and why we decided to do one more in Moscow</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>