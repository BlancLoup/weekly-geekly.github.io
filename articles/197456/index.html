<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How we migrated millions of countries in a working day</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Badoo is the world's largest social network for meeting new people, with 190 million users. 
 All data is stored in two data centers - European and Am...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How we migrated millions of countries in a working day</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/storage3/3a5/4b9/79f/3a54b979f0cc58302ccf647c66e5a419.png" align="left">  Badoo is the world's largest social network for meeting new people, with 190 million users. <br>  All data is stored in two data centers - European and American.  Some time ago we investigated the quality of the Internet connection among our users from Asia and found that for 7 million users, our site will load 2 times faster if we move them from a European data center to an American one.  For the first time, we faced the challenge of large-scale migration of user data between data centers, which we successfully coped with: we learned to move 1.5 million users in one working day!  We were able to move whole countries!  In the first part, we will describe in detail the task set before us and what result we have achieved. <br><a name="habracut"></a><br><h4>  Badoo architecture </h4><br>  The architecture of Badoo has been much discussed at various conferences and at Habr√© itself, but we will still repeat the main points that are important for understanding our task.  In Badoo, the standard technology stack is used to upload web pages: Linux, nginx, PHP-FPM (developed by Badoo), memcached, MySQL. <br>  Almost all user data is located on a couple of hundreds of MySQL servers and is distributed (‚Äúshared‚Äù) over it using a ‚Äúself-written‚Äù service called authorizer.  It stores in itself a large table of correspondences user id and server id on which the user is located.  We do not use the "remnants of division" and other similar ways to distribute users across servers, so we can move users between database servers without any particular problems: this was originally incorporated in the Badoo architecture. <br><br><img src="https://habrastorage.org/storage3/695/c82/4ad/695c824adfce5c3bf2920753aaad7a3f.png"><br><br>  In addition to MySQL, we have many C / C ++ services that store various data: sessions, dates of the last visit to the site, photos of users for voting, the basic data of all users for searching. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      We also have a service that serves the "Dating" section.  It differs from others in that it exists in one copy for each country.  Simply, it consumes so much RAM that all its data can not physically fit on one server, even if you put 384 GB there.  At the same time, he ‚Äúlives‚Äù only in ‚Äúhis‚Äù data center. <br><br>  In addition, we have a number of ‚Äúcentral‚Äù MySQL databases that store general information about all users, and separate billing databases, which also store information for all Badoo users at once.  In addition, we have separate storage for uploaded photos and videos, which will be a separate article.  This data must also be moved. <br><br><h4>  Formulation of the problem </h4><br>  The task is formulated very simply: to transfer user data from a whole country in one working day, and so that during migration these users can use the site.  The largest country we have migrated is Thailand, where we have about 1.5 million registered users.  If we divide this number into 8 working hours (plus lunch), we get the required migration speed, which is about 170 thousand users per hour. <br>  The requirement to migrate the country in one working day is dictated by the fact that anything can happen at that time.  For example, they can ‚Äúlie down‚Äù or start to slow down some servers or services, and then it will be necessary to edit the code ‚Äúlive‚Äù to reduce the load created on them.  There are also errors in the migration code that will lead to problems for users, and then it should be possible to quickly see it and pause (or even roll back) the process.  In short, the implementation of such a large-scale user transfer operation requires the presence of an ‚Äúoperator‚Äù who will control what is happening and make the necessary adjustments during work. <br><br>  Technically, for each user, you need to make selections from all tables of all MySQL instances that may contain data about this user, as well as transfer data that is stored in C / C ++ services.  And for one of the services, you need to transfer the daemon itself, and not the data between the running daemon instances in both data centers. <br>  The delay in data transfer between data centers is about 100 ms, so operations should be optimized so that data is loaded by the stream, rather than by a large number of small queries.  During migration, the site‚Äôs unavailability time for each user should be minimal, so the process should be carried out for each user individually, and not in a large bundle.  The time we focused on is not more than 5 minutes of inaccessibility of the site for a specific user (preferably 1-2 minutes). <img src="https://habrastorage.org/storage3/39c/1cf/bef/39c1cfbefa46c5f681805e032d7b39be.png" align="right"><br><br><h4>  Work plan </h4><br>  Based on the fact that we needed to migrate 170,000 users per hour, and the migration time of each user should be about 2-3 minutes, we calculated the number of concurrently executed threads that will be required to fulfill these conditions.  Each thread can transfer an average of 25 users per hour, so the total number of threads was 6,800 (ie, 170,000 / 25).  In fact, we were able to limit ourselves to ‚Äúonly‚Äù 2,000 streams, since  Most of the time, the user simply ‚Äúwaits‚Äù for various events to occur (for example, MySQL replication between data centers).  Thus, each thread took into the processing of three users at the same time and switched between them when one of them went into the waiting state for something. <br><br>  The migration of each user consisted of many consecutive steps.  The execution of each step began strictly after the end of the previous one and provided that the last one was completed successfully. <br>  Also, each step must be repeatable, or, ‚Äúspeaking in Russian,‚Äù idempotent.  Those.  Each step can be interrupted at any time for various reasons, and he should be able to determine which operations he has to complete and perform these operations correctly. <br>  This is required in order not to lose user information in case of an emergency stop of migration and in case of temporary failures of internal services or database servers. <br><br><h5>  Structure and sequence of our actions </h5><br>  <b>Preparatory steps</b> <br>  We mark the user as ‚Äúmigrated‚Äù at the moment and wait until the end of its processing with background scripts, if any.  This step took us about a minute, during which you can migrate another user in the same stream. <br><br>  <b>Billing Data Migration</b> <br>  During the migration of the country, we completely disconnected the billing in it, so no special actions and locks were required for this - the data was simply transferred from one central MySQL database to another.  MySQL connection from each stream was established to this database, so the total number of connections to the billing database was over 2,000. <br><br>  <b>Photo migration</b> <br>  Here we ‚Äúcounted a little‚Äù, because we found a way to transfer photos relatively easily in advance and separately.  Therefore, for most users, this step simply checked that they did not have new photos from the moment of transfer. <br><br>  <b>Fill user master data</b> <br>  In this step, we formed a SQL dump of each user's data and applied it on the remote side.  In this case, the old data in this step was not deleted. <br><br>  <b>Updating data in the authorizer service</b> <br>  The authorizer service stores correspondences between the user id and server id, and until we update the data in this service, the scripts will follow the user data to the old place. <br><br>  <b>Delete user data from the old place</b> <br>  Using DELETE FROM queries, we clear the user's data on the source MySQL server. <br><br>  <b>Steps for transferring data from central databases</b> <br>  One of the central bases under the eloquent title Misc (from the English. Miscellaneous - different) contains many different tables, and for each of them we did one SELECT and DELETE per user.  We ‚Äúsqueezed‚Äù 40,000 SQL queries per second from the poor database and kept more than 2,000 connections open to it. <br><br>  <b>Steps for transferring data from services</b> <br>  As a rule, all data is contained in the database, and services only allow you to quickly access it.  Therefore, for most services, we simply deleted data from one place and refilled it with information from a database that was already in a new place.  However, we simply transferred one service entirely, and not by users, because the data was stored in it in a single copy. <br><br>  <b>Waiting for replication</b> <br>  Our databases are replicated between data centers, and until replication is ‚Äúfinished‚Äù, user data is in an inconsistent state in different data centers.  Therefore, we had to wait for the end of replication for each user, so that everything worked correctly and the data were consistent with each other.  And in order not to lose time at this step (from 20 seconds to a minute), it was used to migrate other users at this moment. <br><br>  <b>Final steps</b> <br>  We mark the user as finished migration and allow him to log in on the site, already in the new data center. <br><br><h4>  MySQL data transfer </h4><img src="https://habrastorage.org/storage3/5ee/916/e0a/5ee916e0afa8fc8b396ef6996c1139e7.png" align="left"><br>  As mentioned earlier, we store user data on MySQL servers, which are about a hundred and fifty for each data center.  Each server has several databases, each of which contains thousands of tables (on average, we try to have one table per 1000 users).  The data is arranged in such a way as to either not use auto-increment fields at all, or at least not to refer to them in other tables.  Instead, the combination of user_id and sequence_id is used as the primary key, where user_id is the user identifier and sequence_id is the counter that automatically increases and is unique within the same server.  Thus, records about each user can be freely moved to another server without loss of referential integrity and the need to build correspondences between the values ‚Äã‚Äãof old and new auto-increment fields. <br><br>  Data transfer is done in the same way for most MySQL servers (note that in case of any errors, the entire step crashes and restarts after a short time): <br><br><ul><li>  Go to the "side-receiver" and check if there is already user data there.  If there is, it means that the data filling was successful, but the step was not completed correctly. </li><li>  If there is no data on the remote side, we do a SELECT from all the necessary tables with filtering by user and form a SQL dump containing BEGIN at the beginning and COMMIT at the end. </li><li>  Fill the dump via SSH to the "proxy" on the remote side and use it with the console utility mysql.  It may happen that the COMMIT request passes, but we cannot get an answer, for example, due to network problems.  To do this, we first check to see if the dump failed in the previous attempt.  Moreover, in some databases, the lack of data for the user is a normal situation, and in order to be able to check whether the data was transferred, we in such cases added INSERT to a special table, according to which we checked if necessary. </li><li>  Delete the original data using the DELETE FROM with the same WHERE that was in the SELECT queries.  As a rule, WHERE conditions contained user_id, and on part of the tables this field was not part of the primary key for a variety of reasons.  Where possible, an index has been added.  Where it turned out to be difficult or impractical, when deleting data, a sample was first taken by user_id, and then deleted by the primary key, thus avoiding locks for reading and significantly speeding up the process. </li></ul><br>  If we know for sure that we have never before proceeded to the corresponding step, then we skip the data availability check on the remote side.  This allows us to win about a second for each server to which we transfer data (which is due to a delay of 100 ms for each packet being sent). <br><br>  During the migration, we encountered a number of problems that we want to talk about. <br><br><h4>  Auto-increment fields (auto_increment) </h4><br>  In the billing database, auto-increment fields are actively used, so for them they had to write the complex logic of ‚Äúmapping‚Äù old id into new ones. <br>  The difficulty was that the data from the tables, where only the above described sequence_id is included in the primary key, cannot simply be transferred, as sequence_id is unique only within the server.  Replacing sequence_id with NULL, thus causing the generation of a new auto-increment, is also impossible, because, first, the generation of sequence_id is performed by inserting data into one table, and the resulting value is used in another.  And secondly, other tables reference the table using sequence_id.  That is, you need to get the required number of values ‚Äã‚Äãof the auto-increment field on the server where the data is being transferred, replace the old sequence_id with new ones in the user data and write the finished INSERTs to a file that will later be used by the console utility mysql. <br>  To do this, we opened a transaction on the receiving server, made the required number of inserts, called mysql_insert_id (), which, in the case of inserting multiple rows in one transaction, returns the auto increment value for the first row, and then rolls back the transaction.  In this case, after the transaction is rolled back, the auto-increment will remain increased by the number of rows inserted by us, unless the database server is rebooted.  Having obtained the autoincrement values ‚Äã‚Äãwe need, we formed the corresponding insertion requests, including the table responsible for autoincrement generation.  But in these queries, the auto-increment values ‚Äã‚Äãwere already explicitly indicated in order to fill the holes formed in it after the rollback of the transaction. <br><br>  <b>Max_connections and MySQL load</b> <br><br>  Each thread created one MySQL connection to the server with which it had to deal.  Therefore, we kept 2,000 connections on all central MySQL servers.  With more connections, MySQL started to behave inadequately, until the fall (we use versions 5.1 and 5.5). <br><br>  One day during the migration, one of the central MySQL servers fell down (one of those that had a very large load).  The migration was immediately aborted, and we began to find out the cause of the fall.  It turned out that the RAID controller simply ‚Äúflew out‚Äù on it.  And although the administrators said that it was not related to the load we gave to this server, but the sediment remained. <br><br>  <b>InnoDB, MVCC and DELETE FROM: pitfalls</b> <br><br>  Since we store all the data in InnoDB and all the transferred data was immediately deleted, we have started to slow down all the scripts that rake up the queues that are in tables on some servers.  We were surprised to see how SELECT from an empty table took minutes.  MySQL purge thread did not have time to clear the deleted records, and despite the fact that the tables with queues were empty, they contained a lot of deleted records that were not physically deleted yet and were just skipped MySQL during the sample.  A quantitative description of the length of a queue for clearing records can be obtained by typing SHOW ENGINE INNODB STATUS and looking at the History list length line.  The greatest value we have seen is several million entries.  We recommend carefully deleting many entries from InnoDB tables using DELETE FROM.  It is much better to avoid this and use, for example, TRUNCATE TABLE, if possible.  Requests of the form TRUNCATE TABLE completely clear the table, and these operations are DDL, therefore the deleted records do not add up to the undo / redo log (InnoDB does not support transactions for DDL operations). <br>  If, after deleting all data using DELETE FROM, you need to select a table, then try to impose a BETWEEN condition on the primary key.  For example, if you use auto_increment, select MIN (id) and MAX (id) from the table, then select all the records between them - this is significantly faster than choosing records with some limit or only with one of the conditions of the form <b>id&gt; N</b> or <b>id &lt;n</b> .  Requests that receive MIN (id) and MAX (id) will take a very long time, because InnoDB will skip deleted records.  But requests for key ranges will be executed with the same speed as usual - deleted records with such requests will not get into the sample. <br><br>  We were also surprised to see many ‚Äúhanging‚Äù queries of the form DELETE FROM WHERE user_id =, where all the queries are the same, and there is no index on user_id in this table.  As it turned out, MySQL version 5.1 (and to a lesser extent 5.5) has a very poor scalability of such queries, if FULL SCAN tables are made when deleting records and the REPEATABLE READ isolation level (by default).  There is a very high competition of locks for the same records, which leads to an avalanche-like increase in the processing time of the request. <br>  One of the possible solutions to the problem is to set the isolation level of READ COMMITED for a transaction that deletes data, and then InnoDB will not put locks on those rows that are not suitable for the WHERE clause.  To illustrate how serious this problem was, let's take a screenshot taken during the migration.  The tmp.tiw_fix table in the screenshot contains only about 60 entries (!) And does not contain an index on user_id. <br><br><img src="https://habrastorage.org/storage3/27a/836/61d/27a83661da123c8cfd95b97802968b92.png"><br><br><h4>  Distribution of users by threads </h4><br>  Initially, we distributed users to threads evenly, regardless of which server a particular user is on.  Also in each stream we leave open connections to all MySQL servers we had to meet with for migrating users assigned to the corresponding stream.  As a result, we got two more problems: <br><ul><li>  When a MySQL server started to slow down, the migration of users living on this server slowed down.  As all other threads continued execution, they gradually also reached the user living on the problem server.  Gradually, an increasing number of threads accumulated on this server, and it began to slow down even more.  To prevent the server from falling down, we inserted temporary patches into the code while working, using various ‚Äúcrutches‚Äù, limiting the load on this server. </li><li>  Since we kept open MySQL connections in each of the streams to all the necessary MySQL servers, we gradually came to the conclusion that each thread had a large number of open connections to all MySQL servers, and we started abutting max_connections. </li></ul><br>  In the end, we changed the algorithm for distributing users to threads and assigned users to each thread who live on only one MySQL server.  Thus, we immediately solved the problem of the avalanche-like growth of the load in case of ‚Äúbrakes‚Äù of this server, as well as the problem with too many simultaneous connections on some weak hosts. <br><br><h4>  To be continued‚Ä¶ </h4><br>  In the following sections, we will describe how we pre-migrated user photos and what data structures we used to limit the load on servers with photos.  After that, we will describe in more detail how we managed to coordinate the work of 2,000 simultaneously executing migration flows on 16 servers in two data centers and what technical solutions were used to make all this work. <br><br>  Yury <a href="http://habrahabr.ru/users/yourock/" class="user_link">youROCK,</a> Nasretdinov, PHP developer <br>  Anton <a href="http://habrahabr.ru/users/antonstepanenko/" class="user_link">antonstepanenko</a> Stepanenko, Team Lead, PHP developer </div><p>Source: <a href="https://habr.com/ru/post/197456/">https://habr.com/ru/post/197456/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../197444/index.html">How to discern a forest behind trees: creating a three-dimensional image of the world's forests</a></li>
<li><a href="../197448/index.html">Google Maps API: location map, animation and styling</a></li>
<li><a href="../197450/index.html">As I did W-Mouse - a gaming mouse with unique abilities.</a></li>
<li><a href="../197452/index.html">Who needs interns?</a></li>
<li><a href="../197454/index.html">Laravel. Install, configure, create and deploy applications</a></li>
<li><a href="../197460/index.html">LG TVs: 2013 model range</a></li>
<li><a href="../197462/index.html">Big data is an integral part of our lives.</a></li>
<li><a href="../197464/index.html">The ninth Forum for IT directors "Efficiency in our genes!"</a></li>
<li><a href="../197466/index.html">How I hacked Habrahabr</a></li>
<li><a href="../197468/index.html">A couple of shortcomings in creating a web application on Go</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>