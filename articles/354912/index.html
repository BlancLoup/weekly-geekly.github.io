<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>PyTorch Tour</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! 

 Before the end of May, we will publish a translation of the book by Francois Chollet, Deep Learning in Python (examples using the Keras a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>PyTorch Tour</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr! <br><br>  Before the end of May, we will publish a translation of the book by Francois Chollet, <a href="https://www.manning.com/books/deep-learning-with-python">Deep Learning in Python</a> (examples using the Keras and Tensorflow libraries).  Do not miss! <br><br><img src="https://habrastorage.org/webt/71/ib/0b/71ib0bl144jgpdsslemhzx1enke.jpeg">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      But we, naturally, are looking into the impending future and are starting to look closely at the even more innovative <a href="https://pytorch.org/">PyTorch</a> library.  Today, we offer you a translation of the article by Peter Goldsboro, which is ready to arrange a <s><a href="http://kinglib.net/books/184-stiven-king-dolgaya-progulka.html">long walk</a></s> for you to <s><a href="http://kinglib.net/books/184-stiven-king-dolgaya-progulka.html">take a</a></s> tour of this library.  Under the cut a lot and interesting. <br><a name="habracut"></a><br>  For the last two years I have been seriously engaged in <a href="https://www.tensorflow.org/">TensorFlow</a> - I <a href="https://arxiv.org/pdf/1610.01178.pdf">wrote articles</a> on this library, <a href="https://www.youtube.com/watch%3Fv%3DLo1rXJdAJ7w">gave lectures</a> on expanding its backend, or <a href="https://www.biorxiv.org/content/early/2017/12/02/227645">used it in my own research</a> related to deep learning.  For this work, I learned quite well what were the strengths and what were the weaknesses of TensorFlow - and also got acquainted with specific architectural solutions that leave the field for competition.  With such baggage, I recently joined the PyTorch team in the department of <a href="https://research.fb.com/category/facebook-ai-research/">artificial intelligence research</a> at Facebook (FAIR) - perhaps this is currently the strongest competitor to TensorFlow.  Today, PyTorch is quite popular in the research community;  why - I will tell in the following paragraphs. <br><br>  In this article I want to give a quick overview of the PyTorch library, explain why it was created and introduce you to its API. <br><br>  <b>General picture and philosophy</b> <br><br>  First, let's look at what PyTorch represents from a fundamental point of view, which programming model you have to use when working with it, and how it fits into the ecosystem of modern deep learning tools: <br><br>  In essence, PyTorch is a Python library that provides tensor computations with GPU acceleration, like NumPy.  On top of this, PyTorch offers a rich API for solving applications related to neural networks. <br><br>  PyTorch differs from other machine learning frameworks in that it does not use static calculation graphs ‚Äî defined in advance, immediately and finally ‚Äî as in TensorFlow, <a href="https://caffe2.ai/">Caffe2</a> or <a href="https://mxnet.apache.org/">MXNet</a> .  In contrast, the calculated graphs in PyTorch are dynamic and are determined on the fly.  Thus, with each invocation of layers in the PyTorch model, a new calculated graph is dynamically determined.  This graph is created implicitly - that is, the library itself writes the flow of data going through the program, and links the function calls (nodes) together (via edges) into the calculated graph. <br><br>  <b>Comparison of dynamic and static graphs</b> <br><br>  Let's take a closer look at how static graphs differ from dynamic ones.  In general, in most programming environments, when adding two variables x and y, meaning numbers, their total value is obtained (the result of addition).  For example, in Python: <br><br><pre><code class="python hljs">In [<span class="hljs-number"><span class="hljs-number">1</span></span>]: x = <span class="hljs-number"><span class="hljs-number">4</span></span> In [<span class="hljs-number"><span class="hljs-number">2</span></span>]: y = <span class="hljs-number"><span class="hljs-number">2</span></span> In [<span class="hljs-number"><span class="hljs-number">3</span></span>]: x + y Out[<span class="hljs-number"><span class="hljs-number">3</span></span>]: <span class="hljs-number"><span class="hljs-number">6</span></span></code> </pre> <br>  But not in TensorFlow.  In TensorFlow, x and y are not numbers per se, but descriptors of nodes of the graph representing these values, but not explicitly containing them.  Moreover (more importantly), adding <code>x</code> and <code>y</code> will not result in the sum of these numbers, but in a descriptor of the calculated graph, which will give the desired value only after it is executed: <br><br><pre> <code class="python hljs">In [<span class="hljs-number"><span class="hljs-number">1</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf In [<span class="hljs-number"><span class="hljs-number">2</span></span>]: x = tf.constant(<span class="hljs-number"><span class="hljs-number">4</span></span>) In [<span class="hljs-number"><span class="hljs-number">3</span></span>]: y = tf.constant(<span class="hljs-number"><span class="hljs-number">2</span></span>) In [<span class="hljs-number"><span class="hljs-number">4</span></span>]: x + y Out[<span class="hljs-number"><span class="hljs-number">4</span></span>]: &lt;tf.Tensor <span class="hljs-string"><span class="hljs-string">'add:0'</span></span> shape=() dtype=int32&gt;</code> </pre> <br>  In principle, when we write TensorFlow code, it‚Äôs not actually programming, but <i>metaprogramming</i> .  We write a program (our code), which, in turn, creates another program (the TensorFlow calculation graph).  Naturally, the first programming model is much simpler than the second.  It is much more convenient to speak and reason in the context of <i>real phenomena</i> , rather than their <i>ideas</i> . <br><br>  The most important advantage of PyTorch is that its execution model is much closer to the first paradigm than to the second.  At its core, PyTorch is the most common Python with support for tensor calculations (like NumPy), but with GPU-accelerated tensor operations and, most importantly, with built-in <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">automatic differentiation</a> (AD).  Since most modern machine learning algorithms are heavily dependent on data types from linear algebra (matrices and vectors) and use gradient information to refine the estimates, these two pillars of PyTorch are enough to cope with arbitrarily large-scale machine learning problems. <br><br>  Returning to the simple case above, we can see that programming with PyTorch feels like ‚Äúnatural‚Äù Python: <br><br><pre> <code class="python hljs">In [<span class="hljs-number"><span class="hljs-number">1</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch In [<span class="hljs-number"><span class="hljs-number">2</span></span>]: x = torch.ones(<span class="hljs-number"><span class="hljs-number">1</span></span>) * <span class="hljs-number"><span class="hljs-number">4</span></span> In [<span class="hljs-number"><span class="hljs-number">3</span></span>]: y = torch.ones(<span class="hljs-number"><span class="hljs-number">1</span></span>) * <span class="hljs-number"><span class="hljs-number">2</span></span> In [<span class="hljs-number"><span class="hljs-number">4</span></span>]: x + y Out[<span class="hljs-number"><span class="hljs-number">4</span></span>]: <span class="hljs-number"><span class="hljs-number">6</span></span> [torch.FloatTensor of size <span class="hljs-number"><span class="hljs-number">1</span></span>]</code> </pre> <br>  PyTorch is slightly different from the basic logic of Python programming in one particular aspect: the library records the execution of a running program.  That is, PyTorch quietly ‚Äútracks‚Äù what operations you perform on its data types, and behind the scenes - again!  - collects the calculated graph.  Such a calculated graph is needed for automatic differentiation, since it must pass in the opposite direction along the chain of operations that yielded the resulting value in order to calculate the derivatives (for reverse automatic differentiation).  A serious difference between this calculated graph (or rather, the method of assembling this calculated graph) from the version of TensorFlow or MXNet is that the new graph is assembled ‚Äúgreedily‚Äù on the fly, when interpreting each code fragment. <br><br>  On the contrary, in Tensorflow, the calculated graph is constructed only once, the metaprogram (your code) is responsible for this.  Moreover, whereas PyTorch dynamically traverses the graph in the opposite direction whenever you request a derived value, TensorFlow simply inserts additional nodes into the graph, which (implicitly) calculate these derivatives and are interpreted exactly like all other nodes.  Here the difference between dynamic and static graphs is manifested especially clearly. <br><br>  Choosing which graphs to work with ‚Äî static or dynamic ‚Äî seriously simplifies the programming process in one of these environments.  The control flow is an aspect that is particularly affected by this choice.  In an environment with static graphs, the control flow must be represented at the graph level as specialized nodes.  For example, in Tensorflow, to provide branching, there is an operation <code>tf.cond()</code> , which accepts three subgraphs as input: a conditional subgraph and two subgraphs for two development branches of a condition: <code>if</code> and <code>else</code> .  Similarly, loops in Ternsorflow columns should be represented as <code>tf.while()</code> operations, taking the <code>condition</code> and the <code>body</code> subgraph as input.  In a situation with a dynamic graph, all this is simplified.  Since the graphs for each interpretation are viewed from the Python code as is, flow control can be natively implemented in the language using <code>if</code> conditions and <code>while</code> loops, as in any other program.  Thus, the clumsy and confused Tensorflow code: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf x = tf.constant(<span class="hljs-number"><span class="hljs-number">2</span></span>, shape=[<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>]) w = tf.while_loop( <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: tf.reduce_sum(x) &lt; <span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: tf.nn.relu(tf.square(x)), [x])</code> </pre> <br>  Turns into natural and understandable PyTorch code: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.autograd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Variable x = Variable(torch.ones([<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>]) * <span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> x.sum() &lt; <span class="hljs-number"><span class="hljs-number">100</span></span>: x = torch.nn.ReLU()(x**<span class="hljs-number"><span class="hljs-number">2</span></span>)</code> </pre> <br>  Naturally, in terms of ease of programming, the use of dynamic graphs is far from limited.  Just being able to check intermediate values ‚Äã‚Äãusing <code>print</code> instructions (and not using <code>tf.Print()</code> nodes) or in the debugger is a big plus.  Of course, dynamism can both optimize programmability as well as degrade performance ‚Äî that is, it is more difficult to optimize such graphs.  Therefore, the differences and trade-offs between PyTorch and TensorFlow are much the same as between a dynamic interpreted language, for example, Python, and a static compiled language, for example, C or C ++.  The first is easier and faster to work with, and from the second and third it is more convenient to collect entities that are well optimizable.  This is the trade-off between flexibility and performance. <br><br>  <b>PyTorch API Note</b> <br><br>  I want to make a general comment about the PyTorch API, especially regarding the calculation of neural networks compared to other libraries, for example, TensorFlow or MXNet - this API is hung with many modules (the so-called ‚Äúbatteries-included‚Äù).  As one of my colleagues noted, API Tensorflow didn‚Äôt really go beyond the ‚Äúassembly‚Äù level, in the sense that this API provides only the simplest assembly instructions needed to create computed graphs (addition, multiplication, pointwise functions, etc. d.).  But it is deprived of the ‚Äústandard library‚Äù for the most common program fragments, which the programmer has to reproduce thousands of times when working.  Therefore, to build higher-level APIs on top of Tensorflow, you have to rely on the help of the community. <br>  Indeed, the community has created such high-level APIs.  True, unfortunately, not one, but a dozen - in the order of rivalry.  Thus, on an unsuccessful day, you can read five articles on your specialization - and in all five you can find different frontends for TensorFlow.  As a rule, there is very little in common between these APIs, so in fact you will have to study 5 different frameworks, and not just TensorFlow.  Here are some of the most popular of these APIs: <br><br><ul><li>  <a href="https://keras.io/">Keras</a> </li><li>  <a href="https://github.com/tflearn/tflearn">Tflearn</a> </li><li>  <a href="https://github.com/google/prettytensor">Prettytensor</a> </li><li>  <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim">Tf-slim</a> </li></ul><br>  PyTorch, in turn, is already equipped with the most common elements needed for daily research in the field of deep learning.  In principle, it has a ‚Äúnative‚Äù Keras-like API in the torch.nn package, which provides the linkage of high-level modules of neural networks. <br><br>  <b>Place PyTorch in a shared ecosystem</b> <br><br>  So, explaining how PyTorch differs from static graph frameworks like MXNet, TensorFlow or Theano, I must say that PyTorch is actually not unique in its approach to computing neural networks.  Before PyTorch, other libraries already existed, for example, <a href="https://chainer.org/">Chainer</a> or <a href="https://github.com/clab/dynet">DyNet</a> , which provided a similar dynamic API.  However, today PyTorch is more popular than these alternatives. <br><br>  In addition, PyTorch is not the only framework used in Facebook.  The main workload in the production we now have is <a href="https://caffe2.ai/">Caffe2</a> - this is a static graph framework built on the basis of <a href="http://caffe.berkeleyvision.org/">Caffe</a> .  To make friends with the flexibility that PyTorch gives the researcher, with the advantages of static graphs in the field of production optimization, Facebook also develops <a href="https://onnx.ai/">ONNX</a> , a kind of information exchange between PyTorch, Caffe2 and other libraries, for example, MXNet or <a href="https://www.microsoft.com/en-us/cognitive-toolkit/">CNTK</a> . <br><br>  Finally, a small historical digression: before PyTorch, <a href="http://torch.ch/">Torch</a> existed - a very old (sample from the early 2000s) library for scientific computing, written in the <a href="https://www.lua.org/">Lua language</a> .  Torch wraps a code base written in C, which makes it fast and efficient.  In principle, PyTorch wraps exactly the same code base in C (but with <a href="https://github.com/zdevito/ATen">an additional intermediate level of abstraction</a> ), and exposes the API to the user in Python.  Next, let's talk about this API in Python. <br><br>  <b>Work with PyTorch</b> <br><br>  Next, we will discuss the basic concepts and key components of the PyTorch library, examine its basic data types, automatic differentiation mechanisms, specific functionality related to neural networks, as well as utilities for loading and processing data. <br><br>  <b>Tensors</b> <br><br>  The most fundamental data type in PyTorch is <code>tensor</code> .  The tensor data type is very similar in meaning and function to the <code>ndarray</code> of NumPy.  Moreover, since PyTorch is focused on reasonable interoperability with NumPy, the API <code>tensor</code> also resembles the <code>ndarray</code> API (but is not identical to it).  PyTorch tensors can be created using the <code>torch.Tensor</code> constructor, which accepts tensor dimensions as input and returns a tensor that occupies an <i>uninitialized</i> memory region: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch x = torch.Tensor(<span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>)</code> </pre> <br>  In practice, most often you have to use one of the following PyTorch functions that return tensors initialized in some way, for example: <br><br><ul><li>  <code>torch.rand</code> : values ‚Äã‚Äãare initialized from a random uniform distribution, </li><li>  <code>torch.randn</code> : values ‚Äã‚Äãare initialized from a random normal distribution, </li><li>  <code>torch.eye(n)</code> : <code>n√ónn√ón</code> identity matrix, </li><li>  <code>torch.from_numpy(ndarray)</code> : PyTorch tensor based on <code>ndarray</code> from NumPy </li><li>  <code>torch.linspace(start, end, steps)</code> : 1-D tensor with values ‚Äã‚Äãof <code>steps</code> evenly distributed between <code>start</code> and <code>end</code> , </li><li>  <code>torch.ones</code> : single-unit tensor, </li><li>  <code>torch.zeros_like(other)</code> : a tensor of the same shape as the <code>other</code> and with only zeros, </li><li>  <code>torch.arange(start, end, step)</code> : 1-D tensor with values ‚Äã‚Äãfilled out of range. </li></ul><br>  Similar to <code>ndarray</code> of NumPy, PyTorch tensors provide a very rich API for combination with other tensors, as well as for situational changes.  As in NumPy, unary and binary operations can usually be performed using functions from the <code>torch</code> module, for example, <code>torch.add(x, y)</code> or directly using methods in tensor objects, for example, <code>x.add(y)</code> .  For the most common places, there are overload operators, for example, <code>x + y</code> .  Moreover, for many functions, there are situational alternatives that will not create a new tensor, but change the recipient instance.  These functions are named the same as the standard variants, however, they contain an underscore in the title, for example: <code>x.add_(y)</code> . <br><br>  Selected operations: <br><br>  <code>torch.add(x, y)</code> : elementwise addition <br>  <code>torch.mm(x, y)</code> : matrix multiplication (not <code>matmul</code> or <code>dot</code> ), <br>  <code>torch.mul(x, y)</code> : elementwise multiplication <br>  <code>torch.exp(x)</code> : elementwise exponent <br>  <code>torch.pow(x, power)</code> : elementwise exponentiation <br>  <code>torch.sqrt(x)</code> : squaring by element <br>  <code>torch.sqrt_(x)</code> : situational, elementwise squaring <br>  <code>torch.sigmoid(x)</code> : elementwise sigmoid <br>  <code>torch.cumprod(x)</code> : the product of all values <br>  <code>torch.sum(x)</code> : the sum of all values <br>  <code>torch.std(x)</code> : standard deviation of all values <br>  <code>torch.mean(x)</code> : average of all values <br><br>  Tensors largely support semantics familiar from ndarray from NumPy, for example, translation, complex (whimsical) indexing ( <code>x[x &gt; 5]</code> ) and element-wise relational operators ( <code>x &gt; y</code> ).  PyTorch tensors can also be converted directly to <code>ndarray</code> NumPy using the <code>torch.Tensor.numpy()</code> function.  Finally, since the main superiority of PyTorch tensors over ndarray NumPy is GPU acceleration, you also have a feature <code>torch.Tensor.cuda()</code> , which copies the tensor memory to a GPU device with CUDA support, if any. <br><br>  <b>Autograd</b> <br><br>  At the heart of most modern machine learning techniques is the calculation of gradients.  This is especially true for neural networks, where the back-propagation algorithm is used to update the weights.  That is why Pytorch has a strong native support for gradient computing of functions and variables defined inside the framework.  Such a technique in which gradients are automatically calculated for arbitrary calculations is called automatic (sometimes <i>algorithmic</i> ) differentiation. <br><br>  In frameworks that use a static model for calculating graphs, automatic differentiation is implemented by analyzing the graph and adding additional computational nodes to it, where the gradient of one value relative to another is calculated step by step and the chain rule connecting these additional gradient nodes with edges is collected piece by piece. <br><br>  However, in PyTorch there are no statically calculated graphs, therefore, here we cannot afford the luxury of adding gradient nodes after the remaining calculations are defined.  Instead, PyTorch has to record or track the flow of values ‚Äã‚Äãthrough the program as they arrive, that is, dynamically build a calculated graph.  As soon as such a graph is written, PyTorch will have the information needed to reverse the flow of such a calculation and calculate the gradients of the output values ‚Äã‚Äãbased on the input. <br><br>  <code>Tensor</code> PyTorch <code>Tensor</code> does not yet have full mechanisms for participating in automatic differentiation.  To be able to record the tensor, it must be wrapped in <code>torch.autograd.Variable</code> .  The <code>Variable</code> class provides almost the same API as <code>Tensor</code> , but it complements its ability to interact with <code>torch.autograd.Function</code> precisely for the sake of automatic differentiation.  More precisely, the history of operations on the <code>Tensor</code> recorded in the <code>Variable</code> . <br><br>  Using <code>torch.autograd.Variable</code> very simple.  You just need to pass a <code>Tensor</code> to it and tell <code>torch</code> whether this variable requires writing gradients: <br><br><pre> <code class="python hljs">x = torch.autograd.Variable(torch.ones(<span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>), requires_grad=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br>  The function <code>requires_grad</code> may require a value of <code>False</code> , for example, when entering data or working with labels, since such information is usually not differentiated.  However, they still need to be <code>Variables</code> in order to be suitable for automatic differentiation.  Note: requires_grad is <code>False</code> by default, therefore, for the parameters to be trained, it should be set to <code>True</code> . <br><br>  To calculate gradients and perform automatic differentiation, the <code>backward()</code> function is applied to the <code>Variable</code> .  This calculates the gradient of this tensor relative to the leaves of the calculated graph (all input values ‚Äã‚Äãthat have influenced this).  These gradients are then assembled into a member of a <code>grad</code> class <code>Variable</code> : <br><br><pre> <code class="python hljs">In [<span class="hljs-number"><span class="hljs-number">1</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch In [<span class="hljs-number"><span class="hljs-number">2</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.autograd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Variable In [<span class="hljs-number"><span class="hljs-number">3</span></span>]: x = Variable(torch.ones(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>)) In [<span class="hljs-number"><span class="hljs-number">4</span></span>]: w = Variable(torch.randn(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), requires_grad=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) In [<span class="hljs-number"><span class="hljs-number">5</span></span>]: b = Variable(torch.randn(<span class="hljs-number"><span class="hljs-number">1</span></span>), requires_grad=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) In [<span class="hljs-number"><span class="hljs-number">6</span></span>]: y = x.mm(w) + b <span class="hljs-comment"><span class="hljs-comment"># mm = matrix multiply In [7]: y.backward() # perform automatic differentiation In [8]: w.grad Out[8]: Variable containing: 1 1 1 1 1 [torch.FloatTensor of size (5,1)] In [9]: b.grad Out[9]: Variable containing: 1 [torch.FloatTensor of size (1,)] In [10]: x.grad None</span></span></code> </pre><br>  Since all <code>Variable</code> except input values ‚Äã‚Äãare results of operations, each Variable is associated with <code>grad_fn</code> , which is a function <code>torch.autograd.Function</code> for calculating the inverse step.  For input values, it is <code>None</code> : <br><br><pre> <code class="python hljs">In [<span class="hljs-number"><span class="hljs-number">11</span></span>]: y.grad_fn Out[<span class="hljs-number"><span class="hljs-number">11</span></span>]: &lt;AddBackward1 at <span class="hljs-number"><span class="hljs-number">0x1077cef60</span></span>&gt; In [<span class="hljs-number"><span class="hljs-number">12</span></span>]: x.grad_fn <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> torch.nn</code> </pre> <br>  The <code>torch.nn</code> module provides <code>torch.nn</code> users with functionality specific to neural networks.  One of its most important members is <code>torch.nn.Module</code> , representing a reusable block of operations and associated (trained) parameters, most often used in layers of neural networks.  Modules may contain other modules and implicitly receive the <code>backward()</code> function for back distribution.  An example of a module is <code>torch.nn.Linear()</code> , which represents a linear (dense / fully connected) layer (ie, an affine transform <code>Wx+bWx+b</code> ): <br><br><pre> <code class="python hljs">In [<span class="hljs-number"><span class="hljs-number">1</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch In [<span class="hljs-number"><span class="hljs-number">2</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> nn In [<span class="hljs-number"><span class="hljs-number">3</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.autograd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Variable In [<span class="hljs-number"><span class="hljs-number">4</span></span>]: x = Variable(torch.ones(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>)) In [<span class="hljs-number"><span class="hljs-number">5</span></span>]: x Out[<span class="hljs-number"><span class="hljs-number">5</span></span>]: Variable containing: <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> [torch.FloatTensor of size (<span class="hljs-number"><span class="hljs-number">5</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>)] In [<span class="hljs-number"><span class="hljs-number">6</span></span>]: linear = nn.Linear(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) In [<span class="hljs-number"><span class="hljs-number">7</span></span>]: linear(x) Out[<span class="hljs-number"><span class="hljs-number">7</span></span>]: Variable containing: <span class="hljs-number"><span class="hljs-number">0.3324</span></span> <span class="hljs-number"><span class="hljs-number">0.3324</span></span> <span class="hljs-number"><span class="hljs-number">0.3324</span></span> <span class="hljs-number"><span class="hljs-number">0.3324</span></span> <span class="hljs-number"><span class="hljs-number">0.3324</span></span> [torch.FloatTensor of size (<span class="hljs-number"><span class="hljs-number">5</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)]</code> </pre> <br>  When training, it is often necessary to call <code>backward()</code> in a module in order to calculate gradients for its variables.  Since when calling <code>backward()</code> , the <code>Variables</code> member of the <code>Variables</code> , there is also the <code>nn.Module.zero_grad()</code> method, which drops the <code>grad</code> member of all <code>Variable</code> to zero.  Your training loop usually calls <code>zero_grad()</code> at the very beginning, or just before calling <code>backward()</code> , to reset the gradients for the next optimization step. <br><br>  When writing your own models for neural networks, you often have to write your own subclasses of the module to encapsulate the common functionality that you want to integrate with PyTorch.  This is done very simply - we inherit the class from <code>torch.nn.Module</code> and give it the <code>forward</code> method.  For example, here is the module that I wrote for one of my models (in it Gaussian noise is added to the input information): <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">AddNoise</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(torch.nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, mean=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.0</span></span></span></span><span class="hljs-function"><span class="hljs-params">, stddev=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> super(AddNoise, self).__init__() self.mean = mean self.stddev = stddev <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, input)</span></span></span><span class="hljs-function">:</span></span> noise = input.clone().normal_(self.mean, self.stddev) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> input + noise</code> </pre> <br>  You can use the container <code>torch.nn.Sequential()</code> , which is transferred a sequence of modules, to connect or <i>hook</i> modules into full-featured models - and it, in turn, begins to act as an independent module, each call sequentially calculating those modules that were passed to it.  For example: <br><br><pre> <code class="python hljs">In [<span class="hljs-number"><span class="hljs-number">1</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch In [<span class="hljs-number"><span class="hljs-number">2</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> nn In [<span class="hljs-number"><span class="hljs-number">3</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.autograd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Variable In [<span class="hljs-number"><span class="hljs-number">4</span></span>]: model = nn.Sequential( ...: nn.Conv2d(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), ...: nn.ReLU(), ...: nn.Conv2d(<span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), ...: nn.ReLU()) ...: In [<span class="hljs-number"><span class="hljs-number">5</span></span>]: image = Variable(torch.rand(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>)) In [<span class="hljs-number"><span class="hljs-number">6</span></span>]: model(image) Out[<span class="hljs-number"><span class="hljs-number">6</span></span>]: Variable containing: (<span class="hljs-number"><span class="hljs-number">0</span></span> ,<span class="hljs-number"><span class="hljs-number">0</span></span> ,.,.) = <span class="hljs-number"><span class="hljs-number">0.0026</span></span> <span class="hljs-number"><span class="hljs-number">0.0685</span></span> <span class="hljs-number"><span class="hljs-number">0.0000</span></span> ... <span class="hljs-number"><span class="hljs-number">0.0000</span></span> <span class="hljs-number"><span class="hljs-number">0.1864</span></span> <span class="hljs-number"><span class="hljs-number">0.0413</span></span> <span class="hljs-number"><span class="hljs-number">0.0000</span></span> <span class="hljs-number"><span class="hljs-number">0.0979</span></span> <span class="hljs-number"><span class="hljs-number">0.0119</span></span> ... <span class="hljs-number"><span class="hljs-number">0.1637</span></span> <span class="hljs-number"><span class="hljs-number">0.0618</span></span> <span class="hljs-number"><span class="hljs-number">0.0000</span></span> <span class="hljs-number"><span class="hljs-number">0.0000</span></span> <span class="hljs-number"><span class="hljs-number">0.0000</span></span> <span class="hljs-number"><span class="hljs-number">0.0000</span></span> ... <span class="hljs-number"><span class="hljs-number">0.1289</span></span> <span class="hljs-number"><span class="hljs-number">0.1293</span></span> <span class="hljs-number"><span class="hljs-number">0.0000</span></span> ... ‚ã± ... <span class="hljs-number"><span class="hljs-number">0.1006</span></span> <span class="hljs-number"><span class="hljs-number">0.1270</span></span> <span class="hljs-number"><span class="hljs-number">0.0723</span></span> ... <span class="hljs-number"><span class="hljs-number">0.0000</span></span> <span class="hljs-number"><span class="hljs-number">0.1026</span></span> <span class="hljs-number"><span class="hljs-number">0.0000</span></span> <span class="hljs-number"><span class="hljs-number">0.0000</span></span> <span class="hljs-number"><span class="hljs-number">0.0000</span></span> <span class="hljs-number"><span class="hljs-number">0.0574</span></span> ... <span class="hljs-number"><span class="hljs-number">0.1491</span></span> <span class="hljs-number"><span class="hljs-number">0.0000</span></span> <span class="hljs-number"><span class="hljs-number">0.0191</span></span> <span class="hljs-number"><span class="hljs-number">0.0150</span></span> <span class="hljs-number"><span class="hljs-number">0.0321</span></span> <span class="hljs-number"><span class="hljs-number">0.0000</span></span> ... <span class="hljs-number"><span class="hljs-number">0.0204</span></span> <span class="hljs-number"><span class="hljs-number">0.0146</span></span> <span class="hljs-number"><span class="hljs-number">0.1724</span></span></code> </pre> <br>  <b>Losses</b> <br><br>  <code>torch.nn</code> also provides a number of loss functions, naturally important for machine learning applications.  Examples of such functions: <br><br><ul><li>  <code>torch.nn.MSELoss</code> : mean square loss function </li><li>  <code>torch.nn.BCELoss</code> : loss function of binary cross-entropy, </li><li>  <code>torch.nn.KLDivLoss</code> : loss function of Kullback-Leibler information discrepancy </li></ul><br>  In the context of PyTorch, loss functions are often referred to as <i>criteria</i> .  In essence, the criteria are very simple modules that can be parameterized immediately after creation, and from this point on can be used as ordinary functions: <br><br><pre> <code class="python hljs">In [<span class="hljs-number"><span class="hljs-number">1</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch In [<span class="hljs-number"><span class="hljs-number">2</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn In [<span class="hljs-number"><span class="hljs-number">3</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.autograd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Variable In [<span class="hljs-number"><span class="hljs-number">4</span></span>]: x = Variable(torch.randn(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>)) In [<span class="hljs-number"><span class="hljs-number">5</span></span>]: y = Variable(torch.ones(<span class="hljs-number"><span class="hljs-number">10</span></span>).type(torch.LongTensor)) In [<span class="hljs-number"><span class="hljs-number">6</span></span>]: weights = Variable(torch.Tensor([<span class="hljs-number"><span class="hljs-number">0.2</span></span>, <span class="hljs-number"><span class="hljs-number">0.2</span></span>, <span class="hljs-number"><span class="hljs-number">0.6</span></span>])) In [<span class="hljs-number"><span class="hljs-number">7</span></span>]: loss_function = torch.nn.CrossEntropyLoss(weight=weights) In [<span class="hljs-number"><span class="hljs-number">8</span></span>]: loss_value = loss_function(x, y) Out [<span class="hljs-number"><span class="hljs-number">8</span></span>]: Variable containing: <span class="hljs-number"><span class="hljs-number">1.2380</span></span> [torch.FloatTensor of size (<span class="hljs-number"><span class="hljs-number">1</span></span>,)]</code> </pre> <br>  <b>Optimizers</b> <br><br>  After the "primary elements" of neural networks ( <code>nn.Module</code> ) and loss functions, it remains to consider only the optimizer, which triggers a stochastic gradient descent (variant).    PyTorch   <code>torch.optim</code> ,       ,  : <br><br><ul><li> <code>torch.optim.SGD</code> : <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">  </a> , </li><li> <code>torch.optim.Adam</code> : <a href="https://arxiv.org/pdf/1412.6980.pdf">  </a> , </li><li> <code>torch.optim.RMSprop</code> : <a href="https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop"></a> ,         Coursera, </li><li> <code>torch.optim.LBFGS</code> : <a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS"> ---    </a> </li></ul><br>        -,    <code>parameters()</code>   <code>nn.Module</code> , ,     .           ,    .  For example: <br><br><pre> <code class="python hljs">In [<span class="hljs-number"><span class="hljs-number">1</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch In [<span class="hljs-number"><span class="hljs-number">2</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.optim In [<span class="hljs-number"><span class="hljs-number">3</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch.autograd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Variable In [<span class="hljs-number"><span class="hljs-number">4</span></span>]: x = Variable(torch.randn(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>)) In [<span class="hljs-number"><span class="hljs-number">5</span></span>]: y = Variable(torch.randn(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), requires_grad=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) In [<span class="hljs-number"><span class="hljs-number">6</span></span>]: z = x.mm(y).mean() <span class="hljs-comment"><span class="hljs-comment"># Perform an operation In [7]: opt = torch.optim.Adam([y], lr=2e-4, betas=(0.5, 0.999)) In [8]: z.backward() # Calculate gradients In [9]: y.data Out[9]: -0.4109 -0.0521 0.1481 1.9327 1.5276 -1.2396 0.0819 -1.3986 -0.0576 1.9694 0.6252 0.7571 -2.2882 -0.1773 1.4825 0.2634 -2.1945 -2.0998 0.7056 1.6744 1.5266 1.7088 0.7706 -0.7874 -0.0161 [torch.FloatTensor of size 5x5] In [10]: opt.step() #  y     Adam In [11]: y.data Out[11]: -0.4107 -0.0519 0.1483 1.9329 1.5278 -1.2398 0.0817 -1.3988 -0.0578 1.9692 0.6250 0.7569 -2.2884 -0.1775 1.4823 0.2636 -2.1943 -2.0996 0.7058 1.6746 1.5264 1.7086 0.7704 -0.7876 -0.0163 [torch.FloatTensor of size 5x5]</span></span></code> </pre> <br> <b> </b> <br><br>    PyTorch      ,       .       <code>torch.utils.data module</code> .        : <br><br><ol><li> <code>Dataset</code> ,   , </li><li> <code>DataLoader</code> ,    , ,   . </li></ol><br>       <code>torch.utils.data.Dataset</code>    <code>__len__</code> , ,       ,    <code>__getitem__</code>        . ,    ,      : <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> math <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">RangeDataset</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(torch.utils.data.Dataset)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, start, end, step=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> self.start = start self.end = end self.step = step <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__len__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, length)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> math.ceil((self.end - self.start) / self.step) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__getitem__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, index)</span></span></span><span class="hljs-function">:</span></span> value = self.start + index * self.step <span class="hljs-keyword"><span class="hljs-keyword">assert</span></span> value &lt; self.end <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> value</code> </pre> <br>  <code>__init__</code>   -         .  <code>__len__</code>    ,      <code>__getitem__</code> ,   <code>__getitem__</code>   , ,   . <br><br>   , ,  ,   <code>for i in range</code>       <code>__getitem__</code> . ,    ,       ,          <code>for sample in dataset</code> .  ,      <code>DataLoader</code> .  <code>DataLoader</code>     ,    . ,    ,   .    <code>DataLoader</code>   <code>num_workers</code> .  : <code>DataLoader</code>   ,      <code>batch_size</code> .  A simple example: <br><br><pre> <code class="python hljs">dataset = RangeDataset(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>) data_loader = torch.utils.data.DataLoader( dataset, batch_size=<span class="hljs-number"><span class="hljs-number">4</span></span>, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, num_workers=<span class="hljs-number"><span class="hljs-number">2</span></span>, drop_last=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, batch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(data_loader): print(i, batch)</code> </pre> <br>   <code>batch_size</code>  4,         .   <code>shuffle=True</code> ,        ,        .    <code>drop_last=True</code> , ,         ,    <code>batch_size</code> ,     . ,    <code>num_workers</code>  ¬´¬ª,  ,      .  ,  <code>DataLoader</code>  ,   , ,  ,    . <br><br>    ,    : <code>DataLoader</code>  <a href="https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py">  </a> , ,  <i></i>  ,    <code>__getitem__</code>  ,   ,  <code>DataLoader</code>  . ,  <code>__getitem__</code>  ,  <code>DataLoader</code>       ,   ,   .  , ,   <code>__getitem__</code>   <code>dict(example=example, label=label)</code> ,  ,  <code>DataLoader</code> ,    <code>dict(example=[example1, example2, ...], label=[label1, label2, ...])</code> ,  ,    ,         .    ,       <code>collate_fn</code>  <code>DataLoader</code> . <br><br>  :   <code><a href="https://github.com/pytorch/vision">torchvision</a></code>    , , <code>torchvision.datasets.CIFAR10</code> .     <code>torchaudio</code>  <code>torchtext</code> . <br><br>  <b>Conclusion</b> <br><br> ,       PyTorch,    API,  ,      PyTorch.       PyTorch,         ,            PyTorch. ,    PyTorch <a href="https://github.com/goldsborough/cytogan/blob/master/playground/gan/lsgan.py"> LSGAN,   TensorFlow</a> ,       .     ,  <a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"></a>  <a href="https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"></a> . </div><p>Source: <a href="https://habr.com/ru/post/354912/">https://habr.com/ru/post/354912/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../354896/index.html">MIT course "Computer Systems Security". Lecture 1: "Introduction: threat models", part 3</a></li>
<li><a href="../354898/index.html">Fractal manifold method in Data Science problems</a></li>
<li><a href="../354902/index.html">As I wrote a graphic bot and what it turned into</a></li>
<li><a href="../354906/index.html">Distribution of programs on Go. Part 1</a></li>
<li><a href="../354910/index.html">By the Day of Radio. Oh connection, you are the world</a></li>
<li><a href="../354914/index.html">Setting security for applications on the SAP Cloud Platform cloud platform</a></li>
<li><a href="../354916/index.html">Experience using Mikrotik CHR for organizing virtual routing</a></li>
<li><a href="../354930/index.html">BigInt - long arithmetic in javascript</a></li>
<li><a href="../354934/index.html">Plausible reconstruction of Instagram-like filters</a></li>
<li><a href="../354936/index.html">(Akin's laws) laws of space engineering</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>