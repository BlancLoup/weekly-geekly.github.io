<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Onto-engineer: from the creation of the world to the generation of entities</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this post I will continue the story about the part of Compreno, which is related to the profession of engineer. Well, or about the work of an engin...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Onto-engineer: from the creation of the world to the generation of entities</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/1f0/125/a34/1f0125a346be474187289914441232ee.jpg" align="right">  In this post I will continue the <a href="http://habrahabr.ru/company/abbyy/blog/245509/">story</a> about the part of Compreno, which is related to the profession of engineer.  Well, or about the work of an engineer who is associated with the above-mentioned technology - this is someone who is more comfortable to perceive. <br><br>  Let me remind you that the first part has led us to the fact that onto-engineers build <abbr title="simplistic speaking formal world models">ontologies</abbr> so that the technology can work (without them, nowhere, everything is so arranged). <br><div class="spoiler">  <b class="spoiler_title">A little more complete description of the first part:</b> <div class="spoiler_text"><ul><li>  Our system of information extraction is based on the presentation of the text in the form of syntactic-semantic trees Compreno. </li><li>  The tree nodes correspond approximately to the words in the sentence, and the arcs reflect the dependencies between them (from the point of view of the <a href="http://en.wikipedia.org/wiki/Dependency_grammar">dependency grammar</a> ). </li><li>  Trees are a formal representation of the ‚Äúmeaning‚Äù of a statement, therefore language ambiguities are already resolved in them. </li><li>  Having received these trees at the input, at the output the system issues information objects - entities (persons, organizations, locations, etc.) or facts (arrests, deaths, purchases, relatives, education, etc.). </li><li>  The formal models of reality, within which all these facts and entities exist, are called ontologies.  Automotive engineers develop ontologies using <a href="http://www.w3.org/TR/owl2-overview/">the OWL standard</a> . </li></ul></div></div><br>  About what else, and, of course, why the on-engineers are doing, I propose to find out right now. <br><br><h4>  Seven battles - one subtree </h4><br>  The main engineer devotes the bulk of his work time not to ‚Äúmodeling the world‚Äù (although this sounds very proudly), but to creating an extraction system.  And although we are increasingly experimenting with statistics, machine learning and automatic pattern extraction, for now our products and projects use rules written by hand.  However, these rules are not some kind of rigid patterns based on the linear order of words in a sentence, but descriptions of fragments of ABBYY Compreno semantic-syntactic trees.  This allows us to bypass the variability and ambiguity of a language relatively easily by briefly specifying the many options used to express the same meaning. <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      A simple example: if we ask you to find such a subtree in which there is a node node with the semantic class ‚ÄúCONFLICT_INTERACTION‚Äù (a high enough class of our hierarchy, from which all concepts related to confrontation, conflicts, confrontations and competitive activity are inherited), under which there is the child node with the Agent deep position (actively acting participant in the situation) and the ContrAgent (special ‚Äúcounterparty‚Äù deep position), various examples of confrontations will be found: <br><blockquote>  656 BC  e.  - The Battle of Tullise - The <b>Battle of the Elamites with the Assyrians</b> <br>  474 BC  e.  - Battle of Cumas - <b>victory of the Syracusans</b> under the leadership of Hieron I over the Etruscans <br>  February 2 - Day of the <b>defeat of</b> nazi <b>troops by the</b> Soviet <b>troops</b> in the Battle of Stalingrad <br>  On October 1, the <b>battle of Povetkin and American</b> Tommy Connelly was originally scheduled <br>  A series of regicides marked the beginning of the <b>struggle of the nobility with kings</b> <br>  The artist V.P. Vereshchagin painted the painting ‚Äú <b>The Battle of Dobrynya</b> with the <b>Serpent</b> Gorynych‚Äù for the palace of Grand Duke Vladimir Alexandrovich <br>  For almost three decades, a small <b>detachment of</b> archers <b>fought</b> with <b>foreigners</b> <br>  How <b>Ivan</b> Ivanovich <b>fell out</b> with <b>Ivan</b> Nikiforovich <br>  French ambassador to Turkey began to make every effort to <b>embroil Russia with Turkey</b> <br>  In 1983, it came to street <b>punks clashes with police</b> <br>  in <b>Medvedev‚Äôs conflict with Kudrin,</b> Putin ‚Äúnon-publicly, apparently, sided with the Minister of Finance <br>  Twice <b>they broke the</b> Siberian <b>Tatars</b> , on the Tour and at the mouth of the Tavda <br>  Courageously, like thousands of other citizens, <b>Shostkins fought</b> with the Nazi <b>invaders</b> on the fronts of the Great Patriotic War, in partisan detachments <br>  Since 1989, the <b>Count</b> begins uncompromising <b>rivalry</b> with the Yugoslav <b>tennis player</b> Monica Seles <br>  Other feline cats, especially long-tailed cats and ocelots, are food competitors of the jaguarundi, however this cat avoids direct competition with them due to its daytime lifestyle;  <b>they</b> also <b>compete</b> with <b>foxes</b> , coyotes, ginger lynxes and pumas </blockquote><br><img src="https://habrastorage.org/files/94a/8ab/e7f/94a8abe7fae34d889c73d453344a646b.png"><br><img src="https://habrastorage.org/files/38f/be3/19a/38fbe319aec54bf498a24805b256a749.png"><br><br>  Then we can introduce many other conditions.  For example, it is easy to limit the semantic classes of participants and demand that the class ‚ÄúHUMAN‚Äù be there - then only examples like Ivan Ivanovich with Ivan Nikiforovich will remain, and the quarrel between Russia and Turkey will not be found.  In addition, you can enter conditions for different grammatical characteristics (time, number, pledge), positions in the surface syntax, semantic properties - thousands of them.  And now more about how we are looking for such subtrees and what else can you ask from Compreno. <br><br>  Templates for subtrees are set using production rules.  These rules consist of classic ‚Äúif - then‚Äù products, in which the left and right sides are separated using the =&gt; operator.  On the left, we write a condition that defines a set of subtrees.  In the right - statements about the existence of information objects (ie, entities and facts), their relationship and binding to the text (ie, to some nodes of the trees).  In the tree patterns of the left, standard logical connectives are used (conjunction, disjunction, negation), as well as conditions for the mutual arrangement of nodes in the tree.  In particular, it is possible to check whether a node is directly a child or whether it enters into the subtree of another node at all. <br><br>  We give an example - one of the rules for the extraction of persons.  It deals with the case when a person is mentioned with a generic prefix (‚Äúvon Bismarck‚Äù, etc.).  Square brackets indicate a transition to a child node.  In the left part of the rule, two variables are entered: von and this.  In the right part, logical statements are formulated that use the variables entered: <br><br><pre><code class="javascript hljs"><span class="hljs-comment"><span class="hljs-comment">//,      . ~&lt;Lex_NameBracketed&gt; [ //      ‚Äì ,   . . von "PART_OF_SURNAME_PREFIX" [ //   :      ,     //    .  , , //        (  //    ,     // ) this ( &lt;!InitialCore!&gt; ~&lt;Lex_NameBracketed&gt; ~"FOREIGN_WORD" ~"ACRONYM_") | "PERSON_BY_LASTNAME" ] ] =&gt; //     Person,   , //    von.      P // (    ). Person P( von ), //         :  //    ,   von,   //  ‚Äì  ,   this. //   core ,     //     (),   . annotation( P, von.core, this.core ), // ,   P     ,   //  this.  c Coreferential ,   //      ,    //  ,  . anchor( P, this, Coreferential ), // ,    middlename  , //         , //      middlename_cs.  //  :      // middlename_cs     middlename. P.middlename == Norm( P.middlename_cs ), // ,    surname  , //          //  ,   von  this. P.surname == Norm( von.core, this.core );</span></span></code> </pre> <br><br>  When developing the system, it quickly became clear that we want to be able to refer to already extracted objects from other rules, rely on them in conditions, modify them after extraction, etc.  For this, a functional was implemented that allows you to search in a subtree not only for nodes with some linguistic properties, but also for objects attached to these nodes created by other rules.  Thus, object conditions appeared in our products.  Here is a fragment of the rule in which such an object condition is used.  In this rule, we are looking for an already extracted person to add the necessary attributes to it. <br><br><pre> <code class="javascript hljs"><span class="hljs-comment"><span class="hljs-comment">//    ‚Äì ,    this "PERSON_BY_FIRSTNAME" [ //      ,    //      P.   //   ‚Äì     ,  //   - (    // ).     P  // . //  ,   ,       //  P         // Classifier_Name.  ,   ,   //   middle. //  ,   P       // Person,    surname     ,  //  firstname . ...( Classifier_Name: middle "PERSON_BY_FIRSTNAME" ) !P ( "PERSON_BY_LASTNAME" | "PART_OF_SURNAME_PREFIX" ) &lt;% Person, surname ~= null, firstname == null %&gt; ] =&gt; // ,   ,    middle, //      middlename_cs. Pomiddlename_cs == middle, ...</span></span></code> </pre><br><br>  In the examples above, we relied either on a name we know (part of the name) or on an explicit name pointer (noble prefix).  However, it is clear that all persons (as well as all organizations) cannot be included in our hierarchy.  Often, company names and people's names are recognized as unknown words. <br><br>  In this case, ABBYY Compreno allows us to rely on numerous indirect signs.  For example, if an unknown word (and even with a capital letter) turns out to be ‚Äúregistered in California,‚Äù conducts an IPO, or, for example, ‚Äúgets married‚Äù or ‚Äúhas a cold‚Äù, everything becomes clear to us.  Using the ABBYY Compreno trees, indicating the necessary semantic classes, deep positions and other signs, we can minimize erroneous extracts and catch such nontrivial organizations and persons. <br><br>  In addition, thanks to the object conditions, we can improve the extraction of some entities based on others.  For example, a person with an unknown surname ‚ÄúPyschysch‚Äù stood out in our text for some signs.  And below the phrase Pyschspish sells assets.  At the same time, the engineer who creates the rules for extracting organizations knows that the assets of the organization are most often sold, and would like to extract them in such cases.  Then he can put in his condition a negative object condition ~ &lt;% Person%&gt; (= there is no Person object on the node) - and in this particular case the rule will not work, because  there will already be a person on the Pimple. <br><br>  Now about how we extract events and facts, and all these people and organizations become their participants.  For example, let's experiment with the extraction of the fact of buying or selling something.  Create a new rule with a single product and write the following condition: <br><br><pre> <code class="javascript hljs"><span class="hljs-string"><span class="hljs-string">"TO_ACQUIRE"</span></span> =&gt; PurchaseAndSale P (<span class="hljs-keyword"><span class="hljs-keyword">this</span></span>), <span class="hljs-comment"><span class="hljs-comment">//     PurchaseAndSale annotation (P, this.core); //  </span></span></code> </pre><br><br>  Here we ask to find in the tree a node with the semantic class TO_ACQUIRE or its descendant.  This is the semantic class in the hierarchy for Russian and English: <br><br><img src="https://habrastorage.org/files/a92/851/878/a928518781464f38b10672f16be32c55.png"><img src="https://habrastorage.org/files/288/89e/682/28889e68248b4f0589e41d8a0a5b3f09.png"><br><br>  Products will be triggered in all cases when someone buys something, and it doesn‚Äôt even matter whether the participants are in the text. <br><br>  Now create a new rule: <br><pre> <code class="javascript hljs">&lt;%PurchaseAndSale%&gt; [Possessor: !customer] =&gt; CustomerRole CR (customer), <span class="hljs-comment"><span class="hljs-comment">//     PurchaseAndSale annotation (CR, customer.core), //   this.o.customer == CR; //      customer ()     PurchaseAndSale</span></span></code> </pre><br><br>  This rule searches under the node with the fact that PurchaseAndSale has already been retrieved a child node in the Possessor deep position (possessor).  For example, Vasya gets into the position of Possessor in statements like Vasya bought, Vasya purchased, etc. <br><br>  In the right part, we create a special role object CustomerRole, which we then place in relation to the customer (buyer) of the found fact.  If on the same node some full-fledged entity is extracted (for example, the person ‚ÄúVasya‚Äù), then the special rule will replace the role object with this entity.  In the end, we will have the fact of PurchaseAndSale, in which Vasya‚Äôs person is in relation to the customer. <br><br>  Similarly arranged rules for the extraction of the buyer, product and price.  The only difference is that the product in this case falls into the deep Object position (the object undergoing action), the price goes to the special position Ch_Parameter_Price (price parameter), the buyer goes to the Source position (source role): <br><br><pre> <code class="javascript hljs">&lt;%PurchaseAndSale%&gt; [<span class="hljs-built_in"><span class="hljs-built_in">Object</span></span>: !sold_property] =&gt; ‚Ä¶ &lt;%PurchaseAndSale%&gt; [Ch_Parameter_Price: !price] =&gt; ‚Ä¶ &lt;%PurchaseAndSale%&gt; [Source: !seller] =&gt; ‚Ä¶</code> </pre><br><br>  These rules will work for examples like " <i>Vasya bought a toy from Masha</i> ", " <i>Petya bought a car</i> ", " <i>Nikolai goes to buy a pink elephant</i> ", " <i>Auto.ru was bought by Yandex for $ 175 million</i> ", " <i>Pavel Durov acquired the telegram.me domain</i> " etc.  First, the fact of PurchaseAndSale will be extracted, and then the correct participants will be allocated to it: the buyer Vasya, the seller Masha and the object of sale and purchase ‚Äútoys‚Äù, etc.  for all examples.  At the same time, neither the word order (‚ÄúI <i>bought a toy Vasya</i> ,‚Äù ‚ÄúI <i>bought a toy Vasya</i> ,‚Äù ‚ÄúI <i>bought a</i> <i>toy Vasya</i> ‚Äù), neither specific names, nor the verb tense is important to us.  Moreover, since all elements of the trees on which we relied (deep positions and semantic classes) are universal in our system, that is, common to all supported languages, the rule will work not only for Russian, but also for English.  There are much more synonyms for an action that means a purchase (to put it in our terms, the semantic class ‚ÄúTO_ACQUIRE‚Äù has many more lexical classes).  Therefore, these simple products will find the fact of sale and correctly extract the participants in all the examples below: <br><br>  <i>Lenovo buys Motorola from Google.</i> <i><br></i>  <i>Lenovo purchased Motorola.</i> <i><br></i>  <i>Lenovo has just acquired Motorola.</i> <i><br></i>  <i>Motorola has been acquired by Lenovo.</i> <i><br></i>  <i>Lenovo's purchase of Motorola.</i> <i><br></i>  <i>Lenovo's acquisition of Motorola.</i> <i><br></i>  <i>Motorola's acquisition by Lenovo.</i> <br><br>  This will work precisely because in all cases there is a subtree in the analysis in which there is a vertex with the semantic class ‚ÄúTO_ACQUIRE‚Äù (or its descendant) and its children in the deep positions Possessor (Lenovo), Object (Motorola), Source (Google in the first example ). <br><br><img src="https://habrastorage.org/files/ec1/bfd/707/ec1bfd707e6e4cfab44358926758dac1.png"><br><br>  In addition to the extraction rules, there is another type of rules - local identification rules.  They allow you to collect all references to the same entity or fact within the same text.  For example, in one part of the text the name of the person is found, and in the other - the pronoun ‚Äúhe‚Äù (how ABBYY Compreno copes with the pronominal anaphor can be read <a href="http://habrahabr.ru/company/abbyy/blog/229515/">here</a> , and even more - <a href="http://www.dialog-21.ru/digests/dialog2014/materials/pdf/BogdanovAV.pdf">here</a> ).  Or the same person is mentioned several times (once - by full name, and then - only by last name).  Identification rules work within the same text and do not apply to trees, but operate only on objects, their attributes and the distance between them.  For example, we can write a rule that connects two facts of purchase and sale into one if they have a buyer and a seller, and they are close to each other.  They work in this (completely real) example: <br><br>  <i><b>Google</b> is <b>selling Motorola</b> to <b>Lenovo</b> , giving it a share in the US market.</i>  <i><b>Lenovo</b> will <b>buy Motorola</b> for <b>$ 2.91 billion</b> in a mixture of cash and stock.</i>  <i>(The Verge)</i> <br><br>  That is, here the system truly understands that the two sentences are about the same fact.  As a result, all information about the transaction (price, buyer, seller, product), scattered across two separate statements, is combined in ONE information object. <br><br>  It is clear that the problem of identification arises on a more general level - when analyzing text collections.  Often the same information objects are transferred from text to text.  To do this, separate global identification mechanisms are implemented, based on special patterns and machine learning methods, but for a meaningful story about them, you will need a separate large article. <br><br>  After all the rules are triggered, the extracted facts and entities are written into an XML document, in accordance with the RDF data description format.  This document defines an RDF graph, in which the connections of objects and their association with text fragments are visible (= their annotations).  The resulting information graph is the end result of the extraction system.  Then you can use it as you like - fill the information in the database, visualize, etc.  So, for example, visualization of the graph of facts and entities extracted from the offer from TechCranch looks like: <i>In 2005, Yahoo purchased Alibaba for $ 1 billion</i> <br><br><img src="https://habrastorage.org/files/b32/37a/678/b3237a6786204615b04ab8544a986531.png"><br><br><h4>  Why is all this necessary? </h4><br>  The system described above is essentially a factory for creating various ontologies and corresponding models for extracting facts and entities.  What exactly will be ‚Äúproduced‚Äù in this factory depends already on the diverse needs of specific customers.  And I must say that in many cases, the ‚Äúsemantic depth‚Äù of the analysis and the ability to remove ambiguity turn out to be simply irreplaceable for us - for example, the ABBYY Compreno parser is able to distinguish the grass that is on the lawn (the semantic class ‚ÄúGRASS‚Äù) from the one you thought (semantic class ‚ÄúMARIJUANA‚Äù) ... <br><br>  In the course of the work, not only technical difficulties arise, but also problems of a philosophical and ideological nature.  Therefore, in the engineering department it is often possible to hear disputes about whether Santa Claus and Batman are personas, whether the essence of the ‚Äúdrug‚Äù must be extracted on ‚Äúheroin addicts‚Äù, whether murder is always a crime and what fact to isolate if dogs have been bitten off by someone.  In the course of these discussions, you can hear a lot of funny phrases, and in my free time I even started collecting a collection called ‚ÄúSo they say ontonenergy.‚Äù  Here is a little bit from there: <br><br><ul><li>  And you can ask how I should separate the corpse from the bones? </li><li>  She needs a concept in which not immediately full depravity! </li><li>  And the gods stand out as people?  - These are not people, but these are personalities! </li><li>  Please kill this person! </li><li>  The engineer_1: This came to us like his ... <br>  Ontoinzhener_2: (obscene word, meaning the end of everything)? <br>  Onto engineer_1: No, advance! <br></li><li>  Who broke the Bank account? </li></ul><br>  In addition to purely commercial ventures, there are those that are not about profit, but about eternal values.  Many have probably heard about the project ABBYY and the Museum of Tolstoy " <a href="http://www.readingtolstoy.ru/">All Tolstoy in one click</a> ", which <a href="http://www.theguardian.com/books/2013/oct/16/all-leo-tolstoy-one-click-project-digitisation">Guardian</a> and <a href="http://www.newyorker.com/books/page-turner/crowdsourcing-tolstoy">New Yorker</a> wrote about as an impressive crowdsourcing breakthrough.  The objectives of this project - digitization and reading (by indifferent volunteers) 90 volumes of the complete works of the writer - were achieved ahead of all plans, and now a new task has appeared - <a href="http://tolstoy.ru/projects/tolstoy-digital/">Tolstoy's semantic edition</a> .  This project aims to set the standards for publishing a classic heritage in the digital age ‚Äî with semantic markup, extraction and identification of fictional and real entities, links to publicly available knowledge bases like dbpedia or freebase.  We hope that the use of an information extraction system based on ABBYY Compreno will help reduce the amount of manual labor in this project as significantly as the use of ABBYY FineReader when digitizing a 90-volume book. <br><br>  In parallel with the work to order, we are creating on the basis of our technologies a more universal product of ‚Äúcommon use‚Äù - InfoExtractor.  It extracts all traditional entities (persons, organizations, locations) and facts (buying and selling, employment, education, family ties, and much more) appearing in news and journalistic texts.  Now InfoExtractor exists in the form of <a href="http://www.abbyy.ru/itagger">search and analytical SDK IntelligentTagger</a> , in the future we plan to release several new "smart" products with an eye to extracting information. </div><p>Source: <a href="https://habr.com/ru/post/246039/">https://habr.com/ru/post/246039/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../246025/index.html">Grab full control over someone else's Digital Ocean cloud (Doorkeeper vulnerability)</a></li>
<li><a href="../246027/index.html">A tale about how I disassembled Supaplex and almost wrote a clone with 3D graphics</a></li>
<li><a href="../246029/index.html">Dnipropetrovsk Ciklum Speakers' Corner with Igor Kryzhanovsky: UX is not UI. ‚ÄúThe Art of Washing the Elephant‚Äù, December 18</a></li>
<li><a href="../246031/index.html">More features with the new version of products DevExpress 14.2</a></li>
<li><a href="../246037/index.html">2 powerful psychological techniques for a significant increase in conversion</a></li>
<li><a href="../246041/index.html">Rails or Sinatra: the best of both worlds?</a></li>
<li><a href="../246043/index.html">Consulo Beta will continue in 2015</a></li>
<li><a href="../246047/index.html">Web analytics: why does your business need it?</a></li>
<li><a href="../246049/index.html">Earnings on venture investments</a></li>
<li><a href="../246051/index.html">Earnings on cybersquatting</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>