<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ABBYY FineReader text recognition (1/2)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Content   ABBYY FineReader text recognition (1/2) 
 ABBYY FineReader (2/2) text recognition 

 The text recognition system in FineReader can be descri...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>ABBYY FineReader text recognition (1/2)</h1><div class="post__text post__text-html js-mediator-article"><div class="spoiler">  <b class="spoiler_title">Content</b> <div class="spoiler_text"> <b><img src="http://habrahabr.ru/favicon.ico" alt="image"></b>  <b>ABBYY FineReader text recognition (1/2)</b> <br><img src="http://habrahabr.ru/favicon.ico" alt="image">  <a href="http://habrahabr.ru/company/abbyy/blog/228251/">ABBYY FineReader (2/2) text recognition</a> <br></div></div><br>  The text recognition system in FineReader can be described very simply. <br><br>  We have a page with text, we parse it into text blocks, then we parse the blocks into separate lines, lines into words, words into letters, we recognize letters, then we collect everything back to the text of the page further down the chain. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8f2/bd5/6dc/8f2bd56dc5de3241a18d7f21474f3649.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      It looks very simple, but the devil, as usual, is in the details. <br><br>  Let's talk about the level from the document to the text line some time next time.  This is a large system in which there are many of its difficulties.  As a kind of introduction, perhaps, you can leave here the following illustration for the string extraction algorithm. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1ee/732/a87/1ee732a87d448ba4f7f3c0e50fbbb45c.png"><br><br>  In this article we will begin the story about text recognition from the level of the line and below. <a name="habracut"></a><br><br>  A small warning: the FineReader recognition system is very large and has been constantly updated for many years.  To describe this system entirely with all its nuances, firstly, better code, secondly, it will take a lot of space, thirdly, read <a href="http://habrahabr.ru/company/abbyy/blog/144913/">it</a> .  Therefore, we recommend to treat the written below as a kind of very generalized theory behind the practical system.  That is, the general ideas and trends in technology are about the same as the truth, but in order to understand in detail what is happening in practice, it is better not to read this article, but to work with us to develop this system. <br><br><h4>  Linear division graph </h4><br>  So, we have a black and white image of a line of text.  In fact, the image is, of course, gray or color, and it becomes black and white after binarization (you also need to write a separate article about binarization, but for <a href="https://www.google.com/">now this</a> can be partly helpful). <br><br>  So, let there be a black and white image of a line of text.  It is necessary to divide it into words, and words - into characters for recognition.  The basic idea, as usual, is obvious - look for vertical lines of white gaps in the image, and then cluster them across the width: wide gaps are spaces between words, narrow spaces between characters. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/184/83a/a33/18483aa330ffc3ea2c318dca095fe100.png"><br><br>  The idea is wonderful, but in real life, the width of the spaces can be a very ambiguous indicator, for example, for sloped text or an unsuccessful combination of characters or stuck text. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a01/a73/242/a01a73242a123838b5b6d74ff658b71e.png"><br><br>  There are two solutions to the problem, in general.  The first solution is to assume some "visible" width of the gaps.  A person can practically any text, even in an unfamiliar language, be divided exactly into words, and words into symbols.  This is because the brain does not fix the vertical distance between the characters, but a certain apparent volume of empty space between them.  The solution is good, we, of course, use it, it only does not always work.  For example, the text may be damaged when scanning and some of the necessary gaps may decrease or, conversely, increase greatly. <br><br>  This leads us to the second solution - the linear division graph.  The idea is as follows - if there are several options for dividing a line into words and words into letters, then let's mark all possible division points we could come up with.  A piece of the image between the two marked points will be considered a letter (or word) candidate.  The linear graph option can be simple if the text is good and there are no problems with the definition of division points or difficult if the image was bad. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2bd/dc3/4d2/2bddc34d2e43947636486e8b25edef3d.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/977/19f/fef/97719ffefd262f2f1b55fdd0807e3264.png"><br><br>  Now the challenge.  There are many vertices of the graph, you need to find a path from the first vertex to the last, passing through a certain number of intermediate vertices (not necessarily all) with the best quality.  We start to think that it reminds.  We recall the course of optimal control from the institute, we understand that this is suspiciously similar to the problems of dynamic programming. <br><br>  Let's think about what we need so that the algorithm for iterating over all the options does not explode. <br><br>  For each arc in the graph you need to determine its quality.  If we work with a graph of linear division of a word into symbols, then each arc in our country is a symbol.  In the role of arc quality, we use the confidence of character recognition (how to calculate it - let's talk later).  And if we work with GLD at the line level, then each arc of this GLD is a variant of word recognition, which in turn was obtained from a character graph.  That is, we need to be able to assess the overall quality of the full path in the linear division graph. <br><br>  We shall define the quality of the full path in the graph as the sum of the quality of all the arcs of the MINUS penalty for the whole option.  Why exactly minus?  This gives us the opportunity to quickly assess the maximum possible quality of a path variant by the sum of the quality of the arcs of this path, which means that we will cut off most of the options even before calculating the total quality of the variant. <br><br>  Thus, for GLD, we arrive at a standard dynamic programming algorithm ‚Äî we find linear division points, build a path from beginning to end along arcs with the highest quality, calculate the total cost of the constructed version.  And then we go through the paths in the FGD in order of decreasing the total quality of the elements with constant updating of the found best option, until we understand that all the raw options are obviously worse than the current best option. <br><br><h4>  Image hypotheses </h4><br>  Before we go down to the level of recognition of individual words, we have another topic that has not been discussed, the fragment image hypothesis. <br><br>  The idea is as follows - we have an image of the text with which we are going to work.  I really want to process all images in the same way, but the truth is that in the real world, images are all different - they can be obtained from different sources, they can be of different quality, they can be scanned in different ways. <br><br>  On the one hand, it seems that the variety of possible distortions should be very large, but if you start to understand, only a limited set of possible distortions is found.  Therefore, we use the text hypothesis system. <br><br>  We have a predefined set of possible hypotheses for problem text.  For each hypothesis, you need to define: <br><ul><li>  A quick way to find out if a given hypothesis is applicable to the current image, and to do so only on the basis of the characteristics of the image, before recognition. </li><li>  A method for correcting a specific hypothesis on an image. </li><li>  The criterion of the quality of the correctness of the choice of the hypothesis on the basis of image recognition, plus, possibly, recommendations for the following hypotheses. </li></ul><br><br><img src="https://habrastorage.org/getpro/habr/post_images/4c7/0f1/0e2/4c70f10e29ee32f27eb940efbf21003d.png"><img src="https://habrastorage.org/getpro/habr/post_images/4a5/f6e/bb5/4a5f6ebb50a817c748fc333ad496175e.png"><br>  In the image above you can see the hypotheses for different binarization and contrast of the original image. <br><br>  As a result, the processing of hypotheses is as follows: <br><br><ol><li>  In the image generate the most appropriate hypothesis. </li><li>  Correct the distortion of the selected hypothesis. </li><li>  Recognize the resulting image. </li><li>  Assess the quality of recognition. </li><li>  If the quality of recognition has improved, then assess whether it is necessary to apply new hypotheses to the modified image. </li><li>  If the quality has deteriorated, then return to the original image and try to apply some other hypothesis to it. </li></ol><br><br><img src="https://habrastorage.org/getpro/habr/post_images/ce5/f67/d10/ce5f67d10831c1796271dc287d43064a.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/eda/147/2af/eda1472afdc252c12cff59b4ab702bcf.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/1ea/ce1/d6e/1eace1d6e6db718e9ce8369b4c2d5985.png"><br><br>  The images show the consistent use of white noise and compressed text hypotheses. <br><br><h4>  Word quality assessment </h4><br>  Two important topics remained unsolved: the assessment of the overall quality of word recognition and character recognition.  Character recognition is a topic in several sections, therefore, we first discuss the assessment of the quality of the recognized word. <br><br>  So, we have some kind of word recognition.  The first thing that comes to mind is to check it with the dictionary and give it a fine if it is not in the dictionary.  The idea is good, but not all languages ‚Äã‚Äãare dictionaries, not all words in the text can be vocabulary (proper names, for example), and if we delve into complexity, not everything in the text can be words in the standard sense of the term. <br><br>  A little earlier, we said that any marks for the whole word should be negative, so that the DHF brute force works normally.  Now it will begin to actively interfere with us, so let's fix that we have a certain predetermined maximum positive evaluation of the word, we give positive bonuses to the word, and we define the final negative penalty as the difference between the collected bonuses and the maximum evaluation. <br><br>  Ok, let us recognize the phrase ‚ÄúVasya arrives by flight SU106 at 23.55 07/20/2015‚Äù.  Of course, we can evaluate the quality of each word here by the general rules, but it will be quite strange.  For example, SU106 and Vasya are quite clear words in this line, but it is obvious that the rules of education are different and, in theory, verification should also be different. <br><br>  From here comes the idea of ‚Äã‚Äãmodels.  A word model is a kind of generalized description of a specific type of word in a language.  We will, of course, have a model of a standard word in a language, but there will also be models of numbers, abbreviations, dates, abbreviations, proper names, URLs, etc. <br><br>  What do models give us and how to use them normally?  In fact, we reverse our word verification system - instead of finding out what it is for a long time, we let each model decide whether this word suits her and how well she evaluates it. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c93/ece/df8/c93ecedf8219c2505fd67d969cb36f12.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/cb8/6bb/b82/cb86bbb82c7595e82029b68a33d10ba0.png"><br><br>  From the very formulation of the problem, our requirements for the model architecture are formed.  The model should be able to: <br><br><ul><li>  Quickly say whether or not the word is suitable for her.  The standard check includes all checks for allowed character sets for each letter in a word.  For example, in a vocabulary word, punctuation should be only at the beginning or at the end, and in the middle of the word the punctuation set is severely limited, and the combination of punctuation is severely limited (super-ability ?!), and in the number model there should basically be numbers except given language of the symbol suffix (10th, 10th). </li><li>  To be able to assess the quality of a recognizable word by its own internal logic.  For example, a word from a dictionary should be clearly rated higher than just a set of characters. </li></ul><br><br>  When assessing the quality of the model, we should not forget that our task as a result is to compare the models with each other, therefore their evaluations should be consistent.  A more or less normal way to achieve this is to relate to an assessment of a model as an assessment of the probability of constructing a word using a given model.  For example, there are a lot of vocabulary words in ordinary language, and it is easy to get a vocabulary word when it is recognized incorrectly.  But to collect a normal, suitable for all the rules of the phone number is much more difficult. <br><br>  As a result, when recognizing a fragment of a string, we get something like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/951/171/4c0/9511714c05d91212ba121d8a2c913c74.png"><br><br>  A separate item in the evaluation of recognition options are additional empirical penalties that do not fit into the concept of models or in the assessment of recognition.  Say, ‚ÄúLLC Horns and hoofs‚Äù and ‚Äú000 Horns and hoofs‚Äù look like two equally normal options (especially if the font 0 (zero) and O (the letter O) are slightly different proportions).  But at the same time, it is quite obvious which version of recognition should be correct.  For such small specific knowledge of the world, a separate system of rules was made, which can additionally penalize the options that did not like it after the model evaluations. <br><br>  We'll talk about recognition itself in the next part of this post.  Subscribe to the company blog not to miss :) </div><p>Source: <a href="https://habr.com/ru/post/225215/">https://habr.com/ru/post/225215/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../225197/index.html">Google has released a 64-bit version of Chrome for Windows</a></li>
<li><a href="../225199/index.html">We study Three.js.Glava 2: Work with the main components of whichThree.js-scene</a></li>
<li><a href="../225209/index.html">The most complete picture of the visible universe from Hubble</a></li>
<li><a href="../225211/index.html">Dell Alienware A18 laptop video review</a></li>
<li><a href="../225213/index.html">Tetris Birthday</a></li>
<li><a href="../225221/index.html">History of the RTB market: stages of development and the emergence of major players</a></li>
<li><a href="../225225/index.html">The Ministry of Communications proposes to close sites forever for posting two links to pirated content</a></li>
<li><a href="../225227/index.html">As I wrote skad. Part Six</a></li>
<li><a href="../225229/index.html">US Secret Service buys software that recognizes sarcasm in social networks</a></li>
<li><a href="../225235/index.html">Swift video course in Russian</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>