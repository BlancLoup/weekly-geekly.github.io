<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>multi_get - download sites wholesale</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The topic will be interesting to those who want to index Internet sites at top speeds (homemade search engines, word frequency analyzes, html analysis...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>multi_get - download sites wholesale</h1><div class="post__text post__text-html js-mediator-article"> The topic will be interesting to those who want to index Internet sites at top speeds (homemade search engines, word frequency analyzes, html analysis services, etc.) Threading here does not give speed limits, urllib - all the more ... The solution here is to use asynchronous requests from libcurl. <br><br>  Speed? <br>  At 500MHZ ( <i>very very</i> weak VPS) - about <i>100 URLs per second</i> (100 connections, 2 processes). <br>  On Amazon EC2 ‚ÄúHigh-CPU Medium Instance‚Äù (.2 $ / hour) ~ <i>1200 URLs per second</i> (300 connections, 5 simultaneous processes).  In one process up to 660 URLs per second. <br><br>  For pumping out a lot of sites and further processing, I want to share one of my useful functions - multi_get - in fact it is a convenient wrapper for CurlMulti (libcurl), modified from their example CurlMulti. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     <code>&gt;&gt;&gt; urls = ['http://google.com/', 'http://statcounter.com/'] <br> &gt;&gt;&gt; res = {} <br> &gt;&gt;&gt; <b>multi_get</b> (res, urls, num_conn = 30, timeout = 5, percentile = 95) <br> &gt;&gt;&gt; res['http://google.com/'] <br> '&lt;html&gt;&lt;title&gt;Google.... <br> #   res,   HTML   URL'</code> <a name="habracut"></a> <br><br>  This code downloads two sites in 30 connections.  More precisely, of course in two connections, but I just did not have enough space here to write 10,000 urls. <br><br>  Goodies and usefulness: <br><br>  0. With num_conn = 1, the function turns into a serial (not parallel) download, but with all the advantages below (cookies, user-agents, unconditional timeouts) <br><br>  1. If we allow res in advance to define 'http://google.com/' as having some value - this address will not be downloaded (will be skipped).  The bottom line is that if you have res not just an ordinary dict, but somehow persistent (for example, is stored in a file or in some kind of SQL), then only those sites that have not been downloaded before will be downloaded at each call. <br><br>  2. multi_get (res, url, <b>debug = 1</b> ) - display information about the download progress (the console slows down the process, so it‚Äôs better to turn off production). <br><br>  3. multi_get (res, url, <b>percentile = 95</b> ) - often 90-99% of sites from a large list are downloaded almost in microseconds each, but 1-2 sites from a large list will be very slow.  As a result, you have 9990 websites flying by in a minute, let's say, and the remaining 10 you will wait another minute - this terribly reduces efficiency - therefore there is such a parameter - download 95% (or 99, 50, 75) of the fastest URLs and exit without expecting slow. <br><br>  4. multi_get (res, url, <b>timeout = 5</b> ) - timeout per URL - 5 seconds (unlike the timeout of embedded sockets in Python - it always works and does not freeze for no reason). <br><br>  5. ... <b>ua = 'Mozilla / 5.0 (compatible; Googlebot / 2.1; + http: //www.google.com/bot.html)'</b> ... - so we believed, but still - which one to pass to User-Agent. <br><br>  6. ... <b>ref = 'http://google.com/bot.html'</b> ... - which one to pass to Referer <br>  ... <b>ref_dict = {'http://google.com/': 'http://mysite.com/?url-google.com'}</b> - dict values ‚Äã‚Äãwhich URL should be sent to which Referer. <br><br>  7. ... <b>cf = 'cookiefile.txt'</b> ... - use cookies, store them in this file. <br><br>  8. ... <b>follow = 0</b> ... - do not follow redirects (by default). <br>  If, for example, all .com domains are indexed, many of them are redirected to the same, so redirects should be simply ignored. <br><br>  9. res - not necessarily dict, for example, you can define a new class MyDict, where you can do def __setitem __ (self, url, html): and process html asynchronously, right during the download - without waiting for the end of the multi_get call, just define def keys ( self): return [] - return an empty list or list of URLs that should not be downloaded. <br><br><h4>  Code </h4><br>  Unfortunately, Habr kills whitespace (indentation), without which the python code will not work, so the code is here: <a href="http://rarestblog.com/py/multi_get.py.txt">rarestblog.com/py/multi_get.py.txt</a> (or <a href="http://rarest.s3.amazonaws.com/multi_get.py.txt">rarest.s3.amazonaws.com/multi_get.py.txt</a> ) <br><br>  There is also an example in the code that makes 10 YQL queries to get 1000 random links and download 80% of them and measure the speed. <br><br><h4>  IMPORTANT MOMENTS </h4><br>  You will need to install pycurl ( <a href="http://pycurl.sourceforge.net/download/">pycurl.sourceforge.net/download</a> ) <br>  &gt; <code>easy_install pycurl</code> <br>  if easy_install is not installed, then first: <br>  &gt; <code>python -murllib http://peak.telecommunity.com/dist/ez_setup.py | python - -U setuptools</code> <code>python -murllib http://peak.telecommunity.com/dist/ez_setup.py | python - -U setuptools</code> <br>  and then the above line. <br><br>  For the test script will still need <br>  &gt; <code>easy_install cjson</code> <br>  (you can optionally replace cjson.decode with simplejson.loads - if you understand why) <br><br><h5>  Installing c-ares under Linux / FreeBSD </h5><br>  Under Windows, everything is in order (.exe setup already includes compiled 'c-ares' support), but on the server (Linux / FreeBSD) you will need to install support for 'c-ares' (asynchronous DNS queries), otherwise the speed pycurl / multi_get drops tenfold - you will not be able to use more than 20-30 connections without c-ares. <br><br> <code># wget http://curl.haxx.se/download/curl- <i>7.19.4</i> .tar.gz <br> # tar zxvf curl-7.19.4.tar.gz <br> # cd curl-7.19.4 <br>  Linux: # ./configure --enable-ares --with-ssl --enable-ipv6 --with-libidn <br>  FreeBSD: # ./configure --enable-ares=/usr/local --with-ssl --enable-ipv6 --with-libidn <br> <br> "--with-ssl --enable-ipv6 --with-libidn" -    . <br> <br> # make <br> # make install <br> <br> [linux only] <br>        : <br> # rm -rf /usr/lib/libcu* <br> # ln -s /usr/local/lib/libcurl.so.4 /usr/lib/libcurl.so.4 <br> # ln -s /usr/local/lib/libcurl.so.4 /usr/lib/libcurl.so <br> # ldconfig <br> [ linux only] <br> <br> # cd .. <br> # rm -rf curl-7* <br> <br> # python -c "import pycurl;print pycurl.version" <br> ,      c-ares <br></code> <br><br><h5>  Anti-dos </h5><br>  2. Since the script is rather stupid, but powerful - you can accidentally start a DOSit of someone‚Äôs website to avoid it ‚Äî the small function reduce_by_domain is included, which compresses the list so that there is only 1 URL from one domain - a precaution not to put someone's website. <br><br> <code>short_list_of_urls = reduce_by_domain(urls)</code> <br> <br>  How to download all the URLs, without killing sites?  Call reduce_by_domain, multi_get several times in a row - remember that if res is not cleared, the same URLs will not be downloaded a second time (see 1. in ‚ÄúGoodies and Utility‚Äù), it remains only to remove from the list of urls what you have already downloaded and do short_list_of_urls = reduce_by_domain (urls) again;  multi_get (res, short_list_of_urls). <br><br>  <b>More nuances:</b> <br><br>  Erroneous URLs will be returned with a <code>"---"</code> value. <br>  Files larger than 100,000 bytes will not be downloaded. <br>  .Pdf files will not download. <br><br>  This is done as a precautionary measure - everything can be changed very easily in the function code, so as not to index what is not needed (images, .pdf). <br><br><img src="http://habrahabr.ru/media/userpic/avatar/14/45/58/31055.png"><br>  Yoi Haji <br>  <a href="http://yoihj.habrahabr.ru/blog/">view from Habra</a> </div><p>Source: <a href="https://habr.com/ru/post/61960/">https://habr.com/ru/post/61960/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../61953/index.html">Using google static maps</a></li>
<li><a href="../61954/index.html">Fx 3.0.11</a></li>
<li><a href="../61955/index.html">.NET Questions</a></li>
<li><a href="../61957/index.html">Review of Nvidia ION motherboards</a></li>
<li><a href="../61959/index.html">Work with sound in * nix. Part 3</a></li>
<li><a href="../61961/index.html">Zend Studio for Eclipse 7.0 EA</a></li>
<li><a href="../61962/index.html">Cyclops Battle</a></li>
<li><a href="../61963/index.html">Explay 3</a></li>
<li><a href="../61964/index.html">MyNotifier is offered / for sale, an informer for freelancers</a></li>
<li><a href="../61965/index.html">Trace monkey and java script</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>