<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Grasp2Vec: Learning to Represent Objects Through Self-Learning Capture</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="People from a surprisingly early age are already able to recognize their favorite objects and lift them, despite the fact that they are not specifical...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Grasp2Vec: Learning to Represent Objects Through Self-Learning Capture</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/220/c80/5fb/220c805fb8ffb53d2b33413fa2e9eeda.png"><br><br>  People from a surprisingly early age are already able to recognize their favorite objects and lift them, despite the fact that they are not specifically taught this.  According to <a href="https://www.ncbi.nlm.nih.gov/pubmed/8542971">research on the</a> development of cognitive abilities, the ability to interact with the objects of the world around us plays a crucial role in the development of such abilities as sensation and manipulation of objects ‚Äî for example, targeted capture.  Interacting with the outside world, people can learn by correcting their own mistakes: we know what we have done, and we learn from the results.  In robotics, this type of learning with self-correction of errors is actively explored, since it allows robotic systems to learn without a huge amount of training data or manual adjustment. <br><br>  We, at Google, inspired by the <a href="https://en.wikipedia.org/wiki/Object_permanence">concept of object persistence</a> , offer the <a href="https://sites.google.com/site/grasp2vec/">Grasp2Vec</a> system, a simple but effective algorithm for constructing the representation of objects.  Grasp2Vec is based on the intuitive understanding that trying to pick up any object will give us some information - if the robot captures the object and picks it up, then the object needs to be in this place before the capture.  In addition, the robot knows that if the captured object is in its capture, it means that the object is no longer in the place where it was.  Using this form of self-study, the robot can learn to recognize the object due to the visual change of the scene after its capture. <br><a name="habracut"></a><br>  Based on our <a href="https://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html">collaboration with X Robotics</a> , where several robots were simultaneously trained to capture household items using only one camera as an input source, we use robotic capture for ‚Äúunintentional‚Äù capture of objects, and this experience allows us to get a rich idea of ‚Äã‚Äãthe object.  This view can already be used to acquire the ability of "intentional capture", when the robot arm can pick up objects on demand. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/QzlI_ny4l8s" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h2>  Creating a perceptual reward function </h2><br>  In the <a href="https://ru.wikipedia.org/wiki/%25D0%259E%25D0%25B1%25D1%2583%25D1%2587%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D1%2581_%25D0%25BF%25D0%25BE%25D0%25B4%25D0%25BA%25D1%2580%25D0%25B5%25D0%25BF%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5%25D0%25BC">reinforcement learning</a> platform <a href="https://ru.wikipedia.org/wiki/%25D0%259E%25D0%25B1%25D1%2583%25D1%2587%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D1%2581_%25D0%25BF%25D0%25BE%25D0%25B4%25D0%25BA%25D1%2580%25D0%25B5%25D0%25BF%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5%25D0%25BC">, the</a> success of a task is measured through the reward function.  By maximizing rewards, robots learn various skills to capture <a href="http://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html">from scratch</a> .  Creating a reward function is easy when success can be measured by simple sensor readings.  A simple example is a button that, by clicking on it, sends a reward <a href="https://ru.wikipedia.org/wiki/%25D0%25AF%25D1%2589%25D0%25B8%25D0%25BA_%25D0%25A1%25D0%25BA%25D0%25B8%25D0%25BD%25D0%25BD%25D0%25B5%25D1%2580%25D0%25B0">directly to the input of a robot</a> . <br><br>  However, creating a reward function is much more difficult when the success criterion depends on the perceptual understanding of the task.  Consider the capture task using an example where the robot is given an image of the desired object held in the capture.  After the robot tries to capture the object, it examines the contents of the capture.  The reward function for this task depends on the answer to the pattern recognition question: do the objects match? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f23/a41/c24/f23a41c24b4cc60b062d055bbb5b9347.png"><br>  <i>On the left, the grip holds the brush, and on the background you can see several objects (yellow cup, blue plastic block).</i>  <i>On the right, the grip holds the cup, and the brush is in the background.</i>  <i>If the left image represented the desired result, a good reward function would have to ‚Äúunderstand‚Äù that these two photos correspond to two different objects.</i> <br><br>  To solve the problem of recognition, we need a perceptual system that extracts meaningful concepts of objects from unstructured images (not signed by people), and that learns to visualize objects without a teacher.  In essence, unsupervised learning algorithms work by making structural assumptions about the data.  It is often assumed that images can be <a href="https://ru.wikipedia.org/wiki/%25D0%2590%25D0%25B2%25D1%2582%25D0%25BE%25D0%25BA%25D0%25BE%25D0%25B4%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D1%2589%25D0%25B8%25D0%25BA">compressed to a space with fewer dimensions</a> , and video frames can be <a href="https://papers.nips.cc/paper/6161-unsupervised-learning-for-physical-interaction-through-video-prediction.pdf">predicted from previous ones</a> .  However, without additional assumptions about the content of the data, this is usually not enough to train on unrelated views of objects. <br><br>  What if we used a robot to physically separate objects during data collection?  Robotics offer an excellent opportunity for learning to represent objects, since robots can manipulate them, which will give the necessary factors of variation.  Our method is based on the idea that capturing an object removes it from the scene.  The result is 1) an image of the scene before the capture, 2) an image of the scene after the capture, and 3) a separate view of the captured object. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8da/43f/0d7/8da43f0d74077b6c65a80026ce7041f0.png"><br>  <i>On the left - objects before capture.</i>  <i>In the center - after capture.</i>  <i>On the right is the captured object.</i> <br><br>  If we consider a built-in function that extracts a ‚Äúset of objects‚Äù from images, it should retain the following subtraction relation: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1c8/aa9/723/1c8aa97238f20d832c90f02335c0367c.png"><br>  <i>pre-capture objects - post-capture objects = captured object</i> <br><br>  We achieve this equality with the help of convolutional architecture and simple metric learning algorithm.  During training, the architecture shown below embeds the images before and after capturing into a dense <a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf">map of spatial properties</a> .  These maps turn into vectors through averaged union, and the difference between the ‚Äúbefore capture‚Äù and ‚Äúafter capture‚Äù vectors represents a set of objects.  This vector and the corresponding vector representation of this perceived object are equated through the function of N-pairs. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/32f/e51/c0d/32fe51c0d4915374be646fc0bb2ba76c.png"><br><br>  After training, our model naturally has two useful properties. <br><br><h2>  1. Similarity of objects </h2><br>  <a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25BE%25D1%258D%25D1%2584%25D1%2584%25D0%25B8%25D1%2586%25D0%25B8%25D0%25B5%25D0%25BD%25D1%2582_%25D0%259E%25D1%2582%25D0%25B8%25D0%25B0%25D0%25B8">The cosine coefficient of the</a> distance between the vector inserts allows us to compare objects and determine whether they are identical.  This can be used to implement the reward function for training with reinforcements, and allows robots to learn how to use examples without marking up human data. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c47/fb7/dd4/c47fb7dd4c79a4e16564f9d6ab669f5e.png"><br><br><h2>  2. Finding targets </h2><br>  We can combine spatial maps of the scene and the embedding of objects to localize the ‚Äúdesired object‚Äù in the image space.  By performing elementwise multiplication of the maps of spatial features and the vector correspondence of the desired object, we can find all the pixels on the spatial map corresponding to the target object. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c42/271/47b/c4227147baeb69c052549544b9065961.png"><br>  <i>Use the Grasp2Vec plugin to localize objects in the scene.</i>  <i>Top left - objects in the basket.</i>  <i>At the bottom left - the desired object to be captured.</i>  <i>The scalar product of the vector of the target object and the spatial features of the image gives us a pixel-by-pixel ‚Äúactivation map‚Äù (top right) of the similarity of the specified image segment to the target one.</i>  <i>This map can be used to get closer to the target object.</i> <br><br>  Our method also works when several objects match the target, or even when the target consists of several objects (average of two vectors).  For example, in this scenario, the robot detects several orange blocks in the scene. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4f3/f9e/b15/4f3f9eb159738345f5a8da4c1dfb83fc.png"><br>  <i>The resulting "heat map" can be used to plan the approach of the robot to the target object (s).</i>  <i>We combine the localization from Grasp2Vec and the recognition of examples with our policy of ‚Äúcapturing everything, anything,‚Äù and achieve success in 80% of cases during data collection and in 59% with new objects that the robot has not previously encountered.</i> <br><br><h2>  Conclusion </h2><br>  In our <a href="https://arxiv.org/abs/1811.06964">work,</a> we have shown how the skills of robotic captures can create the data used for teaching object representations.  We can then use representational training to quickly acquire more complex skills, such as capturing by example, while retaining all the unsupervised learning properties in our autonomous capture system. <br><br>  In addition to our work, several other recent works also explored how unsupervised interaction can be used to obtain representations of objects, by <a href="https://arxiv.org/abs/1806.08756">capturing</a> , <a href="https://arxiv.org/abs/1606.07419">pushing,</a> and other kinds of <a href="https://arxiv.org/abs/1806.08354">interactions</a> with objects in the environment.  We are in joyful anticipation not only that machine learning can give robotics in terms of better perception and control, but also that robotics can give machine learning in terms of new paradigms of independent learning. </div><p>Source: <a href="https://habr.com/ru/post/434898/">https://habr.com/ru/post/434898/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../434888/index.html">New Year's gift from Binary District</a></li>
<li><a href="../434890/index.html">Kiwi Bank (JSC) assigns money to users</a></li>
<li><a href="../434892/index.html">We draw code in Swift, PaintCode</a></li>
<li><a href="../434894/index.html">The art of shamanism or custom firmware for Olinuxino. Part 1</a></li>
<li><a href="../434896/index.html">Hall of Fame consumer electronics: the history of the best gadgets of the last 50 years, part 1</a></li>
<li><a href="../434900/index.html">Happy New Year 2019 - competition</a></li>
<li><a href="../434902/index.html">Creating a custom query generator in Spring Data Neo4j (Part 1)</a></li>
<li><a href="../434906/index.html">C ++ tests without macros and dynamic memory</a></li>
<li><a href="../434908/index.html">Programming Education - What? Where? When?</a></li>
<li><a href="../434912/index.html">The annual stock of Porsche Taycan is already reserved, mainly by Tesla owners.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>