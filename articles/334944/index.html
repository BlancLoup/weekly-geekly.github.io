<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>37 reasons why your neural network is not working</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The network has been trained for the last 12 hours. Everything looked good: the gradients were stable, the loss function was decreasing. But then the ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>37 reasons why your neural network is not working</h1><div class="post__text post__text-html js-mediator-article">  The network has been trained for the last 12 hours.  Everything looked good: the gradients were stable, the loss function was decreasing.  But then the result came: all zeros, one background, nothing recognized.  ‚ÄúWhat did I do wrong?‚Äù I asked the computer, which said nothing in reply. <br><br>  Why does the neural network produce garbage (for example, the average of all the results or does it have really weak accuracy)?  How to start checking? <br><br>  The network may not be trained for several reasons.  As a result of many debug sessions, I noticed that I often do the same checks.  Here I gathered my experience together with the best ideas of my colleagues.  I hope this list will be useful to you. <br><a name="habracut"></a><br><h1>  Content </h1><br><blockquote>  <i><a href="https://habr.com/ru/post/334944/">0. How to use this guide?</a></i> <i><a name="0_0"></a><br></i>  <i><a href="https://habr.com/ru/post/334944/">I. Problems with data set</a></i> <i><a name="1_1"></a><br></i>  <i><a href="https://habr.com/ru/post/334944/">Ii.</a></i>  <i><a href="https://habr.com/ru/post/334944/">Data Normalization / Augmentation Problems</a></i> <i><a name="2_2"></a><br></i>  <i><a href="https://habr.com/ru/post/334944/">Iii.</a></i>  <i><a href="https://habr.com/ru/post/334944/">Implementation issues</a></i> <i><a name="3_3"></a><br></i>  <i><a href="https://habr.com/ru/post/334944/">Iv.</a></i>  <i><a href="https://habr.com/ru/post/334944/">Learning problems</a></i> <i><a name="4_4"></a></i> </blockquote><br><a name="0"></a><h1>  0. How to use this guide? </h1><br>  Much can go wrong.  But some problems are more common than others.  I usually start with this small list as an emergency kit: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ol><li>  Start with a simple model that works correctly for this type of data (for example, VGG for images).  Use the standard loss function if possible. </li><li>  Disable all trinkets, such as regularization and data augmentation. </li><li>  In the case of fine tuning the model, double check the preprocessing to match the learning of the original model. </li><li>  Verify that the input data is correct. </li><li>  Start with a really small dataset (2-20 samples).  Then expand it, gradually adding new data. </li><li>  Start gradually adding back all the fragments that were omitted: augmentation / regularization, custom loss functions, try more complex models. </li></ol><br>  If all else fails, proceed to reading this long list and check each item. <br><br><a name="1"></a><h1>  I. Problems with data set </h1><br><img src="https://habrastorage.org/getpro/habr/post_images/ef6/032/8ff/ef60328ff5f505334860490ca248270c.png"><br>  <sub><font color="gray">Source: <a href="http://dilbert.com/strip/2014-05-07">http://dilbert.com/strip/2014-05-07</a></font></sub> <br><br><h3>  1. Check the input data </h3><br>  Verify that the input data makes sense.  For example, I have often mixed in a heap the height and width of images.  Sometimes by mistake I gave all zeros to a neural network.  Or used the same batch over and over again.  So type / view a couple of batches of input data and planned output ‚Äî make sure everything is in order. <br><br><h3>  2. Try random input values. </h3><br>  Try sending random numbers instead of real data and see if the same error remains.  If so, then this is a sure sign that your network at some stage turns data into garbage.  Try debugging layer by layer (operation by operation) and see where the crash occurs. <br><br><h3>  3. Check data loader </h3><br>  With data, everything can be in order, and an error in the code that transmits the input data of the neural network.  Print and check the input data of the first layer before starting its operations. <br><br><h3>  4. Make sure the input connects to the output. </h3><br>  Check that several samples of input data are labeled correctly.  Also check that the swapping of input samples is also reflected in the output labels. <br><br><h3>  5. Is the relationship between input and output too random? </h3><br>  Maybe the non-random parts of the relationship between the input and the output are too small compared to the random part (someone might say that these are quotes on the stock exchange).  That is, the input is not sufficiently connected to the output.  There is no universal method here, because the measure of randomness depends on the type of data. <br><br><h3>  6. Too much noise in the data set? </h3><br>  Once this happened to me when I pulled off a set of food images from the site.  There were so many bad marks that the network could not learn.  Manually check a series of sample input values ‚Äã‚Äãand see that all labels are in place. <br><br>  This item is worth a separate discussion, because <a href="https://arxiv.org/pdf/1412.6596.pdf">this work</a> shows accuracy above 50% on the basis of MNIST with 50% of damaged tags. <br><br><h3>  7. Shuffle the dataset </h3><br>  If your data is not mixed and arranged in a certain order (sorted by tags), this may adversely affect the training.  Shuffle the dataset: make sure you mix both the input data and the tags. <br><br><h3>  8. Reduce class imbalances </h3><br>  Is there a thousand class A images per class B image in the dataset?  Then you may need to balance the loss function or <a href="http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/">try other imbalance approaches</a> . <br><br><h3>  9. Are there enough samples for training? </h3><br>  If you train the network from scratch (that is, do not configure it), then a lot of data may be needed.  For example, to classify images, <a href="https://stats.stackexchange.com/a/226693/30773">they say</a> , you need a thousand images for each class, or even more. <br><br><h3>  10. Make sure there are no batches with a single tag. </h3><br>  This happens in a sorted dataset (that is, the first 10,000 samples contain the same class).  Easily corrected by mixing the data set. <br><br><h3>  11. Reduce batch size </h3><br>  <a href="https://arxiv.org/abs/1609.04836">This work</a> indicates that too large batches may reduce the ability of the model to generalize. <br><br><h3>  Addition 1. Use a standard data set (for example, mnist, cifar10) </h3><br>  Thanks <a href="https://medium.com/%40hengcherkeng">hengcherkeng</a> for this: <br><br><blockquote>  <i>When testing a new network architecture or writing new code, first use the standard data sets instead of yours.</i>  <i>Because for them there are already a lot of results and they are guaranteed to be ‚Äúsolvable‚Äù.</i>  <i>There will be no problems with noise in the tags, the difference in the distribution of training / testing, too much complexity of the data set, etc.</i> </blockquote><br><a name="2"></a><h1>  Ii.  Data Normalization / Augmentation Problems </h1><br><img src="https://habrastorage.org/getpro/habr/post_images/a37/7f1/71d/a377f171d5b6bdc0a01822ed42952033.png"><br><br><h3>  12. Calibrate the symptoms. </h3><br>  Did you calibrate the input data to zero mean and unit variance? <br><br><h3>  13. Too strong augmentation data? </h3><br>  Augmentation has a regularizing effect.  If it is too strong, then this, together with other forms of regularization (L2-regularization, dropout, etc.) can lead to under-training of the neural network. <br><br><h3>  14. Verify pre-trained model preprocessing. </h3><br>  If you are using a model that has already been prepared, then make sure that the same normalization and preprocessing is used as in the model you are teaching.  For example, should a pixel be in the range [0, 1], [-1, 1] or [0, 255]? <br><br><h3>  15. Check pre-processing for recruitment training / validation / testing </h3><br>  CS231n pointed to a <a href="http://cs231n.github.io/neural-networks-2/">typical trap</a> : <br><br><blockquote>  <i>‚Äú... any pre-processing statistics (for example, average of data) need to be calculated on the data for training, and then applied on the validation / testing data.</i>  <i>For example, it will be an error to calculate the average and subtract it from each image in the entire data set, and then divide the data into fragments for training / validation / testing. ‚Äù</i> </blockquote><br>  Also check for different preprocessing of each sample and batch. <br><br><a name="3"></a><h1>  Iii.  Implementation issues </h1><br><img src="https://habrastorage.org/getpro/habr/post_images/a3c/00e/878/a3c00e878e007200b1f3501115b65066.png"><br>  <sub><font color="gray">Source: <a href="https://xkcd.com/1838/">https://xkcd.com/1838/</a></font></sub> <br><br><h3>  16. Try to solve a simpler version of the problem. </h3><br>  This will help determine where the problem is.  For example, if the target output is the object class and coordinates, try restricting the prediction to only the object class. <br><br><h3>  17. Look for the correct loss function "in probability" </h3><br>  Again from matchless <a href="http://cs231n.github.io/neural-networks-3/">CS231n</a> : <i>Initialize with small parameters, without regularization.</i>  <i>For example, if we have 10 classes, then ‚Äúby probability‚Äù means that the correct class is determined in 10% of cases, and the Softmax loss function is the inverse logarithm to the probability of the correct class, that is, it turns out</i> <math> </math><i><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1"><span class="MJXp-mo" id="MJXp-Span-2" style="margin-left: 0em; margin-right: 0.111em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-3">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-4">n</span><span class="MJXp-mo" id="MJXp-Span-5" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mn" id="MJXp-Span-6">0.1</span><span class="MJXp-mo" id="MJXp-Span-7" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-8" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mrow" id="MJXp-Span-9"><span class="MJXp-mo" id="MJXp-Span-10" style="margin-left: 0em; margin-right: 0em;">$</span></span><span class="MJXp-mn" id="MJXp-Span-11">2.30</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="17.072ex" height="2.66ex" viewBox="0 -832 7350.6 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-2212" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMATHI-6C" x="778" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMATHI-6E" x="1077" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-28" x="1677" y="0"></use><g transform="translate(2067,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-2E" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-31" x="779" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-29" x="3346" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-3D" x="4013" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-24" x="5070" y="0"></use><g transform="translate(5570,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-32"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-2E" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-33" x="779" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-30" x="1279" y="0"></use></g></g></svg></span><script type="math/tex" id="MathJax-Element-1"> -ln (0.1) = $ 2.30</script></i> <br><br>  After that, try increasing the regularization strength, which should increase the loss function. <br><br><h3>  18. Check loss function </h3><br>  If you have implemented your own, check it for bugs and add unit tests.  I have often had a slightly wrong loss function subtly harming network performance. <br><br><h3>  19. Check the input loss function </h3><br>  If you use the loss function of the framework, then make sure that you give it what you need.  For example, in PyTorch, I would mix NLLLoss and CrossEntropyLoss, because the first one requires softmax input data, and the second one does not. <br><br><h3>  20. Adjust the weight loss function </h3><br>  If your loss function consists of several functions, check their relation to each other.  For this you may need to test in different versions of the relationships. <br><br><h3>  21. Follow other indicators. </h3><br>  Sometimes the loss function is not the best predictor of how well your neural network is learning.  If possible, use other indicators, such as accuracy. <br><br><h3>  22. Check each custom layer. </h3><br>  Have you independently implemented any of the network layers?  Double check that they work as expected. <br><br><h3>  23. Verify that there are no ‚Äústuck‚Äù layers or variables. </h3><br>  Look, maybe you unintentionally turned off the gradient updates of some layers / variables. <br><br><h3>  24. Increase network size </h3><br>  Perhaps the expressive power of the network is not enough to assimilate the objective function.  Try adding layers or more hidden units to fully connected layers. <br><br><h3>  25. Look for hidden measurement errors. </h3><br>  If your input looks like <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-12"><span class="MJXp-mo" id="MJXp-Span-13" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-14">k</span><span class="MJXp-mo" id="MJXp-Span-15" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-16">H</span><span class="MJXp-mo" id="MJXp-Span-17" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-18">W</span><span class="MJXp-mo" id="MJXp-Span-19" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-20" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-21" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mn" id="MJXp-Span-22">64</span><span class="MJXp-mo" id="MJXp-Span-23" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-24">64</span><span class="MJXp-mo" id="MJXp-Span-25" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-26">64</span><span class="MJXp-mo" id="MJXp-Span-27" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="23.538ex" height="2.66ex" viewBox="0 -832 10134.2 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-28" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMATHI-6B" x="389" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-2C" x="911" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMATHI-48" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-2C" x="2244" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMATHI-57" x="2689" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-29" x="3738" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-3D" x="4405" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-28" x="5461" y="0"></use><g transform="translate(5851,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-36"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-34" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-2C" x="6852" y="0"></use><g transform="translate(7297,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-36"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-34" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-2C" x="8298" y="0"></use><g transform="translate(8743,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-36"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-34" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/334944/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjRThtqyhCfAW3fgXZApykO9iFYyQ#MJMAIN-29" x="9744" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-2"> (k, H, W) = (64, 64, 64) </script>  then it is easy to miss the error due to incorrect measurements.  Use unusual numbers to measure input data (for example, different simple numbers for each dimension) and see how they are distributed throughout the network. <br><br><h3>  26. Explore Gradient Checking </h3><br>  If you independently implemented Gradient Descent, then with the help of Gradient Checking you can be sure of the correct feedback.  Additional information: <a href="http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/">1</a> , <a href="http://cs231n.github.io/neural-networks-3/">2</a> , <a href="https://www.coursera.org/learn/machine-learning/lecture/Y3s6r/gradient-checking">3</a> . <br><br><a name="4"></a><h1>  Iv.  Learning problems </h1><br><img src="https://habrastorage.org/getpro/habr/post_images/1f6/298/603/1f6298603efa05d69690060e56aa7178.png"><br>  <sub><font color="gray">Source: <a href="http://carlvondrick.com/ihog/">http://carlvondrick.com/ihog/</a></font></sub> <br><br><h3>  27. Solve the problem for a really small data set. </h3><br>  <b>Re-train the network on a small dataset and make sure it works</b> .  For example, teach it with just 1-2 examples and see if the network is able to distinguish objects.  Go to more samples for each class. <br><br><h3>  28. Check the initialization of the scale. </h3><br>  If unsure, use <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Xavier</a> or <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf">He</a> initialization.  In addition, your initialization can lead to a bad local minimum, so try a different initialization, it can help. <br><br><h3>  29. Change the hyperparameters </h3><br>  Maybe you are using a bad set of hyperparameters.  If possible, try <a href="http://scikit-learn.org/stable/modules/grid_search.html">grid search</a> . <br><br><h3>  30. Reduce regularization </h3><br>  Due to too much regularization, the network may be specifically under-trained.  Reduce regularization, such as dropout, batch norm, L2-regularization weight / bias, etc. In an excellent course ‚Äú <a href="http://course.fast.ai/">Practical depth training for programmers,</a> ‚Äù <a href="https://twitter.com/jeremyphoward">Jeremy Howard</a> recommends getting rid of under- <a href="http://course.fast.ai/">training</a> first.  That is, you need enough to retrain the network on the source data, and only then deal with retraining. <br><br><h3>  31. Give time </h3><br>  Maybe the network needs more time to learn before it starts making meaningful predictions.  If the loss function is steadily decreasing, let it learn a little longer. <br><br><h3>  32. Switch from learning mode to testing mode. </h3><br>  In some frameworks, the Batch Norm, Dropout, and others layers behave differently during training and testing.  Switching to the right mode can help your network start making correct predictions. <br><br><h3>  33. Visualize learning </h3><br><ul><li>  Track activations, weights and updates for each layer.  Make sure the ratios of their values ‚Äã‚Äãmatch.  For example, the ratio of the magnitude of updates to parameters (weights and offsets) <a href="https://cs231n.github.io/neural-networks-3/">should be 1e-3</a> . </li><li>  View visualization libraries like <a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard">Tensorboard</a> and <a href="https://github.com/torrvision/crayon">Crayon</a> .  In extreme cases, you can simply print the values ‚Äã‚Äãof weights / shifts / activations. </li><li>  Be careful with network activations with an average much greater than zero.  Try Batch Norm or ELU. </li><li>  <a href="https://deeplearning4j.org/visualization">Deeplearning4j</a> indicated what to look in the histograms of weights and shifts: </li></ul><br><blockquote>  <i>‚ÄúFor weights, these histograms should have an <b>approximately Gaussian (normal)</b> distribution, after some time.</i>  <i>Shift histograms usually start from zero and usually end at <b>about a Gaussian</b> distribution (the only exception is LSTM).</i>  <i>Watch for parameters that deviate at plus / minus infinity.</i>  <i>Watch for shifts that get too big.</i>  <i>Sometimes this happens in the output layer for classification if the distribution of classes is too unbalanced. ‚Äù</i> </blockquote><br><ul><li>  Check for updates of layers, they should have a normal distribution. </li></ul><br><h3>  34. Try a different optimizer. </h3><br>  Your choice of optimizer should not prevent the neural network from learning, unless you have specifically chosen poor hyperparameters.  But the right optimizer for the task can help you get the best training in the shortest possible time.  A scientific article describing the algorithm that you are using should also be mentioned by the optimizer.  If not, I prefer to use Adam or plain SGD. <br><br>  Read the <a href="http://ruder.io/optimizing-gradient-descent/">excellent article by</a> Sebastian Ruder to learn more about gradient descent optimizers. <br><br><h3>  35. Explosion / Disappearance of Gradients </h3><br><ul><li>  Check for layer updates, as very large values ‚Äã‚Äãmay indicate gradient explosions.  Clipping gradient can help. </li><li>  Check layer activation.  <a href="https://deeplearning4j.org/visualization">Deeplearning4j</a> gives excellent advice: <i>‚ÄúA good standard deviation for activations is in the region from 0.5 to 2.0.</i>  <i>A significant overrun may indicate an explosion or disappearance of activations. ‚Äù</i> </li></ul><br><h3>  36. Accelerate / slow down learning. </h3><br>  Low learning speed will lead to a very slow convergence of the model. <br><br>  A high learning rate will first quickly reduce the loss function, and then it will be difficult for you to find a good solution. <br><br>  Experiment with the speed of learning, accelerating or slowing it down 10 times. <br><br><h3>  37. Elimination of NaN states </h3><br>  NaN (Non-a-Number) states are much more common when learning RNN (as I heard).  Some ways to eliminate them: <br><br><ul><li>  Reduce the learning rate, especially if NaN appears in the first 100 iterations. </li><li>  Non-numbers can occur due to division by zero, taking the natural logarithm of zero, or a negative number. </li><li>  Russell Stewart offers good advice on <a href="http://russellsstewart.com/notes/0.html">what to do when NaN appears</a> . </li><li>  Try to evaluate the network layer by layer and see where NaN appears. </li></ul><br><div class="spoiler">  <b class="spoiler_title">Sources</b> <div class="spoiler_text">  <a href="http://cs231n.github.io/neural-networks-3/">cs231n.github.io/neural-networks-3</a> <br>  <a href="http://russellsstewart.com/notes/0.html">russellsstewart.com/notes/0.html</a> <br>  <a href="https://stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-class">stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-class</a> <br>  <a href="https://deeplearning4j.org/visualization">deeplearning4j.org/visualization</a> <br>  <a href="https://www.reddit.com/r/MachineLearning/comments/46b8dz/what_does_debugging_a_deep_net_look_like/">www.reddit.com/r/MachineLearning/comments/46b8dz/what_does_debugging_a_deep_net_look_like</a> <br>  <a href="https://www.researchgate.net/post/why_the_prediction_or_the_output_of_neural_network_does_not_change_during_the_test_phase">www.researchgate.net/post/why_the_prediction_or_the_output_of_neural_network_does_not_change_during_the_test_phase</a> <br>  <a href="http://book.caltech.edu/bookforum/showthread.php%3Ft%3D4113">book.caltech.edu/bookforum/showthread.php?t=4113</a> <br>  <a href="https://gab41.lab41.org/some-tips-for-debugging-deep-learning-3f69e56ea134">gab41.lab41.org/some-tips-for-debugging-deep-learning-3f69e56ea134</a> <br>  <a href="https://www.quora.com/How-do-I-debug-an-artificial-neural-network-algorithm">www.quora.com/How-do-I-debug-an-artificial-neural-network-algorithm</a> </div></div></div><p>Source: <a href="https://habr.com/ru/post/334944/">https://habr.com/ru/post/334944/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../334934/index.html">Security Week 31: WannaCry fighter arrested in the US, Svpeng got a new chip, Cisco patch 15 holes</a></li>
<li><a href="../334936/index.html">How did we fix our process and become less distracted</a></li>
<li><a href="../334938/index.html">How to check your site for the integrity of the content: does the client see what they should?</a></li>
<li><a href="../334940/index.html">Oil price fluctuations: is algorithmic trading to blame?</a></li>
<li><a href="../334942/index.html">Android Architecture Components. Part 4. ViewModel</a></li>
<li><a href="../334948/index.html">Look up from the bottom or Ubuntu Server for the developer of electronics. Part 1</a></li>
<li><a href="../334950/index.html">Linux processes in Bash man</a></li>
<li><a href="../334952/index.html">Server side rendering on Vue.js</a></li>
<li><a href="../334954/index.html">Dream job hunt</a></li>
<li><a href="../334956/index.html">How to start young mobile game developers from Russia in the current realities</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>