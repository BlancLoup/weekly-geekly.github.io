<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Sparse matrices: how scientists accelerated machine learning on the GPU</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In early December, researchers from OpenAI presented a library of tools that will help accelerate the training of neural networks on the GPU from Nvid...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Sparse matrices: how scientists accelerated machine learning on the GPU</h1><div class="post__text post__text-html js-mediator-article">  In early December, researchers from OpenAI presented a library of tools that will help accelerate the training of neural networks on the GPU from Nvidia through the use of sparse matrices.  About the difficulties that the developers of neural networks face and what is the main idea of ‚Äã‚Äãthe solution from OpenAI, we will tell further. <br><br> <a href="https://habrahabr.ru/company/it-grad/blog/344320/"><img src="https://habrastorage.org/webt/7r/ph/fg/7rphfgalymifxzncvdwnbntt4y0.jpeg"></a> <a name="habracut"></a><br>  <font color="#A9A9A9"><i>/ photo <a href="https://www.flickr.com/photos/digitaljourney/5424241457/">alantankenghoe</a> <a href="https://creativecommons.org/licenses/by/2.0/">CC</a></i></font> <br><br><h2>  The difficulties of training large neural networks on the GPU </h2><br>  Graphic processors (GPUs) are better suited for machine learning than central processing units (CPUs).  Technical features help the GPU to simultaneously perform many <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B0%25D1%2582%25D1%2580%25D0%25B8%25D1%2586%25D0%25B0_(%25D0%25BC%25D0%25B0%25D1%2582%25D0%25B5%25D0%25BC%25D0%25B0%25D1%2582%25D0%25B8%25D0%25BA%25D0%25B0)">matrix operations</a> that are <a href="https://www.quora.com/Why-are-matrices-vectors-used-in-machine-learning-data-analysis">used</a> to train neural networks. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      To achieve a similar result on the central processor, it is necessary to build the infrastructure of several clusters of CPUs, which is very expensive.  The Google system for training neural networks on a CPU <a href="https://www.analyticsvidhya.com/blog/2017/05/gpus-necessary-for-deep-learning/">cost</a> about $ 5 billion.  Today, scientists from Stanford have built a system with a similar computing power on the GPU for only 33 thousand dollars. <br><br>  However, there are difficulties: use the full potential of the GPU in resource-intensive tasks is not so simple.  For processing, the data must be stored in the memory of the GPU, but its volume <a href="https://www.ibm.com/blogs/research/2017/12/10x-faster-using-gpu/">is small</a> , which makes it difficult to train large models.  For example, the VGG-16 model <a href="https://medium.com/%40Synced/how-to-train-a-very-large-and-deep-model-on-one-gpu-7b7edfe2d072">requires</a> about 14 GB, while the memory size of the Nvidia Titan X is 12 GB.  And this map Nvidia is <a href="http://www.nvidia.ru/object/blog-nvidia-digits-devbox-ru.html">positioning</a> as one of the most powerful GPU for deep learning. <br><br>  <i>As <a href="https://habrahabr.ru/users/evilgenius18/" class="user_link">EvilGenius18</a> correctly noted in the comments, on December 7, Nvidia unveiled a new Titan V card on the Volta architecture.</i>  <i>It <a href="https://www.nvidia.com/en-us/titan/titan-v/">has</a> 110 TFLOPS computing capacity for deep learning tasks, which is 9 times <a href="https://www.forbes.com/sites/patrickmoorhead/2017/12/07/nvidia-introduces-titan-v-for-machine-learning-acceleration-on-the-pc/">more</a> than its predecessor.</i> <br><br>  At the same time, for effective training of large models of neural networks, various approaches are used.  One of them is the <a href="https://www.ibm.com/blogs/research/2017/12/10x-faster-using-gpu/">processing of</a> data on the graphics processor in consecutive batches, when the CPU <a href="https://medium.com/%40Synced/how-to-train-a-very-large-and-deep-model-on-one-gpu-7b7edfe2d072">acts as a</a> temporary container.  The disadvantage of this approach is the use of resources for data transfer. <br><br>  It is possible to <a href="http://journal.jp.fujitsu.com/en/2017/01/12/01/">use</a> several GPUs simultaneously, but the number of GPUs on a single computer is limited, therefore a high-speed connection between computing systems is required.  The intercomputer communication channel affects learning speed, since the machines in this case <a href="https://devblogs.nvidia.com/parallelforall/tag/multi-gpu/">spend</a> more time on ‚Äúcommunication‚Äù than on calculations. <br><br>  There is another solution that is <a href="https://dziganto.github.io/Sparse-Matrices-For-Efficient-Machine-Learning/">used</a> in machine learning for optimization, <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D0%25B7%25D1%2580%25D0%25B5%25D0%25B6%25D0%25B5%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BC%25D0%25B0%25D1%2582%25D1%2580%25D0%25B8%25D1%2586%25D0%25B0">sparse matrices</a> .  These are matrices that mostly contain zero elements.  The advantage is that zeros in matrix operations are <a href="http://wwwcdl.bmstu.ru/iu7/book1/stage6.htm">perceived</a> as empty components.  Therefore, such matrices consume less GPU memory.  This speeds up the machine learning process, which is important for large models. <br><br>  But there is a problem: Nvidia solutions, the <a href="https://finance.yahoo.com/news/nvidia-running-away-gpu-market-031200215.html">main supplier of</a> GPUs, <a href="https://www.theregister.co.uk/2017/12/06/openai_block_sparsity_gpu_kernels_stuff/">do not support</a> working with sparse matrices.  But OpenAI found a way out of this situation. <br><br><h2>  OpenAI solution </h2><br>  The OpenAI team has <a href="https://blog.openai.com/block-sparse-gpu-kernels/">developed</a> software that models the work of tiny kernels that can interact with such matrices.  The kernels were tested on training networks analyzing reviews on Amazon and IMDB sites.  According to the team, the level of errors in working with IMDB data has <a href="https://siliconangle.com/blog/2017/12/06/openai-creates-block-sparse-gpu-kernels-speed-neural-networks/">been reduced</a> from 5.91% to 5.01%. <br><br>  The kernels are implemented using <a href="https://en.wikipedia.org/wiki/CUDA">CUDA</a> , Nvidia's parallel computing software and hardware architecture.  But the OpenAI model is currently available only for TensorFlow.  Scott Gray, a member of the Open AI team, said the solution could be extended to other architectures besides Google TPU2.  Nvidia already knows about the work of OpenAI and is ready to optimize its systems. <br><br><h2>  Alternative projects </h2><br>  The concept of sparse matrices <a href="https://www.theregister.co.uk/2017/11/02/taco_ai_kernel_sparse_matrices/">was</a> embodied in <a href="https://www.osp.ru/cw/2001/06/9339/">an</a> open source <a href="https://www.osp.ru/cw/2001/06/9339/">compiler</a> called Taco.  About the project, which is working on a team of scientists from the Massachusetts Institute of Technology in partnership with Adobe Research, it became known in November.  The developers were looking for a way to automate the process of processing numbers in sparse matrices.  And used for this <a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B5%25D0%25BD%25D0%25B7%25D0%25BE%25D1%2580">tensors</a> . <br><br>  About their developments in the field of machine learning in December, and <a href="https://www.ibm.com/blogs/research/2017/12/10x-faster-using-gpu/">reported the</a> company IBM.  The solution of the IT giant - DuHL - offers a new method of transferring data from the CPU to the GPU.  The main task of the technology is to determine which information is most important for the learning algorithm and transmit it to the network in the correct order.  Studies have shown that the new approach based on DuHL is 10 times faster than the classical method of serial data transfer between processors.  The company's next goal is to offer DuHL as a service in the cloud. <br><br>  But IBM is not the first to come up with transferring GPU computing to the cloud.  Such projects, including those working on the IaaS model, are already <a href="https://habrahabr.ru/company/it-grad/blog/307486/">known</a> .  Initially vGPU was provided by Nvidia.  Both AMD and Intel <a href="http://www.brianmadden.com/opinion/NVIDIA-AMD-and-Intel-How-they-do-their-GPU-virtualization">are doing</a> this now. <br><br><h5>  About OpenAI </h5><br>  OpenAI is a non-profit research organization founded by the head of Tesla Ilon Mask.  It aims to promote and develop artificial intelligence for the benefit of mankind.  The organization closely cooperates with other institutions and researchers, providing open access to its developments. <br><br><hr><br>  PS A few more materials from our corporate blog: <br><br><ul><li>  <a href="http://iaas-blog.it-grad.ru/kejsy/azoft-kak-iaas-oblaka-pomogayut-v-proektax-po-servisnoj-razrabotke/">How IaaS-clouds help in service development projects: Azoft case</a> </li><li>  <a href="http://iaas-blog.it-grad.ru/tendencii/bim-uxodit-v-oblaka-obzor-klyuchevyx-reshenij-dlya-informacionnogo-modelirovaniya/">BIM Technologies: Solution Overview for Information Modeling</a> </li><li>  <a href="http://iaas-blog.it-grad.ru/funkcionalnost/balansirovka-nagruzki-v-oblake-iaas/">IaaS cloud load balancing: why you need it and how it works</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/344320/">https://habr.com/ru/post/344320/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../344304/index.html">Language Lua and Corona SDK (1/3 part)</a></li>
<li><a href="../344306/index.html">Life at the Unity Asset Store. Briefly</a></li>
<li><a href="../344310/index.html">Fake design</a></li>
<li><a href="../344312/index.html">Language Lua and Corona SDK (2/3 part)</a></li>
<li><a href="../344314/index.html">Dagger 2 for novice Android developers. Dagger 2. Part 1</a></li>
<li><a href="../344324/index.html">Continuous integration and deployment of Docker in GitLab CI</a></li>
<li><a href="../344326/index.html">Introduction to VxLAN</a></li>
<li><a href="../344328/index.html">Accelerate the site. How to understand if this is relevant for your site</a></li>
<li><a href="../344330/index.html">The digest of interesting materials for the mobile developer # 233 (December 4 - December 10)</a></li>
<li><a href="../344332/index.html">How to read technical literature: Quora, Reddit and Hacker News Resident Tips</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>