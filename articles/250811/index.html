<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Apache Spark Introduction</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! 



 Last time, we looked at the wonderful Vowpal Wabbit tool, which is useful in cases when you have to study on samples that do not fit in...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Apache Spark Introduction</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/550/b31/bd9/550b31bd91269dd77ce8b0041798d8f8.png" alt="image"><br><br>  Last time, we looked at the wonderful <a href="http://habrahabr.ru/post/248779/">Vowpal Wabbit</a> tool, which is useful in cases when you have to study on samples that do not fit in the RAM.  Recall that a feature of this tool is that it allows you to build primarily linear models (which, by the way, have a good generalizing ability), and the high quality of the algorithms is achieved through selection and generation of features, regularization and other additional techniques.  Today we consider a tool that is more popular and designed for processing large amounts of data - <b>Apache Spark</b> . <br><a name="habracut"></a><br>  We will not go into the details of the history of this instrument, as well as its internal structure.  Focus on practical things.  In this article we will look at the basic operations and basic things that can be done in Spark, and next time we will take a closer look at the <b>MlLib</b> machine learning library, as well as <b>GraphX</b> for graph processing (the author of this post mainly uses this tool for just the case when it is often necessary to keep a graph in RAM on a cluster, while Vowpal Wabbit is often enough for machine learning).  There will not be a lot of code in this manual, since  discusses the basic concepts and philosophy of Spark.  In the following articles (about MlLib and GraphX) we will take some dataset and take a closer look at Spark in practice. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      At once, we say that Spark natively supports <b>Scala</b> , <b>Python</b> and <b>Java</b> .  Examples will be considered in Python, because  it is very convenient to work directly in <b>IPython Notebook</b> , unloading a small part of the data from the cluster and processing, for example, with the <b>Pandas</b> package - you get a rather convenient bundle <br><br>  Let's start with the fact that the basic concept in Spark is <b>RDD (Resilient Distributed Dataset)</b> , which is a Dataset, on which you can do two types of transformations (and, accordingly, all the work with these structures is in the sequence of these two actions) . <br><img src="https://habrastorage.org/getpro/habr/post_images/f61/113/978/f611139782ef062d9490da990707218e.jpg" alt="image"><br><br><h3>  Transformations </h3><br>  The result of applying this operation to RDD is a new RDD.  As a rule, these are operations that in some way convert the elements of a given dataset.  Here is the incomplete of the most common transformations, each of which returns a new dataset (RDD): <br><br>  <b>.map (function)</b> - applies function function to each element of dataset <br><br>  <b>.filter (function)</b> - returns all elements of the dataset on which the function function returns the true value <br><br>  <b>.distinct ([numTasks])</b> - returns a dataset that contains unique elements of the source dataset <br><br>  It is also worth noting about operations on sets, the meaning of which is clear from the names: <br><br>  <b>.union (otherDataset)</b> <br><br>  <b>.intersection (otherDataset)</b> <br><br>  <b>.cartesian (otherDataset)</b> - the new dataset contains all sorts of pairs (A, B), where the first element belongs to the original dataset, and the second one - to the dataset argument <br><br><h3>  Actions </h3><br>  Actions are applied when it is necessary to materialize the result - as a rule, save the data to disk, or output part of the data to the console.  Here is a list of the most common actions that can be applied over RDD: <br><br>  <b>.saveAsTextFile (path)</b> - saves data to a text file (in hdfs, to a local machine or to any other supported file system - a full list can be found in the documentation) <br><br>  <b>.collect ()</b> - returns <b>dataset</b> elements as an array.  As a rule, this is used in cases when there is little data in the dataset (various filters and transformations are applied) - and visualization or additional data analysis is necessary, for example, using the Pandas package <br><br>  <b>.take (n)</b> - returns the first n <b>dataset</b> elements as an array <br><br>  <b>.count ()</b> - returns the number of elements in dataset <br><br>  <b>.reduce (function)</b> is a familiar operation for those familiar with <b>MapReduce</b> .  From the mechanism of this operation, it follows that the function function (which takes 2 arguments as an input returns one value) must be commutative and associative <br><br>  These are the basics you need to know when working with a tool.  Now let's do a little practice and show you how to load data into Spark and do simple calculations with it. <br><br>  When you start Spark, the first thing you need to do is create a <b>SparkContext</b> (in simple terms, an object that is responsible for implementing lower-level operations with a cluster ‚Äî see the documentation for details), which is automatically created when you start <b>Spark-Shell.</b> ( <b>sc</b> object) <br><br><h3>  Data loading </h3><br>  You can upload data to Spark in two ways: <br><br>  <b>but).</b>  Directly from a local program using the <b>.parallelize (data)</b> function <br><br><pre><code class="python hljs">localData = [<span class="hljs-number"><span class="hljs-number">5</span></span>,<span class="hljs-number"><span class="hljs-number">7</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">12</span></span>,<span class="hljs-number"><span class="hljs-number">10</span></span>,<span class="hljs-number"><span class="hljs-number">25</span></span>] ourFirstRDD = sc.parallelize(localData)</code> </pre> <br>  <b>b).</b>  From supported storages (for example, hdfs) using the <b>.textFile (path)</b> function <br><br><pre> <code class="python hljs">ourSecondRDD = sc.textFile(<span class="hljs-string"><span class="hljs-string">"path to some data on the cluster"</span></span>)</code> </pre><br>  At this point, it is important to note one feature of data storage in Spark and at the same time the most useful function <b>.cache ()</b> (partly due to which Spark became so popular), which allows you to cache data in RAM (taking into account the availability of the latter).  This allows you to perform iterative calculations in memory, thereby getting rid of IO-overhead.  This is especially important in the context of machine learning and graphing;  most algorithms are iterative, ranging from gradient methods to algorithms such as <b>PageRank</b> <br><br><h3>  Work with data </h3><br>  After loading the data in RDD, we can do various transformations and actions on it, as mentioned above.  For example: <br><br>  Let's look at the first few elements: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> item <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> ourRDD.top(<span class="hljs-number"><span class="hljs-number">10</span></span>): <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> item</code> </pre><br>  Either we immediately load these elements into Pandas and work with the DataFrame: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd pd.DataFrame(ourRDD.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: x.split(<span class="hljs-string"><span class="hljs-string">";"</span></span>)[:]).top(<span class="hljs-number"><span class="hljs-number">10</span></span>))</code> </pre><br>  In general, as you can see, Spark is so convenient that further, probably there is no point in writing various examples, or you can just leave this exercise to the reader - many calculations are written literally in a few lines. <br><br>  Finally, we will show only an example of transformation, namely, we calculate the maximum and minimum elements of our dataset.  As you can easily guess, this can be done, for example, using the <b>.reduce ()</b> function: <br><br><pre> <code class="python hljs">localData = [<span class="hljs-number"><span class="hljs-number">5</span></span>,<span class="hljs-number"><span class="hljs-number">7</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">12</span></span>,<span class="hljs-number"><span class="hljs-number">10</span></span>,<span class="hljs-number"><span class="hljs-number">25</span></span>] ourRDD = sc.parallelize(localData) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> ourRDD.reduce(max) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> ourRDD.reduce(min)</code> </pre><br>  So, we have reviewed the basic concepts needed to work with the tool.  We did not consider working with SQL, working with pairs <b>&lt;key, value&gt;</b> (what is done is easy - to do this, first apply a filter to RDD, for example, to select the key, and then it‚Äôs easy to use built-in functions like <b>sortByKey</b> , <b>countByKey</b> , <b>join</b> , etc.) - the reader is invited to familiarize himself with this, and if you have questions, write in a comment.  As already noted, the next time we will look in detail at the MlLib library and separately - GraphX </div><p>Source: <a href="https://habr.com/ru/post/250811/">https://habr.com/ru/post/250811/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../250797/index.html">Android for developers</a></li>
<li><a href="../250799/index.html">Ask Lenovo</a></li>
<li><a href="../250803/index.html">Centralized logs for applications using heka + elasticsearch + kibana</a></li>
<li><a href="../250807/index.html">Visual brute force on the example of solving a 3D puzzle</a></li>
<li><a href="../250809/index.html">Using an alternative memory allocator in a C / C ++ project</a></li>
<li><a href="../250813/index.html">Spatial positioning system for aviation (using FPGA)</a></li>
<li><a href="../250815/index.html">Lock-free data structures. Concurrent maps: skip list</a></li>
<li><a href="../250817/index.html">The mathematical problem of 100 boxes and the rescue of prisoners</a></li>
<li><a href="../250819/index.html">Black Swift - at Embedded World Conference</a></li>
<li><a href="../250821/index.html">The quality of data networks. Software and hardware measurements</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>