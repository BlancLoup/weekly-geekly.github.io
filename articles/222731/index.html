<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Automatic placement of search tags</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this article we will try to talk about the problem of multiple classification on the example of solving the problem of automatic placement of searc...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Automatic placement of search tags</h1><div class="post__text post__text-html js-mediator-article">  In this article we will try to talk about the problem of multiple classification on the example of solving the problem of automatic placement of search tags for text documents in our project <a href="http://favoraim.com/">www.favoraim.com</a> .  Readers familiar with the subject will most likely not find anything new for themselves, but in the process of solving this problem we read a lot of different literature where very little was said about the problem of multiple classification, or not at all. <br><br>  So, let's start with the formulation of the classification problem.  Let X be the set of descriptions of objects, Y the set of numbers (or names) of classes.  There is an unknown target relationship - mapping <img src="https://habrastorage.org/getpro/habr/post_images/15a/ae4/fb7/15aae4fb7e5052ee59893b56c67495c1.png" alt="image">  whose values ‚Äã‚Äãare known only on the objects of the final training set <img src="https://habrastorage.org/getpro/habr/post_images/01e/edf/a4b/01eedfa4b9348c9faa76ce75cec26eb0.png" alt="image">  .  Need to build an algorithm <img src="https://habrastorage.org/getpro/habr/post_images/9b1/46d/6bc/9b146d6bc74d3bea499caf31395f7279.png" alt="image">  able to classify an arbitrary object x‚ààX.  However, the most common is the probabilistic formulation of the problem.  Let X be the set of descriptions of objects, Y the set of numbers (or names) of classes.  On the set of pairs ‚Äúobject, class‚Äù X √ó Y, a probability measure P is defined. There is a finite training set of independent observations <img src="https://habrastorage.org/getpro/habr/post_images/01e/edf/a4b/01eedfa4b9348c9faa76ce75cec26eb0.png" alt="image">  obtained according to a probabilistic measure P. <br><a name="habracut"></a><br>  We now turn to the task of automatically placing search tags.  We have an object x - a text document and many classes <img src="https://habrastorage.org/getpro/habr/post_images/6a5/803/fee/6a5803fee18da3583ef23e5b14208d6d.png" alt="image">  - search tags.  Each document must be matched with one or more search tags.  For example, we have an event with the title ‚ÄúConcert of the Apocalyptica Group‚Äù, this event can be assigned the following search tags: ‚ÄúApocalyptica‚Äù, ‚Äúconcert‚Äù, ‚Äúheavy metal‚Äù, ‚Äúcello‚Äù, etc.  We also have a training set, i.e.  set of documents with already placed search tags.  Thus, we have a classification task with overlapping classes, i.e.  an object can belong to several classes simultaneously.  But instead, we will solve N problems of binary classification, every pair <img src="https://habrastorage.org/getpro/habr/post_images/a07/338/d12/a07338d124c5695f760215b28822a767.png" alt="image">  we classify into classes {0,1}, i.e.  Determine whether you can put a search tag <img src="https://habrastorage.org/getpro/habr/post_images/0df/0c8/870/0df0c887052ed9ffbd2a3e33b2427353.png" alt="image">  x document or not. <br>  To solve this problem, we will present text documents in the form of ‚Äúbag of words‚Äù or a multidimensional vector of words and their weight (frequency) in a document ( <a href="http://en.wikipedia.org/wiki/Bag-of-words_model">http://en.wikipedia.org/wiki/Bag-of-words_model</a> ).  In fig.  1 shows a histogram of the words describing business training, in Fig.  2 - a histogram of words describing the master class in photography. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2ed/7cc/6f3/2ed7cc6f3f1dcae43a1020cf5cf1c86a.jpg" alt="image">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/7dd/4ef/ad5/7dd4efad5fd51b21cb084a55307b746d.jpg" alt="image"><br><br>  As a classification method, you can take any statistical (Bayesian) classification method.  The probabilistic model for the classifier is the conditional model p (y‚îÇd), y‚ààY.  Now we need to restore the density p (y‚îÇd), y‚ààY, i.e.  the probability that for our document d you can put the search tag y‚ààY.  To restore the density, there are many methods, you can start with the naive Bayesian document classification model ( <a href="http://ru.wikipedia.org/wiki/%25D0%259A%25D0%25BB%25D0%25B0%25D1%2581%25D1%2581%25D0%25B8%25D1%2584%25D0%25B8%25D0%25BA%25D0%25B0%25D1%2586%25D0%25B8%25D1%258F_%25D0%25B4%25D0%25BE%25D0%25BA%25D1%2583%25D0%25BC%25D0%25B5%25D0%25BD%25D1%2582%25D0%25BE%25D0%25B2">http://ru.wikipedia.org/wiki/Document</a> classification).  For this model, a ‚Äúnaive‚Äù assumption is made about the independence of the words in the document, and although this is obviously an incorrect assumption, the model works quite well. <br>  Now that we have restored the distribution density and for each tag y‚ààY we have found the probability that it can be assigned to our document d, we need to determine which of the tags should be assigned to the document and which should be discarded, i.e.  find the minimum cut-off threshold for the probability p (y‚îÇd).  There will have to use the analysis of the ROC-curve ( <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3DROC-%25D0%25BA%25D1%2580%25D0%25B8%25D0%25B2%25D0%25B0%25D1%258F">http://www.machinelearning.ru/wiki/index.php?title=ROC-curve</a> ). <br>  The ROC curve shows the dependence of the number of correctly classified positive examples (true positive rate) on the number of incorrectly classified negative examples (false positive rate).  In this case, it is assumed that the classifier has a certain parameter; by varying which, we will receive a particular partition into two classes.  This parameter is often called a threshold, or a cut-off point.  In our case, the probability p (y‚îÇd) will play the role of this parameter.  Construct the ROC curve for the control sample, which is usually part of the training sample.  In this case, the objects of the control sample can not be used for training the classifier, otherwise we can get an overly optimistic ROC curve due to the phenomenon of retraining.  However, if resources allow us, we can use cross-validation.  Its essence lies in the fact that the training sample is divided into k parts, k-1 of which are used to train the model, the remaining part is used as a control sample.  This procedure is performed k times.  In practice, this technique is problematic, because  calculations take a lot of time. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/988/b49/37e/988b4937e52b7cf229d0c75c6c97fc37.jpg" alt="image"><br><br>  The ROC curve has the following main characteristics.  Sensitivity (sensitivity) - this is the proportion of truly positive cases.  Specificity (specificity) - the proportion of true negative cases that have been correctly identified by the model.  Note that fpr = 100-specifity.  Area under the ROC curve (AUC).  The AUC value is usually used to assess the quality of the classifier.  For example, the dependence fpr = tpr corresponds to the worst case (random fortune-telling).  In fig.  The worst case is indicated by the dotted line.  The higher the density, the better the predictions the classifier gives.  For an ideal classifier, the ROC curve goes through the top left corner. <br>  Now you need to select the minimum cut-off threshold.  There are 3 approaches here. <br><ul><li>  The requirement of the minimum value of the sensitivity (specificity) of the model.  Those.  one of the values ‚Äã‚Äãis set constant and, based on this, the value of the cut-off threshold is selected. </li><li>  The requirement of the maximum total sensitivity and specificity of the model. </li><li>  The requirement of a balance between sensitivity and specificity, i.e.  when specificity‚âàsensivity.  Here the threshold is the intersection point of the two curves, when the cut-off threshold is plotted along the x axis, and the sensitivity and specificity of the model along the y axis (Fig. 4). </li></ul><br><br><img src="https://habrastorage.org/getpro/habr/post_images/fdf/aaa/342/fdfaaa342327706e836935d2903daace.jpg" alt="image"><br><br>  Thus, we can assign to our document d those search tags for which p (y‚îÇd)&gt; c is executed, where c is the value of the cut-off threshold. <br><br>  And now a little practice.  The first thing to do is to convert the text of our document to normal form with the removal of stop words (for example, normalized_string (‚Äúexample of a string in normal form‚Äù) = ‚Äúexample string is a normal form‚Äù).  For these purposes, postgreSQL FTS dictionaries are quite suitable.  Next we need a set of documents with already tagged for training our classifier.  As an example, I will cite the pseudo-code for learning the Bayesian naive classifier. <br><br><pre><code class="java hljs">Map&lt;String, Map&lt;String, Integer&gt;&gt; naiveBayes; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Entry&lt;String, String[]&gt; entry: docSet) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (String lexem: get_normalized_string(entry.key).split(MY_DELIMS)) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (String tag: entry.value) { naiveBayes[tag][lexem]++; } } }</code> </pre> <br><br>  Thus, we assigned to each search tag a histogram of tokens from documents in our training sample.  After the classifier is trained, you can proceed to the calculation of the probabilities of the presence of a tag in a new document.  In the naive Bayesian model, the probability of the appearance of the tag t for the document d is calculated by the formula <img src="https://habrastorage.org/getpro/habr/post_images/2d2/0c1/9ef/2d20c19ef74b3209b1c39c2a472d19f2.png" alt="image">  , where P (t) is the probability that the tag t occurs, <img src="https://habrastorage.org/getpro/habr/post_images/04c/738/2c5/04c7382c5c9e20ecf0378214831dfe31.png" alt="image">  - document tokens including repetitions.  The probability P (t) can be estimated as <img src="https://habrastorage.org/getpro/habr/post_images/65d/648/751/65d6487516d0174f93db59f0027d0e2a.png" alt="image">  where <img src="https://habrastorage.org/getpro/habr/post_images/633/450/0a2/6334500a26658cad706f59b8f71eac1e.png" alt="image">  Is the number of documents in the training set with the t tag, and N is the number of all documents in the training set.  Estimation of probabilities P (l‚îÇt) with the help of the training set is as follows <img src="https://habrastorage.org/getpro/habr/post_images/3b7/b28/9f3/3b7b289f361b4172af01f12fc832e07d.png" alt="image">  where <img src="https://habrastorage.org/getpro/habr/post_images/112/7ff/f25/1127fff258918a71723a6166b4f1336b.png" alt="image">  - the number of occurrences of the lexeme l in all documents with the tag t, and <img src="https://habrastorage.org/getpro/habr/post_images/bce/f82/ff5/bcef82ff55d803a1c4520c90a4ea1fcf.png" alt="image">  - the number of all tokens in all documents with a tag t.  To avoid overflow in the formula for calculating probabilities due to the large number of factors, in practice, instead of the product, the sum of logarithms is usually used.  Logarithm does not affect the finding of the maximum, since the logarithm is a monotonically increasing function. <br><br><pre> <code class="java hljs">Map&lt;String, Double&gt; probs; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (String tag: naiveBayes.keySet()) { probs[tag] = log(P(t)); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (String lexem: get_normalized_string(document).split(MY_DELIMS)) { probs[tag] += log(naiveBayes[tag][lexem]/sum(naiveBayes[tag])); } }</code> </pre><br><br>  It remains to construct the ROC curve.  As a pseudocode, I, perhaps, will insert copy-paste. <br><br><blockquote>  <b>Canonical ROC curve construction algorithm</b> <br>  Inputs: L - many examples;  f [i] is the rating obtained by the model, or the probability that the i-th example has a positive outcome;  min and max are the minimum and maximum values ‚Äã‚Äãreturned by f;  dx - step;  P and N are the number of positive and negative examples, respectively. <br><br><pre> <code class="bash hljs">t=min  FP=TP=0    i  L {  f[i] &gt;= t  //       i    { TP=TP+1 }  //    { FP=FP+1 } } Se=TP/P*100 100_m_Sp=FP/N //  (100  Sp)   (100_m_Sp, Se)  ROC  t=t+dx  (t&gt;max)</code> </pre> </blockquote><br>  To search for the cut-off threshold by default, it is proposed to choose method 2: the requirement of the maximum total sensitivity and specificity of the model, I think it will not be difficult to find it.  That's probably all, if you have questions or suggestions, write, I will be happy to answer them. </div><p>Source: <a href="https://habr.com/ru/post/222731/">https://habr.com/ru/post/222731/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../222717/index.html">Lunokhod-1. The first track on the moon</a></li>
<li><a href="../222719/index.html">As we wrote the URL filtering Rostelecom</a></li>
<li><a href="../222725/index.html">Auto Layout and UIScrollView. How to cook it?</a></li>
<li><a href="../222727/index.html">About novice developers and how to work with them</a></li>
<li><a href="../222729/index.html">FLAC.JS Web Player (HTML5)</a></li>
<li><a href="../222737/index.html">As I wrote a web application using only clojure</a></li>
<li><a href="../222739/index.html">Imagine Cup 2014 through the eyes of C4L</a></li>
<li><a href="../222743/index.html">The free review of Chris Gilbo‚Äôs book ‚ÄúA startup for 100 dollars‚Äù</a></li>
<li><a href="../222745/index.html">The 7th meeting of the Odessa Testing community on May 22 in Ciklum</a></li>
<li><a href="../222747/index.html">The third meeting of the Kiev Android Community in Ciklum. Do not miss, it will be hot!</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>