<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>NetApp FAS performance optimization</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this article, I will focus on optimizing the performance of NetApp FAS systems. 

 Optimization objects in terms of storage can be settings: 


- S...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>NetApp FAS performance optimization</h1><div class="post__text post__text-html js-mediator-article">  In this article, I will focus on optimizing the performance of NetApp <abbr title="Fabric attached storage">FAS</abbr> systems. <br><br>  Optimization objects in terms of <abbr title="Storage System">storage</abbr> can be settings: <br><ul><li>  SAN in <abbr title="Storage System">storage</abbr> </li><li>  Ethernet to <abbr title="Storage System">storage</abbr> </li><li>  NAS in <abbr title="Storage System">storage</abbr> </li><li>  Disk subsystem on the Back-End <abbr title="Storage System">storage system</abbr> </li><li>  Disk Subsystem on Front-End <abbr title="Storage System">DSS</abbr> </li><li>  Firmware compatibility check </li><li>  Boosters </li></ul><br><img src="https://habrastorage.org/files/be5/c47/6a1/be5c476a135a4005afd8769cd921bdf0.jpg"><br><br>  For a bottleneck search, a sequential exception technique is usually performed.  I suggest first thing to start with the <abbr title="Storage System">storage system</abbr> .  And move on to the <b>storage system</b> -&gt; Network ( <a href="http://habrahabr.ru/post/243119/">Ethernet</a> / FC) -&gt; Host ( <a href="http://habrahabr.ru/post/243153/">Windows</a> / <a href="http://habrahabr.ru/post/245357/">Linux</a> / <a href="http://habrahabr.ru/post/247833/">VMware ESXi 5.X</a> and <a href="https://habrahabr.ru/post/303844/">ESXi 6.X</a> ) -&gt; Application. <br><a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  <a href="https://habr.com/ru/post/243045/">A brief educational program on NetApp:</a> </h4><a name="Likbez"></a><br>  I recall the paradigm of the device's internal structure of the NetApp <abbr title="Fabric attached storage">FAS</abbr> <abbr title="Storage System">storage system</abbr> , according to the ‚Äúshare nothing‚Äù ideology, which is almost always respected by <abbr title="Fabric attached storage">FAS</abbr> systems: <i>Disks</i> are combined into a <i>Raid group</i> ( <i>RAID-DP</i> ), Raid groups are combined into a <i>Plex</i> (Plex. analogue of RAID1), both Plexes are combined in <i>Aggregate</i> , <i>FlexVol</i> are created on Aggregate, <i>FlexVol</i> data is evenly spread across all disks in Aggregate, Qtree are created in <i>FlexVol</i> (something like folders to which any quotas can be assigned), Qtree cannot to be nested, then LUNs are created inside Qtree. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a0e/1f4/8b2/a0e1f48b2f03d02548a141fb77bccaac.png"><br>  NetApp FAS storage objects <br><br>  At one time, one disk can belong to only one controller; this is the most basic need of DataONTAP, deeply embedded in the bowels of the OS.  It is only necessary to note that the disk for DataONTAP can actually be both a disk as a whole and a partition (partition).  In other words, one such object (the entire disk or its partition) always has ownership, only one controller in the HA pair.  Thus, there is a nuance to which one can now disregard for simplicity of the picture: in the second case, all the same, a single physical disk can have several partitions whose ownership is assigned and used, for some partitions by one controller, for other partitions by another controller couples. <br><br>  So at one time, ‚Äúdisks‚Äù (in terms of DataONTAP), the moon, volyum, and aggregates belong to only one controller.  Access to this moon can nevertheless be accomplished through a partner in the <abbr title="High availability">HA</abbr> pair or in general through other nodes of the cluster.  But the best way to such a moon will always be only those that pass through the ports of the controller that owns the disks on which the moon is located -&gt; volyum-&gt; aggregate.  If there is more than one port on the controller, then all these ports are the best ways to the moon which is located on this controller.  The use of all possible optimal paths to the moon through all ports on the controller, as a rule, is positively related to the speed of access to it.  The host can use all the optimal paths at the same time or only a part of the optimal paths, it depends on the multipassing settings in the <abbr title="Operating system">OS of</abbr> the host and the portset settings on the storage. <br><br><img src="https://habrastorage.org/files/f44/cd6/23e/f44cd623eb064256a538aa97c7507a4b.gif"><br><br>  The same applies to all other protocols for <abbr title="Fabric attached storage">FAS</abbr> systems in Cluster-Mode - the best ways are those that pass through the ports of the controller on which the data is actually located.  In 7-Mode, there is simply no access to the data through non-optimal paths for the <abbr title="Common Internet File System">CIFS</abbr> / <abbr title="iSCSI">iSCSI</abbr> / <abbr title="Network file system">NFS</abbr> protocols. <br><br>  <b>Thus, it is necessary to ensure that the paths to the <i>Front-End are</i> always optimal for all storage access protocols.</b> <br><br>  So in the case of <abbr title="Fiber channel">FC</abbr> / <abbr title="Fiber channel over ethernet">FCoE</abbr> / <abbr title="iSCSI">iSCSI</abbr> <abbr title="Asymmetric Logical Unit Access">protocols</abbr> , the <abbr title="Asymmetric Logical Unit Access">ALUA</abbr> mechanism helps to clarify this issue, explaining to hosts where the optimal paths are and where not, and the correct multipaging settings ensure <a href="https://library.netapp.com/ecmdocs/ECMP1656701/html/GUID-CA36E7FD-B176-4C02-B02E-D9A0118B0A0A.html">that only optimal paths are used</a> in normal operation. <br><br>  For the <abbr title="Network file system">NFS</abbr> protocol, the issue of using optimal paths is resolved in <abbr title="Paralel Network File System">pNFS</abbr> , which support is <a href="http://community.netapp.com/t5/Tech-OnTap-Articles/NetApp-and-Red-Hat-Collaborate-on-pNFS/ta-p/85177">already implemented in RedHat Linux</a> .  That allows the client to understand and switch to the optimal path automatically without additional settings.  For the VMware ESXi 6.0 virtualization environment with the NFS v3.0 protocol, the issue of optimal paths is implemented using vVol technology, which is supported on NetApp FAS with ClusteredONTAP. <br><br>  For the <abbr title="Common Internet File System">CIFS</abbr> (SMB) protocol in version 3.0, the <a href="http://community.netapp.com/t5/Tech-OnTap-Articles/What-s-New-in-Clustered-Data-ONTAP-8-2/ta-p/85828">SMB Auto Location</a> mechanism is implemented, which allows, like <abbr title="Paralel Network File System">pNFS, to</abbr> switch to the optimal path to the file sphere. <br><br><h4>  <a href="https://habr.com/ru/post/243045/">SAN Multipathing</a> </h4><a name="SAN_Multipathing"></a><br>  When using NetApp <abbr title="Fabric attached storage">FAS</abbr> in 7-Mode and Cluster-Mode on <abbr title="Fiber channel">FC</abbr> / <abbr title="Fiber channel over ethernet">FCoE,</abbr> you must enable <abbr title="Asymmetric Logical Unit Access">ALUA</abbr> .  The settings for <abbr title="iSCSI">iSCSI</abbr> for 7-Mode and Cluster-Mode are different, so for the first case the <abbr title="Asymmetric Logical Unit Access">ALUA</abbr> mode <abbr title="Asymmetric Logical Unit Access">can</abbr> not be enabled, and for the second one is required. <br><br>  Multipathing should by default use the preferred paths - paths to the <abbr title="Logical unit number">LUN</abbr> through the ports of the controller on which it is located.  Messages in the console <a href="http://blog.aboutnetapp.ru/archives/971">FCP Partner Path Misconfigured</a> will talk about <a href="http://blog.aboutnetapp.ru/archives/631">incorrectly configured</a> <abbr title="Asymmetric Logical Unit Access">ALUA</abbr> or <abbr title="Multipath Input-Output">MPIO</abbr> .  This is an important parameter; you should not ignore it, since there was one real case where the enraged host multipassing driver continuously switched between paths, thus creating large queues in the I / O system. <br><br><h5>  <a href="https://habr.com/ru/post/243045/">Portset</a> </h5><a name="PortSet"></a><br>  With a SAN cluster size of 8 nodes (4 ON pairs) of FAS systems, the number of access paths (and therefore ports on the controllers) to the moon can reach unimaginable quantities.  The number of nodes in the cluster will only grow; at the same time, for frequent, you can limit yourself to a completely reasonable number of main and alternate paths without sacrificing fault tolerance.  So a <i>portset</i> comes to the rescue of this issue, allowing ‚Äúto see‚Äù moons only on specified ports.  Therefore, the storage ports through which your moons are visible should be switched and configured on the switch, plus the zoning should be configured accordingly. <br><br><h5>  <a href="https://habr.com/ru/post/243045/">Selective LUN Map</a> </h5><a name="SLM"></a><br>  Starting from DataONTAP 8.3, the default <abbr title="Selective lun mapping">SLM</abbr> method is applied, reducing the number of paths to the moon to two controllers: the owner of the moon and its partner in the <abbr title="High availability">HA</abbr> pair.  When migrating the moon using the <abbr title="Selective lun mapping">SLM</abbr> mechanism to another node of the cluster, no additional manipulation is required from the administrator to specify the ports through which the moons will be available, everything happens automatically.  We need to make <abbr title="World Wide Port Name">sure</abbr> that the <abbr title="World Wide Port Name">WWPN of</abbr> our <abbr title="Logical interface">LIF</abbr> storage interfaces, on which migrated moons will be available, are added to the required zone on the switches.  <a href="https://library.netapp.com/ecmdocs/ECMP1636036/html/frameset.html">It is recommended to</a> immediately register all possible <abbr title="World Wide Port Name">WWPNs of</abbr> all the nodes of the cluster into the corresponding zones, and the <abbr title="Selective lun mapping">SLM</abbr> mechanism will take care that the ways to the moon are ‚Äúnot adequately large‚Äù.  <a href="http://blog.aboutnetapp.ru/archives/1416">More details</a> . <br><br><h5>  <a href="https://habr.com/ru/post/243045/">Online Migration and SLM</a> </h5><a name="SLM-with-online-migration"></a><br>  In the case of online migration, it is necessary, on the nodes that accept the migrating moon, <a href="https://library.netapp.com/ecmdocs/ECMP1636035/html/GUID-62ABF745-6017-40B0-9D65-CE9F7FF66AB3.html">to allow the</a> hosts <a href="https://library.netapp.com/ecmdocs/ECMP1636035/html/GUID-62ABF745-6017-40B0-9D65-CE9F7FF66AB3.html">to be told to</a> the multipathing drivers that the moon is now accessible via new, additional ways. <br><br><h5>  <a href="https://habr.com/ru/post/243045/">Zoning</a> </h5><a name="zoning"></a><br>  Learn more about <a href="https://kb.netapp.com/support/index%3Fpage%3Dcontent%26id%3D1012699%26impressions%3Dfalse">NetApp + VMWare</a> troubleshooting <a href="https://kb.netapp.com/support/index%3Fpage%3Dcontent%26id%3D1012699%26impressions%3Dfalse">with SAN</a> .  Learn more about <a href="http://habrahabr.ru/post/260107/">zoning</a> recommendations <a href="http://habrahabr.ru/post/260107/">for NetApp in pictures</a> . <br><br><h4>  <a href="https://habr.com/ru/post/243045/">Ethernet</a> </h4><a name="Ethernet"></a><br>  Starting with Data Ontap 8.2.1, <abbr title="Data center bridging">DCB</abbr> (Lossless) Ethernet is supported on all converged ( <abbr title="Converged Network Adapter">CNA</abbr> / <abbr title="Unified Target Adapter">UTA</abbr> ) ports of NetApp <abbr title="Fabric attached storage">FAS</abbr> storage.  Recommendations <a href="http://habrahabr.ru/post/243119/">for setting up an Ethernet network</a> . <br><br><h6>  <a href="https://habr.com/ru/post/243045/">Thin provisioning</a> </h6><a name="Thin_Provitioning"></a><br>  In the case of the use of the "file" protocols <abbr title="Network file system">NFS</abbr> and <abbr title="Common Internet File System">CIFS, it is</abbr> very easy to take advantage of the use of the technology of Thin Provitioning, returning the freed up space inside the file balls.  But in the case of <abbr title="Storage area network">SAN, the</abbr> use of ThinProvitioning leads to the need for constant control over free space plus free space release ( <a href="http://habrahabr.ru/post/224869/">SCSI-3 mechanism is available for modern OS</a> and is available in Data Ontap starting from version 8.1.3) not ‚Äúinside‚Äù the same <abbr title="Logical unit number">LUN</abbr> , And as if inside the Volume containing this <abbr title="Logical unit number">LUN</abbr> . <br><br><h5>  <a href="https://habr.com/ru/post/243045/">Nfs</a> </h5><a name="NFS"></a><br>  In the case when there are 10 <abbr title="Gigabit ethernet">GBE</abbr> connections and Oracle <abbr title="Database">databases</abbr> , it is highly recommended to consider connectivity using the <abbr title="Direct network file system">dNFS</abbr> protocol, as according to NetApp internal tests (for other <abbr title="Storage System">storage</abbr> vendors this situation may differ), performance and latency are the same or slightly better than <abbr title="Fiber channel">FC</abbr> 8G. <abbr title="Online transaction processing">OLTP</abbr> load.  <abbr title="Network file system">NFS</abbr> is also very convenient for virtualization, when there is one big datastor with all virtual machines that all hosts ‚Äúsee‚Äù, facilitating migration between hosts, easier maintenance and setting up the network infrastructure, unlike zoning in <abbr title="Storage area network">SAN</abbr> networks. <br>  It is also very convenient to create <i>thick</i> virtual machine disks (following the best VMWare practices for high-loaded environments) while having Thin (from the <abbr title="Storage System">storage</abbr> point of view) datastor <abbr title="Network file system">NFS</abbr> given at the same time having both performance and economy, and not a compromise between the two.  Using one datastor allows you to more rationally allocate free space, giving it to those virtual machines that need it more, taking space savings to a new level, rather than rigidly fixing ‚Äúhow many‚Äù, as it happens in <abbr title="Fiber channel">FC</abbr> and <abbr title="iSCSI">iSCSI</abbr> .  At the same time, the free space freed for example by block deduplication or relocation "returns" this space and can be used by the same <abbr title="Network file system">NFS</abbr> / <abbr title="Common Internet File System">CIFS</abbr> ball. <br><br><h5>  <a href="https://habr.com/ru/post/243045/">Showmount</a> </h5><a name="Showmount"></a><br>  The Linux <i>showmount -e</i> command is supported starting with cDOT 8.3.  in order for the showmount -e command to discover NFS, you need to enable this feature on the storage side: <br><pre><code class="bash hljs">nfs server modify -vserver * -showmount enabled</code> </pre> <br><br><h5>  <a href="https://habr.com/ru/post/243045/">iSCSI</a> </h5><a name="iSCSI"></a><br>  When using MS Windows, it is possible to use <abbr title="iSCSI">iSCSI</abbr> <abbr title="Multi-Connection Sessions">MCS</abbr> allowing to use several <abbr title="Transmission Control Protocol">TCP</abbr> connections within one <abbr title="iSCSI">iSCSI</abbr> session.  The combination of <abbr title="Multipath Input-Output">MPIO</abbr> with <abbr title="Multi-Connection Sessions">MCS</abbr> can give a significant performance boost. <br><br>  Throughput in MB / s with different number of sessions / <abbr title="Transmission Control Protocol">TCP</abbr> connections for <abbr title="Multi-Connection Sessions">MCS</abbr> / <abbr title="Multipath Input-Output">MPIO</abbr> and 10 <abbr title="Gigabit ethernet">GBE</abbr> for Windows 2008 R2 and <abbr title="Fabric-attached storage">FAS</abbr> 3070 <br><table><tbody><tr><td>  <b>Number of connections</b> </td><td>  <b>SQLIO NTFS MCS</b> </td><td>  <b>SQLIO NTFS MPIO</b> </td></tr><tr><td>  one </td><td>  481 </td><td>  481 </td></tr><tr><td>  2 </td><td>  796 </td><td>  714 </td></tr><tr><td>  3 </td><td>  893 </td><td>  792 </td></tr><tr><td>  four </td><td>  <b>1012</b> </td><td>  890 </td></tr><tr><td>  five </td><td>  944 </td><td>  852 </td></tr><tr><td>  6 </td><td>  906 </td><td>  902 </td></tr><tr><td>  7 </td><td>  945 </td><td>  <b>961</b> </td></tr><tr><td>  eight </td><td>  903 </td><td>  911 </td></tr></tbody></table><br><br><h5>  <a href="https://habr.com/ru/post/243045/">CIFS</a> </h5><a name="CIFS"></a><br>  The new version of <abbr title="Common Internet File System">CIFS</abbr> (SMB) 3.0 allows you to use this protocol not only for the purposes of the file garbage bin, but also reveals new features for use on MS SQL databases and virtual machines with MS Hyper-V. <br>  Continuous availability shares (CA) extend protocol capabilities.  In previous versions, clients were forced to reconnect to the repository in case of fail <i>-over</i> or moving the <abbr title="Logical interface">LIF</abbr> to another port.  Now with the <abbr title="Continuous availability shares">CA</abbr> mechanism, the <a href="http://community.netapp.com/t5/Tech-OnTap-Articles/What-s-New-in-Clustered-Data-ONTAP-8-2/ta-p/85828">files will be available without interrupting the service</a> for a short time without connection during a fail-over or <abbr title="Logical interface">LIF</abbr> move. <br><br><h6>  <a href="https://habr.com/ru/post/243045/">VMWare VAAI &amp; NFS</a> </h6><a name="VAAI_NFS"></a>  NetApp supports <abbr title="vStorage API for Array Integration">VAAI</abbr> primitives for VMWare ESXi with the <abbr title="Network file system">NFS</abbr> protocol, allowing you to download "routine tasks" from the host to the <abbr title="Storage System">storage system</abbr> .  Requires installation of <abbr title="Network file system">NFS</abbr> <abbr title="vStorage API for Array Integration">VAAI</abbr> plug-in for each ESXi host (no plug-in required for <abbr title="Fiber channel">FC</abbr> ). <br><br><h5>  <a href="https://habr.com/ru/post/243045/">CPU storage</a> </h5><a name="CPU"></a><br>  For the most optimal utilization of all cores of the storage CPUs, use no less than 8 volums, this will improve parallelization, their utilization and, as a result, operation speed. <br><br><h5>  <a href="https://habr.com/ru/post/243045/">PerfStat</a> </h5><a name="PerfStat"></a><br>  The <a href="http://support.netapp.com/eservice/toolchest%3Ftoolid%3D433">perfstat</a> utility <a href="http://support.netapp.com/eservice/toolchest%3Ftoolid%3D433">that</a> collects the load on the <abbr title="Storage System">storage</abbr> in a text file.  It can also simultaneously collect information on the load from the host on which the utility is running.  Download from the site <a href="http://support.netapp.com/eservice/toolchest">support.netapp.com</a> section Download - Utility ToolChest (You need a NetApp NOW ID to enter).  It is very important to collect statistics from the <abbr title="Storage System">storage system</abbr> at the moment of load, as it often happens that the host is not able to provide sufficient load on the <abbr title="Storage System">storage system</abbr> , as can be seen from the perfstat log, in this case you will need to use several hosts. <br><br><pre> <code class="dos hljs">perfstat7.exe -f <span class="hljs-number"><span class="hljs-number">192</span></span>.<span class="hljs-number"><span class="hljs-number">168</span></span>.<span class="hljs-number"><span class="hljs-number">0</span></span>.<span class="hljs-number"><span class="hljs-number">10</span></span>,<span class="hljs-number"><span class="hljs-number">192</span></span>.<span class="hljs-number"><span class="hljs-number">168</span></span>.<span class="hljs-number"><span class="hljs-number">0</span></span>.<span class="hljs-number"><span class="hljs-number">12</span></span> -t <span class="hljs-number"><span class="hljs-number">2</span></span> -i <span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span> -l root -S pw:<span class="hljs-number"><span class="hljs-number">123456</span></span> -F -I -w <span class="hljs-number"><span class="hljs-number">1</span></span> &gt;"<span class="hljs-number"><span class="hljs-number">20140114</span></span>_1311.out"</code> </pre><br>  Using this utility, you can track the total load on the <abbr title="Storage System">storage system</abbr> from all hosts, including problems on the <abbr title="Storage System">storage</abbr> side, for example, when a <a href="http://blog.aboutnetapp.ru/archives/841">damaged disk slows down the entire system on the Back-End</a> and, as a result, on the <i>Front-End</i> . <br><br><h5>  <a href="https://habr.com/ru/post/243045/">Perfstat output interpretation</a> </h5><a name="perfstat_interpretation"></a><br>  By comparing the test results from the host and the <abbr title="Storage System">storage system,</abbr> you can find a bottleneck in the system under test.  So, following the output of the <i>perfstat command</i> below, it is clear that the <abbr title="Storage System">storage system is</abbr> not sufficiently loaded ( <abbr title="Central processing unit">CPU</abbr> , Disk Usage and Total parameters) using <abbr title="Fiber channel">FC</abbr> .  Most of the operations on this system are done in the form of reading the contents of the cache (the Cache hit value).  From what we conclude, the host cannot load the storage system enough. <br><br><div class="spoiler">  <b class="spoiler_title">perfstat.out</b> <div class="spoiler_text">  <a href="http://rghost.ru/54153112">Open text file</a> . <br><pre> <code class="bash hljs">=-=-=-=-=-= PERF 192.168.0.12 POSTSTATS =-=-=-=-=-= Begin: Wed Jan 15 07:16:45 GMT 2014 CPU NFS CIFS HTTP Total Net kB/s Disk kB/s Tape kB/s Cache Cache CP CP Disk OTHER FCP iSCSI FCP kB/s iSCSI kB/s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> out <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> write <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> write age hit time ty util <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> out <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> out 8% 0 0 0 227 1 1 2307 24174 0 0 8 89% 11% : 22% 3 224 0 12401 852 0 0 12% 0 0 0 191 1 5 3052 24026 0 0 6s 90% 14% F 27% 1 190 0 21485 864 0 0 18% 0 0 0 517 0 1 5303 46467 0 0 0s 90% 33% 2 38% 1 516 0 23630 880 0 0 15% 0 0 0 314 1 2 4667 24287 0 0 0s 91% 15% F 33% 27 287 0 27759 853 0 0 12% 0 0 0 252 0 1 3425 24601 0 0 9 91% 16% F 28% 20 232 0 22280 852 0 0 24% 0 0 0 1472 2 5 9386 46919 0 0 1s 82% 34% F 47% 9 1463 0 26141 673 0 0 14% 0 0 0 303 1 3 3970 24527 0 0 8s 90% 27% F 33% 1 302 0 22810 967 0 0 14% 0 0 0 299 2 6 3862 24776 0 0 0s 91% 21% F 29% 1 298 0 21981 746 0 0 13% 0 0 0 237 1 3 4608 24348 0 0 9 94% 15% F 30% 1 236 0 22721 958 0 0 17% 0 0 0 306 1 2 5603 48072 0 0 2s 92% 32% F 37% 1 305 0 22232 792 0 0 13% 0 0 0 246 0 1 3208 24278 0 0 8s 92% 14% F 26% 20 226 0 24137 598 0 0 -- Summary Statistics ( 11 samples 1.0 secs/sample) CPU NFS CIFS HTTP Total Net kB/s Disk kB/s Tape kB/s Cache Cache CP CP Disk OTHER FCP iSCSI FCP kB/s iSCSI kB/s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> out <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> write <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> write age hit time ty util <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> out <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> out Min 8% 0 0 0 191 0 1 2307 24026 0 0 0s 82% 11% * 22% 0 190 0 0 0 0 0 Avg 14% 0 0 0 396 0 2 4490 30588 0 0 2 90% 21% * 31% 7 389 0 22507 821 0 0 Max 24% 0 0 0 1472 2 6 9386 48072 0 0 9 94% 34% * 47% 27 1463 0 27759 967 0 0 Begin: Wed Jan 15 07:18:36 GMT 2014</code> </pre><br></div></div><br><br>  Decryption parameter <a href="http://blog.aboutnetapp.ru/archives/199">Cache age, Cache hit, CP ty</a> .  A little more about optimizing performance and finding a <a href="http://blog.aboutnetapp.ru/archives/839">bottleneck</a> . <br><br><h6>  <a href="https://habr.com/ru/post/243045/">File system</a> </h6><a name="FS"></a><br>  Host <abbr title="File system">FS</abbr> can make significant adjustments when testing performance. <br>  The size of the <abbr title="File system">FS</abbr> block should be a multiple of 4KB.  For example, if we run a synthetic load similar to the generated <abbr title="Online transaction processing">OLTP</abbr> , where the size of the operated block is on average 8KB, then we put 8KB.  I also want to note that as a <abbr title="File system">file system</abbr> itself, its implementation for a specific <abbr title="Operating system">OS</abbr> and version can greatly influence the overall performance picture.  So for when writing 10 MB blocks in 100 streams with the dd file command from the <abbr title="Database">database</abbr> on the UFS <abbr title="File system">file system</abbr> located on the <abbr title="Logical unit number">LUN</abbr> , the <abbr title="Fiber channel">FC</abbr> 4G with <abbr title="Fabric-attached storage">FAS</abbr> 2240 <abbr title="Database">data</abbr> <abbr title="Storage System">storage</abbr> and 21 + 2 <abbr title="Serial attached SCSI">SAS</abbr> 600 10k disks in one unit showed a speed of 150 MB / s, while however, the <abbr title="Zettabyte File System">ZFS</abbr> configuration showed two times more <abbr title="File system">FS</abbr> (approaching the theoretical maximum of the network channel), and the Noatime parameter didn‚Äôt affect the situation at all. <br><br><h6>  <a href="https://habr.com/ru/post/243045/">Noatime for file balloon on storage</a> </h6><a name="Noatime_on_storage"></a><br>  Also, the same parameter must be set on the <abbr title="Storage System">storage</abbr> partition with the data that is being accessed.  Enabling this option prevents updating the file access time in <abbr title="Write Anywhere File Layout">WAFL</abbr> inodes.  Thus, the following command is applicable for <abbr title="Server message block">SMB</abbr> <abbr title="Common Internet File System">CIFS</abbr> / <abbr title="Network file system">NFS</abbr> file <abbr title="Network file system">balls</abbr> . <br><br><pre> <code class="bash hljs">vol options vol1 no_atime_update on</code> </pre><br><br><h6>  <a href="https://habr.com/ru/post/243045/">Misalignment</a> </h6><a name="Misalignment"></a><br>  For any <abbr title="Operating system">OS,</abbr> you need to select the correct geometry when creating a <abbr title="Logical unit number">LUN</abbr> .  In the case of an incorrectly specified size of the <abbr title="File system">FS</abbr> block, an incorrectly specified <abbr title="Logical unit number">LUN</abbr> geometry, the <abbr title="Master boot record">MBR</abbr> / <abbr title="GUID Partition Table">GPT</abbr> parameter that is not correctly selected on the host, we will observe, in peak loads, messages in the console about a certain " <abbr title="Logical unit number">LUN</abbr> misalignment" event.  Sometimes these messages may appear erroneously, in the case of their rare appearance just ignore them.  You can check this by running the <i>lun stats</i> command on the storage system, below is an example of output, pay attention to align_histo.1: 100% and write_align_histo.1: 97%, so it should not be in a normally configured system - the block should start with "0" in the parameters "Align_histo". <br><br><div class="spoiler">  <b class="spoiler_title">TR-3593</b> <div class="spoiler_text">  There are no alignment.  Using perfstat counters, under the wafl_susp section, ‚Äúwp.partial_writes‚Äú, ‚Äúpw.over_limit‚Äú, and ‚Äúpw.async_read‚Äú are indicators of improper alignment.  The ‚Äúwp.partial write‚Äú is the block counter of unaligned I / O.  WAFL will launch a background read.  These are counted in ‚Äúpw.async_read‚Äú;  "Pw.over_limit" <br>  Using Data ONTAP 7.2.1 or newer, I / O alignment: <br><ul><li>  luna: read_align_histo: 8b WAFL block an I / O was.  Reported as a% of reads. </li><li>  lun: write_align_histo: Same for writes. </li><li>  lun: read_partial_blocks:% reads that are not a multiple of 4k. </li><li>  lun: write_partial_blocks: same for writes. </li></ul><br>  Read / write_align_histo [0] is nonzero, you had some misaligned I / O. <br></div></div><br><div class="spoiler">  <b class="spoiler_title">lun stats for 7-Mode</b> <div class="spoiler_text"><pre> <code class="bash hljs">priv <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> -q advanced; lun show -v lun status lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:display_name:/vol/vol0/drew_smi lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:read_ops:1/s lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:write_ops:26/s lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:other_ops:0/s lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:read_data:10758b/s lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:write_data:21997484b/s lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:queue_full:0/s lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:avg_latency:290.19ms lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:total_ops:27/s lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:scsi_partner_ops:0/s lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:scsi_partner_data:0b/s lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:read_align_histo.0:0% lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:read_align_histo.1:100% lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:read_align_histo.2:0% lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:read_align_histo.3:0% lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:read_align_histo.4:0% lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:read_align_histo.5:0% lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:read_align_histo.6:0% lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:read_align_histo.7:0% lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:write_align_histo.0:0% lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:write_align_histo.1:97% lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:write_align_histo.2:0% lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:write_align_histo.3:1% lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:write_align_histo.4:0% lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:write_align_histo.5:0% lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:write_align_histo.6:0% lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:write_align_histo.7:0% lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:read_partial_blocks:0% lun:/vol/vol0/drew_smi-W9mAeJb3vzGS:write_partial_blocks:1%</code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">lun show for Cluster-Mode</b> <div class="spoiler_text"><pre> <code class="bash hljs">cl1::*&gt; <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> -privilege advanced; lun show -vserver &lt;your_vserver&gt; -path &lt;your_path_to_the_lun&gt; Vserver Name: vs_infra LUN Path: /vol/vol_infra_10/qtree_1/lun_10 Volume Name: vol_infra_10 Qtree Name: qtree_1 LUN Name: lun_10 LUN Size: 500.1GB Prefix Size: 0 Extent Size: 0 Suffix Size: 0 OS Type: vmware Space Reservation: disabled Serial Number: 804j6]FOQ3Ls Comment: Space Reservations Honored: <span class="hljs-literal"><span class="hljs-literal">true</span></span> Space Allocation: disabled State: online LUN UUID: 473e9853-cc39-4eab-a1ef-88c43f42f9dc Mapped: mapped Vdisk ID: 80000402000000000000000000006a6a0335be76 Block Size: 512 Device Legacy ID: - Device Binary ID: - Device Text ID: - Read Only: <span class="hljs-literal"><span class="hljs-literal">false</span></span> Inaccessible Due to Restore: <span class="hljs-literal"><span class="hljs-literal">false</span></span> Inconsistent Filesystem: <span class="hljs-literal"><span class="hljs-literal">false</span></span> Inconsistent Blocks: <span class="hljs-literal"><span class="hljs-literal">false</span></span> NVFAIL: <span class="hljs-literal"><span class="hljs-literal">false</span></span> Alignment: partial-writes Used Size: 89.44GB Maximum Resize Size: 4.94TB Creation Time: 10/31/2014 20:40:10 Class: regular Clone: <span class="hljs-literal"><span class="hljs-literal">false</span></span> Clone Autodelete Enabled: <span class="hljs-literal"><span class="hljs-literal">false</span></span> Has Metadata Provisioned: <span class="hljs-literal"><span class="hljs-literal">true</span></span> QoS Policy Group: - cl1::*&gt; <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> -privilege diagnostic; lun alignment show -vserver vs_infra -path * Vserver Name: vs_infra LUN Path: /vol/vol_infra_10/qtree_1/lun_10 OS Type: vmware Alignment: partial-writes Write Alignment Histogram: 62, 0, 0, 0, 0, 0, 0, 0 Read Alignment Histogram: 89, 0, 2, 0, 0, 0, 0, 0 Write Partial Blocks: 36 Read partial Blocks: 4 Vserver Name: vs_infra LUN Path: /vol/vol_0_a/lun_03032015_095833 OS Type: windows_2008 Alignment: aligned Write Alignment Histogram: 99, 0, 0, 0, 0, 0, 0, 0 Read Alignment Histogram: 99, 0, 0, 0, 0, 0, 0, 0 Write Partial Blocks: 0 Read partial Blocks: 0 Vserver Name: vs_infra LUN Path: /vol/vol_1_b/lun_03052015_095855 OS <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>: linux Alignment: misaligned Write alignment histogram percentage: 0, 0, 0, 0, 0, 0, 0, 100 Read alignment histogram percentage: 0, 0, 0, 0, 0, 0, 0, 100 Partial writes percentage: 0 Partial reads percentage: 0 Vserver Name: vs_infra LUN Path: /vol/vol_infra_20/qtree_2/lun_20 OS Type: linux Alignment: indeterminate Write Alignment Histogram: 0, 0, 0, 0, 0, 0, 0, 0 Read Alignment Histogram: 100, 0, 0, 0, 0, 0, 0, 0 Write Partial Blocks: 0 Read partial Blocks: 0</code> </pre><br></div></div><br>  misalignment can also occur in a virtualization environment, on ‚Äúshared datastors‚Äù with <abbr title="Virtual Machine File System">VMFS</abbr> , which can lead to degradation of system performance.  In this case, there are two options for resolving the problem of an existing virtual machine ‚Äî through the NetApp <abbr title="Virtual Storage Console">VSC</abbr> plugin for vCenter, the virtual machine (Storage vMoution) is migrated to a new datastore (with correctly set <abbr title="Logical unit number">LUN</abbr> geometry) or manually using the MBRalign command line utility to align the block boundaries in <i>vmdk</i> file.  I want to note that the manual mode will most likely make <abbr title="Operating system">the</abbr> virtual machine <abbr title="Operating system">OS</abbr> loader not working and will need to be restored.  See the <a href="http://www.netapp.com/us/system/pdf-reader.aspx%3Fpdfuri%3Dtcm:10-61563-16%26m%3Dtr-3747.pdf">Best Practices for File System alignment in Virtual Environments</a> document <a href="http://www.netapp.com/us/system/pdf-reader.aspx%3Fpdfuri%3Dtcm:10-61563-16%26m%3Dtr-3747.pdf">for</a> more details.  Learn more <a href="https://kb.netapp.com/support/index%3Fpage%3Dcontent%26id%3D3011193%26actp%3Dsearch%26viewlocale%3Den_US%26searchid%3D1419848156220">about what I / O Misalignment is</a> and <a href="https://kb.netapp.com/support/index%3Fpage%3Dcontent%26id%3D1010881">how to fix it</a> . <br><br><h4>  <a href="https://habr.com/ru/post/243045/">Reallocation</a> </h4><a name="Reallocation"></a><br>  It is imperative to check that all data is smeared across all disks, for example after <a href="http://blog.aboutnetapp.ru/archives/1294">adding disks to an already created unit</a> .  To check the state of "blurring" of the data on the disks on disks, run on 7-Mode: <br><pre> <code class="bash hljs">reallocate measure ‚Äìo /vol/&lt;volume&gt;</code> </pre><br>  on Custered ONTAP: <br><pre> <code class="bash hljs">reallocate measure -vserver &lt;vserver&gt; -path /vol/&lt;volume&gt; -once <span class="hljs-literal"><span class="hljs-literal">true</span></span> -threshold 3</code> </pre><br>  Let's see an example of the output of such a command: <br><pre> <code class="bash hljs">Sat Mar 1 20:26:16 EET [fas<span class="hljs-_"><span class="hljs-_">-a</span></span>:wafl.reallocate.check.highAdvise:info]: Allocation check on <span class="hljs-string"><span class="hljs-string">'/vol/vol_vol01'</span></span> is 12, hotspot 5 (threshold 4), consider running reallocate.</code> </pre><br>  A value of 12, greater than threshold 4, indicates that a relocation must be started on the volume.  The hotspot 5 value indicates that the hot blocks are located on 5 disks, and not on all disks of the unit (if it is not eliminated by simple realocation, look <a href="https://habr.com/ru/post/243045/">here</a> ). <br>  <a href="https://library.netapp.com/ecmdocs/ECMP1155684/html/GUID-E1789834-11E5-4748-BFEA-12C699DAF242.html">Insufficient free space and the presence of snapshots may not allow realization</a> .  On a volume, you need to have not less than 5% free space in the active file system and 10% free space in SnapReserve. <br>  In the case of adding new disks, it makes sense to unbalance not only data but also empty blocks. <br><br>  Run the physical relocation of the volume data to optimize performance on 7-Mode <br><pre> <code class="bash hljs">aggr options aggr0 free_space_realloc on reallocate start -f -p /vol/&lt;volume&gt;</code> </pre><br>  On Clustered ONTAP.  For Clustered ONTAP, the <i>reallocate</i> command <i>will</i> not return the output, you need to request it: <br><pre> <code class="bash hljs">storage aggregate modify -free-space-realloc on reallocate start -vserver &lt;vserver&gt; -path /vol/&lt;volume&gt; -force <span class="hljs-literal"><span class="hljs-literal">true</span></span> -threshold 3 -space-optimized <span class="hljs-literal"><span class="hljs-literal">true</span></span> event <span class="hljs-built_in"><span class="hljs-built_in">log</span></span> show -messagename wafl.reallocate* -severity *</code> </pre><br><div class="spoiler">  <b class="spoiler_title">Uniformly distributed load on disks after relocation:</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/bc1/ce7/396/bc1ce739688d4099bb5dde8d9d5ee28e.png"><br></div></div><br><br><h4>  <a href="https://habr.com/ru/post/243045/">Check disk storage</a> </h4><a name="backend_storage_disks_performance"></a><br><br>  During the test it is very important to make sure that one of the disks did not die and / or the whole unit does not slow down. <br><br><div class="spoiler">  <b class="spoiler_title">statit for 7-Mode</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#  statit -b #     statit -e disk ut% xfers ureads--chain-usecs writes--chain-usecs cpreads-chain-usecs greads--chain-usecs gwrites-chain-usecs /aggr0/plex0/rg0: 01.0a.16 9 3.71 0.47 1.00 90842 2.94 15.14 1052 0.30 7.17 442 0.00 .... . 0.00 .... . 01.0b.17 11 3.86 0.47 1.00 126105 3.14 14.31 1170 0.25 2.20 1045 0.00 .... . 0.00 .... . 01.0a.18 35 35.52 33.62 1.24 14841 1.63 26.23 965 0.27 15.09 392 0.00 .... . 0.00 .... . 01.0b.25 78 35.15 33.47 1.13 64924 1.48 28.77 2195 0.20 16.75 1493 0.00 .... . 0.00 .... . 01.0a.24 34 33.96 32.26 1.13 17318 1.51 28.21 1007 0.20 17.00 257 0.00 .... . 0.00 .... . 01.0b.22 36 35.40 33.67 1.15 16802 1.51 28.25 1003 0.22 15.56 721 0.00 .... . 0.00 .... . 01.0a.21 35 34.98 33.27 1.16 17126 1.48 28.75 950 0.22 14.78 820 0.00 .... . 0.00 .... . 01.0b.28 77 34.93 33.02 1.13 66383 1.56 27.40 3447 0.35 10.21 8392 0.00 .... . 0.00 .... . 01.0a.23 32 33.02 31.12 1.17 14775 1.53 27.65 1018 0.37 10.80 1321 0.00 .... . 0.00 .... . 01.0b.20 35 34.41 32.38 1.29 15053 1.66 25.73 976 0.37 9.67 1076 0.00 .... . 0.00 .... . 01.0a.19 34 34.80 33.07 1.20 15961 1.51 28.30 930 0.22 15.00 681 0.00 .... . 0.00 .... . 01.0b.26 76 34.41 32.41 1.05 68532 1.63 26.09 3482 0.37 11.93 7698 0.00 .... . 0.00 .... . 01.0a.27 36 35.15 33.32 1.26 15327 1.56 27.35 1018 0.27 12.82 1170 0.00 .... . 0.00 .... . /aggr0/plex0/rg1: 02.0b.29 5 2.00 0.00 .... . 1.63 27.89 1023 0.37 9.80 231 0.00 .... . 0.00 .... . 02.0a.33 5 2.03 0.00 .... . 1.68 27.13 1095 0.35 8.21 330 0.00 .... . 0.00 .... . 02.0b.34 32 34.46 32.75 1.19 14272 1.51 29.87 927 0.20 16.63 617 0.00 .... . 0.00 .... . 02.0a.35 31 32.85 31.00 1.15 14457 1.51 29.87 895 0.35 12.36 1075 0.00 .... . 0.00 .... . 02.0b.41 32 33.10 31.44 1.20 13396 1.51 29.87 930 0.15 21.83 618 0.00 .... . 0.00 .... . 02.0a.43 31 32.73 30.92 1.19 13827 1.58 28.47 1005 0.22 15.22 920 0.00 .... . 0.00 .... . 02.0b.44 31 32.65 31.02 1.11 14986 1.51 29.85 913 0.12 26.00 408 0.00 .... . 0.00 .... . 02.0a.32 31 32.68 30.87 1.13 14437 1.58 28.48 956 0.22 15.78 627 0.00 .... . 0.00 .... . 02.0b.36 32 34.70 32.95 1.13 14680 1.56 28.94 975 0.20 16.75 582 0.00 .... . 0.00 .... . 02.0a.37 31 32.43 30.70 1.21 13836 1.51 29.89 929 0.22 14.78 797 0.00 .... . 0.00 .... .</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">statit / statistics for Cluster-Mode</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#    storage disk show -broken #  run -node * -command "priv set diag; statit -b" #     run -node * -command "priv set diag; statit -e" disk ut% xfers ureads--chain-usecs writes--chain-usecs cpreads-chain-usecs greads--chain-usecs gwrites-chain-usecs /aggr0/plex0/rg0: 01.0a.16 9 3.71 0.47 1.00 90842 2.94 15.14 1052 0.30 7.17 442 0.00 .... . 0.00 .... . 01.0b.17 11 3.86 0.47 1.00 126105 3.14 14.31 1170 0.25 2.20 1045 0.00 .... . 0.00 .... . 01.0a.18 35 35.52 33.62 1.24 14841 1.63 26.23 965 0.27 15.09 392 0.00 .... . 0.00 .... . 01.0b.25 78 35.15 33.47 1.13 64924 1.48 28.77 2195 0.20 16.75 1493 0.00 .... . 0.00 .... . 01.0a.24 34 33.96 32.26 1.13 17318 1.51 28.21 1007 0.20 17.00 257 0.00 .... . 0.00 .... . 01.0b.22 36 35.40 33.67 1.15 16802 1.51 28.25 1003 0.22 15.56 721 0.00 .... . 0.00 .... . 01.0a.21 35 34.98 33.27 1.16 17126 1.48 28.75 950 0.22 14.78 820 0.00 .... . 0.00 .... . 01.0b.28 77 34.93 33.02 1.13 66383 1.56 27.40 3447 0.35 10.21 8392 0.00 .... . 0.00 .... . 01.0a.23 32 33.02 31.12 1.17 14775 1.53 27.65 1018 0.37 10.80 1321 0.00 .... . 0.00 .... . 01.0b.20 35 34.41 32.38 1.29 15053 1.66 25.73 976 0.37 9.67 1076 0.00 .... . 0.00 .... . 01.0a.19 34 34.80 33.07 1.20 15961 1.51 28.30 930 0.22 15.00 681 0.00 .... . 0.00 .... . 01.0b.26 76 34.41 32.41 1.05 68532 1.63 26.09 3482 0.37 11.93 7698 0.00 .... . 0.00 .... . 01.0a.27 36 35.15 33.32 1.26 15327 1.56 27.35 1018 0.27 12.82 1170 0.00 .... . 0.00 .... . /aggr0/plex0/rg1: 02.0b.29 5 2.00 0.00 .... . 1.63 27.89 1023 0.37 9.80 231 0.00 .... . 0.00 .... . 02.0a.33 5 2.03 0.00 .... . 1.68 27.13 1095 0.35 8.21 330 0.00 .... . 0.00 .... . 02.0b.34 32 34.46 32.75 1.19 14272 1.51 29.87 927 0.20 16.63 617 0.00 .... . 0.00 .... . 02.0a.35 31 32.85 31.00 1.15 14457 1.51 29.87 895 0.35 12.36 1075 0.00 .... . 0.00 .... . 02.0b.41 32 33.10 31.44 1.20 13396 1.51 29.87 930 0.15 21.83 618 0.00 .... . 0.00 .... . 02.0a.43 31 32.73 30.92 1.19 13827 1.58 28.47 1005 0.22 15.22 920 0.00 .... . 0.00 .... . 02.0b.44 31 32.65 31.02 1.11 14986 1.51 29.85 913 0.12 26.00 408 0.00 .... . 0.00 .... . 02.0a.32 31 32.68 30.87 1.13 14437 1.58 28.48 956 0.22 15.78 627 0.00 .... . 0.00 .... . 02.0b.36 32 34.70 32.95 1.13 14680 1.56 28.94 975 0.20 16.75 582 0.00 .... . 0.00 .... . 02.0a.37 31 32.43 30.70 1.21 13836 1.51 29.89 929 0.22 14.78 797 0.00 .... . 0.00 .... .</span></span></code> </pre><br>  or using the <i>statistics</i> command: <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">set</span></span> -privilege advanced ; statistics disk show -interval 5 -iterations 1 cl03 : 3/5/2015 10:30:29 Busy *Total Read Write Read Write Latency Disk Node (%) Ops Ops Ops (Bps) (Bps) (us) ----- --------------- ---- ------ ---- ----- ----- ----- ------- 1.0.4 Multiple_Values 1 5 2 3 31744 69632 6000 1.0.2 Multiple_Values 1 4 2 2 31744 65536 5263 1.0.8 Multiple_Values 0 2 1 1 62464 43008 5363 1.0.1 cl03-02 0 2 0 1 12288 7168 5500 1.0.0 cl03-02 0 2 0 1 12288 7168 5000 1.0.12 Multiple_Values 0 1 0 0 59392 53248 5000 1.0.10 Multiple_Values 0 1 0 0 60416 56320 6000 1.0.7 cl03-02 0 0 0 0 10240 0 0 1.0.6 cl03-02 0 0 0 0 11264 0 7500 1.0.5 cl03-02 0 0 0 0 10240 0 4000 1.0.3 cl03-02 0 0 0 0 10240 0 13000 1.0.17 cl03-02 0 0 0 0 10240 0 1000 1.0.16 cl03-01 0 0 0 0 10240 0 0 1.0.15 cl03-01 0 0 0 0 0 0 - 1.0.14 cl03-02 0 0 0 0 10240 0 0 1.0.11 cl03-02 0 0 0 0 10240 0 0</code> </pre><br></div></div><br>  We see that our aggregate consists of two RAID groups - <i>rg0</i> and <i>rg1</i> , in the configuration <i>RAID-DP 11d + 2p</i> .  Discs <i>0c.16, 1b.17</i> and <i>0c.29, 0c.33</i> are parity disks, the rest are Data. <br><br>  <a href="https://habr.com/ru/post/243045/">When data drives are not evenly loaded</a> <a name="Disk_load_mismatching"></a><br><br>  In the output of the <i><a href="https://habr.com/ru/post/243045/">statit / statistics</a></i> command, we are looking for weirdness.  The value of the use of disks <i>0s.25, 26 and 28</i> , compared with other data-disks (for parity disks, other rules apply, we do not look at them).  With an average load of <i>ut%</i> on disks of a group of 35%, on these disks the <u>load is almost twice as high</u> , about 75% plus their high latency values ‚Äã‚Äã( <i>ureads usecs</i> and <i>write usecs</i> ), compared to other disks, which reaches 60-70 milliseconds, against 14-17 for the rest.  In a normally operating aggregate, the load should be evenly distributed across all data disks in the aggregate.  It is necessary to reduce the load on the unit by relocation or to replace such discs. <br>  For 7-Mode: <pre> <code class="bash hljs">disk fail 0.25</code> </pre> <br>  For C-Mode: <pre> <code class="bash hljs">storage disk fail ‚Äìdisk 0.25</code> </pre> <br><br><div class="spoiler">  <b class="spoiler_title">Now check the damaged 7-Mode disks.</b> <div class="spoiler_text"><pre> <code class="bash hljs">aggr status -f sysconfig -d sysconfig -a disk show -v</code> </pre> <br><br>  We illuminate the LED indicator of the disk for its easy detection and subsequent removal <pre> <code class="bash hljs">led_on disk_name</code> </pre> <br><br>  We look at the chart in which shelf and position the disk is installed. <pre> <code class="bash hljs">sasadmin shelf</code> </pre> <br></div></div><br><div class="spoiler">  <b class="spoiler_title">Now, check for damaged Cluster-Mode disks.</b> <div class="spoiler_text"><pre> <code class="bash hljs">storage aggregate show -state failed run -node * -<span class="hljs-built_in"><span class="hljs-built_in">command</span></span> sysconfig -d run -node * -<span class="hljs-built_in"><span class="hljs-built_in">command</span></span> sysconfig -a disk show -pool * -fields disk,usable-size,shelf,<span class="hljs-built_in"><span class="hljs-built_in">type</span></span>,container-type,container-name,owner,pool disk show -prefailed <span class="hljs-literal"><span class="hljs-literal">true</span></span> disk show -broken</code> </pre><br><br>  We illuminate the LED indicator of the disk for its easy detection and subsequent removal <pre> <code class="bash hljs">storage disk <span class="hljs-built_in"><span class="hljs-built_in">set</span></span>-led -disk disk_name 2</code> </pre> <br><br>  We look at the chart in which shelf and position the disk is installed. <pre> <code class="bash hljs">run -node * -<span class="hljs-built_in"><span class="hljs-built_in">command</span></span> sasadmin shelf</code> </pre> <br></div></div><br><br><h6>  <a href="https://habr.com/ru/post/243045/">CPU load</a> </h6><a name="CPU_Load"></a><br>  Make sure there is no gag on the CPU or in the cache and check the load on the protocols: <br><div class="spoiler">  <b class="spoiler_title">sysstat for 7-Mode</b> <div class="spoiler_text">  Read more about <a href="http://blog.aboutnetapp.ru/archives/tag/sysstat">sysstat</a> <br><pre> <code class="bash hljs">7M&gt; sysstat -x 1 CPU NFS CIFS HTTP Total Net kB/s Disk kB/s Tape kB/s Cache Cache CP CP Disk FCP iSCSI FCP kB/s iSCSI kB/s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> out <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> write <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> write age hit time ty util <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> out <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> out 5% 0 726 0 726 2555 1371 2784 24 0 0 1 91% 0% - 89% 0 0 0 0 0 0 4% 0 755 0 755 1541 1136 3312 0 0 0 1 92% 0% - 89% 0 0 0 0 0 0 6% 0 1329 0 1334 3379 2069 3836 8 0 0 1 90% 0% - 79% 0 5 0 0 74 0 4% 0 637 0 637 2804 2179 3160 24 0 0 1 92% 0% - 86% 0 0 0 0 0 0 4% 0 587 0 587 2386 1241 2532 8 0 0 1 94% 0% - 98% 0 0 0 0 0 0 8% 0 381 0 381 2374 1063 5224 15120 0 0 6s 96% 45% Tf 78% 0 0 0 0 0 0 7% 0 473 0 473 2902 840 3020 20612 0 0 6s 98% 100% :f 100% 0 0 0 0 0 0 5% 0 1131 0 1133 3542 1371 2612 400 0 0 6s 92% 35% : 70% 0 2 0 0 20 0 7% 0 1746 0 1746 3874 1675 3572 0 0 0 6s 92% 0% - 79% 0 0 0 0 0 0 8% 0 2056 0 2056 5754 3006 4044 24 0 0 6s 95% 0% - 83% 0 0 0 0 0 0 6% 0 1527 0 1527 2912 2162 2360 0 0 0 6s 94% 0% - 86% 0 0 0 0 0 0 6% 0 1247 0 1265 3740 1341 2672 0 0 0 6s 94% 0% - 96% 0 18 0 0 98 0 6% 0 1215 0 1220 3250 1270 2676 32 0 0 6s 92% 0% - 86% 0 5 0 0 61 0 4% 0 850 0 850 1991 915 2260 0 0 0 6s 90% 0% - 75% 0 0 0 0 0 0 7% 0 1740 0 1740 3041 1246 2804 0 0 0 13s 92% 0% - 80% 0 0 0 0 0 0 3% 0 522 0 531 1726 1042 2340 24 0 0 16s 88% 0% - 69% 7 0 0 12 0 0 6% 0 783 0 804 5401 1456 3424 0 0 0 1 92% 0% - 89% 17 0 0 21 0 0 10% 0 478 0 503 4229 919 5840 13072 0 0 1 95% 65% Tf 98% 12 9 0 17 94 0 9% 0 473 0 487 3290 945 2720 23148 0 0 31s 97% 100% :f 100% 12 0 0 17 0 0 6% 0 602 0 606 3196 729 2380 12576 0 0 31s 97% 89% : 100% 0 0 0 0 0 0 10% 0 1291 0 1291 15950 3017 2680 0 0 0 31s 94% 0% - 100% 0 0 0 0 0 0 9% 0 977 0 977 13452 4553 4736 24 0 0 31s 96% 0% - 92% 0 0 0 0 0 0 6% 0 995 0 995 3923 2210 2356 8 0 0 31s 94% 0% - 85% 0 0 0 0 0 0 4% 0 575 0 583 1849 2948 3056 0 0 0 31s 93% 0% - 96% 0 8 0 0 111 0 5% 0 789 0 789 2316 742 2364 24 0 0 31s 94% 0% - 91% 0 0 0 0 0 0 4% 0 550 0 550 1604 1125 3004 0 0 0 31s 92% 0% - 80% 0 0 0 0 0 0 7% 0 1398 0 1398 2910 1358 2716 0 0 0 31s 94% 0% - 87% 0 0 0 0 0 0</code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">sysstat / statistics for Cluster-Mode</b> <div class="spoiler_text">  Read more about <a href="http://blog.aboutnetapp.ru/archives/tag/sysstat">sysstat</a> <br><pre> <code class="bash hljs">CM::*&gt; system node run -node <span class="hljs-built_in"><span class="hljs-built_in">local</span></span> -<span class="hljs-built_in"><span class="hljs-built_in">command</span></span> <span class="hljs-string"><span class="hljs-string">"priv set diag; sysstat -x 1"</span></span> CPU NFS CIFS HTTP Total Net kB/s Disk kB/s Tape kB/s Cache Cache CP CP Disk FCP iSCSI FCP kB/s iSCSI kB/s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> out <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> write <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> write age hit time ty util <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> out <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> out 5% 0 726 0 726 2555 1371 2784 24 0 0 1 91% 0% - 89% 0 0 0 0 0 0 4% 0 755 0 755 1541 1136 3312 0 0 0 1 92% 0% - 89% 0 0 0 0 0 0 6% 0 1329 0 1334 3379 2069 3836 8 0 0 1 90% 0% - 79% 0 5 0 0 74 0 4% 0 637 0 637 2804 2179 3160 24 0 0 1 92% 0% - 86% 0 0 0 0 0 0 4% 0 587 0 587 2386 1241 2532 8 0 0 1 94% 0% - 98% 0 0 0 0 0 0 8% 0 381 0 381 2374 1063 5224 15120 0 0 6s 96% 45% Tf 78% 0 0 0 0 0 0 7% 0 473 0 473 2902 840 3020 20612 0 0 6s 98% 100% :f 100% 0 0 0 0 0 0 5% 0 1131 0 1133 3542 1371 2612 400 0 0 6s 92% 35% : 70% 0 2 0 0 20 0 7% 0 1746 0 1746 3874 1675 3572 0 0 0 6s 92% 0% - 79% 0 0 0 0 0 0 8% 0 2056 0 2056 5754 3006 4044 24 0 0 6s 95% 0% - 83% 0 0 0 0 0 0 6% 0 1527 0 1527 2912 2162 2360 0 0 0 6s 94% 0% - 86% 0 0 0 0 0 0 6% 0 1247 0 1265 3740 1341 2672 0 0 0 6s 94% 0% - 96% 0 18 0 0 98 0 6% 0 1215 0 1220 3250 1270 2676 32 0 0 6s 92% 0% - 86% 0 5 0 0 61 0 4% 0 850 0 850 1991 915 2260 0 0 0 6s 90% 0% - 75% 0 0 0 0 0 0 7% 0 1740 0 1740 3041 1246 2804 0 0 0 13s 92% 0% - 80% 0 0 0 0 0 0 3% 0 522 0 531 1726 1042 2340 24 0 0 16s 88% 0% - 69% 7 0 0 12 0 0 6% 0 783 0 804 5401 1456 3424 0 0 0 1 92% 0% - 89% 17 0 0 21 0 0 10% 0 478 0 503 4229 919 5840 13072 0 0 1 95% 65% Tf 98% 12 9 0 17 94 0 9% 0 473 0 487 3290 945 2720 23148 0 0 31s 97% 100% :f 100% 12 0 0 17 0 0 6% 0 602 0 606 3196 729 2380 12576 0 0 31s 97% 89% : 100% 0 0 0 0 0 0 10% 0 1291 0 1291 15950 3017 2680 0 0 0 31s 94% 0% - 100% 0 0 0 0 0 0 9% 0 977 0 977 13452 4553 4736 24 0 0 31s 96% 0% - 92% 0 0 0 0 0 0 6% 0 995 0 995 3923 2210 2356 8 0 0 31s 94% 0% - 85% 0 0 0 0 0 0 4% 0 575 0 583 1849 2948 3056 0 0 0 31s 93% 0% - 96% 0 8 0 0 111 0 5% 0 789 0 789 2316 742 2364 24 0 0 31s 94% 0% - 91% 0 0 0 0 0 0 4% 0 550 0 550 1604 1125 3004 0 0 0 31s 92% 0% - 80% 0 0 0 0 0 0 7% 0 1398 0 1398 2910 1358 2716 0 0 0 31s 94% 0% - 87% 0 0 0 0 0 0</code> </pre><br>  Or with the command <br><pre> <code class="bash hljs">CM::*&gt; <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> -privilege advanced ; statistics show-periodic cpu cpu total fcache total total data data data cluster cluster cluster disk disk pkts pkts avg busy ops nfs-ops cifs-ops ops recv sent busy recv sent busy recv sent <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> write recv sent ---- ---- -------- -------- -------- -------- -------- -------- ---- -------- -------- ------- -------- -------- -------- -------- -------- -------- 27% 88% 4 4 0 0 46.2KB 13.7KB 0% 35.4KB 2.36KB 0% 10.8KB 10.9KB 962KB 31.7KB 62 55 12% 62% 3 3 0 0 12.7KB 12.9KB 0% 207B 268B 0% 12.3KB 12.5KB 2.40MB 7.73MB 51 47 11% 41% 27 27 0 0 119KB 39.2KB 0% 104KB 25.6KB 0% 13.8KB 13.5KB 1.65MB 0B 155 116 cl03: cluster.cluster: 3/5/2015 10:16:17 cpu cpu total fcache total total data data data cluster cluster cluster disk disk pkts pkts avg busy ops nfs-ops cifs-ops ops recv sent busy recv sent busy recv sent <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> write recv sent ---- ---- -------- -------- -------- -------- -------- -------- ---- -------- -------- ------- -------- -------- -------- -------- -------- -------- Minimums: 11% 30% 1 1 0 0 12.7KB 12.9KB 0% 148B 245B 0% 10.8KB 10.9KB 947KB 0B 51 47 Averages <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> 12 samples: 20% 72% 7 7 0 0 58.5KB 20.4KB 0% 42.6KB 4.52KB 0% 15.7KB 15.7KB 1.57MB 4.06MB 105 90 Maximums: 30% 94% 27 27 0 0 145KB 39.2KB 0% 121KB 25.6KB 0% 23.9KB 24.4KB 3.06MB 12.4MB 198 177</code> </pre><br></div></div><br>  Read more about optimization and verification of the disk subsystem <a href="http://blog.aboutnetapp.ru/archives/841">here</a> . <br><br><h4> <a href="https://habr.com/ru/post/243045/">   </a> </h4><a name="snap_aggr"></a><br>       ,    ,            ,   SyncMirror (MetroCluster)   . <br><pre> <code class="bash hljs">snap shced -A aggr1 0 0 0 snap delete -A aggr1 snap list -A aggr1 snap reserve -A aggr1 0</code> </pre><br>  Clustered ONTAP     <br><pre> <code class="bash hljs">system node run -node * -<span class="hljs-built_in"><span class="hljs-built_in">command</span></span></code> </pre> <br><br><h4> <a href="https://habr.com/ru/post/243045/">   VSC: FlashCache/FlashPool</a> </h4><a name="VSC"></a><br> Virtual Storage Tiering (VST) ‚Äî    . FlashCache ‚Äî  PCIe,      . FlashPool ‚Äî       (SAS/SATA HDD)   SSD   RAID (),    ,     (       , ..   ).        ,          ,     : <br><ul><li>  <a href="http://www.netapp.com/us/system/pdf-reader.aspx%3Fm%3Dtr-3832.pdf">FlashCache</a>  Predictive Cache Statistics (PCS) </li><li>  <a href="http://www.netapp.com/us/system/pdf-reader.aspx%3Fm%3Dtr-4070.pdf%26cc%3Dus">FlashPool</a>  Advanced Workload Analyzer (AWA) </li></ul><br><br><h4>  <a href="http://habrahabr.ru/post/154205/">Compatibility</a> </h4><a name="compatibility"></a><br>  <a href="http://habrahabr.ru/post/154205/">  </a>          <abbr title="  "></abbr> . <br><br>              ,       . <br><br>  <b>Comments on errors in the text and suggestions please send to the <abbr title="Private message">LAN</abbr> .</b> </div><p>Source: <a href="https://habr.com/ru/post/243045/">https://habr.com/ru/post/243045/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../243027/index.html">Wargaming Developers Contest: Winners Announcement</a></li>
<li><a href="../243033/index.html">How and why does Yandex disable its own data centers</a></li>
<li><a href="../243035/index.html">We catch snmp mac-notification traps from Cisco devices</a></li>
<li><a href="../243037/index.html">Decorative decorative lamp SDB-Z "Evlampiya"</a></li>
<li><a href="../243039/index.html">Sandcastle and SHFB</a></li>
<li><a href="../243047/index.html">A 19-year-old vulnerability can capture a computer through Internet Explorer.</a></li>
<li><a href="../243049/index.html">Which GIS to use after the actual death of Google Maps?</a></li>
<li><a href="../243051/index.html">Stack programming with a human face (part two)</a></li>
<li><a href="../243055/index.html">Own tor2web-service using Nginx and Lua</a></li>
<li><a href="../243059/index.html">Another script for generating icons for Android</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>