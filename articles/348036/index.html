<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Software sound synthesis on early personal computers. Part 1</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This is an article about the first software synthesizers that were once created on the most common personal computers. I give some practical examples ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Software sound synthesis on early personal computers. Part 1</h1><div class="post__text post__text-html js-mediator-article">  This is an article about the first software synthesizers that were once created on the most common personal computers.  I give some practical examples on the implementation of simple methods of sound synthesis in a historical context. <br><br>  <a href="https://habrahabr.ru/post/348192/">Go to the second part</a> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mr/vt/xd/mrvtxdyykyasj6c3v6ehp2p34vc.png" width="75%"></div><a name="habracut"></a><br><h3>  Introduction </h3><br>  In the second half of the 90s, I enthusiastically got acquainted with computer music on a PC and tried to compose my compositions in popular then tracker programs, such as Fast Tracker II and EdLib.  But the greatest impression on me in those days was the Csound audio language, which I learned about, thanks to a 1998 article from <a href="http://old.computerra.ru/1998/277/194848/">Computerra magazine</a> .  To work with Csound you need only a text editor.  The speed of the computer, the presence of libraries of sampled sounds or the quality of the sound card did not matter.  It was a software synthesis of sound in its purest form. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      On modern personal computers, it is not difficult to implement the once-impressive musical examples for Csound in real time.  Today, specialized hardware solutions for problems of sound synthesis are used by musicians less and less, and audio plugs have become most popular.  So the matter, of course, was not always the case.  What were the first attempts to create software synthesis on early personal computers?  What were these computers?  Finally, what are the details of the implementation of the early methods of program synthesis?  I will try to answer these questions further. <br><br><h4>  Practice </h4><br>  In my opinion, teaching in the historical context has many advantages and therefore, along with the description of the cases of bygone days, I wanted to encourage readers, not alien to programming, to practical sound design.  I will not go into the theory, because I see it as my task to develop an initial and intuitive idea for interested readers about how synthesizers are programmed.  For this reason, the examples that will be shown below are implemented without typical tricks and tricks familiar to experienced developers. <br><br>  We will work in the spirit of the pioneers of computer music of the 60s, who used in their research the MUSIC N audio language series developed by Max Mathews.  Csound, which was discussed above, is one of the many descendants of MUSIC N. Of course, we have certain advantages over the pioneers: you do not need to wait for the code to be compiled for long hours, you do not need to write the result to magnetic tape, and then go with it to another city in order to transfer the received data to sound on another computer. <br><br>  Sound synthesis examples will be given in Python.  One of the important advantages of this language is the similarity of its syntax with pseudocode, the reading of which should not cause particular difficulties even for readers who are not familiar with Python.  The programs proposed in the article serve purely illustrative purposes and are intended to encourage the reader to experiment with sound algorithms.  Each example is a separate program for versions 2 and 3 of the language and depends only on the standard libraries.  Running examples ends with the formation of a wav file.  Using <a href="https://pypy.org/">PyPy</a> will speed up the calculations several times. <br><br>  To save the result in a wav-file, I use the standard wave module.  All further examples are preceded by the code shown below. <br><br><pre><code class="python hljs">SR = <span class="hljs-number"><span class="hljs-number">44100</span></span> <span class="hljs-comment"><span class="hljs-comment">#   16-     #    SR  wav- def write_wave(filename, samples): f = wave.open(filename, "w") f.setparams((1, 2, SR, len(samples), "NONE", "")) f.writeframes(b"".join( [struct.pack('&lt;h', round(x * 32767)) for x in samples])) f.close() #      def sec(x): return SR * x</span></span></code> </pre> <br>  Here is a simple example in which only pairs of sinusoids are used. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  ,  ,   bank def sines(bank, t): mix = 0 for f in bank: mix += math.sin(2 * math.pi * f * t / SR) return mix # DTMF-   1-9 DTMF = [ [697, 1209], [697, 1336], [697, 1477], [770, 1209], [770, 1336], [770, 1477], [852, 1209], [852, 1336], [852, 1477] ] samples = [] #      for d in [3, 1, 1, 5, 5, 5, 2, 3, 6, 8]: for t in range(int(sec(0.05))): samples.append(0.5 * sines(DTMF[d - 1], t)) for t in range(int(sec(0.05))): samples.append(0) write_wave("dtmf.wav", samples)</span></span></code> </pre><br>  <a href="https://github.com/true-grue/old_softsynths_article_examples/blob/master/dtmf.py">Source</a> <br><br>  <a href="https://soundcloud.com/peter-sovietov/dtmf">Sound</a> <br><br>  Here the tone dial sound is generated for the fictitious phone number 3115552368. The digits of the <a href="http://www.radioscanner.ru/info/article58/">DTMF number are</a> encoded with pairs of sinusoids whose frequencies are specified in the DTMF array.  Pay attention to the characteristic clicks in the sound, which are the result of sudden amplitude drops.  To get rid of such clicks, you can use the amplitude envelope or low-pass filter.  The use of envelopes and filters will be described in more detail below. <br><br><h3>  TWANG </h3><br><blockquote>  And they, in fact, showed me three things.  But I was so impressed with the first of these things that I did not pay attention to the rest.  Among other things, they showed me object-oriented programming, which I did not notice.  They also showed me a computer network ... in which more than a hundred Alto computers were connected, there was email, etc., etc.  I did not notice all this, as I was completely blinded by the first thing that was shown to me - a graphical user interface. <br></blockquote>  So, after a while, Steve Jobs recalled his visit to the Xerox PARC research center in 1979.  Computer Xerox Alto, which was created in 1973, in many respects anticipated the appearance of personal computers in the next decades.  The author of the Alto concept is the scholar Alan Kay (Alan Kay), who is no stranger to music: in his youth he managed to visit a jazz guitarist, and in his mature years he mastered a church organ.  Perhaps for this reason, Alto had not only outstanding graphics capabilities for its time, but also good sound support. <br><br>  In 1975, the 16-bit Alto, which operated at a frequency of 5.88 MHz and possessed 128 Kbytes of RAM, already had some music software.  In particular, for this computer there was a program <a href="https://quod.lib.umich.edu/cache//b/b/p/bbp2372.1975.005/bbp2372.1975.005.pdf">TWANG</a> , developed by Ted Kaehler (Ted Kaehler) in the Smalltalk language.  In graphic form, quite in the spirit of modern musical MIDI editors, a stream of note events was demonstrated that could be added using the mouse or from the keyboard of an electronic organ.  The sound was generated in real time by the method of FM synthesis, and each timbre parameter presented in the form of a graph could be adjusted, again, with the mouse.  Drawing user envelopes with an arbitrary number of segments, as well as a visual representation of all the parameters of FM synthesis on a single graphic can not always be found even in modern software synthesizers. <br><br><img src="https://habrastorage.org/webt/sl/y1/du/sly1duirqhs_xduzy9k4u5iblfu.png"><br>  <i>View and edit note events in TWANG</i> <br><br>  FM synthesis was discovered in the late 60s by musician and scholar John Chowning from Stanford University during his experiments with one of the early MUSIC N audio languages ‚Äã‚ÄãN. Chowning noted that when using the usual vibrato effect (which is a periodic change in pitch) on sufficiently high frequency, the timbre of the modulated signal begins to change significantly.  From the point of view of the listener, the greatest interest is not a static spectrum, but its various modifications that unfold in time.  In this sense, FM synthesis has great potential with modest computational requirements, since, by controlling the amplitude and frequency of the modulator, it is easy to dynamically influence the spectrum of the carrier signal. <br><br><img src="https://habrastorage.org/webt/di/48/f5/di48f50haephib2dyxlm4bexz2c.png"><br>  <i>Editing FM synthesis parameters in TWANG</i> <br><br>  By the way, thanks to the funds obtained by Stanford University from the sale of the license for FM synthesis to Yamaha in the early 70s, the University established the CCRMA (Center for Computer Research in Music and Acoustics) research center.  <a href="https://mitpress.mit.edu/books/sound-innovation">Many important results</a> in the field of sound synthesis methods are associated with the name of this center. <br><br>  Known parameters of FM-synthesis, implemented on Xerox Alto: <br><br><ul><li>  5 votes (2 oscillators each) working in real time </li><li>  12-bit samples with a sampling frequency of 13.7 kHz, </li><li>  Update the parameters of voices at 60 Hz. </li></ul><br>  It is noteworthy that no special equipment, other than a DAC, was used to work with sound in Alto.  FM synthesis was implemented in software and was performed simultaneously with GUI processing and other tasks.  How, then, was the modest computer able to cope with real-time synthesis?  The fact is that in Alto, the programmer was allowed to update the microcode right while the computer was running.  Work at the microcode level was widely used by Xerox PARC developers to control external devices, as well as to virtualize a set of instructions and create problem-oriented commands, for example, for working with graphics.  In the computer there was also hardware support for cooperative multitasking for the processes that performed the microcode. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://player.vimeo.com/video/111334074" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>An example of working in TWANG</i> <br><br>  To output the sound in TWANG, special instructions were used.  The appropriate microcode for implementing FM synthesis was added by Steve Saunders.  In his implementation, he managed to do without multiplication operations using <a href="https://quod.lib.umich.edu/cgi/p/pod/dod-idx/real-time-digital-fm-audio-synthesis.pdf%3Fc%3Dicmc%3Bidno%3Dbbp2372.1975.004%3Bformat%3Dpdf">two simple techniques</a> : by replacing the sine with a triangular signal in the modulator, and also using the trigonometric identity <i>sin (x + a) + sin (x - a) = 2 * cos (a) * sin (x)</i> for amplitude control.  It is interesting to note that Yamaha later, in its own way, got rid of multiplication operations in its FM chips using <a href="https://docs.google.com/document/d/18IGx18NQY_Q1PJVZ-bHywao9bhsDoAqoIn1rIm42nwo/edit">log and exp tables</a> . <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://player.vimeo.com/video/111334072" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>TWANG sound sample</i> <br><br>  The TWANG program has not received further development.  Saunders in the early 80s had a hand in the development of the <a href="http://hype.retroscene.org/blog/171.html">Atari Amy</a> sound chip.  This chip was a fairly powerful additive synthesizer for microcomputers, leading the sound quality of typical sound generators of the time, but Atari‚Äôs turmoil prevented Amy from seeing the light. <br><br><h4>  Practice </h4><br>  At once I will clarify what will be discussed later on phase (PM), and not frequency modulation, although the second naming will be used.  It is the phase modulation option implemented by Yamaha in their FM synthesizers. <br><br>  In the examples of this section, it will be convenient to use the form of the description of the sine wave generator in the form of an object that stores the current phase and has the next method for issuing the next sample.  The parameter t is not explicitly used here, as was the case with the DTMF signals, and the phase value changes within the period of the sin function. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Sine</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.phase = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-comment"><span class="hljs-comment">#       freq    pm, #  ,   ,     #      phase,     def next(self, freq, pm=0): s = math.sin(self.phase + pm) self.phase = (self.phase + 2 * math.pi * freq / SR) % (2 * math.pi) return s</span></span></code> </pre><br>  In FM synthesis, oscillators are combined in various configurations based on the following basic compounds: serial, parallel (additive synthesis), and with feedback.  Even using only two oscillators connected in series, you can get quite complex timbres.  In this case, the output of one of the oscillators is fed to the input of the phase offset of the other oscillator.  From the point of view of creating timbres, such a variant of connection can be approximately considered as a pair of oscillator-filter in an analog synthesizer.  Below is an example of the implementation of a serial connection for the synthesis of the simplest tone of a bell. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># FM-   # y(t) = Ac * sin(2 * PI * fc * t + Am * sin(2 * PI * fm * t)) oc = Sine() om = Sine() samples = [] for t in range(int(sec(1))): env = 1 - t / SR samples.append(0.5 * oc.next(80, 3 * env * om.next(450))) write_wave("bell.wav", samples)</span></span></code> </pre><br>  <a href="https://github.com/true-grue/old_softsynths_article_examples/blob/master/bell.py">Source</a> <br><br>  <a href="https://soundcloud.com/peter-sovietov/bell-1">Sound</a> <br><br>  In this example, the osc oscillator om ("modulator") controls the sound of the main oscillator oc ("carrier").  The following parameters are responsible for the characteristic timbre of the bell: <br><br><ul><li>  the ratio of the modulator and carrier frequencies, creating non-harmonic overtones, </li><li>  linear attenuation of the amplitude of the modulator (3 * env), which introduces the desired dynamics in the spectral composition. </li></ul><br>  It is quite difficult to give a short recipe on the choice of parameters for synthesizing FM timbres.  Theoretical information on this subject can be found in the book <a href="http://www.burnkit2600.com/manuals/fm_theory_and_applications.pdf">FM Theory &amp; Applications</a> .  But, often, intuition and practice are more important.  In general, in sound design, the basic approach is to decompose a complex timbre, dividing it into separate layers and elements so that you can work on them almost independently. <br><br>  For convenient work with control signals that dynamically change, as, for example, this happens with the modulator amplitude in this case, it is useful to implement an envelope generator.  Its code is shown below. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># - ,       def linear_env(segs, t): x0 = 0 y0 = 0 for x1, y1 in segs: if t &lt; x1: return y0 + (t - x0) * ((y1 - y0) / (x1 - x0)) x0, y0 = x1, y1 return y0 class Env: def __init__(self, segs): self.segs = segs self.phase = 0 def next(self, scale=1): s = linear_env(self.segs, self.phase) self.phase += scale / SR return s</span></span></code> </pre><br>  The envelope is determined using an array of linear segments.  The x coordinate usually sets the time, and the y coordinate can be interpreted differently, for example, as amplitude or frequency. <br><br>  The following example on the topic of FM synthesis and envelopes is related to the simulation of percussions.  Here the timbres of a bass drum (kick) and a snare drum (snare) are implemented.  Note that the synthesis code of the working drum uses a combination of oscillators with feedback (variable fb).  The use of feedback in FM synthesis (it was an invention of Yamaha) allows you to create a large variety of timbres by simple efforts.  With its help, it is possible to generate noise, as in this case, as well as creating sawtooth and rectangular sound waves.  Numerous envelope generators are used in this example, not only to control the amplitude, but also the frequency of the oscillators. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">kick</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(samples, dur)</span></span></span><span class="hljs-function">:</span></span> freq = <span class="hljs-number"><span class="hljs-number">100</span></span> o1 = Sine() o2 = Sine() e1 = Env([(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), (<span class="hljs-number"><span class="hljs-number">0.03</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), (<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>)]) e2 = Env([(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), (<span class="hljs-number"><span class="hljs-number">0.01</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>)]) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(int(sec(dur))): o = o1.next(freq * e1.next(<span class="hljs-number"><span class="hljs-number">2.5</span></span>), <span class="hljs-number"><span class="hljs-number">14</span></span> * e2.next() * o2.next(freq)) samples.append(<span class="hljs-number"><span class="hljs-number">0.5</span></span> * o) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">snare</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(samples, dur)</span></span></span><span class="hljs-function">:</span></span> freq = <span class="hljs-number"><span class="hljs-number">100</span></span> o1 = Sine() o2 = Sine() e1 = Env([(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), (<span class="hljs-number"><span class="hljs-number">0.2</span></span>, <span class="hljs-number"><span class="hljs-number">0.2</span></span>), (<span class="hljs-number"><span class="hljs-number">0.4</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>)]) e2 = Env([(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), (<span class="hljs-number"><span class="hljs-number">0.17</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>)]) e3 = Env([(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), (<span class="hljs-number"><span class="hljs-number">0.005</span></span>, <span class="hljs-number"><span class="hljs-number">0.15</span></span>), (<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>)]) fb = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(int(sec(dur))): fb = e2.next() * o1.next(freq, <span class="hljs-number"><span class="hljs-number">1024</span></span> * fb) samples.append(<span class="hljs-number"><span class="hljs-number">0.5</span></span> * o2.next(e1.next() * freq * <span class="hljs-number"><span class="hljs-number">2.5</span></span>, <span class="hljs-number"><span class="hljs-number">5.3</span></span> * e3.next() * fb)) samples = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">4</span></span>): kick(samples, <span class="hljs-number"><span class="hljs-number">0.25</span></span>) kick(samples, <span class="hljs-number"><span class="hljs-number">0.25</span></span>) snare(samples, <span class="hljs-number"><span class="hljs-number">0.5</span></span>) write_wave(<span class="hljs-string"><span class="hljs-string">"drums.wav"</span></span>, samples)</code> </pre><br>  <a href="https://github.com/true-grue/old_softsynths_article_examples/blob/master/drums.py">Source</a> <br><br>  <a href="https://soundcloud.com/peter-sovietov/drums">Sound</a> <br><br>  In general, the most realistic timbres are obtained in FM synthesis using 6 or more oscillators, but operating with such configurations lies beyond the capabilities of ordinary electronic musicians, therefore at present this type of synthesis is most often combined with other approaches. <br><br><h3>  SAM </h3><br>  In the late 70s, special speech synthesis chips began to gain popularity.  Among the first was the chip TMS5100, which was used in the popular children's toy Speak &amp; Spell.  The development of this microcircuit demanded remarkable efforts from Texas Instruments engineers and its appearance is considered an important milestone in the development of processors for digital signal processing.  Against this background, the story of SAM (Software Automatic Mouth) seems to be especially surprising - a fully software speech synthesizer, which was born back in 1979 and by the early 80s was implemented on the simplest 8-bit microcomputers by Apple, Atari and Commodore. <br><br><img src="https://habrastorage.org/webt/5_/jh/8b/5_jh8bfjyjkisd5xqdbymo1nn8i.png"><br>  <i>SAM Advertising</i> <br><br>  SAM was a set of programs without any graphical interface.  With the help of these programs, it was possible to immediately translate arbitrary text to speech or, using the phonetic alphabet, to preliminarily describe in detail the method of sounding texts with a synthesizer.  The clarity of phrases, in my opinion, at SAM is not inferior to chips from Texas Instruments.  Moreover, in the version of formant synthesis implemented in SAM, it is possible on the fly to change the timbre and intonation of the voice with which user phrases are pronounced. <br><br>  What is formant synthesis?  The most thorough approach to the synthesis of voice are solutions based on physical modeling, differing in both computational complexity and complexity of customization.  A simpler way is to use tone and noise generators, which are processed by a set of band-pass filters that simulate formants (peaks in the signal spectrum that determine speech sounds).  In the case of Speak &amp; Spell, ready-made phrases were coded based on linear prediction (LPC).  The corresponding parameters determine the operation of the tone and noise generator, as well as the 10th order digital filter that is common to them. <br><br>  The calculation of the next value at the output of the Speak &amp; Spell filter required 20 multiplication operations, which was clearly beyond the capabilities of microcomputers of that time.  What is the trick used by the creators of SAM?  Apparently, this question did not give rest to researchers for many years, until finally, in the mid-2000s, the SAM code for the Commodore C64 microcomputer was not disassembled and <a href="https://github.com/s-macke/SAM">translated into C language</a> . <br><br>  As it turned out, the implementation of speech synthesis in SAM did without a single multiplication operation at all.  To generate formants, only the generators of a sine wave and a square wave appear in the code.  Generally speaking, there is a synthesis of sinusoidal waves, in which the formation of formants is simply replaced by sinusoids at the corresponding frequencies, but this option is not distinguished by intelligibility of speech.  SAM's approach is different.  Its origins are in the theory of wavelets and <a href="https://mitpress.mit.edu/books/microsound">granular synthesis</a> .  There are many variations of this approach, but their main feature is the imitation of the effect of the band-pass filter.  Since the ‚Äúfilter‚Äù is fictitious, it is impossible to send any third-party signal to it.  Instead of the complex implementation of real band-pass filters, formant synthesis is carried out at the level of generation of ‚Äúbursts‚Äù or ‚Äúgranules‚Äù of waveforms.  Similarly, the filters in the Casio CZ, Roland MT-32 and Yamaha FS1r synthesizers are simulated.  One of the most well-known varieties of this approach is FOF (Formant Wave Function), which has successfully proved itself in the <a href="http://anasynth.ircam.fr/home/english/media/singing-synthesis-chant-program">synthesis of singing</a> . <br><br>  Beg a small digression.  A good third of the film ‚ÄúSteve Jobs‚Äù (2015) is devoted to an overly dramatic depiction of events related to the problems of launching a speech synthesizer during the presentation of the first Macintosh, which took place in 1984.  You can learn how things were in reality from the memories of Andy Hertzfeld, one of the key Apple developers at the time.  Steve Jobs was a connoisseur of music, both classical and popular.  Obviously, for this reason, as well as impressed by the advanced computers of the 70s Xerox, the sound subsystem of a good (at that time) quality was implemented in the Macintosh: mono, 22 kHz, 8 bits.  It was important for Jobs that at the presentation the computer would lose the solemn melody from the Chariots of Fire by Vangelis.  Alas, time was running out, and Herzfeld managed to synthesize timbres only at the level of DTMF-signals, which we implemented in the first example.  In this situation, the Macintosh team was rescued by Mark Barton, none other than the author of SAM.  Of course, Jobs was delighted with the idea that the computer would introduce itself.  So it happened.  That's just ‚ÄúChariots of Fire‚Äù was launched during a presentation from a CD player, and the 128K computer model had to be secretly replaced for 512K for the sake of a speech synthesizer, which was still in development.  However, this is another story. <br><br>  The SAM version of Mac for Macintosh called MacinTalk, in contrast to the microcomputer options, generated a higher quality 8-bit sound.  SAM, the maker of SoftVoice, eventually released versions of speech synthesizers for Amiga and Windows.  To date, apparently, SoftVoice no longer exists, but the popularity of the restored SAM source code is growing among developers of embedded systems who want to implement simple speech synthesis on resource-limited microcontrollers. <br><br><h4>  Practice </h4><br>  From studying the SAM code, it is clear that this speech synthesizer has only 4 oscillators: <br>  2 sine wave generators, a square wave generator (square wave), as well as a pulse generator that simulates the operation of the vocal cords.  The parameters of these oscillators are encoded in a stream of frames, which are updated with a certain frequency corresponding to the speed of "pronouncing" phrases. <br><br>  The meander generator can be implemented in the most primitive way using the ternary conditional operator (in C syntax it looks like this: <i>phase &lt;0.5? 1: -1</i> ).  This option, generally speaking, suffers from one very common drawback from the world of digital signal processing: frequency overlaying (aliasing).  An ideal meander has an infinite spectrum, but sound waves should not contain frequencies greater than or equal to half the sampling frequency (Kotelnikov's theorem), otherwise we risk getting ‚Äúdirt‚Äù that is quite audible in the spectrum.  An entire book can be devoted to different ways to achieve the correct signal spectrum for the simplest ‚Äúsquare‚Äù or ‚Äúsaw‚Äù.  One of the simple tricks is using FM synthesis with feedback.  Now, the ‚Äúwrong‚Äù way will be enough for us.  I also put the word ‚Äúwrong‚Äù in quotes because the music synthesis algorithms are not exactly the same as regular digital signal processing.  In music, the result is important from an aesthetic point of view, even if it was not obtained ‚Äúaccording to science‚Äù.  In the case of SAM, this is also true, since this voice synthesizer was created for sound output by 8-bit microcomputers. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Sam</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.phases = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-comment"><span class="hljs-comment">#  frame   , voice    def next(self, frame, voice): flags, ampl1, freq1, ampl2, freq2, ampl3, freq3, pitch = frame mix = ampl1 * math.sin(self.phases[1]) + ampl2 * math.sin(self.phases[2]) mix += ampl3 * (1 if self.phases[3] &lt; 0.5 else -1) self.phases[1] = (self.phases[1] + 2 * math.pi * freq1 / SR) % (2 * math.pi) self.phases[2] = (self.phases[2] + 2 * math.pi * freq2 / SR) % (2 * math.pi) self.phases[3] = (self.phases[3] + freq3 / SR) % 1 self.phases[0] += 1 if self.phases[0] &gt; pitch * voice * SR: self.phases = [0, 0, 0, 0] return 0.5 * mix #       SAM  64 COEFFS = [1, 0.1, 27, 0.1, 27, 0.1, 27, 0.00001] #      def parse(frames): frames = [[int(y) * COEFFS[i] for i, y in enumerate(x.split())] \ for x in frames.strip().split("\n")] return frames #  sam.txt      # https://github.com/s-macke/SAM   -debug with open("sam.txt") as f: frames = parse(f.read()) s = Sam() samples = [] #    " " for voice in range(25, 5, -2): for frame in frames: for t in range(int(sec(0.01))): samples.append(0.5 * s.next(frame, voice)) write_wave("sam.wav", samples)</span></span></code> </pre><br>  <a href="https://github.com/true-grue/old_softsynths_article_examples/blob/master/sam.py">Source</a> <br> <a href="https://github.com/true-grue/old_softsynths_article_examples/blob/master/sam.txt"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Frames </font></font></a> <br><br> <a href="https://soundcloud.com/peter-sovietov/sam"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sound</font></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> A key in the work of SAM is a pulse generator (its index in the array of arrays is zero), imitating the work of the vocal cords, although it does not produce sound by itself. The result of his work is the zeroing of the phases of the other oscillators with a frequency corresponding to the chosen type of voice. This method of synthesizing formants by resetting the oscillator-carrier oscillator-modulator phase is known in the world of analog synthesizers as </font></font><a href="http://www.harmonycentral.com/articles/all-about-synthesizer-hard-sync"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">hard sync</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><img src="https://habrastorage.org/webt/ls/up/pt/lsupptgfficwrwecyufmj9glxvy.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Example of ‚Äútight‚Äù synchronization of a sine wave in SAM</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Initially, I wanted to implement the synthesis of the phrase My name is Sam, but then the hissing and whistling consonants would have to be supported with the help of pre-recorded sound bites (as was done in the original program). </font><font style="vertical-align: inherit;">Therefore, I stopped only on the part of My name. </font><font style="vertical-align: inherit;">A more complete solution would be to support noise generators with various parameters.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> SoftSynth and TurboSynth </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">One of the first developers of commercial music software for the Apple Macintosh 128K computer was Peter Gotcher and Evan Brooks, friends who played on the same student team and were fond of both music and programming. </font><font style="vertical-align: inherit;">In 1986, Gotcher and Brooks released a program with a talking name SoftSynth. </font><font style="vertical-align: inherit;">Yesterday‚Äôs students implemented high-grade additive synthesis with the following parameters:</font></font><br><br><ul><li> 32      (, ¬´¬ª, ¬´¬ª  3   ), </li><li>        ,   40  . </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SoftSynth had advanced tools for editing synthesis parameters, as well as their visualization (including in the form of a 3d spectrogram - in the spirit of the famous Fairlight CMI synthesizer). </font></font><br><br><img src="https://habrastorage.org/webt/43/wd/ie/43wdieruehwqd_knacjbsv2i828.jpeg"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SoftSynth 3d spectrogram </font></font></i> <br><br><img src="https://habrastorage.org/webt/nd/l6/kt/ndl6ktzxrhrb3xyn0oat0wee0nk.jpeg"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Editing oscillator envelopes in SoftSynth</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> It seems surprising that the possibilities of the ancient Macintosh were enough to implement such a synthesis. The secret is simple: the process of creating sound in SoftSynth did not occur in real time, but by clicking on a special icon. Typing a typical short sample took no more than a few seconds. SoftSynth can be attributed to the now forgotten class of program-generator samples. Distracting, you can remember Oleg Sharonov's Orangator, a popular program from the same class in the late 90s. </font></font><br><br><img src="https://habrastorage.org/webt/ut/qq/3l/utqq3ltwkm-da2p0zv457cexxis.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Orangator Interface</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">By the mid-80s, the popularity of hardware samplers (such as, for example, E-MU Emulator, released in 1982) had grown enormously, and the program, which made it possible to synthesize samples from scratch for later loading them into the memory of hardware tools, was in demand by musicians. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But how did young developers, such as Gotscher and Brooks, receive information related to the principles of computer sound in the early 80s? </font><font style="vertical-align: inherit;">By that time there were already at least three important sources on this topic:</font></font><br><br><ul><li> <a href="http://bitsavers.informatik.uni-stuttgart.de/pdf/bellLabs/The_Technology_of_Computer_Music_1969.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The Technology of Computer Music</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (1969) by Max Mathews (Max Mathews), which described the audio language MUSIC V and the simplest methods of synthesis.</font></font></li><li> <a href="http://sites.music.columbia.edu/cmc/courses/g6610/fall2016/week8/Musical_Applications_of_Microprocessors-Charmberlin.pdf">Musical Applications of Microprocessors</a> (1980)   (Hal Chamberlin).     ,        .            ,        . </li><li>  Computer Music Journal,    1977       , ,     . </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Obviously, under the influence of both modular analog synthesizers and the graphic notation MUSIC V from Matthews, an interface was developed for another, quite advanced for its time, musical program for the Macintosh computer from the Gotscher and Brooks team. </font><font style="vertical-align: inherit;">This program was called Turbosynth (1988). </font><font style="vertical-align: inherit;">Turbosynth, like SoftSynth, is a sample generator. </font><font style="vertical-align: inherit;">The user is provided with a palette of sound modules, copies of which can be transferred with the mouse to the workspace and, further, connected to taste with virtual ‚Äúwires‚Äù. </font><font style="vertical-align: inherit;">The program has implemented a large number of modules:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> an oscillator with a choice of different waveforms, the ability to morph these shapes using the time scale, as well as the ability to draw custom waveforms, </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> sampler, </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> noise generator </font></font></li><li>     c      , </li><li> -  , </li><li>  , </li><li> , </li><li>     , </li><li> AM-, FM-  PM-, </li><li>  ¬´¬ª    , </li><li>  (waveshaper)     , </li><li> . </li></ul><br><img src="https://habrastorage.org/webt/uj/po/vg/ujpovgxvzjtc_jggra12c95towm.jpeg"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Connecting modules in Turbosynth</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> In general, working with Turbosynth differs little from later visual music programming environments (Kyma, Max / MSP, Pure Data, Reaktor). It is known that experimenter musician Trent Reznor (Trent Reznor, Nine Inch Nails) widely used TurboSynth in 1993, during the recording of the album Mr. Self Destruct. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What was the fate of Gotcher and Brooks? They became known as the creators of the Pro Tools software and hardware system, which is still used in many professional recording studios.</font></font><br><br><h4>  Practice </h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I now propose to consider another example of additive synthesis called the Arpeggio Rissa. </font><font style="vertical-align: inherit;">The author of this simple but interesting sound algorithm is Jean-Claude Risset, one of the pioneers of computer music. </font><font style="vertical-align: inherit;">In the implementation of the Arpeggio Risse, 63 sine wave generators are used, the frequencies of which are set so that the effect of the beats leads to sound with a constantly changing timbre.</font></font><br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sines</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(bank, t)</span></span></span><span class="hljs-function">:</span></span> mix = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> f <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> bank: mix += math.sin(<span class="hljs-number"><span class="hljs-number">2</span></span> * math.pi * f * t / SR) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> mix <span class="hljs-comment"><span class="hljs-comment">#     f = 96 i1 = 0.03 i2 = i1 * 2 i3 = i1 * 3 i4 = i1 * 4 risset = [] #  63 (9 * 7)    risset for i in [f, f + i1, f + i2, f + i3, f + i4, f - i1, f - i2, f - i3, f - i4]: for j in [i, 5 * i, 6 * i, 7 * i, 8 * i, 9 * i, 10 * i]: risset.append(j) samples = [] for t in range(int(sec(20))): samples.append(0.01 * sines(risset, t)) write_wave("risset.wav", samples)</span></span></code> </pre><br>  <a href="https://github.com/true-grue/old_softsynths_article_examples/blob/master/risset.py">Source</a> <br><br> <a href="https://soundcloud.com/peter-sovietov/risset"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The sound of</font></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> the Arpeggio Riss is the simplest example of a process music. </font><font style="vertical-align: inherit;">There is no division into instruments and notes. </font><font style="vertical-align: inherit;">A single algorithm spawns the entire piece of music.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The following example is less related to sound synthesis. </font><font style="vertical-align: inherit;">The implementation of the sound algorithm using the description of blocks and the links between them can be represented both in text (MUSIC N) and in graphical (TurboSynth) form. </font><font style="vertical-align: inherit;">In both cases, an approach is used in which calculations are controlled by data flows (dataflow). </font><font style="vertical-align: inherit;">This approach is known in many areas of computer science: compilation theory, processor architectures, distributed systems, etc. </font><font style="vertical-align: inherit;">Let's try to create a simple dataflow model in text form. </font><font style="vertical-align: inherit;">We will use blocks (box), which are engaged in calculations, and also provide the ability to randomly connect blocks with wires (wire). </font><font style="vertical-align: inherit;">Programs such as Pure Data and Max / MSP use two types of links:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> asynchronous, for example, note events, </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> synchronous, for working with audio signals. </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">For simplicity, we restrict ourselves to synchronous connections. </font><font style="vertical-align: inherit;">More specifically, to start the calculation inside the block, we will require the following conditions:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> the input wires of the block contain ready data, </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> block output wires are free to get new data. </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So, the unit has a number of input and output ports. </font><font style="vertical-align: inherit;">In the following implementation, it is prohibited to connect several wires to one input port, but it is allowed to stretch several wires from a single output port of the unit. </font><font style="vertical-align: inherit;">For this reason, in the block object, each element of the array of output ports is itself an array of connected consumer blocks.</font></font><br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#      def reset_ins(ins): for i, x in enumerate(ins): ins[i] = None #    def is_ins_full(ins): for x in ins: if x is None: return False return True #       def is_outs_empty(outs): for out in outs: for box, port in out: if box.ins[port] is not None: return False return True #     .   def send_to_outs(results, outs): for i, x in enumerate(results): for box, port in outs[i]: box.ins[port] = x class Box: #  ,   op, #  ins   outs  def __init__(self, op, ins, outs): self.ins = [None] * ins self.outs = [[] for i in range(outs)] self.op = op def compute(self): if is_ins_full(self.ins) and is_outs_empty(self.outs): send_to_outs(self.op(*self.ins), self.outs) reset_ins(self.ins) #    port1  box1    port2  box2 def wire(box1, port1, box2, port2): box1.outs[port1].append((box2, port2)) def compute(schedule): for b in schedule: b.compute()</span></span></code> </pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To describe the calculations, it is enough to specify the list of blocks (‚Äúschedule‚Äù) and determine the appropriate links. Pay attention to the compute function. She simply tries to run the calculations for each block specified in the schedule. The order of the blocks here does not matter for the correctness of the calculations. However, this sequence affects the efficiency of the calculations. It is desirable that the blocks that produce values ‚Äã‚Äãprecede the blocks that consume these values. Arrange the schedule accordingly using </font></font><a href="https://neerc.ifmo.ru/wiki/index.php%3Ftitle%3D%25D0%2598%25D1%2581%25D0%25BF%25D0%25BE%25D0%25BB%25D1%258C%25D0%25B7%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%25BE%25D0%25B1%25D1%2585%25D0%25BE%25D0%25B4%25D0%25B0_%25D0%25B2_%25D0%25B3%25D0%25BB%25D1%2583%25D0%25B1%25D0%25B8%25D0%25BD%25D1%2583_%25D0%25B4%25D0%25BB%25D1%258F_%25D1%2582%25D0%25BE%25D0%25BF%25D0%25BE%25D0%25BB%25D0%25BE%25D0%25B3%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25BE%25D0%25B9_%25D1%2581%25D0%25BE%25D1%2580%25D1%2582%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25BA%25D0%25B8"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">the topological sorting algorithm</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . The fact that any of the blocks, in accordance with the rule of readiness of input and output data, can independently start calculations, makes it possible to easily parallelize the operation of the graph of the sound algorithm.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Now let's try to put into practice the above. </font><font style="vertical-align: inherit;">Let us create the simplest sound of a siren.</font></font><br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     Sine def Osc(): o = Sine() return Box(lambda x, y: [o.next(x, y)], 2, 1) def Out(samples): def compute(x): samples.append(x) return [] return Box(compute, 1, 0) def clip(x, y): return -y if x &lt; -y else y if x &gt; y else x Const = lambda x: Box(lambda: [x], 0, 1) Mul = lambda: Box(lambda x, y: [x * y], 2, 1) Clip = lambda: Box(lambda x, y: [clip(x, y)], 2, 1) samples = [] patch = { "k1": Const(550), "k2": Const(50), "k3": Const(2), "k4": Const(0), "k5": Const(0.1), "k6": Const(1), "o1": Osc(), "o2": Osc(), "c1": Clip(), "m1": Mul(), "m2": Mul(), "out": Out(samples) } wire(patch["k3"], 0, patch["o1"], 0) wire(patch["k4"], 0, patch["o1"], 1) wire(patch["k2"], 0, patch["m1"], 0) wire(patch["o1"], 0, patch["m1"], 1) wire(patch["k1"], 0, patch["o2"], 0) wire(patch["m1"], 0, patch["o2"], 1) wire(patch["o2"], 0, patch["c1"], 0) wire(patch["k5"], 0, patch["c1"], 1) wire(patch["k6"], 0, patch["m2"], 0) wire(patch["c1"], 0, patch["m2"], 1) wire(patch["m2"], 0, patch["out"], 0) schedule = patch.values() while len(samples) &lt; int(sec(2)): compute(schedule) write_wave("siren.wav", samples)</span></span></code> </pre><br>  <a href="https://github.com/true-grue/old_softsynths_article_examples/blob/master/siren.py">Source</a> <br><br> <a href="https://soundcloud.com/peter-sovietov/siren"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sound</font></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> In this example, the following block types are used:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Const, constant, </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Mul, multiplier, </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Osc, sine wave generator, </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Out, output samples to the output buffer, </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Clip, limiting the signal within the specified level. </font></font></li></ul><br><img src="https://habrastorage.org/webt/ry/40/qf/ry40qfiuwk6izx3z7pzgslrpkby.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Graphic representation of the implementation of the "siren"</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> One may ask why the Clip block was required. </font><font style="vertical-align: inherit;">With it, we achieve the simplest effect of sound distortion. </font><font style="vertical-align: inherit;">Instead of a ‚Äústerile‚Äù sinusoid, we got a signal close to the meander. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Graphic representation of this simple sound algorithm turned out to be more vivid than its text recording. </font><font style="vertical-align: inherit;">But it is rather a lack of a specific implementation. </font><font style="vertical-align: inherit;">In general, the text form usually wins over the graphical variant when using a sufficiently large number of blocks and links. </font><font style="vertical-align: inherit;">The informal </font></font><a href="http://www.infovis-wiki.net/index.php%3Ftitle%3DDeutsch_Limit"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúDeutsch limit‚Äù</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , in particular, features 50 blocks as the corresponding upper bound. </font></font><br><br> <a href="https://habrahabr.ru/post/348192/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Go to the second part</font></font></a> </div><p>Source: <a href="https://habr.com/ru/post/348036/">https://habr.com/ru/post/348036/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../348024/index.html">SecurityWeek 2: army of clones, Google hunts for ghosts, Blizzard patches</a></li>
<li><a href="../348026/index.html">Installing Linux without .ISO and virtualization</a></li>
<li><a href="../348028/index.html">Convolutional neural network, part 2: training in the error back-propagation algorithm</a></li>
<li><a href="../348032/index.html">Mobility Express - when they decided to migrate and scale the wireless network, but as always, there is no money</a></li>
<li><a href="../348034/index.html">Meeting Room Little Helper</a></li>
<li><a href="../348038/index.html">WSTester - JS library for testing web services with WebSocket</a></li>
<li><a href="../348040/index.html">How Atlassian has built a $ 10 billion business. Part 2</a></li>
<li><a href="../348042/index.html">Developing a PCI device driver for Linux</a></li>
<li><a href="../348044/index.html">Rook - "self-service" data store for Kubernetes</a></li>
<li><a href="../348046/index.html">What exactly is personal data?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>