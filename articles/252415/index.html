<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>LVS + OpenVZ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Good day, dear readers! 
 In this article I want to tell you about load balancing technology, a little bit about fault tolerance and how to make frien...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>LVS + OpenVZ</h1><div class="post__text post__text-html js-mediator-article">  Good day, dear readers! <br>  In this article I want to tell you about load balancing technology, a little bit about fault tolerance and how to make friends with it all in OpenVZ containers.  The basics of LVS, modes of operation and configuration of the LVS bundle with containers in OpenVZ will be discussed.  The article contains both the theoretical aspects of the operation of these technologies and the practical part - forwarding traffic from the balancer into the containers.  If this interested you - welcome! <br><a name="habracut"></a><br><br>  To begin with - links to publications on this topic on Habr√©: <br>  <a href="http://habrahabr.ru/post/104621/">Detailed description of the work of LVS.</a>  <a href="http://habrahabr.ru/post/104621/">You can't say better</a> <br>  <a href="http://habrahabr.ru/post/211915/">Article about OpenVZ containers</a> <br><br>  A summary of the material above: <br>  LVS (Linux Virtual Server) is a technology based on IPVS (IP Virtual Server), which is present in Linux kernels from version 2.4.x and later.  It is a kind of virtual switch 4 levels. <br><img src="https://habrastorage.org/files/f37/1ca/839/f371ca8399094e4dbe5aa5bfccf7970a.jpg">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      This picture shows LVS (the balancer itself), the virtual address to which calls (VIP) occur are 192.168.1.100 and 2 servers playing back-end roles: 192.168.1.201 and 192.168.1.202 <br>  In general, everything works this way - we make an entry point (VIP) where all requests come.  Further, the traffic is redirected to its backends, where it is processed and responded directly to the client (in the NAT scheme, the answer is through LVS back) <br>  With methods of balancing can be found here: <a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/2.1/html/AS_Install_Guide/s1-lvs-scheduling.html">link</a> .  later in the article we believe that Round-Robin balancing <br>  Modes of operation: <br><ul><li>  1) Direct <a href="http://www.austintek.com/LVS/LVS-HOWTO/HOWTO/LVS-HOWTO.LVS-DR.html">A lot of things about him</a> </li><li>  2) NAT <a href="http://www.austintek.com/LVS/LVS-HOWTO/HOWTO/LVS-HOWTO.LVS-NAT.html">Information about this mode of operation</a> </li><li>  3) Tunnel <a href="http://www.austintek.com/LVS/LVS-HOWTO/HOWTO/LVS-HOWTO.LVS-Tun.html">And about him</a> </li></ul><br>  A little more about each: <br><br><h5>  1) Direct </h5><br>  In this mode of operation, the request is processed according to the following scenario: <br>  The client sends the packet to the network to the VIP address.  This packet is caught by the LVS server, the <b>MAK</b> destination address (and only he !!) is replaced with the <b>MAC</b> address of one of the back-end servers and the packet is sent back to the network.  It is received by the back-end server, it processes and sends the request directly to the client.  The client receives an answer to his request physically from another server, but does not see the substitution and happily rustles.  The attentive reader will be indignant of course - how can the Real Server process the request that came to another IP address (in the package the destination address is still the same VIP)?  And he will be right, because  To work correctly, this address (VIP) should be placed somewhere near the back end, most often it is hung up on the loopback.  Thus, the biggest scam is committed in this technology. <br><br><h5>  2) NAT </h5><br>  The easiest mode of operation.  The request from the client comes to LVS, LVS forwards the request to the back-end, he processes the request, answers back to LVS and he answers to the client.  Ideally fit into the scheme where you have a gateway through which all subnet traffic goes.  Otherwise, it will be extremely wrong to do NAT in a part of the network. <br><br><h5>  3) Tunnel </h5><br>  It is analogous to the first method, only traffic from LVS to backends is encapsulated in a packet.  That‚Äôs what we‚Äôll set up below, so now I‚Äôll not reveal all the cards :) <br><br><h4>  Practice! </h4><br>  Install the server with LVS.  Configuring on CentOS 6.6. <br>  On a clean system, do <br><pre><code class="bash hljs">yum install piranha</code> </pre> <br>  Behind her, she will drag the Apache, pkhp and the ipvsadm we need.  Apache and pkhp are set to access the administrative web-muzzle, which I advise you to look at once, to set up everything in a basic way and not to go there again :) <br>  After installing all the packages do <br><pre> <code class="bash hljs">/etc/init.d/piranha-gui start ; /etc/init.d/httpd start</code> </pre><br>  set the password for access to the admin panel: <br><pre> <code class="bash hljs">piranha-passwd</code> </pre><br>  After that, go to <a href="http://ip_lvs/">IP_LVS</a> : 3636 /, enter the login (piranha) and password from the step above and get into the admin area: <br><div class="spoiler">  <b class="spoiler_title">So to say admin</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/672/e89/27c/672e8927c90b4d11af3aff00b37f587b.png"><br></div></div><br>  Two tabs are interesting for us now - GLOBAL SETTINGS and VIRTUAL SERVERS <br>  Go to GLOBAL SETTINGS and set the Tunneling mode. <br><div class="spoiler">  <b class="spoiler_title">A small lyrical digression about OpenVZ</b> <div class="spoiler_text">  As you probably already know, if you worked with OpenVZ, the user is given a choice of two types of interfaces - venet and veth.  The principal difference between them is that veth is essentially a virtual network interface for each virtual machine with its MAC address.  Venet is a huge 3 level switch that all your machines are connected to. <br>  You can read more <a href="https://openvz.org/Differences_between_venet_and_veth">here</a> <br>  Comparative table of interfaces from the link above: <br><img src="https://habrastorage.org/files/7ba/8ef/27a/7ba8ef27ad1442e190a8c2423754bbcd.png"><br><br>  It so happened that venet is commonly used in my work, so the tuning is done on it. <br>  I‚Äôll say right away that I didn‚Äôt manage to configure LVS-Direcrt for this type of interface.  Everything is fixed on the fact that the virtual machine node receives traffic, but does not know which machine to send it to.  I‚Äôll stop on this in more detail when forwarding traffic inside the container. <br></div></div><br>  On the VIRTUAL SERVERS tab, we create one Virtual Server with the address 192.168.1.100 and port 80 <br>  In the same place, on the REAL SERVER tab, we will set two back end addresses with addresses 192.168.1.201 and 192.168.1.202 with ports 80 <br>  Details on the screenshots under the spoiler <br><div class="spoiler">  <b class="spoiler_title">Customization Screenshots</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/65f/d1e/26c/65fd1e26cc414df29774bda0cba65b54.png"><br><img src="https://habrastorage.org/files/f65/178/2b2/f651782b23b3446cba9463206cea25d1.png"><br></div></div><br>  The MONITORING SCRIPTS tab remains unchanged, although you can configure the very flexible operation of the availability check for nodes on it.  By default, this is just a request of type GET / HTTP / 1.0 and checking that the web server responded to us. <br>  You can execute arbitrary scripts, for example, such as under the spoiler. <br><div class="spoiler">  <b class="spoiler_title">Check scripts for various services</b> <div class="spoiler_text">  For example muskul <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/sh CMD=/usr/bin/mysqladmin IS_ALIVE=`timeout 2s $CMD -h $1 -P $2 ping | grep -c "alive"` if [ "$IS_ALIVE" = "1" ]; then echo "UP" else echo "DOWN" fi</span></span></code> </pre><br>  And the check in LVS is considered successful if the script returned UP and not successful if DOWN.  Server it displays the results of the test <br>  A piece of config for this check <br><pre> <code class="bash hljs"> expect = <span class="hljs-string"><span class="hljs-string">"UP"</span></span> use_regex = 0 send_program = <span class="hljs-string"><span class="hljs-string">"/opt/admin/mysql_chesk.sh %h 9005"</span></span></code> </pre><br></div></div><br>  Save the config, close the window and go to our usual console.  If all of a sudden you wondered what we pissed off there, then the service config is in /etc/sysconfig/ha/lvs.cf <br>  We look at the current settings: <br><pre> <code class="bash hljs">ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 192.168.1.100:80 wlc</code> </pre><br>  Not much.  This is because our final nodes are not raised!  Installation and creation of containers OpenVZ skip, we believe that you magically appeared two containers with addresses 192.168.1.201 and 192.168.1.202, and inside any web server on port 80 :) <br>  Again, we look at the output of the command: <br><pre> <code class="bash hljs">ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 192.168.1.100:80 wlc -&gt; 192.168.1.201:80 Tunnel 1 0 0 -&gt; 192.168.1.202:80 Tunnel 1 0 0</code> </pre><br>  Perfect!  Exactly what is needed.  If suddenly we turn off the web server on one of the nodes, then our balancer will honestly not send traffic there until the situation is corrected. <br>  Let's take a closer look at our config for LVS: <br><div class="spoiler">  <b class="spoiler_title">/etc/sysconfig/ha/lvs.cf</b> <div class="spoiler_text">  serial_no = 4 <br>  primary = 192.168.1.25 <br>  service = lvs <br>  network = tunnel <br>  debug_level = NONE <br>  virtual habrahabr { <br>  active = 1 <br>  address = 192.168.1.100 eth0: 1 <br>  port = 80 <br>  send = "GET / HTTP / 1.0 \ r \ n \ r \ n" <br>  expect = "HTTP" <br>  use_regex = 0 <br>  load_monitor = none <br>  scheduler = wlc <br>  protocol = tcp <br>  timeout = 6 <br>  reentry = 15 <br>  quiesce_server = 0 <br>  server test1 { <br>  address = 192.168.1.202 <br>  active = 1 <br>  weight = 1 <br>  } <br>  server test2 { <br>  address = 192.168.1.201 <br>  active = 1 <br>  weight = 1 <br>  } <br>  } <br></div></div><br>  The most interesting options here are timeout and reentry.  In the above configuration, if our backend does not respond to us within 6 seconds - we will not send anything there.  As soon as our bad guy responds to us within 15 seconds, we can send traffic there. <br>  There are still quiesce_server - if the server returns to the system, then all connection counters are reset to zero and connections begin to be distributed as after starting the service. <br>  LVS has its own Active-Pass mechanism, which is not considered in this article, and I don‚Äôt really like it.  I would recommend using Pacemaker, because  it has built-in mechanisms for throwing the pulse service (which is responsible for the whole mechanism) <br>  But back to reality. <br>  Our cars are seen, the balancer is ready to send traffic to them.  Let's do on LVS <pre> <code class="bash hljs">chkconfig pulse on</code> </pre><br>  and try to turn for example to our VIP: <br><br><div class="spoiler">  <b class="spoiler_title">Result</b> <div class="spoiler_text"><pre> <code class="bash hljs">curl -vv http://192.168.1.100 * Rebuilt URL to: http://192.168.1.100/ * About to connect() to 192.168.1.100 port 80 (<span class="hljs-comment"><span class="hljs-comment">#0) * Trying 192.168.1.100... * Adding handle: conn: 0x7e9aa0 * Adding handle: send: 0 * Adding handle: recv: 0 * Curl_addHandleToPipeline: length: 1 * - Conn 0 (0x7e9aa0) send_pipe: 1, recv_pipe: 0 * Connection timed out * Failed connect to 192.168.1.100:80; Connection timed out * Closing connection 0 curl: (7) Failed connect to 192.168.1.100:80; Connection timed out</span></span></code> </pre><br><img src="https://habrastorage.org/files/5d0/d71/1a2/5d0d711a2fb4474381354afa16fa64c3.gif"><br></div></div><br><br>  Let's figure it out!  There may be several reasons, and one of them is iptables on lvs.  Although he is engaged in the transfer of traffic, but the port must be available.  Armed with tcpdump and climb on LVS. <br>  Run and see: <br><pre> <code class="bash hljs">tcpdump -i any host 192.168.1.100 tcpdump: verbose output suppressed, use -v or -vv <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> full protocol decode listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes 13:36:09.802373 IP 192.168.1.18.37222 &gt; 192.168.1.100.http: Flags [S], seq 3328911904, win 29200, options [mss 1460,sackOK,TS val 2106524 ecr 0,nop,wscale 7], length 0 13:36:10.799885 IP 192.168.1.18.37222 &gt; 192.168.1.100.http: Flags [S], seq 3328911904, win 29200, options [mss 1460,sackOK,TS val 2106774 ecr 0,nop,wscale 7], length 0 13:36:12.803726 IP 192.168.1.18.37222 &gt; 192.168.1.100.http: Flags [S], seq 3328911904, win 29200, options [mss 1460,sackOK,TS val 2107275 ecr 0,nop,wscale 7], length 0</code> </pre><br>  Requests came, what became of them next? <br>  Ibid <br><pre> <code class="bash hljs">tcpdump -i any host 192.168.1.201 or host 192.168.1.202 tcpdump: verbose output suppressed, use -v or -vv <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> full protocol decode listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes 13:37:08.257049 IP 192.168.1.25 &gt; 192.168.1.201: IP 192.168.1.18.37293 &gt; 192.168.1.100.http: Flags [S], seq 1290874035, win 29200, options [mss 1460,sackOK,TS val 2121142 ecr 0,nop,wscale 7], length 0 (ipip-proto-4) 13:37:08.257538 IP 192.168.1.201 &gt; 192.168.1.25: ICMP 192.168.1.201 protocol 4 unreachable, length 88 13:37:09.255564 IP 192.168.1.25 &gt; 192.168.1.201: IP 192.168.1.18.37293 &gt; 192.168.1.100.http: Flags [S], seq 1290874035, win 29200, options [mss 1460,sackOK,TS val 2121392 ecr 0,nop,wscale 7], length 0 (ipip-proto-4) 13:37:09.256192 IP 192.168.1.201 &gt; 192.168.1.25: ICMP 192.168.1.201 protocol 4 unreachable, length 88</code> </pre><br>  And the traffic does not go ... Trouble!  We go to our nodes with OpenVZ, we go inside the virtualok and look at the traffic there.  Requests from LVS reached them, but cannot be processed - protocol 4 is our <a href="http://en.wikipedia.org/wiki/List_of_IP_protocol_numbers">IP-in-IP</a> <br>  We include support for tunnels for virtualok <br><pre> <code class="bash hljs"> : modprobe ipip    lsmod | grep ipip </code> </pre><br>  Do not forget to add them to autoload - <br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /etc/sysconfig/modules/ <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"#!/bin/sh"</span></span> &gt; ipip.modules <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"/sbin/modprobe ipip"</span></span> &gt;&gt; ipip.modules chmod +x ipip.modules</code> </pre><br>  Allow our virtual machines to have tunnel interfaces: <br><pre> <code class="bash hljs">vzctl <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> 201 --feature ipip:on --save vzctl <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> 202 --feature ipip:on --save</code> </pre><br>  After this, restart the containers. <br>  Now we need to add the address inside the containers to this (tun0) interface.  So do: <br><pre> <code class="bash hljs">ifconfig tunl0 192.168.1.100 netmask 255.255.255.255 broadcast 192.168.1.100</code> </pre><br>  Why is that? <br><div class="spoiler">  <b class="spoiler_title">Direct, Tunnel and Addresses</b> <div class="spoiler_text">  The common feature of these two methods is that in the final system (backend) VIP addresses are added for correct operation.  Why is that?  The answer is simple: the client addresses the fixed address and waits for the answer from him.  If someone answers him from another address, the client will consider such an answer as an error and simply ignore it.  Imagine that you are asking to call the cute girl Oksana to the telephone, and Valentina Yakovlevich‚Äôs hoarse voice answers you. <br>  For Direct, the package processing steps are lined up: <br>  The client makes an ARP request to the VIP address, receives a response, forms a request with data, sends it to the VIP, LVS caught these packets, changed the MAK there, gave it back to the network, the network equipment delivered the backend package to the Mac, he began to deploy it from the second level  Mak my?  Yes.  And my IP?  Yes.  I processed and answered with the source VIP (the package is intended for it) and at the destination client. <br>  For Tunnel, the situation is almost the same, but without the substitution of the MAC, and full encapsulated traffic.  The backend received a packet intended specifically for him, and inside a request to the VIP address, which the backend should process and respond. <br></div></div><br>  Now running tcpdump we will see the long-awaited requests! <br><pre> <code class="bash hljs">tcpdump -i any host 192.168.1.18 tcpdump: verbose output suppressed, use -v or -vv <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> full protocol decode listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes 14:03:28.907670 IP 192.168.1.18.38850 &gt; 192.168.1.100.http: Flags [S], seq 3110076845, win 29200, options [mss 1460,sackOK,TS val 2516581 ecr 0,nop,wscale 7], length 0 14:03:29.905359 IP 192.168.1.18.38850 &gt; 192.168.1.100.http: Flags [S], seq 3110076845, win 29200, options [mss 1460,sackOK,TS val 2516831 ecr 0,nop,wscale 7], length 0</code> </pre><br>  192.168.1.18 is the client. <br>  Request reached the car!  All cookies!  But stop early, continue.  Why does nobody answer us?  It's all about the tricky kernel configuration, which checks the path back to the source - <a href="http://tldp.org/HOWTO/Adv-Routing-HOWTO/lartc.kernel.rpf.html">rp_filter</a> <br>  Turn off this check for our interface inside the container: <br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> 0 &gt; /proc/sys/net/ipv4/conf/tunl0/rp_filter</code> </pre><br>  Checking: <br><pre> <code class="bash hljs">tcpdump -i any host 192.168.1.18 tcpdump: verbose output suppressed, use -v or -vv <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> full protocol decode listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes 14:07:03.870449 IP 192.168.1.18.39051 &gt; 192.168.1.100.http: Flags [S], seq 89280152, win 29200, options [mss 1460,sackOK,TS val 2570336 ecr 0,nop,wscale 7], length 0 14:07:03.870499 IP 192.168.1.100.http &gt; 192.168.1.18.39051: Flags [S.], seq 593110812, ack 89280153, win 14480, options [mss 1460,sackOK,TS val 3748869 ecr 2570336,nop,wscale 7], length 0</code> </pre><br>  Answers!  Answers!  But the miracle is still not happening.  Sorry, Mario, your princess is in another castle.  To go to another castle, first write down everything: <br>  Disable the rp_filter check and add an interface.  Inside containers: <br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"net.ipv4.conf.tunl0.rp_filter = 0"</span></span> &gt;&gt; /etc/sysctl.conf <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"ifconfig tunl0 192.168.1.100 netmask 255.255.255.255 broadcast 192.168.1.100"</span></span> &gt;&gt; /etc/rc.local</code> </pre><br>  And on the nodes: <br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"net.ipv4.conf.venet0.rp_filter = 0"</span></span> &gt;&gt; /etc/sysctl.conf</code> </pre><br>  Restart to confirm that everything is correct. <br>  As a result, the restart of the container should have the following picture: <br><pre> <code class="bash hljs">ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 16436 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: venet0: &lt;BROADCAST,POINTOPOINT,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN link/void inet 127.0.0.1/32 scope host venet0 inet 192.168.1.202/32 brd 192.168.1.202 scope global venet0:0 3: tunl0: &lt;NOARP,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UNKNOWN link/ipip 0.0.0.0 brd 0.0.0.0 inet 192.168.1.100/32 brd 192.168.1.100 scope global tunl0 cat /proc/sys/net/ipv4/conf/tunl0/rp_filter 0</code> </pre><br><br>  And the princess is hidden in the venet, as it is not sad.  The technology of this device imposes the following <a href="https://openvz.org/Virtual_network_device">limitations</a> : <br><blockquote>  The ip-packets of the ip-packets are set <b>to the ip-address of the container</b> . </blockquote><br>  Those.  Our node does not accept packets that come with left sors.  And now the main crutch - add this address to the container!  Let them be two addresses for each machine! <br><div class="spoiler">  <b class="spoiler_title">Illustration of such a decision</b> <div class="spoiler_text"><img src="https://habrastorage.org/storage2/2b1/651/807/2b1651807ce7629204f3d53d32ec859e.gif"><br></div></div><br>  On the nodes, execute: <br><pre> <code class="bash hljs">vzctl <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> 201 --ipadd 192.168.1.100 --save vzctl <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> 202 --ipadd 192.168.1.100 --save       ip ro del 192.168.1.100 dev venet0 scope link</code> </pre><br>  Of course, we‚Äôll get a varning that such an address is already in the grid!  But balancing requires sacrifice. <br>  Why do we need to delete routes - so that we do not broadcast this address to the network and other machines did not know about it.  Those.  Formally, all the requirements are met - the answer from the machine comes with the address 192.168.1.100, it has such an address.  We work! <br>  To simplify the work, I would like to recommend the <a href="">mount</a> engine of <a href="">scripts</a> in OpenVZ, but in its pure form it will not help us, because  The route of addresses is added after the mount operation, and the start scripts are executed inside the container. <br>  The solution came from the <a href="http://forum.openvz.org/index.php%3Ft%3Dmsg%26goto%3D38992%26">OpenVZ forum.</a> <br>  We make two files (example for one container): <br><pre> <code class="bash hljs">cat /etc/vz/conf/202.mount <span class="hljs-comment"><span class="hljs-comment">#!/bin/bash . /etc/vz/start_stript/202.sh &amp; disown exit 0 cat /etc/vz/start_stript/202.sh #!/bin/bash _sleep() { sleep 4 status=(`/usr/sbin/vzctl status 202`) x=1 until [ $x == 6 ] ; do sleep 1 if [ ${status[4]} == "running" ] ; then ip ro del 192.168.1.100 dev venet0 scope link exit 0 else x=`expr $x + 1` fi done } _sleep   : chmod +x /etc/vz/start_stript/202.sh chmod +x /etc/vz/conf/202.mount</span></span></code> </pre><br>  Restart the container for inspection, and now that moment has come - we open <a href="http://192.168.1.100/">192.168.1.100</a> iiiii ... VICTORY! <br><br>  A few more brief notes: <br>  1) The worst thing that happens with this balancing is when the address, carefully hung inside the container or on lo (for Direct mode), starts broadcasting to the network.  To prevent this scenario, two tools will help you - setting tests and <a href="http://en.wikipedia.org/wiki/Arptables">arptables</a> .  The tool is similar to iptables, but for ARP requests.  I actively use it for my own purposes - we prohibit certain arpas from falling into the network. <br>  2) This solution is not Enterprise;  replete with crutches and narrow places.  If you have the opportunity - use NAT, Direct and only then Tunnel.  This is due to the fact that, for example, in Direct - if the backend is active in the output of ipvsadm, then you will receive traffic.  Here he may not receive it, although the port is considered available and the packages will fly there. <br>  4) In normal virtualization (KVM, VmWare and others) - there will be no problems, as well as not with the use of veth devices. <br>  5) To diagnose any problems with LVS - use tcpdump.  And just use it too :) <br><br>  Thanks for attention! </div><p>Source: <a href="https://habr.com/ru/post/252415/">https://habr.com/ru/post/252415/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../252403/index.html">Budget SAN Storage on LSI Syncro Part 1</a></li>
<li><a href="../252405/index.html">How to rewrite a big project or refactoring that is painless for business</a></li>
<li><a href="../252407/index.html">Intel¬Æ Galileo Gen 2. Features of the beginning of operation</a></li>
<li><a href="../252409/index.html">PaintCode 2, a brief digression and notes</a></li>
<li><a href="../252411/index.html">Putting Wi-Fi robot</a></li>
<li><a href="../252417/index.html">Announcement of the book by Brian Kernigan "The Go Programming Language"</a></li>
<li><a href="../252419/index.html">C #: how not to "shoot yourself in the leg"</a></li>
<li><a href="../252421/index.html">A hundred lines of code for the beloved</a></li>
<li><a href="../252429/index.html">DICOM Viewer from the inside. Voxel render</a></li>
<li><a href="../252433/index.html">HSTS-based super cookies will track you even in private mode</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>