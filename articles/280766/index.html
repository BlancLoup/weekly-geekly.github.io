<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Bayesian neural network is now orange (part 2)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="What do you think, what's more in the orange - peel, or, hm, orange? 



 I suggest, if possible, go to the kitchen, take an orange, peel and check. I...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Bayesian neural network is now orange (part 2)</h1><div class="post__text post__text-html js-mediator-article">  What do you think, what's more in the orange - peel, or, hm, orange? <br><br><img src="https://habrastorage.org/files/571/35a/d1b/57135ad1be644885a92efc0c9c44cf9b.jpg"><br><br>  I suggest, if possible, go to the kitchen, take an orange, peel and check.  If laziness or not at hand - let's use boring mathematics: we remember the volume of the ball from school.  Let, say, the thickness of the peel is <img src="http://tex.s2cms.ru/svg/%5Cfrac%7B1%7D%7B20%7D">  from radius then <img src="http://tex.s2cms.ru/svg/V_%7B1%7D%3D%5Cfrac%7B4%7D%7B3%7D%5Cpi%20r%5E%7B3%7D">  , <img src="http://tex.s2cms.ru/svg/V_%7B2%7D%3D%5Cfrac%7B4%7D%7B3%7D%5Cpi%20%281.05%20r%29%5E%7B3%7D">  ;  subtract one from the other, divide the volume of the peel by the volume of the orange ... it turns out that the peel is about 16%.  Not so little, by the way. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      How about an orange in millennial space? <br><br>  Going to the kitchen this time will fail;  I suspect that not everyone knows the formula by heart, but <a href="https://en.wikipedia.org/wiki/Volume_of_an_n-ball">Wikipedia</a> helps us.  We repeat similar calculations, and with interest we find that: <br><br><ul><li>  firstly, in the thousand-dimensional hyperapellin the rind is larger than the pulp </li><li>  and secondly, it is about 246993291800602563115535632700000000000000 more than </li></ul><br>  That is, no matter how strange and controversial this may seem, but <i>almost the entire volume of hyperepel'sin is contained in a negligibly thin layer just below its surface.</i> <br><br>  Let's start with this, perhaps. <br><br><a name="habracut"></a><br><br><h4>  Synopsis </h4><br>  This is the second part of the <a href="https://habrahabr.ru/post/276355/">post</a> , where before that we stopped at the fact that we found the main Grail - the a posteriori probability of the model parameters.  Here it is, just in case: <img src="http://tex.s2cms.ru/svg/P%28w%20%7C%20D%29">  .  Remember again that <img src="http://tex.s2cms.ru/svg/w">  - these are the model parameters (neural network weights, for example), and <img src="http://tex.s2cms.ru/svg/D">  - data from dataset (I slightly changed the notation, earlier instead of <img src="http://tex.s2cms.ru/svg/w">  was <img src="http://tex.s2cms.ru/svg/%5Ctheta">  , but we will need theta later). <br><br>  So, the orange is just that.  The dimensions of this posterior grow with the same alarming speed, as the volume of hyperepel'sin;  the more parameters we have, the ‚Äúbigger‚Äù the distribution becomes.  In fact, it is probably better to imagine not even an orange - let's imagine a mountain.  In the millennial dimension.  Yes, I know that it is slightly inconsistent, but the enive is a mountain: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/797/77e/7d3/79777e7d3f6949b0a7e0dc02e4541321.png"></div><br>  <em>This is all one-dimensional, true.</em>  <em>According to the precepts of <a href="https://www.coursera.org/course/neuralnets">Hinton</a> , you can imagine a thousand-meter mountain like this: look at the picture above and say ‚Äúone thousand!‚Äù Loudly - or how many will be needed there.</em> <br><br>  Our challenge is to find out the volume of this mountain.  However, we: <br>  - we do not know what form it is (maybe any) <br>  - (so far) we have only one method of measurement - standing at a point, we can calculate the height to the foot (the probability of being at this point) <br>  - the surface of the mountain grows exponentially when it grows <img src="http://tex.s2cms.ru/svg/w">  - similar to how the skin of hyperepelin grows <br><br>  What is the plan? <br><br><h4>  Plan One: Sampling </h4><br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/c5b/66e/3f9/c5b66e3f9cfd486e869e5c3a17f0dbd7.png"></div><br><br>  We, generally speaking, do not need to measure the mountain right <em>at every</em> point.  We can choose several random parts of the mountain, take measurements and outline the result.  It will not be as accurate, but we will have to do much less measurements. <br>  It can be noticed quite quickly that this idea is promising, but it will not help us much when our multidimensional mountain begins to grow with an increase in the number of dimensions.  Let us recall that the surface of a mountain is equal to &lt;the number of values ‚Äã‚Äãin one dimension&gt; to the extent &lt;number of dimensions&gt;.  Sampling will help us reduce the basis of the degree - but the indicator is not going anywhere, and the problem still remains exponential. <br><br><h4>  Plan Two: Approximation </h4><br>  The main problem of measuring the mountain is not that it is large (in terms of the number of measurements) - after all, we have just counted the volume of a thousand-meter orange without much difficulty using the formula from Wikipedia.  The problem is that <em>there is no formula</em> for the mountain.  We do not know the analytical rule by which the mountain grows (unlike orange - it grows evenly in all directions). <br><br>  Well, okay, but what if we build <em>another</em> mountain of ours, which will be similar to the desired one, but this time with the formula?  Like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/3e7/f93/745/3e7f93745c494aa5ba1c28b8576fa6d7.png"></div><br><br>  Well ... actually the devil knows.  First of all, it‚Äôs not very clear yet, the approximation can be so accurate - it doesn‚Äôt look very much in the picture.  Secondly, we do not yet know how to do this at all.  So let's start and see. <br><br><h4>  Laplace approximation </h4><br>  Once again we will draw our Bayes theorem, just a little bit different: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0A%20%20%20%20P%28w%20%7C%20D%29%20%3D%20%5Cfrac%7B1%7D%7BP%28D%29%7D%20P%28D%20%7C%20w%29P%28w%29%20%3D%20%5Cfrac%7B1%7D%7BP%28D%29%7D%20P%28D%2C%20w%29%0A"></div><br>  I just collected the product of two probabilities to the right of the fraction into one - the so-called joint probability (and once again I remind you that we slightly changed the notation to w instead of theta).  People often view it in the sense that <img src="http://tex.s2cms.ru/svg/P%28D%2C%20w%29">  - our "desired" distribution, and <img src="http://tex.s2cms.ru/svg/%5Cfrac%7B1%7D%7BP%28D%29%7D">  - the normalizing constant, which is needed to sum the result to one.  So let's temporarily focus on searching. <img src="http://tex.s2cms.ru/svg/P%28D%2C%20w%29">  . <br><br>  We say "approximation", remember <a href="https://en.wikipedia.org/wiki/Taylor_series">the Taylor series</a> .  In accordance with the first course of mathematical analysis, any function is decomposed into an infinite sum of defined polynomials.  We write our function <img src="http://tex.s2cms.ru/svg/P%28D%2C%20w%29">  as <img src="http://tex.s2cms.ru/svg/f%28x%29">  , and we will lay out at some point <img src="http://tex.s2cms.ru/svg/x%5E%7B%2A%7D">  which coincides with the maximum distribution (we do not know where it is, but this is not important yet).  And at the same time, we saw off an infinite sum after a term with a power of two: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0A%20%20%20%20f%28x%29%20%5Capprox%20f%28x%5E%7B%2A%7D%29%20%2B%20f%27%28x%5E%7B%2A%7D%29%28x-x%5E%7B%2A%7D%29%20%2B%20%5Cfrac%7B1%7D%7B2%7Df%27%27%28x%5E%7B%2A%7D%29%28x-x%5E%7B%2A%7D%29%5E%7B2%7D%0A"></div><br>  (this is a straightforward one-to-one decomposition in a Taylor series from Wikipedia) <br><br>  Now we remember that we have chosen <img src="http://tex.s2cms.ru/svg/x%5E%7B%2A%7D">  as a point of maximum, and therefore, the derivative in it is zero.  That is, the second term can be thrown out with a clear conscience.  Get <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0A%20%20%20%20f%28x%29%20%5Capprox%20f%28x%5E%7B%2A%7D%29%20%2B%20%5Cfrac%7B1%7D%7B2%7D%28f%27%27%28x%5E%7B%2A%7D%29%28x-x%5E%7B%2A%7D%29%5E%7B2%7D%29%0A"></div><br>  Nothing like?  In fact, this piece has the same shape as the logarithm from the Gaussian.  To see this, you can write <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0A%20%20%20%20%5Clog%20%5Cmathcal%7BN%7D%28%5Ctheta%20%7C%20%5Cmu%2C%20%5Csigma%29%20%3D%20%5Clog%20%5Cfrac%7B1%7D%7B%5Csigma%20%5Csqrt%7B2%5Cpi%7D%7D%20-%5Cfrac%7B1%7D%7B2%7D%28%5Cfrac%7B1%7D%7B%5Csigma%5E%7B2%7D%7D%28%5Ctheta-%5Cmu%29%5E%7B2%7D%29%0A"></div><br>  Slightly change the notation: we wanted to choose <img src="http://tex.s2cms.ru/svg/f%28x%29%3DP%28D%2C%20w%29">  and now let it be <img src="http://tex.s2cms.ru/svg/f%28x%29%3D%5Clog%20P%28D%2C%20w%29">  .  Then <img src="http://tex.s2cms.ru/svg/f%28x%29%20%3D%20%5Clog%20%5Cmathcal%7BN%7D%28%5Ctheta%20%7C%20%5Cmu%2C%20%5Csigma%29%20%5Capprox%20%5Clog%20P%28D%2C%20w%29">  and it turns out that our desired joint probability can be approximated by a Gaussian with a center at the maximum point and a standard deviation <img src="http://tex.s2cms.ru/svg/-%5Cfrac%7B1%7D%7Bf%27%27%28x%5E%7B%2A%7D%29%7D">  (inverse second derivative at the maximum point or curvature). <br><br>  If it was too many incomprehensible symbols for one section, sum up the idea in three short paragraphs.  So, if we want to find our volume of a mountain, we need: <br>  - find the maximum point <br>  - measure the curvature in it (calculate the second derivative; only attention: the derivative with respect to <img src="http://tex.s2cms.ru/svg/%5Clog%20P%28D%2C%20w%29">  , but not <img src="http://tex.s2cms.ru/svg/P%28D%2C%20w%29">  ) <br>  - take a normal curve with a center at the maximum point and a standard deviation - negative-inverse to the curvature. <br><br>  We are able to search for the maximum point of the mountain and it is much easier than measuring it all.  Actually, let's apply magic to the same Bayes regression with which we worked in the last post! <br><br>  We searched for the maximum point at the end of the last post ‚Äî this is the center of the ‚ÄúBayesian beam‚Äù, ‚Äúthe hottest line‚Äù, etc.  I, just in case, once again give you how to find it, but I will hide it under the spoiler so as not to scare people with formulas: <br><br><div class="spoiler">  <b class="spoiler_title">Spoiler header</b> <div class="spoiler_text"><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0A%20%20%20%20argmax%20%5C%3A%20%5B%5Csum_%7Bi%7D%20%5Clog%20%5Cmathcal%7BN%7D%28y_%7Bi%7D%20%7C%20b%20%2B%20%5Cmathbf%7Bw%5E%7BT%7Dx_%7Bi%7D%7D%2C%5Ctheta%5E%7B2%7D%29%20%2B%20%5Csum_%7Bj%7D%20%5Clog%20%5Cmathcal%7BN%7D%28w_%7Bj%7D%20%7C%200%2C%20%5Ctau%5E%7B2%7D%29%5D%0A"></div><br>  Here the first part under argmaks is likelihood, and the second is Gaussian prior.  The product was turned into a sum, taking the logarithm, as usual. <br></div></div><br>  Derivative by <img src="http://tex.s2cms.ru/svg/w">  you can calculate it analytically, but since I'm a terrible bummer, I'd rather hammer all this into a python and let aurograd do the work for me. <br><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># b  w       ,      #       ext_w = np.hstack([b, w]) #      #     ,  ext_data.dot(ext_w) #  data.dot(w) + b ext_data = np.ones((data.shape[0], data.shape[1] + 1)) ext_data[:, 1:] = data #   log joint,      #        def log_joint(w): regression_value = ext_data.dot(w) return ( np.sum(-np.power(target - regression_value, 2.) / (2 * np.power(1., 2.))) + np.sum(-np.power(w, 2.) / (2 * np.power(1., 2.))) ) from autograd import elementwise_grad second_grad_log_joint = elementwise_grad(elementwise_grad(log_joint)) mu = ext_w sigma = -1. / second_grad_log_joint(ext_w) cov = np.diag(sigma) # ,      # some_value = multivariate_normal.pdf(some_point, mu, cov)...</span></span></code> </pre> <br><br>  When we have a distribution, drawing it is a matter of technique;  We use approximately the same method as in the last part of the post with a slight modification (sample distribution at different distances from the center).  We get just such a pretty picture: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/aa7/bf3/a87/aa7bf3a879bf4373906ec65b1bf09959.png"></div><br><br>  And by the way, each curve here is a 15 degree polynomial.  The old ‚Äúbrute force‚Äù Bayes regression would have long since leaned back to lie down for several years of computer time (there I somehow squeezed the fifth degree). <br><br>  The approximation of Laplace is good for everyone - quickly, conveniently, beautifully - but one thing is bad: it is estimated by the maximum point.  This is how to judge all the grief, looking at it from the top - despite the fact that we do not know whether we are really on top or stuck in a local maximum, and despite what is far from all visible (suddenly we are very flat and beautiful, and under it three kilometers of a continuous cliff?).  In general, they invented it for neural networks, as you might guess, not in the time of Laplace, but even in 1991, but since then it has not particularly won the world.  So let's look at something more fashionable and beautiful. <br><br><h4>  Bayes by backprop: the beginning </h4><br>  Finally we got to that method from the heading article DeepMind.  The authors called it Bayes by backprop - good luck with the translation into Russian, yeah.  Reverse Bayesoraspread? <br><br>  The starting point here is this: imagine that we have some kind of approximation <img src="http://tex.s2cms.ru/svg/q%28w%20%7C%20%5Ctheta%29">  (this is where the theta symbol came in handy - these will be our approximation distribution parameters), and let it be some simple form, also Gaussian, say.  The trick is this: try to write down the expression of some ‚Äúdistance‚Äù from our approximation to the original mountain, and minimize this distance.  Since both values ‚Äã‚Äãare probability distributions, we zayuzay thing, which was specially invented to compare distributions: <a href="https://en.wikipedia.org/wiki/Kullback%25E2%2580%2593Leibler_divergence">Kullback-Leibler distance</a> .  In fact, it just sounds a little scary, and so this is just this: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0AKL%5Bq%28w%20%7C%20%5Ctheta%29%20%7C%7C%20P%28w%20%7C%20D%29%5D%20%3D%20%5Cint_%7Bw%7D%20q%28w%20%7C%20%5Ctheta%29%20%5Clog%20%5Cfrac%7Bq%28w%20%7C%20%5Ctheta%29%7D%7BP%28w%20%7C%20D%29%7D%20dw%0A"></div><br>  If you carefully look at the integral, it becomes clear that it looks like a <a href="https://en.wikipedia.org/wiki/Expected_value">mathematical expectation</a> - under the integral is some kind of thing multiplied by <img src="http://tex.s2cms.ru/svg/q%28w%20%7C%20%5Ctheta%29">  , and the integral is taken by <img src="http://tex.s2cms.ru/svg/w">  .  Then you can write it like this: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0AE_%7Bq%28w%20%7C%20%5Ctheta%29%7D%5B%5Clog%20%5Cfrac%7Bq%28w%20%7C%20%5Ctheta%29%7D%7BP%28w%20%7C%20D%29%7D%5D%0A"></div><br>  We go further: in the denominator we have <img src="http://tex.s2cms.ru/svg/P%28w%20%7C%20D%29">  , and we know that by the Bayes theorem it is <img src="http://tex.s2cms.ru/svg/%5Cfrac%7BP%28D%20%7C%20w%29P%28w%29%7D%7BP%28D%29%7D">  .  Put it in the formula above and note that under the expectation is <img src="http://tex.s2cms.ru/svg/P%28D%29">  - which does not depend on <img src="http://tex.s2cms.ru/svg/q">  , and we can take it out of the expectations: <br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0AE_%7Bq%28w%20%7C%20%5Ctheta%29%7D%5B%5Clog%20%5Cfrac%7Bq%28w%20%7C%20%5Ctheta%29P%28D%29%7D%7BP%28D%20%7C%20w%29P%28w%29%7D%5D%20%3D%20E_%7Bq%28w%20%7C%20%5Ctheta%29%7D%5B%5Clog%20%5Cfrac%7Bq%28w%20%7C%20%5Ctheta%29%7D%7BP%28D%20%7C%20w%29P%28w%29%7D%5D%20%2B%20%5Clog%20P%28D%29%0A"></div><br><br>  This we did a very cool thing.  Because if you have not forgotten, this whole thing is equal to the difference between the approximation and our mountain.  The controlled parameter here is <img src="http://tex.s2cms.ru/svg/%5Ctheta">  , and we want to adjust it so that the difference is minimal.  So, in the process of this minimization, we do not need to worry about <img src="http://tex.s2cms.ru/svg/P%28D%29"><br>  - because it does not depend on <img src="http://tex.s2cms.ru/svg/%5Ctheta">  .  And this, by the way, was the main problem component of our posterior - in order to calculate it, we just had to go around all the points of the mountain and put them together (because <img src="http://tex.s2cms.ru/svg/P%28D%29%20%3D%20%5Cint_%7Bw%7DP%28D%20%7C%20w%29P%28w%29dw">  total for all <img src="http://tex.s2cms.ru/svg/w">  , and there‚Äôs no other way to get it). <br><br>  However, we go further.  We are interested in finding the minimum of what is left: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0Aargmin%20%5C%3A%20E_%7Bq%28w%20%7C%20%5Ctheta%29%7D%5B%5Clog%20%5Cfrac%7Bq%28w%20%7C%20%5Ctheta%29%7D%7BP%28D%20%7C%20w%29P%28w%29%7D%5D%20%3D%20argmin%20%5C%3A%20E_%7Bq%28w%20%7C%20%5Ctheta%29%7D%5B%5Clog%20q%28w%20%7C%20%5Ctheta%29%20-%20%5Clog%20P%28D%20%7C%20w%29%20-%20%5Clog%20P%28w%29%5D%0A"></div><br>  How do we usually look for lows?  Well, we take the derivative and make a gradient descent.  Any idea how to get a derivative of this stuff?  I still have something very. <br><br><h4>  Bayes by backprop: continued </h4><br>  Then comes the part in which I could not intuitively enter, so I have to glue my teeth and follow the math.  The part that allows us to take a derivative is called reparameterization trick, and consists of the following steps: <br><br><ol><li>  Suppose we took some random variable <img src="http://tex.s2cms.ru/svg/%5Cepsilon">  .  We don‚Äôt know anything about her, except that she‚Äôs chosen so that <img src="http://tex.s2cms.ru/svg/q%28w|%5Ctheta%29dw%20%3D%20q%28%5Cepsilon%29d%5Cepsilon">  .  Here is her property, and everything is about her. </li><li>  Generally speaking, the derivative of the expectation is not as terrible as it seems: it is only a derivative of the integral, that is, roughly speaking, of the sum where the summation goes over <img src="http://tex.s2cms.ru/svg/w">  .  In and of itself, it is not scary, but tedious, because again we are returning to the problem called ‚Äúpile up all the points of the infinitely growing <img src="http://tex.s2cms.ru/svg/w">  ".  Let's imagine in general terms that under the expectation of some kind of function <img src="http://tex.s2cms.ru/svg/f%28w%2C%20%5Ctheta%29">  , and write our derivative: </li></ol><br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0A%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Ctheta%7DE_%7Bq%28w%20%7C%20%5Ctheta%29%7D%5Bf%28w%2C%20%5Ctheta%29%5D%20%3D%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Ctheta%7D%20%5Cint_%7Bw%7D%20f%28w%2C%20%5Ctheta%29%20q%28w%20%7C%20%5Ctheta%29%20dw%0A"></div><br>  We did not do anything special - we just uncovered the expectation of Wikipedia by definition, recorded it again as an integral.  But hey, see <img src="http://tex.s2cms.ru/svg/q%28w%20%7C%20%5Ctheta%29%20dw">  in the end?  We just said it‚Äôs equal <img src="http://tex.s2cms.ru/svg/q%28%5Cepsilon%29d%5Cepsilon">  .  And-iii ... <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0A%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Ctheta%7D%20%5Cint_%7Bw%7D%20f%28w%2C%20%5Ctheta%29%20q%28w%20%7C%20%5Ctheta%29%20dw%20%3D%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Ctheta%7D%20%5Cint_%7Bw%7D%20f%28w%2C%20%5Ctheta%29%20q%28%5Cepsilon%29%20d%20%5Cepsilon%0A"></div><br>  Now our integral is done by epsilon, which means we can drive the derivative with respect to <img src="http://tex.s2cms.ru/svg/%5Ctheta"><br>  under the sign of the integral. <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0A%5Cint_%7Bw%7D%20%5Cfrac%7B%5Cpartial%20f%28w%2C%20%5Ctheta%29%7D%7B%5Cpartial%20%5Ctheta%7D%20q%28%5Cepsilon%29%20d%20%5Cepsilon%20%3D%20E_%7Bq%28%5Cepsilon%29%7D%5B%5Cfrac%7B%5Cpartial%20f%28w%2C%20%5Ctheta%29%7D%7B%5Cpartial%20%5Ctheta%7D%5D%0A"></div><br>  Here is the reparametrization trick: we had a derivative of the expectation, it was the expectation of the derivative.  Cool?  I suspect that if you are still following my slow finger movements along the lines, then you are not very cool, and maybe even not very clear what has changed - we need to integrate one FIG. <img src="http://tex.s2cms.ru/svg/w">  What have we achieved at all? <br><br>  The bottom line is that now we can approximate this integral at several points (samples).  But before they could not.  Previously, the derivative had to be taken from the <em>entire</em> amount: first to collect a bunch of points together, and then to differentiate it, and there any inaccuracy in collecting could lead us away when differentiating is far from the wrong direction.  And now we can differentiate at points, and only <em>then</em> sum up the results - and this means that we can easily do with a partial sum. <br><br>  These are the harsh orders in the world of approximate inference: here we need approximations in order to calculate the <em>derivative</em> , not even the posterior-distribution itself.  Well, nothing, we are almost there. <br><br><h4>  Bayes by backprop: algorithm </h4><br>  Until now, we have been <img src="http://tex.s2cms.ru/svg/%5Ctheta">  as abstract "parameters": it is time to clarify them.  Let our approximation <img src="http://tex.s2cms.ru/svg/q%28w%20%7C%20%5Ctheta%29">  will be gaussian: then imagine <img src="http://tex.s2cms.ru/svg/%5Ctheta">  as <img src="http://tex.s2cms.ru/svg/%5Cmu">  - average value, and <img src="http://tex.s2cms.ru/svg/%5Csigma">  - standard deviation.  We also need to decide on <img src="http://tex.s2cms.ru/svg/%5Cepsilon">  - let's make it a Gaussian random variable with center at zero and standard deviation 1, i.e. <img src="http://tex.s2cms.ru/svg/%5Cmathcal%7BN%7D%280%2C%201%29">  .  Then in order to fulfill the conditions for reparameterization, let <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/w%3D%5Cmu%20%2B%20%5Csigma%20%5Ccirc%20%5Cepsilon"></div><br>  ( <img src="http://tex.s2cms.ru/svg/%5Ccirc">  here means elementwise multiplication).  For me, however, it‚Äôs not very obvious why <img src="http://tex.s2cms.ru/svg/q%28w|%5Ctheta%29dw%20%3D%20q%28%5Cepsilon%29d%5Cepsilon">  (if someone can explain on the fingers in the comments - I will be very grateful), but take this piece of article on faith. <br><br>  In total, our reverse bayesoraspread consists of the following steps: <br><br><ol><li>  We start with random <img src="http://tex.s2cms.ru/svg/%5Cmu">  and <img src="http://tex.s2cms.ru/svg/%5Csigma">  for each neural net weight (or regression coefficient) </li><li>  Sample a little <img src="http://tex.s2cms.ru/svg/%5Cepsilon">  of <img src="http://tex.s2cms.ru/svg/%5Cmathcal%7BN%7D%280%2C%201%29"></li><li>  We get from it <img src="http://tex.s2cms.ru/svg/w%3D%5Cmu%20%2B%20%5Csigma%20%5Ccirc%20%5Cepsilon"><br></li><li>  We recall our function of the distance between the "true mountain" and the approximation.  We once designated it as <img src="http://tex.s2cms.ru/svg/f%28w%2C%20%5Ctheta%29">  but it is good <img src="http://tex.s2cms.ru/svg/%5Clog%20q%28w%20%7C%20%5Ctheta%29%20-%20%5Clog%20P%28D%20%7C%20w%29%20-%20%5Clog%20P%28w%29">  .  I'll write her below anyway <img src="http://tex.s2cms.ru/svg/f%28w%2C%20%5Ctheta%29">  , it's easier (keep in mind that <img src="http://tex.s2cms.ru/svg/%5Ctheta%3D%28%5Cmu%2C%20%5Csigma%29">  ). </li><li>  We consider the derivative with respect to <img src="http://tex.s2cms.ru/svg/%5Cmu">  .  It will be: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0A%20%20%20%20%5CDelta_%7B%5Cmu%7D%20%3D%20%5Cfrac%7B%5Cpartial%20f%28w%2C%20%5Ctheta%29%7D%7B%5Cpartial%20w%7D%20%2B%20%5Cfrac%7B%5Cpartial%20f%28w%2C%20%5Ctheta%29%7D%7B%5Cpartial%20%5Cmu%7D%0A"></div><br>  (where is the plus? because <img src="http://tex.s2cms.ru/svg/f">  - a function of two variables, and both depend on <img src="http://tex.s2cms.ru/svg/%5Cmu">  ) <br></li><li>  We consider the derivative with respect to <img src="http://tex.s2cms.ru/svg/%5Csigma">  : <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0A%20%20%20%20%5CDelta_%7B%5Csigma%7D%20%3D%20%5Cfrac%7B%5Cpartial%20f%28w%2C%20%5Ctheta%29%7D%7B%5Cpartial%20w%7D%20%5Cepsilon%20%2B%20%5Cfrac%7B%5Cpartial%20f%28w%2C%20%5Ctheta%29%7D%7B%5Cpartial%20%5Csigma%7D%0A"></div></li><li>  There are derivatives - it is figuratively wearing a hat.  Next good old gradient descent: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0A%20%20%20%20%5Cmu%20%5Cleftarrow%20%20%5Cmu%20-%20%5Calpha%20%5CDelta_%7B%5Cmu%7D%20%5C%5C%0A%20%20%20%20%5Csigma%20%5Cleftarrow%20%20%5Csigma%20-%20%5Calpha%20%5CDelta_%7B%5Csigma%7D%0A"></div></li></ol><br><br><h4>  Any results </h4><br>  I don‚Äôt know about you, but I‚Äôm already tired of this black and yellow bundle, so let's skip this mandatory stage and do something more like a neural network.  The authors of the original article obtained nice results on MNIST-digits without any additional perversions such as using convolutional networks - let's try to get close to them.  And perhaps it is time to put aside cute autograd to the side and arm yourself with something heavier like Theano.  There will be a little bit of Theano-specific code, so if you are not targeting it, feel free to browse through it. <br><br>  We get the data: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.datasets <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> fetch_mldata <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.cross_validation <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np mnist = fetch_mldata(<span class="hljs-string"><span class="hljs-string">'MNIST original'</span></span>) N = <span class="hljs-number"><span class="hljs-number">5000</span></span> data = np.float32(mnist.data[:]) / <span class="hljs-number"><span class="hljs-number">255.</span></span> idx = np.random.choice(data.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], N) data = data[idx] target = np.int32(mnist.target[idx]).reshape(N, <span class="hljs-number"><span class="hljs-number">1</span></span>) train_idx, test_idx = train_test_split(np.array(range(N)), test_size=<span class="hljs-number"><span class="hljs-number">0.05</span></span>) train_data, test_data = data[train_idx], data[test_idx] train_target, test_target = target[train_idx], target[test_idx] train_target = np.float32(preprocessing.OneHotEncoder(sparse=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>).fit_transform(train_target))</code> </pre><br>  We declare the parameters.  Here we apply a little trick, which is in the article, but I deliberately did not tell.  The point is this: for each network weight, we have two parameters ‚Äî mu and sigma, right?  A small problem may arise from the fact that sigma must always be greater than zero (this is the standard deviation of the Gaussian, which cannot be negative by definition).  First, how to initialize it?  Well, you can take random numbers from something very close to zero (like 0.0001) to unity.  Secondly, and if she does not stop in the process of gradient descent below zero?  Well, it should not be like that, although at the expense of any arithmetic inaccuracies after the point, it can and.  In general, the authors suggested solving this elegantly - we will replace sigma with the logarithm of sigma, and make the appropriate amendment to the weights formula: <br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/w%3D%5Cmu%20%2B%20%5Clog%20%28e%5E%7B%5Clog%20%5Csigma%7D%20%2B%201%29%20%20%5Ccirc%20%5Cepsilon"></div><br>  (why is it under the logarithm +1? Apparently, with the same goal - in order not to accidentally take the logarithm from zero). <br><br>  Oh, and we used to talk all the time about scales as <img src="http://tex.s2cms.ru/svg/w">  , but we often have our own nomenclature in neural networks - zero weight, <img src="http://tex.s2cms.ru/svg/w_%7B0%7D">  called there <img src="http://tex.s2cms.ru/svg/b">  (bias).  We will not break the agreement.  So: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">init</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(shape)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.asarray( np.random.normal(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0.05</span></span>, size=shape), dtype=theano.config.floatX ) n_input = train_data.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-comment"><span class="hljs-comment"># L1 n_hidden_1 = 200 W1_mu = theano.shared(value=init((n_input, n_hidden_1))) W1_logsigma = theano.shared(value=init((n_input, n_hidden_1))) b1_mu = theano.shared(value=init((n_hidden_1,))) b1_logsigma = theano.shared(value=init((n_hidden_1,))) # L2 n_hidden_2 = 200 W2_mu = theano.shared(value=init((n_hidden_1, n_hidden_2))) W2_logsigma = theano.shared(value=init((n_hidden_1, n_hidden_2))) b2_mu = theano.shared(value=init((n_hidden_2,))) b2_logsigma = theano.shared(value=init((n_hidden_2,))) # L3 n_output = 10 W3_mu = theano.shared(value=init((n_hidden_2, n_output))) W3_logsigma = theano.shared(value=init((n_hidden_2, n_output))) b3_mu = theano.shared(value=init((n_output,))) b3_logsigma = theano.shared(value=init((n_output,)))</span></span></code> </pre><br>  This logsigm needs to somehow be able to cram it into the formula for a normal distribution, so we will do our function for it.  At the same time and the usual declare: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">log_gaussian</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x, mu, sigma)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">-0.5</span></span> * np.log(<span class="hljs-number"><span class="hljs-number">2</span></span> * np.pi) - T.log(T.abs_(sigma)) - (x - mu) ** <span class="hljs-number"><span class="hljs-number">2</span></span> / (<span class="hljs-number"><span class="hljs-number">2</span></span> * sigma ** <span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">log_gaussian_logsigma</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x, mu, logsigma)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">-0.5</span></span> * np.log(<span class="hljs-number"><span class="hljs-number">2</span></span> * np.pi) - logsigma / <span class="hljs-number"><span class="hljs-number">2.</span></span> - (x - mu) ** <span class="hljs-number"><span class="hljs-number">2</span></span> / (<span class="hljs-number"><span class="hljs-number">2.</span></span> * T.exp(logsigma))</code> </pre><br>  It is time to evaluate our probabilities.  We do this, as we did before, by sampling - that is, we are spinning in a cycle, at each iteration we get a random-initializing epsilon, turn it into weights and put it together.  In general, for the cycles in Theano there is a scan, but: 1) it was clearly developed by a team of manic inquisitors in order to break the brain of the user as much as possible and 2) for a small number of iterations, the usual cycle will fit us.  Total: <br><br><pre> <code class="python hljs">n_samples = <span class="hljs-number"><span class="hljs-number">10</span></span> log_pw, log_qw, log_likelihood = <span class="hljs-number"><span class="hljs-number">0.</span></span>, <span class="hljs-number"><span class="hljs-number">0.</span></span>, <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(n_samples): epsilon_w1 = get_random((n_input, n_hidden_1), avg=<span class="hljs-number"><span class="hljs-number">0.</span></span>, std=sigma_prior) epsilon_b1 = get_random((n_hidden_1,), avg=<span class="hljs-number"><span class="hljs-number">0.</span></span>, std=sigma_prior) W1 = W1_mu + T.log(<span class="hljs-number"><span class="hljs-number">1.</span></span> + T.exp(W1_logsigma)) * epsilon_w1 b1 = b1_mu + T.log(<span class="hljs-number"><span class="hljs-number">1.</span></span> + T.exp(b1_logsigma)) * epsilon_b1 epsilon_w2 = get_random((n_hidden_1, n_hidden_2), avg=<span class="hljs-number"><span class="hljs-number">0.</span></span>, std=sigma_prior) epsilon_b2 = get_random((n_hidden_2,), avg=<span class="hljs-number"><span class="hljs-number">0.</span></span>, std=sigma_prior) W2 = W2_mu + T.log(<span class="hljs-number"><span class="hljs-number">1.</span></span> + T.exp(W2_logsigma)) * epsilon_w2 b2 = b2_mu + T.log(<span class="hljs-number"><span class="hljs-number">1.</span></span> + T.exp(b2_logsigma)) * epsilon_b2 epsilon_w3 = get_random((n_hidden_2, n_output), avg=<span class="hljs-number"><span class="hljs-number">0.</span></span>, std=sigma_prior) epsilon_b3 = get_random((n_output,), avg=<span class="hljs-number"><span class="hljs-number">0.</span></span>, std=sigma_prior) W3 = W3_mu + T.log(<span class="hljs-number"><span class="hljs-number">1.</span></span> + T.exp(W3_logsigma)) * epsilon_w3 b3 = b3_mu + T.log(<span class="hljs-number"><span class="hljs-number">1.</span></span> + T.exp(b3_logsigma)) * epsilon_b3 a1 = nonlinearity(T.dot(x, W1) + b1) a2 = nonlinearity(T.dot(a1, W2) + b2) h = T.nnet.softmax(nonlinearity(T.dot(a2, W3) + b3)) sample_log_pw, sample_log_qw, sample_log_likelihood = <span class="hljs-number"><span class="hljs-number">0.</span></span>, <span class="hljs-number"><span class="hljs-number">0.</span></span>, <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> W, b, W_mu, W_logsigma, b_mu, b_logsigma <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> [(W1, b1, W1_mu, W1_logsigma, b1_mu, b1_logsigma), (W2, b2, W2_mu, W2_logsigma, b2_mu, b2_logsigma), (W3, b3, W3_mu, W3_logsigma, b3_mu, b3_logsigma)]: <span class="hljs-comment"><span class="hljs-comment"># first weight prior log_pw += log_gaussian(W, 0., sigma_prior).sum() log_pw += log_gaussian(b, 0., sigma_prior).sum() # then approximation log_qw += log_gaussian_logsigma(W, W_mu, W_logsigma * 2).sum() log_qw += log_gaussian_logsigma(b, b_mu, b_logsigma * 2).sum() # then the likelihood log_likelihood += log_gaussian(y, h, sigma_prior).sum() log_qw /= n_samples log_pw /= n_samples log_likelihood /= n_samples</span></span></code> </pre><br>  Fuh.  Now we collect objective.  Somewhere in this place in the post there should be a pause for two weeks, because the objective that is proposed in the article (something like <code>(log_qw - log_pw - log_likelihood / M).sum()</code> ) didn‚Äôt work for me and gave a very bad results.  Then at some point I realized to finish the article to the end and found that the authors advise working with minibats and averaging the objective in a certain way.  More precisely, even this: <br><br><pre> <code class="python hljs">objective = ((<span class="hljs-number"><span class="hljs-number">1.</span></span> / n_batches) * (log_qw - log_pw) - log_likelihood).sum() / batch_size</code> </pre><br>  At the same time advised to use the Adam optimizer instead of the usual gradient descent.  I have never used it in my life, so we will resist the temptation to write it ourselves and use the ready-made one. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> lasagne.updates <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> adam all_params = [ W1_mu, W1_logsigma, b1_mu, b1_logsigma, W2_mu, W2_logsigma, b2_mu, b2_logsigma, W3_mu, W3_logsigma, b3_mu, b3_logsigma ] updates = adam(objective, all_params, learning_rate=<span class="hljs-number"><span class="hljs-number">0.001</span></span>)</code> </pre><br>  Well, then everything is standard - the train-function and go directly to learn.  All code can be viewed <a href="https://gist.github.com/rocknrollnerd/c5af642cf217971d93f499e8f70fcb72">here</a> .  A great percentage of accuracy is not there, it is true, but also the bread. <br><br><pre> epoch 0 cost 6.83701634889 Accuracy 0.764
 epoch 1 cost -73.3193287832 Accuracy 0.876
 epoch 2 cost -89.2973277879 Accuracy 0.9
 epoch 3 cost -95.9793596695 Accuracy 0.924
 epoch 4 cost -100.416764595 Accuracy 0.924
 epoch 5 cost -104.000705026 Accuracy 0.928
 epoch 6 cost -10.16.166556952 Accuracy 0.936
 epoch 7 cost -110.469004896 Accuracy 0.928
 epoch 8 cost -112.143595876 Accuracy 0.94
 epoch 9 cost -113.680839646 Accuracy 0.948
</pre><br><br>  Leave the <a href="http://arxiv.org/pdf/1505.05424v2.pdf">link to the article</a> , just in case, once again, because there are a lot of interesting things there: how to move from approximation by Gaussians to something more complicated, or for example, how to use this thing in reinforcement learning (well, this is DeepMind, at the end all). <br><br><h4>  Q &amp; A </h4><br><ol><li>  There, in the article and in general everywhere, the word ‚Äúvariational‚Äù is often used, and nothing is said about it in the post. <br><br>  Here, to my shame, I was simply ashamed: at one time in school I didn‚Äôt have variational calculus, and I‚Äôm a little bit wary of using unfamiliar terms, especially since you can do without them.  But in general, yes: you can read about the approach as a whole in the section on <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">variational Bayesian methods</a> .  On the fingers, as I understand it, the meaning of the title is as follows: the calculus of variations works with functions in the same way as ordinary mathematical analysis with numbers.  That is, where we in school were looking for the point at which the minimum of the function was achieved, here we are looking for the <em>function</em> (that <img src="http://tex.s2cms.ru/svg/q%28%5Ctheta%29">  ), which minimizes KL divergence, for example. </li><li>  In the last post there was a question - what is the <a href="https://en.wikipedia.org/wiki/Dropout_%2528neural_networks%2529">dropout</a> and in general, is it somehow connected with the whole thing? <br><br>  And how.  Dropout can be considered as a cheap version of Bayesian, but very simple.  The idea is based on the same analogy with the ensembles, about which I mentioned at the end of the last post: now imagine that you have a neural network.  Now imagine that you take it, accidentally tear off several neurons to it, and put it aside.  After ~ 1000 such operations, you get an ensemble of thousands of networks, where each is slightly different from each other randomly.  We average their predictions, and we find that random deviations in places compensate each other and give actual predictions.  Now imagine that you have a Bayesian network, and you take out a set of its weights out of uncertainty a thousand times, and you get the same ensemble of slightly different networks. <br><br>  Than the Bayesian approach is cooler - it allows you to use this randomness in a controlled way.  Look at the picture again: <br><br><img src="https://habrastorage.org/files/0ca/a5c/0fc/0caa5c0fc56d417abd902b030d9df394.png"><br><br>  Every weight here is a random variable, but these are different accidents.  Somewhere the peak of the Gaussians is strongly shifted to the left, somewhere to the right, somewhere in the center and has a large variance.  Presumably as a result of training, each weight acquires a form that is most suitable for the network to perform its task.  That is, if we have some very important weight, for example, which must be strictly equal to something else, otherwise the whole network will break, then when sampling weights, this neuron will most likely remain in place.  In the case of a dropout, we simply disconnect the weights evenly and randomly, and we can easily bang this important weight.  Dropout does not know anything about its ‚Äúimportance‚Äù, for it all the weights are the same.  In practice, this is reflected in the fact that the Dimpindov network produces better results than a network with a dropout, albeit only slightly. <br><br>  Than dropout steeper - this is because it is very simple, of course. </li><li>  What do you do all this?  Neural networks, presumably, must be built in the image of the thing in our head.  There are obviously no Gaussian scales, but there are a lot of things that you machine operators ignore outright (time factor, discrete spikes and other biology).  Throw out your textbooks on Terver and repent! <br><br>  A good way I like to think about neural networks includes the following historical chain: <br><br><ul><li>  people at some point thought that all thinking and processes in the world in general can be organized in a cause-effect chain.  Something happened, then something else happened, and it all together affected something third. </li><li>  they began to build such symbolic models such as <a href="https://en.wikipedia.org/wiki/Bayesian_network">ordinary Bayesian networks</a> (not to be confused with the subject), and realized that such models could answer all sorts of different questions (‚Äúif it's sunny and Sebastian is happy, what is the likelihood that Sebastian was paid today?‚Äù ") And respond pretty well. </li><li>  but in such models all variables had to be driven in by hands.  At some point, <em>connectionism</em> came and said: let's just create a bunch of random variables and link them all together, and then somehow teach this model so that it works normally. </li></ul><br>  And so it turned out including neural networks.  They really <em>should not</em> be like the wet biological networks in our heads.  They are made in order to get the right combination of causes and effects in the form of neurons, which are stuck one into another, somewhere inside them.  Each neuron inside is a potential ‚Äúfactor‚Äù, some kind of variable responsible for something, and Bayesian magic helps to make these factors more useful. <br><br>  What does every neuron inside our head do?  Hell knows.  It may well be that nothing special. </li><li>  What else is on this topic? <br><br>  Oh, a lot of things: <br><br><ul><li>  <a href="http://arxiv.org/pdf/1312.6114v10.pdf">Variational autoencoders</a> - in my opinion, almost the most popular model, and in an amicable way, it was necessary to start with it, but I really liked this one. </li><li>  In fact, if you look a little deeper into the history of machine learning, then variational approximations stick out of each iron there.  Say, in the <a href="http://www.cs.toronto.edu/~fritz/absps/dbm.pdf">deep Bolztmann machine</a> (which are not particularly used anywhere, it seems, but enive), or in a piece called <a href="http://jmlr.csail.mit.edu/proceedings/papers/v15/larochelle11a/larochelle11a.pdf">NADE</a> , which remakes the Boltzmann machine in the language of the reverse propagation of an error. </li><li>  <a href="http://jmlr.org/proceedings/papers/v33/ranganath14.pdf">Variational black box</a> - I do not even know what it is, because I stumbled upon it only when writing a post, but I already like the name and the promise in the introduction. </li><li>  <a href="http://arxiv.org/pdf/1502.04623v2.pdf">DRAW</a> , which I can‚Äôt get to in any way and which looks simply amazing: a recurrent network with attention-mechanism that can draw digits like a pencil. </li><li>  a thing called <a href="http://arxiv.org/pdf/1507.02672v2.pdf">Ladder Network</a> that combines supervised and unsupervised learning </li></ul><br></li></ol></div><p>Source: <a href="https://habr.com/ru/post/280766/">https://habr.com/ru/post/280766/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../280756/index.html">NFS traffic visualization using elasticsearch + kibana</a></li>
<li><a href="../280758/index.html">Underground carders market. Translation of the book "KingPIN". Chapter 36. "Aftermath"</a></li>
<li><a href="../280760/index.html">Receive reports to the conference on artificial intelligence and big data AI & BigData Lab</a></li>
<li><a href="../280762/index.html">A brief history of Bitcoin Blockchain for dummies. Tale for adults</a></li>
<li><a href="../280764/index.html">What should be in the c-file, and what should be in the h-file?</a></li>
<li><a href="../280768/index.html">StartCOM: Certificate Transparency, Free * EV SSL Certificates</a></li>
<li><a href="../280770/index.html">The digest of interesting materials for the mobile developer # 147 (March 28 - April 3)</a></li>
<li><a href="../280774/index.html">Behavior Analysis with Apache Spark</a></li>
<li><a href="../280776/index.html">IBM is working to strengthen information protection of "connected" cars</a></li>
<li><a href="../280782/index.html">Java programmer cheat sheet 8. Libraries for working with Json (Gson, Fastjson, LoganSquare, Jackson, JsonPath and others)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>