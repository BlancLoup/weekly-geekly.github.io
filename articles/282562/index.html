<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How GearVR differs from a cardboard box, or the pursuit of latency</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Disclaimer: The post is written on the basis of fairly edited chat logs closedcircles.com , hence the style of presentation, and the availability of c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How GearVR differs from a cardboard box, or the pursuit of latency</h1><div class="post__text post__text-html js-mediator-article"><p>  <em>Disclaimer: The post is written on the basis of fairly edited chat logs <a href="http://closedcircles.com/%3Finvite%3Da1688d28d1d8c292a82e001473f34659c2df11ed">closedcircles.com</a> , hence the style of presentation, and the availability of clarifying questions.</em> </p><br><p><img src="https://habrastorage.org/files/d0e/00f/786/d0e00f786bc5445b986c88cd43a3296d.png" alt="image"></p><br><p>  The main term you need to know about VR is <strong>motion-to-photon latency</strong> . <br>  In other words, the delay between the turn of the head and the last photon of the image (drawn from the perspective of the new head position) left the screen. <br>  It is empirically deduced that motion-to-photon latency is 20 msec and below allows to reach presence - i.e.  feeling that you move your head in the virtual world. <br>  Whether values ‚Äã‚Äãless than 20 ms are important or not is not clear, but in general the goal is to achieve 20. <br>  GearVR by hook or by crook reaches, and I will tell you how. </p><a name="habracut"></a><br><p>  <em>I apologize in advance for the free use of English terms interspersed with transliterated and translated.</em> </p><br><p>  GearVR is if someone does not know the headset in which the phone is stuck.  There is no screen in the headset itself - there are <strong>lenses and IMUs</strong> . <br>  IMU is a standard gyroscope + accelerometer.  Whether there is a magnetometer in GearVR, I don‚Äôt know exactly, it seems not - but in any case, in practice, the magnetometer is not very important. <br>  There are standard ways to get device orientation using gyro and accelerometer, I will not stop there. </p><br><p>  Important point: The GearVR has its own IMU, which provides new data on orientation at a frequency of 1000 Hz with minimal latency (most likely about 1 ms). <br>  This is many times better than the built-in sensors in the phones (in Samsung Note 5 gyro the frequencies are 200 Hz and latency is 10-15 ms). </p><br><blockquote>  <em>And how does it feed?</em>  <em>From the phone?</em> <br>  As far as I understand, it is powered from the phone via USB - there is almost no iron inside there, so probably the flow is minimal. </blockquote><br><p>  This means that the basic arrangement - we have IMU data, we read them at the beginning of the frame and we need to render two frames (stereo image).  There is no positional tracking in GearVR - we only know the orientation of the head. </p><br><p>  Using the orientation of the head, we obtain approximate data for the cameras of two eyes - conditionally, if we turned our head to the right, then the position of the eyes also moves to the right, since  the head rotates around the joint in the neck, and it is known how far the eyes from this point of rotation are displaced. <br>  Well, we know the approximate (average) distance between the user's eyes, so with simple geometry we get two cameras, draw two pictures. </p><br><p>  As already mentioned, there are two lenses in GearVR - one for each eye.  Lenses are needed for two reasons: </p><br><ol><li>  FOV increase.  If you measure the FOV of a screen stuck to your head, it turns out that the value is not high enough - you want 90+ degrees for each eye so that the screen can be small enough. </li><li>  Change the optical focal length. <br>  If there were no lenses, then different pixels would be in different focal points because  the screen is very close and the difference in the distances of different pixels is quite significant. </li></ol><br><p>  The lens "straightens" the rays coming from the eye, you get the feeling of an optical focus at infinity: <br><img src="https://habrastorage.org/files/877/5ed/712/8775ed712ced4f4b8e1a5dad33c579fb.png" alt="image"></p><br><p>  Actually, it turns out that a lens with such parameters has quite a significant distortion - if you just render the picture in both eyes, then the objects will be noticeably distorted;  the farther from the center of the lens (eye), the greater. <br>  Therefore, we take the picture obtained by the standard 3D renderer and distort it when displaying on the screen, so that after distorting the lens, it will be about the same. </p><br><p>  For this, a distortion mesh is usually drawn that looks like this: <br><img src="https://habrastorage.org/files/86e/dc3/b8f/86edc3b8f6a041b9a2583cebc6a1c456.png" alt="image"></p><br><p>  Plus, if you have a lot of extra time on the GPU, you can try to correct chromatic aberration in the same pass. <br>  (which also results from the properties of the lens). <br>  In practice, on GearVR, by default this filter is disabled - to compensate for it, you need to do 3 instead of 1 texture sample in the shader, and this is mobile hardware. </p><br><p>  <strong>So, the introductory is finished, let's deal with latency :)</strong> </p><br><p>  In the proposed scheme, there will be no 20 ms and in general everything will be very bad. <br>  Because... </p><br><ul><li>  At the beginning of the frame we read IMU data - with say 1 ms latency; </li><li>  Then we collected the frame matrices, rendered the scene - this is generally a long process, especially considering the quality of OpenGL drivers on Android; <br>  (this is another 16 ms for example, if 60 FPS render ...) </li><li>  Then we gave the frame to the GPU - it draws this frame for a long time and persistently, and then it makes distortion to get the final picture; <br>  (this is still 16 ms) </li><li>  Then the display scan-out starts! </li></ul><br><p>  The display is clearly not a CRT at all, but the concept of a ‚Äúbeam‚Äù reading the data still remains - the display controller reads the display behind the scanline and lights the pixels. <br>  And in order to read the entire display - it takes 16 ms. <br>  Scanlines on the phone are usually ‚Äúshort‚Äù - when you use VR, the phone is in landscape mode, and the scanlines are vertical stripes going from left to right (well, or right to left if the phone is turned upside down ...) <br>  Accordingly, the leftmost scanline (in the left eye) is updated 16 ms earlier than the rightmost (in the right eye). </p><br><ul><li>  And finally, there is another joy: <br>  The described mechanism is standard double buffering ... <br>  The gallant operating system is actually triple buffering. <br>  Those.  when the GPU draws a picture, it is given not to the display but to the composer, which may be combined with other windows and so on. <br>  And it turns out another frame (16 ms) delay. </li></ul><br><p>  <strong>As a result, something like 65 ms latency turned out to be shorter.</strong> <br>  What is unimaginably more than 20. </p><br><p>  Plus there is another problem ... <br>  Usually the display works in full persistence mode - namely, the scanline is lit with new colors, and then virtually the entire frame remains in this mode - i.e.  16 ms <br>  It turns out that in 16 ms you can turn your head far enough, and as a result this scanline is registered with the eye at different points of the retina with the same color. <br>  And it turns out smearing when turning the head. </p><br><p>  So if all this does not win - GearVR games would look like Google Cardboard. <br>  We must win. </p><br><p>  The key technology is the technology that Oculus calls <strong>Time Warp</strong> . <br>  It's also called reprojection, and probably something else, but I like the word timewarp. <br>  The idea is as follows - if you take a picture rendered by camera C, then take camera C 'which is camera C turned (but not displaced) for a not very long distance (units or tens of degrees), and render a picture to it ... <br>  It turns out that the second picture from the first can be obtained by a simple 2D transformation. </p><br><p>  Why is this so?  On the fingers, since in a picture rendered from the camera, each pixel corresponds to a beam from the camera to the point where this pixel is located, it is clear that when you turn the camera, the intersection of two frustums will contain rays that will correspond to pixels of the same color. </p><br><p> If you fix the matrix and turn your head left-right, then at small angles of rotation the fact that you fixed the matrix is ‚Äã‚Äãnot obvious. <br>  This is actually the main test of the fact that time warp is working correctly. <br>  (fixed the matrix of your render and see if you can turn your head, plus or minus 5 degrees). <br>  It is possible and more than 5 degrees, but there is already a lot of information to take nowhere, so most of the screen turns black. <br>  Actually, ‚Äúshake the device‚Äù is a standard stress test for headsets that have positional tracking - fix the head and then with high-frequency hands move the headset on the head left-right, which means the picture must be stable - for this, you actually need sub-20 ms (well quality tracking). </p><br><blockquote>  <em>Listen, and if on Gear VR to render with a fixed matrix, I twist the \ projack and turn on the repro, then start shaking the device, does the picture shake on it?</em> <br>  Mmm, shaking is bad.  no positional tracking and that will be incomprehensible <br>  <em>Well, if there is no positional tracking, then shaking - it will just change the angle, even if it is small.</em> <br>  Well, yes, but in the real world the position will change and there is no position in the headset - i.e.  stable pictures will not work. <br>  You can try to turn the head high-frequency left-right if the neck does not fall off :) </blockquote><br><p>  Timewarp is simple - we <em>already</em> draw the same distortion mesh. <br>  Therefore, you can simply ‚Äúrotate‚Äù the texture coordinates in the vertex shader (in 3D space). <br>  The pixel shader does not change, timewarp is actually free. </p><br><p>  How much to turn? <br>  The difference between turning the head <em>now</em> and turning the head with which the original picture was drawn. <br>  The greater the turn - the more empty space, you can fill it with black <br>  (or you can do clamp addressing, at small angles of rotation it turns out a little better than black, but at large it turns out strange). </p><br><p>  Now, when we have a mechanism for "turning" the image, let's see how it can be used to reduce latency. </p><br><p>  First, let's deal with full persistence - this does not technically affect the motion-to-photon, but still need to be removed. <br>  For phones that GearVR supports, a special display driver is written that includes <strong>low persistence mode</strong> : <br>  Each scanline lights up as before with the necessary colors, and then almost immediately - I don‚Äôt know exactly how much, but let's say 3-4 ms - it goes dead (turns black). <br>  It turns out that a sufficiently large number of people due to the peculiarities of perception do not notice the "blinking". <br>  It is said that the perception boundary varies from somewhere 50 Hz blinking to 85 Hz - which is one of the main reasons why desktop VR headsets (Rift / Vive) run at 90 Hz. <br>  Plus, low persistence in a headset is perceived better than just a flashing 60 Hz panel in the real world, because everything seems to be blinking, and the brain adapts ... </p><br><p>  Reference links and videos: <br>  <a href="https://www.youtube.com/watch%3Fv%3D_FlV6pgwlrk">https://www.youtube.com/watch?v=_FlV6pgwlrk</a> <br>  <a href="https://en.wikipedia.org/wiki/Persistence_of_vision">https://en.wikipedia.org/wiki/Persistence_of_vision</a> <br>  <a href="https://en.wikipedia.org/wiki/Flicker_fusion_threshold">https://en.wikipedia.org/wiki/Flicker_fusion_threshold</a> </p><br><blockquote>  <em>Aah, that is because we have already ‚Äúremoved‚Äù the image (the screen has been turned off), is this better perceived?</em>  <em>And the problem is when it burns all 16 ms, and the head moves?</em> <br>  Yes sir. <br>  There is a thing called vestibulo ocular reflex: <br>  When you turn your head, your eye automatically turns in the opposite direction. <br>  Therefore, when turning the head, you can fix the eyes on the object and, for example, read when not very strong turns without problems. <br>  And that means if your pixels are burning for a long time, then when you project one pixel on the retina, you get a strip because of the turn of the eye and you get smearing / ghosting. <br>  And if the pixel immediately almost turns off, then smearing does not appear, and in view of some features of the view, high-frequency blinking is not perceived as blinking - I do not know exactly why. <br>  VOR is generally a cool thing, the vestibular apparatus is naturally directly connected to the eye muscles. <br>  Brain involvement is practically not required, and ultra low latency :) </blockquote><br><p>  Now when we move our head, we no longer have smearing, but still have 65 ms latency ... <br>  Let's now see how the render actually works in GearVR. <br>  What I wrote above about the fact that at the beginning of the render frame we read the data from the IMU, then we generate OpenGL commands and then give them to the GPU that draws the frame - it remains valid. <br>  This is a game thread and game GL context. </p><br><p>  There is another thread that creates GearVR SDK and it has its own <strong>GL context, which works in high priority mode.</strong> <br>  (on newer versions of Android, this context can now be created by using the EGL extension). <br>  High priority mode means that commands issued by the GPU in this context will be executed almost immediately by the GPU. </p><br><p>  I don‚Äôt know the details of the GPU scheduling specifically on the GPUs that support GearVR - I assume that there is a preemption granularity draw call - that is,  if you don‚Äôt have huge 10 msec draw calls, you can finish the current one and two milliseconds and start the next one. <br>  On NVidia‚Äôs desktop GPU, this granularity is usually in a draw call and on AMD, if I‚Äôm not mistaken 1 wave (for compute), well, desktop vendors are working on improving since  This feature is also important on desktop VR ... </p><br><p>  The second feature of this VR context is ... <br>  What he draws at once in the <strong>front buffer</strong> is the very buffer from which the scan out occurs, thus skipping all the composers and other nonsense. <br>  As we have already figured out, scan out takes 16 ms and follows the scanlines one after another in a strict order. <br>  Scanlines are vertical stripes that go from left to right (in landscape mode), so half of the stripes contain a picture from the left eye (scanned for 8 ms), and the second half of the stripes contains a picture from the right eye (scanned for 8 ms). <br>  So you can try to draw in the right eye while the display scans the left and vice versa. </p><br><p>  I will note just in case - this does not mean that we will draw straight triangles from the frame of the game! <br>  Here you need a very neat timing and clear guarantees, so the game there as it turns out draws a frame, and the VR context will adjust it using time warp. </p><br><p>  So, this very VR thread wakes up twice - when the vsync interval started, and 8 ms after that (in the middle of scan out). <br>  Reads the latest IMU data (with latency 1 ms), and then generates <em>two</em> corrective rotation matrices. </p><br><p>  Why two? <br>  Due to scan out, the left scanline of the left eye and the right scanline of the left eye are separated by 8 ms in time (from the point of view of the photon emission time), therefore, ideally, they should be adjusted differently. <br>  Therefore, we read IMU data (latency 1 ms), and generate two head orientations using a gyroscope data for prediction ahead. <br>  We predict the first one by 8 ms, the second by 16 ms (now it will be clear why). </p><br><p>  Then we send a draw call to a high priority context in which half of the distortion mesh is on top for the desired eye and we hope that the GPU will quickly select and draw it. <br>  After 8 ms after we generated the orientation, scan out will reach the part of the screen where we just drew the data and start to show it. <br>  As a result, full latency to the photon emission of the leftmost scanline of each eye is approximately 9 ms, and the rightmost one is approximately 17 ms. </p><br><p>  (those who wrote <strong>race-the-beam</strong> rasterizers 30 years ago on Amiga - shed a tear) </p><br><p>  Actually, if you cut the distortion mesh into vertical stripes and have clearer timing guarantees - as I understand it, this is impossible with the current GPU - then you could reduce the motion-to-photon to &lt;10 ms. <br>  As you can see, half (8 ms) of the current latency is a type of safety buffer to fit and draw a call on the GPU and so that it draws it, and half is the worst scanout latency for the scanline. <br>  If you cut into stripes and have a clearer timing, then you can try to solve it. <br>  Or directly integrate the time warp into the scanout controller, the warp operation is just a bilinear texture sampling, not very difficult. <br>  But by and large, the current latency is good enough.  If you increase the display frequency to 90 Hz in order to remove blinking problems for more sensitive people, you will get about 10 ms on the same technology right away. </p><br><blockquote>  <em>Yes, 8 ms is generally an arbitrary buffer.</em> <br>  <em>It is possible theoretically and less</em> <br>  Theoretically, it can be less, almost noticeably less probably becomes dangerous. <br>  If preemption is on a draw call basis, then this should also be taken into account <br>  Especially shorter in VR on mobile there is a tendency to push the whole scene in 1-2 draw calls <br>  Because OpenGL is slow, Unity is slow, and so forth. </blockquote><br><p>  So, we woke up right after vsync - at that time scan out began to process the left eye - and drew the right eye in the front buffer. <br>  Then we fell asleep until the middle of the vsync interval, woke up - at that time scan out began to process the right eye - and drew the left eye in the front buffer. <br>  Ad infinitum. </p><br><p>  I note, time warp has a funny problem ... <br>  He can only screw the picture. <br>  It is not able to shift it - at the offset the disocclusion problems begin, the depth buffer is needed, more resources and so on. <br>  But in fact, there are two eyes, and when you turn your head, your eyes shift and turn at the same time. </p><br><p>  Therefore, the more timewarp correction, the more incorrect the stereo picture is and the harder it is to focus. <br>  In practice, it turns out that a time warp of 20-30 msec does not lead to visible problems, except at high turning speeds where the convergence stops working, I guess. </p><br><p>  A couple more moments that GearVR can do, and which are impossible in normal Android ... </p><br><p>  First, you can turn on <strong>realtime priority</strong> to your render stream (well, it turns on VR stream) <br>  So that garbage collection streams of every kind do not interfere very much. </p><br><p>  Secondly, you can turn on (more precisely, you need) <strong>fixed clock frequency</strong> . <br>  By default, the frequency of the CPU and GPU varies depending on the load and other factors. <br>  In practice, in Android, these mechanisms work very strangely, and can lead to the fact that the load that should fit at 16 ms per frame does not fit for several seconds in a row. <br>  Therefore, the SDK disables the dynamic adjustment and allows the application to select fixed frequency scales for the CPU / GPU separately. </p><br><blockquote>  <em>Here, I mean clarify.</em>  <em>That this whole cycle is for this special thread about time warp</em> <br>  <em>And the actual rendering of the application is parallel to it.</em> <br>  Yes, I wrote it somewhere above.  Render thread interacts with this time warp thread through the function "I finished submit commands for the next frame on the GPU, hold it." <br>  And so two independent threads and two actually independent contexts (well, probably fences are inserted into the first context so as not to have problems with synchronization). <br>  <em>And in the application there may still be 16 msec on the CPU in the driver, and 16 msec on the drawing</em> <br>  In principle, it can.  Maybe 40 ms on the CPU.  :) <br>  <em>These are the numbers you need to reduce it for the rest of the time warp to take out?</em> <br>  It's hard to say for sure.  Very dependent on the content.  In general, Oculus recommends keeping a stable 60 fps (16 ms cpu / gpu) and counting on a time warp to remove these extra 20-30 ms latency, and to smooth out sudden frame spikes. <br>  But if you have content plus or minus static - for example, imagine an adventure game - then in the SDK there is a mode in which they expect that you give them frames every 33 ms and not 16. <br>  And time warp works almost perfectly (almost - because see above about eye movement when turning) <br>  <em>60 FPS, but 32 msec latency, huh?</em> <br>  20-30, it depends on how specifically the driver for the GPU works which is usually tiled, how quickly you can add all the draw calls for the first frame buffer, etc. <br>  But overall 32. <br>  You will mainly see this in the positions of other objects, if you will. <br>  Those.  this is no different from standard latency questions in regular games - the reaction is from pressing a key to movement, etc., it‚Äôs all not as critical as head tracking. </blockquote><br><p>  Oh, I forgot! <br>  <strong>Pro Cardboard.</strong> <br>  As you can see from the above, there is exactly ONE moment in hardware for GearVR. <br>  Good sensors with low latency and high refresh rate. <br>  All the rest is software. <br>  (there are still lenses, but in GearVR the lenses, as far as I understand, do not stand out with anything, consider the same in the cardboard devices) </p><br><blockquote>  <em>And about this you can read somewhere in detail?</em>  <em>About the lens itself.</em>  <em>I wonder why with objects that get very close garbage.</em>  <em>There, the left-right pictures diverge so much that I cannot focus.</em>  <em>And on a real object - I can.</em>  <em>It's easy to test on the HTC Vive - bring the controller to the helmet, then take off the helmet and look at it in real life.</em> <br>  There is a problem called vergence-accomodation conflict, maybe because of this ... <br>  Vergence is a phenomenon in which the eyes turn so as to look at an object: <br>  If the object is far away then the eyes look straight; <br>  If the object is very close, then they turn to the nose, for example, if the object is in front of the nose. <br>  Accomodation is the very focusing of view, the optical properties of the eye change there to focus ( <a href="https://en.wikipedia.org/wiki/Accommodation_(eye">https://en.wikipedia.org/wiki/Accommodation_(eye</a> )). <br>  And so the following thing happens in VR: <br>  Vergence works correctly.  Each eye is given its own picture, and if the object is close, then in the left and right eyes there are different images - the object is greatly displaced and the eyes turn as it should. <br>  And accomodation doesn't work because  the lens is one and it is not adjusted in any way therefore all objects are at optical infinity. <br>  This conflict is differently resolved by different people in different ways, and the closer the object, the greater the conflict. </blockquote><br><p>  Judging by my experiments with sensors on different phones with tz.  latency ... <br>  Samsung and Apple have noticeably worse tracking, but Nexus 6 has worse tracking but within reason. <br>  (EMNIP 4ms refresh interval, avg 3 ms latency) <br>  Those.  on Nexus 6 - you can do everything that I wrote above and have a natural experience comparable to GearVR, but in a cardboard box. <br>  The problem is that almost everything I wrote above cannot be made of user mode - you need to patch the kernel and drivers. <br>  So the authors of the applications can not do this, but Google is fine. </p><br><blockquote>  <em>I was played yesterday just on Gear VR - I know who as it is, but it makes me sick, although I do not notice lag at a conscious level</em> <br>  <em>Well, the pixels are huge!</em> <br>  The main problem GearVR with tz.  motion sickness - lack of positional tracking. <br>  When you move your head, the picture does not reflect this movement - it creates a conflict between the vestibular and the optical system. <br>  <em>There were rumors that the direction of the future ocular scan in the Oculus is the positional tracking for mobile and AR</em> <br>  Well, I'm not surprised, positional tracking is missing feature # 1, # 2 and # 3 in gear. <br>  # 4 probably full RGB display <br>  So far, all successful positional tracking approaches require an external camera. <br>  Or heaps of cameras on the headset as in HoloLens ... <br>  <em>so I tried both that rift dk2 that gear vr - equally seemed like a useless toy</em> <br>  <em>well, I still have a squint</em> <br>  I, too, it does not bother me personally. <br>  This is what I wrote above about convergence <br>  Personally, I do not care what the IPD is in cameras :) <br>  I have a large part of perception through one of my eyes, not through both - depending on which one you focus on. <br>  <em>yeah - or I see bad 3d</em> <br>  <em>I had the feeling that I was inside a projection sphere</em> <br>  <em>not inside 3d world</em> <br>  By the way, it is possible that for people with squint, positional tracking is more critical by default - when you have stereo vision, then you perceive the distance to objects as such, and when it is not very good, then the main visual cue is parallax <br>  And parallax is obtained mainly from the movement of the head <br>  Especially in gunjack where the turret around you <br>  But here I am a little out of my depth, I don‚Äôt understand very well the significance of various perceptual factors, so I‚Äôm not exactly sure. <br>  I generally agree that GearVR didn‚Äôt really insert me personally, unlike desktop experiences. </blockquote></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/282562/">https://habr.com/ru/post/282562/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../282552/index.html">Apache Parquet performance</a></li>
<li><a href="../282554/index.html">Announcement NGINX 1.10 and 1.11</a></li>
<li><a href="../282556/index.html">Cocos creator</a></li>
<li><a href="../282558/index.html">We invite you to the conference on artificial intelligence and big data AI & BigData Lab June 4</a></li>
<li><a href="../282560/index.html">Big Data: Silver Bullet or another tool</a></li>
<li><a href="../282564/index.html">Driver anatomy</a></li>
<li><a href="../282566/index.html">Who is faster: "clouds" vs "IT market"?</a></li>
<li><a href="../282568/index.html">Scenarios of application of free tools Veeam for development and testing in Microsoft Azure</a></li>
<li><a href="../28257/index.html">Javascript viruses</a></li>
<li><a href="../282570/index.html">Intel Xeon processors equip FPGA Altera</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>