<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ML Boot Camp V, solution history for 2nd place</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this article, I will tell the story of how the ML Boot Camp V contest ‚ÄúPredicting Cardiovascular Diseases‚Äù solved and took second place in it. 
 Pr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>ML Boot Camp V, solution history for 2nd place</h1><div class="post__text post__text-html js-mediator-article"><p>  In this article, I will tell the story of how the <a href="http://mlbootcamp.ru/">ML Boot Camp</a> V contest ‚ÄúPredicting Cardiovascular Diseases‚Äù solved and took second place in it. </p><br><h2 id="postanovka-zadachi-i-dannye">  Problem statement and data </h2><br><p>  <a href="http://mlbootcamp.ru/round/12/sandbox/">The data</a> contained 100,000 patients, of which 70% were in the training set, 10% for the public leaderboard (public) and the final 20% (private), on which the result of the competition was determined.  The data was the result of a medical examination of patients, based on which it was necessary to predict whether a patient has cardiovascular disease (CVD) or not (this information was available for 70% and it was necessary to predict the probability of CVD for the remaining 30%).  In other words, this is a classic binary classification problem.  Quality metric - <a href="http://wiki.fast.ai/index.php/Log_Loss">log loss</a> . </p><a name="habracut"></a><br><p>  The result of the medical examination consisted of 11 signs: </p><br><ul><li>  General - age, gender, height, weight </li><li>  Objective - upper and lower pressure, cholesterol level (3 categories: normal, higher than normal) and blood glucose (also 3 categories). </li><li>  Subjective - smoking, alcohol, active lifestyle (binary signs) </li></ul><br><p>  Since the subjective signs were based on the patient's responses (may be unreliable), the organizers of the competition concealed 10% of each of the subjective signs in the test data.  The sample was balanced.  Height, weight, upper and lower pressure needed to be cleaned, as they contained typos. </p><br><h2 id="kross-validaciya">  Cross validation </h2><br><p>  The first important point is correct cross-validation, since the test data had missing data in the smoke, alco, active fields.  Therefore, in a validation sample, 10% of these fields were also hidden.  Using 7 folds of validation (CV) with a modified validation set, I looked at several different strategies for improving predictions on smoke, alco, active: </p><br><ul><li>  Leave the data in training as it is (in validation 10% of missing values ‚Äã‚Äãin smoke, alco, active).  This approach requires algorithms that can handle missing values ‚Äã‚Äã(NaN), for example - <a href="http://xgboost.readthedocs.io/">XGBoost</a> . </li><li>  Hide in training 10% so that the training sample is more like validation.  This approach also requires algorithms that can work with NaN. </li><li>  Predict NaN in validation on three trained classifiers </li><li>  Replace signs of smoke, alco, active with predicted probabilities. </li></ul><br><p>  The strategy of weighting training examples by proximity to validation / test data was also considered, which, however, did not increase due to the same distribution of train-test. </p><br><p>  Hiding in training almost always showed the best CV results, and the optimal share of hidden values ‚Äã‚Äãin training also turned out to be 10%. </p><br><p>  I would like to add that if standard cross-validation is used without hiding values ‚Äã‚Äãin validation, then CV was obtained better, but overestimated, since the test data in this case is not similar to local validation. </p><br><h2 id="korrelyaciya-s-liderbordom">  Leaderboard Correlation </h2><br><p>  An interesting question that always excites participants is the correlation between the CV and the test data.  Already having complete data after the end of the competition (link), I conducted a small analysis of this correlation.  For almost all submitters, I wrote down CV results in the description.  Having also the result on public and private, we construct pairwise schedules of submits for CV, public, private values ‚Äã‚Äã(Since all logloss values ‚Äã‚Äãstart at 0.5, for clarity, I omitted the first digits, for example, 370 is 0.5370, and 427.78 - 0.542778): </p><br><p><img src="https://habrastorage.org/web/e20/a34/d78/e20a34d78bb24b8c9a2c19aeec2795c3.png"></p><br><p>  To get a numerical estimate of the correlation, I chose the Spearman coefficient (others will do as well, but in this case it is the monotonic dependence that is important). </p><br><table><thead><tr><th>  Spearman rho </th><th>  CV </th><th>  Public </th><th>  Private </th></tr></thead><tbody><tr><td>  <strong>CV</strong> </td><td>  one </td><td>  0.723 </td><td>  0.915 </td></tr><tr><td>  <strong>Public</strong> </td><td>  - </td><td>  one </td><td>  0.643 </td></tr><tr><td>  <strong>Private</strong> </td><td>  - </td><td>  - </td><td>  one </td></tr></tbody></table><br><p>  It can be concluded that the cross-validation introduced in the previous section correlated well with the private for my submissions (during the whole competition), while the public correlation with CV or private is weak. </p><br><p>  Small comments: I didn‚Äôt sign the CV result for all submitters, and among the CV data there are results with not the best strategies for working with NaN (but the overwhelming majority with the best strategy described in the previous section).  Also, these graphs do not contain my two final submissions, which I will discuss below.  I depicted them separately in the public-private space with a red and green dot. </p><br><p><img src="https://habrastorage.org/web/845/0ab/c96/8450abc96ec04edd8a210b4cc0d866f1.png"></p><br><h2 id="modeli">  Models </h2><br><p>  During this contest, I used the following models with the appropriate libraries: </p><br><ul><li>  Regularized Gradient Boosting (XGBoost library) is the main model on which I relied, as it showed the best results in my experiments.  Also, most of the averages were built on only a few xgb. </li><li>  Neural Networks ( <a href="http://keras.io/">Keras</a> library) - experimented with feed forward networks, autoencoders, but it was impossible to beat your baseline from xgb, even in averaging. </li><li>  Different <a href="http://scikit-learn.org/">sklearn</a> models that participated in averaging with xgb - RF, ExtraTrees, etc. </li><li>  Stacking ( <a href="http://brew.readthedocs.io/">brew</a> library) - it was not possible to improve the baseline averaging of several different xgb. </li></ul><br><p>  After experimenting with different models and getting the best results by mixing 2-3 xgb (cross-validation took 3-7 minutes), I decided to concentrate more on data cleansing, trait conversion and careful tuning of 1-5 different xgb hyperparameter sets. <br>  I tried to search for <a href="https://github.com/fmfn/BayesianOptimization">hyper parameters</a> using Bayesian optimization (the <a href="https://github.com/fmfn/BayesianOptimization">bayes_opt</a> library), but mostly relied on a random search that served as initialization for Bayesian optimization.  Also, in addition to the banal optimal number of trees, after such a search, I tried alternately to pull up the parameters (mainly the regularization parameters of the min_child_weight and reg_lambda trees) ‚Äîa method that some people call graduate student descent. </p><br><h2 id="chistka-dannyh-i">  Data cleaning I </h2><br><p>  The first data cleansing option, which was implemented by the participants in one way or another, consisted in the simple application of emission control rules: </p><br><ul><li>  For pressures of 12000 and 1200 - divided by 100 and 10 </li><li>  Multiply by 10 and 100 for pressures of 10 and 1 </li><li>  Weight type 25 replaced by 125 </li><li>  Growth of a type 70 to replace with 170 </li><li>  If the upper pressure is less than the lower, swap </li><li> If the lower pressure is 0, then replace with the upper minus 40 </li><li>  etc. </li></ul><br><p>  Using a few simple rules for cleaning, I was able to achieve an average CV of ~ 0.5375 and a public ~ 0.5435, which showed quite an average result. <br>  The figures show the sequential processing of extreme values ‚Äã‚Äãand emissions with the arrival of the cleared values ‚Äã‚Äãof the upper and lower pressure in the last image. </p><br><p><img src="https://habrastorage.org/web/5a6/d16/948/5a6d16948829464882a95045b0e87265.png"><br><img src="https://habrastorage.org/web/715/942/4d0/7159424d07c943aead7721a0041d9250.png"><br><img src="https://habrastorage.org/web/c3d/870/d56/c3d870d5651f48f099ead24bb11768c4.png"><br><img src="https://habrastorage.org/web/9e6/da9/25c/9e6da925c22747d3a4159df7a7f1befd.png"></p><br><p>  In my experiments, removing emissions did not improve CV. </p><br><h2 id="chistka-dannyh-ii">  Data Cleaning II </h2><br><p>  The previous data cleansing is quite adequate, however, after long attempts at improving the models, I revised it more carefully, which made it possible to significantly improve the quality.  Subsequent models with this purge showed an increase in CV to ~ 0.5370 (from ~ 0.5375), public to ~ 0.5431 (from ~ 0.5435). </p><br><p>  The basic idea is that there is an exception to every rule.  My process of finding such exceptions was rather routine ‚Äî for a small group (for example, people with upper pressure between 1100 and 2000), I looked at the values ‚Äã‚Äãin train and test.  For most, of course, the ‚Äúsplit by 10‚Äù rule worked, but there were always exceptions.  These exceptions were easier to change separately for examples than to look for the general logic of exceptions.  For example, I replaced such pressure from the general pressure group as 1211 and 1620 with 120 and 160. </p><br><p>  In some cases, it was possible to correctly handle exceptions, only including information from other fields (for example, on a bundle of upper and lower pressure).  Thus, pressures of the form 1/1099 and 1/2088 were replaced with 110/90 and 120/80, and 14900/90 were replaced with 140/90.  The most difficult cases, for example, when replacing the pressure of 585 to 85, 701 to 170, 401 to 140. <br>  In difficult, less than unambiguous cases, I checked how corrections are similar to training and test.  For example, I replaced the 13/0 case with 130/80, since it is the most likely.  For exceptions from the training set, knowledge of the CVD field also helped me. </p><br><p>  A very important point is to distinguish the noise from the signal, in this case, a typo from the real anomalous values.  For example, after cleaning I left a small group of people with pressure of 150/60 (they have CVD in training, their pressure fits into <a href="https://en.wikipedia.org/wiki/Systolic_hypertension">one</a> of the categories of CVD) or about 90 cm tall with a small weight. </p><br><p>  I will add that pressure has cleared the main increase, whereas with height and weight there were a lot of ambiguities (processing of height-weight is also based on the search for exceptions with further application of the general rule). </p><br><p>  Using the full <a href="">dataset</a> laid out after the competition, we find that this cleaning affected 1379 objects in training (1.97%), 194 in public (1.94%), 402 in private (2.01%).  Of course, correcting the anomalous values ‚Äã‚Äãfor 2% dataset was not ideal and you can do it better, but even in this case, the largest CV increase was observed.  It is worth noting that after cleaning or working with signs, it is necessary to find more optimal hyperparameters of the algorithms. </p><br><h2 id="rabota-s-priznakami-i-ih-diskretizaciya">  Work with signs and their discretization </h2><br><p>  Initially, age was divided by 365.25 to work over the years.  The age distribution was periodic, where patients with even years were much more.  Age was a Gaussian mixture with 13 centers in even years.  If you just round it up by year, then the CV in the fourth digit improved by ~ 1-2 units compared to the initial age.  The figures show the transition from the original age to rounded up to a year. </p><br><p><img src="https://habrastorage.org/web/eeb/d18/415/eebd18415ac0493086b71070995c2547.png"><br><img src="https://habrastorage.org/web/b78/ba6/c77/b78ba6c7749643e49e475920f893ad1a.png"></p><br><p>  However, I also used a different discretization in order to improve the distribution by years, which I included in the last simple model.  The vertices of the distributions in the Gaussian mixture were found using the Gaussian process, and the ‚Äúyear‚Äù was defined as half the Gaussian distribution (right and left).  Thus, the new distribution by ‚Äúyears‚Äù looked more even.  The figures show the transition from the initial age (with the found vertices of the Gaussian mixture) to a new distribution over the "years" </p><br><p><img src="https://habrastorage.org/web/eee/6e3/ebc/eee6e3ebc1314c298201e74e5a268bfe.png"><br><img src="https://habrastorage.org/web/d59/fb0/b50/d59fb0b505b7414cb03df4ddfc011f26.png"></p><br><p>  <a href="https://ru.wikipedia.org/wiki/%25D0%2598%25D0%25BD%25D0%25B4%25D0%25B5%25D0%25BA%25D1%2581_%25D0%25BC%25D0%25B0%25D1%2581%25D1%2581%25D1%258B_%25D1%2582%25D0%25B5%25D0%25BB%25D0%25B0">BMI</a> (body mass index = <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>h</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mn>100</mn><msup><mo stretchy=&quot;false&quot;>)</mo><mn>2</mn></msup></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="22.04ex" height="2.901ex" viewBox="0 -935.7 9489.4 1249" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335226/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhi2YCyklPqeLVakngWAy2PWCO4MsQ#MJMATHI-77" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335226/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhi2YCyklPqeLVakngWAy2PWCO4MsQ#MJMATHI-65" x="716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335226/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhi2YCyklPqeLVakngWAy2PWCO4MsQ#MJMATHI-69" x="1183" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335226/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhi2YCyklPqeLVakngWAy2PWCO4MsQ#MJMATHI-67" x="1528" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335226/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhi2YCyklPqeLVakngWAy2PWCO4MsQ#MJMATHI-68" x="2009" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335226/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhi2YCyklPqeLVakngWAy2PWCO4MsQ#MJMATHI-74" x="2585" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335226/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhi2YCyklPqeLVakngWAy2PWCO4MsQ#MJMAIN-2F" x="2947" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335226/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhi2YCyklPqeLVakngWAy2PWCO4MsQ#MJMAIN-28" x="3447" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335226/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhi2YCyklPqeLVakngWAy2PWCO4MsQ#MJMATHI-68" x="3837" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335226/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhi2YCyklPqeLVakngWAy2PWCO4MsQ#MJMATHI-65" x="4413" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335226/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhi2YCyklPqeLVakngWAy2PWCO4MsQ#MJMATHI-69" x="4880" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335226/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhi2YCyklPqeLVakngWAy2PWCO4MsQ#MJMATHI-67" x="5225" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335226/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhi2YCyklPqeLVakngWAy2PWCO4MsQ#MJMATHI-68" x="5706" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335226/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhi2YCyklPqeLVakngWAy2PWCO4MsQ#MJMATHI-74" x="6282" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335226/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhi2YCyklPqeLVakngWAy2PWCO4MsQ#MJMAIN-2F" x="6644" y="0"></use><g transform="translate(7144,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335226/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhi2YCyklPqeLVakngWAy2PWCO4MsQ#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335226/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhi2YCyklPqeLVakngWAy2PWCO4MsQ#MJMAIN-30" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335226/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhi2YCyklPqeLVakngWAy2PWCO4MsQ#MJMAIN-30" x="1001" y="0"></use></g><g transform="translate(8646,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335226/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhi2YCyklPqeLVakngWAy2PWCO4MsQ#MJMAIN-29" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335226/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhi2YCyklPqeLVakngWAy2PWCO4MsQ#MJMAIN-32" x="550" y="513"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mo stretchy="false">(</mo><mi>h</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>100</mn><msup><mo stretchy="false">)</mo><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-1"> weight / (height / 100) ^ 2 </script>  ) appeared first in the importance of signs.  Adding the original BMI improved the result; however, the model achieved the greatest improvement after sampling its values.  The sampling threshold was chosen on the basis of quartiles, and the number was determined on the basis of cv, visual validation of distributions.  The figures show the transition from the original BMI to the discretized BMI. </p><br><p><img src="https://habrastorage.org/web/266/f8c/a60/266f8ca608e5474c937ddd1a758967f8.png"><br><img src="https://habrastorage.org/web/9e5/8d6/69f/9e58d669f0604126832c507d0292811c.png"></p><br><p>  Similarly, discretization with a small number of categories was applied to height and weight, and pressure and pulse were rounded to an accuracy of 5. <br>  Search for new signs and their selection was done manually.  Only a small number of new signs could improve CV, and all of them showed a relatively small increase: </p><br><ul><li>  Pulse pressure - the difference between the upper and lower pressure </li><li>  Is the pressure normal (85 &lt;= ap_hi &lt;= 125 &amp; 55 &lt;= ap_lo &lt;= 85) </li><li>  The last digit in the pressure before rounding + permutation of the probability of CVD </li><li>  Parity analogue of the year (age - (age / 2) .round () * 2)&gt; 0 </li></ul><br><p>  The final importance of features (with discretization) for one xgb model can be seen on the graph: </p><br><p><img src="https://habrastorage.org/web/757/637/72d/75763772dabb489b8da0d1e5d9c410bb.png"></p><br><p>  An hour before the end, I had a rather simple model of 2 xgb using the latest data cleansing and feature sampling.  The code is available on <a href="http://github.com/shayakhmetov/mlbootcampV">github</a> (showed CV 0.5370, public 0.5431, private 0.530569 - also 2nd place). </p><br><h2 id="posledniy-chas-sorevnovaniya">  Last hour of competition </h2><br><p>  Having averaged two or three xgb on the last data preprocessing, I decided to try to average the results of the latest models with some previous ones (various transformations and feature set, data cleaning, models) and surprisingly, the weights averaging 8 previous predictions gave an improvement to public with 0.5430- 31 to 0.54288.  The strategy with weights was fixed immediately - inversely proportional to the rounded 4th digit on the public (for example, 0.5431 weighs 1, 0.5432 - 1/2, 0.5433 - 1/3), which correlated quite well with the fact that the models with the latest data cleansing showed also the best CV values.  These 8 predictions were obtained using one, two, three (most), as well as 9 different xgb models.  All but one were based on the latest data cleansing, differing in the set of new features, discretization or its absence, hyper-parameters, as well as the strategy with NaN.  Further, with the same weighting scheme, adding submits worse (with weights less than 1/4) helped to improve the public to 0.542778 (17 predictions in total, the description can be found on <a href="http://github.com/shayakhmetov/mlbootcampV">github</a> ). </p><br><p>  Of course, it was necessary to keep the results of previous cross-validations in a good way in order to correctly assess the quality of such averaging.  Could there be retraining here?  Guided by the fact that more than 90% of the weight in averaging was for models with stable CV 0.5370-0.5371, it was possible to expect that the models could help in extreme errors of the best simple models, however, in general, the predictions differed little from the best models.  Considering also that the public improved significantly, I chose these two averaging as the final, which resulted in a better model, which showed 2nd place with a private 0.5304688.  You may notice that a simple solution, described above, and which was the base in this averaging, would also show 2nd place, but it is less stable. </p><br><h2 id="vyuchennye-uroki">  Lessons learned </h2><br><p>  The final averaging showed that using a combination of relatively simple models on different features / preprocessing can give better results than using multiple models on the same data.  Unfortunately, during the competition I searched for exactly one ‚Äúideal‚Äù data cleansing, one feature conversion, etc. </p><br><p>  Also, for myself, I noticed that in addition to frequent commits in git, it is desirable to store the results of cross-validation of previous models, so that you can quickly assess what mixing of various signs \ preprocessing \ models gives the greatest increase.  However, there are exceptions to the rules, for example, if there is only an hour left before the end of the competition. </p><br><p>  Judging by the results of other participants, I had to continue my experiments with the inclusion of neural networks, stacking.  They, however, were present in my final submission, but only indirectly with little weight. </p><br><p>  In conclusion, the presentation of the author is available <a href="https://www.youtube.com/watch%3Fv%3DQaxExZRWZNA">here</a> , and the presentation is also on <a href="http://github.com/shayakhmetov/mlbootcampV">github</a> . <br></p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/335226/">https://habr.com/ru/post/335226/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../335216/index.html">Asymmetric cryptography and public key cryptography are not the same thing?</a></li>
<li><a href="../335218/index.html">CNCF offered free open source cloud for DevOps / microservices</a></li>
<li><a href="../335220/index.html">How Discord Scaled Elixir to 5 Million Concurrent Users</a></li>
<li><a href="../335222/index.html">Microsoft Office Automation: Another Macro Virus Hole</a></li>
<li><a href="../335224/index.html">Optimization Method Trust-Region DOGLEG. Python implementation example</a></li>
<li><a href="../335228/index.html">Dagaz: Steps</a></li>
<li><a href="../335230/index.html">The number game: how algorithmic trading will change the sphere of finance</a></li>
<li><a href="../335232/index.html">Daniel Story Comics (Part 2)</a></li>
<li><a href="../335234/index.html">"There are pluses for both admins and developers": Oleg Anastasyev about the Odnoklassniki cloud</a></li>
<li><a href="../335238/index.html">What can and can not neural network: a five-minute guide for beginners</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>