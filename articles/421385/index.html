<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Learn OpenGL. Lesson 5.10 - Screen Space Ambient Occlusion</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="SSAO 
 The theme of background lighting was covered by us in the lesson on the basics of lighting , but only in passing. Let me remind you: the backgr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Learn OpenGL. Lesson 5.10 - Screen Space Ambient Occlusion</h1><div class="post__text post__text-html js-mediator-article"><img align="left" src="https://habrastorage.org/web/c9e/9b2/a3b/c9e9b2a3baf749ab8e2b385c6d93d966.png" alt="Ogl3" width="300"><h2>  SSAO </h2><br>  The theme of background lighting was covered by us in the lesson on the <a href="https://habrahabr.ru/post/333932">basics of lighting</a> , but only in passing.  Let me remind you: the background component of illumination is the essence of a constant value added to all calculations of the illumination of the scene to simulate the process <i>of light scattering</i> .  In the real world, light experiences many reflections with varying degrees of intensity, which results in equally uneven illumination of indirectly illuminated portions of the scene.  It is obvious that the illumination with constant intensity is not very plausible. <br><br>  One type of approximate calculation of shading from indirect illumination is the <i>ambient occlusion (AO</i> ) algorithm, which simulates the weakening of indirect illumination in the vicinity of corners, folds, and other surface irregularities.  Such elements, in the main, considerably overlap with the adjacent geometry and therefore leave less opportunities for the light rays to escape, obscuring these areas. <br><br>  Below is a comparison of the rendering without and using the AO algorithm.  Pay attention to how the intensity of background lighting in the vicinity of the corners of the walls and other sharp surface breaks decreases: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <div style="text-align:center;"><img src="https://habrastorage.org/webt/6s/8z/kv/6s8zkvpob8nbgaails8mtfutgw8.png"></div><br>  The effect may not be very noticeable, but the presence of the effect in the whole scene adds to its realism due to the additional illusion of depth created by small details of the self-shadowing effect. <br><a name="habracut"></a><br><div class="spoiler">  <b class="spoiler_title">Content</b> <div class="spoiler_text">  Part 1. Start <br><br><ol><li>  <a href="https://habrahabr.ru/post/310790/">Opengl</a> </li><li>  <a href="https://habrahabr.ru/post/311198/">Creating a window</a> </li><li>  <a href="https://habrahabr.ru/post/311234/">Hello window</a> </li><li>  <a href="https://habrahabr.ru/post/311808/">Hello triangle</a> </li><li>  <a href="https://habrahabr.ru/post/313380/">Shaders</a> </li><li>  <a href="https://habrahabr.ru/post/315294/">Textures</a> </li><li>  <a href="https://habrahabr.ru/post/319144/">Transformations</a> </li><li>  <a href="https://habrahabr.ru/post/324968/">Coordinate systems</a> </li><li>  <a href="https://habrahabr.ru/post/327604/">Camera</a> </li></ol><br>  Part 2. Basic lighting <br><br><ol><li>  <a href="https://habrahabr.ru/post/329592/">Colors</a> </li><li>  <a href="https://habrahabr.ru/post/333932/">Lighting Basics</a> </li><li>  <a href="https://habrahabr.ru/post/336166/">Materials</a> </li><li>  <a href="https://habrahabr.ru/post/337550/">Texture Cards</a> </li><li>  <a href="https://habrahabr.ru/post/337642/">Sources of light</a> </li><li>  <a href="https://habrahabr.ru/post/338254/">Multiple light sources</a> </li></ol><br>  Part 3. Loading 3D Models <br><br><ol><li>  <a href="https://habrahabr.ru/post/338436/">Assimp library</a> </li><li>  <a href="https://habrahabr.ru/post/338436/">Mesh mesh class</a> </li><li>  <a href="https://habrahabr.ru/post/338998/">3D model class</a> </li></ol><br>  Part 4. OpenGL advanced features <br><br><ol><li>  <a href="https://habrahabr.ru/post/342610/">Depth test</a> </li><li>  <a href="https://habrahabr.ru/post/344238/">Stencil test</a> </li><li>  <a href="https://habrahabr.ru/post/343096/">Mixing colors</a> </li><li>  <a href="https://habrahabr.ru/post/346964/">Face clipping</a> </li><li>  <a href="https://habrahabr.ru/post/347354/">Frame buffer</a> </li><li>  <a href="https://habrahabr.ru/post/347750/">Cubic cards</a> </li><li>  <a href="https://habrahabr.ru/post/350008/">Advanced data handling</a> </li><li>  <a href="https://habrahabr.ru/post/350156/">Advanced GLSL</a> </li><li>  <a href="https://habrahabr.ru/post/350782/">Geometric shader</a> </li><li>  <a href="https://habrahabr.ru/post/352962/">Instancing</a> </li><li>  <a href="https://habrahabr.ru/post/351706/">Smoothing</a> </li></ol><br>  Part 5. Advanced Lighting <br><br><ol><li>  <a href="https://habrahabr.ru/post/353054/">Advanced lighting.</a>  <a href="https://habrahabr.ru/post/353054/">Model Blinna-Phong.</a> </li><li>  <a href="https://habrahabr.ru/post/353632/">Gamma Correction</a> </li><li>  <a href="https://habrahabr.ru/post/353956/">Shadow maps</a> </li><li>  <a href="https://habr.com/post/354208/">Omnidirectional shadow maps</a> </li><li>  <a href="https://habr.com/post/415579/">Normal mapping</a> </li><li>  <a href="https://habr.com/post/416163/">Parallax mapping</a> </li><li>  <a href="https://habr.com/post/420409/">Hdr</a> </li><li>  <a href="https://habr.com/post/420375/">Bloom</a> </li><li>  <a href="https://habr.com/post/420565/">Deferred rendering</a> </li><li>  <b>SSAO</b> </li></ol><br></div></div><br>  It is worth noting that the algorithms for calculating the AO are quite resource-intensive, since they require an analysis of the surrounding geometry.  In a naive implementation, it would be possible to simply release a multitude of rays at each point of the surface and determine the degree of its shading, but this approach very quickly reaches the limit of resource intensity that is acceptable for interactive applications.  Fortunately, in 2007, Crytek published a paper describing its own approach to the implementation of the <i>Screen-Space Ambient Occlusion, SSAO algorithm</i> , which was used in the release version of the Crysis game.  The approach calculated the degree of shading in the screen space using only the current depth buffer instead of actual data about the surrounding geometry.  Such optimization radically accelerated the algorithm in comparison with the reference implementation and, at the same time, gave mostly plausible results, which made this approach an approximate calculation of background shading as the de facto standard in the industry. <br><br>  The principle on which the algorithm is based is quite simple: for each fragment of full-screen quad, <i>the shading factor</i> ( <i>occlusion factor</i> ) is calculated based on the depth values ‚Äã‚Äãof the surrounding fragments.  The calculated shading coefficient is then used to reduce the background light intensity (up to complete exclusion).  Obtaining a coefficient requires collecting depth data from a variety of samples from a spherical region surrounding the fragment in question, and comparing these depth values ‚Äã‚Äãwith the depth of the fragment in question.  The number of samples having a depth greater than the current fragment directly determines the shading factor.  Look at this scheme: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/y5/yh/8o/y5yh8oeqvguchqopeu7nz0g-tsy.png"></div><br>  Here, each gray point lies inside a certain geometric object, and therefore contributes to the value of the shading coefficient.  The more samples there are inside the geometry of surrounding objects, the less will be the residual intensity of the background shading in this area. <br><br>  It is obvious that the quality and realism of the effect directly depends on the number of samples taken.  With a small number of samples, the accuracy of the algorithm decreases and leads to the appearance of a <i>banding</i> artifact or ‚Äúpolishing‚Äù due to abrupt transitions between areas with very different shading coefficients.  A large number of samples simply kills performance.  Randomization of the core samples allows, with similar results in quality, to slightly reduce the number of samples required.  This implies a reorientation by turning to a random angle of the set of sample vectors.  However, introducing randomness immediately brings a new problem in the form of a noticeable noise pattern, which requires the use of blur filters to smooth out the result.  Below is an example of how the algorithm works (by <a href="http://john-chapman-graphics.blogspot.com/">John Chapman</a> ) and its typical problems: banding and noise patterns. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/r_/ay/l3/r_ayl3jjozsa6fyuni69ejkwpei.jpeg"></div><br>  As can be seen, a noticeable polish due to the small number of samples is well removed by introducing randomization of the orientation of the samples. <br><br>  Crytek's SSAO concrete implementation had a recognizable visual style.  Since Crytek specialists used a spherical sampling core, this affected even flat surfaces such as walls, making them shaded ‚Äî in fact, half of the core sample size was immersed under the geometry.  Below is a screenshot of a scene from Crysis, depicted in grayscale based on the value of the shading coefficient.  Here the effect of "dullness" is clearly visible: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/j9/zr/r8/j9zrr81dluj-5eobuqcgst48om8.jpeg"></div><br>  To avoid this effect, we move from the spherical core of the sample to a hemisphere oriented along the normal to the surface: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/br/pf/3v/brpf3vfbmzd9pmna58ub5x7-age.png"></div><br>  Sampling from such a <i>hemisphere oriented normal</i> ( <i>normal-oriented hemisphere</i> ) we do not have to take into account in the calculation of the shading coefficient fragments lying under the surface of the adjacent surface.  This approach removes unnecessary shading, in general, gives more realistic results.  This lesson will use a hemispheric approach and a slightly refined code from a brilliant SSAO lesson from <a href="http://john-chapman-graphics.blogspot.com/">John Chapman</a> . <br><br><h2>  Source buffer </h2><br>  The process of calculating the shading factor in each fragment requires the presence of data on the surrounding geometry.  Specifically, we need the following data: <br><br><ul><li>  Position vector for each fragment; </li><li>  Normal vector for each fragment; </li><li>  Diffuse color for each fragment; </li><li>  Core sampling; </li><li>  Random vector of rotation for each fragment, used in reorientation of the sample core. </li></ul><br>  Using the data on the coordinates of the fragment in the species space, we can orient the hemisphere of the sample core along the normal vector defined in the species space for the current fragment.  Then the resulting core is used to make samples with different offsets from the texture, which stores the data on the coordinates of the fragments.  We make a set of samples in each fragment and each performed sample compare its depth value with the depth value from the fragment coordinate buffer to estimate the shading value.  The resulting value is then used to limit the contribution of the background component in the final lighting calculation.  Using a fragmentary random vector of rotation, we can significantly reduce the required number of samples to obtain a decent result, then this will be demonstrated. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wv/xo/aj/wvxoajroexwvjgq77n81-fjhats.png"></div><br>  Since SSAO is an effect implemented in the screen space, it is possible to directly calculate the rendering of a full-screen quad.  But then we will not have data on the geometry of the scene.  To circumvent this limitation, we will render all the necessary information into textures, which will later be used in the SSAO shader to access the geometric and other information about the scene.  If you carefully followed these lessons, you should already know in the described approach the appearance of the deferred shading algorithm.  In many ways, the effect of SSAO as a native gets into the render with deferred shading - after all, textures that store coordinates and normals are already available in the G-buffer. <br><br><blockquote>  In this lesson, the effect is implemented on top of a somewhat simplified version of the code from the lesson about <a href="https://habr.com/post/420565">deferred lighting</a> .  If you have not yet become acquainted with the principles of deferred lighting - I strongly advise you to refer to this lesson. <br></blockquote><br>  Since access to the fragment information about coordinates and normals should already be available through the G-buffer, the fragment shader of the geometry processing stage is quite simple: <br><br><pre><code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#version 330 core layout (location = 0) out vec4 gPosition; layout (location = 1) out vec3 gNormal; layout (location = 2) out vec4 gAlbedoSpec; in vec2 TexCoords; in vec3 FragPos; in vec3 Normal; void main() { </span><span class="hljs-comment"><span class="hljs-meta"><span class="hljs-comment">//        gPosition = FragPos; //       gNormal = normalize(Normal); //    -   gAlbedoSpec.rgb = vec3(0.95); }</span></span></span></span></code> </pre> <br>  Since the SSAO algorithm is an effect in the screen space, and the shading coefficient is calculated based on the visible area of ‚Äã‚Äãthe scene, it makes sense to conduct calculations in the view space.  In this case, the <i>FragPos</i> variable obtained from the vertex shader wounds the situation in the species space.  It is worth making sure that the coordinates and normals data is stored in the G-buffer in the species space, since all further calculations will be carried out in it. <br><br><blockquote>  It is possible to restore the position vector on the basis of only a known fragment depth and a certain amount of mathematical magic, as described, for example, in Matt Pettineo's <a href="https://mynameismjp.wordpress.com/2010/09/05/position-from-depth-3/">blog</a> .  This, of course, requires a lot of computational cost, but it eliminates the need to store position data in a G-buffer, which takes a lot of video memory.  However, for the sake of simplicity of the example code, we will leave this approach for personal study. </blockquote><br>  Texture <i>gPosition</i> color buffer is configured as follows: <br><br><pre> <code class="cpp hljs">glGenTextures(<span class="hljs-number"><span class="hljs-number">1</span></span>, &amp;gPosition); glBindTexture(GL_TEXTURE_2D, gPosition); glTexImage2D(GL_TEXTURE_2D, <span class="hljs-number"><span class="hljs-number">0</span></span>, GL_RGB16F, SCR_WIDTH, SCR_HEIGHT, <span class="hljs-number"><span class="hljs-number">0</span></span>, GL_RGB, GL_FLOAT, <span class="hljs-literal"><span class="hljs-literal">NULL</span></span>); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);</code> </pre> <br>  This texture stores the coordinates of the fragments and can be used to obtain depth data for each point from the sample core.  I will note that the texture uses the floating-point data format - this allows the coordinates of the fragments not to be reduced to the interval [0., 1.].  Also note the replay mode - set <i>GL_CLAMP_TO_EDGE</i> .  This is necessary to eliminate the possibility of not purposely oversampling in the screen space.  Going beyond the main range of texture coordinates will give us incorrect position and depth data. <br><br>  Next, we will deal with the formation of a hemispherical core of samples and the creation of a method of random orientation. <br><br><h2>  Creation of a normal hemisphere </h2><br>  So, the task is to create a set of sample points located inside a hemisphere oriented along the normal to the surface.  Since the creation of the sampling kernel for all possible directions of the normal is computationally unattainable, we use the transition to the <a href="https://habr.com/post/415579">tangent space</a> , where the normal is always represented as a vector in the direction of the positive semi-axis <i>Z.</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/br/pf/3v/brpf3vfbmzd9pmna58ub5x7-age.png"></div><br>  Assuming the hemisphere radius to be single, the process of forming a core of a sample of 64 points looks like this: <br><br><pre> <code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">//      0.0 - 1.0 std::uniform_real_distribution&lt;float&gt; randomFloats(0.0, 1.0); std::default_random_engine generator; std::vector&lt;glm::vec3&gt; ssaoKernel; for (unsigned int i = 0; i &lt; 64; ++i) { glm::vec3 sample( randomFloats(generator) * 2.0 - 1.0, randomFloats(generator) * 2.0 - 1.0, randomFloats(generator) ); sample = glm::normalize(sample); sample *= randomFloats(generator); float scale = (float)i / 64.0; ssaoKernel.push_back(sample); }</span></span></code> </pre> <br>  Here we randomly select the <i>x</i> and <i>y</i> coordinates in the interval [-1., 1.], and the <i>z</i> coordinate in the interval [0., 1.] (if the interval were the same as for <i>x</i> and <i>y</i> , we would get a spherical core sampling).  The resulting vector of the samples will be limited to the hemispheres, since the core of the sample will ultimately be oriented along the normal to the surface. <br><br>  At the moment, all sample points are randomly distributed inside the core, but for the sake of effect quality, samples that are closer to the origin of the core coordinates would be worth making a larger contribution to the calculation of the shading factor.  This can be achieved by changing the distribution of formed sample points, increasing their density near the origin.  This task is easily accomplished using the accelerated interpolation function: <br><br><pre> <code class="cpp hljs">scale = lerp(<span class="hljs-number"><span class="hljs-number">0.1f</span></span>, <span class="hljs-number"><span class="hljs-number">1.0f</span></span>, scale * scale); sample *= scale; ssaoKernel.push_back(sample); }</code> </pre> <br>  The <i>lerp ()</i> function is defined as: <br><br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">float</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">lerp</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> a, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> b, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> f)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> a + f * (b - a); }</code> </pre> <br>  Such a trick gives us a modified distribution, where most of the sample points lie near the origin of the coordinates of the nucleus. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/h7/dy/xm/h7dyxm-1zerxg1krzbxszp7kzqi.png"></div><br>  Each of the obtained sampling vectors will be used to offset the fragment coordinates in the species space to obtain data on the surrounding geometry.  To obtain decent results when working in species space, an impressive number of readings may be required, which inevitably will hurt performance.  However, the introduction of pseudo-random noise or the rotation of the vectors of the samples in each processed fragment will significantly reduce the required number of samples with comparable quality. <br><br><h2>  Random rotation of the sample core </h2><br>  So, introducing randomness into the distribution of points of the sample core can significantly reduce the requirement for the number of these points to obtain a decent effect quality.  It would be possible to create a random rotation vector for each fragment of the scene, but this is too expensive for memory.  It is more efficient to create a small texture containing a set of random rotation vectors, and then simply use it with the repeat mode set <i>GL_REPEAT</i> . <br><br>  Create a 4x4 array and fill it with random rotation vectors oriented along the normal vector in the tangent space: <br><br><pre> <code class="cpp hljs"><span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::<span class="hljs-built_in"><span class="hljs-built_in">vector</span></span>&lt;glm::vec3&gt; ssaoNoise; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">unsigned</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; <span class="hljs-number"><span class="hljs-number">16</span></span>; i++) { glm::<span class="hljs-function"><span class="hljs-function">vec3 </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">noise</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( randomFloats(generator) * </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">2.0</span></span></span></span><span class="hljs-function"><span class="hljs-params"> - </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1.0</span></span></span></span><span class="hljs-function"><span class="hljs-params">, randomFloats(generator) * </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">2.0</span></span></span></span><span class="hljs-function"><span class="hljs-params"> - </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1.0</span></span></span></span><span class="hljs-function"><span class="hljs-params">, </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.0f</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>; ssaoNoise.push_back(noise); }</code> </pre> <br>  Since the core is aligned along the positive semi-axis <i>Z</i> in the tangent space, the component <i>z is</i> left equal to zero - this will ensure rotation only around the <i>Z</i> axis. <br><br>  Next, create a 4x4 size texture and fill in our array of rotation vectors.  Be sure to use the <i>GL_REPEAT</i> repeat <i>mode</i> for texture tiling: <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">unsigned</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> noiseTexture; glGenTextures(<span class="hljs-number"><span class="hljs-number">1</span></span>, &amp;noiseTexture); glBindTexture(GL_TEXTURE_2D, noiseTexture); glTexImage2D(GL_TEXTURE_2D, <span class="hljs-number"><span class="hljs-number">0</span></span>, GL_RGB16F, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, GL_RGB, GL_FLOAT, &amp;ssaoNoise[<span class="hljs-number"><span class="hljs-number">0</span></span>]); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT);</code> </pre> <br>  Well, now we have all the data necessary for the direct implementation of the SSAO algorithm! <br><br><h2>  SSAO Shader </h2><br>  The effect shader will be executed for each fragment of full-screen quad, calculating the shading factor in each of them.  Since the results will be used in another stage of the rendering that creates the final lighting, we will need to create another framebuffer object to store the result of the shader: <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">unsigned</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> ssaoFBO; glGenFramebuffers(<span class="hljs-number"><span class="hljs-number">1</span></span>, &amp;ssaoFBO); glBindFramebuffer(GL_FRAMEBUFFER, ssaoFBO); <span class="hljs-keyword"><span class="hljs-keyword">unsigned</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> ssaoColorBuffer; glGenTextures(<span class="hljs-number"><span class="hljs-number">1</span></span>, &amp;ssaoColorBuffer); glBindTexture(GL_TEXTURE_2D, ssaoColorBuffer); glTexImage2D(GL_TEXTURE_2D, <span class="hljs-number"><span class="hljs-number">0</span></span>, GL_RED, SCR_WIDTH, SCR_HEIGHT, <span class="hljs-number"><span class="hljs-number">0</span></span>, GL_RGB, GL_FLOAT, <span class="hljs-literal"><span class="hljs-literal">NULL</span></span>); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST); glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, ssaoColorBuffer, <span class="hljs-number"><span class="hljs-number">0</span></span>);</code> </pre> <br>  Since the result of the algorithm is a single real number within [0., 1.], for storage it will be sufficient to create a texture with a single accessible component.  That is why <i>GL_RED</i> is set as the internal format for the color buffer. <br><br>  In general, the rendering process of the SSAO stage looks like this: <br><br><pre> <code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">//  :  G- glBindFramebuffer(GL_FRAMEBUFFER, gBuffer); [...] glBindFramebuffer(GL_FRAMEBUFFER, 0); //  G-      SSAO glBindFramebuffer(GL_FRAMEBUFFER, ssaoFBO); glClear(GL_COLOR_BUFFER_BIT); glActiveTexture(GL_TEXTURE0); glBindTexture(GL_TEXTURE_2D, gPosition); glActiveTexture(GL_TEXTURE1); glBindTexture(GL_TEXTURE_2D, gNormal); glActiveTexture(GL_TEXTURE2); glBindTexture(GL_TEXTURE_2D, noiseTexture); shaderSSAO.use(); SendKernelSamplesToShader(); shaderSSAO.setMat4("projection", projection); RenderQuad(); glBindFramebuffer(GL_FRAMEBUFFER, 0); //  :    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); shaderLightingPass.use(); [...] glActiveTexture(GL_TEXTURE3); glBindTexture(GL_TEXTURE_2D, ssaoColorBuffer); [...] RenderQuad();</span></span></code> </pre> <br>  The <i>shaderSSAO</i> shader takes the G-buffer textures it needs as input, as well as the noise texture and the core of the sample: <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#version 330 core out float FragColor; in vec2 TexCoords; uniform sampler2D gPosition; uniform sampler2D gNormal; uniform sampler2D texNoise; uniform vec3 samples[64]; uniform mat4 projection; </span><span class="hljs-comment"><span class="hljs-meta"><span class="hljs-comment">//             //      1280x720 const vec2 noiseScale = vec2(1280.0/4.0, 720.0/4.0); void main() { [...] }</span></span></span></span></code> </pre> <br>  Notice the <i>noiseScale</i> variable.  Our small, noisy texture should be covered over the entire surface of the screen, but since the texture coordinates of <i>TexCoords</i> are within [0., 1.] this will not happen without our intervention.  For this purpose, we calculate the multiplier for the texture coordinates, which is found as the ratio of the screen size to the size of the noise texture: <br><br><pre> <code class="cpp hljs">vec3 fragPos = texture(gPosition, TexCoords).xyz; vec3 normal = texture(gNormal, TexCoords).rgb; vec3 randomVec = texture(texNoise, TexCoords * noiseScale).xyz;</code> </pre> <br>  Since creating a noise texture <i>texNoise</i> we set the repeat mode to <i>GL_REPEAT</i> , now it will be repeated many times on the surface of the screen.  With the <i>randomVec</i> , <i>fragPos</i> and <i>normal</i> values ‚Äã‚Äãin <i>hand</i> , we can create a TBN transformation matrix from the tangent to the specific space: <br><br><pre> <code class="cpp hljs">vec3 tangent = normalize(randomVec - normal * dot(randomVec, normal)); vec3 bitangent = cross(normal, tangent); mat3 TBN = mat3(tangent, bitangent, normal);</code> </pre> <br>  Using the Gram-Schmidt process, we create an orthogonal basis randomly tilted in each fragment based on a random value <i>randomVec</i> .  Important point: since in this case it does not matter to us that the TBN matrix is ‚Äã‚Äãprecisely oriented along the surface of the triangle (as is the case with parallax mapping, note), we do not need pre-calculated data on tangents and bicandals. <br><br>  Next, we go through the array of the sample core, translate each sample vector from the tangent space into the viewport, and get its sum with the current position of the fragment.  Then we compare the value of the depth of the resulting sum with the value of the depth obtained by sampling from the corresponding texture of the G-buffer. <br><br>  While it sounds confusing, let's break it down into steps: <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">float</span></span> occlusion = <span class="hljs-number"><span class="hljs-number">0.0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; kernelSize; ++i) { <span class="hljs-comment"><span class="hljs-comment">//     vec3 sample = TBN * samples[i]; //      - sample = fragPos + sample * radius; [...] }</span></span></code> </pre> <br>  Here, <i>kernelSize</i> and <i>radius</i> are variables that control the characteristics of the effect.  In this case, they are 64 and 0.5, respectively.  At each iteration, we translate the core vector of the sample into the view space.  Next, add to the obtained value of the sample offset in the species space value of the position of the fragment in the species space.  The offset value is then multiplied by the radius variable, which controls the core radius of the SSAO effect sample. <br><br>  After these steps, we need to convert the resulting vector <i>sample</i> into screen space, so that we can sample the G-buffer texture that stores the positions and depths of the fragments using the resulting projected value.  Since the <i>sample</i> is in view space, we need a projection <i>projection</i> matrix: <br><br><pre> <code class="cpp hljs">vec4 offset = vec4(sample, <span class="hljs-number"><span class="hljs-number">1.0</span></span>); offset = projection * offset; <span class="hljs-comment"><span class="hljs-comment">//     offset.xyz /= offset.w; //   offset.xyz = offset.xyz * 0.5 + 0.5; //    [0., 1.]</span></span></code> </pre> <br>  After conversion to the clip space, we manually perform the perspective division by simply dividing the <i>xyz</i> components into the <i>w</i> component.  The resulting vector in the normalized coordinates of the device ( <i>NDC</i> ) is translated into the interval of values ‚Äã‚Äã[0., 1.] so that it can be used as texture coordinates: <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">float</span></span> sampleDepth = texture(gPosition, offset.xy).z;</code> </pre> <br>  Use the <i>xy</i> components of the <i>sample</i> vector to fetch the G-buffer positions from the texture.  We obtain the depth value ( <i>z</i> components) corresponding to the sampling vector when viewed from the observer‚Äôs position (this is the first non-obscured visible fragment).  If the obtained sample depth is greater than the stored depth, then we increase the shading factor: <br><br><pre> <code class="cpp hljs">occlusion += (sampleDepth &gt;= sample.z + bias ? <span class="hljs-number"><span class="hljs-number">1.0</span></span> : <span class="hljs-number"><span class="hljs-number">0.0</span></span>);</code> </pre> <br>  Notice the <i>bias</i> offset, which is added to the original fragment depth (in the example set to 0.025).  This offset is not always necessary, but the presence of a variable allows you to control how the SSAO effect looks, and, in certain situations, removes problems with ripples in shaded areas. <br><br>  But that's not all, since such an implementation leads to noticeable artifacts.  It manifests itself in cases where a fragment is considered lying near the edge of a certain surface.  In such situations, when comparing depths, the algorithm will inevitably capture the depths of the surfaces, which may lie very far behind the considered one.  In these places, the algorithm erroneously greatly increases the degree of shading, which will create noticeable dark halos at the edges of the objects.  An artifact is treated by introducing an additional distance check (example by <a href="http://john-chapman-graphics.blogspot.com/">John Chapman</a> ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/zv/yv/eq/zvyveqh3zc_rjcy6fo-z8d76eme.png"></div><br>  The check will limit the contribution to the shading factor only for depth values ‚Äã‚Äãwithin the radius of the sample: <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">float</span></span> rangeCheck = smoothstep(<span class="hljs-number"><span class="hljs-number">0.0</span></span>, <span class="hljs-number"><span class="hljs-number">1.0</span></span>, radius / <span class="hljs-built_in"><span class="hljs-built_in">abs</span></span>(fragPos.z - sampleDepth)); occlusion += (sampleDepth &gt;= sample.z + bias ? <span class="hljs-number"><span class="hljs-number">1.0</span></span> : <span class="hljs-number"><span class="hljs-number">0.0</span></span>) * rangeCheck;</code> </pre> <br>  We also use the GLSL <i>smoothstep ()</i> function, which implements a smooth interpolation of the third parameter between the first and second.  In this case, returning 0 if the third parameter is less than or equal to the first, or 1 if the third parameter is greater than or equal to the second.  If the depth difference is within the <i>radius</i> , then its value will be smoothly smoothed in the interval [0., 1.] in accordance with this curve: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jq/9h/p4/jq9hp4-yun_sc277m6pslbjyin0.png"></div><br>  If we used clear boundaries in the depth check conditions, this would add sharp artifacts in the places where the difference values ‚Äã‚Äãof the depths are outside the <i>radius</i> . <br><br>  With the final touch, we normalize the shading coefficient using the sample core size and record the result.  We also invert the final value by subtracting it from the unit, so that the final value can be used directly to modulate the background component of the lighting without additional actions: <br><br><pre> <code class="cpp hljs">} occlusion = <span class="hljs-number"><span class="hljs-number">1.0</span></span> - (occlusion / kernelSize); FragColor = occlusion;</code> </pre> <br>  For a scene with a lying nanosuit we know, performing an SSAO shader gives the following texture: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-z/a4/fx/-za4fxhsbref6easc-cgxnsp94q.png"></div><br>  As you can see, the effect of background shading creates a good illusion of depth.  The only output image of the shader already allows you to distinguish the details of the suit and make sure that it really lies on the floor, and does not levitate at some distance from it. <br><br>  And yet the effect is far from ideal, since the noise pattern introduced by the texture of random rotation vectors is easily noticeable.  To smooth the result of the SSAO calculation, we apply a blur filter. <br><br><h2>  Blur background shading </h2><br>  After constructing the result of the SSAO and before the final mixing of the light, it is necessary to blur the texture that stores the data on the shading coefficient.  For this we will create another framebuffer: <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">unsigned</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> ssaoBlurFBO, ssaoColorBufferBlur; glGenFramebuffers(<span class="hljs-number"><span class="hljs-number">1</span></span>, &amp;ssaoBlurFBO); glBindFramebuffer(GL_FRAMEBUFFER, ssaoBlurFBO); glGenTextures(<span class="hljs-number"><span class="hljs-number">1</span></span>, &amp;ssaoColorBufferBlur); glBindTexture(GL_TEXTURE_2D, ssaoColorBufferBlur); glTexImage2D(GL_TEXTURE_2D, <span class="hljs-number"><span class="hljs-number">0</span></span>, GL_RED, SCR_WIDTH, SCR_HEIGHT, <span class="hljs-number"><span class="hljs-number">0</span></span>, GL_RGB, GL_FLOAT, <span class="hljs-literal"><span class="hljs-literal">NULL</span></span>); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST); glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, ssaoColorBufferBlur, <span class="hljs-number"><span class="hljs-number">0</span></span>);</code> </pre> <br>  The tiling noise texture in screen space provides well-defined randomness characteristics that can be used to your advantage when creating a blur filter: <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#version 330 core out float FragColor; in vec2 TexCoords; uniform sampler2D ssaoInput; void main() { vec2 texelSize = 1.0 / vec2(textureSize(ssaoInput, 0)); float result = 0.0; for (int x = -2; x &lt; 2; ++x) { for (int y = -2; y &lt; 2; ++y) { vec2 offset = vec2(float(x), float(y)) * texelSize; result += texture(ssaoInput, TexCoords + offset).r; } } FragColor = result / (4.0 * 4.0); }</span></span></code> </pre> <br>  The shader simply goes through SSAO texture texels with an offset from -2 to +2, which corresponds to the actual size of the noise texture.  The offset is equal to the exact size of one texel: for the calculation, the <i>textureSize ()</i> function is used, which returns <i>vec2</i> with the dimensions of the specified texture.  So  the shader simply averages the results stored in the texture, which gives a quick and fairly effective blur: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5p/0g/i7/5p0gi7qn_v5w738uyindxexally.png"></div><br>  Total we have a texture with the hands on the background shading for each fragment on the screen - everything is ready for the final stage of image reduction! <br><br><h2>  Apply background shading </h2><br>  The stage of applying the shading coefficient in the final lighting calculation is surprisingly simple: for each fragment, it is enough to simply multiply the value of the background component of the light source by the shading coefficient from the prepared texture.  You can take a ready-made shader with the Blinna-Phong model from the lesson on <a href="https://habr.com/post/420565">deferred shading</a> and tweak it a bit: <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#version 330 core out vec4 FragColor; in vec2 TexCoords; uniform sampler2D gPosition; uniform sampler2D gNormal; uniform sampler2D gAlbedo; uniform sampler2D ssao; struct Light { vec3 Position; vec3 Color; float Linear; float Quadratic; float Radius; }; uniform Light light; void main() { </span><span class="hljs-comment"><span class="hljs-meta"><span class="hljs-comment">//    G- vec3 FragPos = texture(gPosition, TexCoords).rgb; vec3 Normal = texture(gNormal, TexCoords).rgb; vec3 Diffuse = texture(gAlbedo, TexCoords).rgb; float AmbientOcclusion = texture(ssao, TexCoords).r; //   -    //   :   -  vec3 ambient = vec3(0.3 * Diffuse * AmbientOcclusion); vec3 lighting = ambient; //    (0, 0, 0)   - vec3 viewDir = normalize(-FragPos); //   vec3 lightDir = normalize(light.Position - FragPos); vec3 diffuse = max(dot(Normal, lightDir), 0.0) * Diffuse * light.Color; //   vec3 halfwayDir = normalize(lightDir + viewDir); float spec = pow(max(dot(Normal, halfwayDir), 0.0), 8.0); vec3 specular = light.Color * spec; //   float dist = length(light.Position - FragPos); float attenuation = 1.0 / (1.0 + light.Linear * dist + light.Quadratic * dist * dist); diffuse *= attenuation; specular *= attenuation; lighting += diffuse + specular; FragColor = vec4(lighting, 1.0); }</span></span></span></span></code> </pre> <br>  There are only two major changes: the transition to calculations in species space and the multiplication of the background lighting component by the value of <i>AmbientOcclusion</i> .  An example of a scene with a single blue point source of light: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bz/8_/1i/bz8_1in-othscilg_udfyscghg0.png"></div><br>  The full source code is <a href="https://learnopengl.com/code_viewer_gh.php%3Fcode%3Dsrc/5.advanced_lighting/9.ssao/ssao.cpp">here</a> . <br><br>  The manifestation of the SSAO effect is highly dependent on parameters such as <i>kernelSize</i> , <i>radius</i> and <i>bias</i> , often their fine tuning is a matter of course the artist‚Äôs <i>workout</i> during the development of a particular location / scene.  There are no ‚Äúbest‚Äù and universal combinations of parameters: for some scenes, a small core radius of the sample is good, others benefit from an increased radius and number of samples.  The example uses 64 sample points, which, frankly, is redundant, but you can always edit the code and see what happens with a smaller number of samples. <br><br>  In addition to these uniforms, which are responsible for adjusting the effect, there is the possibility to explicitly adjust the severity of the background shading effect.  To do this, it is enough to raise the coefficient to a power controlled by another uniform: <br><br><pre> <code class="cpp hljs">occlusion = <span class="hljs-number"><span class="hljs-number">1.0</span></span> - (occlusion / kernelSize); FragColor = <span class="hljs-built_in"><span class="hljs-built_in">pow</span></span>(occlusion, power);</code> </pre> <br>  I advise you to spend some time on the game with the settings, as this will give a better understanding of the nature of the changes in the final picture. <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Summing up, it is worth saying that although the visual effect of the use of SSAO is rather subtle, but in scenes with well-spaced lighting it undeniably adds a noticeable bit of realism. </font><font style="vertical-align: inherit;">Having such a tool in your arsenal is definitely valuable.</font></font><br><br><h2>  Additional resources </h2><br><ol><li> <a href="http://john-chapman-graphics.blogspot.nl/2013/01/ssao-tutorial.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SSAO Tutorial</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : an excellent lesson-article from John Chapman, on the basis of which the code of this lesson is built.</font></font></li><li> <a href="https://mtnphil.wordpress.com/2013/06/26/know-your-ssao-artifacts/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Know your SSAO artifacts</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : A very valuable article lucidly showing not only the most pressing problems with the quality of SSAO, but also ways to solve them. </font><font style="vertical-align: inherit;">Recommended for reading.</font></font></li><li> <a href="http://ogldev.atspace.co.uk/www/tutorial46/tutorial46.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SSAO With Depth Reconstruction</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : A supplement to the main SSAO lesson by the author OGLDev, concerning the frequently used technique to restore the fragment coordinates based on the depth value. </font><font style="vertical-align: inherit;">The importance of this approach is due to the significant savings in memory due to the lack of need to store positions in the G-buffer. </font><font style="vertical-align: inherit;">The approach is so universal, as it applies to SSAO.</font></font></li></ol><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PS</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : We have a </font></font><a href="https://t.me/joinchat/Cpb05A46UPpMWdNVVCb4Vg"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">telegram-konf</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> to coordinate transfers. </font><font style="vertical-align: inherit;">If there is a serious desire to help with the translation, then you are welcome!</font></font></div><p>Source: <a href="https://habr.com/ru/post/421385/">https://habr.com/ru/post/421385/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../421375/index.html">How uncertainty kills commerce</a></li>
<li><a href="../421377/index.html">7 misconceptions of a novice project manager in game dev</a></li>
<li><a href="../421379/index.html">Intel's toxic culture</a></li>
<li><a href="../421381/index.html">Free course "Cisco ASA Administrator"</a></li>
<li><a href="../421383/index.html">Epic Growth Conference Autumn 2018 - a conference on grocery marketing in Moscow</a></li>
<li><a href="../421387/index.html">Interview with Lennart Pottering on Linux Piter about Linux changes, systemd, and why attend conferences</a></li>
<li><a href="../421389/index.html">Separation of administrative authority in Zimbra</a></li>
<li><a href="../421391/index.html">HackThings - a big hackathon on the Internet of things September 7-9 in Skoltech</a></li>
<li><a href="../421393/index.html">Mailchimp dump basket: a guide for the lazy</a></li>
<li><a href="../421395/index.html">Rome Club Report 2018, Chapter 3.7: ‚ÄúClimate: good news, but big problems‚Äù</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>