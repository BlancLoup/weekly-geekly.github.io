<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Build your own failover cloud based on OpenNebula with Ceph, MariaDB Galera Cluster and OpenvSwitch</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This time I would like to tell you how to set up this subject, in particular, each individual component, in order to finally get your own, expandable,...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Build your own failover cloud based on OpenNebula with Ceph, MariaDB Galera Cluster and OpenvSwitch</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/1b8/185/6c4/1b81856c42da42ba903e85e1653969e4.png"><br><br>  This time I would like to tell you how to set up this subject, in particular, each individual component, in order to finally get your own, expandable, fault-tolerant cloud based on OpenNebula.  In this article I will consider the following points: <br><br><ul><li>  <b><a href="http://habrahabr.ru/post/270187/">Install Ceph, distributed storage</a></b> .  <i>(I will describe the installation of a two-tier storage with a caching pool of SSDs)</i> </li><li>  <b><a href="http://habrahabr.ru/post/270187/">Install MySQL, Galera Cluster with master replication</a></b> </li><li>  <b><a href="http://habrahabr.ru/post/270187/">Installing OpenvSwitch soft switch</a></b> </li><li>  <b><a href="http://habrahabr.ru/post/270187/">Installing directly OpenNebula itself</a></b> </li><li>  <b><a href="http://habrahabr.ru/post/270187/">Configuring Failover Cluster</a></b> </li><li>  <b><a href="http://habrahabr.ru/post/270187/">Initial configuration</a></b> </li></ul><br>  The topics themselves are very interesting, so even if you are not interested in the final goal, but you are interested in setting up a separate component.  You are welcome under the cat. <br><a name="habracut"></a><br><img src="https://habrastorage.org/getpro/habr/post_images/e40/61c/7f7/e4061c7f76c18a3953e5953be17fb612.png" alt="image">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1>  Small introduction </h1><br><h2>  So, what do we get in the end? </h2><br>  After reading this article, you will be able to deploy your own flexible, expandable, and, moreover, fault-tolerant, cloud based on OpenNebula.  What do these words mean?  - Let's look at: <br><br><ul><li>  <b>Expandable</b> - this means that you do not have to rebuild your cloud during the expansion.  At any time, you can expand your place in the cloud just by adding additional hard drives to the ceph pool.  You can also configure a new node without problems and enter it into the cluster if you wish. <br><br></li><li>  <b>Flexible</b> - OpenNebula‚Äôs motto is ‚ÄúFlexible Enterprise Cloud Made Simple‚Äù.  OpenNebula is very easy to learn and also very flexible.  You will not be difficult to deal with it, as well as, if necessary, write your module for it, because  the whole system is designed to be as simple and modular as possible. <br><br></li><li>  <b>Failsafe</b> - In the event of a hard drive failure, the cluster itself will be rebuilt so as to provide the necessary number of replicas of your data.  In case of failure of one node, you will not lose control, and the cloud will continue to function until you eliminate the problem. </li></ul><br><br><img src="https://habrastorage.org/getpro/habr/post_images/290/bda/89c/290bda89c44b48438c999d651dd305e0.png" alt="image"><br><h2>  What do we need for this? </h2><br><ul><li>  I will describe the installation on <b>3 nodes</b> , but in your case there can be as many as you want. <br>  You can also install OpenNebula on the same node, but in this case you will not be able to build a fault-tolerant cluster, and your entire installation with this guide will be reduced to installing OpenNebula itself, and for example, OpenvSwitch. <br><br>  <i>By the way, you can also install CentOS on ZFS by reading my <a href="http://habrahabr.ru/post/268711/">previous article</a> (not for production) and configure OpenNebula on ZFS using the <a href="https://github.com/OpenNebula/addon-zfs">ZFS driver</a> written by me</i> <i><br></i> <br></li><li>  Also, for the functioning of Ceph, a <b>10G network is</b> highly desirable.  Otherwise, it does not make sense for you to raise a separate cache pool, since the speed characteristics of your network will be even lower than the write speed to the pool from the HDD alone. <br><br></li><li>  <b>CentOS 7 is</b> installed on all nodes. <br><br></li><li>  Also each node contains: <br><ul><li>  2SSD by 256GB - for cache pool </li><li>  3HDD by 6TB - for the main pool </li><li>  RAM, sufficient for the functioning of Ceph (1GB of RAM per 1TB of data) </li><li>  Well, the resources needed for the cloud itself, CPU and RAM, which we will use to run virtual machines </li></ul><br></li><li>  I also wanted to add that the installation and operation of most components requires disabled <b>SELINUX</b> .  So on all three nodes it is disabled: <pre><code class="bash hljs">sed -is/SELINUX=enforcing/SELINUX=disabled/g /etc/selinux/config setenforce 0</code> </pre> <br></li><li>  Each node has an <a href="https://fedoraproject.org/wiki/EPEL">EPEL</a> repository installed: <pre> <code class="bash hljs">yum install epel-release</code> </pre></li></ul><br><br><h2>  Cluster layout </h2><br>  To understand what is happening, here is an approximate diagram of our future cluster: <br><img src="https://habrastorage.org/getpro/habr/post_images/11e/50e/404/11e50e40492855b7ab40f6dd0b4d93b0.png" alt="image"><br><br>  And a label with the characteristics of each node: <br><table><tbody><tr><th>  <b>Hostname</b> </th><th>  kvm1 </th><th>  kvm2 </th><th>  kvm3 </th></tr><tr><td>  <b>Network interface</b> </td><td>  enp1 </td><td>  enp1 </td><td>  enp1 </td></tr><tr><td>  <b>IP address</b> </td><td>  192.168.100.201 </td><td>  192.168.100.202 </td><td>  192.168.100.203 </td></tr><tr><td>  <b>HDD</b> </td><td>  sdb </td><td>  sdb </td><td>  sdb </td></tr><tr><td>  <b>HDD</b> </td><td>  sdc </td><td>  sdc </td><td>  sdc </td></tr><tr><td>  <b>HDD</b> </td><td>  sdd </td><td>  sdd </td><td>  sdd </td></tr><tr><td>  <b>SSD</b> </td><td>  sde </td><td>  sde </td><td>  sde </td></tr><tr><td>  <b>SSD</b> </td><td>  sdf </td><td>  sdf </td><td>  sdf </td></tr></tbody></table><br><br>  Everything, now it is possible to start setup!  And we begin perhaps with building a repository. <br><br><hr><br><a name="ceph"></a><h1>  Ceph </h1><br>  About ceph on Habr√© has already been written.  For example, <a href="https://habrahabr.ru/users/teraflops/" class="user_link">teraflops</a> described its device and basic concepts in some detail in his <a href="http://habrahabr.ru/company/performix/blog/218065/">article</a> .  Recommended to read. <br><br>  Here I will also describe the ceph setting for storing RBD (RADOS Block Device) block devices for our virtual machines, as well as setting the cache pool to speed up I / O operations in it. <br><br>  So we have three nodes kvm1, kvm2, kvm3.  Each of them has 2 SSD drives and 3 HDDs.  On these drives, we will raise two pools, one - the main on the HDD, the second - caching on the SSD.  In total, we should have something like this: <br><img src="https://habrastorage.org/getpro/habr/post_images/30e/52c/132/30e52c13207ca6b1973c833d930d3ad7.png" alt="image"><br><br><h2>  Training </h2><br>  Installation will be done using ceph-deploy, and it implies installation from the so-called admin server. <br><br>  Any computer with an installed ceph-depoy and ssh client can serve as an admin server, in our case one of the <b>kvm1</b> nodes will act as such server. <br><br>  We need to have a ceph user on each node, as well as allow him to walk between the nodes without a password and execute any commands via sudo without a password. <br><br>  <b>On each node we perform:</b> <br><br><pre> <code class="bash hljs">sudo useradd -d /home/ceph -m ceph sudo passwd ceph sudo <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"ceph ALL = (root) NOPASSWD:ALL"</span></span> &gt; /etc/sudoers.d/ceph sudo chmod 0440 /etc/sudoers.d/ceph</code> </pre><br><br>  <b>Go to kvm1.</b> <br><br>  Now we will generate the key and copy it to the other nodes. <br><pre> <code class="bash hljs">sudo ssh-keygen -f /home/ceph/.ssh/id_rsa sudo cat /home/ceph/.ssh/id_rsa.pub &gt;&gt; /home/ceph/.ssh/authorized_keys sudo chown -R ceph:users /home/ceph/.ssh <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 2 3; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> scp /home/ceph/.ssh/* ceph@kvm<span class="hljs-variable"><span class="hljs-variable">$i</span></span>:/home/ceph/.ssh/ <span class="hljs-keyword"><span class="hljs-keyword">done</span></span></code> </pre><br><br><h2>  Installation </h2><br>  Add the key, install the ceph and ceph-depoy repository from it: <br><br><pre> <code class="bash hljs">sudo rpm --import <span class="hljs-string"><span class="hljs-string">'https://download.ceph.com/keys/release.asc'</span></span> sudo yum -y localinstall http://download.ceph.com/rpm/el7/noarch/ceph-release-1-1.el7.noarch.rpm sudo yum install -y ceph-deploy</code> </pre><br><br>  Ok, now we go for user ceph and create a folder in which we will store configs and keys for ceph. <br><pre> <code class="bash hljs">sudo su - ceph mkdir ceph-admin <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> ceph-admin</code> </pre><br><br>  Now install ceph on all our nodes: <br><pre> <code class="bash hljs">ceph-deploy install kvm{1,2,3}</code> </pre><br><br>  Now create a cluster <br><pre> <code class="bash hljs">ceph-deploy new kvm{1,2,3}</code> </pre><br><br>  Create monitors and get the keys: <br><pre> <code class="bash hljs">ceph-deploy mon create kvm{1,2,3} ceph-deploy gatherkeys kvm{1,2,3}</code> </pre><br><br>  Now, according to our original scheme, we will prepare our disks, and launch the OSD daemons: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Flush disks ceph-deploy disk zap kvm{1,2,3}:sd{b,c,d,e,f} # SSD-disks ceph-deploy osd create kvm{1,2,3}:sd{e,f} # HDD-disks ceph-deploy osd create kvm{1,2,3}:sd{b,c,d}</span></span></code> </pre><br><br>  Let's see what we got: <br><pre> <code class="bash hljs">ceph osd tree</code> </pre><div class="spoiler">  <b class="spoiler_title">conclusion</b> <div class="spoiler_text"><pre> <code class="bash hljs">ID WEIGHT TYPE NAME UP/DOWN REWEIGHT PRIMARY-AFFINITY -1 3.00000 root default -2 1.00000 host kvm1 0 1.00000 osd.0 up 1.00000 1.00000 1 1.00000 osd.1 up 1.00000 1.00000 6 1.00000 osd.6 up 1.00000 1.00000 7 1.00000 osd.7 up 1.00000 1.00000 8 1.00000 osd.8 up 1.00000 1.00000 -3 1.00000 host kvm2 2 1.00000 osd.2 up 1.00000 1.00000 3 1.00000 osd.3 up 1.00000 1.00000 9 1.00000 osd.9 up 1.00000 1.00000 10 1.00000 osd.10 up 1.00000 1.00000 11 1.00000 osd.11 up 1.00000 1.00000 -4 1.00000 host kvm3 4 1.00000 osd.4 up 1.00000 1.00000 5 1.00000 osd.5 up 1.00000 1.00000 12 1.00000 osd.12 up 1.00000 1.00000 13 1.00000 osd.13 up 1.00000 1.00000 14 1.00000 osd.14 up 1.00000 1.00000</code> </pre></div></div><br><br>  Check the status of the cluster: <br><pre> <code class="bash hljs">ceph -s</code> </pre><br><br><h2>  Cache pool setup </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/567/3df/738/5673df7385ffa00e9ff5c46e34dd340c.jpg" alt="image"><br>  So, we have a full ceph cluster. <br>  Let's set up a caching pool for it, first we need to edit the CRUSH cards to determine the rules according to which we will distribute data.  To our cache pool was only on SSD-drives, and the main pool only on the HDD. <br><br>  First we need to disable ceph to update the map automatically, we will add it in ceph.conf <br><pre> <code class="bash hljs">osd_crush_update_on_start = <span class="hljs-literal"><span class="hljs-literal">false</span></span></code> </pre><br><br>  And update it on our nodes: <br><pre> <code class="bash hljs">ceph-deploy admin kvm{1,2,3}</code> </pre><br><br>  Let's save our current map and translate it into text format: <br><pre> <code class="bash hljs">ceph osd getcrushmap -o map.running crushtool -d map.running -o map.decompile</code> </pre><br><br>  let's bring it to this form: <br><br><div class="spoiler">  <b class="spoiler_title">map.decompile</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># begin crush map tunable choose_local_tries 0 tunable choose_local_fallback_tries 0 tunable choose_total_tries 50 tunable chooseleaf_descend_once 1 tunable straw_calc_version 1 # devices device 0 osd.0 device 1 osd.1 device 2 osd.2 device 3 osd.3 device 4 osd.4 device 5 osd.5 device 6 osd.6 device 7 osd.7 device 8 osd.8 device 9 osd.9 device 10 osd.10 device 11 osd.11 device 12 osd.12 device 13 osd.13 device 14 osd.14 # types type 0 osd type 1 host type 2 chassis type 3 rack type 4 row type 5 pdu type 6 pod type 7 room type 8 datacenter type 9 region type 10 root # buckets host kvm1-ssd-cache { id -2 # do not change unnecessarily # weight 0.000 alg straw hash 0 # rjenkins1 item osd.0 weight 1.000 item osd.1 weight 1.000 } host kvm2-ssd-cache { id -3 # do not change unnecessarily # weight 0.000 alg straw hash 0 # rjenkins1 item osd.2 weight 1.000 item osd.3 weight 1.000 } host kvm3-ssd-cache { id -4 # do not change unnecessarily # weight 0.000 alg straw hash 0 # rjenkins1 item osd.4 weight 1.000 item osd.5 weight 1.000 } host kvm1-hdd { id -102 # do not change unnecessarily # weight 0.000 alg straw hash 0 # rjenkins1 item osd.6 weight 1.000 item osd.7 weight 1.000 item osd.8 weight 1.000 } host kvm2-hdd { id -103 # do not change unnecessarily # weight 0.000 alg straw hash 0 # rjenkins1 item osd.9 weight 1.000 item osd.10 weight 1.000 item osd.11 weight 1.000 } host kvm3-hdd { id -104 # do not change unnecessarily # weight 0.000 alg straw hash 0 # rjenkins1 item osd.12 weight 1.000 item osd.13 weight 1.000 item osd.14 weight 1.000 } root ssd-cache { id -1 # do not change unnecessarily # weight 0.000 alg straw hash 0 # rjenkins1 item kvm1-ssd-cache weight 1.000 item kvm2-ssd-cache weight 1.000 item kvm3-ssd-cache weight 1.000 } root hdd { id -100 # do not change unnecessarily # weight 0.000 alg straw hash 0 # rjenkins1 item kvm1-hdd weight 1.000 item kvm2-hdd weight 1.000 item kvm3-hdd weight 1.000 } # rules rule ssd-cache { ruleset 0 type replicated min_size 1 max_size 10 step take ssd-cache step chooseleaf firstn 0 type host step emit } rule hdd { ruleset 1 type replicated min_size 1 max_size 10 step take hdd step chooseleaf firstn 0 type host step emit }# end crush map</span></span></code> </pre></div></div><br><br>  You can see that instead of one root I did two, for hdd and ssd, the same thing happened with rule and each host. <br>  When editing the map manually, be extremely careful not to get confused in id'shniki! <br><br>  Now compile and assign it: <br><pre> <code class="bash hljs">crushtool -c map.decompile -o map.new ceph osd setcrushmap -i map.new</code> </pre><br><br>  Let's see what we got: <br><pre> <code class="bash hljs">ceph osd tree</code> </pre><div class="spoiler">  <b class="spoiler_title">conclusion</b> <div class="spoiler_text"><pre> <code class="bash hljs">ID WEIGHT TYPE NAME UP/DOWN REWEIGHT PRIMARY-AFFINITY -100 3.00000 root hdd -102 1.00000 host kvm1-hdd 6 1.00000 osd.6 up 1.00000 1.00000 7 1.00000 osd.7 up 1.00000 1.00000 8 1.00000 osd.8 up 1.00000 1.00000 -103 1.00000 host kvm2-hdd 9 1.00000 osd.9 up 1.00000 1.00000 10 1.00000 osd.10 up 1.00000 1.00000 11 1.00000 osd.11 up 1.00000 1.00000 -104 1.00000 host kvm3-hdd 12 1.00000 osd.12 up 1.00000 1.00000 13 1.00000 osd.13 up 1.00000 1.00000 14 1.00000 osd.14 up 1.00000 1.00000 -1 3.00000 root ssd-cache -2 1.00000 host kvm1-ssd-cache 0 1.00000 osd.0 up 1.00000 1.00000 1 1.00000 osd.1 up 1.00000 1.00000 -3 1.00000 host kvm2-ssd-cache 2 1.00000 osd.2 up 1.00000 1.00000 3 1.00000 osd.3 up 1.00000 1.00000 -4 1.00000 host kvm3-ssd-cache 4 1.00000 osd.4 up 1.00000 1.00000 5 1.00000 osd.5 up 1.00000 1.00000</code> </pre></div></div><br><br>  Now we will describe our configuration in the ceph.conf config, and in particular, we will write data about monitors and osd. <br><br>  I got this config: <br><br><div class="spoiler">  <b class="spoiler_title">ceph.conf</b> <div class="spoiler_text"><pre> <code class="bash hljs">[global] fsid = 586df1be-40c5-4389-99ab-342bd78566c3 mon_initial_members = kvm1, kvm2, kvm3 mon_host = 192.168.100.201,192.168.100.202,192.168.100.203 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx filestore_xattr_use_omap = <span class="hljs-literal"><span class="hljs-literal">true</span></span> osd_crush_update_on_start = <span class="hljs-literal"><span class="hljs-literal">false</span></span> [mon.kvm1] host = kvm1 mon_addr = 192.168.100.201:6789 mon-clock-drift-allowed = 0.5 [mon.kvm2] host = kvm2 mon_addr = 192.168.100.202:6789 mon-clock-drift-allowed = 0.5 [mon.kvm3] host = kvm3 mon_addr = 192.168.100.203:6789 mon-clock-drift-allowed = 0.5 [client.admin] keyring = /etc/ceph/ceph.client.admin.keyring [osd.0] host = kvm1 [osd.1] host = kvm1 [osd.2] host = kvm2 [osd.3] host = kvm2 [osd.4] host = kvm3 [osd.5] host = kvm3 [osd.6] host = kvm1 [osd.7] host = kvm1 [osd.8] host = kvm1 [osd.9] host = kvm2 [osd.10] host = kvm2 [osd.11] host = kvm2 [osd.12] host = kvm3 [osd.13] host = kvm3 [osd.14] host = kvm3</code> </pre></div></div><br><br>  And distribute it to our hosts: <br><pre> <code class="bash hljs">ceph-deploy admin kvm{1,2,3}</code> </pre><br><br>  Check the status of the cluster: <br><pre> <code class="bash hljs">ceph -s</code> </pre><br><br><h2>  Pooling </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/4b8/17a/1ad/4b817a1ad64116113df9ceae7c6e07d0.png" alt="image"><br>  To create pools, we need to calculate the correct number of pg (Placment Group), they are needed for the CRUSH algorithm.  The calculation formula is as follows: <br><pre> <code class="bash hljs"> (OSDs * 100) Total PGs = ------------ Replicas</code> </pre>  and round up to the nearest power of 2 <br><br>  That is, in our case, if we plan to have only one pool on the SSD and one pool on the HDD with replica 2, the calculation formula is the following: <br><pre> <code class="bash hljs">HDD pool pg = 9*100/2 = 450[] = 512 SSD pool pg = 6*100/2 = 300[] = 512</code> </pre><br>  If there are several pools in our root, then the resulting value should be divided into number of pools <br><br>  Create pools, assign them size 2 - the size of the replica, this means that the data recorded in it will be duplicated on different disks, and min_size 1 - the minimum size of the replica at the time of recording, that is, how many replicas need to be made at the time of recording to ‚Äúrelease‚Äù the operation records <br><br><pre> <code class="bash hljs">ceph osd pool create ssd-cache 512 ceph osd pool <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> ssd-cache min_size 1 ceph osd pool <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> ssd-cache size 2 ceph osd pool create one 512 ceph osd pool <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> one min_size 1 ceph osd pool <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> one size 2</code> </pre>  Pool one - understandably will be used to store OpenNebula images <br><br>  Assign rules to our pools: <br><pre> <code class="bash hljs">ceph osd pool <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> ssd-cache crush_ruleset 0 ceph osd pool <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> one crush_ruleset 1</code> </pre><br><br>  We configure that the entry in the pool one will be made through our cache pool: <br><pre> <code class="bash hljs">ceph osd tier add one ssd-cache ceph osd tier cache-mode ssd-cache writeback ceph osd tier <span class="hljs-built_in"><span class="hljs-built_in">set</span></span>-overlay one ssd-cache</code> </pre><br><br>  Ceph uses 2 basic cache flush operations: <br><ul><li>  Flushing: the agent detects cooled objects and dumps them into the storage pool. </li><li>  Evicting (eviction): the agent detects un-cooled objects and, starting with the oldest, dumps them into the storage pool </li></ul><br>  To determine the ‚Äúhot‚Äù objects, the so-called <a href="https://ru.wikipedia.org/wiki/%25D0%25A4%25D0%25B8%25D0%25BB%25D1%258C%25D1%2582%25D1%2580_%25D0%2591%25D0%25BB%25D1%2583%25D0%25BC%25D0%25B0">Bloom filter is used</a> . <br><br>  Configure our cache settings: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#   bloom ceph osd pool set ssd-cache hit_set_type bloom #          ceph osd pool set ssd-cache hit_set_count 4 #       ceph osd pool set ssd-cache hit_set_period 1200</span></span></code> </pre><br><br>  Just set up <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#           ceph osd pool set ssd-cache target_max_bytes 200000000000 #   ,      ceph osd pool set ssd-cache cache_target_dirty_ratio 0.4 #   ,      ceph osd pool set ssd-cache cache_target_full_ratio 0.8 #         ceph osd pool set ssd-cache cache_min_flush_age 300 #         ceph osd pool set ssd-cache cache_min_evict_age 300</span></span></code> </pre><br><br><h2>  Keys </h2><br>  Create user one and generate a key for it. <br><pre> <code class="bash hljs">ceph auth get-or-create client.oneadmin mon <span class="hljs-string"><span class="hljs-string">'allow r'</span></span> osd <span class="hljs-string"><span class="hljs-string">'allow rw pool=ssd-cache'</span></span> -o /etc/ceph/ceph.client.oneadmin.keyring</code> </pre><br>  Since he will not write directly to the main pool, we will issue him rights only to the ssd-cache pool. <br><br>  At this setting Ceph can be considered complete. <br><br><hr><br><a name="galera"></a><h1>  MariaDB Galera Cluster </h1><br><img src="https://habrastorage.org/getpro/habr/post_images/d7b/e5b/e9e/d7be5be9ee84eb2b0d0bb35125a80b47.png" alt="image"><br><br>  Now we will configure a fail-safe MySQL database on our nodes, in which we will store the configuration of our data center. <br>  MariaDB Galera Cluster is a MariaDB cluster with master replication that uses the <a href="http://galeracluster.com/products/technology/">galera library</a> to synchronize. <br>  Plus, it's pretty simple to set up: <br><br><h2>  Installation </h2><br>  <b>On all nodes</b> <br>  Install the repository: <br><pre> <code class="bash hljs">cat &lt;&lt; EOT &gt; /etc/yum.repos.d/mariadb.repo [mariadb] name = MariaDB baseurl = http://yum.mariadb.org/10.0/centos7-amd64 gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDB gpgcheck=1 EOT</code> </pre><br><br>  And the server itself: <br><pre> <code class="bash hljs">yum install MariaDB-Galera-server MariaDB-client rsync galera</code> </pre><br><br>  run the daemon and do the initial installation: <br><pre> <code class="bash hljs">service mysql start chkconfig mysql on mysql_secure_installation</code> </pre><br><br><h2>  Configure the cluster: </h2><br>  On each node, create a user for replication: <br><pre> <code class="bash hljs">mysql -p GRANT USAGE ON *.* to sst_user@<span class="hljs-string"><span class="hljs-string">'%'</span></span> IDENTIFIED BY <span class="hljs-string"><span class="hljs-string">'PASS'</span></span>; GRANT ALL PRIVILEGES on *.* to sst_user@<span class="hljs-string"><span class="hljs-string">'%'</span></span>; FLUSH PRIVILEGES; <span class="hljs-built_in"><span class="hljs-built_in">exit</span></span> service mysql stop</code> </pre><br><br>  We give the configuration /etc/my.cnf to the following form: <br>  For kvm1: <br><pre> <code class="bash hljs">cat &lt;&lt; EOT &gt; /etc/my.cnf collation-server = utf8_general_ci init-connect = <span class="hljs-string"><span class="hljs-string">'SET NAMES utf8'</span></span> character-set-server = utf8 binlog_format=ROW default-storage-engine=innodb innodb_autoinc_lock_mode=2 innodb_locks_unsafe_for_binlog=1 query_cache_size=0 query_cache_type=0 <span class="hljs-built_in"><span class="hljs-built_in">bind</span></span>-address=0.0.0.0 datadir=/var/lib/mysql innodb_log_file_size=100M innodb_file_per_table innodb_flush_log_at_trx_commit=2 wsrep_provider=/usr/lib64/galera/libgalera_smm.so wsrep_cluster_address=<span class="hljs-string"><span class="hljs-string">"gcomm://192.168.100.202,192.168.100.203"</span></span> wsrep_cluster_name=<span class="hljs-string"><span class="hljs-string">'galera_cluster'</span></span> wsrep_node_address=<span class="hljs-string"><span class="hljs-string">'192.168.100.201'</span></span> <span class="hljs-comment"><span class="hljs-comment"># setup real node ip wsrep_node_name='kvm1' # setup real node name wsrep_sst_method=rsync wsrep_sst_auth=sst_user:PASS EOT</span></span></code> </pre><br><br>  By analogy with kvm1, we write the configs for the remaining nodes: <br><div class="spoiler">  <b class="spoiler_title">For kvm2</b> <div class="spoiler_text"><pre> <code class="bash hljs">cat &lt;&lt; EOT &gt; /etc/my.cnf collation-server = utf8_general_ci init-connect = <span class="hljs-string"><span class="hljs-string">'SET NAMES utf8'</span></span> character-set-server = utf8 binlog_format=ROW default-storage-engine=innodb innodb_autoinc_lock_mode=2 innodb_locks_unsafe_for_binlog=1 query_cache_size=0 query_cache_type=0 <span class="hljs-built_in"><span class="hljs-built_in">bind</span></span>-address=0.0.0.0 datadir=/var/lib/mysql innodb_log_file_size=100M innodb_file_per_table innodb_flush_log_at_trx_commit=2 wsrep_provider=/usr/lib64/galera/libgalera_smm.so wsrep_cluster_address=<span class="hljs-string"><span class="hljs-string">"gcomm://192.168.100.201,192.168.100.203"</span></span> wsrep_cluster_name=<span class="hljs-string"><span class="hljs-string">'galera_cluster'</span></span> wsrep_node_address=<span class="hljs-string"><span class="hljs-string">'192.168.100.202'</span></span> <span class="hljs-comment"><span class="hljs-comment"># setup real node ip wsrep_node_name='kvm2' # setup real node name wsrep_sst_method=rsync wsrep_sst_auth=sst_user:PASS EOT</span></span></code> </pre></div></div><div class="spoiler">  <b class="spoiler_title">For kvm3</b> <div class="spoiler_text"><pre> <code class="bash hljs">cat &lt;&lt; EOT &gt; /etc/my.cnf collation-server = utf8_general_ci init-connect = <span class="hljs-string"><span class="hljs-string">'SET NAMES utf8'</span></span> character-set-server = utf8 binlog_format=ROW default-storage-engine=innodb innodb_autoinc_lock_mode=2 innodb_locks_unsafe_for_binlog=1 query_cache_size=0 query_cache_type=0 <span class="hljs-built_in"><span class="hljs-built_in">bind</span></span>-address=0.0.0.0 datadir=/var/lib/mysql innodb_log_file_size=100M innodb_file_per_table innodb_flush_log_at_trx_commit=2 wsrep_provider=/usr/lib64/galera/libgalera_smm.so wsrep_cluster_address=<span class="hljs-string"><span class="hljs-string">"gcomm://192.168.100.201,192.168.100.202"</span></span> wsrep_cluster_name=<span class="hljs-string"><span class="hljs-string">'galera_cluster'</span></span> wsrep_node_address=<span class="hljs-string"><span class="hljs-string">'192.168.100.203'</span></span> <span class="hljs-comment"><span class="hljs-comment"># setup real node ip wsrep_node_name='kvm3' # setup real node name wsrep_sst_method=rsync wsrep_sst_auth=sst_user:PASS EOT</span></span></code> </pre></div></div><br><br>  Done, it's time to start our cluster, on the first node we start: <br><pre> <code class="bash hljs">/etc/init.d/mysql start --wsrep-new-cluster</code> </pre><br><br>  On the remaining nodes: <br><pre> <code class="bash hljs">/etc/init.d/mysql start</code> </pre><br><br>  Let's check our cluster, on each node we will launch: <br><pre> <code class="bash hljs">mysql -p SHOW STATUS LIKE <span class="hljs-string"><span class="hljs-string">'wsrep%'</span></span>;</code> </pre><br><div class="spoiler">  <b class="spoiler_title">Sample output</b> <div class="spoiler_text"><pre> <code class="bash hljs">+------------------------------+----------------------------------------------------------------+ | Variable_name | Value | +------------------------------+----------------------------------------------------------------+ | wsrep_local_state_uuid | 5b32cb2c-39df-11e5-b26b-6e85dd52910e | | wsrep_protocol_version | 7 | | wsrep_last_committed | 4200745 | | wsrep_replicated | 978815 | | wsrep_replicated_bytes | 4842987031 | | wsrep_repl_keys | 3294690 | | wsrep_repl_keys_bytes | 48870270 | | wsrep_repl_data_bytes | 4717590703 | | wsrep_repl_other_bytes | 0 | | wsrep_received | 7785 | | wsrep_received_bytes | 62814 | | wsrep_local_commits | 978814 | | wsrep_local_cert_failures | 0 | | wsrep_local_replays | 0 | | wsrep_local_send_queue | 0 | | wsrep_local_send_queue_max | 2 | | wsrep_local_send_queue_min | 0 | | wsrep_local_send_queue_avg | 0.002781 | | wsrep_local_recv_queue | 0 | | wsrep_local_recv_queue_max | 2 | | wsrep_local_recv_queue_min | 0 | | wsrep_local_recv_queue_avg | 0.002954 | | wsrep_local_cached_downto | 4174040 | | wsrep_flow_control_paused_ns | 0 | | wsrep_flow_control_paused | 0.000000 | | wsrep_flow_control_sent | 0 | | wsrep_flow_control_recv | 0 | | wsrep_cert_deps_distance | 40.254320 | | wsrep_apply_oooe | 0.004932 | | wsrep_apply_oool | 0.000000 | | wsrep_apply_window | 1.004932 | | wsrep_commit_oooe | 0.000000 | | wsrep_commit_oool | 0.000000 | | wsrep_commit_window | 1.000000 | | wsrep_local_state | 4 | | wsrep_local_state_comment | Synced | | wsrep_cert_index_size | 43 | | wsrep_causal_reads | 0 | | wsrep_cert_interval | 0.023937 | | wsrep_incoming_addresses | 192.168.100.202:3306,192.168.100.201:3306,192.168.100.203:3306 | | wsrep_evs_delayed | | | wsrep_evs_evict_list | | | wsrep_evs_repl_latency | 0/0/0/0/0 | | wsrep_evs_state | OPERATIONAL | | wsrep_gcomm_uuid | 91e4b4f9-62cc-11e5-9422-2b8fd270e336 | | wsrep_cluster_conf_id | 0 | | wsrep_cluster_size | 3 | | wsrep_cluster_state_uuid | 5b32cb2c-39df-11e5-b26b-6e85dd52910e | | wsrep_cluster_status | Primary | | wsrep_connected | ON | | wsrep_local_bf_aborts | 0 | | wsrep_local_index | 1 | | wsrep_provider_name | Galera | | wsrep_provider_vendor | Codership Oy &lt;info@codership.com&gt; | | wsrep_provider_version | 25.3.9(r3387) | | wsrep_ready | ON | | wsrep_thread_count | 2 | +------------------------------+----------------------------------------------------------------+</code> </pre></div></div><br>  That's all.  Just - isn't it? <br><br>  <b>Note:</b> if all your nodes are turned off at the same time, MySQL will not rise by itself, you will have to select the most current node, and start the daemon with the option <b>--wsrep-new-cluster</b> , so that the other nodes can replicate the information from it. <br><hr><br><br><a name="openvswitch"></a><h1>  Openvswitch </h1><br><br>  About OpenvSwitch <a href="https://habrahabr.ru/users/ls1/" class="user_link">ls1</a> wrote a cool <a href="http://habrahabr.ru/post/242741/">article</a> , I recommend reading. <br><br><h2>  Installation </h2><br><br>  Since OpenvSwitch is not in standard packages on CentOS, <s>we will compile and install it separately</s> . <div class="spoiler">  <b class="spoiler_title">Manual assembly instructions</b> <div class="spoiler_text">  First, install all necessary dependencies: <br><pre> <code class="bash hljs">yum -y install wget openssl-devel gcc make python-devel openssl-devel kernel-devel graphviz kernel-debug-devel autoconf automake rpm-build redhat-rpm-config libtool</code> </pre><br><br>  To compile OpenvSwitch, create an ovs user and log in under it; we will perform further actions on its behalf. <br><pre> <code class="bash hljs">adduser ovs su - ovs</code> </pre><br><br>  Download the source code, according to the recommendation of <a href="https://n40lab.wordpress.com/2015/06/28/centos-7-installing-openvswitch-2-3-2-lts/">n40lab,</a> disable openvswitch-kmod, and compile them. <br><pre> <code class="bash hljs">mkdir -p ~/rpmbuild/SOURCES wget http://openvswitch.org/releases/openvswitch-2.3.2.tar.gz cp openvswitch-2.3.2.tar.gz ~/rpmbuild/SOURCES/ tar xfz openvswitch-2.3.2.tar.gz sed <span class="hljs-string"><span class="hljs-string">'s/openvswitch-kmod, //g'</span></span> openvswitch-2.3.2/rhel/openvswitch.spec &gt; openvswitch-2.3.2/rhel/openvswitch_no_kmod.spec rpmbuild -bb --nocheck ~/openvswitch-2.3.2/rhel/openvswitch_no_kmod.spec <span class="hljs-built_in"><span class="hljs-built_in">exit</span></span></code> </pre><br><br>  Create a folder for configs <br><pre> <code class="bash hljs">mkdir /etc/openvswitch</code> </pre><br><br>  Install the received RPM package <br><pre> <code class="bash hljs">yum localinstall /home/ovs/rpmbuild/RPMS/x86_64/openvswitch-2.3.2-1.x86_64.rpm</code> </pre></div></div><br>  In the comments, <a href="https://habrahabr.ru/users/dimonyga/" class="user_link">Dimonyga</a> suggested that OpenvSwitch is in the RDO repository and you don‚Äôt need to compile it <br><br>  Let's install it from there: <br><pre> <code class="bash hljs">yum install https://rdoproject.org/repos/rdo-release.rpm yum install openvswitch</code> </pre><br><br>  Run the daemon: <br><pre> <code class="bash hljs">systemctl start openvswitch.service chkconfig openvswitch on</code> </pre><br><br><h2>  Bridge creation </h2><br><br>  Now we will configure the network bridge to which ports will be added. <br><br><pre> <code class="bash hljs">ovs-vsctl add-br ovs-br0 ovs-vsctl add-port ovs-br0 enp1</code> </pre><br><br>  Let's fix the configs of our autorun interfaces: <br><br>  / etc / sysconfig / network-scripts / ifcfg-enp1 <br><pre> <code class="bash hljs">DEVICE=<span class="hljs-string"><span class="hljs-string">"enp1"</span></span> NM_CONTROLLED=<span class="hljs-string"><span class="hljs-string">"no"</span></span> ONBOOT=<span class="hljs-string"><span class="hljs-string">"yes"</span></span> IPV6INIT=no TYPE=<span class="hljs-string"><span class="hljs-string">"OVSPort"</span></span> DEVICETYPE=<span class="hljs-string"><span class="hljs-string">"OVSIntPort"</span></span> OVS_BRIDGE=ovs-br0</code> </pre><br><br>  / etc / sysconfig / network-scripts / ifcfg-ovs-br0 <br><br>  For kvm1: <br><pre> <code class="bash hljs">DEVICE=<span class="hljs-string"><span class="hljs-string">"ovs-br0"</span></span> NM_CONTROLLED=<span class="hljs-string"><span class="hljs-string">"no"</span></span> ONBOOT=<span class="hljs-string"><span class="hljs-string">"yes"</span></span> TYPE=<span class="hljs-string"><span class="hljs-string">"OVSBridge"</span></span> BOOTPROTO=<span class="hljs-string"><span class="hljs-string">"static"</span></span> IPADDR=<span class="hljs-string"><span class="hljs-string">"192.168.100.201"</span></span> NETMASK=<span class="hljs-string"><span class="hljs-string">"255.255.255.0"</span></span> GATEWAY=<span class="hljs-string"><span class="hljs-string">"192.168.100.1"</span></span> DNS1=<span class="hljs-string"><span class="hljs-string">"192.168.100.1"</span></span> HOTPLUG=<span class="hljs-string"><span class="hljs-string">"no"</span></span></code> </pre><br><div class="spoiler">  <b class="spoiler_title">For kvm2</b> <div class="spoiler_text"><pre> <code class="bash hljs">DEVICE=<span class="hljs-string"><span class="hljs-string">"ovs-br0"</span></span> NM_CONTROLLED=<span class="hljs-string"><span class="hljs-string">"no"</span></span> ONBOOT=<span class="hljs-string"><span class="hljs-string">"yes"</span></span> TYPE=<span class="hljs-string"><span class="hljs-string">"OVSBridge"</span></span> BOOTPROTO=<span class="hljs-string"><span class="hljs-string">"static"</span></span> IPADDR=<span class="hljs-string"><span class="hljs-string">"192.168.100.202"</span></span> NETMASK=<span class="hljs-string"><span class="hljs-string">"255.255.255.0"</span></span> GATEWAY=<span class="hljs-string"><span class="hljs-string">"192.168.100.1"</span></span> DNS1=<span class="hljs-string"><span class="hljs-string">"192.168.100.1"</span></span> HOTPLUG=<span class="hljs-string"><span class="hljs-string">"no"</span></span></code> </pre></div></div><br><div class="spoiler">  <b class="spoiler_title">For kvm3</b> <div class="spoiler_text"><pre> <code class="bash hljs">DEVICE=<span class="hljs-string"><span class="hljs-string">"ovs-br0"</span></span> NM_CONTROLLED=<span class="hljs-string"><span class="hljs-string">"no"</span></span> ONBOOT=<span class="hljs-string"><span class="hljs-string">"yes"</span></span> TYPE=<span class="hljs-string"><span class="hljs-string">"OVSBridge"</span></span> BOOTPROTO=<span class="hljs-string"><span class="hljs-string">"static"</span></span> IPADDR=<span class="hljs-string"><span class="hljs-string">"192.168.100.203"</span></span> NETMASK=<span class="hljs-string"><span class="hljs-string">"255.255.255.0"</span></span> GATEWAY=<span class="hljs-string"><span class="hljs-string">"192.168.100.1"</span></span> DNS1=<span class="hljs-string"><span class="hljs-string">"192.168.100.1"</span></span> HOTPLUG=<span class="hljs-string"><span class="hljs-string">"no"</span></span></code> </pre></div></div><br>  Restart the network, everything should start: <br><pre> <code class="bash hljs">systemctl restart network</code> </pre><br><br><hr><br><a name="opennebula"></a><h1>  Opennebula </h1><br><br><h2>  Installation </h2><br>  So it's time to install OpenNebula <br><br>  <b>On all nodes:</b> <br><br>  Install the OpenNebula repository: <br><pre> <code class="bash hljs">cat &lt;&lt; EOT &gt; /etc/yum.repos.d/opennebula.repo [opennebula] name=opennebula baseurl=http://downloads.opennebula.org/repo/4.14/CentOS/7/x86_64/ enabled=1 gpgcheck=0 EOT</code> </pre><br><br>  Install the OpenNebula server, Sunstone web-interface and the node <br><pre> <code class="bash hljs">yum install -y opennebula-server opennebula-sunstone opennebula-node-kvm</code> </pre><br><br>  Run an interactive script that installs the necessary gems into our system: <br><pre> <code class="bash hljs"> /usr/share/one/install_gems</code> </pre><br><br><h2>  Node configuration </h2><br>  At each node, we have the user one, we need to allow him to walk between the nodes without a password and execute any commands via sudo without a password, just like we did with the ceph user. <br><br>  <b>On each node we perform:</b> <br><br><pre> <code class="bash hljs">sudo passwd oneadmin sudo <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"%oneadmin ALL = (root) NOPASSWD:ALL"</span></span> &gt; /etc/sudoers.d/oneadmin sudo chmod 0440 /etc/sudoers.d/oneadmin</code> </pre><br><br>  Let's start the Libvirt and MessageBus services: <br><pre> <code class="bash hljs">systemctl start messagebus.service libvirtd.service systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> messagebus.service libvirtd.service</code> </pre><br><br>  <b>Go to kvm1</b> <br><br>  Now we will generate the key and copy it to the other nodes: <br><pre> <code class="bash hljs">sudo ssh-keygen -f /var/lib/one/.ssh/id_rsa sudo cat /var/lib/one/.ssh/id_rsa.pub &gt;&gt; /var/lib/one/.ssh/authorized_keys sudo chown -R oneadmin: /var/lib/one/.ssh <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 2 3; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> scp /var/lib/one/.ssh/* oneadmin@kvm<span class="hljs-variable"><span class="hljs-variable">$i</span></span>:/var/lib/one/.ssh/ <span class="hljs-keyword"><span class="hljs-keyword">done</span></span></code> </pre><br><br>  <b>On each node we perform:</b> <br><br>  Let Sunstone listen to any IP, not only local: <br><pre> <code class="bash hljs">sed -i <span class="hljs-string"><span class="hljs-string">'s/host:\ 127\.0\.0\.1/host:\ 0\.0\.0\.0/g'</span></span> /etc/one/sunstone-server.conf</code> </pre><br><br><h2>  DB setting </h2><br><br>  <b>Go to kvm1.</b> <br><br>  Create a database for OpenNebula: <br><pre> <code class="bash hljs">mysql -p create database opennebula; GRANT USAGE ON opennebula.* to oneadmin@<span class="hljs-string"><span class="hljs-string">'%'</span></span> IDENTIFIED BY <span class="hljs-string"><span class="hljs-string">'PASS'</span></span>; GRANT ALL PRIVILEGES on opennebula.* to oneadmin@<span class="hljs-string"><span class="hljs-string">'%'</span></span>; FLUSH PRIVILEGES;</code> </pre><br><br>  Now move the database from sqlite to mysql: <br><br>  Download the script sqlite3-to-mysql.py: <br><pre> <code class="bash hljs">curl -O http://www.redmine.org/attachments/download/6239/sqlite3-to-mysql.py chmod +x sqlite3-to-mysql.py</code> </pre><br><br>  Convert and write our database: <br><pre> <code class="bash hljs">sqlite3 /var/lib/one/one.db .dump | ./sqlite3-to-mysql.py &gt; mysql.sql mysql -u oneadmin -pPASS &lt; mysql.sql</code> </pre><br><br>  Now let's say OpenNebula connect to our database, fix the /etc/one/oned.conf config: <br><br>  Replace <br><pre> <code class="bash hljs">DB = [ backend = <span class="hljs-string"><span class="hljs-string">"sqlite"</span></span> ]</code> </pre><br>  on <br><pre> <code class="bash hljs">DB = [ backend = <span class="hljs-string"><span class="hljs-string">"mysql"</span></span>, server = <span class="hljs-string"><span class="hljs-string">"localhost"</span></span>, port = 0, user = <span class="hljs-string"><span class="hljs-string">"oneadmin"</span></span>, passwd = <span class="hljs-string"><span class="hljs-string">"PASS"</span></span>, db_name = <span class="hljs-string"><span class="hljs-string">"opennebula"</span></span> ]</code> </pre><br><br>  Copy it to other nodes: <br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 2 3; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> scp /etc/one/oned.conf oneadmin@kvm<span class="hljs-variable"><span class="hljs-variable">$i</span></span>:/etc/one/oned.conf <span class="hljs-keyword"><span class="hljs-keyword">done</span></span></code> </pre><br><br>  We also need to copy the oneadmin authorization key in the cluster to the other nodes, since all the OpenNebula cluster is managed just under it. <br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 2 3; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> scp /var/lib/one/.one/one_auth oneadmin@kvm<span class="hljs-variable"><span class="hljs-variable">$i</span></span>:/var/lib/one/.one/one_auth <span class="hljs-keyword"><span class="hljs-keyword">done</span></span></code> </pre><br><br><h2>  Check </h2><br>  Now, on each node, we try to start OpenNebula‚Äôs serialis and check whether it works or not: <br><br>  Run <br><pre> <code class="bash hljs">systemctl start opennebula opennebula-sunstone</code> </pre><br><ul><li>  Checking: <code>http://node:9869</code> </li><li>  Checking logs for errors ( <code>/var/log/one/oned.log /var/log/one/sched.log /var/log/one/sunstone.log</code> ). </li></ul><br>  If all is well, turn off: <br><pre> <code class="bash hljs">systemctl stop opennebula opennebula-sunstone</code> </pre><br><br><hr><br><a name="pacemaker"></a><h1>  Configuring Failover Cluster </h1><br><br>  It's time to set up your OpenNebula HA cluster <br>  For some reason, pcs conflicts with OpenNebula.  By this we will use pacemaker, corosync and crmsh. <br><br>  <b>On all nodes:</b> <br><br>  Disable autorun daemon OpenNebula <br><pre> <code class="bash hljs">systemctl <span class="hljs-built_in"><span class="hljs-built_in">disable</span></span> opennebula opennebula-sunstone opennebula-novnc</code> </pre><br><br>  Add a repository: <br><pre> <code class="bash hljs">cat &lt;&lt; EOT &gt; /etc/yum.repos.d/network\:ha-clustering\:Stable.repo [network_ha-clustering_Stable] name=Stable High Availability/Clustering packages (CentOS_CentOS-7) <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=rpm-md baseurl=http://download.opensuse.org/repositories/network:/ha-clustering:/Stable/CentOS_CentOS-7/ gpgcheck=1 gpgkey=http://download.opensuse.org/repositories/network:/ha-clustering:/Stable/CentOS_CentOS-7/repodata/repomd.xml.key enabled=1 EOT</code> </pre><br><br>  Install the necessary packages: <br><pre> <code class="bash hljs">yum install corosync pacemaker crmsh resource-agents -y</code> </pre><br><br>  <b>On kvm1:</b> <br><br>  Let's edit /etc/corosync/corosync.conf, bring it to this form: <br><div class="spoiler">  <b class="spoiler_title">corosync.conf</b> <div class="spoiler_text"><pre> <code class="bash hljs">totem { version: 2 crypto_cipher: none crypto_hash: none interface { ringnumber: 0 bindnetaddr: 192.168.100.0 mcastaddr: 226.94.1.1 mcastport: 4000 ttl: 1 } } logging { fileline: off to_stderr: no to_logfile: yes logfile: /var/<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>/cluster/corosync.log to_syslog: yes debug: off timestamp: on logger_subsys { subsys: QUORUM debug: off } } quorum { provider: corosync_votequorum } service { name: pacemaker ver: 1 } nodelist { node { ring0_addr: kvm1 nodeid: 1 } node { ring0_addr: kvm2 nodeid: 2 } node { ring0_addr: kvm3 nodeid: 3 } }</code> </pre></div></div><br><br>  Generate keys: <br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /etc/corosync corosync-keygen</code> </pre><br><br>  Copy the config and keys to other nodes: <br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 2 3; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> scp /etc/corosync/{corosync.conf,authkey} oneadmin@kvm<span class="hljs-variable"><span class="hljs-variable">$i</span></span>:/etc/corosync ls <span class="hljs-keyword"><span class="hljs-keyword">done</span></span></code> </pre><br><br>  And run the HA services: <br><pre> <code class="bash hljs">systemctl start pacemaker corosync systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> pacemaker corosync</code> </pre><br><br>  Check: <br><pre> <code class="bash hljs">crm status</code> </pre><br><div class="spoiler">  <b class="spoiler_title">Conclusion</b> <div class="spoiler_text"><pre> <code class="bash hljs">Last updated: Mon Nov 16 15:02:03 2015 Last change: Fri Sep 25 16:36:31 2015 Stack: corosync Current DC: kvm1 (1) - partition with quorum Version: 1.1.12-a14efad 3 Nodes configured 0 Resources configured Online: [ kvm1 kvm2 kvm3 ]</code> </pre></div></div><br>  Disable STONITH (mechanism for finishing the faulty node) <br><pre> <code class="bash hljs">crm configure property stonith-enabled=<span class="hljs-literal"><span class="hljs-literal">false</span></span></code> </pre><br>  If you have only two nodes, disable the quorum, in order to avoid a <a href="https://en.wikipedia.org/wiki/Split-brain_%2528computing%2529">splitbrain situation</a> <br><pre> <code class="bash hljs">crm configure property no-quorum-policy=stop</code> </pre><br><br>  Now create the resources: <br><pre> <code class="bash hljs">crm configure primitive ClusterIP ocf:heartbeat:IPaddr2 params ip=<span class="hljs-string"><span class="hljs-string">"192.168.100.200"</span></span> cidr_netmask=<span class="hljs-string"><span class="hljs-string">"24"</span></span> op monitor interval=<span class="hljs-string"><span class="hljs-string">"30s"</span></span> primitive opennebula_p systemd:opennebula \ op monitor interval=60s timeout=20s \ op start interval=<span class="hljs-string"><span class="hljs-string">"0"</span></span> timeout=<span class="hljs-string"><span class="hljs-string">"120s"</span></span> \ op stop interval=<span class="hljs-string"><span class="hljs-string">"0"</span></span> timeout=<span class="hljs-string"><span class="hljs-string">"120s"</span></span> primitive opennebula-sunstone_p systemd:opennebula-sunstone \ op monitor interval=60s timeout=20s \ op start interval=<span class="hljs-string"><span class="hljs-string">"0"</span></span> timeout=<span class="hljs-string"><span class="hljs-string">"120s"</span></span> \ op stop interval=<span class="hljs-string"><span class="hljs-string">"0"</span></span> timeout=<span class="hljs-string"><span class="hljs-string">"120s"</span></span> primitive opennebula-novnc_p systemd:opennebula-novnc \ op monitor interval=60s timeout=20s \ op start interval=<span class="hljs-string"><span class="hljs-string">"0"</span></span> timeout=<span class="hljs-string"><span class="hljs-string">"120s"</span></span> \ op stop interval=<span class="hljs-string"><span class="hljs-string">"0"</span></span> timeout=<span class="hljs-string"><span class="hljs-string">"120s"</span></span> group Opennebula_HA ClusterIP opennebula_p opennebula-sunstone_p opennebula-novnc_p <span class="hljs-built_in"><span class="hljs-built_in">exit</span></span></code> </pre><br><br>  With these actions, we created a virtual IP (192.168.100.200), added three of our services to the HA-cluster and combined them into the Opennebula_HA group. <br><br>  Check: <br><pre> <code class="bash hljs">crm status</code> </pre><br><div class="spoiler">  <b class="spoiler_title">Conclusion</b> <div class="spoiler_text"><pre> <code class="bash hljs">Last updated: Mon Nov 16 15:02:03 2015 Last change: Fri Sep 25 16:36:31 2015 Stack: corosync Current DC: kvm1 (1) - partition with quorum Version: 1.1.12-a14efad 3 Nodes configured 4 Resources configured Online: [ kvm1 kvm2 kvm3 ] Resource Group: Opennebula_HA ClusterIP (ocf::heartbeat:IPaddr2): Started kvm1 opennebula_p (systemd:opennebula): Started kvm1 opennebula-sunstone_p (systemd:opennebula-sunstone): Started kvm1 opennebula-novnc_p (systemd:opennebula-novnc): Started kvm1</code> </pre></div></div><br><br><hr><br><a name="configuration"></a><h1>  OpenNebula setup </h1><br>  Installation is complete, it remains only to add our nodes, storage and virtual networks to the cluster. <br><br>  The web interface will always be available at <code>http://192.168.100.200:9869</code> <br>  <b>login</b> : oneadmin <br>  <b>password</b> in /var/lib/one/.one/one_auth <br><br><ul><li>  Create a cluster </li><li>  Add nodes </li><li>  Add your virtual network: <br><pre> <code class="bash hljs">cat &lt;&lt; EOT &gt; ovs.net NAME=<span class="hljs-string"><span class="hljs-string">"main"</span></span> BRIDGE=<span class="hljs-string"><span class="hljs-string">"ovs-br0"</span></span> DNS=<span class="hljs-string"><span class="hljs-string">"192.168.100.1"</span></span> GATEWAY=<span class="hljs-string"><span class="hljs-string">"192.168.100.1"</span></span> NETWORK_ADDRESS=<span class="hljs-string"><span class="hljs-string">"192.168.100.0"</span></span> NETWORK_MASK=<span class="hljs-string"><span class="hljs-string">"255.255.255.0"</span></span> VLAN=<span class="hljs-string"><span class="hljs-string">"NO"</span></span> VLAN_ID=<span class="hljs-string"><span class="hljs-string">""</span></span> EOT onevnet create ovs.net</code> </pre><br></li><li>  Add your Ceph repository: <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> First you need to save the authorization key: </font></font><br><pre> <code class="bash hljs">UUID=`uuidgen` cat &gt; secret.xml &lt;&lt;EOT &lt;secret ephemeral=<span class="hljs-string"><span class="hljs-string">'no'</span></span> private=<span class="hljs-string"><span class="hljs-string">'no'</span></span>&gt; &lt;uuid&gt;<span class="hljs-variable"><span class="hljs-variable">$UUID</span></span>&lt;/uuid&gt; &lt;usage <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=<span class="hljs-string"><span class="hljs-string">'ceph'</span></span>&gt; &lt;name&gt;client.libvirt secret&lt;/name&gt; &lt;/usage&gt; &lt;/secret&gt; EOT</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> We spread it to our nodes: </font></font><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 1 2 3; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> virsh --connect=qemu+ssh://oneadmin@kvm<span class="hljs-variable"><span class="hljs-variable">$i</span></span>/system secret-define secret.xml virsh --connect=qemu+ssh://oneadmin@kvm<span class="hljs-variable"><span class="hljs-variable">$i</span></span>/system secret-set-value --secret <span class="hljs-variable"><span class="hljs-variable">$UUID</span></span> --base64 $(cat /etc/ceph/ceph.client.oneadmin.keyring | grep -oP <span class="hljs-string"><span class="hljs-string">'[^ ]*=='</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">done</span></span></code> </pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Now let's add the repository itself: </font></font><br><pre> <code class="bash hljs">cat &lt;&lt; EOT &gt; rbd.conf NAME = <span class="hljs-string"><span class="hljs-string">"cephds"</span></span> DS_MAD = ceph TM_MAD = ceph DISK_TYPE = RBD POOL_NAME = one BRIDGE_LIST =<span class="hljs-string"><span class="hljs-string">"192.168.100.201 192.168.100.202 192.168.100.203"</span></span> CEPH_HOST =<span class="hljs-string"><span class="hljs-string">"192.168.100.201:6789 192.168.100.202:6789 192.168.100.203:6789"</span></span> CEPH_SECRET =<span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$UUID</span></span></span><span class="hljs-string">"</span></span> CEPH_USER = oneadmin EOT onedatastore create rbd.conf</code> </pre> <br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Add nodes, networks, your storages to the created cluster via web interface </font></font></li></ul><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> HA VM </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Now, if you want to configure High Availability for your virtual machines, following the official </font></font><a href="http://docs.opennebula.org/4.12/advanced_administration/high_availability/ftguide.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">documentation</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> just add to /etc/one/oned.conf</font></font><br><pre> <code class="bash hljs">HOST_HOOK = [ name = <span class="hljs-string"><span class="hljs-string">"error"</span></span>, on = <span class="hljs-string"><span class="hljs-string">"ERROR"</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">command</span></span> = <span class="hljs-string"><span class="hljs-string">"ft/host_error.rb"</span></span>, arguments = <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$ID</span></span></span><span class="hljs-string"> -m -p 5"</span></span>, remote = <span class="hljs-string"><span class="hljs-string">"no"</span></span> ]</code> </pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> And copy it to other nodes: </font></font><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 2 3; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> scp /etc/one/oned.conf oneadmin@kvm<span class="hljs-variable"><span class="hljs-variable">$i</span></span>:/etc/one/oned.conf <span class="hljs-keyword"><span class="hljs-keyword">done</span></span></code> </pre><br><br><hr><br><h1>  Sources </h1><br><ul><li> <a href="http://docs.ceph.com/docs/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ceph documentation</font></font></a> </li><li> <a href="http://opennebula.org/documentation/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenNebula Documentation</font></font></a> </li><li> <a href="http://opennebula.org/installation-of-ha-opennebula-on-centos-7-with-ceph-as-a-datastore-and-ipoib-as-backend-network/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alexey Vyrodov - Installation of HA OpenNebula on CentOS 7 with Ceph as a datastore and IPoIB as backend network</font></font></a> </li><li> <a href="https://n40lab.wordpress.com/2015/06/28/centos-7-installing-openvswitch-2-3-2-lts/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">N40LAB - CentOS 7 - Installing Openvswitch 2.3.2 LTS</font></font></a> </li><li> <a href="https://code44free.files.wordpress.com/2014/03/proxmox_ceph_v0-11.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alexey Smovzh - Configuring a Failover Solution for Virtualization Proxmox + Ceph</font></font></a> </li><li> <a href="http://www.sebastien-han.fr/blog/2014/08/25/ceph-mix-sata-and-ssd-within-the-same-box/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">S√©bastien Han - Ceph: Mix SATA and SSD Within the Same Box</font></font></a> </li><li> <a href="http://onreader.mdl.ru/LearningCeph/content/Ch10.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Karan Singh - Ceph Performance Tuning and Reference Testing</font></font></a> </li><li> <a href="https://software.intel.com/en-us/blogs/2015/03/03/ceph-cache-tiering-introduction"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zhiqiang W. (Intel) - Ceph cache tiering introduction</font></font></a> </li></ul><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PS: Please, if you notice any shortcomings or errors, write to me in </font></font><a href="http://habrahabr.ru/conversations/kvaps/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">private messages</font></font></a> </div><p>Source: <a href="https://habr.com/ru/post/270187/">https://habr.com/ru/post/270187/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../270173/index.html">AST analysis using patterns</a></li>
<li><a href="../270175/index.html">Microsoft will refuse to support digital certificates based on SHA-1</a></li>
<li><a href="../270179/index.html">The most important argument against MySQL?</a></li>
<li><a href="../270181/index.html">Android and iOS applications send user data to third parties much more often than is commonly believed.</a></li>
<li><a href="../270185/index.html">CLion 1.2: even more features and benefits</a></li>
<li><a href="../270189/index.html">[Translation] Working with files in the programming language D</a></li>
<li><a href="../270191/index.html">On the thirtieth anniversary of the first C ++ compiler: look for errors in Cfront</a></li>
<li><a href="../270193/index.html">Not a flux</a></li>
<li><a href="../270195/index.html">The book "The Perfect Programmer. How to become a software development professional?</a></li>
<li><a href="../270197/index.html">SAP - ABAP. Modifying a summary line in the ALV grid</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>