<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Acquaintance with the levels of parallelization</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="You can parallelize the solution of a problem on several levels. There is no clear boundary between these levels and a specific parallelization techno...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Acquaintance with the levels of parallelization</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/989/755/de6/989755de6359e0c57fc36079ec1b7fdf.png" alt="image"><br>  You can parallelize the solution of a problem on several levels.  There is no clear boundary between these levels and a specific parallelization technology; it can be difficult to attribute to one of them.  The division given here is conditional and serves to demonstrate the diversity of approaches to the problem of parallelization. <br><a name="habracut"></a><br><br><h5>  Task-level Parallelization </h5><br><img align="left" src="https://habrastorage.org/getpro/habr/post_images/1d8/8af/12c/1d88af12c088f9af0f515064b5c198d3.png" alt="image"><br>  Often paralleling at this level is the easiest and most efficient at the same time.  Such parallelization is possible in cases where the problem being solved naturally consists of independent subtasks, each of which can be solved separately.  A good example is audio album compression.  Each entry can be processed separately, as it is not related to the others. <br><br>  Parallelization at the task level shows us the operating system by running programs on different cores on a multi-core machine.  If the first program shows us a movie, and the second is a file-sharing client, then the operating system will easily be able to organize their parallel work. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Other examples of parallelization at this level of abstraction are parallel compilation of files in Visual Studio 2008, data processing in batch modes. <br><br>  As mentioned above, this type of parallelization is simple and in some cases is very effective.  But if we are dealing with a homogeneous task, then this kind of parallelization is not applicable.  The operating system cannot speed up a program that uses only one processor, no matter how many cores are available.  A program that divides the encoding of sound and image in a video into two tasks will not get anything from the third or fourth core.  To parallelize homogeneous tasks, you need to go down to the level below. <br><br><h5>  Level of data parallelism </h5><br>  The name of the model ‚Äúdata parallelism‚Äù comes from the fact that parallelism consists in applying the same operation to a set of data elements.  Data parallelism is demonstrated by an archiver using several processor cores for packaging.  Data is divided into blocks, which are uniformly processed (packaged) at different nodes. <br><img src="https://habrastorage.org/getpro/habr/post_images/532/1b7/5b8/5321b75b859833b40d55f3a1096a9071.png" alt="image"><br>  This type of parallelism is widely used in solving problems of numerical simulation.  The counting region is represented in the form of cells describing the state of the medium at the corresponding points in space ‚Äî pressure, density, percentage ratio of gases, temperature, and so on.  The number of such cells can be huge - millions and billions.  Each of these cells must be processed in the same way.  Here, the data parallelism model is extremely convenient, since it allows you to load each core by allocating a certain set of cells to it.  The counting area is divided into geometric objects, such as parallelepipeds, and the cells that are included in this area are given for processing to a specific core.  In mathematical physics, this type of parallelism is called geometric parallelism. <br><br>  Although geometric parallelism may seem similar to task-level parallelization, it is more complex to implement.  In the case of modeling problems, it is necessary to transfer data obtained at the boundaries of geometric regions to other cores.  Often, special methods of increasing the speed of calculation are used, due to load balancing between computing nodes. <br><img src="https://habrastorage.org/getpro/habr/post_images/1f2/957/6a7/1f29576a79e98ed3595564ffcc0606e9.png" alt="image"><br>  In a number of algorithms, the speed of computation, where processes actively take place, takes longer than where the environment is calm.  As shown in the figure, breaking the counting region into unequal parts can result in a more uniform loading of the cores.  Kernels 1, 2, and 3 process small areas where the body moves, and core 4 processes a large area that has not yet been disturbed.  All this requires additional analysis and the creation of a balancing algorithm. <br><br>  The reward for this complication is the ability to solve problems of long-term movement of objects for an acceptable calculation time.  An example is the launch of a rocket. <br><img src="https://habrastorage.org/getpro/habr/post_images/7aa/0c6/590/7aa0c65908b2cd8ab5c9858f43398306.png" alt="image"><br><br><h5>  Algorithm Parallelization Level </h5><br>  The next level is the parallelization of individual procedures and algorithms.  These include parallel sorting algorithms, matrix multiplication, the solution of a system of linear equations.  At this level of abstraction, it is convenient to use such parallel programming technology as OpenMP. <br><img align="left" src="https://habrastorage.org/getpro/habr/post_images/616/2c9/2de/6162c92deaf9e2345ef01812dc4ba23b.png" alt="image"><br>  <a href="http://software.intel.com/ru-ru/articles/About-OpenMP/">OpenMP</a> (Open Multi-Processing) is a set of compiler directives, library procedures, and environment variables that are designed to program multi-threaded applications on multiprocessor systems.  OpenMP uses the branch-merge parallel execution model.  An OpenMP program begins as a single <a href="http://www.viva64.com/terminology/Thread_Parallel_thread_rus.html">thread of</a> execution called the initial thread.  When a thread encounters a parallel construct, it creates a new group of threads, consisting of itself and a certain number of additional threads, and becomes the main one in the new group.  All members of the new group (including the main thread) execute code inside the parallel structure.  At the end of the parallel construction there is an implicit barrier.  After a parallel construct, only the main thread continues to execute user code.  Other parallel regions may be nested in a parallel region. <br><br>  Due to the idea of ‚Äã‚Äã"incremental parallelization" OpenMP is ideal for developers who want to quickly parallelize their computing programs with large parallel loops.  The developer does not create a new parallel program, but simply sequentially adds OpenMP directives to the text of a sequential program. <br><br>  The task of implementing parallel algorithms is quite complex and therefore there is a sufficiently large number of libraries of parallel algorithms that allow building programs like cubes, without going into the device of parallel data processing implementations. <br><br><h5>  Instruction level parallelism </h5><br>  The lowest level of parallelism implemented at the level of parallel processing by the processor of several instructions.  At the same level is batch processing of several data items with a single processor command.  We are talking about technologies MMX, SSE, SSE2 and so on.  This kind of parallelism is sometimes distinguished into an even deeper level of parallelization ‚Äî parallelism at the bit level. <br><br>  The program is a stream of instructions executed by the processor.  You can change the order of these instructions, distribute them into groups that will be executed in parallel, without changing the result of the entire program.  This is called instruction level concurrency.  To implement this type of parallelism in microprocessors, several instruction pipelines are used, such technologies as command prediction, register renaming. <br><br>  The programmer rarely looks at this level.  Yes, and this makes no sense.  Work on the location of commands in the most convenient sequence for the processor performs the compiler.  This level of parallelization can be of interest only for a narrow group of specialists squeezing out all the possibilities from SSEx or compiler developers. <br><br><h5>  Instead of conclusion </h5><br>  This text does not pretend to be complete about the levels of parallelism, but simply shows the many facets of the issue of using multicore systems.  For those interested in program development, I want to offer several links to resources devoted to parallel programming issues: <br><ol><li>  <a href="http://software.intel.com/ru-ru/">Community of</a> software developers.  I am not an Intel employee, but I highly recommend this resource as a member of this community.  A lot of interesting articles, blog entries and discussions related to parallel programming. </li><li>  <a href="http://www.viva64.com/ru/links/parallel-programming-ru/">Reviews of articles</a> on parallel programming using OpenMP technology. </li><li>  <a href="http://www.parallel.ru/">http://www.parallel.ru/</a> Everything about the world of supercomputers and parallel computing.  Academic community.  Technologies, conferences, discussion club (forum) on parallel computing. </li></ol></div><p>Source: <a href="https://habr.com/ru/post/80342/">https://habr.com/ru/post/80342/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../80334/index.html">Babbage analytical machine. Part one - who is Babbage and why do we need counting machines?</a></li>
<li><a href="../80335/index.html">Plastic Logic Introduces Que E-Reader New Ebook</a></li>
<li><a href="../80339/index.html">Landscaping Twisted</a></li>
<li><a href="../80340/index.html">Chinese singularity</a></li>
<li><a href="../80341/index.html">Disable autocomplete</a></li>
<li><a href="../80343/index.html">CES 2010: ioSafe Solo NAS under the tracks of the bulldozer</a></li>
<li><a href="../80345/index.html">The Third & The Seventh</a></li>
<li><a href="../80350/index.html">Nokia, Apple, Google, Microsoft, ... Which world to choose?</a></li>
<li><a href="../80352/index.html">Optimization of gradients in Photoshop</a></li>
<li><a href="../80354/index.html">N900 through the eyes of apple maker</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>