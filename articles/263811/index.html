<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The formation of musical preferences in the neural network - an experiment to create a smart player</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This article is devoted to the work on the study of the possibility of teaching the simplest (relatively) neural network to ‚Äúlisten‚Äù to music and to d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The formation of musical preferences in the neural network - an experiment to create a smart player</h1><div class="post__text post__text-html js-mediator-article">  This article is devoted to the work on the study of the possibility of teaching the simplest (relatively) neural network to ‚Äúlisten‚Äù to music and to distinguish ‚Äúgood‚Äù in the opinion of the listener from ‚Äúbad‚Äù. <br><br><h4>  purpose </h4><br>  To teach a neural network to distinguish ‚Äúbad‚Äù music from ‚Äúgood‚Äù or to show that a neural network is incapable of it (this particular implementation of it). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a76/196/98a/a7619698af134600ce13a6844cb6408d.png" alt="image"><br><a name="habracut"></a><br><h4>  Stage One: Data Normalization </h4><br>  Considering that music is a combination of an uncountable number of sounds, it will not work out to feed its neural network just like that, so it‚Äôs necessary to determine what the network will ‚Äúlisten to‚Äù.  There are an infinite number of options for selecting the most important features from the music.  You need to determine what you need to select from the music.  As a reference data, I determined that you need to highlight some idea of ‚Äã‚Äãthe frequencies of sounds used in music. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      I decided to select the frequencies because  Firstly, they are <a href="https://www.blogger.com/htt%25D1%2580://www.independentrecording.net/irn/resources/freqchart/main_display.htm">different for different musical instruments</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/790/dfd/7bc/790dfd7bcb64871e01ae39c776c6b1c5.jpg" alt="image"><br><br>  Secondly, the <a href="https://www.blogger.com/htt%25D1%2580://www.independentrecording.net/irn/resources/freqchart/ear_sensitivity.htm">sensitivity of hearing, depending on the frequency, is different</a> and quite individual (within reasonable limits). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/39b/62c/8a7/39b62c8a706f6c63799e80edbd538a87.jpg" alt="image"><br><br>  Thirdly, the track saturation with certain frequencies is quite individual, but it is similar for similar compositions, for example, a guitar solo in two different, but similar tracks will give a similar picture of the track "saturation" at the level of individual frequencies. <br><br>  Thus, the task of normalization is reduced to the selection of some information about frequencies, which shows: <br><ul><li>  how often does the sound from this frequency range sound in the composition </li><li>  how loud he sounded </li><li>  how long has it sounded </li><li>  and so for each specific frequency range (it is necessary to divide the entire "audible" spectrum into a certain number of ranges). </li></ul><br>  To select the frequency saturation in the track at each time interval, you can use the <a href="http://websound.ru/articles/theory/fft.htm">FFT</a> data, you can manually calculate this data if you want, but I will use the already open Bass open library, or rather the <a href="http://bass.radio42.com/">Bass.NET</a> wrapper for it, which allows you to get this data more humanely. <br><br>  To get FFT data from a track, it‚Äôs enough to write a small <a href="http://neurowareblog.blogspot.ru/2015/07/c-fft-mp3-getfftdata.html">function</a> . <br>  After receiving the raw data you need to process them. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f6f/f2a/03e/f6ff2a03ee396378eb627f3e56d81de6.png" alt="image"><br>  <i>Data visualization</i> <br><br>  Firstly, it is necessary to determine by how many frequency ranges to divide the desired sound, this parameter determines how detailed the result of the analysis will be, but also gives a greater load to the neural network (it will need more neurons to operate with large data).  For our task, we take 1024 gradations; this is a fairly detailed frequency spectrum and a relatively small amount of information at the output.  Now it is necessary to determine how to obtain 1 array from the N float [] arrays, which contains more or less all the information we need: sound saturation with certain spectra, frequency of occurrence of various sound spectra, its volume, its duration. <br><br>  With the first parameter ‚Äúsaturation‚Äù everything is quite simple, you can simply sum up all the arrays and at the output we get how much was each spectrum in the entire track, but this will not reflect other parameters. <br><br>  In order to ‚Äúreflect‚Äù on the final array, other parameters can be summed up a little more difficult, I will not go into details of the implementation of such a ‚Äúsumming‚Äù function, since  the number of its possible implementations is virtually infinite. <br><br>  Graphic representation of the resulting array: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/27b/1b0/273/27b1b02735cfafab1e90e9c4cd585679.png" alt="image"><br>  <i>Example 1. Relatively calm melody with light rock elements, piano and vocals</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/065/4a8/a7e/0654a8a7efc72b194176c4998a00c4d7.png" alt="image"><br>  <i>Example 2. Relatively ‚Äúsoft‚Äù <a href="https://ru.wikipedia.org/wiki/%25D0%2594%25D0%25B0%25D0%25B1%25D1%2581%25D1%2582%25D0%25B5%25D0%25BF">dubstep</a> with elements of <s>brain scaling from the walls of a</s> not very soft dubstep</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4cb/a8d/10d/4cba8d10d1c576ff863dddc6c49eac78.png" alt="image"><br>  <i>Example 3. Music in style close to <a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D1%2580%25D0%25B0%25D0%25BD%25D1%2581_(%25D0%25BC%25D1%2583%25D0%25B7%25D1%258B%25D0%25BA%25D0%25B0%25D0%25BB%25D1%258C%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25B6%25D0%25B0%25D0%25BD%25D1%2580)">trance.</a></i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ed1/ee5/1fb/ed1ee51fb88a06cd00fc5ee130e6b9c8.png" alt="image"><br>  <i>Example 4. <a href="https://ru.wikipedia.org/wiki/Pink_Floyd">Pink Floyd</a></i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b94/d1c/1df/b94d1c1dfc282d85fdf5a15fe90f9880.png" alt="image"><br>  <i>Example 5. <a href="https://ru.wikipedia.org/wiki/Van_Halen">Van Halen</a></i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/99c/379/e22/99c379e2262311f9eb0a393294e90bae.png" alt="image"><br>  <i>Example 6. Anthem of Russia</i> <br><br>  In the above examples, it becomes clear that different genres provide different ‚Äúspectral pictures‚Äù, this is good, which means we have identified at least some key features of the track. <br><br>  Now we need to prepare a neural network that we will ‚Äútrain‚Äù.  There are a lot of algorithms of neural networks, some are better for certain tasks, some are worse, I don‚Äôt set goals to study all types in the context of the task, I‚Äôll take its first <a href="http://neurowareblog.blogspot.ru/2015/07/c-classlibraryneuralnetworkscs.html">realization</a> (thanks to <a href="http://kernel-zone.ru/">dr.kernel</a> ) that is flexible enough to adapt it to the task. .  I do not choose a ‚Äúmore suitable‚Äù neural network, since  the task is to check the ‚Äúneural network‚Äù and if its random implementation shows a good result, then definitely there are more suitable types of neural networks that show even better results, but if the network does not cope, then it will only show that this neural network did not cope with the task. <br><br>  This neural network is trained by data that lies in the range from 0 to 1 and, at the output, also gives values ‚Äã‚Äãfrom 0 to 1. Therefore, the data must be brought to a ‚Äúsuitable form‚Äù.  It is possible to bring data to the appropriate form in a variety of ways, the result will still be similar. <br><br>  <b>Stage Two: Neural Network Preparation</b> <br>  The neural network I use is determined by the number of layers, the number of inputs, outputs, and the number of neurons on each layer. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ac6/beb/133/ac6beb1337b5d5be32f91b5ee7bc8272.gif" alt="image"><br>  Obviously, not all configurations are ‚Äúequally useful‚Äù, but I don‚Äôt know certain methods for determining the ‚Äúbest‚Äù configuration for this task, if there are any at all.  One could simply try several different configurations and dwell on the ‚Äúbest suited‚Äù, but I will go a different way, I will grow a neural network with an evolutionary algorithm, later in the article I will explain exactly how. <br><br>  At this time, the format of the input data has been determined, the format of the output data needs to be determined.  Obviously, you can just divide the tracks into ‚Äúgood‚Äù and ‚Äúbad‚Äù and 1 output neuron will be enough, but I think that good and bad is a loose concept, in particular, certain music is better suited for waking up in the morning, another walk around the city on the way to work, the third for a rest in the evening after work, etc.  that is, the quality of the track must also be determined relative to the time of day and day of the week, for a total of 24 * 7 output neurons. <br><br>  Now you need to determine the sample for training, for this you can of course take all the tracks and sit to celebrate what time you want to listen or better never to hear, but I‚Äôm not one of those who would sit for hours and note the tracks, it‚Äôs much easier to do it ‚Äúalong the way‚Äù plays, that is, while listening to the track.  That is, the ‚Äútraining‚Äù sample should form the player, while listening to tracks in which one could mark the track as ‚Äúgood‚Äù or ‚Äúbad‚Äù.  And so imagine that there is such a player (it really is, that is, it was written on the basis of the open codes of another player).  After a dozen hours of listening to music on different days, data is collected for the first sample.  Each sample contains input data (1024 values ‚Äã‚Äãof the spectral pattern of the track) and output (24 * 7 values ‚Äã‚Äãfrom 0 to 1, where 0 is a completely bad track and 1 is a very good track for each hour from 7 days of the week).  At the same time, with the mark ‚Äúgood‚Äù track + put on all days of the week and hours, but at this hour / day of the week + was more, and similarly for ‚Äúbad‚Äù, that is, the data is not 0 and 1, and some values ‚Äã‚Äãare between 0 and 1 . <br><br>  There is data for training, now you need to determine what is considered a trained network, in this case, we can assume that for the data loaded into it, the difference between the network response to the input data should not be and the initial output data should be minimal.  The ability of the network to ‚Äúpredict‚Äù from unknown data is also extremely important, that is, it should not be retrained, there will be little use from the retraining network.  There is no problem to determine the ‚Äúquality of response‚Äù, for this there is a function calculating the error, the question remains how to determine the quality of the prediction.  To solve this problem, it is enough to train the network on 1 sample, but check the quality for another, while the samples must be random and the elements of the first and second should not be repeated. <br><br>  The algorithm for checking the network learning level has been found; now we need to determine its configuration.  As I said, an evolutionary algorithm will be used for this.  To do this, we take the starting configuration, say 10 layers, each with 100 neurons (this will be definitely not enough and the quality of such a network will not be very good), then we will train in a certain number of steps (say 1000), then we will determine the quality of its training.  Next, go to the "evolution", create 10 configurations, each of which is the "mutant" of the original, that is, it has changed in a random direction, either the number of layers or the number of neurons on each or some layers.  Next, we train each configuration in the same way as the initial one, select the best of them, and define it as the initial one.  We continue this process until such a moment comes that we cannot find a configuration that learns better than the original one.  We consider this configuration to be the best; it turned out to be able to ‚Äúremember‚Äù the original data best of all and predicts best of all, that is, the result of its training is the highest quality possible. <br><br>  The evolutionary process took about 6 hours for a sample of 6000 elements in size, after a couple of hours of optimization, the process takes about 30 minutes, the configuration may vary in different samples, but most often the configuration is from about 7 layers, with the number of neurons gradually increasing to 3-4 layers further more quickly reduced to the last layer, a peculiar hump on a 3-4 layer out of 7, apparently this configuration is the most ‚Äúcapable‚Äù for this network. <br><br>  So the network has ‚Äúgrown‚Äù and is capable of learning, the usual training for the network begins, long and tedious (up to 15 minutes), after the network is ready to ‚Äúlisten to music‚Äù and say ‚Äúbad‚Äù or ‚Äúgood‚Äù. <br><br><h4>  Stage Three: Collecting Results </h4><br>  The weight of the ‚Äúbrain‚Äù - the configuration of the trained network is 25 mb, the weight will vary for different samples, in general, the larger the sample, the more neurons you need to cope with, but the weight of the average network will be approximately the same. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/04c/a1b/fe8/04ca1bfe82062da389d41bae9c52f592.png" alt="image"><br><br>  The training set consisted of ‚Äúgood‚Äù tracks in my humble opinion, such as Van Halen, Pink Floyd, classical music (not any), soft rock, melodic and calm tracks.  ‚ÄúBad‚Äù in the sample in my opinion were considered rap, pop, too heavy rock. <br><br>  We define the "quality index" of random tracks, which will count the neural network after training. <br><ul><li>  Van Halen - 29 pts </li><li>  Rammstein-Mutter (album) - 20-23 pts </li><li>  Rihanna - 26 pts </li><li>  The Punisher -11-17 pts </li><li>  Radiorama - 25 pts </li><li>  R Claudermann - 25-29 pts </li><li>  The Gregorians - 27-29 pts </li><li>  Pink Floyd - 29-33 pts </li><li>  Russian ‚Äúrap‚Äù low quality - 9-11 pts </li><li>  Red mold 16-19 pts </li><li>  Bi-2 24-29 pts </li><li>  Leap year - 25-33 pts </li></ul><br><h4>  Conclusion: </h4><br>  The first neural network that came across, after the ‚Äúevolutionary‚Äù configuration configuration and training, showed the presence of ‚Äúskills‚Äù in determining the characteristics of the track.  The neural network can be taught to ‚Äúlisten to music‚Äù and separate tracks that will or will not like the ‚Äúlistener‚Äù who taught it. <br><br>  <b>The full source code of</b> the C # <b>application</b> and the compiled version can be <a href="https://github.com/vpuhoff/Athena-NeuroPlay/">downloaded on GitHub</a> or via <a href="https://link.getsync.com/">Sync</a> . <br><br>  <b>Appearance of the application</b> , which implements all the functionality described in the article: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/acd/eb6/006/acdeb6006180662361d92aa9fddac78c.png" alt="image"><br><br>  I hope my experience in this study will help those who want to explore the possibilities of neural networks in the future. </div><p>Source: <a href="https://habr.com/ru/post/263811/">https://habr.com/ru/post/263811/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../263797/index.html">Modern Adaptec RAID controllers from A to Z. Part 1</a></li>
<li><a href="../263803/index.html">Sock cyborg your trainer and friend</a></li>
<li><a href="../263805/index.html">InfoboxCloud Cloud Infrastructure Calculator</a></li>
<li><a href="../263807/index.html">Button "Return" and other trifles in the assembly Vivaldi 1.0.233.3</a></li>
<li><a href="../263809/index.html">Writing extensions for the game Balanced Annihilation based on the Spring Engine</a></li>
<li><a href="../263813/index.html">Underground carders market. Translation of the book "Kingpin". Chapter 5. ‚ÄúCyberwar!‚Äù</a></li>
<li><a href="../263817/index.html">Video reports from the conference IT NonStop Odessa 2015</a></li>
<li><a href="../263819/index.html">Translation of the book "Kingpin". Chapter 3. ‚ÄúThe Hungry Programmers‚Äù</a></li>
<li><a href="../263821/index.html">Django ORM. Add sugar</a></li>
<li><a href="../263823/index.html">Implementing a search engine with Python rankings (Part 1)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>