<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Elixir: Preparing parsing correctly - yecc and leex</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Lexical analysis (tokenization) and parsing are among the most important concepts in computer science and programming. These concepts are based on a h...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Elixir: Preparing parsing correctly - yecc and leex</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/718/1d3/f55/7181d3f55b0043a1a8da7d13acf23ca6.png"><br><p>  Lexical analysis (tokenization) and parsing are among the most important concepts in computer science and programming.  These concepts are based on a huge amount of theoretical knowledge, but today we will not talk about them, because there are <strong>really a</strong> lot of them.  In addition, the approach to parsing through "science" can cause severe disgust and scare.  Meanwhile, the practical application is very simple and straightforward.  If you want to know more about the theory - go to Wikipedia ( <a href="https://ru.wikipedia.org/wiki/%25D0%259B%25D0%25B5%25D0%25BA%25D1%2581%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B8%25D0%25B9_%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7">lexical analysis</a> and <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B8%25D0%25BD%25D1%2582%25D0%25B0%25D0%25BA%25D1%2581%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B8%25D0%25B9_%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7">parsing</a> ), or read the delightful <a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25BE%25D0%25BC%25D0%25BF%25D0%25B8%25D0%25BB%25D1%258F%25D1%2582%25D0%25BE%25D1%2580%25D1%258B:_%25D0%25BF%25D1%2580%25D0%25B8%25D0%25BD%25D1%2586%25D0%25B8%25D0%25BF%25D1%258B,_%25D1%2582%25D0%25B5%25D1%2585%25D0%25BD%25D0%25BE%25D0%25BB%25D0%25BE%25D0%25B3%25D0%25B8%25D0%25B8_%25D0%25B8_%25D0%25B8%25D0%25BD%25D1%2581%25D1%2582%25D1%2580%25D1%2583%25D0%25BC%25D0%25B5%25D0%25BD%25D1%2582%25D1%258B">book of the dragon</a> (recommended for reading in general to all programmers). </p><br><p>  An ordinary person is afraid to use lexers and parsers, and instead he writes a bicycle in regular expressions.  It seems to me that the apparent complexity is the cause.  In this post, I will try to debunk her! </p><a name="habracut"></a><br><h3 id="pochemu">  Why? </h3><br><p>  To begin with, we note that lexers and parsers are usually used together, but this is generally not necessary.  You can use a lexer, tokenize some string into a set of tokens.  Or you can use the parser to understand the grammar of anything. </p><br><p>  I said that people often use regulars for "parsing" and understanding the text.  Despite the fact that it works on simple tasks for parsing, in most cases the bike is very fragile and does not ride at all.  In addition, the regulars are very limited in the grammars that you are trying to describe with them (for example, try parsing <em>html with</em> regulars) ( <em>translator</em> : not really. But you can also write a cluster application in assembler. The scale of the problem is approximately the same).  Therefore, sometimes you need something more powerful. </p><br>
<h3 id="skazhi-privet-leex-i-yecc">  Say hi <code>leex</code> and <code>yecc</code> </h3><br><p>  Two modules are built into <strong>erlang</strong> that make parsing great: <strong>leex</strong> and <strong>yecc</strong> .  Corresponding to the name, <strong>leex</strong> is a lexer: it reads a file with a specific syntax and generates an <strong>erlang</strong> module (as a <code>*.erl</code> file), which can then be compiled and used for tokenization.  <strong>yecc</strong> basically behaves the same way, only it generates not a lexer, but a parser. </p><br><p>  Since the modules are available in the form of <strong>erlang</strong> batteries (somewhere in the <em>Parsing tools</em> group), then, in theory, they can be great to use in all places where they can solve the problem. </p><br><h3 id="malenkiy-hilenkiy-i-nepravdopodobnyy-primer">  A small, frail and implausible example. </h3><br><p>  Each article that explains something requires examples, so let's do it: we will stream and parse the <code>list</code> of the <strong>Elixir</strong> language, which can consist only of numbers and atoms, which is simply written in a string.  The final goal is to get the original <strong>Elixir</strong> list from this line: </p><br><pre> <code class="bash hljs">iex&gt; ListParser.parse(<span class="hljs-string"><span class="hljs-string">"[1, 2, [:foo, [:bar]]]"</span></span>) [1, 2, [:foo, [:bar]]]</code> </pre> <br><p>  Since it is absolutely pointless - take as an excellent example. </p><br><h3 id="itak-lekser">  So: lexer </h3><br><p>  First of all, we need to tokenize the line: tokenization simply means turning the string into a list of tokens, which in fact are not very structured compared to the usual list of characters (string). </p><br><p>  For example, one of the tokens could be a number, for example <code>4917</code> : the number <code>4917</code> bit more structure than the list of characters <code>[?4, ?9, ?1, ?7]</code> because we can consider it as one. </p><br><p>  Tokenize our list is generally very simple - we separately tokenize: </p><br><ul><li>  brackets (left <code>[</code> and right <code>]</code> ), </li><li>  commas </li><li>  numbers </li><li>  atoms. </li></ul><br><p>  For simplicity, we will deal only with simple atoms, such as <code>:foo</code> or <code>:foo_bar</code> , and with hard ones <code>:'foo and bar'</code> or <code>"hello world"</code> will not be dealt with. </p><br><p>  It is very easy and quick to make your own tokenizer for such a simple syntax, but <strong>leex will greatly</strong> simplify our work by allowing you to write a lexer with a very simple syntax.  Basically, we set tokens with regulars, and associate them with Erlang expressions that represent this token.  I mentioned that regulars are evil for such work: yes, they are bad for parsing because of the usually recursive data structure, but they are excellent for splitting rows into one-dimensional structures. </p><br><p>  The syntax of these rules is simple: </p><br><blockquote>  Regular expression: Erlang code. </blockquote><p>  Here in <strong>Erlang code</strong> you need to return the <code>{:token, value}</code> tuple if we want the lexer to generate a token for us (actually <code>{token, Value}</code> tuple if you write to <strong>Erlang</strong> and not <strong>Elixir</strong> ). </p><br><p>  Our lexer looks very simple: </p><br><pre> <code class="erlang hljs">Rules. [<span class="hljs-number"><span class="hljs-number">0</span></span>-<span class="hljs-number"><span class="hljs-number">9</span></span>]+ : {token, {int, TokenLine, TokenChars}}. :[a-z_]+ : {token, {atom, TokenLine, TokenChars}}. \[ : {token, {'[', TokenLine}}. \] : {token, {']', TokenLine}}. , : {token, {',', TokenLine}}.</code> </pre> <br><p>  We return <code>{:token, Value}</code> to indicate <strong>leex</strong> that we need to get matching tokens (therefore the first element of the tuple is <code>:token</code> ), and we want to add this to the result of lexical analysis. </p><br><p>  <strong>TokenLine</strong> and <strong>TokenChars</strong> are variables that <strong>leex</strong> substitutes for <strong>Erlang</strong> expressions that follow every regular schedule.  These variables contain the string of the associated token and the contents of the associated token as a list of characters. </p><br><p>  We will use two- or three-element tuples as token values, because this format will therefore need <strong>yecc</strong> .  Sometimes we are interested in the value of the token, so we use a three-element tuple.  But sometimes, the value of the token itself is not important (for example, if it is a comma), so a two-element tuple is enough.  This type of token is required.  In this case, <strong>yecc</strong> can easily give us a clear error message. </p><br><p>  We don‚Äôt have to save all the tokens that you found;  you can easily drop them.  To do this, pass <code>:skip_token</code> to the tuple.  The most typical application is to eliminate gaps: </p><br><pre> <code class="erlang hljs">[\s\t\n\r]+ : skip_token.</code> </pre> <br><p>  Regulars can very easily become nauseous, but we can simply define them using the form </p><br><blockquote>  ALIAS = REGEX </blockquote><p>  Such definitions are placed at the beginning of the file, before the list of rules.  To use them in regular definitions, you can wrap them in <code>{}</code> : </p><br><pre> <code class="erlang hljs">Definitions. INT = [<span class="hljs-number"><span class="hljs-number">0</span></span>-<span class="hljs-number"><span class="hljs-number">9</span></span>]+ ATOM = :[a-z_]+ WHITESPACE = [\s\t\n\r] Rules. {INT} : {token, {int, TokenLine, TokenChars}}. {ATOM} : {token, {atom, TokenLine, TokenChars}}. \[ : {token, {'[', TokenLine}}. \] : {token, {']', TokenLine}}. , : {token, {',', TokenLine}}. {WHITESPACE}+ : skip_token.</code> </pre> <br><p>  We are quite ready to try our lexer.  First, we need to write a file with the <code>.xrl</code> extension.  Then, we will turn this file into an <strong>erlang</strong> module as a <code>.erl</code> file using <code>:leex.file/1</code> .  Finally, we can compile the newly generated <strong>Erlang</strong> module.  Remember that most <strong>erlang</strong> modules accept a list of characters instead of binary strings, so you need to wrap in single and not double quotes.  (Note: <strong>Erlang</strong> uses single quotes for complex atoms, such as <code>'foo bar'</code> . These atoms cannot be represented through a regular schedule, but do you remember that?) </p><br><pre> <code class="bash hljs">iex&gt; :leex.file(<span class="hljs-string"><span class="hljs-string">'list_lexer.xrl'</span></span>) iex&gt; c(<span class="hljs-string"><span class="hljs-string">"list_lexer.erl"</span></span>) iex&gt; {:ok, tokens, _} = :list_lexer.string(<span class="hljs-string"><span class="hljs-string">'[1, [:foo]]'</span></span>) iex&gt; tokens {:<span class="hljs-string"><span class="hljs-string">"["</span></span>, 1}, {:int, 1, <span class="hljs-string"><span class="hljs-string">'1'</span></span>}, {:<span class="hljs-string"><span class="hljs-string">","</span></span>, 1}, {:<span class="hljs-string"><span class="hljs-string">"["</span></span>, 1}, {:atom, 1, <span class="hljs-string"><span class="hljs-string">':foo'</span></span>}, {:<span class="hljs-string"><span class="hljs-string">"]"</span></span>, 1}, {:<span class="hljs-string"><span class="hljs-string">"]"</span></span>, 1}]</code> </pre> <br><p>  Crunchy!  <strong>leex</strong> also provides the ability to determine the erlang code that will be associated with the lexer.  This is implemented using the <code>Erlang code.</code> section <code>Erlang code.</code>  at the very end of the <code>.xrl</code> file.  We can use this advantage to convert atomic tokens to directly atoms. </p><br><pre> <code class="erlang hljs">... {INT} : {token, {int, TokenLine, list_to_integer(TokenChars)}}. {ATOM} : {token, {atom, TokenLine, to_atom(TokenChars)}}. ... Erlang code. to_atom([$:|Chars]) -&gt; list_to_atom(Chars).</code> </pre> <br><p>  <code>to_atom/1</code> simply <code>to_atom/1</code> first character of the token of an atom (which is a colon, <code>$:</code> in the world of <strong>Erlang</strong> ), and converts everything else into an atom.  Also use <code>list_to_integer/1</code> to convert the token number to the direct number. </p><br><p>  Lexer completely looks like this: </p><br><pre> <code class="erlang hljs">Definitions. INT = [<span class="hljs-number"><span class="hljs-number">0</span></span>-<span class="hljs-number"><span class="hljs-number">9</span></span>]+ ATOM = :[a-z_]+ WHITESPACE = [\s\t\n\r] Rules. {INT} : {token, {int, TokenLine, list_to_integer(TokenChars)}}. {ATOM} : {token, {atom, TokenLine, to_atom(TokenChars)}}. \[ : {token, {'[', TokenLine}}. \] : {token, {']', TokenLine}}. , : {token, {',', TokenLine}}. {WHITESPACE}+ : skip_token. Erlang code. to_atom([$:|Chars]) -&gt; list_to_atom(Chars).</code> </pre> <br><p>  Everything works as expected: </p><br><pre> <code class="bash hljs">iex&gt; {:ok, tokens, _} = :list_lexer.string(<span class="hljs-string"><span class="hljs-string">'[1, :foo]'</span></span>) iex&gt; tokens [{:<span class="hljs-string"><span class="hljs-string">"["</span></span>, 1}, {:int, 1, 1}, {:<span class="hljs-string"><span class="hljs-string">","</span></span>, 1}, {:atom, 1, :foo}, {:<span class="hljs-string"><span class="hljs-string">"]"</span></span>, 1}]</code> </pre> <br><h3 id="parser">  Parser </h3><br><p>  Now we have a flat list token.  We want to give them some kind of structure, and then turn them into an <strong>Elixir</strong> list: we need to pair the list of tokens.  The work of the parser is based on grammar, which is a list of rules that describe how tokens should be structured. </p><br><p>  Of course, we can write our own parser (although it is much more complicated than our own lexer), it is easier to use <strong>yecc</strong> : it allows you to describe the rules of grammar in a very declarative way, and besides, it is very easy to use with <strong>leex</strong> . </p><br><p>  A small digression: at this stage you may think that the names of the parser and lexer do not make sense.  In fact, it is not.  Ob are named after two very famous programs: <strong>lex</strong> lexer and <strong>yacc</strong> parser.  Looks like the guys from <strong>Erlang are</strong> not that crazy? </p><br><p>  Let's <strong>go</strong> back to <strong>yecc</strong> .  The main element of the syntax is the rules that are described in the following form: </p><br><blockquote>  Left-hand side -&gt; Right-hand side: Erlang expressions. </blockquote><p>  To the left is the token category, to the right is the token list category.  The category of tokens can be of two types - deadlock and <em>non-</em> deadlock ( <em>terminal</em> and <em>non-terminal</em> ).  Dead-end - these are tokens that do not contain anything inside;  non-dead-end, respectively. </p><br><p>  For example,: <code>:"["</code> or <code>{atom, Atom}</code> token - deadlock.  A non-peak list can be presented through a list of stub elements: </p><br><pre> <code class="erlang hljs">list -&gt; '[' ']'. <span class="hljs-comment"><span class="hljs-comment">% or... list -&gt; '[' elems ']'. %  , '%'       Erlang.</span></span></code> </pre> <br><p>  As you can see, you can select several conditions for each category: a category can take any of the listed values ‚Äã‚Äã(think of them as <strong>OR</strong> ). </p><br><p>  <code>elems</code> not a dead end itself.  We can represent it as a single element, or an element + a comma + a list of elements: </p><br><pre> <code class="erlang hljs">elems -&gt; elem. elems -&gt; elem ',' elems.</code> </pre> <br><p>  Accordingly, <code>elems</code> can be <code>elem</code> , or <code>elem, elem</code> and so on. </p><br><p>  <code>elem</code> itself is also not a dead-end: it represents a number, an atom, or a list.  Notice how elegantly we can present the list nesting to the list: </p><br><pre> <code class="erlang hljs">elem -&gt; int. elem -&gt; atom. elem -&gt; list.</code> </pre> <br><p>  Krasava! </p><br><p>  All non-dead-end elements must at some stage be revealed in dead-end: it is impossible to have non-dead-end elements that do not open up to anything.  <strong>yecc</strong> also requires users to determine which categories are terminal and which ones are not at the beginning of the file: </p><br><pre> <code class="erlang hljs">Terminals '[' ']' ',' int atom. Nonterminals list elems elem.</code> </pre> <br><p>  You can also define the root element, which will be the starting non-stub element that generates the original grammar.  In our case, this is the list: </p><br><pre> <code class="erlang hljs">Rootsymbol list.</code> </pre> <br><p>  We are almost done!  It remains only to turn the lists that have just been parsed into <strong>Elixir</strong> lists.  We will do this in the <em>Erlang code</em> associated with each parsing rule.  In these expressions, we have a couple of special atoms: <code>$1</code> , <code>$2</code> , <code>$3</code> and so on.  <strong>yecc</strong> substitutes in them the result of <strong>Erlang</strong> code, which is associated with a category with the same index as in the right part of the rule.  Now I can hear you <em>thinking, "Shield?"</em>  You are right, without an example you will not understand: </p><br><pre> <code class="erlang hljs">list -&gt; '[' ']' : []. <span class="hljs-comment"><span class="hljs-comment">% an empty list translate to, well, an empty list list -&gt; '[' elems ']' : '$2'. % the list is formed by its elements elems -&gt; elem : ['$1']. % single-element list (and base case for the recursion) elems -&gt; elem ',' elems : ['$1'|'$3']. % '$3' will be replaced recursively elem -&gt; int : extract_token('$1'). elem -&gt; atom : extract_token('$1'). elem -&gt; list : '$1'. % ,      Erlang. Erlang code. extract_token({_Token, _Line, Value}) -&gt; Value.</span></span></code> </pre> <br><p>  All is ready!  Here is the final version of our parser: </p><br><pre> <code class="erlang hljs">Nonterminals list elems elem. Terminals '[' ']' ',' int atom. Rootsymbol list. list -&gt; '[' ']' : []. list -&gt; '[' elems ']' : '$<span class="hljs-number"><span class="hljs-number">2</span></span>'. elems -&gt; elem : ['$<span class="hljs-number"><span class="hljs-number">1</span></span>']. elems -&gt; elem ',' elems : ['$<span class="hljs-number"><span class="hljs-number">1</span></span>'|'$<span class="hljs-number"><span class="hljs-number">3</span></span>']. elem -&gt; int : extract_token('$<span class="hljs-number"><span class="hljs-number">1</span></span>'). elem -&gt; atom : extract_token('$<span class="hljs-number"><span class="hljs-number">1</span></span>'). elem -&gt; list : '$<span class="hljs-number"><span class="hljs-number">1</span></span>'. Erlang code. extract_token({_Token, _Line, Value}) -&gt; Value.</code> </pre> <br><p>  Now we can create an <strong>Erlang</strong> file from a <strong>yecc</strong> file (which has the extension <code>.yrl</code> ), just like we did with <strong>leex</strong> : </p><br><pre> <code class="bash hljs">iex&gt; :yecc.file(<span class="hljs-string"><span class="hljs-string">'list_parser.yrl'</span></span>) iex&gt; c(<span class="hljs-string"><span class="hljs-string">"list_parser.erl"</span></span>) iex&gt; :list_parser.parse([{:<span class="hljs-string"><span class="hljs-string">"["</span></span>, 1}, {:atom, 1, :foo}, {:<span class="hljs-string"><span class="hljs-string">"]"</span></span>, 1}]) {:ok, [:foo]}</code> </pre> <br><p>  Works! </p><br><h3 id="kleim-tanchik">  Sticking tanchik </h3><br><p>  We can throw the result of the lexer immediately into the parser and-ii: </p><br><pre> <code class="bash hljs">iex&gt; <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> = <span class="hljs-string"><span class="hljs-string">"[:foo, [1], [:bar, [2, 3]]]"</span></span> iex&gt; {:ok, tokens, _} = <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> |&gt; String.to_char_list |&gt; :list_lexer.string iex&gt; :list_parser.parse(tokens) {:ok, [:foo, [1], [:bar, [2, 3]]]}</code> </pre> <br><p>  Amazing! </p><br><h3 id="ya-ne-ponyal-a-gde-elixir">  I do not understand, but where is Elixir? </h3><br><p>  Manually generating <strong>Erlang</strong> files from <code>.xrl</code> and <code>.yrl</code> files, and then compiling these files is already very annoying.  By the way, <strong>Mix</strong> can do everything for us! </p><br><p>  <strong>Mix</strong> supports the concept of "compilers": they do just what you can assume - compile.  <strong>Mix</strong> provides compilers for <strong>Erlang</strong> (they simply compile <code>.erl</code> files through the installed <strong>Erlang</strong> ), another compiler for Elixir, but there are also compilers for <strong>leex</strong> and <strong>yecc</strong> .  They are actually even included by default, you can check this by calling the <code>Mix.compilers/0</code> function inside the <strong>Mix</strong> project ( <em>translator</em> : and not only. By the way, really by default, check now!): </p><br><pre> <code class="bash hljs">iex&gt; Mix.compilers() [:yecc, :leex, :erlang, :elixir, :app]</code> </pre> <br><p>  The last thing that should be done to make it all work perfectly in the <strong>Mix</strong> project is to put the <code>.xrl</code> and <code>.yrl</code> in the <code>src/</code> project directory, and voila - the modules are visible in your code after compilation. </p><br><pre> <code class="bash hljs">mix new list_parser mkdir list_parser/src mv ./list_parser.yrl ./list_lexer.xrl ./list_parser/src/</code> </pre> <br><p>  And we will add a small wrapper: </p><br><pre> <code class="ruby hljs"><span class="hljs-comment"><span class="hljs-comment"># ./list_parser/lib/list_parser.ex defmodule ListParser do </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">@spec</span></span></span><span class="hljs-comment"> parse(binary) :: list def parse(str) do {:ok, tokens, _} = str |&gt; to_char_list |&gt; :list_lexer.string {:ok, list} = :list_parser.parse(tokens) list end end</span></span></code> </pre> <br><h3 id="taki-gde-tut-gesheft">  Where's geshaft? </h3><br><p>  All of the above may look very abstract, but I am sure that <strong>leex</strong> and <strong>yecc</strong> have a billion uses.  For example, I recently wrote a parser for <a href="https://www.gnu.org/software/gettext/manual/html_node/PO-Files.html"><code>PO</code> files</a> as part of the <a href="https://www.gnu.org/software/gettext/"><strong>GNU gettext</strong></a> binding development for <strong>Elixir</strong> .  I used <strong>yecc</strong> to describe the parser: it all resulted in a very declarative, clean and easy-to-understand grammar ( <a href="">look here</a> ), and in general I am super-keen and happy.  We did not use <strong>leex</strong> , mainly because it seemed to us to be too powerful a tool for such a simple task, and we wrote our own lexer (as I said, this is also possible). </p><br><p>  Another example?  Well, there is this: have you ever heard of the <strong>Elixir</strong> language anywhere?  The language is very bad, built at the top of <strong>Erlang VM</strong> , focused on parallel programming, pad resistance ... Yes, it <a href="">parses <strong>yecc</strong></a> :) </p><br><h3 id="rezyumiruem">  We summarize </h3><br><p>  We easily made a lexer and a parser for converting <strong>Elixir</strong> list-dump lines into real <strong>Elixir</strong> lists.  We used <strong>leex</strong> to generate lexer and <strong>yecc</strong> to generate parser. </p><br><p>  We considered only the most elementary capabilities of these tools: they can be more complicated ( <strong>yecc</strong> generates <strong>LALR</strong> parsers, if you know what it is).  But for this - welcome to the <a href="http://www.erlang.org/doc/apps/parsetools/">documentation</a> . </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/309382/">https://habr.com/ru/post/309382/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../309370/index.html">Basics of Serverless Applications in Amazon Web Services</a></li>
<li><a href="../309372/index.html">How to "bleed" programming skills ... with virtually no programming</a></li>
<li><a href="../309376/index.html">SPA is not a silver bullet, or an alternative approach to web development. Part 1</a></li>
<li><a href="../309378/index.html">Tower Defense on the Unity engine - Part 1</a></li>
<li><a href="../309380/index.html">Introduction to GitLab CI</a></li>
<li><a href="../309384/index.html">Compensation of lags for weapons in MechWarrior Online</a></li>
<li><a href="../309386/index.html">OSPF (Quagga), Shorewall and Policy Routing: inactive route problem</a></li>
<li><a href="../309388/index.html">Bad code kills</a></li>
<li><a href="../309390/index.html">Selection of equipment for corporate cloud storage</a></li>
<li><a href="../309394/index.html">The myth of RAM and O (1)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>