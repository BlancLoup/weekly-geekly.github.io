<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Proxmox cluster storage. Part two. Launch</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! 

 This is the second part of an article about working with cluster storage in Proxmox . Today we will talk about connecting the storage to the...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Proxmox cluster storage. Part two. Launch</h1><div class="post__text post__text-html js-mediator-article">  Hello! <img src="https://habrastorage.org/storage2/5ca/4fc/554/5ca4fc554367ee11b8fdce0a19b490c8.jpg" align="right"><br><br>  This is the second part of an article about working with cluster storage in <i>Proxmox</i> .  Today we will talk about connecting the storage to the cluster. <br><br>  In the beginning I want to give an excerpt from the previous article so that no one will forget why we need the whole garden with a cluster: <blockquote>  In our case, the problem of organizing a common repository boils down to two aspects: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ol><li>  We have a block device sent over the network to which several hosts will have access simultaneously.  In order for these hosts not to fight for space on the device, we need <i>CLVM</i> - <i>Clustered Logical Volume Manager</i> .  This is the same as <i>LVM</i> , only <i>Clustered</i> .  Thanks to <i>CLVM,</i> each host has up-to-date information ( <i>and can change it safely, without compromising integrity</i> ) about the status of <i>LVM</i> volumes on <i>Shared Storage</i> .  Logical volumes in <i>CLVM</i> live just like normal <i>LVM</i> .  Logical volumes contain either <i>KVM</i> images or cluster <i>FS</i> . </li><li>  In the case of <i>OpenVZ</i> , we have a logical volume on which the file system is located.  The simultaneous operation of several machines with a noncluster file system leads to inevitable errors in the operation of everything - it is swan, cancer and pike, only worse.  The file system must be aware that it lives on a shared resource, and be able to work in this mode. </li></ol></blockquote><br>  As a cluster file system, we use <a href="http://en.wikipedia.org/wiki/GFS2">Global File System 2</a> . <br><a name="habracut"></a><br>  <i>Proxmox</i> <i>GFS2</i> is functional, but the <i>Proxmox</i> developers <i>do</i> not officially support it.  However, the core of <i>Proxmox is</i> based on the <a href="http://pve.proxmox.com/wiki/Proxmox_VE_Kernel">RedHat core of the RHEL6x</a> branch.  That is, <i>GFS2</i> support in the kernel is very mature.  The harness also behaves quite stable, with the exception of a few nuances, which I will discuss later.  The <i>gfs2-utils</i> package <i>itself</i> is practically <i>unchanged</i> ( <a href="https://git.proxmox.com/%3Fp%3Dgfs2-utils.git%3Ba%3Dcommit%3Bh%3D1c268b5e019411f09a759fb35fa221bb6a96fb47">there are only patches</a> in the start-up scripts for customization for Debian specifics) a stable Redhat version of <a href="https://fedorahosted.org/cluster/wiki/HomePage">gfs2-utils-3.1.3</a> <br><br>  The <i>gfs2-utils</i> package appeared in <i>Proxmox</i> in <a href="https://bugzilla.proxmox.com/show_bug.cgi%3Fid%3D33">February 2012</a> .  The native Debian's <i>gfs2-tools</i> package wildly conflicts ( <i>which is not surprising</i> ) with the entire <i>RedHat cluster</i> from <i>Proxmox</i> , so before <i>Proxmox</i> version <i>2.0</i> from the <i>GFS2</i> box it was <a href="http://habrahabr.ru/post/133987/">completely unprovable</a> . <br><br>  So, a huge plus is that the foundation for the cocking of <i>GFS2</i> in <i>Proxmox is</i> already built. <br><br>  As <i>iSCSI</i> storage we use <a href="http://h18000.www1.hp.com/products/quickspecs/12992_na/12992_na.HTML">HP MSA 2012i</a> .  This machine is a fault-tolerant solution based on the use of an array of hard drives connected to two independent <i>raid</i> controllers.  Each <i>raid</i> controller has two interfaces for data transfer, in this article it is interesting because the controller does not know how to <a href="http://en.wikipedia.org/wiki/Link_aggregation">combine</a> these interfaces.  We will use <a href="http://ru.wikipedia.org/wiki/Multipath_I/O">multipath</a> to load both interfaces of the controller.  I will not describe the creation of volumes.  Volumes are created without any authorization ( <i>I will tell you about the features of an authorized connection from <i>Proxmox</i> to <i>iSCSI</i> in the next article</i> ). <br><br><h4>  Procedure </h4><br><h5>  The following actions are performed on each node of the cluster. </h5><br>  It is advisable to customize <a href="http://www.microhowto.info/howto/change_the_mtu_of_a_network_interface.html">jumbo frames</a> . <br><br>  To work with multiple network storage interfaces, we set up <i>multipath</i> .  Create a file <i>/etc/multipath.conf with the</i> following content: <br><br><pre><code class="nginx hljs"><span class="hljs-section"><span class="hljs-section">blacklist</span></span> { <span class="hljs-attribute"><span class="hljs-attribute">devnode</span></span> <span class="hljs-string"><span class="hljs-string">"cciss"</span></span> } defaults { <span class="hljs-attribute"><span class="hljs-attribute">user_friendly_names</span></span> <span class="hljs-literal"><span class="hljs-literal">yes</span></span> }</code> </pre> <br>  The <i>blacklist</i> includes block devices that must be excluded from processing ( <i>local disks</i> ).  In our case, these are <i>cciss</i> devices, which are the <i>HP Smart Array</i> volumes of the controller served by the <i>cciss</i> core <i>module</i> . <br><br>  The parameter " <i>user_friendly_names</i> " allows you to create <i>user-friendly</i> devices in <i>/ dev / mapper of the</i> form " <i>mpath0-part1</i> ". <br><br>  Install the missing packages: <br><br><pre> <code class="bash hljs">root@pve03:~<span class="hljs-comment"><span class="hljs-comment"># apt-get install multipath-tools gfs2-utils open-iscsi parted</span></span></code> </pre><br>  The installed <i>multipath</i> immediately takes off and happily picks up the config. <br><br>  Preparing an <i>open-iscsi</i> daemon.  We need to automatically connect available targets at system startup.  <i>Edit the /etc/iscsi/iscsid.conf</i> file.  In it we change the line: <br><br><pre> <code class="bash hljs">node.startup = manual</code> </pre><br>  on <br><br><pre> <code class="bash hljs">node.startup = automatic</code> </pre><br>  Customize <i>LVM</i> .  Switch the lock method from file to cluster: <br><br><pre> <code class="bash hljs">root@pve03:~<span class="hljs-comment"><span class="hljs-comment"># lvmconf --enable-cluster</span></span></code> </pre><br>  <i>Allow CLVM</i> start.  File <i>/ etc / default / clvm</i> : <br><br><pre> <code class="bash hljs">START_CLVM=yes</code> </pre><br>  We start CLVM.  If <i>fenced is</i> not configured here (see <a href="http://habrahabr.ru/company/alawar/blog/163297/">previous article</a> ), we get an error: <br><br><pre> <code class="hljs pgsql">root@pve03:~# service clvm <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> Starting <span class="hljs-keyword"><span class="hljs-keyword">Cluster</span></span> LVM Daemon: clvmclvmd could <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">connect</span></span> <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> <span class="hljs-keyword"><span class="hljs-keyword">cluster</span></span> manager Consult syslog <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> more information failed!</code> </pre><br>  <i>CLVM</i> does not work if our node does not belong to the <i>fence</i> domain. <br><br>  We connect storage to a cluster. <br><br>  In the admin we say " <i>Add iSCSI-target</i> ".  After that, all the nodes of the cluster should see several ( <i>in our case, two</i> ) block devices, and <i>multipath</i> should make one of them and put it in the <i>/ dev / mapper</i> directory. <br><br><img src="https://habrastorage.org/storage2/4cb/1f4/092/4cb1f4092f1ee8f307c94f6c2903e6ed.png"><br><br>  Make sure that the <i>multipath</i> device <i>/ dev / mapper / mpath0</i> is the <i>iSCSI</i> we need. <br><br>  On one of the machines mark up storage: <br><br><pre> <code class="bash hljs">root@pve03:~<span class="hljs-comment"><span class="hljs-comment"># parted /dev/mapper/mpath0 mklabel gpt root@pve03:~# parted /dev/mapper/mpath0 mkpart cluster01 0% 512G root@pve03:~# parted /dev/mapper/mpath0 mkpart kvm01 512G 100%</span></span></code> </pre><br>  In the above example, the volume is divided into two sections: one partition with a volume of <i>512G</i> , and the second, occupying the remaining space on the volume. <br><br>  Tom <i>kvm01</i> we need in the future, when we set up storage for <i>KVM</i> . <br><br>  Restart <i>multipath</i> daemon: <br><br><pre> <code class="bash hljs">root@pve03:~<span class="hljs-comment"><span class="hljs-comment"># service multipath-tools restart</span></span></code> </pre><br>  On the same machine, we create two cluster volume groups: <br><br><pre> <code class="bash hljs">root@pve03:~<span class="hljs-comment"><span class="hljs-comment"># vgcreate -cy CLUSTER01 /dev/mapper/mpath0-part1 root@pve03:~# vgcreate -cy KVM01 /dev/mapper/mpath0-part2</span></span></code> </pre><br>  The "-c" parameter indicates that the volume group is clustered. <br><br>  In principle, it was possible to create only one volume group, and keep in it sections for the <i>KVM</i> machines and the <i>GFS2</i> partition.  Here is a matter of taste. <br><br>  In the group <i>CLUSTER01</i> create a <i>logical volume</i> : <br><br><pre> <code class="bash hljs">root@pve03:~<span class="hljs-comment"><span class="hljs-comment"># lvcreate -n STORAGE -l +100%Free CLUSTER01</span></span></code> </pre><br>  On all nodes of the cluster, this <i>Logical Volume</i> should be visible: <br><br><pre> <code class="bash hljs">root@srv-01:~<span class="hljs-comment"><span class="hljs-comment"># lvscan ACTIVE '/dev/CLUSTER01/STORAGE' [976.56 GiB] inherit ACTIVE '/dev/pve/swap' [4.00 GiB] inherit ACTIVE '/dev/pve/root' [16.75 GiB] inherit ACTIVE '/dev/pve/data' [38.21 GiB] inherit</span></span></code> </pre><br>  We <i>tell the CLVM</i> which <i>Volume Groups</i> to activate / deactivate when starting / stopping: <br><br>  File <i>/ etc / default / clvm</i> : <br><br><pre> <code class="bash hljs">LVM_VGS=<span class="hljs-string"><span class="hljs-string">"CLUSTER01 KVM01"</span></span></code> </pre><br>  Everything is ready to create a cluster file system.  We look, what is the name of our cluster: <br><br><pre> <code class="bash hljs">root@srv-01:~<span class="hljs-comment"><span class="hljs-comment"># pvecm status | grep "Cluster Name" Cluster Name: alapve root@srv-01:~#</span></span></code> </pre><br>  The cluster name must be specified when creating the <i>FS</i> . <br><br>  On one of the nodes of the cluster we format <i>FS</i> : <br><br><pre> <code class="bash hljs">root@pve03:~<span class="hljs-comment"><span class="hljs-comment"># mkfs.gfs2 -t alapve:storage01 -j 3 /dev/mapper/CLUSTER01-STORAGE</span></span></code> </pre><br>  Here: <br><ul><li>  "-t alapve: storage01" is the name of the lock table. <br><ul><li>  <i>alapve</i> is the name of the cluster </li><li>  <i>storage01</i> is a unique file system name. </li></ul></li><li>  "-j 3" is the number of logs that must be created when creating an <i>FS</i> .  Usually equal to the number of nodes in the cluster.  For each host that mounts <i>FS</i> , a different log is required. </li></ul><br>  Look at our <i>FS</i> <i>UUID</i> : <br><br><pre> <code class="bash hljs">root@srv-01:~<span class="hljs-comment"><span class="hljs-comment"># blkid /dev/CLUSTER01/STORAGE /dev/CLUSTER01/STORAGE: LABEL="alapve:storage01" UUID="8b3f1110-8a30-3f2d-6486-a3728baae57d" TYPE="gfs2"</span></span></code> </pre><br>  On each node we create an entry in <i>fstab</i> to mount the <i>FS</i> : <br><br><pre> <code class="bash hljs">root@srv-01:~<span class="hljs-comment"><span class="hljs-comment"># echo "UUID=8b3f1110-8a30-3f2d-6486-a3728baae57d /mnt/cluster/storage01 gfs2 noatime,_netdev 0 0" &gt;&gt; /etc/fstab</span></span></code> </pre><br>  Create a directory <i>/ mnt / cluster / storage01</i> , mount <i>FS</i> into it: <br><br><pre> <code class="bash hljs">root@srv-01:~<span class="hljs-comment"><span class="hljs-comment"># mount /mnt/cluster/storage01</span></span></code> </pre><br>  There is one moment.  When the system is turned off, the script <i>/etc/init.d/umountiscsi.sh</i> is called during the shutdown of the <i>open-iscsi</i> daemon in <i>Proxmox</i> .  It is committed to disabling <i>iSCSI-</i> mounted file systems.  To search for such systems, he uses a rather complex logic, which sometimes fails, which is why an attempt is made to unmount more than necessary, or vice versa - the necessary one is not unmounted.  For example, we encountered attempts to unmount the root file system.  Of course, he didn‚Äôt manage to do this, after which the <i>OS</i> entered into a state of permanent waiting: without stopping <i>iSCSI</i> tags, the system could not reboot, and <i>umountiscsi</i> could not unmount all <i>iSCSI-FS</i> because it ranked the root. <br><br>  We did not dig deep into the logic of <i>umountiscsi.sh</i> .  It was decided that we <i>should</i> not rely on <i>umountiscsi.sh</i> , we will manage the mounted file systems on <i>iSCSI</i> volumes ourselves, and the role of <i>umountiscsi.sh</i> will be reduced to a brave report that ‚Äú <i>All systems are unmounted, my general!</i> ‚Äù. <br><br>  So, in <i>/etc/init.d/umountiscsi.sh</i> change the section " <i>stop</i> ". <br>  It was: <br><br><pre> <code class="bash hljs"> stop|<span class="hljs-string"><span class="hljs-string">""</span></span>) do_stop ;;</code> </pre><br>  It became: <br><br><pre> <code class="bash hljs"> stop|<span class="hljs-string"><span class="hljs-string">""</span></span>) <span class="hljs-comment"><span class="hljs-comment">#do_stop exit 0 ;;</span></span></code> </pre><br>  Now the system will fold correctly.  True, on one condition - at the time of the shutdown, the system should not have <i>iSCSI-</i> mounted file systems.  If you do not want to disable <i>FS</i> manually, for example, you can unmount it in <i>/etc/init.d/clvm</i> before calling " <i>stop</i> ".  At this point, all virtual machines have ( <i>must be</i> ) extinguished.  We don‚Äôt hope so, and before restart we unmount the <i>FS</i> manually. <br><br>  We only <i>need</i> to create a shared storage of the type ‚Äú <i>Directory</i> ‚Äù in the <i>Proxmox</i> admin <i>panel</i> , point it to the path to the directory with mounted <i>FS</i> , and check the box ‚Äú <i>publicly accessible</i> ‚Äù.  All OpenVZ containers created on this repository will be able to safely migrate between nodes. <br><br><h4>  About problems </h4><br>  After several months of testing, we caught the <i>kernel panic a</i> couple of times in the <i>gfs2-</i> module.  <i>Fencing</i> works fine, so at first we didn‚Äôt even understand what was happening, we just rebooted the nodes several times.  After the transition to a new version of the kernel ( <i>2.6.32-17-pve</i> ), there was no hangup.  The new kernel is based on 2.6.32-279.14.1.el6 core from RHEL6.  There are some edits related to gfs2. <br><br><h4>  Let's go to KVM </h4><br>  Everything is much simpler here.  We have already created a volume group, it remains to set <i>Proxmox</i> on it.  In the admin panel we create a repository of the type " <i>LVM Group</i> ", in the field " <i>main repository</i> " we indicate " <i>existing partition groups</i> ", in the field " <i>partition group</i> " we select <i>KVM01</i> , and set the checkbox " <i>publicly available</i> ".  For <i>KVM</i> machines in this section, the system will automatically create logical volumes. <br><br><img src="https://habrastorage.org/storage2/9c7/206/db1/9c7206db1fc36f406bb8606a35ee487f.png"><br><br>  Perhaps it is worth rounding out.  In the next part I will talk about how you can try to live in OpenVZ on network storage without cluster <i>FS</i> , about some of the nuances in working with network storage, plus some solutions for automating and optimizing life in <i>OpenVZ</i> . <br><br>  Thanks for attention! <br><br><ul><li>  <a href="http://habrahabr.ru/company/alawar/blog/163297/">Proxmox cluster storage.</a>  <a href="http://habrahabr.ru/company/alawar/blog/163297/">Part one.</a>  <a href="http://habrahabr.ru/company/alawar/blog/163297/">Fencing</a> </li><li>  Proxmox cluster storage.  Part two.  Launch </li><li>  <a href="http://habrahabr.ru/company/alawar/blog/178429/">Proxmox cluster storage.</a>  <a href="http://habrahabr.ru/company/alawar/blog/178429/">Part Three</a>  <a href="http://habrahabr.ru/company/alawar/blog/178429/">Nuances</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/166919/">https://habr.com/ru/post/166919/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../166907/index.html">PHDays Young School: Young Information Security Specialists Wanted</a></li>
<li><a href="../166909/index.html">Announcement of the school of functional programming in the framework of DevCon 2013</a></li>
<li><a href="../166911/index.html">CES Results: Samsung New Laptops at the International Consumer Electronics Show</a></li>
<li><a href="../166913/index.html">Digest of new materials in Russian TechNet for December 2012 and January 2013</a></li>
<li><a href="../166917/index.html">Mobile development. Template Gallery</a></li>
<li><a href="../166921/index.html">Examples of working with Web Audio from the BBC</a></li>
<li><a href="../166923/index.html">How to know the color of borscht? Colorchuzer will help</a></li>
<li><a href="../166927/index.html">Late 70s Calculator - B3-18A Electronics</a></li>
<li><a href="../166929/index.html">Little Brave Arkanoid (Part 3 - Box2D)</a></li>
<li><a href="../166931/index.html">What languages ‚Äã‚Äãto translate the project in the first place?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>