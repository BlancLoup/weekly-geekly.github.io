<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>MLBootCamp "Performance Evaluation". Very simple and quick solution.</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this post I want to share my idea of ‚Äã‚Äãsolving the MLBootCamp ‚ÄúPerformance Evaluation‚Äù task from Mail.ru. The main advantage of this method is its ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>MLBootCamp "Performance Evaluation". Very simple and quick solution.</h1><div class="post__text post__text-html js-mediator-article">  In this post I want to share my idea of ‚Äã‚Äãsolving the <a href="http://mlbootcamp.ru/championship/7/">MLBootCamp ‚ÄúPerformance Evaluation‚Äù</a> task from Mail.ru.  The main advantage of this method is its simplicity and speed of the script execution.  And although he will not be able to compete exactly with the winners of the competition (my congratulations!), It may be useful in practice if a few tenths of a percent is not critical, or a starting point for further development.  The script is written in R. <br><br><a name="habracut"></a><br><h2>  1. Download and prepare data </h2><br>  Briefly about the competition, here's how the task was set: <br><blockquote>  "We suggest you teach a computer to predict how many seconds two matrices of the size mÔΩòk and kÔΩòn will multiply on this computing system, if you know how many times this problem was solved on other computing systems with different matrix sizes." <br></blockquote><br>  It is worth noting immediately that strictly speaking it is not.  There is not a single computing system in the test sample, which would not be in the training.  The average percentage error ( <a href="https://en.wikipedia.org/wiki/Mean_absolute_percentage_error">MAPE</a> ) is used as a metric. <br><br>  And now a little more.  In both samples, there are 951 traits for each observation.  The first three of them are the dimensions of the multiplied matrices, the rest are the parameters of the systems on which the measurements were made.  It turns out that if we discard the parameters of the multiplied matrices, then in the training sample there are 92 unique configurations of the used computing systems, i.e.  the remaining 948 parameters have only 92 combinations.  Replacing these 948 parameters with one, the configuration identifier, we get that in our data there are only 4 signs for each observation.  We will prepare the data for further work by adding the product of all matrix sizes.  Now there are signs in the data, <em>m</em> , <em>k</em> , <em>n</em> , <em>m * k * n</em> and <em>conf</em> (computer identifier).  Also, the training sample contains the target variable <em>time</em> - the time over which the matrix multiplication was performed. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <pre><code class="python hljs">library(quantreg) library(dplyr) <span class="hljs-comment"><span class="hljs-comment">#    y &lt;- read.csv('y_train.csv')[,1] X.train &lt;- read.csv('x_train.csv', stringsAsFactors = FALSE) X.test &lt;- read.csv('x_test.csv', stringsAsFactors = FALSE) #      X.all &lt;- rbind(X.train, X.test) X.uni &lt;- unique(X.all[,4:951]) #           X.uni &lt;- cbind(conf=paste0('conf', formatC(1:nrow(X.uni), 2, flag = '0')), X.uni) confs &lt;- unique(X.uni$conf) #          #          cols &lt;- c('m', 'k', 'n', 'conf') X.train &lt;- dplyr::inner_join(X.train, X.uni, by=colnames(X.uni)[2:ncol(X.uni)])[cols] X.train[,1:3] &lt;- lapply(X.train[,1:3], as.numeric) # int-&gt;numeric X.train$mkn &lt;- X.train$m * X.train$k * X.train$n X.train$time &lt;- y #      X.test &lt;- dplyr::inner_join(X.test, X.uni, by=colnames(X.uni)[2:ncol(X.uni)])[cols] X.test[,1:3] &lt;- lapply(X.test[,1:3], as.numeric) # int-&gt;numeric X.test$mkn &lt;- X.test$m * X.test$k * X.test$n</span></span></code> </pre> <br><br><h2>  2. Preliminary data analysis and selection of a working model. </h2><br>  Now you can look at the data and see the expected dependence of the target <em>time</em> variable on <em>m * k * n</em> .  For example, configuration 39 (left).  It is well approximated by linear regression.  However, not for all configurations the linear model can give a good result, for example, configuration 11 (on the right): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/4dc/fd0/3d3/4dcfd03d3c5745e3bfc3513bd132a723.png" alt="image"></div><br><br>  Noise and emissions make a significant negative contribution to the model, so one of the options for dealing with them is to remove such observations.  The disadvantage of this method is to decide how far the observation must be ‚Äúwrong‚Äù so that it can be removed.  Moreover, for some systems there is very little data and it is wasteful to throw out even noisy data. <br><br>  An alternative is to use models that are resistant to emissions and noise.  Examples of such models are <a href="https://en.wikipedia.org/wiki/Quantile_regression">quantile</a> and <a href="https://en.wikipedia.org/wiki/Robust_regression">robust</a> regressions. <br>  In R, they can be found in the <strong><em>quantreg</em></strong> and <strong><em>MASS</em></strong> packages, respectively.  Each of them has its own settings, <nobr>and / or</nobr> more advanced variations.  Here these methods were used with default parameters.  Here is how the graph with all regressions for a noisy configuration 11 will look like: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/27e/9b0/065/27e9b006514c477d8d80d2fef9d3f2fb.png" alt="image"></div><br><br><h2>  3. Training and test models </h2><br>  The scatter of the values ‚Äã‚Äãof the target variable varies widely for each of the systems, so the solution suggests itself: to train a stack of models, one for each configuration.  Let the dependence in each model be a little more difficult, namely, the time spent on multiplying the matrices will depend not only on the product of their sizes, but also on each of them separately, i.e.  <nobr><em>time ~ m + k + n + mkn</em></nobr> : <br><br><pre> <code class="python hljs">models &lt;- list() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>:length(confs)){ <span class="hljs-comment"><span class="hljs-comment">#   X.conf &lt;- subset(X.train, conf==confs[i]) X.conf$conf &lt;- NULL #        fit &lt;- rq(time ~ ., data=X.conf) models[[confs[i]]] &lt;- fit }</span></span></code> </pre><br>  Now it is easy to calculate the predictions by going through the models and choosing from the dataset the data corresponding to each individual configuration and its model. <br><br><pre> <code class="python hljs">y.test &lt;- rep(<span class="hljs-number"><span class="hljs-number">1</span></span>, nrow(X.test)) y.train &lt;- rep(<span class="hljs-number"><span class="hljs-number">1</span></span>, nrow(X.train)) <span class="hljs-comment"><span class="hljs-comment">#     for (i in 1:length(confs)){ X.conf &lt;- subset(X.train, conf==confs[i]) y.train[as.numeric(rownames(X.conf))] &lt;- predict(models[[confs[i]]]) } #     for (i in 1:length(confs)){ X.conf &lt;- subset(X.test, conf==confs[i]) y.test[as.numeric(rownames(X.conf))] &lt;- predict(models[[confs[i]]], newdata = X.conf) }</span></span></code> </pre><br>  Due to the non-ideality of the source data and, therefore, the models obtained in the predicted values ‚Äã‚Äãwill be numbers less than 1, which, by the condition of the problem, should not be.  Here it turns out to be useful to use the predictions on the training sample and replace all inappropriate values.  A simple and effective method is to replace all the numbers less than a certain constant cutoff by itself.  The graph shows the metric graph (MAPE) versus the cutoff cutoff <em>value</em> .  The horizontal line is a metric with no replacement, the vertical line is a cutoff at which the metric takes the smallest value. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/049/1b7/c7b/0491b7c7b3bd499c8de66bd0b749b9a5.png" alt="image"></div><br><br>  Using such a replacement can slightly improve the value of the metric.  As practice has shown, the cutoff level in the training and test samples is approximately the same. <br><br>  The obvious disadvantage of the above code is that the models were built without cross-validation, therefore, the obtained value of the metric in the training sample cannot be trusted, as well as the evaluation on the public leaderboard.  The deficiency is eliminated by an additional corresponding code. <br><br>  However, there is also a clear advantage - in speed (the author‚Äôs script was completed in half a second), which makes it possible in a short time to evaluate many different variants of model combinations.  For comparison, models using gradient boosting, random forests or neural networks were built from at least several tens of minutes. <br><br>  Nevertheless, the algorithm with a choice between quantile and robust regression for each configuration showed the result of MAPE = 5.23% on a public leaderboard.  According to the results of the competition, there is no metric, but on the basis of cross-validation data, MAPE is expected to have about 5.4%. <br><br><h2>  4. Conclusion </h2><br>  Thus, the training of the stack of models and the prediction of the target variable are performed very quickly, in <strong>less than a second</strong> and at the same time good accuracy is achieved.  The described method can be improved due to more detailed attention to a particular configuration, for example, selecting a quantile for each regression or parameters when using robust methods, removing obvious outliers, selecting a separate cut-off, etc., which, as a rule, does not greatly affect runtime script. <br><br>  In the modest opinion of the author, the use of quantile (or other emission-resistant) regression to predict the execution time of computer experiments is very practical, accurate, and far less computationally expensive than the method given in the <a href="http://num-meth.srcc.msu.ru/zhurnal/tom_2014/pdf/v15r150.pdf">article</a> for the competition, which used linear regression with bike?) and random woods. </div><p>Source: <a href="https://habr.com/ru/post/306198/">https://habr.com/ru/post/306198/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../306186/index.html">How to represent your own company</a></li>
<li><a href="../306188/index.html">Development for SailfishOS: Basics</a></li>
<li><a href="../306190/index.html">Security Week 29: Ubuntu forum leak, proxy vulnerability in PHP, Go and Python, 276 Oracle patches</a></li>
<li><a href="../306194/index.html">‚ÄúIs this your backup so tired?‚Äù *</a></li>
<li><a href="../306196/index.html">Asyncpg released - PostgreSQL client library for Python / asyncio</a></li>
<li><a href="../306200/index.html">Novice Investor's FAQ: How money is actually protected on the exchange</a></li>
<li><a href="../306202/index.html">Industrial Management Systems - 2016: Vulnerability and Availability</a></li>
<li><a href="../306204/index.html">Compare the implementation of the languages ‚Äã‚Äãof Python and Ruby by the density of errors</a></li>
<li><a href="../306208/index.html">Strata + Hadoop 2016 review</a></li>
<li><a href="../306210/index.html">One pixel instead of a thousand words</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>