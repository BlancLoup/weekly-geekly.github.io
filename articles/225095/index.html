<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>What do neural networks hide?</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The article is a free translation of The Flaw Lurking In Every Deep Neural Net . 

 A recently published article with an innocuous title is probably t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>What do neural networks hide?</h1><div class="post__text post__text-html js-mediator-article">  <i>The article is a free translation of <a href="http://www.i-programmer.info/news/105-artificial-intelligence/7352-the-flaw-lurking-in-every-deep-neural-net.html">The Flaw Lurking In Every Deep Neural Net</a> .</i> <br><br>  A recently published article with an innocuous title is probably the biggest news in the world of neural networks since the invention of <a href="http://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%25BE%25D0%25B1%25D1%2580%25D0%25B0%25D1%2582%25D0%25BD%25D0%25BE%25D0%25B3%25D0%25BE_%25D1%2580%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25BE%25D1%2581%25D1%2582%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B5%25D0%25BD%25D0%25B8%25D1%258F_%25D0%25BE%25D1%2588%25D0%25B8%25D0%25B1%25D0%25BA%25D0%25B8">the back propagation algorithm</a> .  But what is written in it? <br><br>  The article " <a href="http://cs.nyu.edu/~zaremba/docs/understanding.pdf">Intriguing Properties of Neural Networks</a> " by Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow and Rob Fergus, a team that includes authors from Google‚Äôs deep learning project, briefly describes two discoveries in the behavior of neural networks , contrary to what we thought before.  And one of them, frankly, is amazing. <br><a name="habracut"></a><br>  The first discovery casts doubt on the assumption that we have so long believed is true that neural networks order data.  For a long time it was believed that in multilayer networks at each level, neurons are trained to recognize features for the next layer.  Less strong was the assumption that the latter level can recognize essential and usually significant features. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The standard way to find out if this is actually the case is to find a set of input data for a particular neuron that maximizes the output value.  Whatever the particularity of this set, it is assumed that the neuron responds to it.  For example, in a face recognition task, a neuron can react to the presence of an eye or nose in an image.  But note - there is no reason why such features should coincide with those that people allocate. <br><br>  It was found that the peculiarity of an individual neuron can be interpreted as meaningful no more than a random set of neurons.  That is, if you randomly select a set of neurons and find images that maximize the output value, these images will be as semantically similar as in the case of a single neuron. <br><br>  This means that neural networks do not ‚Äúdescramble‚Äù data, displaying features on individual neurons, for example, the output layer.  The information that the neural network extracts is as distributed among all neurons as it is concentrated in one of them.  This is an interesting find, and it leads to another, even more interesting ... <br><br>  <b>Each neural network has ‚Äúblind spots‚Äù</b> in the sense that there are input data sets that are very close to being classified correctly, which in this case are not recognized correctly. <br><br>  From the very beginning of the study of neural networks, it was assumed that neural networks are able to make generalizations.  That is, if you teach the network to recognize the images of cats using a certain set of their photos, it will be able, provided that it has been trained correctly, to recognize cats that it has not met before. <br><br>  This assumption included another, even more ‚Äúobvious‚Äù one, according to which if a neural network classifies a cat's photo as a ‚Äúcat‚Äù, then it will also classify a slightly modified version of this image.  To create such an image, you need to slightly change the values ‚Äã‚Äãof some pixels, and as long as these changes are small, people will not notice the difference.  Presumably, the computer will not notice it either. <br><br><h5>  However, it is not. </h5><br>  The work of researchers was the invention of the optimization algorithm, which starts with a correctly classified example and tries to find a small change in the input values, which will lead to a false classification.  Of course, it is not guaranteed that such a change exists at all - and if the assumption about the sequence of operation of the neural network, mentioned earlier, is true, then the search would not bring results. <br><br>  However, there are results. <br><br>  It was proved that for different sets of neural networks and source data it is very likely that such ‚Äúcontradictory examples‚Äù can be generated from those that are recognized correctly.  Quoting an article: <br><br><blockquote>  For all the networks that we studied, and for each data set we always manage to generate very similar, visually indistinguishable, contradictory examples that are not recognized correctly. </blockquote><br><br>  To be clear, for a person, the original and controversial images look the same, but the network classifies them differently.  You may have two photos in which not just two cats are depicted, but even the same cat, from the point of view of a person, but the computer will recognize one correctly and the other will not. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4a7/c28/3a9/4a7c283a9ae53679951030f00eec9974.jpg"></div><br>  <i>The pictures on the right are classified correctly, the pictures on the left are wrong.</i>  <i>In the middle are the differences of two images multiplied by 10 to make the differences visible.</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/969/b03/139/969b031392d961b72731e134dd032e49.jpg"></div><br>  <i>In the left picture, odd columns are classified correctly, and even columns are not.</i>  <i>On the right picture, everything is recognized correctly, even the random distortion of the original images presented in even columns.</i>  <i>This shows that the changes must be very specific - you need to move in a strictly defined direction in order to find an example of contradiction.</i> <br><br>  What is even more striking is the kind of universality that seems to unite all these examples.  A relatively large proportion of examples are recognized incorrectly by networks trained on common data, but with different parameters (number of layers, regularization or initial coefficients), and networks with the same parameters trained on different data sets. <br><br><blockquote>  The observations described above suggest that the inconsistency of the examples is something global, and not just the result of <a href="http://ru.wikipedia.org/wiki/%25D0%259F%25D0%25B5%25D1%2580%25D0%25B5%25D0%25BE%25D0%25B1%25D1%2583%25D1%2587%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">retraining</a> . </blockquote><br><br>  This is probably the most prominent part of the result: for each correctly classified example, there is another such example, indistinguishable from the original, but classified incorrectly, regardless of which neural network or training sample was used. <br><br>  Therefore, if you have a photo of a cat, there is a set of small changes that can make the network recognize the cat as a dog - regardless of the network and its training. <br><br><h5>  What does all this mean? </h5><br>  Researchers are positive and use conflicting examples to train the network, ensuring its proper operation.  They attribute these examples to particularly complex types of training data that can be used to improve the network and its ability to generalize. <br><br>  However, this discovery seems to be more than just an improved training set. <br><br>  The first <i>thing</i> you might think about is <i>‚ÄúWell, what if a cat can be classified as a dog?‚Äù</i> .  But if you change the situation a little, the question may sound like <i>‚ÄúWhat if an unmanned vehicle using a deep neural network does not recognize the pedestrian in front of him and thinks that the road is clear?‚Äù</i> . <br><br>  The consistency and stability of deep neural networks is important for their practical application. <br><br>  There is also a philosophical question regarding the blind areas mentioned earlier.  If the basis of deep neural networks is a biological model, can the result be applied to it?  Or, to put it simply, does the human brain contain similar embedded errors?  If not, how is it so different from the neural networks trying to copy its work?  What is the secret of its stability and consistency? <br><br>  One explanation may be that this is another manifestation of the <a href="http://ru.wikipedia.org/wiki/%25D0%259F%25D1%2580%25D0%25BE%25D0%25BA%25D0%25BB%25D1%258F%25D1%2582%25D0%25B8%25D0%25B5_%25D1%2580%25D0%25B0%25D0%25B7%25D0%25BC%25D0%25B5%25D1%2580%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D0%25B8">curse of dimension</a> .  It is known that as the dimension of space grows, the volume of the hypersphere concentrates exponentially on its boundary.  Given that the boundaries of solutions are in a very large dimension, it seems logical that the most well-classified examples will be located close to the boundary.  In this case, the ability to classify an example is incorrectly very close to the ability to do it correctly - you just need to determine the direction towards the nearest border. <br><br>  If this explains everything, then it is clear that even the human brain cannot avoid this effect and must somehow cope with it.  Otherwise, cats would turn into dogs with alarming regularity. <br><br><h5>  Total </h5><br>  Neural networks have revealed a new type of instability, and it does not seem that they can make decisions consistently.  And instead of ‚Äúpatching holes‚Äù, including conflicting examples in training samples, science should investigate and fix the problem.  Until this happens, we cannot rely on neural networks where safety is critical ... <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b09/39b/8aa/b0939b8aa2132048ccfa266d65497699.jpg"></div></div><p>Source: <a href="https://habr.com/ru/post/225095/">https://habr.com/ru/post/225095/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../225079/index.html">How to build WhatsApp for the day. Part 1</a></li>
<li><a href="../225083/index.html">June 5, 2014 - World Day Against Internet Surveillance</a></li>
<li><a href="../225089/index.html">Swift - innovations</a></li>
<li><a href="../225091/index.html">Back / Forward Cache - browser caching mechanism</a></li>
<li><a href="../225093/index.html">About common sense and company management</a></li>
<li><a href="../225097/index.html">Floating password</a></li>
<li><a href="../225099/index.html">How to deploy MS SQL Server 2012 failover cluster on Windows Server 2012R2 for beginners</a></li>
<li><a href="../225101/index.html">My experience developing Android games and a few words about monetization</a></li>
<li><a href="../225103/index.html">Sending Zabbix notifications to Skype (Skype calls are a bonus)</a></li>
<li><a href="../225109/index.html">Informal Club Outreach Event DCExpedition for Data Center Managers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>