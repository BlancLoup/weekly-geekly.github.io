<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Telepresence Tod Bot - go for coffee without getting up from the table</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="After an unsuccessful previous post and a forced absence, we return to Habr and continue to cover the Robot Tod Bot project. In this post I want to ta...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Telepresence Tod Bot - go for coffee without getting up from the table</h1><div class="post__text post__text-html js-mediator-article">  After an unsuccessful previous post and a forced absence, we return to Habr and continue to cover the Robot Tod Bot project.  In this post I want to talk about the completion of the functionality of the robot - the implementation of telepresence.  Now control the robot is available from anywhere in the world.  How it works and how, in our opinion, a good telepresence interface should look like - read under the cut.  And, of course, everyone's favorite picture in this topic. <br>  Go <img src="https://habrastorage.org/getpro/habr/post_images/a7f/ae8/644/a7fae8644117beb0a06696849893df0a.jpg"><br><a name="habracut"></a><br>  As the wiki tells us, telepresence is a set of technologies that allows you to get the impression of being in a place other than our physical location.  In my opinion, the top performance of the implementation of telepresence would be the transfer of touch, smell, sight, hearing, taste to the possibility of movement in space.  Thus, the meaning of technology lies in the ability to influence the environment and receive feedback in the form of those sensations experienced by the user, being present in that place. <br>  In order to provide the user with a convincing presence effect, at a minimum, a number of technologies are needed that will allow: <br><ul><li>  Receive and transmit video stream </li><li>  Receive and transmit sound </li><li>  Move in space </li><li>  Ability to manipulate objects in a remote environment </li></ul><br>  Speaking of our robot, he has a telepresence implemented via a web interface.  This approach has an undeniable advantage - independence from the used platform on the client side. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8b4/ace/c5c/8b4acec5ce32b333c25ea3e3b1673866.jpg"><br><br><h5>  Video </h5><br>  To get the video on the side of the robot, we use the KGB RGB camera and an additional webcam, in our case it is a webcam built into the laptop, but it is also possible to use a separate one.  One is frontal, where we, in fact, see everything that happens, the other - under our feet, where we can see all the objects that could potentially prevent us from controlling the robot.  Images from cameras are located in the web interface in a convenient and natural way for a person.  If we want to see what is under our feet, then we put our head down, the same principle is used in our web interface - we look at the lower screen. <br>  Video feedback is provided by a webcam located on the client side.  The resulting picture is displayed on the display of the robot or on a laptop installed on the mobile platform.  Video streaming is organized via the SIP protocol - this is an analogue of the Skype protocol, which is widely used as a mini-PBX in offices and on service websites. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h5>  Sound </h5><br>  To transmit sound, we use the integrated microphone network in Kinect.  These microphones are of good quality, which minimizes the occurrence of various noises.  For the transmission of the audio stream we use the same SIP. <br>  In the future, we want to use the Kinect microphones to the maximum and realize the transmission of stereo sound.  Undoubtedly, this will allow the user to get an even greater sense of the effect of telepresence. <br><br><h5>  Moving in space </h5><br>  As for the movement, there are two options for remote control of the robot: manual and autonomous.  Since our robot is already able to independently navigate in space, it would be foolish not to use this opportunity here.  Like a human, for a robot it is necessary to navigate in space, and for this he uses a room map, pre-built on the basis of sensory information. <br>  In the web-interface for more convenient orientation of the user in the room in the lower left corner of the displayed mini-map, which is useful to us for any control mode.  We are always in the center of the mini-map, and the map itself rotates and moves according to the movement of the robot.  So we can understand exactly where the robot is, and when double-clicking on it, the global map unfolds. <br><img src="https://habrastorage.org/getpro/habr/post_images/4b7/404/fd9/4b7404fd912fec4f311c5f10926e5687.jpg"><br><br><h6>  Manual control </h6><br>  Real hardcore mode.  In this mode, we control the robot from the keyboard with the arrow keys and are free to go anywhere, even despite the obstacles.  The result is a kind of racing simulator from the first person, only at the other end of the wire is a real robot and a real environment.  In addition, speed control in the form of sliders is also available.  The first slider is responsible for the linear speed, and the second - for the speed of rotation of the robot.  Despite the seeming simplicity, it is rather difficult to control it in the first place due to the lack of dimensions. <br><br><h6>  Autonomous navigation </h6><br>  Autonomous navigation involves minimizing user roles.  It is necessary and sufficient for the user to specify one or several destination points, and the robot, navigating the route and avoiding obstacles, will independently navigate through them.  At the same time, both visual and sound contact will be maintained.  If necessary, the user can stop the execution of autonomous movement and switch to manual control mode.  For the autonomous mode, the function of following the interlocutor, whose implementation is in our plans, will also be relevant. <br>  In spite of the seeming completeness of the functional, there is still something to work on.  We want to make the web-based interface a kind of ‚Äúcontrol panel‚Äù with battery level indicators and other sensors vital for the robot, through which the setup and calibration of the robot will be available. <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/56UGxAQ0fR4%3Ffeature%3Doembed&amp;xid=17259,15700022,15700186,15700190,15700253&amp;usg=ALkJrhj-KrbsJCSLVvDrXupTowmne8NiLQ" frameborder="0" allowfullscreen=""></iframe><br><br><h5>  Manipulating objects in a remote environment </h5><br>  Not every telepresence robot can boast the function of manipulating objects.  There are a number of reasons for this, one of which is the high cost of the manipulators themselves, the cost of which is sometimes comparable to the cost of the robots themselves and the complexity of the software implementation. <br>  Tod Bot is an exception and he still has a hand.  So it is up to the software.  The easiest way to manage a manipulator is to place the sliders / scrolls on the web interface panel, which are responsible for each degree of freedom of the manipulator.  There is only one thing: to grab something with such control is incredibly difficult.  I know firsthand - I tried to take a box of matches from the table.  Spent on all this about 5 minutes.  At the same time the probability of grazing or tilting adjacent objects tends to 100%.  You can, of course, work out a week and improve your results, but in this case it is not the easiest and most convenient way. <br>  As we see the manipulator control through the web-interface.  This process should be as automated as possible, which would not require the nerves and forces of the user.  The user should restrict himself only to the choice of the object in the video stream, and the system will do the rest.  Therefore, we entrusted hand control to the MoveIt program module, which we wrote about <a href="http://habrahabr.ru/company/tod/blog/216681/">earlier</a> .  At the moment, the hand is already able to avoid colliding with surrounding objects when moving.  We will integrate MoveIt with the web interface to achieve satisfactory results in capturing objects.  Now we have some success in this direction. <br><br><h5>  How do we make a web interface with ROS? </h5><br>  Speaking about the telepresence software implementation, for a start it is worth recalling that the Tod Bot robot is controlled under the ROS framework.  In ROS, all the functionality is distributed among the program nodes communicating with each other through messages published in topics.  Accordingly, any new software integrated into the system must be represented as such a ROS node. <br>  To implement telepresence functionality, on the one hand, we need a web server with the ability to generate HTML pages and process POST / GET requests, on the other hand, we need to receive odometer and navigation map data from ROS and send room patrol and motion control commands . <br>  Based on these requirements, we decided to decorate all telepresence functionality as a ROS software node, use CherryPy, a minimalist Python web framework, as the web server, and store the data in NoSQL Redis storage in a simple key-value format.  The HTML5 client sipML5 was used as a SIP client, which allows you to make audio / video calls directly in the browser. <br>  How does it work together?  The operator in the web browser sends data through AJAX requests to the robot‚Äôs web server.  The telepresence host Python script processes the data from the web server and sends it to other ROS nodes that are already directly executing commands on the robot.  From the telepresence node to the operator‚Äôs side, the data on the map, odometry and video stream from Kinect, which are rendered in HTML5 Canvas, come in the same way.  In parallel, the audio / video stream is transmitted through the operator‚Äôs and the robot‚Äôs sipML5 clients.  By the way, the quality of free SIP-services for communication of SIP-clients does not cause any complaints.  The only thing you need is a fairly wide Internet channel. <br><br>  We would also like to hear your opinion.  What should a telepresence reference system look like? </div><p>Source: <a href="https://habr.com/ru/post/219991/">https://habr.com/ru/post/219991/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../219977/index.html">IRON-MAN Package: 5 Stratoplan Management Books</a></li>
<li><a href="../219979/index.html">Sensu Brush - the first brush for a capacitive display</a></li>
<li><a href="../219985/index.html">SpaceX space truck Dragon from successfully launched for the ISS for the third time</a></li>
<li><a href="../219987/index.html">The first launch of the SpaceX Falcon-9 rocket with folding supports for landing was successful</a></li>
<li><a href="../219989/index.html">History of checkers (in illustrations)</a></li>
<li><a href="../219993/index.html">Encoding of binary data into a string with an arbitrary length alphabet (BaseN)</a></li>
<li><a href="../219995/index.html">jWidget - object-oriented JavaScript MV * framework</a></li>
<li><a href="../219997/index.html">Serialization of objects in MultiCAD.NET. Management of compatibility of drawings and proxies</a></li>
<li><a href="../219999/index.html">Arbitrary order of template initialization list</a></li>
<li><a href="../220001/index.html">Kenju fork Kendo UI Web (GPL3)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>