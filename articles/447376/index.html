<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>SNA Hackathon 2019</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In February-March 2019, a competition was held to rank the tape of the social network SNA Hackathon 2019 , in which our team won first place. In the a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>SNA Hackathon 2019</h1><div class="post__text post__text-html js-mediator-article"><p>  In February-March 2019, a competition was held to rank the tape of the social network <a href="https://mlbootcamp.ru/round/20/rating/">SNA Hackathon 2019</a> , in which our team won first place.  In the article I will tell you about the organization of the competition, the methods we tried, and the catboost settings for training on big data. </p><br><p><img src="https://habrastorage.org/webt/8b/bd/vv/8bbdvvg9ih5_0h5tdqmx4fho3io.jpeg"></p><a name="habracut"></a><br><p>  <strong>Sna hackathon</strong> </p><br><p>  Hackathon under this name is held for the third time.  It is organized by the ok.ru social network, respectively, the task and the data are directly related to this social network. <br>  SNA (social network analysis) in this case is more correctly understood not as an analysis of a social graph, but rather as an analysis of a social network. </p><br><ul><li>  In 2014, the task was to predict the number of likes that the post will collect. </li><li>  In 2016, the VVZ task (perhaps you are familiar), more close to the analysis of the social graph. </li><li>  In 2019 - ranking of the user's tape by the likelihood that the user will like the post. </li></ul><br><p>  I can‚Äôt say about 2014, but in 2016 and 2019, besides my ability to analyze data, I also needed skills in working with big data.  I think that it was precisely the combination of machine learning and big data processing tasks that attracted me to these contests, and the experience in these areas helped me to win. </p><br><p>  <strong>mlbootcamp</strong> </p><br><p>  In 2019, the competition was organized on the platform <a href="https://mlbootcamp.ru/">https://mlbootcamp.ru</a> . </p><br><p>  The competition began online on February 7 and consisted of 3 tasks.  Anyone could register on the site, download the <a href="https://github.com/MailRuChamps/snahackathon">baseline</a> and load their car for several hours.  At the end of the online stage on March 15, the top 15 of each showdown was invited to the Mail.ru office for an offline stage, which ran from March 30 to April 1. </p><br><h1 id="zadacha">  Task </h1><br><p>  In the source data, user identifiers (userId) and post identifiers (objectId) are provided.  If the user was shown a post, then there is a line in the data containing the userId, objectId, the user's reaction to this post (feedback) and a set of various signs or links to pictures and texts. </p><br><table><thead><tr><th>  userId </th><th>  objectId </th><th>  ownerId </th><th>  feedback </th><th>  images </th></tr></thead><tbody><tr><td>  3555 </td><td>  22 </td><td>  5677 </td><td>  [liked, clicked] </td><td>  [hash1] </td></tr><tr><td>  12842 </td><td>  55 </td><td>  32144 </td><td>  [disliked] </td><td>  [hash2, hash3] </td></tr><tr><td>  13145 </td><td>  35 </td><td>  5677 </td><td>  [clicked, reshared] </td><td>  [hash2] </td></tr></tbody></table><br><p>  The test dataset contains a similar structure, but there is no feedback field.  The goal is to predict the presence of an 'liked' reaction in the feedback field. <br>  The submission file has the following structure: </p><br><table><thead><tr><th>  userId </th><th>  SortedList [objectId] </th></tr></thead><tbody><tr><td>  123 </td><td>  78,13,54,22 </td></tr><tr><td>  128 </td><td>  35,61,55 </td></tr><tr><td>  131 </td><td>  35,68,129,11 </td></tr></tbody></table><br><p>  Metric - average ROC AUC by users. </p><br><p>  A more detailed description of the data can be found on the <a href="https://mlbootcamp.ru/round/18/sandbox/">website of the commission</a> .  Also there you can download data, including tests and pictures. </p><br><p>  <strong>Online stage</strong> </p><br><p>  At the online stage the task was divided into 3 parts. </p><br><ul><li>  <a href="https://mlbootcamp.ru/round/17/sandbox/">Collaborative system</a> - includes all signs, except images and texts; </li><li>  <a href="https://mlbootcamp.ru/round/18/sandbox/">Images</a> - includes only information about images; </li><li>  <a href="https://mlbootcamp.ru/round/19/sandbox/">Texts</a> - includes information about texts only. </li></ul><br><p>  <strong>Offline stage</strong> </p><br><p>  At the offline stage, the data included all signs, while the texts and images were sparse.  There are a lot of lines in dataset, which were already many, 1.5 times more. </p><br><h1 id="reshenie-zadachi">  The solution of the problem </h1><br><p>  Since I work on cv at work, I began my journey in this competition with the task of "Images".  The data that was provided is the userId, objectId, ownerId (the group in which the post was published), the timestamps of creating and displaying the post and, of course, the image for this post. <br>  After generating a few timestamp-based features, the next idea was to take the penultimate layer of the imagenet-trained neurons and send these embeddings to boosting. </p><br><p><img src="https://habrastorage.org/webt/g6/pq/id/g6pqidmcs0hqgilcqvmii7qdzz0.png"></p><br><p>  The results were not impressive.  Embeddings from imagenet neurons are irrelevant, I thought, you need to wash down your autoencoder. </p><br><p><img src="https://habrastorage.org/webt/8b/n_/pu/8bn_pui29109tsvwsf_s5sktlgq.png"></p><br><p>  It took a lot of time and the result did not improve. </p><br><p>  <strong>Feature generation</strong> </p><br><p>  Working with images takes a lot of time, and I decided to do something more simple. <br>  As you can see at once, there are several categorical signs in dataset, and in order not to bother much, I just took catboost.  The solution was excellent, without any settings, I immediately got to the first line of the leaderboard. </p><br><p>  There is a lot of data and they are laid out in parquet format, so I, without thinking twice, took scala and started writing everything in spark. </p><br><p>  The simplest features that gave more growth than image embeddings: </p><br><ul><li>  how many times objectId, userId and ownerId met in the data (must correlate with popularity); </li><li>  how many posts userId saw from ownerId (should correlate with user interest in the group); </li><li>  how many unique userId watched posts at ownerId (reflects the size of the group's audience). </li></ul><br><p>  From the timestamps it was possible to get the time of day at which the user watched the tape (morning / afternoon / evening / night).  By combining these categories, you can continue to generate features: </p><br><ul><li>  how many times userId came in the evening; </li><li>  what time more often this post is shown (objectId) and so on. </li></ul><br><p>  All this gradually improved the metric.  But the size of the training dataset is about 20M records, so adding features greatly slowed down the training. </p><br><p>  I revised the approach to using data.  Although the data are time-dependent, I did not see any clear information leaks ‚Äúin the future‚Äù, nevertheless I broke it just in case: </p><br><p><img src="https://habrastorage.org/webt/jf/zr/gq/jfzrgq7s0l3q465pa-cc0x-rkxq.png"></p><br><p>  The training set provided to us (February and 2 weeks of March) was divided into 2 parts. <br>  On the data of the last N days I trained the model.  The aggregations that I described above built on all the data, including the test.  At the same time, data appeared on which to build various encodings of the target variable.  The simplest approach is to reuse the code that is already creating new features, and simply provide it with data that will not be trained and target = 1. </p><br><p>  Thus, similar features turned out: </p><br><ul><li>  How many times has userId seen a post in the ownerId group; </li><li>  How many times userId liked a post to the ownerId group; </li><li>  Percentage of posts that userId liked by ownerId. </li></ul><br><p>  That is, the <em>mean target encoding</em> is obtained on the part of dataset for various combinations of categorical features.  In principle, catboost also builds target encoding, and from this point of view there is no benefit, but, for example, it became possible to count the number of unique users who liked posts in this group.  At the same time, the main goal has been achieved - my dataset has decreased several times, and it was possible to continue generating features. </p><br><p>  While catboost can only encode by the liked reaction, there are other reactions in feedback: reshared, disliked, unliked, clicked, ignored, which can be hand-made encoders.  I recalculated all sorts of units and sifted out features with low importance so as not to inflate datasets. </p><br><p>  By the time I was in the first place by a wide margin.  The only embarrassment was that image embeddings almost did not increase.  The idea came to give everything at the mercy of catboost.  Cluster Kmeans images and get a new categorical feature imageCat. </p><br><p> Here are some classes after manual filtering and merging of clusters obtained from KMeans. </p><br><p><img src="https://habrastorage.org/webt/i3/s5/m2/i3s5m2ftpdmvr0m_0dvpau4eggs.png"></p><br><p>  Based on imageCat we generate: </p><br><ul><li>  New categorical features: <br><ul><li>  Which imageCat was most often viewed by userId; </li><li>  Which imageCat is most often shown by ownerId; </li><li>  What imageCat most often liked by userId; </li></ul></li><li>  Various counters: <br><ul><li>  How many unique imageCat userId watched; </li><li>  About 15 similar features plus target encoding as described above. </li></ul></li></ul><br><p>  <strong>Texts</strong> </p><br><p>  The results in the competition of images suited me and I decided to try myself in the texts.  Previously, I did not work a lot with the texts and, foolishly, killed the day on tf-idf and svd.  Then I saw the baseline with doc2vec, which does exactly what I need.  By adjusting the parameters of doc2vec a little, I received text embeddings. </p><br><p>  And then I simply reused the code for the images, in which I replaced the embeddings of images with text embeddings.  As a result, got on 2nd place in the contest of texts. </p><br><p>  <strong>Collaborative system</strong> </p><br><p>  There was only one contest left in which I hadn‚Äôt ‚Äúpoked it with a stick‚Äù, and judging by the AUC on the leaderboard, the results of this particular contest should have the most impact on the offline stage. <br>  I took all the signs that were in the original data, chose categorical and calculated the same aggregates as for the images, except the features of the images themselves.  Just sticking it into catboost, I got 2nd place. </p><br><h1 id="pervye-shagi-optimizacii-catboost">  The first steps to optimize catboost </h1><br><p>  One first and two second places pleased me, but there was an understanding that I did not do anything special, which means we can expect a loss of positions. </p><br><p>  The task of the competition is the ranking of posts within the user, and all this time I was solving the classification problem, that is, I optimized the wrong metric. </p><br><p>  I will give a simple example: </p><br><table><thead><tr><th>  userId </th><th>  objectId </th><th>  prediction </th><th>  ground truth </th></tr></thead><tbody><tr><td>  one </td><td>  ten </td><td>  0.9 </td><td>  one </td></tr><tr><td>  one </td><td>  eleven </td><td>  0.8 </td><td>  one </td></tr><tr><td>  one </td><td>  12 </td><td>  0.7 </td><td>  one </td></tr><tr><td>  one </td><td>  13 </td><td>  0.6 </td><td>  one </td></tr><tr><td>  one </td><td>  14 </td><td>  0.5 </td><td>  0 </td></tr><tr><td>  2 </td><td>  15 </td><td>  0.4 </td><td>  0 </td></tr><tr><td>  2 </td><td>  sixteen </td><td>  0.3 </td><td>  one </td></tr></tbody></table><br><p>  We make a small permutation </p><br><table><thead><tr><th>  userId </th><th>  objectId </th><th>  prediction </th><th>  ground truth </th></tr></thead><tbody><tr><td>  one </td><td>  ten </td><td>  0.9 </td><td>  one </td></tr><tr><td>  one </td><td>  eleven </td><td>  0.8 </td><td>  one </td></tr><tr><td>  one </td><td>  12 </td><td>  0.7 </td><td>  one </td></tr><tr><td>  one </td><td>  13 </td><td>  0.6 </td><td>  0 </td></tr><tr><td>  2 </td><td>  sixteen </td><td>  0.5 </td><td>  one </td></tr><tr><td>  2 </td><td>  15 </td><td>  0.4 </td><td>  0 </td></tr><tr><td>  one </td><td>  14 </td><td>  0.3 </td><td>  one </td></tr></tbody></table><br><p>  We get the following results: </p><br><table><thead><tr><th>  Model </th><th>  AUC </th><th>  User1 AUC </th><th>  User2 AUC </th><th>  mean AUC </th></tr></thead><tbody><tr><td>  Option 1 </td><td>  0.8 </td><td>  1.0 </td><td>  0.0 </td><td>  0.5 </td></tr><tr><td>  Option 2 </td><td>  0.7 </td><td>  0.75 </td><td>  1.0 </td><td>  0.875 </td></tr></tbody></table><br><p>  As can be seen, an improvement in the total AUC metric does not mean an improvement in the average AUC metric within the user. </p><br><p>  Catboost <a href="https://github.com/catboost/catboost/blob/master/catboost/tutorials/ranking/ranking_tutorial.ipynb">can optimize ranking metrics</a> out of the box.  I read about the ranking metrics, <a href="http://proceedings.mlr.press/v14/gulin11a.html">success stories</a> when using catboost and set YetiRankPairwise to study at night.  The result was not impressive.  Deciding that I had not been trained, I changed the error function to QueryRMSE, which, judging by the catboost documentation, converges faster.  As a result, I received the same results as in the training for classification, but the ensembles of these two models gave a good increase, which brought me to the first places in all three competitions. </p><br><p>  5 minutes before the closing of the online stage in the competition "Collaborative Systems" Sergei Shalnov pushed me to second place.  Further path we passed together. </p><br><h1 id="podgotovka-k-oflayn-etapu">  Preparing for the offline stage </h1><br><p>  The victory in the online stage was guaranteed by the RTX 2080 TI video card, but the main prize of 300,000 rubles and, most likely, the final first place made us work these 2 weeks. </p><br><p>  As it turned out, Sergey also used catboost.  We exchanged ideas and features, and I found out about the <a href="https://www.youtube.com/watch%3Fv%3DUYDwhuyWYSo%26feature%3Dyoutu.be">report of Anna Veronika Dorozh</a> in which there were answers to many of my questions, and even to those that I had not yet had by that time. </p><br><p>  Viewing the report led me to the idea that it is necessary to return all the parameters to the default value, and to tune the settings very carefully and only after fixing the set of features.  Now one training took about 15 hours, but one model managed to get faster sooner than it did in the ensemble with ranking. </p><br><p>  <strong>Feature generation</strong> </p><br><p>  In the competition "Collaborative Systems" a large number of features are evaluated as important for the model.  For example, <em>auditweights_spark_svd</em> is the most important feature, and there is no information about what it means.  I thought that it was worth counting the various units, based on important signs.  For example, the average auditweights_spark_svd by user, by group, by object.  The same can be calculated from the data on which training is not performed and target = 1, that is, the average <em>auditweights_spark_svd</em> for the user for the objects that he liked.  <em>There</em> were several important signs besides <em>auditweights_spark_svd</em> .  Here are some of them: </p><br><ul><li>  <em>auditweightsCtrGender</em> </li><li>  <em>auditweightsCtrHigh</em> </li><li>  <em>userOwnerCounterCreateLikes</em> </li></ul><br><p>  For example, the average <em>auditweightsCtrGender</em> by userId turned out to be an important feature, as well as the average <em>userOwnerCounterCreateLikes</em> by userId + ownerId.  This should have made you think about the need to understand the meaning of the fields. </p><br><p>  Also important features were <em>auditweightsLikesCount</em> and <em>auditweightsShowsCount</em> .  Having divided one into another, it turned out to be an even more important feature. </p><br><p>  <strong>Data leakage</strong> </p><br><p>  Competition and production models are very different tasks.  When preparing the data it is very difficult to take into account all the details and not to pass some kind of non-trivial information about the target variable on the test.  If we create a production solution, then we will try to avoid using data leaks when training the model.  But if we want to win the competition, then data leaks are the best features. </p><br><p>  After examining the data, you can see that for objectId, the values ‚Äã‚Äãof <em>auditweightsLikesCount</em> and <em>auditweightsShowsCount</em> change, which means that the ratio of the maximum values ‚Äã‚Äãof these attributes reflects the conversion of the post much better than the ratio at the time of display. </p><br><p>  The first leak we found was <em>auditweightsLikesCountMax / auditweightsShowsCountMax</em> . <br>  And what if you look at the data more closely?  Sort by date of display and get: </p><br><table><thead><tr><th>  objectId </th><th>  userId </th><th>  auditweightsShowsCount </th><th>  auditweightsLikesCount </th><th>  target (is liked) </th></tr></thead><tbody><tr><td>  one </td><td>  one </td><td>  12 </td><td>  3 </td><td>  probably not </td></tr><tr><td>  one </td><td>  2 </td><td>  15 </td><td>  3 </td><td>  maybe yes </td></tr><tr><td>  one </td><td>  3 </td><td>  sixteen </td><td>  four </td><td></td></tr></tbody></table><br><p>  It was amazing when I found the first such example and it turned out that my prediction did not come true.  But, taking into account the fact that the maximum values ‚Äã‚Äãof these attributes within the object gave an increase, we were not lazy and decided to find <em>auditweightsShowsCountNext</em> and <em>auditweightsLikesCountNext</em> , that is, the values ‚Äã‚Äãat the next moment in time.  Adding feature <br>  <em>(auditweightsShowsCountNext-auditweightsShowsCount) / (auditweightsLikesCount-auditweightsLikesCountNext)</em> we made a sharp jump over early. <br>  Similar leaks could be used if the following values ‚Äã‚Äãwere found for <em>userOwnerCounterCreateLikes</em> within userId + ownerId and, for example, <em>auditweightsCtrGender</em> within objectId + userGender.  We found 6 similar fields with leaks and pulled the most information out of them. </p><br><p>  By that time, we squeezed the maximum information from the collaborative traits, but did not return to contests of images and texts.  There was a great idea to check: how many features are given directly by the images or texts in the respective contests? </p><br><p>  There were no leaks in contests in images and texts, but by that time I had returned the default catboost parameters, brushed my code and added a few features.  Total happened: </p><br><table><thead><tr><th>  Decision </th><th>  is fast </th></tr></thead><tbody><tr><td>  Maximum with images </td><td>  0.6411 </td></tr><tr><td>  Maximum without images </td><td>  0.6297 </td></tr><tr><td>  Second place result </td><td>  0.6295 </td></tr></tbody></table><br><table><thead><tr><th>  Decision </th><th>  is fast </th></tr></thead><tbody><tr><td>  Maximum with lyrics </td><td>  0.666 </td></tr><tr><td>  Maximum without texts </td><td>  0.660 </td></tr><tr><td>  Second place result </td><td>  0.656 </td></tr></tbody></table><br><table><thead><tr><th>  Decision </th><th>  is fast </th></tr></thead><tbody><tr><td>  Maximum in collaborative </td><td>  0.745 </td></tr><tr><td>  Second place result </td><td>  0.723 </td></tr></tbody></table><br><p>  It became obvious that it would hardly be possible to squeeze a lot out of texts and images, and we, having tried a couple of the most interesting ideas, stopped working with them. </p><br><p>  Further generation of features in collaborative systems did not give an increase, and we started ranking.  At the online stage, the ensemble of classification and ranking gave me a small increase, as it turned out, because I did not train the classification.  None of the error functions, including YetiRanlPairwise, even came close to the result that LogLoss gave (0.745 versus 0.725).  There was hope for QueryCrossEntropy, which could not be launched. </p><br><h1 id="oflayn-etap">  Offline stage </h1><br><p>  At the offline stage, the data structure remained the same, but there were minor changes: </p><br><ul><li>  userId, objectId, ownerId identifiers have been re-randomized; </li><li>  several signs were removed and several renamed; </li><li>  data was about 1.5 times more. </li></ul><br><p>  In addition to the listed difficulties, there was one big plus: a large server with RTX 2080TI was allocated to the team.  I enjoyed htop for a long time. <br><img src="https://habrastorage.org/webt/0c/6n/zp/0c6nzpburmw8nxsxz9y8iwkxmwa.jpeg"></p><br><p>  The idea was one - just to reproduce what is already there.  After spending a couple of hours setting up the environment on the server, we gradually began to check that the results are reproduced.  The main problem we are facing is an increase in data volume.  We decided to slightly reduce the load and set the parameter catboost ctr_complexity = 1.  This lowers the speed a bit, but my model began to work, the result was good - 0.733.  Sergey, unlike me, did not break the data into 2 parts and studied all the data, although it gave the best result at the online stage, at the offline stage there were a lot of difficulties.  If we take all the features that we wrote and try to shove in catboost, then it would not have worked on the online stage.  Sergey did type optimization, for example, converting float64 to float32 types.  <a href="https://www.dataquest.io/blog/pandas-big-data/">In this article</a> you can find information on memory optimization in pandas.  As a result, Sergey studied on the CPU on all data and it turned out about 0.735. </p><br><p>  These results were enough to win, but we hid our real soon and could not be sure that other teams are not doing the same. </p><br><h1 id="bitva-do-poslednego">  Battle to the last </h1><br><p>  <strong>Tuning catboost</strong> </p><br><p>  Our solution was completely reproduced, we added features of text data and images, so it only remained to tune the catboost parameters.  Sergey learned on a CPU with a small number of iterations, and I learned on ctr_complexity = 1.  There was only one day left, and if you just add iterations or increase ctr_complexity, you could get a better day by morning, and go for a walk all day. </p><br><p>  At an offline stage, quick it was possible to hide very easily, just choosing not the best solution on the site.  We expected dramatic changes in the leaderboard in the last minutes before the closure of the submits and decided not to stop. </p><br><p>  From Anna's video, I learned that to improve the quality of the model, it is best to select the following parameters: </p><br><ul><li>  <em>learning_rate</em> - The default value is calculated based on the size of the dataset.  When learning_rate decreases, in order to preserve quality, it is necessary to increase the number of iterations. </li><li>  <em>l2_leaf_reg</em> - Regularization coefficient, default value 3, it is desirable to choose from 2 to 30. A decrease in the value leads to an increase in the overfit. </li><li>  <em>bagging_temperature</em> - adds randomization to the weights of the objects in the sample.  The default value is 1, at which the weights are selected from the exponential distribution.  Decreasing the value leads to an increase in overfit. </li><li>  <em>random_strength</em> - Affects the choice of splits at a specific iteration.  The higher the random_strength, the higher the chance for a split with low importance to be selected.  At each subsequent iteration, randomness decreases.  Decreasing the value leads to an increase in overfit. </li></ul><br><p>  Other parameters have a much smaller effect on the final result, so I did not try to select them.  One iteration of training on my dataset on a GPU with ctr_complexity = 1 took 20 minutes, and the selected parameters on a smaller dataset were slightly different from the optimal ones on a full dataset.  As a result, I did about 30 iterations on 10% of the data, and then about 10 more iterations on all the data.  It turned out about the following: </p><br><ul><li>  I increased <em>learning_rate</em> by 40% from default; </li><li>  <em>l2_leaf_reg</em> left <em>unchanged</em> ; </li><li>  <em>bagging_temperature</em> and <em>random_strength</em> reduced to 0.8. </li></ul><br><p>  It can be concluded that the model is under-trained with default parameters. </p><br><p>  I was very surprised when I saw the result on the leaderboard: </p><br><table><thead><tr><th>  Model </th><th>  model 1 </th><th>  model 2 </th><th>  model 3 </th><th>  ensemble </th></tr></thead><tbody><tr><td>  Without tuning </td><td>  0.7403 </td><td>  0.7404 </td><td>  0.7404 </td><td>  0.7407 </td></tr><tr><td>  With tuning </td><td>  0.7406 </td><td>  0.7405 </td><td>  0.7406 </td><td>  0.7408 </td></tr></tbody></table><br><p>  I made a conclusion for myself that if you do not need a quick application of the model, then it is better to replace the selection of parameters with an ensemble of several models with non-optimized parameters. </p><br><p>  Sergey was engaged in the optimization of the size of the dataset to run it on the GPU.  The easiest option is to cut off part of the data, but this can be done in several ways: </p><br><ul><li>  gradually remove the oldest data (beginning of February) until it starts to fit into memory; </li><li>  remove features with the lowest importance; </li><li>  remove userId for which there is only one entry; </li><li>  leave only userId, which is in the test. </li></ul><br><p>  And in the end - to make an ensemble of all the options. </p><br><p>  <strong>Last ensemble</strong> </p><br><p>  By the late evening of the last day we laid out the ensemble of our models, which gave 0.742.  At night, I launched my model with ctr_complexity = 2 and instead of 30 minutes, she studied for 5 hours.  Only at 4 in the morning she was counted, and I did the last ensemble, which gave 0.7433 on a public leaderboard. </p><br><p>  Due to the different approaches to solving the problem, our predictions did not correlate strongly, which gave a good increase in the ensemble.  To get a good ensemble, it is better to use the raw predictions of the predict model (prediction_type = 'RawFormulaVal') and set scale_pos_weight = neg_count / pos_count. </p><br><p><img src="https://habrastorage.org/webt/ki/yv/yj/kiyvyjpnsouo46k-ny9e9hmyjxa.png"></p><br><p>  On the site you can see the <a href="https://mlbootcamp.ru/round/20/rating/">final results on a private leaderboard</a> . </p><br><p>  <strong>Other solutions</strong> </p><br><p>  Many teams followed the canons of recommender algorithms.  I, being not an expert in this field, cannot assess them, but I remember 2 interesting solutions. </p><br><ul><li>  <a href="https://habr.com/ru/company/mailru/blog/445348/">The decision of Nikolai Anokhin</a> .  Nikolay, being an employee of Mail.ru, did not claim for prizes, therefore he set himself the goal of not getting the maximum possible rate, but getting an easily scalable solution. </li><li>  The decision of the team that won the jury prize, based on <a href="https://arxiv.org/pdf/1807.05520.pdf">this article from facebook</a> , made it possible to cluster images very well without manual work. </li></ul><br><h1 id="zaklyuchenie">  Conclusion </h1><br><p>  What is most deposited in the memory: </p><br><ul><li>  If there are categorical features in the data, and you know how to do target encoding correctly, it's still best to try catboost. </li><li>  If you participate in the competition, you should not spend time on the selection of parameters, except for learning_rate and iterations.  A quicker solution is to make an ensemble of several models. </li><li>  Boosting know how to study on the GPU.  Catboost can learn very quickly on the GPU, but eats a lot of memory. </li><li>  During development and validation of ideas, it is better to set a small rsm ~ = 0.2 (CPU only) and ctr_complexity = 1. </li><li>  Unlike other teams, the ensemble of our models gave a big boost.  We only exchanged ideas and wrote in different languages.  We had a different approach to data partitioning and, I think, everyone had their own bugs. </li><li>  It is not clear why ranking optimization yielded a result worse than classification optimization. </li><li>  I got a little experience with texts and understanding how recommender systems are made. </li></ul><br><p><img src="https://habrastorage.org/webt/en/m_/to/enm_topolhyttqfezaldsddvjs0.jpeg"></p><br><p>  Thanks to the organizers for the received emotions, knowledge and prizes. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/447376/">https://habr.com/ru/post/447376/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../447366/index.html">What is the best way to start a project or how to make it so that it would not be then painfully painful</a></li>
<li><a href="../447368/index.html">Not just catching fleas. Why speed is so important for any store</a></li>
<li><a href="../447370/index.html">Why, because of a broken patent system, the shadow of Theranos still hangs over us</a></li>
<li><a href="../447372/index.html">IT transformation is inevitable: where to start</a></li>
<li><a href="../447374/index.html">Creating interface elements programmatically using PureLayout (Part 2)</a></li>
<li><a href="../447380/index.html">Exceptions in Kotlin and their features</a></li>
<li><a href="../447382/index.html">The book "Unity and C #. Gamedev from idea to implementation. 2nd ed ¬ª</a></li>
<li><a href="../447384/index.html">Power semiconductors on guard environmental</a></li>
<li><a href="../447388/index.html">TL; ITMO University DR-digest: non-classical admission to university, upcoming events and the most interesting materials</a></li>
<li><a href="../447390/index.html">Recommendations for configuring AFA AccelStor when working with VMware vSphere</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>