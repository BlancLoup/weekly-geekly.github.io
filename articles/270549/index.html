<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How we moved the disk space of hundreds of bank branches to a single storage system in Moscow without losing LAN speeds in the field</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="It is given: a bank with a data center in Moscow and many branches. 

 In the data center there is a bunch of x86-machines and a serious high-end data...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How we moved the disk space of hundreds of bank branches to a single storage system in Moscow without losing LAN speeds in the field</h1><div class="post__text post__text-html js-mediator-article">  <b>It is given: a</b> bank with a data center in Moscow and many branches. <br><br>  In the data center there is a bunch of x86-machines and a serious high-end data storage system (DSS).  In branches, a network with a central server or mini cluster (+ backup server and low-end storage) with a disk basket is implemented.  Backup of general data is done on the tape (and in the evening in the safe) or on another server next to the first one.  Critical data (financial transactions, for example) are replicated asynchronously to the center.  The server is running Exchange, AD, antivirus, file server, and so on.  There is also data that is not critical for the banking network (these are not direct transactions), but still very important - for example, documents.  They are not replicated, but sometimes they are backed up at night when the branch does not work.  Half an hour after the end of the working day, all sessions are extinguished, and a large copy begins. <br><br><img src="https://habrastorage.org/files/4ee/152/19c/4ee15219c6ed417d978404ce218cded6.png"><br>  <i>That's how it was arranged before the start of work.</i> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The problem, of course, is that <b>all this is slowly beginning to increase the technological debt.</b>  A good solution would be to make VDI access (this would eliminate the need to keep a huge service team and would greatly simplify administration), but VDI requires extensive channels and small delays.  And this is not always easy in Russia due to the lack of main optics in a number of cities.  With each month, the number of unpleasant ‚Äúpre-emergency‚Äù incidents increases, and iron restrictions constantly interfere. <br><br>  And now the bank decided to do what seems to be more expensive to implement, if we take it directly, but it greatly simplifies servicing the server infrastructure in the branches and guarantees the safety of the branch data: <b>consolidate all the data into one central storage system</b> .  Only not simple, but also with a smart local cache. <a name="habracut"></a><br><br><h4>  Study </h4><br>  Naturally, we turned to world experience - after all, such problems were solved several times for mining companies for sure.  First found was Alamos Gold, a corporation that digs for gold in Mexico and explores deposits in Turkey. <br><br>  The data (and there are quite a lot of them, especially raw from geological exploration) must be transferred to the head office in Toronto.  WAN channels are narrow, slow, and often weather dependent.  As a result, they wrote on flash drives and blanks and sent data by physical mail or physical couriers.  IT support was a natural ‚Äúphone sex‚Äù, only corrected by 2-3 language leaps through translators.  Because of the need to save data locally, increasing the WAN speed alone would not solve the problem.  Alamos was able to avoid the cost of deploying physical servers at each mine by using Riverbed SteelFusion, a specialized Riverbed SFED device that combines the ability to increase WAN speed, the Virtual Services Platform (VSP) virtualization environment and the SteelFusion peripheral virtual server infrastructure (edge-VSI).  VSP gave local computing resources.  Without upgrading the channels, it was possible, after receiving the master volume snapshot, to transfer data back and forth normally.  Return on investment - 8 months.  There was a normal disaster recovery procedure. <br><br>  They began to pick this solution and found two more cases with the iron manufacturer, describing our situation almost exactly. <br><br>  Bill Barrett Corporation needed to upgrade equipment at remote sites; at first, it looked at the traditional half-rack solution, which was expensive but would not solve many current problems.  In addition, most likely, it would be necessary to increase the channel bandwidth to these sites, which doubled the cost.  High costs are not the only drawback of this approach.  The IT skills of personnel at remote sites were limited, and the proposed solution required someone to manage servers, switches and backup equipment.  They also installed Riverbed SteelFusion, as a result, the case turned out to be three times cheaper than the traditional solution, and the space in the racks was significantly less. <br><br>  The law firm Paul Hastings LLP grew, opening offices in Asia, Europe and the USA, and the number of its data processing centers increased (four central data centers and many small data centers for 19 offices).  Although this architecture provided the availability of remote offices, each data center needed a manager and 1‚Äì2 analytics, as well as physical host servers and tape backup systems.  This was costly, and in some regions data protection was not as reliable as the company would have liked.  We decided the same, only the second motive was safety. <br><br>  Accordingly, they calculated this and a couple more options in traditional architectures and showed them to the customer.  The customer thought for a long time, thought, asked a bunch of questions and chose this option with the condition of test deployment of one ‚Äúbranch‚Äù (before purchasing the main project hardware) on the test bases. <br><br>  That's what we did <br><br>  We do not have the ability to change channels, but we can put the hardware on the side of the data center and on the branch side.  The storage system in the data center is split into multiple volumes, and each branch works with 2‚Äì4 of them (injective connection: several branches cannot work with the same volume).  We throw out disk shelves and some servers at places, they are no longer needed as storage and replication controllers.  In the data center, we set up simple Riverbed Steelhead CX traffic optimizers + SteelFusion Core virtual devices; a pair of Riverbed SFEDs (SteelFusion Edge) is installed on site. <br><br><img src="https://habrastorage.org/files/140/5b6/8c2/1405b68c2ba546a3984657bad92644ff.png"><br><br><img src="https://habrastorage.org/files/24d/cbd/9a2/24dcbd9a265e4fb8889756f663b4f93f.png"><br><br>  Previously, servers worked with data locally (on local disks or on low-end storage systems).  The servers now work with data from the central storage system through a local projection of LUNs provided by SteelFusion.  At the same time, the servers ‚Äúthink‚Äù that they work with local volumes in the local branch network. <br><br>  The main branch of the branch is called Riverbed SFED (SteelFusion Edge), it consists of three components.  These are SteelHead (optimization + traffic compression), SteelFusion Edge (element of the data centralization system) and VSP (virtualization by the integrated hypervisor ESXi). <br><br><h4>  What happened </h4><br><ul><li>  Addressing for a branch is a single LAN with a central storage system (more precisely, a pair of its volumes).  Servers access the central storage system as an instance within the LAN. </li><li>  At the first request, data blocks begin to be broadcast from the center to the branch (slowly, but only once).  Running a little further, we use the Pin the LUN mode when the cache (blockstore) is equal to the size of the LUN, that is, we immediately remove the full data cast from the central storage system on the first run. </li><li>  When any data changes on our side, they queue up for synchronization and immediately become available ‚Äúas if in the center‚Äù, but from the SteelFusion Edge cache located locally. </li><li>  When transferring data to any of the parties, efficient compression, deduplication and protocol overrides are used to optimize them (bulky and chatty protocols are translated by devices into optimized for narrow channels with a large delay). </li><li>  In general, all data is always stored in a data storage system in a central data center. </li><li>  "For dessert" there was a cool block-level prefetching.  By reading the contents of the blocks and applying knowledge of file systems, SteelFusion is able to determine what the OS is actually doing (for example, loading, launching an application, opening a document).  It can then determine which data blocks will be needed for reading, and loads them before the OS requests them.  Which is much faster than doing everything consistently. </li></ul><br><br><img src="https://habrastorage.org/files/be4/b4b/f6b/be4b4bf6b26c47bf9f1bea88a9508f34.png"><br><br>  At a practical level, the software has earned almost without delay, replication at night is forgotten like a bad dream.  Another feature - in the center is really all the data of the branch.  That is, if previously ‚Äúfile-washing‚Äù scans of documents, presentations, various non-critical documents and everything that is expensive for ordinary users could be on local disks - now it also lies in the center and is also easily restored if it fails.  But about this and increasing resiliency just below.  And of course, one storage system is much easier to maintain than a dozen.  No more "phone sex" with a "programmer" from a faraway small town. <br><br><img src="https://habrastorage.org/files/1b1/332/20f/1b133220fb09414c80863a3e62080027.png"><br><br>  They are mounted like this: <br><br><img src="https://habrastorage.org/files/576/aaa/f6b/576aaaf6b885467b861b1b76b0d67484.png"><br><br><h4>  What happened: </h4><br><ul><li>  The so-called quasi-synchronous replication (this is when for the branch they look like synchronous, and for the center - as fast asynchronous). </li><li>  There were quick and convenient recovery points up to minutes, and not "at least a day ago." </li><li>  Previously, in the event of a fire at the branch, the local file ball and mail of the city employees were lost.  Now all this is also synchronized and will not be lost in case of an accident. </li><li>  The new box has simplified all branch recovery procedures - a new office is deployed on a new hardware in a matter of minutes (the new city is being developed from ready-made images in the same way). </li><li>  We had to remove servers from the infrastructure on the ground, plus throw out small tape drives for backup, also on the ground.  Instead, Riverbed's hardware was purchased, a central storage system was replenished with disks, and a new large backup library was purchased for backup installed in another Moscow data center. </li><li>  Data security has improved thanks to uniform and easily controlled access rules and more robust channel encryption. </li><li>  With the disaster of the branch, accompanied by the destruction of infrastructure, there was a simple opportunity to run virtual servers in the data center.  As a result, RTO and RPO are rapidly decreasing. </li></ul><br><br><h4>  What happens when a short break connection? </h4><br>  In the current infrastructure - nothing special.  Data continues to be written to the SFED cache, and when the channel is restored, it is synchronized.  Users at the request "in the center" is given a local cache.  It is worth adding that the problem of ‚Äúschizophrenia‚Äù does not arise, since access to the LUN is only through the SFED of a specific branch, that is, from the data center side nobody writes to our volume. <br><br><h4>  What happens when a connection breaks for more than 4 hours? </h4><br>  If the connection is broken for a long time, the moment may come when the local SFED cache is full, and the file system will signal that there is not enough space to write.  The necessary data can not be recorded, as the system will warn the user or the server. <br><br>  We tested this emergency branch in Moscow several times.  The time calculation is: <br><ol><li>  Delivery time spares. </li><li>  SFED initial configuration time (&lt;30 minutes). </li><li>  The load time of the OS virtual servers through the communication channel with an empty cache. </li></ol><br>  Windows boot time on the communication channel with a delay of 100 milliseconds is less than 10 minutes.  The point is that to boot the OS, you do not need to wait until all the data from the C drive is transferred to the local cache.  Booting the OS, of course, is accelerated by intelligent block prefetching, mentioned above, and advanced Riverbed optimization. <br><br><img src="https://habrastorage.org/files/963/644/e2f/963644e2f34e4ee9aa456040af2cfa2c.png"><br><br><h4>  Load on storage </h4><br>  Naturally, the load on the central storage system drops, because most of the work falls on the devices in the branches.  Here is a picture from the manufacturer on tests of storage performance optimization, because there are now far fewer references to it: <br><br><img src="https://habrastorage.org/files/588/7d5/76d/5887d576dec84804a7d2c231f90f749b.png"><br><br><h4>  Explanation of quasi-synchronous replication modes </h4><br>  Before I spoke about Pin the LUN, when the cache is equal LUN.  Since the cache on the devices in the branches is not upgraded (you need to buy a new piece of hardware or put a second row, and by the time it is needed - in 4‚Äì5 years, they will already be a new generation, most likely), you need to take into account the demand for a sharp increase in the number data in the branch.  The planned is calculated and laid for many years to come, but the unplanned will be decided by switching to the Working Set mode. <br><br><img src="https://habrastorage.org/files/c5f/533/1ff/c5f5331ff9294016ba71a0aa95968690.png"><br><br>  This is when the blockstore (local cache dictionary) is less than branch data.  Typically, the proportion of the cache in comparison with the total amount of data - 15-25%.  It is not possible to work completely autonomously here, using the cache as a copy of the central LUN: at this moment, in the link fail mode, the recording goes but is buffered.  The channel will be restored - give the record in the center.  When a block is requested that is not in the local storage, a normal connection error is generated.  If there is a block, the data is sent.  I suppose that in those 5 years, when the amount of data exceeds the capacity of the branch cache, admins will not buy more hardware, but simply centralize mail and transfer the file into the Working Set mode, the critical data will be left in Pin the LUN mode. <br><br>  One more thing.  Retrofitting with the second SFED creates a failover cluster, which may also be important in the future. <br><br><h4>  Tests and trial operation </h4><br>  We did such a rather unusual combination and virtualization of storage systems for the first time - the project differs from others by setting up local blockstore, linked in the PAC with traffic optimizers and virtualization servers.  I collapsed several times and collected clusters of devices to see possible problems in the process.  A couple of times on a significant reconfiguration, I caught a full warm-up of the cache at the branch, but found a way to bypass it (when not needed).  From pitfalls - it is quite nontrivial to completely reset the blockstore on one specific device, it is better to learn from tests before working with combat data.  Plus, on the same tests, they caught one exotic kernel crush on a central machine, described the situation in detail, sent it to the manufacturer, they sent a patch. <br><br>  By recovery time - the wider the channel, the faster the data will be restored at the branch. <br><br>  It is important that the core does not give the possibility to give the data to specific SFEDs in priority, that is, the storage operations and channels are used evenly - to give a ‚Äúgreen‚Äù for fast data transfer to a fallen branch using this PAC will not work.  Therefore, one more recommendation is to leave a small power storage margin for such cases.  In our case, the capacity of the storage system is enough for the eyes.  However, it is possible for QoS configurations on the same SteelHead or other network devices to allocate bandwidth for each branch.  And on the other hand, limit SteelFusion synchronization traffic so that centralized business application traffic does not suffer. <br><br>  The second most important - resistance to cliffs.  As I understand it, their security officers added a voice for the project, who liked the idea of ‚Äã‚Äãkeeping all the data in the center.  Admins are glad that they no longer fly to the field, but, of course, some of the local enikeev suffered because the tape and servers did not have to be serviced. <br><br>  By itself, this architecture on the Riverbed hardware allows us to do a lot of everything, in particular, fumble printers around the cities, do not quite ordinary proxies and firewalls, use the server power of other cities for big miscalculations, etc. But we are not in the project required (at least for now), so it remains only to rejoice and wonder how many more features you can nakovyryat. <br><br><h4>  Iron </h4><br>  <b>SteelFusion Edge: A</b> converged device that integrates servers, storage, networking and virtualization to run local branch office applications.  No other infrastructure in the branch office is required anymore. <br><br><img src="https://habrastorage.org/files/c5f/18b/6be/c5f18b6be3664992867116ef2bad7953.png"><br><br>  <b>SteelFusion Core: The</b> storage delivery controller is located in the data center and interacts with the storage system.  SteelhFusion Core projects centralized data to the branches, eliminating backups in the branches, and provides rapid deployment of new branches and disaster recovery. <br><br><img src="https://habrastorage.org/files/f0f/139/b40/f0f139b40b8640cd94532e5a30b46d4c.png"><br><br><h4>  Links </h4><br>  Actually, perhaps you will still be interested to know: <br><ul><li>  Pro <a href="http://habrahabr.ru/company/croc/blog/214693/">basic use of traffic optimizers with a local blockstore</a> . </li><li>  <a href="http://habrahabr.ru/company/croc/blog/267883/">The practical solution to optimize traffic</a> , where it was not necessary to do LAN speeds in the branches (satellite channels). </li><li>  About the <a href="http://habrahabr.ru/company/croc/blog/215585/">network detective with the search for anomalies</a> (it was an educational detective, no one was hurt). </li><li>  Here you can download a general description of the solution and application options from the vendor (but there you will have to fill in the form with the mail). </li><li>  And my mail, if your question is not for comments, or if you need to first estimate the cost of a similar solution for yourself: <b>AVrublevsky@croc.ru</b> </li><li>  The day after tomorrow, November 12, we are <a href="http://www.croc.ru/action/detail/59316/">holding a webinar on how to reduce the cost of this part of the IT infrastructure</a> .  Everything is detailed there, with practical calculations and with an overview of various options for action. </li></ul></div><p>Source: <a href="https://habr.com/ru/post/270549/">https://habr.com/ru/post/270549/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../270537/index.html">How I became an android developer without a core education, simultaneously hindering concrete</a></li>
<li><a href="../270541/index.html">How to calculate the range of communication equipment Ubiquiti. New version of the calculator AirLink</a></li>
<li><a href="../270543/index.html">TensorFlow: Google's machine learning, now smarter for everyone</a></li>
<li><a href="../270545/index.html">Declarative C ++ programming</a></li>
<li><a href="../270547/index.html">Hidden dependencies as a design ‚Äúsmell‚Äù</a></li>
<li><a href="../270551/index.html">Use VTune Amplifier 2016 to analyze the HelloOpenCL application for GPU</a></li>
<li><a href="../270555/index.html">Veeam Cloud Connect in Microsoft Azure</a></li>
<li><a href="../270557/index.html">What is useful you can extract from the report on the clouds in Russia</a></li>
<li><a href="../270559/index.html">The book "Learning C + + through game programming"</a></li>
<li><a href="../270563/index.html">Creating a VPN tunnel between two apartments based on routers with dd-wrt</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>