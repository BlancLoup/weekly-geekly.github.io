<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Playing Mortal Kombat with TensorFlow.js</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Experimenting with improvements to the Guess.js prediction model , I began to look closely at the deep learning: recurrent neural networks (RNN), in p...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Playing Mortal Kombat with TensorFlow.js</h1><div class="post__text post__text-html js-mediator-article">  Experimenting with improvements to the <a href="https://github.com/guess-js/guess">Guess.js</a> prediction <a href="https://github.com/guess-js/guess">model</a> , I began to look closely at the deep learning: recurrent neural networks (RNN), in particular, LSTM, because of their <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">‚Äúunreasonable efficiency‚Äù</a> in the area where Guess.js works.  At the same time, I started playing with convolutional neural networks (CNN), which are also often used for time series.  CNN is commonly used to classify, recognize and detect images. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1fb/9be/edc/1fb9beedcad00d1c0dcdc7bbef67e6d9.png"><br>  <i><font color="gray">Control <a href="">MK.js</a> with TensorFlow.js</font></i> <br><br><blockquote>  The source code for <a href="https://github.com/mgechev/mk-tfjs">this article</a> and <a href="">MK.js</a> are on my <a href="https://github.com/mgechev">GitHub</a> .  I have not laid out the training data set, but you can build your own and train the model as described below! </blockquote><a name="habracut"></a><br>  Having played with CNN, I remembered an <a href="https://www.youtube.com/watch%3Fv%3D0_yfU_iNUYo">experiment</a> that I conducted several years ago when browser developers released the <code>getUserMedia</code> API.  In it, the user's camera served as a controller for playing a small JavaScript-clone of Mortal Kombat 3. You can find that game in <a href="">the GitHub repository</a> .  As part of the experiment, I implemented a basic positioning algorithm, which classifies an image into the following classes: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  Blow left or right hand </li><li>  Kick left or right foot </li><li>  Steps left and right </li><li>  Squat </li><li>  None of the above </li></ul><br>  The algorithm is so simple that I can explain it in a few sentences: <br><br><blockquote>  The algorithm photographs the background.  As soon as the user appears in the frame, the algorithm calculates the difference between the background and the current frame with the user.  So he determines the position of the user's figure.  The next step is to display the user's body in white on black.  After that, vertical and horizontal histograms are constructed, summarizing the values ‚Äã‚Äãfor each pixel.  Based on this calculation, the algorithm determines the current body position. </blockquote><br>  The video shows how the program works.  <a href="">GitHub</a> source code. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/0_yfU_iNUYo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Although the tiny MK clone worked successfully, the algorithm is far from perfect.  Requires a frame with a background.  For proper operation, the background must be the same color throughout the program.  Such a restriction means that changes in light, shadows and other things will interfere and give an inaccurate result.  Finally, the algorithm does not recognize actions;  it classifies the new frame only as a body position from a predefined set. <br><br>  Now, thanks to the progress in the web API, namely WebGL, I decided to return to this task by applying TensorFlow.js. <br><br><h1>  Introduction </h1><br>  In this article, I will share the experience of creating a body position classification algorithm using TensorFlow.js and MobileNet.  Consider the following topics: <br><br><ul><li>  Collection of training data for image classification </li><li>  Data <a href="https://github.com/aleju/imgaug">augmentation</a> with <a href="https://github.com/aleju/imgaug">imgaug</a> </li><li>  Transfer training with MobileNet </li><li>  Binary classification and N-tric classification </li><li>  Learning the model for classifying images TensorFlow.js in Node.js and using it in a browser </li><li>  A few words about the classification of actions with LSTM </li></ul><br>  In this article, we will reduce the problem to determining the position of the body on the basis of one frame, as opposed to recognizing an action by a sequence of frames.  We will develop a model of deep learning with a teacher, which, based on the image from the user's webcam, determines the movements of a person: a punch, kick or none of this. <br><br>  By the end of the article we will be able to build a model for playing <a href="">MK.js</a> : <br><br><img src="https://habrastorage.org/webt/2u/0e/g6/2u0eg6ng2p4kwxosmut1koa751g.gif"><br><br>  For a better understanding of the article, the reader should be familiar with the basic concepts of programming and JavaScript.  A basic understanding of deep learning is also helpful, but not necessary. <br><br><h1>  Data collection </h1><br>  The accuracy of the deep learning model largely depends on the quality of the data.  We must strive to collect an extensive set of data, as in production. <br><br>  Our model should be able to recognize punches and kicks.  This means that we must collect images of three categories: <br><br><ul><li>  Hand punches </li><li>  Foot kicks </li><li>  Other </li></ul><br>  In this experiment, two volunteers helped me to collect photos ( <a href="https://twitter.com/lili_vs">@lili_vs</a> and <a href="https://twitter.com/gsamokovarov">@gsamokovarov</a> ).  We recorded 5 QuickTime videos on my MacBook Pro, each containing 2‚Äì4 punches and 2‚Äì4 kicks. <br><br>  Then we use ffmpeg to extract individual frames from videos and save them as <code>jpg</code> images: <br><br> <code>ffmpeg -i video.mov $filename%03d.jpg</code> <br> <br>  To run the above command, you first need to <a href="https://www.ffmpeg.org/download.html">install</a> <code>ffmpeg</code> on your computer. <br><br>  If we want to train a model, we must provide input data and corresponding output data, but at this stage we only have a bunch of images of three people in different poses.  To structure the data, you need to classify frames in three categories: punches, kicks, and others.  For each category, a separate directory is created where all relevant images are moved. <br><br>  Thus, each directory should have about 200 images similar to the ones below: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/798/e9a/908/798e9a9083a1f5dfa5811fbb7de3bcc9.jpg"><br><br>  Please note that there will be much more images in the Others catalog, because relatively few frames contain photos of punches and kicks, while in the remaining frames people walk, turn around or control the video.  If we have too many images of one class, then we risk teaching a model biased to this particular class.  In this case, when classifying an image with a blow, the neural network can still determine the class ‚ÄúOthers‚Äù.  To reduce this bias, you can remove some photos from the Others directory and train the model in an equal number of images from each category. <br><br>  For convenience, we assign images in catalogs numbers from <code>1</code> to <code>190</code> , so the first image will be <code>1.jpg</code> , the second <code>2.jpg</code> , etc. <br><br>  If we train the model only on 600 photographs taken in the same environment with the same people, we will not reach a very high level of accuracy.  To get the most out of our data, it‚Äôs better to generate a few extra samples using data augmentation. <br><br><h1>  Data augmentation </h1><br>  Data augmentation is a technique that increases the number of data points by synthesizing new points from an existing set.  Usually augmentation is used to increase the size and variety of the training set.  We transfer the original images to the transformation pipeline that creates new images.  You can not be too aggressive approach to transformation: from a punch should be generated only other punches. <br><br>  Acceptable transformations are rotation, color inversion, blur, etc. There are excellent open source tools for augmentation of data.  At the time of writing the article on JavaScript there were not too many options, so I used the library implemented in Python - <a href="https://github.com/aleju/imgaug">imgaug</a> .  It has a set of augmenters that can be applied probabilistically. <br><br>  Here is the data augmentation logic for this experiment: <br><br><pre> <code class="python hljs">np.random.seed(<span class="hljs-number"><span class="hljs-number">44</span></span>) ia.seed(<span class="hljs-number"><span class="hljs-number">44</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">main</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">191</span></span>): draw_single_sequential_images(str(i), <span class="hljs-string"><span class="hljs-string">"others"</span></span>, <span class="hljs-string"><span class="hljs-string">"others-aug"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">191</span></span>): draw_single_sequential_images(str(i), <span class="hljs-string"><span class="hljs-string">"hits"</span></span>, <span class="hljs-string"><span class="hljs-string">"hits-aug"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">191</span></span>): draw_single_sequential_images(str(i), <span class="hljs-string"><span class="hljs-string">"kicks"</span></span>, <span class="hljs-string"><span class="hljs-string">"kicks-aug"</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">draw_single_sequential_images</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename, path, aug_path)</span></span></span><span class="hljs-function">:</span></span> image = misc.imresize(ndimage.imread(path + <span class="hljs-string"><span class="hljs-string">"/"</span></span> + filename + <span class="hljs-string"><span class="hljs-string">".jpg"</span></span>), (<span class="hljs-number"><span class="hljs-number">56</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>)) sometimes = <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> aug: iaa.Sometimes(<span class="hljs-number"><span class="hljs-number">0.5</span></span>, aug) seq = iaa.Sequential( [ iaa.Fliplr(<span class="hljs-number"><span class="hljs-number">0.5</span></span>), <span class="hljs-comment"><span class="hljs-comment"># horizontally flip 50% of all images # crop images by -5% to 10% of their height/width sometimes(iaa.CropAndPad( percent=(-0.05, 0.1), pad_mode=ia.ALL, pad_cval=(0, 255) )), sometimes(iaa.Affine( scale={"x": (0.8, 1.2), "y": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis translate_percent={"x": (-0.1, 0.1), "y": (-0.1, 0.1)}, # translate by -10 to +10 percent (per axis) rotate=(-5, 5), shear=(-5, 5), # shear by -5 to +5 degrees order=[0, 1], # use nearest neighbour or bilinear interpolation (fast) cval=(0, 255), # if mode is constant, use a cval between 0 and 255 mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples) )), iaa.Grayscale(alpha=(0.0, 1.0)), iaa.Invert(0.05, per_channel=False), # invert color channels # execute 0 to 5 of the following (less important) augmenters per image # don't execute all of them, as that would often be way too strong iaa.SomeOf((0, 5), [ iaa.OneOf([ iaa.GaussianBlur((0, 2.0)), # blur images with a sigma between 0 and 2.0 iaa.AverageBlur(k=(2, 5)), # blur image using local means with kernel sizes between 2 and 5 iaa.MedianBlur(k=(3, 5)), # blur image using local medians with kernel sizes between 3 and 5 ]), iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.01*255), per_channel=0.5), # add gaussian noise to images iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value) iaa.AddToHueAndSaturation((-20, 20)), # change hue and saturation # either change the brightness of the whole image (sometimes # per channel) or change the brightness of subareas iaa.OneOf([ iaa.Multiply((0.9, 1.1), per_channel=0.5), iaa.FrequencyNoiseAlpha( exponent=(-2, 0), first=iaa.Multiply((0.9, 1.1), per_channel=True), second=iaa.ContrastNormalization((0.9, 1.1)) ) ]), iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast ], random_order=True ) ], random_order=True ) im = np.zeros((16, 56, 100, 3), dtype=np.uint8) for c in range(0, 16): im[c] = image for im in range(len(grid)): misc.imsave(aug_path + "/" + filename + "_" + str(im) + ".jpg", grid[im])</span></span></code> </pre> <br>  This script uses the <code>main</code> method with three <code>for</code> loops ‚Äî one for each category of images.  In each iteration, in each of the cycles, we call the <code>draw_single_sequential_images</code> method: the first argument is the file name, the second is the path, and the third is the directory where to save the result. <br><br>  After that, we read the image from the disk and apply a number of transformations to it.  I have documented most of the transformations in the above code snippet, so we will not repeat. <br><br>  For each image creates 16 other images.  Here is an example of what they look like: <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/759/ad9/43d/759ad943d7aa07dbccee4a6f26a1d920.jpg"></a> <br><br>  Please note that in the above script, we scale images up to <code>100x56</code> pixels.  We do this to reduce the amount of data and, accordingly, the number of calculations that our model performs during training and evaluation. <br><br><h1>  Model building </h1><br>  Now we will build a model for classification! <br><br>  Since we are dealing with images, we use the convolutional neural network (CNN).  This network architecture is known to be suitable for image recognition, object detection and classification. <br><br><h3>  Transfer training </h3><br>  The image below shows the popular CNN VGG-16 used to classify images. <br><br><img src="https://habrastorage.org/webt/7t/0u/zk/7t0uzk4kdf4pbesgvlojn5nal18.png"><br><br>  The neural VGG-16 recognizes 1000 image classes.  It has 16 layers (not counting the pooling layers and the output).  This multi-layer network is difficult to train in practice.  This will require a large data set and many hours of training. <br><br>  The hidden layers of the trained CNN recognize the various elements of the images from the training set, starting at the edges, moving on to more complex elements, such as figures, individual objects, and so on.  A trained CNN-style VGG-16 for recognizing a large set of images should have hidden layers that have learned many features from the training set.  Such signs will be common to most images and, accordingly, be reused for different tasks. <br><br>  Transfer learning allows you to reuse an existing and trained network.  We can take the output from any of the layers of the existing network and transmit it as input to the new neural network.  Thus, by teaching the newly created neural network, over time it can be taught to recognize new features of a higher level and correctly classify images from classes that the original model has never seen before. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7n/cc/a7/7ncca7e5ne2ammearn2sqnk4by0.png"></div><br><br>  For our purposes, let's take the MobileNet neural network from the <a href="https://www.npmjs.com/package/%40tensorflow-models/mobilenet">@ tensorflow-models / mobilenet package</a> .  MobileNet is just as powerful as VGG-16, but it is much smaller, which speeds up forward propagation, that is, network activation (forward propagation), and reduces download time in the browser.  MobileNet trained on a data set for image classification <a href="http://www.image-net.org/challenges/LSVRC/2012/">ILSVRC-2012-CLS</a> . <br><br>  When developing a model with a transfer of training, we have choices for two points: <br><br><ol><li>  The output from which layer of the source model to use as input for the target model. </li><li>  How many layers of the target model are we going to train, if any. </li></ol><br>  The first moment is very significant.  Depending on the selected layer, we will get the signs at a lower or higher level of abstraction as input for our neural network. <br><br>  We are not going to train any layers of MobileNet.  Select the output from <code>global_average_pooling2d_1</code> and pass it as input to our tiny model.  Why did I choose this particular layer?  Empirically.  I did some tests and this layer works quite well. <br><br><h3>  Model definition </h3><br>  The initial task was to classify the image into three classes: hand, foot and other movements.  Let's first solve the problem of a smaller one: determine whether there is a punch in the frame or not.  This is a typical binary classification task.  For this purpose, we can define the following model: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> <span class="hljs-string"><span class="hljs-string">'@tensorflow/tfjs'</span></span>; const model = tf.sequential(); model.add(tf.layers.inputLayer({ inputShape: [<span class="hljs-number"><span class="hljs-number">1024</span></span>] })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">1024</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'relu'</span></span> })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">1</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span> })); model.compile({ optimizer: tf.train.adam(<span class="hljs-number"><span class="hljs-number">1e-6</span></span>), loss: tf.losses.sigmoidCrossEntropy, metrics: [<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>] });</code> </pre> <br>  This code defines a simple model, a layer with <code>1024</code> units and <code>ReLU</code> activation, as well as one output unit that passes through the <code>sigmoid</code> activation <code>sigmoid</code> .  The latter gives a number from <code>0</code> to <code>1</code> , depending on the probability of the presence of a hand strike in a given frame. <br><br>  Why did I choose <code>1024</code> units for the second level and learning speed <code>1e-6</code> ?  Well, I tried several different options and saw that such parameters work best.  The ‚Äúspear method‚Äù does not seem to be the best approach, but to a large extent this is exactly how setting up hyper parameters in deep learning - based on our understanding of the model, we use intuition to update orthogonal parameters and empirically check how the model works. <br><br>  The <code>compile</code> method compiles the layers together, preparing a model for learning and evaluation.  Here we announce that we want to use the <code>adam</code> optimization algorithm.  We also declare that we will calculate the loss (loss) from the cross entropy, and indicate that we want to evaluate the accuracy of the model.  Then TensorFlow.js calculates the accuracy by the formula: <br><br> <code>Accuracy = (True Positives + True Negatives) / (Positives + Negatives)</code> <br> <br>  If you transfer training from the original MobileNet model, you first need to download it.  Since it is impractical to train our model on more than 3000 images in the browser, we will apply Node.js and load the neural network from the file. <br><br>  Download MobileNet <a href="https://github.com/mgechev/mk-tfjs/tree/master/mobile-net">here</a> .  The catalog contains the <code>model.json</code> file, which contains the model architecture - layers, activations, etc.  The remaining files contain the parameters of the model.  You can load a model from a file using this code: <br><br><pre> <code class="python hljs">export const loadModel = <span class="hljs-keyword"><span class="hljs-keyword">async</span></span> () =&gt; { const mn = new mobilenet.MobileNet(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>); mn.path = `file://PATH/TO/model.json`; <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> mn.load(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (input): tf.Tensor1D =&gt; mn.infer(input, <span class="hljs-string"><span class="hljs-string">'global_average_pooling2d_1'</span></span>) .reshape([<span class="hljs-number"><span class="hljs-number">1024</span></span>]); };</code> </pre> <br>  Note that in the <code>loadModel</code> method <code>loadModel</code> we return a function that takes a one-dimensional tensor as input and returns <code>mn.infer(input, Layer)</code> .  The <code>infer</code> method takes a tensor and a layer as arguments.  The layer determines which hidden layer we want to get the output from.  If you open <a href="">model.json</a> and <code>global_average_pooling2d_1</code> , you will find this name in one of the layers. <br><br>  Now you need to create a data set for learning the model.  To do this, we have to skip all the images through the <code>infer</code> method in MobileNet and assign them tags: <code>1</code> for images with strokes and <code>0</code> for images without impact: <br><br><pre> <code class="python hljs">const punches = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Punches) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Punches}/${f}`); const others = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Others) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Others}/${f}`); const ys = tf.tensor1d( new Array(punches.length).fill(<span class="hljs-number"><span class="hljs-number">1</span></span>) .concat(new Array(others.length).fill(<span class="hljs-number"><span class="hljs-number">0</span></span>))); const xs: tf.Tensor2D = tf.stack( punches .map((path: string) =&gt; mobileNet(readInput(path))) .concat(others.map((path: string) =&gt; mobileNet(readInput(path)))) ) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor2D;</code> </pre> <br>  In the code above, we first read the files in the directories with and without hitting.  Then we determine the one-dimensional tensor containing the output labels.  If we have <code>n</code> images with hits and <code>m</code> other images, the tensor will have <code>n</code> elements with a value of 1 and <code>m</code> elements with a value of 0. <br><br>  In <code>xs</code> we add the results of calling the <code>infer</code> method for individual images.  Notice that for each image we call the <code>readInput</code> method.  Here is its implementation: <br><br><pre> <code class="python hljs">export const readInput = img =&gt; imageToInput(readImage(img), TotalChannels); const readImage = path =&gt; jpeg.decode(fs.readFileSync(path), true); const imageToInput = image =&gt; { const values = serializeImage(image); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> tf.tensor3d(values, [image.height, image.width, <span class="hljs-number"><span class="hljs-number">3</span></span>], <span class="hljs-string"><span class="hljs-string">'int32'</span></span>); }; const serializeImage = image =&gt; { const totalPixels = image.width * image.height; const result = new Int32Array(totalPixels * <span class="hljs-number"><span class="hljs-number">3</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (let i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; totalPixels; i++) { result[i * <span class="hljs-number"><span class="hljs-number">3</span></span> + <span class="hljs-number"><span class="hljs-number">0</span></span>] = image.data[i * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">0</span></span>]; result[i * <span class="hljs-number"><span class="hljs-number">3</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>] = image.data[i * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>]; result[i * <span class="hljs-number"><span class="hljs-number">3</span></span> + <span class="hljs-number"><span class="hljs-number">2</span></span>] = image.data[i * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">2</span></span>]; } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> result; };</code> </pre> <br>  <code>readInput</code> first calls the <code>readImage</code> function, and then delegates its <code>imageToInput</code> call.  The <code>readImage</code> function reads the image from the disk and then decodes the jpg from the buffer using the <a href="https://www.npmjs.com/package/jpeg-js">jpeg-js</a> package.  In <code>imageToInput</code> we transform the image into a three-dimensional tensor. <br><br>  As a result, for each <code>i</code> from <code>0</code> to <code>TotalImages</code> , <code>ys[i]</code> should be equal to <code>1</code> if <code>xs[i]</code> corresponds to the image with a stroke, and <code>0</code> otherwise. <br><br><h1>  Model training </h1><br>  Now the model is ready to learn!  Call the <code>fit</code> method: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">await</span></span> model.fit(xs, ys, { epochs: Epochs, batchSize: parseInt(((punches.length + others.length) * BatchSize).toFixed(<span class="hljs-number"><span class="hljs-number">0</span></span>)), callbacks: { onBatchEnd: <span class="hljs-keyword"><span class="hljs-keyword">async</span></span> (_, logs) =&gt; { console.log(<span class="hljs-string"><span class="hljs-string">'Cost: %s, accuracy: %s'</span></span>, logs.loss.toFixed(<span class="hljs-number"><span class="hljs-number">5</span></span>), logs.acc.toFixed(<span class="hljs-number"><span class="hljs-number">5</span></span>)); <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> tf.nextFrame(); } } });</code> </pre> <br>  The above code calls <code>fit</code> with three arguments: <code>xs</code> , ys and a configuration object.  In the configuration object, we set how many epochs the model will study, the packet size, and the callback that TensorFlow.js will generate after processing each packet. <br><br>  The size of the package determines the <code>xs</code> and <code>ys</code> for training the model for the same epoch.  For each epoch, TensorFlow.js will select a subset of <code>xs</code> and the corresponding elements from <code>ys</code> , perform a direct distribution, obtain the output of the layer with <code>sigmoid</code> activation, and then, based on the loss, perform optimization using the <code>adam</code> algorithm. <br><br>  After running the training script, you will see a result similar to the one below: <br><br><pre>  Cost: 0.84212, accuracy: 1.00000
 eta = 0.3&gt; ---------- acc = 1.00 loss = 0.84 Cost: 0.79740, accuracy: 1.00000
 eta = 0.2 =&gt; --------- acc = 1.00 loss = 0.80 Cost: 0.81533, accuracy: 1.00000
 eta = 0.2 ==&gt; -------- acc = 1.00 loss = 0.82 Cost: 0.64303, accuracy: 0.50000
 eta = 0.2 ===&gt; ------- acc = 0.50 loss = 0.64 Cost: 0.51377, accuracy: 0.00000
 eta = 0.2 ====&gt; ------ acc = 0.00 loss = 0.51 Cost: 0.46473, accuracy: 0.50000
 eta = 0.1 =====&gt; ----- acc = 0.50 loss = 0.46 Cost: 0.50872, accuracy: 0.00000
 eta = 0.1 ======&gt; ---- acc = 0.00 loss = 0.51 Cost: 0.62556, accuracy: 1.00000
 eta = 0.1 =======&gt; --- acc = 1.00 loss = 0.63 Cost: 0.65133, accuracy: 0.50000
 eta = 0.1 ========&gt; - acc = 0.50 loss = 0.65 Cost: 0.63824, accuracy: 0.50000
 eta = 0.0 ==========&gt;
 293ms 14675us / step - acc = 0.60 loss = 0.65
 Epoch 3/50
 Cost: 0.44661, accuracy: 1.00000
 eta = 0.3&gt; ---------- acc = 1.00 loss = 0.45 Cost: 0.78060, accuracy: 1.00000
 eta = 0.3 =&gt; --------- acc = 1.00 loss = 0.78 Cost: 0.79208, accuracy: 1.00000
 eta = 0.3 ==&gt; -------- acc = 1.00 loss = 0.79 Cost: 0.49072, accuracy: 0.50000
 eta = 0.2 ===&gt; ------- acc = 0.50 loss = 0.49 Cost: 0.62232, accuracy: 1.00000
 eta = 0.2 ====&gt; ------ acc = 1.00 loss = 0.62 Cost: 0.82899, accuracy: 1.00000
 eta = 0.2 =====&gt; ----- acc = 1.00 loss = 0.83 Cost: 0.67629, accuracy: 0.50000
 eta = 0.1 ======&gt; ---- acc = 0.50 loss = 0.68 Cost: 0.62621, accuracy: 0.50000
 eta = 0.1 =======&gt; --- acc = 0.50 loss = 0.63 Cost: 0.46077, accuracy: 1.00000
 eta = 0.1 ========&gt; - acc = 1.00 loss = 0.46 Cost: 0.62076, accuracy: 1.00000
 eta = 0.0 ==========&gt;
 304ms 15221us / step - acc = 0.85 loss = 0.63 </pre><br>  Notice how accuracy increases over time, and loss decreases. <br><br>  On my data set, the model after training showed an accuracy of 92%.  Keep in mind that accuracy may not be very high due to the small set of training data. <br><br><h1>  Running the model in a browser </h1><br>  In the previous section, we trained the binary classification model.  Now run it in the browser and connect it to the game <a href="">MK.js</a> ! <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> video = <span class="hljs-built_in"><span class="hljs-built_in">document</span></span>.getElementById(<span class="hljs-string"><span class="hljs-string">'cam'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> Layer = <span class="hljs-string"><span class="hljs-string">'global_average_pooling2d_1'</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> mobilenetInfer = <span class="hljs-function"><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">m</span></span></span><span class="hljs-function"> =&gt;</span></span> (p): tf.Tensor&lt;tf.Rank&gt; =&gt; m.infer(p, Layer); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> canvas = <span class="hljs-built_in"><span class="hljs-built_in">document</span></span>.getElementById(<span class="hljs-string"><span class="hljs-string">'canvas'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> scale = <span class="hljs-built_in"><span class="hljs-built_in">document</span></span>.getElementById(<span class="hljs-string"><span class="hljs-string">'crop'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> ImageSize = { <span class="hljs-attr"><span class="hljs-attr">Width</span></span>: <span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-attr"><span class="hljs-attr">Height</span></span>: <span class="hljs-number"><span class="hljs-number">56</span></span> }; navigator.mediaDevices .getUserMedia({ <span class="hljs-attr"><span class="hljs-attr">video</span></span>: <span class="hljs-literal"><span class="hljs-literal">true</span></span>, <span class="hljs-attr"><span class="hljs-attr">audio</span></span>: <span class="hljs-literal"><span class="hljs-literal">false</span></span> }) .then(<span class="hljs-function"><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">stream</span></span></span><span class="hljs-function"> =&gt;</span></span> { video.srcObject = stream; });</code> </pre> <br>  In the given code there are several declarations: <br><br><ul><li>  <code>video</code> contains a link to the <code>HTML5 video</code> element on the page. </li><li> <code>Layer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> contains the name of the layer from MobileNet, from which we want to get the output data and transfer them as input data for our model </font></font></li><li> <code>mobilenetInfer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- A function that accepts a MobileNet instance and returns another function. </font><font style="vertical-align: inherit;">The return function takes the input data and returns the corresponding output from the specified MobileNet layer.</font></font></li><li> <code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">points to the element </font></font><code>HTML5 canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">we will use to extract frames from the video</font></font></li><li> <code>scale</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- another one </font></font><code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">that is used to scale individual frames</font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">After that we get the video stream from the user's camera and set it as the source for the item </font></font><code>video</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The next step is to implement a grayscale filter that accepts </font></font><code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and converts its contents:</font></font><br><br><pre> <code class="python hljs">const grayscale = (canvas: HTMLCanvasElement) =&gt; { const imageData = canvas.getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>).getImageData(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, canvas.width, canvas.height); const data = imageData.data; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (let i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; data.length; i += <span class="hljs-number"><span class="hljs-number">4</span></span>) { const avg = (data[i] + data[i + <span class="hljs-number"><span class="hljs-number">1</span></span>] + data[i + <span class="hljs-number"><span class="hljs-number">2</span></span>]) / <span class="hljs-number"><span class="hljs-number">3</span></span>; data[i] = avg; data[i + <span class="hljs-number"><span class="hljs-number">1</span></span>] = avg; data[i + <span class="hljs-number"><span class="hljs-number">2</span></span>] = avg; } canvas.getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>).putImageData(imageData, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>); };</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> As a next step, let's link the model with MK.js: </font></font><br><br><pre> <code class="python hljs">let mobilenet: (p: any) =&gt; tf.Tensor&lt;tf.Rank&gt;; tf.loadModel(<span class="hljs-string"><span class="hljs-string">'http://localhost:5000/model.json'</span></span>).then(model =&gt; { mobileNet .load() .then((mn: any) =&gt; mobilenet = mobilenetInfer(mn)) .then(startInterval(mobilenet, model)); });</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the code above, we first load the model that was trained above, and then load MobileNet. Pass MobileNet to the method </font></font><code>mobilenetInfer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">to get the path to calculate the output from the hidden network layer. After that we call the method </font></font><code>startInterval</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">with two networks as arguments.</font></font><br><br><pre> <code class="python hljs">const startInterval = (mobilenet, model) =&gt; () =&gt; { setInterval(() =&gt; { canvas.getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>).drawImage(video, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>); grayscale(scale .getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>) .drawImage( canvas, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, canvas.width, canvas.width / (ImageSize.Width / ImageSize.Height), <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, ImageSize.Width, ImageSize.Height )); const [punching] = Array.<span class="hljs-keyword"><span class="hljs-keyword">from</span></span>(( model.predict(mobilenet(tf.fromPixels(scale))) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor1D) .dataSync() <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> Float32Array); const detect = (window <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> any).Detect; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (punching &gt;= <span class="hljs-number"><span class="hljs-number">0.4</span></span>) detect &amp;&amp; detect.onPunch(); }, <span class="hljs-number"><span class="hljs-number">100</span></span>); };</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The most interesting begins in the method </font></font><code>startInterval</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">! First, we start the interval, where every </font></font><code>100ms</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">call an anonymous function. It first </font></font><code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">renders the video with the current frame. Then we reduce the frame size before </font></font><code>100x56</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and apply a gray </font><font style="vertical-align: inherit;">scale </font><font style="vertical-align: inherit;">filter to it. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The next step is to transfer the frame to MobileNet, get the output from the desired hidden layer and transfer it as input to the method of </font></font><code>predict</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">our model. That returns a tensor with one element. With the help </font></font><code>dataSync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">we get the value from the tensor and assign it to a constant </font></font><code>punching</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Finally, we check: if the probability of a hand strike exceeds </font></font><code>0.4</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, then we call the </font></font><code>onPunch</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">global object </font><font style="vertical-align: inherit;">method </font></font><code>Detect</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. MK.js provides a global object with three methods:</font></font><code>onKick</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>onPunch</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and </font></font><code>onStand</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">that we can use to control one of the characters.</font></font><br><br>  Done!<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Here is the result! </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/83e/05c/e0e/83e05ce0e9304865bb6aee072204902b.gif"><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Recognition of kicking and kicking with N-striking classification </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the next section, we will make a smarter model: a neural network that recognizes punches, kicks, and other images. </font><font style="vertical-align: inherit;">This time we start with the preparation of the training set:</font></font><br><br><pre> <code class="python hljs">const punches = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Punches) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Punches}/${f}`); const kicks = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Kicks) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Kicks}/${f}`); const others = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Others) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Others}/${f}`); const ys = tf.tensor2d( new Array(punches.length) .fill([<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>]) .concat(new Array(kicks.length).fill([<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>])) .concat(new Array(others.length).fill([<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>])), [punches.length + kicks.length + others.length, <span class="hljs-number"><span class="hljs-number">3</span></span>] ); const xs: tf.Tensor2D = tf.stack( punches .map((path: string) =&gt; mobileNet(readInput(path))) .concat(kicks.map((path: string) =&gt; mobileNet(readInput(path)))) .concat(others.map((path: string) =&gt; mobileNet(readInput(path)))) ) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor2D;</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As before, we first read the catalogs with the images of punches by hand, foot and other images. After that, unlike last time, we form the expected result in the form of a two-dimensional tensor, but not a one-dimensional one. If we have </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">n</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> pictures with a punch, </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> pictures with a kick and </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> other images, then in the tensor there </font></font><code>ys</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">will be </font></font><code>n</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">elements with a value </font></font><code>[1, 0, 0]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>m</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">elements with a value, </font></font><code>[0, 1, 0]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and </font></font><code>k</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">elements with a value </font></font><code>[0, 0, 1]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A vector of </font></font><code>n</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">elements, in which there are </font></font><code>n - 1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">elements with a value </font></font><code>0</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and one element with a value </font></font><code>1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, we call a unitary vector (one-hot vector). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">After that we form the input tensor</font></font><code>xs</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">by folding the output of each image from MobileNet. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There will have to update the definition of the model:</font></font><br><br><pre> <code class="python hljs">const model = tf.sequential(); model.add(tf.layers.inputLayer({ inputShape: [<span class="hljs-number"><span class="hljs-number">1024</span></span>] })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">1024</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'relu'</span></span> })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">3</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'softmax'</span></span> })); <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> model.compile({ optimizer: tf.train.adam(<span class="hljs-number"><span class="hljs-number">1e-6</span></span>), loss: tf.losses.sigmoidCrossEntropy, metrics: [<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>] });</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The only two differences from the previous model are: </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The number of units in the output layer </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Activations in the output layer </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> There are three units in the output layer, because we have three different categories of images: </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Punch </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Kick </font></font></li><li>  Other </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Activation is triggered on these three units </font></font><code>softmax</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, which converts their parameters to a three-value tensor. Why three units for the output layer? Each of the three values for three classes can be represented by two bits: </font></font><code>00</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>01</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>10</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. The sum of the values ‚Äã‚Äãof the tensor created </font></font><code>softmax</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">is 1, that is, we will never get 00, so we cannot classify the images of one of the classes. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">After learning the model over the </font></font><code>500</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ages, I achieved an accuracy of about 92%! This is not bad, but do not forget that the training was conducted on a small set of data. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The next step is to launch the model in the browser! Since the logic is very similar to running the model for binary classification, take a look at the last step, where an action is chosen based on the model output:</font></font><br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> [punch, kick, nothing] = <span class="hljs-built_in"><span class="hljs-built_in">Array</span></span>.from((model.predict( mobilenet(tf.fromPixels(scaled)) ) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor1D).dataSync() <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> <span class="hljs-built_in"><span class="hljs-built_in">Float32Array</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> detect = (<span class="hljs-built_in"><span class="hljs-built_in">window</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> any).Detect; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (nothing &gt;= <span class="hljs-number"><span class="hljs-number">0.4</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (kick &gt; punch &amp;&amp; kick &gt;= <span class="hljs-number"><span class="hljs-number">0.35</span></span>) { detect.onKick(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span>; } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (punch &gt; kick &amp;&amp; punch &gt;= <span class="hljs-number"><span class="hljs-number">0.35</span></span>) detect.onPunch();</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">First we call MobileNet with a reduced frame in shades of gray, then we transfer the result of our trained model. </font><font style="vertical-align: inherit;">The model returns a one-dimensional tensor, which we convert to </font></font><code>Float32Array</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s </font></font><code>dataSync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">In the next step, we use </font></font><code>Array.from</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">to cast a typed array to a JavaScript array. </font><font style="vertical-align: inherit;">Then we extract the probabilities that there is a punch, a kick or nothing on the frame. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If the probability of the third result exceeds </font></font><code>0.4</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, return. </font><font style="vertical-align: inherit;">Otherwise, if the probability of a kick is higher </font></font><code>0.32</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, send a kick command to MK.js. </font><font style="vertical-align: inherit;">If the probability of hitting with your hand is higher </font></font><code>0.32</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and higher than the probability of hitting with your foot, then send the action of hitting with your hand. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In general, that's all! </font><font style="vertical-align: inherit;">The result is shown below:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/168/f71/f3d/168f71f3df8d267bec3e0791d5857c64.gif"><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Action recognition </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If you collect a large and diverse set of data about people who hit with arms and legs, then you can build a model that works perfectly on individual frames. But is that enough? What if we want to go even further and distinguish two different types of kicks: with a turn and from the back (back kick). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As can be seen on the frames below, at a certain point in time from a certain angle, both hits look the same: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/6c1/567/5bf/6c15675bf7b8c238e7ce9d5aaefeea80.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/a60/e3c/dba/a60e3cdba0eb3ecbc8730c39bc6c95b2.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But if you look at the execution, the movements are completely different: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/e72/28b/fe8/e7228bfe8cfe9bbe73f9011d94778a7a.gif"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">How can you teach the neural network to analyze the sequence of frames, and not just one frame? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">For this purpose, we can explore another class of neural networks, called recurrent neural networks (RNN). For example, RNN is great for working with time series:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Natural language processing (NLP), where each word depends on the preceding and following </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Predict the next page based on your browsing history </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Recognizing actions in a sequence of frames </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Implementing such a model is beyond the scope of this article, but let's look at an example of architecture in order to get an idea of ‚Äã‚Äãhow all this will work together. </font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Might RNN </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The diagram below shows the recognition model of actions: </font></font><br><br><img src="https://habrastorage.org/webt/kz/oq/ie/kzoqieod8t9nhs_taapnhpr_y0c.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Take the last </font></font><code>n</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">frames from the video and transfer them to CNN. </font><font style="vertical-align: inherit;">The CNN output for each frame is passed as input to the RNN. </font><font style="vertical-align: inherit;">The recurrent neural network will determine the dependencies between the individual frames and recognize which action they correspond to.</font></font><br><br><h1>  Conclusion </h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In this article, we developed an image classification model. For this purpose, we collected a data set: extracted video frames and manually divided them into three categories. Then augmented the data by adding images using </font></font><a href="https://github.com/aleju/imgaug"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">imgaug</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">After that, we explained what training transfer is and used the trained MobileNet model from the </font></font><a href="https://www.npmjs.com/package/%40tensorflow-models/mobilenet"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">@ tensorflow-models / mobilenet</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> package for our own purposes </font><font style="vertical-align: inherit;">. We loaded MobileNet from a file in the Node.js process and trained an extra dense layer where data was fed from the hidden MobileNet layer. After training, we have reached an accuracy of more than 90%! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To use this model in the browser, we downloaded it together with MobileNet and ran a frame categorization from the user's webcam every 100 ms. We connected the model with the game.</font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MK.js</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and used the output of the model to control one of the characters. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Finally, we looked at how to improve the model by combining it with a recurrent neural network to recognize actions. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I hope you enjoyed this tiny project as much as I did! </font></font></div><p>Source: <a href="https://habr.com/ru/post/428019/">https://habr.com/ru/post/428019/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../428003/index.html">Responsive design: preserving the shape of markup elements</a></li>
<li><a href="../428005/index.html">Three effective ways to aggravate PR disaster</a></li>
<li><a href="../428007/index.html">Already not luggable pc, not yet notebook: Laptop TOSHIBA T3100 / 20</a></li>
<li><a href="../428009/index.html">Equifax: one year after the largest data breach</a></li>
<li><a href="../428011/index.html">Space Zombie Songs</a></li>
<li><a href="../428021/index.html">Seals against neural networks. Or choose and run the neural network to recognize objects on the Raspberry Zero</a></li>
<li><a href="../428023/index.html">Basics of electrical safety in the design of electronic devices</a></li>
<li><a href="../428025/index.html">Connecting the paging file (SWAP) in MAC OS X when using an external SSD-drive as a system</a></li>
<li><a href="../428027/index.html">How I tried to make a static analyzer GLSL (and what went wrong)</a></li>
<li><a href="../428029/index.html">Digital events in Moscow from October 29 to November 4</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>