<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>SSD caching implementation in QSAN XCubeSAN storage</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Productivity technologies based on the use of SSD and widely used in storage systems, have long been invented. First of all, it is the use of SSD as s...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>SSD caching implementation in QSAN XCubeSAN storage</h1><div class="post__text post__text-html js-mediator-article"><p>  Productivity technologies based on the use of SSD and widely used in storage systems, have long been invented.  First of all, it is the use of SSD as storage space, which is 100% efficient, but expensive.  Therefore, in the course are the technology of tiring and caching, where SSD is used only for the most popular ("hot") data.  Tiring is good for long-term (day-to-week) usage scenarios of ‚Äúhot‚Äù data.  And caching, on the contrary, for short-term (minutes-hours) use.  Both of these options are implemented in the <a href="http://www.qsan.su/goods/sistemyi-xraneniya-dannyix/">QSAN XCubeSAN</a> storage <a href="http://www.qsan.su/goods/sistemyi-xraneniya-dannyix/">system</a> .  In this article we will look at the implementation of the second algorithm - <a href="http://www.qsan.su/soft/san-os-4.0/ssd-keshirovanie-(qcache-2.0).html">SSD caching</a> . </p><br><div style="text-align:center;"><img width="250" height="190" src="https://habrastorage.org/webt/f2/5d/si/f25dsiefejvpnm2e4ynez9nksso.png"></div><br><a name="habracut"></a><br><p>  The essence of SSD caching technology is the use of SSD as an intermediate cache between hard drives and controller RAM.  SSD performance, of course, is lower than the performance of the controller's own cache, but the volume is much higher.  Therefore, we get some compromise between speed and volume. </p><br><p>  <b>Indications for using SSD cache for reading:</b> </p><br><ul><li>  The predominance of read operations on write operations (most often characteristic of databases and web applications); </li><li>  The presence of a bottleneck in the form of the performance of an array of hard drives; </li><li>  The volume of the requested data is less than the SSD cache. </li></ul><br><p>  The indications for using the SSD cache for reading + writing are the same reasons, except for the nature of the operations - a mixed type (for example, a file server). </p><br><p>  Most storage vendors use read-only SSD caches in their products.  The principal difference in <a href="http://www.qsan.su/">QSAN</a> from them is the ability to use the cache also for writing.  To activate the SSD caching functionality in the QSAN storage system, a separate license is required (delivered in electronic form). </p><br><p>  SSD cache in XCubeSAN is physically implemented as separate SSD cache pools.  There can be up to four of them in the system.  Each pool, of course, uses its own set of SSDs.  And already in the properties of the virtual disk, we determine whether it will use the cache pool and which one.  Enabling and disabling cache usage for volumes can be performed online without stopping I / O.  Also on the "hot" SSD can be added to the pool and remove them from there.  When creating a pool of SSD caches, you must choose in which mode it will work: read only or read + write.  His physical organization depends on it.  Once the cache of pools can be several, then the functionality of them can be different (that is, the system can have cache pools for reading and reading + writing at the same time). </p><br><p>  In the case of using a read-only pool cache, it can consist of 1-8 SSDs.  The disks do not have to be of the same size and one vendor, since they are combined into the NRAID + structure.  All SSDs in the pool are shared.  The system independently tries to parallelize incoming requests between all SSDs for maximum performance.  In case of failure of one of the SSDs, nothing terrible will happen: after all, the cache contains only a copy of the data stored on the array of hard drives.  Just the amount of available SSD cache will decrease (or become zero if you use the original SSD cache from one drive). </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/jc/io/jr/jciojrvarhuem3cug2ws-paj7ag.png"></div><br><p>  If the cache is used for read + write operations, then the number of SSDs in the pool should be a multiple of two, since the contents are mirrored on pairs of drives (the NRAID 1+ structure is used).  Duplication of the cache is necessary due to the fact that it may contain data that have not yet had time to register on hard drives.  In this case, the failure of the SSD from the cache pool would lead to loss of information.  In the case of NRAID 1+, the failure of the SSD will simply lead to the transfer of the cache to the read-only state, with the dumping of unwritten data onto an array of hard drives.  After replacing the faulty SSD, the cache will return to its original mode of operation.  By the way, for more security, a cache working for read + write can be assigned dedicated hot spare. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/od/af/ih/odafih8vgcqhofajne-vkusvkcq.png"></div><br><p>  When using the SSD caching feature in XCubeSAN, there are a number of storage controller memory requirements: the more system memory, the larger the cache pool will be available. </p><br><p></p><div style="text-align:center;"><img width="400" src="https://habrastorage.org/webt/i2/wz/gl/i2wzglxirrpm42btflq07g4gp8q.png"></div><br><p>  In contrast, again from most storage vendors, which offer only the enable / disable option for configuring the SSD cache, QSAN provides more options.  In particular, you can select the cache mode depending on the nature of the load.  There are three preset templates that are closest in their work to the corresponding services: a database, a file system, a web service.  In addition, the administrator can create his own profile by setting the required parameter values: </p><br><table><tbody><tr><td><p></p><ul><li>  Cache Block Size - 1/2/4 MB </li><li>  The number of block read requests to be copied to the cache (Populate-on-Read Threshold) is 1..4 </li><li>  The number of requests to write a block so that it was copied to the cache (Populate-on-Write Threshold) is 0..4 </li></ul></td><td><p><img align="right" src="https://habrastorage.org/webt/hv/yk/qu/hvykquyvns-_g5w_jp8isx4ms2s.png"></p></td></tr></tbody></table><br><p>  Profiles can be changed "on the fly", but, of course, with zeroing the contents of the cache and its new "warming up". </p><br><p>  Considering the principle of operation of the SSD cache, you can select the main operations when working with it: </p><br><ul><li>  <a href="https://habr.com/ru/company/skilline/blog/432366/">Reading data when it is not in the cache;</a> </li><li>  <a href="https://habr.com/ru/company/skilline/blog/432366/">Reading data when they are present in the cache;</a> </li><li>  <a href="https://habr.com/ru/company/skilline/blog/432366/">Write data when using the cache for reading;</a> </li><li>  <a href="https://habr.com/ru/company/skilline/blog/432366/">Write data when using cache for read + write.</a> </li></ul><br><hr><br><a name="1"></a><br><p></p><div style="text-align:center;"><img width="400" src="https://habrastorage.org/webt/ae/nm/-h/aenm-hgkhwpkxaurphch_-mfsa4.png"></div><br><p>  <i>Read data when not in cache</i> </p><br><ol><li>  The request from the host goes to the controller; </li><li>  Since the requested are not in the SSD cache, they are read from the hard drives; </li><li>  The read data is sent to the host.  At the same time, it is checked whether these blocks are ‚Äúhot‚Äù; </li><li>  If so, they are copied to the SSD cache for future use. </li></ol><br><hr><br><a name="2"></a><br><p></p><div style="text-align:center;"><img width="400" src="https://habrastorage.org/webt/2y/hj/ce/2yhjceex34-npby_-w60y-1nm_e.png"></div><br><p>  <i>Read data when they are in cache</i> </p><br><ol><li>  The request from the host goes to the controller; </li><li>  Since the requested data is in the SSD cache, they are read from there; </li><li>  The read data is sent to the host. </li></ol><br><hr><br><a name="3"></a><br><p></p><div style="text-align:center;"><img width="400" src="https://habrastorage.org/webt/ey/vu/yx/eyvuyxvvepb1hdzcrvpxz6lrdcw.png"></div><br><p>  <i>Write data when using cache for reading</i> </p><br><ol><li>  The write request from the host goes to the controller; </li><li>  Data is written to hard drives; </li><li>  A successful write response is returned to the host; </li><li>  At the same time, it is checked whether the block is ‚Äúhot‚Äù (the parameter Populate-on-Write Threshold is compared).  If yes, then it is copied to the SSD cache for later use. </li></ol><br><hr><br><a name="4"></a><br><p></p><div style="text-align:center;"><img width="400" src="https://habrastorage.org/webt/td/-o/9z/td-o9z2jfl6dbbyhmi6kqz61bbe.png"></div><br><p>  <i>Write data when using cache for read + write</i> </p><br><ol><li>  The write request from the host goes to the controller; </li><li>  Data is recorded in the SSD cache; </li><li>  A successful write response is returned to the host; </li><li>  Data from the SSD cache in the background is written to hard drives; </li></ol><br><hr><br><h2>  Checking in </h2><br><div class="spoiler">  <b class="spoiler_title">Test stand</b> <div class="spoiler_text"><p>  2 servers (CPU: 2 x Xeon E5-2620v3 2.4Hz / RAM: 32GB) are connected via two ports via Fiber Channel 16G directly to the XCubeSAN XS5224D storage system (16GB RAM / controller). </p><br><p>  Used 16 x Seagate Constellation ES, ST500NM0001, 500GB, SAS 6Gb / s, combined in RAID5 (15 + 1), for an array with data and 8 x HGST Ultrastar SSD800MH.B, HUSMH8010BSS200, 100GB, SAS 12Gb / s as cache </p><br><p>  2 volumes were created: one for each server. </p><br></div></div><br><h3>  Test 1. SSD cache is read only from 1-8 SSD </h3><br><table><tbody><tr><td><p>  <b>SSD Cache</b> </p><br><ul><li>  I / O Type: Customization </li><li>  Cache Block Size: 4MB </li><li>  <b>Populate-on-read Threshold: 1</b> </li><li>  Populate-on-write Threshold: 0 </li></ul></td><td><p>  <b>I / o pattern</b> </p><br><ul><li>  Tool: IOmeter V1.1.0 </li><li>  Workers: 1 </li><li>  Outstanding (Queue Depth): 128 </li><li>  <b>Access Specifications: 4KB, 100% Read, 100% Random</b> </li></ul></td></tr></tbody></table><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/i4/lt/f7/i4ltf7jbkc7yul47drqxdnxiep8.jpeg"></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/kc/qd/ob/kcqdobnqjuybr3i8p-fszbmhlru.jpeg"></div><br><p>  In theory, the larger the SSD in the cache pool, the better the performance.  In practice, this was confirmed.  The only significant increase in the number of SSD with a small number of volumes does not lead to an explosive effect. </p><br><h3>  Test 2. SSD cache in read + write mode with 2-8 SSD </h3><br><table><tbody><tr><td><p>  <b>SSD Cache</b> </p><br><ul><li>  I / O Type: Customization </li><li>  Cache Block Size: 4MB </li><li>  Populate-on-read Threshold: 1 </li><li>  <b>Populate-on-write Threshold: 1</b> </li></ul></td><td><p>  <b>I / o pattern</b> </p><br><ul><li>  Tool: IOmeter V1.1.0 </li><li>  Workers: 1 </li><li>  Outstanding (Queue Depth): 128 </li><li>  <b>Access Specifications: 4KB, 100% Write, 100% Random</b> </li></ul></td></tr></tbody></table><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/da/hq/am/dahqamc9x-ymghoj9emv5lcglvq.jpeg"></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/dl/wc/wv/dlwcwvgexew6awqt6k_xmndjna0.jpeg"></div><br><p>  The same result: explosive growth in performance and scaling with increasing number of SSDs. </p><br><p>  In both tests, the amount of working data was less than the total cache.  Therefore, over time, all blocks are copied to the cache.  And the work has, in fact, been conducted with SSD, almost without affecting the hard drives.  The purpose of these tests was to visually show the effectiveness of warming up the cache and scaling its performance depending on the number of SSDs. </p><br><p>  Now we will return from heaven to earth and check the more vital situation when the amount of data is larger than the cache size.  In order for the test to take place in sane time (the time it takes for the cache to warm up significantly increases as the size of the volume increases), we restrict ourselves to the size of the 120GB volume. </p><br><h3>  Test 3. Database emulation </h3><br><table><tbody><tr><td><p>  <b>SSD Cache</b> </p><br><ul><li>  <b>I / O Type: Database</b> </li><li>  Cache Block Size: 1MB </li><li>  Populate-on-read Threshold: 2 </li><li>  Populate-on-write Threshold: 1 </li></ul></td><td><p>  <b>I / o pattern</b> </p><br><ul><li>  Tool: IOmeter V1.1.0 </li><li>  Workers: 1 </li><li>  Outstanding (Queue Depth): 128 </li><li>  <b>Access Specifications: 8KB, 67% Read, 100% Random</b> </li></ul></td></tr></tbody></table><br><p></p><div style="text-align:center;"><img width="700" src="https://habrastorage.org/webt/bi/ud/d9/biudd9iwyf1dddnevu15o3ipiai.jpeg"></div><br><h2>  Verdict </h2><br><blockquote>  As an obvious conclusion, of course, it suggests a good efficiency of using SSD cache to improve the performance of any storage system.  With regard to <a href="http://www.qsan.su/goods/sistemyi-xraneniya-dannyix/">QSAN XCubeSAN,</a> this statement applies fully: SSD caching is excellently implemented.  This concerns the support of read and read + write modes, flexible operation settings for any use cases, as well as the final performance of the system as a whole.  Therefore, for a very reasonable cost (the price of the license is commensurate with the cost of 1-2 SSD), you can significantly increase the overall performance. </blockquote></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/432366/">https://habr.com/ru/post/432366/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../432354/index.html">The most significant data breaches in 2018. Part One (January-June)</a></li>
<li><a href="../432356/index.html">How to write an excellent VKontakte news feed in 20 hours</a></li>
<li><a href="../432360/index.html">"Descendant" AlphaGo independently learned to play chess, shogi and go</a></li>
<li><a href="../432362/index.html">Chang'e-4 - the mission to the far side of the moon starts today</a></li>
<li><a href="../432364/index.html">Phonetic alphabet: how the decision for aviation will help to transfer login by phone</a></li>
<li><a href="../432368/index.html">Before you - React Modern Web App</a></li>
<li><a href="../432370/index.html">AlphaZero again beat Stockfish in a match of 1000 games</a></li>
<li><a href="../432372/index.html">Microsoft has officially confirmed that the Edge is moving to the engine Chromium</a></li>
<li><a href="../432374/index.html">High Availability and Scalable Elasticsearch at Kubernetes</a></li>
<li><a href="../432376/index.html">Tinkoff Bank has developed a male voice assistant named Oleg or Ivan</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>