<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Testing flash storage. Huawei Dorado 2100 G2</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Our series of articles dedicated to testing various flash storage systems would not be complete without a Huawei product. I admit honestly, Dorado had...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Testing flash storage. Huawei Dorado 2100 G2</h1><div class="post__text post__text-html js-mediator-article">  Our series of articles dedicated to testing various flash storage systems would not be complete without a Huawei product.  I admit honestly, Dorado had already been to our laboratory earlier, almost immediately after it became available in Russia.  This time, following the "wishes of the workers" we have collected more than enough data and are pleased to present them to your attention. <br><div style="text-align:center;"><img height="197" src="https://habrastorage.org/getpro/habr/post_images/9f0/80f/2d5/9f080f2d568de40a05e12c530e907671.jpg" width="400"></div><br><h3>  <b>Testing method</b> </h3><br>  During testing, the following tasks were solved: <br><ul><li>  studies of the process of storage degradation during long-term write load (Write Cliff); </li><li>  Huawei Dorado G2 storage performance study with different load profiles in various configurations (R5 and R0); </li></ul><a name="habracut"></a><br><h4>  Testbed Configuration </h4><table><tbody><tr><td><div style="text-align:center;"><img height="140" src="https://habrastorage.org/getpro/habr/post_images/c14/71c/eb5/c1471ceb5f674eddb8aaaad0bfd61839.jpg" width="640"></div></td></tr><tr><td>  Figure 1. The block diagram of the test bench. </td></tr></tbody></table>  The test bench consists of the IBM x3750 M4 server connected directly to the storage of Huawei Dorado 2100 G2 via 4 FC channels of 8Gb each.  <abbr title="Size in Unit: 2U Installed flash media: 200Gb SSD - 25 pieces. RAID level: Possible levels are R0, R10, R5. (R5 and R0 are tested) Raw capacity: 4.3TB Formatted capacity: 3.56TB in R5 configuration Connection interfaces: FC 8Gb - up to 8 pcs (4 are used at the stand) Firmware version: V100R001C00SPCa00">Storage Huawei OceanStore Dorado 2100 G2</abbr> <br><br>  As an additional software, Symantec Storage Foundation 6.2 is installed on the test server, which implements: <br><ul><li>  Functional logical volume manager (Veritas Volume Manager); </li><li>  Functional fault-tolerant connection to disk arrays (Dynamic Multi Pathing). </li></ul><br><div class="spoiler">  <b class="spoiler_title">See tiresome details and all sorts of clever words.</b> <div class="spoiler_text">  On the test servers, the following settings are made to reduce the disk I / O latency: <br><ul><li>  Changed the I / O scheduler from ‚Äúcfq‚Äù to ‚Äúnoop‚Äù by assigning the noop value to the scheduler parameter of the Symantec VxVolume volume </li><li> Added a parameter in <code>/etc/sysctl.conf</code> minimizes the queue size at the level of the Symantec logical volume manager: <code>¬´vxvm.vxio.vol_use_rq = 0¬ª</code> ; </li><li>  increased limit of simultaneous I / O requests to the device to 1024, by assigning the value 1024 to the <code>nr_requests</code> parameter of the Symantec VxVolume volume </li><li>  Disabling the check of the possibility of merging I / O operations (iomerge) is disabled, by assigning the value 1 to the <code>nomerges</code> parameter of the Symantec VxVolume volume </li><li>  The queue size on FC adapters has been increased by adding the options <code>ql2xmaxqdepth=64 (options qla2xxx ql2xmaxqdepth=64)</code> <code>/etc/modprobe.d/modprobe.conf</code> configuration file <code>ql2xmaxqdepth=64 (options qla2xxx ql2xmaxqdepth=64)</code> ; </li></ul><br>  During the tests, various settings on the storage system are made: <br><ul><li>  implemented SSD configuration of RAID5 drives (for tests of storage in R5) or RAID0 (for tests of storage in configuration R0). </li><li>  8 LUNs of the same size are created with a total volume covering the entire usable storage capacity (for tests on a fully marked storage system) or 70% of the volume (for tests on a not fully marked storage system).  Block size LUN - 512byte.  The moons are created in such a way that for half of them (four LUNs), the owner is one storage controller (A), and for the other half, the other controller (B).  Created LUNs are presented to the test server. </li><li>  Different test groups can be conducted with Write Cache enabled on all LUNs or Write Back Disabled (Write Through).  The value of the configuration parameter is given in the test description in the testing program (1.5). </li></ul><br><h4>  Software used in the testing process </h4><br>  To create a synthetic load (run synthetic tests) on the storage system, use the Flexible IO Tester (fio) version 2.1.10 utility.  All synthetic tests use the following fio configuration parameters of the [global] section: <br><ul><li>  thread = 0 </li><li>  direct = 1 </li><li>  group_reporting = 1 </li><li>  norandommap = 1 </li><li>  time_based = 1 </li><li>  randrepeat = 0 </li><li>  ramp_time = 3 </li></ul><br>  The following utilities are used to remove performance indicators under synthetic load: <br><ul><li>  iostat, part of the sysstat version 9.0.4 package with txk keys; </li><li>  vxstat, which is part of Symantec Storage Foundation 6.2 with the svd keys; </li><li>  vxdmpadm, part of Symantec Storage Foundation 6.2 with the -q iostat keys; </li><li>  fio version 2.1.10, to generate a summary report for each load profile. </li></ul><br>  The removal of performance indicators during the test with the utilities iostat, vxstat, vxdmpstat is performed at intervals of 5 seconds. <br></div></div><br><h4>  Testing program. </h4><br>  The tests are performed by creating a synthetic load on the block device using fio, which is a logical volume of the type <code>¬´stripe, 8 column, stripe unit size=1MiB¬ª</code> , created using Veritas Volume Manager from 8 LUNs presented from the system under test. <br><br><div class="spoiler">  <b class="spoiler_title">Ask for details</b> <div class="spoiler_text"><h5>  <b>Group 1: Tests that implement a long write load and a long mixed load with a change in block size I / O operations (I / O)</b> </h5><br>  When creating a test load, the following fio program parameters are used (in addition to those defined in section 1.4): <br><ul><li>  rw = randwrite </li><li>  blocksize = [4K 8K 16K 32K 64K 1M] (the behavior of storage systems under load with different block sizes of input-output operations is investigated) </li><li>  numjobs = 64 </li><li>  iodepth = 64 </li></ul><br>  A group of tests consists of tests that differ in the total volume of LUNs presented with the tested storage system, RAID configuration, use of the write cache and the direction of the load: <br><ul><li>  Recording tests performed on a fully-marked storage system in the R5 configuration with a variable block size (4,8,16,32,64,1024K).  For each LUN, write cache is disabled.  The duration of each test is 40 minutes, the interval between tests is 1 hour. </li><li>  The same with the write cache enabled. </li><li>  Recording tests performed on an incompletely marked storage system (total LUN volume - 70% of the useful volume) in the R5 configuration with a variable block size (4,8,16,32,64,1024K).  For each LUN, write cache is disabled. </li><li>  Tests with long-term mixed load (50% write, 50% read), performed on a fully-marked storage system in the R5 configuration with varying block size (4,8,16,32,64,1024K).  For each LUN, write cache is disabled.  The duration of each test is 20 minutes, the interval between tests is 40 minutes. </li><li>  Tests with a long mixed load (70% write, 30% read), performed on a fully labeled storage system in the R5 configuration with varying block size (4,8,16,32,64,1024K).  For each LUN, write cache is disabled.  The duration of each test is 20 minutes, the interval between tests is 40 minutes. </li><li>  Recording tests performed on a fully-marked storage system in the R0 configuration with a variable block size (4,8,16,32,64,1024K).  For each LUN, write cache is disabled.  The duration of each test is 40 minutes, the interval between tests is 1 hour. </li></ul><br>  If the test requires changing the configuration of the RAID storage system, then after changing the RAID level, creating the LUN on the storage system and the server volume, the volume is filled with the fio 2.1.10 utility using the 16K block on the volume that is 2 times larger than the volume. <br>  Based on the test results, based on the data output by the vxstat command, graphs are generated that combine the test results: <br><ul><li>  IOPS as a function of time; </li><li>  Bandwidth, as a function of time. </li></ul><br>  The analysis of the received information is carried out and conclusions are drawn about: <br><ul><li>  the presence of performance degradation with long-term load on the record and with a long mixed load; </li><li>  the performance of storage service processes (garbage collection), limiting the performance of the disk array to write during a long peak load in various storage configurations (R5 and R0); </li><li>  the impact of the cache on the record on the performance of storage service processes (garbage collection); </li><li>  the degree of influence of the size of the block I / O operations on the performance of the service processes of the storage; </li><li>  the amount of space reserved for storage for leveling storage service processes; </li><li>  the impact of storage density on the performance of service processes; </li><li>  the influence of storage service processes processes (garbage collection) on read performance. </li></ul><br><h5>  <b>Group 2: Disk array performance tests with different types of load, executed at the block device level, when configuring the storage system R5.</b> </h5><br>  During testing, the following types of loads are investigated: <br><ul><li>  load profiles (changeable software parameters fio: randomrw, rwmixedread): </li></ul><ol><li>  random recording 100%; </li><li>  random write 30%, random read 70%; </li><li>  random read 100%. </li></ol><ul><li>  block sizes: 1KB, 8KB, 16KB, 32KB, 64KB, 1MB (changeable software parameter fio: blocksize); </li><li>  methods of processing I / O operations: synchronous, asynchronous (variable software parameter fio: ioengine); </li><li>  the number of load generating processes: 1, 2, 4, 8, 16, 32, 64, 128, 256 (changeable software parameter fio: numjobs); </li><li>  queue depth (for asynchronous I / O operations): 32, 64 (changeable software parameter fio: iodepth). </li></ul><br>  A test group consists of a set of tests representing all possible combinations of the above types of load.  The duration of each test is 1 minute.  To level the impact of the service processes of the storage system (garbage collection) on the test results, a pause between the tests is equal to the amount of information recorded during the test, divided by the performance of the storage service processes (determined by the results of the first group of tests). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      All of the above tests are performed initially with the write cache turned off for all LUNs presented with the storage system. <br><br>  Based on the test results, based on the data output by the fio software, upon completion of each of the tests, graphs are generated for each combination of load types (load profile, processing method for I / O operations, queue depth, combining tests with different I / O block values): <br><ul><li>  IOPS as a function of the number of load generating processes; </li><li>  Bandwidth as a function of the number of processes that generate the load; </li><li>  Latitude (clat) as a function of the number of load generating processes; </li></ul><br>  The results are analyzed, conclusions are drawn about the load characteristics of the disk array at latency less than or about 1ms, about the maximum performance of the array and about the performance of the array under single-threaded load, as well as about the effect of the cache on the storage performance. <br><br><h5>  <b>Group 3: Disk array performance tests with different types of load, executed at the block device level, with the configuration of the storage system R0.</b> </h5><br>  Tests are conducted similarly to tests of group 2. The configuration of the storage system R0 is being tested.  Based on the test results, based on the data output by the fio software, upon completion of each of the tests, graphs are generated for each combination of load types (load profile, processing method for I / O operations, queue depth, combining tests with different I / O block values): <br><ul><li>  IOPS as a function of the number of load generating processes; </li><li>  Bandwidth as a function of the number of processes that generate the load; </li><li>  Latitude (clat) as a function of the number of load generating processes; </li></ul><br>  The analysis of the obtained results is carried out, conclusions are drawn about the load characteristics of the disk array in R0 with latency less than or about 1ms, the maximum performance indicators of the array in R0 and the performance of the array under single-threaded load. <br></div></div><br><h2>  <b>Test results</b> </h2><br><h5>  <b>Group 1: Tests that implement a long write load and a long mixed load with a change in block size I / O operations (I / O).</b> </h5><br><div class="spoiler">  <b class="spoiler_title">Charts</b>  <b class="spoiler_title">(All pictures are clickable)</b> <div class="spoiler_text"><table border="1" cellpadding="2" cellspacing="2"><tbody><tr><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/b49/85a/aac/b4985aaacc45806896ed0f38c0be214a.jpg" width="226"></a> <br></td><td>  Chart 1 <br>  Performance drop during long-term peak load such as random write on a fully-marked storage system (R5) with write-off cache. </td></tr><tr><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/bb6/59d/bbd/bb659dbbd41f0b27491de7302d0559ce.jpg" width="226"></a> <br></td><td>  Chart 2 <br>  Performance drop during long-term peak load such as random write on a fully-marked storage system (R5) with write-on cache. </td></tr><tr><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/995/7f5/c3b/9957f5c3bed83c3236a0d8573f497639.jpg" width="226"></a> <br></td><td>  Chart 3 <br>  Performance drop during long-term peak load such as random write on a not fully marked storage system (R5) (70% of the volume) with the write cache turned off. </td></tr><tr><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/6fa/bca/c8a/6fabcac8ad3146db8f2e2db19e306917.jpg" width="226"></a> <br></td><td>  Chart 4 <br>  Performance degradation of write operations during long-term peak mixed load (50% write, 50% read) on a fully-marked storage system (R5) with the write cache turned off. </td></tr><tr><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/171/6cf/a2a/1716cfa2a36eef231046ed360d8a3021.jpg" width="226"></a> <br></td><td>  Chart 5 <br>  Drop in read performance during long-term peak mixed load (50% write, 50% read) on a fully-marked storage system (R5) with the write cache turned off. </td></tr><tr><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/5a2/bd9/c67/5a2bd9c67fdef4979a179a467440d509.jpg" width="226"></a> <br></td><td>  Chart 6 <br>  Drop in write performance with long-term peak mixed load (70% write, 30% read) on a fully-marked storage system (R5) with the write cache turned off. </td></tr><tr><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/8bf/e00/1e2/8bfe001e24744be02cb05fdfb17455d0.jpg" width="226"></a> <br></td><td>  Chart 7 <br>  Drop in read performance during long-term peak mixed load (70% write, 30% read) on a fully-marked storage system (R5) with the write cache turned off. </td></tr><tr><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/96a/712/c12/96a712c127ac03f08e81546589fa2689.jpg" width="226"></a> <br></td><td>  Chart 8 <br>  Performance drop during long-term peak load, such as a random write on a fully-marked storage system (R0) in a RAID 0 configuration with the write cache off. </td></tr></tbody></table><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Tables 1-2.</b>  <b class="spoiler_title">(All pictures are clickable)</b> <div class="spoiler_text"><table><tbody><tr><td><div style="text-align:center;"><img height="312" src="https://habrastorage.org/getpro/habr/post_images/85a/44d/ffb/85a44dffb541486e1984db32b57c27ce.png" width="640"></div></td></tr><tr><td>  Table 1. The dependence of the performance of storage on the block size with a long write load in the storage configuration - RAID 5. </td></tr></tbody></table><br><table><tbody><tr><td><div style="text-align:center;"><img height="312" src="https://habrastorage.org/getpro/habr/post_images/532/1b2/158/5321b215865e0464b57dc23b5f93cfd6.png" width="640"></div></td></tr><tr><td>  Table 2. The dependence of the performance of storage on the block size with a long write load in the storage configuration - RAID 0. </td></tr></tbody></table></div></div><br>  <b>Main conclusions:</b> <br><br>  1. When filling the array with data, it is noticed that formatting (RAID initialization) occurs only when writing data.  Accordingly, the first entry is slow. <br><br>  2. With a long load on the recording at a certain point in time, a significant degradation of storage performance is recorded.  A drop in performance is expected and is a feature of the SSD (write cliff) operation related to the inclusion of garbage collection (GC) processes and the limited performance of the indicated processes.  The performance of the disk array, fixed after the write cliff effect (after a drop in performance), can be considered as the maximum average performance of the disk array.  A drop in performance occurs after 2-3 minutes of a peak load on the recording, which is not a very good indicator in comparison with disk arrays from other manufacturers and indicates the insignificant reserve of disk space for damping peak write loads. <br><br>  3. The block size of I / O operations affects the performance of the garbage collection (GC) process. <br><br>  In RAID 5 configuration: <br><ul><li>  For small blocks (4K), GC performance is 180MB / s (45000 IOPS); </li><li>  For medium blocks (8-64K) GC performance - 350MB / s; </li><li>  For large blocks (1M), the GC performance is 800 MB / s. </li></ul><br>  In a RAID 0 configuration: <br><ul><li>  For small blocks (4K), the GC performance is 575 MB / s (143750 IOPS); </li><li>  For medium and large blocks (8-64K), the GC performance is 1200 MB / s. </li></ul><br>  4. The controller cache does not affect the behavior of the storage system during a long peak write load.  The moment of performance loss (Write Cliff) comes at the same time, regardless of the use of the array cache to write (Graph 2).  The performance of the storage after a drop in performance is also independent of the use of the cache. <br><br>  5. Storage density does not affect the operation of GC processes (Graph 3). <br><br>  6. In the conditions of the load profile on the storage system with frequent intensive recording lasting 2-3 minutes or more, an 8K block is most effective for operation. <br><br>  7. From the results of tests with a long mixed load, it can be seen that when the performance of write operations drops, the performance of read operations also decreases proportionally.  This is caused, most likely by the fact that the fio processes generated the load in exactly the specified proportions (70/30 or 50/50), that is, in practice, this behavior of the disk array can be expected if there is a relationship between the write and read processes. <br><br>  8. Tests with a long mixed load showed that the performance of write operations on a mixed load is the same as with a clean load on the record.  It can be concluded that read operations do not affect the performance of the garbage collection. <br><br>  9. The performance of write operations (both before and after the drop in performance) is significantly higher in the storage configuration of RAID0. <br><br><h5>  <b>Group 2: Disk array performance tests with different types of load, executed at the block device level, when configuring the storage system R5.</b> </h5><br><div class="spoiler">  <b class="spoiler_title">Charts</b>  <b class="spoiler_title">(All pictures are clickable)</b> <div class="spoiler_text"><table border="1" cellpadding="2" cellspacing="2"><tbody><tr><td></td><td>  Synchronous way in / in </td><td>  Asynchronous way in / in with a queue depth of 32 </td><td>  Asynchronous way in / in with a queue depth of 64 </td></tr><tr><td>  Random reading </td><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/74e/925/863/74e925863796a123cb88ba8a816fb6bd.jpg" width="226"></a> <br></td><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/e6b/b1f/2b5/e6bb1f2b5392c3416b407c138a6d4c62.jpg" width="226"></a> <br></td><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/542/5ce/745/5425ce745103b48ca2e1d542909e3483.jpg" width="226"></a> <br></td></tr><tr><td>  With random recording </td><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/e12/503/c75/e12503c75f09335cb3b114ef9faceae1.jpg" width="226"></a> <br></td><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/148/2a8/617/1482a86173402608792a52a712ef54c3.jpg" width="226"></a> <br></td><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/ff0/dc7/e11/ff0dc7e119e73c5a0674c0ba3c8d2163.jpg" width="226"></a> <br></td></tr><tr><td>  With mixed load (70% read, 30% write) </td><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/a80/497/55f/a8049755f41d44e445480571cec9d8d3.jpg" width="226"></a> <br></td><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/5f2/9f9/c98/5f29f9c9807c54269f886331a8d42eef.jpg" width="226"></a> <br></td><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/967/3ac/2a0/9673ac2a0c7ba09e360a3e55b3c3184c.jpg" width="226"></a> <br></td></tr></tbody></table><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Tables 3-5.</b>  <b class="spoiler_title">(All pictures are clickable)</b> <div class="spoiler_text"><table><tbody><tr><td><div style="text-align:center;"><img height="538" src="https://habrastorage.org/getpro/habr/post_images/ad9/2e3/0b3/ad92e30b335e5fb0069e3713c516fca5.png" width="640"></div></td></tr><tr><td>  Table 3. Storage Performance in a RAID 5 Configuration with One Process Generating Load (jobs = 1) </td></tr></tbody></table><br><table><tbody><tr><td><div style="text-align:center;"><img height="536" src="https://habrastorage.org/getpro/habr/post_images/8ec/1fe/1e2/8ec1fe1e2ad473724c9da4dffa6aff9a.png" width="640"></div></td></tr><tr><td>  Table 4. Maximum storage performance in a RAID 5 configuration with delays less than 1ms </td></tr></tbody></table><br><table><tbody><tr><td><div style="text-align:center;"><img height="540" src="https://habrastorage.org/getpro/habr/post_images/2d6/447/f29/2d6447f299243d208bca9703b19d0ef5.png" width="640"></div></td></tr><tr><td>  Table 5. Maximum storage performance in a RAID 5 configuration with a different load profile. </td></tr></tbody></table></div></div><br>  <b>Main conclusions:</b> <br><br>  1. The maximum recorded performance parameters for storage in a RAID 5 configuration (from the average during the execution of each test - 1 min): <br><br>  Record: <br><ul><li>  110,000 IOPS with latency 4,7ms (4KB async qd32.64 block); </li><li>  with synchronous I / O - 88,000 IOPS with latency of 2.5 ms (4K and 8K blocks); </li><li>  Bandwidth: 2130MB / c for large blocks (1MB), about 1000Mb / s for medium blocks (16-64K). </li></ul><br>  Reading: <br><ul><li>  140,000 IOPS with latency 1,8ms (blocks 4 and 8K async qd64); </li><li>  with synchronous I / O - 91,000 IOPS with latency of 0.7ms (4K block); </li><li>  Bandwidth: 3160MB / s for large blocks. * </li></ul><br>  Mixed load (70/30 rw): <br><ul><li>  132,000 IOPS with latency 1.9ms (blocks 4 and 8K async qd64); </li><li>  with synchronous I / O - 87,000 IOPS with latency 1,4ms (blocks 4, 8, 16K); </li><li>  Bandwidth 4100MB / s for large blocks (1 MB) and about 3000 MB / s for blocks 64, 32. * </li></ul><br>  <b>*</b> <i>Considering the fact that the disk array is connected to the test server by means of 4 8Gb / s, the obtained values ‚Äã‚Äãare most likely not the limit for the disk array, the limiting factor was the bandwidth of the connection channel to the server.</i> <br><br>  Minimal latency fixed: <br><ul><li>  When recording - 0.9ms for block 4K jobs = 1,2 sync </li><li>  When reading - 0.6ms for 4K and 8K blocks jobs = 1-8 sync </li></ul><br>  2. The storage system enters saturation mode when: <br><ul><li>  asynchronous I / O with 2-4 jobs on 32K, 64K blocks and 8 jobs on small and medium blocks (4-16K). </li><li>  synchronous way in / in at 128-192 jobs. </li></ul><br>  3. On 1MB blocks, the maximum throughput is 2 times higher than the maximum values ‚Äã‚Äãobtained on the middle blocks (16-64K). <br><br>  4. The array produces the same number of I / O operations on blocks 4 and 8K and a little less on block 16K.  The optimal blocks for working with an array are 8 and 16K. <br><br><h5>  <b>Group 3: Disk array performance tests with different types of load, executed at the block device level, with the configuration of the storage system R0.</b> </h5><br><div class="spoiler">  <b class="spoiler_title">Charts</b>  <b class="spoiler_title">(All pictures are clickable)</b> <div class="spoiler_text"><table border="1" cellpadding="2" cellspacing="2"><tbody><tr><td><br></td><td>  Synchronous way in / in </td><td>  Asynchronous way in / in with a queue depth of 32 </td><td>  Asynchronous way in / in with a queue depth of 64 </td></tr><tr><td>  Random reading </td><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/822/621/45b/82262145b34f4a624edd381236396fbd.jpg" width="226"></a> <br></td><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/6f2/cce/977/6f2cce977ddf7f29e0c233f8f3f5b62b.jpg" width="226"></a> <br></td><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/0b7/8b3/f8d/0b78b3f8db2cc1e153c692495ef53f2b.jpg" width="226"></a> <br></td></tr><tr><td>  With random recording </td><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/7ff/5ee/003/7ff5ee003058456130195982efd92602.jpg" width="226"></a> <br></td><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/99c/67e/710/99c67e7104da663165fe6dc6abf55424.jpg" width="226"></a> <br></td><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/426/737/68e/42673768e21c6b97d4942b96d7634fd6.jpg" width="226"></a> <br></td></tr><tr><td>  With mixed load (70% read, 30% write) </td><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/6a3/526/17b/6a352617b2241026a8a5bcf0010b4751.jpg" width="226"></a> <br></td><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/a5e/b29/ccf/a5eb29ccf9554c19132b13bb9fea8dbe.jpg" width="226"></a> <br></td><td> <a href=""><img height="320" src="https://habrastorage.org/getpro/habr/post_images/1c2/355/f1b/1c2355f1b6a3f65e278335cc8d1d4e0d.jpg" width="226"></a> <br></td></tr></tbody></table><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Tables 6-8.</b>  <b class="spoiler_title">(All pictures are clickable)</b> <div class="spoiler_text"><table><tbody><tr><td><div style="text-align:center;"><img height="538" src="https://habrastorage.org/getpro/habr/post_images/c77/0a0/ccf/c770a0ccf1be340fc44641ef8159ee62.png" width="640"></div></td></tr><tr><td>  Table 6. Storage Performance in a RAID 0 Configuration with One Process Generating Load (jobs = 1) </td></tr></tbody></table><br><table><tbody><tr><td><div style="text-align:center;"><img height="538" src="https://habrastorage.org/getpro/habr/post_images/d0c/d2d/33c/d0cd2d33c86292b5546c9f20bf54280c.png" width="640"></div></td></tr><tr><td>  Table 7. Maximum storage performance in a RAID 0 configuration with delays less than 1ms </td></tr></tbody></table><br><table><tbody><tr><td><div style="text-align:center;"><img height="538" src="https://habrastorage.org/getpro/habr/post_images/1d7/048/6fe/1d70486fe845ebcdcc760b2921bb9a25.png" width="640"></div></td></tr><tr><td>  Table 8. Maximum storage performance in a RAID 0 configuration with a different load profile. </td></tr></tbody></table></div></div><br>  <b>Main conclusions:</b> <br><br>  1. Maximum recorded performance parameters for storage in a RAID 0 configuration (from the average for the duration of each test run - 1 min): <br><br>  Record: <br><ul><li>  311,000 IOPS with latency of 1.6ms (4KB async qd32.64 block); </li><li>  with synchronous I / O - 190,000 IOPS with latency of 0.7 ms (4K and 8K blocks); </li><li>  Bandwidth: 3160MB / c for large blocks (1MB), about 2500Mb / s for medium blocks (16-64K). * </li></ul><br>  Reading: <br><ul><li>  310,000 IOPS with latency 3,3ms (4K async qd64 block); </li><li>  with synchronous I / O - 91,000 IOPS with latency of 0.7ms (4K block); </li><li>  Bandwidth: 3260MB / s for large blocks.  <b>*</b> </li></ul><br>  <b>*</b> <i>Considering the fact that the disk array is connected to the test server by means of 4 8Gb / s, the obtained values ‚Äã‚Äãare most likely not the limit for the disk array, the limiting factor was the bandwidth of the connection channel to the server.</i> <br><br>  Mixed load (70/30 rw) <br><ul><li>  305,000 IOPS with latency 1,2ms (4K async qd64 block); </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> with synchronous I / O - 225,000 IOPS with latency of 0.95 ms (4K blocks); </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Bandwidth 4140MB / s for large blocks (1MB, 64K, 32K). </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Minimal latency fixed: </font></font><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> When recording - 0.35ms for 4K jobs = 1.2 sync </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> When reading - 0.6ms for 4K and 8K blocks jobs = 1sync </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 2. The storage system enters saturation mode when: </font></font><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> asynchronous I / O with 2-4 jobs on 32K, 64K blocks and 8 jobs on small and medium blocks (4-16K). </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> synchronous method in / in with 64-128 jobs. </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3. On 1MB blocks, the maximum throughput is 2 times the maximum values ‚Äã‚Äãobtained on the middle blocks (16-64K) </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4. The array in the RAID0 configuration gives the same number of I / O operations on the 4 and 8K blocks and slightly less on the 16K block. </font><font style="vertical-align: inherit;">The optimal blocks for working with an array are 8 and 16K.</font></font><br><br><h2> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">findings</font></font></b> </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There were pleasant impressions from working with the array. </font><font style="vertical-align: inherit;">The performance results were higher than expected. </font><font style="vertical-align: inherit;">The array is distinguished by a classic storage architecture with two controllers, standard RAID levels and ordinary eMLC SSDs. </font><font style="vertical-align: inherit;">The consequence of this architecture is low cost compared to other flash storage systems and higher delays in processing I / O operations. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Huawei Dorado G2 is perfect for multi-threaded loads, not very critical to delays.</font></font></div><p>Source: <a href="https://habr.com/ru/post/265143/">https://habr.com/ru/post/265143/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../265133/index.html">Download video "without a single break"</a></li>
<li><a href="../265135/index.html">Web panels, extension buttons and other new items in Vivaldi 1.0.249.12 assembly</a></li>
<li><a href="../265137/index.html">A note on ansible. Server reboot</a></li>
<li><a href="../265139/index.html">How do we conduct iron test drives with a cast iron bridge</a></li>
<li><a href="../265141/index.html">Fall semester 2015 courses at the Computer Science Club</a></li>
<li><a href="../265149/index.html">Underground carders market. Translation of the book "KingPIN". Chapter 8. ‚ÄúWelcome to America‚Äù</a></li>
<li><a href="../265153/index.html">Preview of the first updates of the Microsoft Edge web platform</a></li>
<li><a href="../265155/index.html">Stabilization of time-lapse video on a calculator (IPython + OpenCV)</a></li>
<li><a href="../265157/index.html">About false posts in the VC or how many operations per second the human brain performs</a></li>
<li><a href="../265159/index.html">Our Service is both dangerous and difficult or some aspects of the survival of services in Android</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>