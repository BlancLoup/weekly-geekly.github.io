<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>12 billion requests per month for $ 120 in java</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="When you launch your product, you absolutely do not know what will happen after launch. You can remain an absolutely unnecessary project to anyone, yo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>12 billion requests per month for $ 120 in java</h1><div class="post__text post__text-html js-mediator-article">  When you launch your product, you absolutely do not know what will happen after launch.  You can remain an absolutely unnecessary project to anyone, you can get a small stream of customers or a whole tsunami of users right away if the leading media write about you.  We did not know either. <br><br>  This post is about the architecture of our system, its evolutionary development for almost 3 years and the trade-offs between development speed, performance, cost and simplicity. <br><br>  Simplified task looked like this - you need to connect the microcontroller with a mobile application via the Internet.  Example - press the button in the application the LED on the microcontroller lights up.  We extinguish the LED on the microcontroller and the button in the application accordingly changes the status. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Since we started the project on kickstarter, before launching the server in production, we already had a fairly large base of first users - 5000 people.  Probably many of you have heard about the famous habr effect that many web resources put in the past.  We, of course, did not want to repeat this fate.  Therefore, this was reflected in the selection of the technical stack and application architecture. <br><br>  Immediately after the launch, our entire architecture looked like this: <br><br><img src="https://habrastorage.org/files/34a/e28/75f/34ae2875f8314f3a8d3fb146783dc7f5.png"><br><br>  It was 1 virtual machine from Digital Ocean for $ 80 per month (4 CPU, 8 GB RAM, 80 GB SSD).  Taken with a margin.  Since ‚Äúwhat if the boot goes?‚Äù.  Then we really thought that, let's start, and thousands of users would rush at us.  As it turned out, attracting and luring users is also a task and load on the server - the last thing worth thinking about.  Of the technologies at that time, only Java 8 and Netty with our own binary protocol on ssl / tcp sockets (yes yes, without a database, spring, hibernate, tomcat, websphere and other delights of a bloody enterprise). <br><br>  All user data was simply stored in memory and periodically flushed to files: <br><br><pre><code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">try</span></span> (BufferedWriter writer = Files.newBufferedWriter(fileTo, UTF_8)) { writer.write(user.toJson()); }</code> </pre> <br><a name="habracut"></a><br>  The whole process of raising the server came down to one line: <br><br><pre> <code class="java hljs">java -jar server.jar &amp;</code> </pre> <br>  Peak load immediately after launch was 40 rec sec.  This tsunami did not happen. <br><br>  Nevertheless, we have worked hard and constantly, constantly adding new features, listening to feedback from our users.  The user base, though slowly but steadily and steadily, grew by 5-10% every month.  The server load also increased. <br><br>  The first serious feature was reporting.  At the moment when we began to implement it - the load on the system was already 1 billion requests per month.  And most of the requests were real data, such as readings of temperature sensors.  It was obvious that storing every request was very expensive.  Therefore, we went to the tricks.  Instead of saving each request, we calculate the average value in memory with minute granularity.  That is, if you send the numbers 10 and 20 within a minute, then you will receive a value of 15 for this minute at the output. <br><br>  At first I succumbed to HYIP and implemented this apache spark approach.  But when it came to deployment, I realized that the game was not worth the candle.  So of course it was ‚Äúright‚Äù and ‚Äúenterprise‚Äù.  But now I had to deploy and monitor 2 systems instead of my cozy monolithic.  In addition, an overhead was added to serialize data and transfer them.  In general, I got rid of the Spark and just count the values ‚Äã‚Äãin the memory and once a minute I drop it onto the disk.  The output looks like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/9af/aa5/a18/9afaa5a18e9944eea0f0059635aef344.png"></div><br>  The single-server monolith system worked fine.  But there were quite obvious disadvantages: <br><br><ul><li>  Since the server was in New York - in remote areas, for example, in Asia, lags were visible when interactively using the application.  For example, when you change the lamp brightness level with the slider.  Nothing critical and none of the users complained about it, but we are changing the world, damn it. </li><li>  Depla required to disconnect all connections and the server was unavailable for ~ 5 seconds each time it was restarted.  In the active phase of development, we did about 6 deployments per month.  What is funny - for all the time of such restarts - not a single user has noticed the unavailability of servers.  That is, the restarts were so fast (hello spring and tomkat) that users did not notice them at all. </li><li>  Failure of one server, data center lozhil everything. </li></ul><br>  8 months after launch, the flow of new features was a bit asleep and I had time to change this situation.  The task was simple - to reduce the delay in different regions, to reduce the risk of the fall of the entire system at the same time.  Well, do it all quickly, easily, cheaply and with minimal effort.  Startup, all the same. <br><br>  The second version is: <br><br><img src="https://habrastorage.org/files/389/f78/cfc/389f78cfc58242778be36b2de7e6f808.png"><br><br><img src="https://habrastorage.org/files/d3b/c8b/514/d3bc8b5140234df8ac5c9fd9e313c138.png"><br><br>  As you probably noticed, I chose GeoDNS.  It was a very quick decision - the entire setting of 30 minutes in Amazon Route 53 was to read and configure.  Pretty cheap - Amazon's Geo DNS routing costs $ 50 per month (I was looking for alternatives cheaper, but I could not find it).  Quite simple - because no load balancer was needed.  And it took a minimum of effort - I only had to prepare a little code (it took less than a day). <br><br>  Now we had 3 monolithic servers for $ 20 (2 CPU, 2 GB RAM, 40 GB SSD) + $ 50 for Geo DNS.  The entire system cost $ 110 per month, while it had 2 cores more for the price of $ 20 cheaper.  At the time of transition to the new architecture, the load was 2000 rec sec.  And the old virtual was loaded only by 6%. <br><br><img src="https://habrastorage.org/files/3cb/aa9/d43/3cbaa9d433824869af36803c90537779.png"><br><br>  All the problems of the monolith above were solved, but a new one appeared - when a person moves to another zone - he will get to another server and nothing will work for him.  It was a conscious risk and we went to him.  The motivation is very simple - users do not pay (at that time the system was completely free), so let them suffer.  We also used statistics, according to which - only 30% of Americans at least once in their lives left their country, and only 5% regularly moved.  Therefore, it was assumed that this problem will affect only a small% of our users.  The prediction came true.  On average, we received about one letter in 2-3 days from the user who ‚ÄúLost projects.  What to do?  Save! ‚Äù  Over time, such letters began to be very annoying (despite the detailed instructions on how quickly the user could fix this).  Moreover, such an approach would hardly have arranged a business, to which we have just begun to switch.  It was necessary to do something. <br><br>  There were many solutions to the problem.  I decided that the cheapest way to do this would be to send microcontrollers and applications to one server (to avoid overhead when transferring messages from one server to another).  In general, the requirements for the new system loomed like this - different connections of one user should fall on one server and need a shared state between such servers in order to know where to connect the user. <br><br>  I heard a lot of good reviews about Cassandra, which perfectly suited this task.  Therefore, I decided to try it.  My plan looked like this: <br><br><img src="https://habrastorage.org/files/df0/156/367/df015636725a4574a12cc36a937b59a6.png"><br><br>  Yes, I am a rogue and naive Chukchi youth.  I thought that I could raise one Cassandra node on the cheapest virtual machine for up to $ 5 - 512 MB RAM, 1 CPU.  And I even read the lucky article that raised the cluster on Rasp PI.  Unfortunately, I failed to repeat his feat.  Although I removed / trimmed all buffers, as described in the article.  I managed to raise one cassandra node only on a 1 GB instance, while the node immediately fell from OOM (OutOfMemory) with a load of 10 rec-sec.  More or less stable, Cassandra behaved with 2GB.  It was not possible to increase the load of one cassandra node to 1000 rec-sec, again OOM.  At this stage, I refused cacandra, because even if it showed a decent performance, the minimum cluster in one data center would cost in 60s.  For me it was expensive, considering that our income then was $ 0.  Since I had to do it yesterday, I proceeded with plan B. <br><br><img src="https://habrastorage.org/files/c8e/0ed/2dc/c8e0ed2dc8c7466ea9a5c843341ac100.png"><br><br>  Good old postgres.  He has never let me down yet (well, almost never, yes, full vacuum?).  Postgres started perfectly on the cheapest virtual machine, absolutely did not eat RAM, the insertion of 5000 lines took 300ms and loaded the single core by 10%.  What you need!  I decided not to deploy the database in each of the data centers, but to make one common storage.  Since postgres scaling / shardit / master slave is harder than the same casing.  Yes, and this margin of safety. <br><br>  Now I had to solve another problem - to direct the client and its microcontrollers to the same server.  In essence, make a sticky session for tcp / ssl connections and your binary protocol.  Since I did not want to make drastic changes to the existing cluster, I decided to reuse Geo DNS.  The idea was this - when a mobile application receives an IP address from Geo DNS, the application opens a connection and sends a login to that IP.  The server, in turn, either processes the login command and continues to work with the client in case it is the ‚Äúcorrect‚Äù server or returns a redirect command specifying the IP where it should go.  In the worst case, the connection process looks like this: <br><br><img src="https://habrastorage.org/files/09c/633/519/09c633519ea74a8499e1e5a6f414727e.png"><br><br>  But there was one little nuance - the load.  At the time of implementation, the system processed 4,700 rec-sec.  ~ 3k devices were constantly connected to the cluster.  Periodically, ~ 10k.  That is, at the current growth rate in a year, it will already be 10k rec-sec.  Theoretically, a situation could arise when many devices are simultaneously connected to the same server (for example, when restarting, ramp up period) and if, all of a sudden, they all connected to the ‚Äúwrong server‚Äù, then too much load on the database could occur, which could lead to her refusal.  So I decided to play it safe and made the information about user-serverIP in radish.  The final system turned out like this. <br><br><img src="https://habrastorage.org/files/660/a0e/9ac/660a0e9ac39f4ce4bd5c99986c49377e.png"><br><br>  With the current load of 12 billion rivers per month, the entire system is loaded on average by 10%.  Network traffic ~ 5 Mbps (in / out, thanks to our simple protocol).  That is, in theory, such a $ 120 cluster can withstand up to 40k rec sec.  From the pros - do not need a load balancer, simple deployment, maintenance and monitoring is rather primitive, there is a possibility of vertical growth of 2 orders of magnitude (10x due to utilization of current iron and 10x due to more powerful virtual machines). <br><br>  Open-Source Project.  Sources <a href="https://github.com/blynkkk/blynk-server">can be found here.</a> <br><br>  That's all.  I hope you liked the article.  Any constructive criticism, advice and questions are welcome. </div><p>Source: <a href="https://habr.com/ru/post/316370/">https://habr.com/ru/post/316370/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../316360/index.html">What is Flussonic Watcher</a></li>
<li><a href="../316362/index.html">GIS utilities: asynchronous model of interaction</a></li>
<li><a href="../316364/index.html">Version autoincrement in pom.xml via Jenkinsfile</a></li>
<li><a href="../316366/index.html">We invite you to a free webinar Oracle Cloud: PaaS Core Services</a></li>
<li><a href="../316368/index.html">Creating a blog engine with Phoenix and Elixir / Part 4. Add processing roles in controllers</a></li>
<li><a href="../316374/index.html">Under the hood of the new crafts Dell + EMC - flash storage at the price of disk</a></li>
<li><a href="../316376/index.html">Entity? Provide your IP address</a></li>
<li><a href="../316378/index.html">Welcome to the DevFest Vladivostok</a></li>
<li><a href="../316380/index.html">FIAS addresses in the PostgreSQL environment. Part 2</a></li>
<li><a href="../316382/index.html">FreeNAS 10 - the new face of the old storage</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>