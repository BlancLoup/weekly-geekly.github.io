<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>NetApp MetroCluster (MCC)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="MetroCluster is a geo-distributed, fault-tolerant cluster built on the basis of NetApp FAS storage systems, such a cluster can be imagined as one stor...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>NetApp MetroCluster (MCC)</h1><div class="post__text post__text-html js-mediator-article">  MetroCluster is a geo-distributed, fault-tolerant cluster built on the basis of NetApp FAS storage systems, such a cluster can be imagined as one storage system stretched across two sites, where in the event of a disaster at one of the sites there is always a complete copy of the data.  MetroCluster is used to create highly accessible (HA) storage and services.  Learn more about the MCC <a href="https://mysupport.netapp.com/documentation/productlibrary/index.html%3FproductID%3D30022">official documentation</a> . <br><br>  MetroCluster working on the old OS Data ONTAP 7-Mode (up to version 8.2.x) had the abbreviation "MC", and working on ClusteredONTAP (8.x and later), so that there is no confusion, it is accepted to call MetroCluster ClusteredONTAP (MCC). <br><br>  An MCC may consist of two or more controllers.  There are three MCC connection schemes: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ol><li>  Fabric-Attached MetroCluster (FCM-MCC) </li><li>  Bridge-Attached Stretch MetroCluster </li><li>  Stretch MetroCluster </li></ol><br>  The difference in these three options is essentially only in the network harness.  The network binding affects two factors: the maximum possible distance over which the cluster can be stretched and the number of nodes in the cluster. <br><img src="https://habrastorage.org/files/1ef/b1f/e81/1efb1fe814154e54bc5b503142caa711.png"><br><a name="habracut"></a><br><br><h4>  Fabric-Attached MetroCluster </h4><br>  This configuration can consist of both two and 4 identical controllers.  The dual node configuration (mcc-2n) can be converted to four node (mcc).  In the two-channel configuration, one controller is located at each site, and if one controller <i>comes</i> out, the second at another site takes control, which is called a <i>switchover</i> .  If there is more than one node on each site, and one node of the cluster fails, then a <i>local HA failover</i> occurs without switching to the second site.  MetroCluster can stretch up to 300 km. <br><br>  This is the most costly scheme of all connection options, as it requires: <br><br><ul><li>  Double the number of shelves </li><li>  One or two IP channels for Cluster peering network </li><li>  Additional SAN switches on back-end (not to be confused with switches for connecting storage systems and hosts) </li><li>  Additional SAS-FC bridges </li><li>  FC-VI ports </li><li>  Possible xWDM multiplexers </li><li>  And at least 2 ISL long-wave FC SFP + links (4 cores: 2x RX, 2x TX).  Those.  ‚Äúdark optics‚Äù are brought to the storage system (in the intervals there can be only xWDM, without any switches), these links are dedicated exclusively to the tasks of storage replication (FC-VI &amp; back-end connections) </li></ul><br><img src="https://habrastorage.org/files/b82/8de/8ba/b828de8bae13457ba133df2230230ff5.png"><br><br>  Total between two sites requires at least 4 cores, plus Cluster peering. <br><br><h5>  8-Node MCC </h5><br>  Similar to the 4-node configuration of the Fabric-Attached MetroCluster, there is an eight-node configuration, which currently supports only NAS access protocols and ONTAP 9 firmware. Four node configurations can be updated to eight nodes.  This configuration does not yet support SAN access protocols.  In the eight-node configuration, 4 nodes are located on one site and 4 on the other.  From a network point of view, nothing particularly changes, in a similar Fabric-Attached MetroCluster of 4 nodes: the number of FC switches on the back-end remains the same, only the number of necessary ports on each site for local switching increases, but there can be inter-site connections and inter-site ports as many.  In such a scheme, cluster switches are required for 4xnn communication at each site, while 2 and 4 node configurations do not require the presence of cluster switches.  The advantage of the 8-node configuration is the possibility of using two types of FAS systems in one cluster, for example, on one site you have FAS8040 (two nodes) and All Flash FAS 8060 (two nodes), on the other site we have exactly the same mirror configuration FAS8040 (two nodes) and All Flash FAS 8060 (two nodes).  The data from one site on the FAS8040 system is replicated to the same FAS8040 system on another site and similarly for All Flash FAS 8060. The data within the site can be transparently migrated across these cluster nodes. <br><br><h4>  Bridge-Attached Stretch MetroCluster </h4><br>  Can stretch to 500 meters and requires: <br><br><ul><li>  Double the number of shelves </li><li>  One or two IP channels for Cluster peering network </li><li>  Additional SAS-FC bridges </li><li>  FC-VI ports </li><li>  at least two links (4 cores: 2x RX, 2x TX) of dark optics dedicated exclusively for the connection of storage controllers (FC-VI) </li><li>  And at least four links (total 4x2 = 8 cores) of dark optics, allocated exclusively for back-end communications </li></ul><br><img src="https://habrastorage.org/files/b11/6a7/6a4/b116a76a4a69471a82dca119e6e4b688.png"><br>  Total between two sites at least 4 + 8 = 12 lived, plus Cluster peering. <br><br><h4>  Direct Power Stretch MetroCluster </h4><br>  The cheapest option.  Can stretch to 500 meters and requires: <br><br><ul><li>  Double the number of shelves </li><li>  One or two IP channels for Cluster peering network </li><li>  FC-VI ports </li><li>  at least two links (4 cores: 2x RX, 2x TX) of dark optics dedicated exclusively for the connection of storage controllers (FC-VI) </li><li>  And at least two 4 channel (for each channel 2 lived: RX and TX) cable (total 2x8 = 16 lived) of dark optics allocated exclusively for replication tasks for access to each shelf in two ways. </li><li>  For back-end connectivity of shelves, a special 4 channel (for each channel 2 lived: RX and TX) optical SAS connector and optical patch panel are used </li></ul><br><img src="https://habrastorage.org/files/5fe/564/ec9/5fe564ec921f4df686d191b7a2ae1b56.png"><br><br>  Total between two sites at least 4 + 16 = 20 lived, plus Cluster peering. <br><br><h5>  Optical cable for Stretch MetroCluster </h5><br>  Depending on the distance and connection speed, a different optical cable is used for direct-on Stretch MetroCluster configurations. <br><table><tbody><tr><th rowspan="3">  Speed ‚Äã‚Äã(Gbps) </th><th colspan="4">  Maximum distance (m) </th></tr><tr><td colspan="3">  16Gbps SW SFP </td><td>  16Gbps LW SFP </td></tr><tr><td>  OM2 </td><td>  OM3 </td><td>  OM3 + / OM4 </td><td>  Single-Mode (SM) Fiber </td></tr><tr><td>  2 </td><td>  N / A </td><td>  N / A </td><td>  N / A </td><td>  N / A </td></tr><tr><td>  four </td><td>  150 </td><td>  270 </td><td>  270 </td><td>  500 </td></tr><tr><td>  eight </td><td>  50 </td><td>  150 </td><td>  170 </td><td>  500 </td></tr><tr><td>  sixteen </td><td>  35 </td><td>  100 </td><td>  125 </td><td>  500 </td></tr></tbody></table><br><h4>  FC-VI ports </h4><br>  Usually, for each FAS cluster metro controller, a specialized expansion card is installed, on which FC ports operate in FC-VI mode.  On some 4-port FC HBA boards for FAS controllers, it is allowed to use 2 ports for FC-VI and 2 others as target or initiator ports.  On some FAS models, the ports onboard the motherboard can switch to FC-VI mode.  Ports with the FC-VI role use the Fiber Channel protocol to mirror NVRAM content between cluster metro controllers. <br><br><h4>  <a href="https://habr.com/ru/post/279989/">Active / Active</a> </h4><a name="ActiveActive"></a><br>  There are two main approaches to data replication: <br><br><ul><li>  ‚ÄúApproach A‚Äù: Split-Brain Protection Provided </li><li>  ‚ÄúApproach B‚Äù: In which there is a possibility of access (reading and writing) to all data through all sites </li></ul><br>  These are by definition two mutually exclusive approaches. <br><br>  The advantage of ‚ÄúApproach B‚Äù is that there are no synchronization delays between the two sites.  Due to circumstances not known to me, some storage vendors allow schemes in which Spit-Brain is possible.  There are implementations in essence working on ‚ÄúApproach A‚Äù, but at the same time they emulate the ability to write on both sides at the same time, as if hiding the Active / Passive architecture, but the essence of such a scheme is that while one specific record transaction is not synchronized between two sites , it will not be confirmed, let's call it the ‚ÄúHybrid approach‚Äù, it still contains the main data set and its mirror copy.  In other words, this ‚ÄúHybrid approach‚Äù is just a special case of ‚ÄúApproach A‚Äù, and should not be confused with ‚ÄúApproach B‚Äù, despite the deceptive similarities.  The ‚Äúhybrid approach‚Äù has the ability to access its data via a remote site, in such implementations, at first glance, there is a certain advantage over the classic version of ‚ÄúApproach A‚Äù, but in fact it does not change anything - the delay in synchronization of sites is as it is and it remains "Must Have" to protect against Spit-Brain.  Let's look at the example in the figure below for all possible options for accessing data according to ‚ÄúApproach A‚Äù (including ‚ÄúHybrid‚Äù). <br><br><img src="https://habrastorage.org/files/363/887/824/363887824fa0484ba8dcbf2bc52121fd.png"><br><br>  In the figure 3 variants of possible ways of accessing data are visualized.  The first option (Path 1) is a classic implementation of the approach to protect against Split-Brain: data passes once a local path and once through a long ISL (Inter Switch Link) connection for mirroring.  This option provides, as it were, an Active / Passive cluster robots mode, but each site has its own hosts, each of which accesses its local storage (Direct Path), where both sides of the cluster are utilized, thus forming the Active / Active configuration.  In such an Active / Active configuration (with ‚ÄúApproach A‚Äù), the host will switch to the backup site only in the event of an accident, and when everything is restored, it will return to using the previous ‚Äúdirect‚Äù path.  While the ‚ÄúHybrid scheme‚Äù (with emulation of the ability to record through both sites at the same time) allows you to work on all 3 options of the paths.  In the last two versions of Path 2 &amp; Path 3, we have a completely opposite picture: the data crosses the long inter-site ISL (Indirect Path) communication channel twice, which increases the response speed.  In the latter two versions of Path 2 &amp; Path 3, there is no sense either in terms of fault tolerance, or in terms of performance, as compared to the first, and therefore they are not supported in the MetA Cluster NetApp, but work in Active / Active configuration mode (according to the classic Approach A ‚Äù), that is, using straight paths on each site, as depicted in Path 1. <br><br><h5>  Split-brain </h5><br>  As mentioned in the <a href="https://habr.com/ru/post/279989/">Active / Active</a> section, the metro cluster is architecturally designed in such a way that local hosts work with the local half of the metro cluster.  And only in the case when the whole site dies, the hosts switch to the second site.  And what happens if both sites are alive, but the connection between them just disappeared?  In the cluster metro architecture, everything is simple - the hosts continue to work, write and read, in their local halves, like nothing at all.  Just at this moment the synchronization between the sites stops.  As soon as the links are restored, both halves of the metro cluster will automatically begin to synchronize.  In this situation, the data will be read from the main plex, and not as it usually happens through NVRAM, after both plexes become equal again, the mirroring will return to the replication mode based on NVRAM memory. <br><br><h5>  <a href="https://habr.com/ru/post/279989/">Active / Passive and Unmirrored Aggregates</a> </h5><a name="ActivePassive"></a><br>  MCC is a completely symmetrical configuration: how many disks on one site are so many on another, what FAS systems are on one site, the same should be on another.  Starting with the ONTAP 9 firmware version for FAS systems, it is allowed to have Unmirrored aggregates (pools).  Thus, in the new firmware, the <i>number of disks</i> can now differ on two sites.  Thus, for example, on one site there can be two aggregates, one of them is mirrored (on the remote site there is a full mirror on the types of disks, their speed, raid groups), the second unit, which is only on the first site, but it is not replicated on remote site. <br><br>  It should be divided into two options for Active / Passive configurations: <br><br><ol><li>  First option.  When only one main site is used, and the second takes a replica and lives for safety in case the main site is turned off </li><li>  The second option.  When there are 4 or more nodes in the cluster and not all controllers are used at each site </li></ol><br>  Unmirrored aggregates allow you to create asymmetric configurations and, as an extreme, special case, the <i>first option is Active / Passive</i> configuration: when only one site is used for productive work, and the second accepts a synchronous replica and secures in case of failure of the main site.  In this scheme, when a single controller fails, it immediately switches to a backup site. <br><br><img src="https://habrastorage.org/files/5f8/275/c55/5f8275c55a9244b5ae44cec440c27750.png"><br><br>  The second <i>version of the Active / Passive</i> configuration is collected to save disks in a cluster of 4 or more nodes: only some of the controllers will serve clients, while controllers that are idle will wait patiently for the neighbor to die.  This scheme allows not to switch between sites in the event of a single controller failure, but to perform a local ON takeover. <br><br><h4>  SyncMirror - Synchronous Replication </h4><br>  SyncMirror technology allows you to mirror data and can work in two modes: <br><br><ol><li>  Local SyncMirror </li><li>  MetroCluster SyncMirror </li></ol><br>  The difference between the local SyncMirror and MCC SyncMirror is that in the first case the data is always mirrored from NVRAM <u>within one controller</u> and immediately into two plexes, it is sometimes used to protect against the failure of an entire shelf.  And in the second case, NVRAM mirroring is performed <i>between multiple controllers</i> .  NVRAM is mirrored across multiple controllers through dedicated FC-VI ports.  MCC SyncMirror is used to protect against the failure of the whole site. <br><br>  SyncMirror performs replication at RAID level, by analogy with mirrored RAID-60: there are two plexes, Plex0 (main dataset) and Plex1 (mirror), each plex can consist of one or several RAID-DP groups.  Why "how-to"?  Because these two plexes, they are visible as composite, mirror parts of one unit (pool), but in fact, in a normally working system, mirroring is performed at the NVRAM level of the controller.  And as you may already know, <a href="https://habrahabr.ru/post/280616/">WAFL, RAID-DP, NVRAM and NVLOG are all components of one whole</a> disk subsystem <a href="https://habrahabr.ru/post/280616/">architecture</a> , and they can be quite conditionally separated from one another.  An important detail of the SyncMirror technology is the need for full symmetry of the disks in two mirrored pools: the size, type, speed of the disks, the RAID groups must be completely the same.  There are small exceptions to the rules, but for now I will not mention them in order not to mislead the reader. <br><br>  Synchronous replication allows on the one hand to relieve the load on the disk subsystem, replicating only memory, on the other hand to solve Split-Brain and consistency problems (from the point of view of the WAFL structure on the storage system) you need to make sure that the data is written to the remote system: as a result ‚ÄúSynchronization‚Äù, in any storage systems, increases the speed of response for a time equal to the time of sending data to a remote site + the time to confirm this record. <br><br><h5>  SnapMirror vs SyncMirror </h5><br>  Do not confuse <br><br><ol><li>  <i>Sync</i> mirror </li><li>  and <i>snap</i> mirror </li></ol><br>  <i>Sync</i> Mirror replicates the contents of one plex to the second plex which constitute one aggregate: all its RAID groups, conditionally speaking ‚Äúdisk-based‚Äù, where the mirror should contain the same disks with the same number, volume, geometry and speed.  The <i>Sync</i> Mirror MCC runs a replica of the NVLOG.  In the case of local <i>Sync</i> Mirror, both plexes live and are served by the same controller.  And in the case of the SyncMirror MCC, the two halves of the plex live, one on one controller, and the other on the remote.  At one time, in normal operation of the storage system, only one plex is active and working, the second only stores a copy of the information. <br><br><img src="https://habrastorage.org/files/be8/9d4/68c/be89d468c84949b1a3d7908d830712b6.png"><br><br>  Each unit can contain one or more Volum FlexVol (data container), each Volum is spread evenly across all the disks in the unit.  Each volum is a separate WAFL structure.  In the case of <i>Snap</i> Mirror, the replica runs at the WAFL level and can run on disks with a completely different geometry, number, and volume. <br><br><img src="https://habrastorage.org/files/2b1/fb9/448/2b1fb9448d74497db481e062d23200b1.png"><br><br>  If you delve into technology, then in fact, both <i>Snap</i> and <i>Sync</i> Mirror use snapshots to replicate data, but in the case of <i>Sync</i> Mirror these are system snapshots based on the CP event (NVRAM / NVLOG) + snapshots at the aggregate level, and in the case of SnapMirror this FlexVol (WAFL) snapshots. <br><br>  SnapMirror and SyncMirror can easily coexist with each other, so you can replicate data from / to the metro cluster from another storage system with ONTAP firmware. <br><br><h5>  Memory and NVRAM </h5><br>  In order to protect data from Split-Brain, the data that is written to enter NVRAM as logs (NVLOG) and into system memory.  Record confirmation to the host will come only after they get into NVRAM of one local controller, its neighbor (if the MCC consists of 4 nodes) and one remote neighbor.  Synchronization between local controllers is performed via HA interconnect (this is usually an internal bus or sometimes it is an external connection), and synchronization to a remote node is performed via FC-IV ports.  The local partner has only a copy of NVLOG, it does not create a full copy of the data, because it already sees disks with this data of its ON neighbor.  A remote DR partner has a copy of NVLOG and has a complete copy of all data (in Plex 1) on its own disks.  This scheme allows you to switch within the site if the second HA controller of the pair has survived or switch to the second site if all the local storage nodes have failed.  switching to the second site takes a couple of seconds. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/c72/91e/db6/c7291edb674d4441993dbaf1049924e7.png"></div><br>  The picture shows a diagram for a four-node metro cluster.  The two-node scheme is similar, but has no local ON partner.  The eight-node scheme is the same two four node schemes: i.e.  In this configuration, the NVRAM replica runs within these 4 nodes, and the combination of two such four node configurations allows you to transparently move data between the metro cluster nodes within each site. <br><br>  NVRAM complements the SyncMirror technology: after the data is received in NVRAM of the remote storage, the record confirmation is immediately received, ie the RAID comes to a fully synchronous state on the second plex with a delay, without compromising the consistency of the mirror copy - this allows the response speed to be significantly accelerated when mirroring the halves of the metro cluster . <br><br><h5>  <a href="https://habr.com/ru/post/279989/">Tie breaker witness</a> </h5><a name="TieBreaker"></a><br>  In order to perform automatic switching between two sites, human intervention is necessary, or a third node, an impartial and all-seeing arbiter who can decide which of the sites should survive after the accident, is called a TieBreaker.  TieBreaker is either free software from NetApp for Windows Server or specialized hardware <a href="http://www.prolion.at/en/clusterlion.html">ClusterLion</a> . <br><br><img src="https://habrastorage.org/files/5ca/f9e/652/5caf9e6529f84503bf4432582d4771a7.png"><br><br><h5>  OnCommand Unified Manager (OCUM) </h5><br>  If TieBreaker is not installed, you can switch between sites manually from the free utility OnCommand Unified Manager (OCUM) or from the command line using the commands <i><a href="https://library.netapp.com/ecmdocs/ECMP1636018/html/GUID-452E291D-1F02-442A-A16D-3E5196A25FE6.html">metrocluster switchover</a></i> . <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/XSaNrpOo5Ko" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h4>  All-Flash </h4><br>  MCC supports All Flash FAS systems for Fabric-Attached and Bridge-Attached Stretch MetroCluster configurations, it is recommended to use ATTO 7500N FibreBridge bridges. <br><br><h4>  FlexArray </h4><br>  The virtualization technology FlexArray allows you to use third-party storage systems as a back-end, connecting them using the FCP protocol.  You may not have native shelves with NetApp disk shelves.  Third-party storage systems can be connected through the same FC factories as for FC-VI connectivity; this can save a lot of money both on the fact that the Fabric-Attached MetroCluster scheme eliminates the need for FC-SAS bridges, and on the fact that existing ones can be disposed of. allowing you to save investments by disposing of old storage systems.  FlexArray requires that the storage system be in the <a href="http://habrahabr.ru/post/154205/">compatibility matrix</a> . <br><br><h4>  VMware vSphere Metro Storage Cluster </h4><br>  VMware can use with <a href="https://kb.vmware.com/kb/2031038">MCC to provide HA</a> based on NetApp hardware replication.  As with <a href="https://habrahabr.ru/post/279993/">SRM / SRA,</a> this is a plugin for vCenter that can interact with MetroCluster TieBreaker to provide automatic switching in the event of a crash. <br><br><img src="https://habrastorage.org/files/e69/c99/afb/e69c99afb1b04ee796b4227c12ccb6ef.JPG"><br><br><h4>  VMware VVOL </h4><br>  <a href="https://habrahabr.ru/post/321366/">VVOL</a> technology <a href="https://habrahabr.ru/post/321366/">is</a> supported with vMSC. <br><br><h4>  findings </h4><br>  The MCC technology is designed to create highly available storage and highly available services on top of it.  Using hardware replication SyncMirror allows you to replicate very large critical corporate infrastructures and, in the event of a disaster, automatically or manually switch between sites while protecting against Split-Brain.  The MCC is designed in such a way that for end hosts it looks like a single device, and the switch for the host is performed at the network fault tolerance level.  This allows the integration of the MCC with almost any solution. <br><br>  This may contain links to Habra articles that will be published later. <br>  I ask to send messages on errors in the text to the <abbr title="Private message">LAN</abbr> . <br>  Comments, additions and questions on the article on the contrary, please in the comments. </div><p>Source: <a href="https://habr.com/ru/post/279989/">https://habr.com/ru/post/279989/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../279977/index.html">Development of service for downloading albums</a></li>
<li><a href="../279979/index.html">Five steps to optimize the performance of the application for Android</a></li>
<li><a href="../279983/index.html">IBM will help eliminate the effects of cyber attacks</a></li>
<li><a href="../279985/index.html">Preparing ASP.NET Core: more about working with a modular framework</a></li>
<li><a href="../279987/index.html">How to become a leader in the ranking of Gartner</a></li>
<li><a href="../279991/index.html">Mobius Announcement 2016: Mobile Development Conference</a></li>
<li><a href="../279995/index.html">Your main desktop environment in linux for 2016 is NOT READING servers</a></li>
<li><a href="../279997/index.html">I decided to write my own language of hardware synthesis, for Minecraft and what came of it</a></li>
<li><a href="../279999/index.html">Under heavy load: our uses for Tarantool</a></li>
<li><a href="../280003/index.html">360 Total Security Antivirus review</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>