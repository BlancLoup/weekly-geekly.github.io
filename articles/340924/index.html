<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Using the neural network to build a model for assessing borrowers in the field of online microfinance</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Currently, to build a scoring model, the de facto standard in the financial industry is the use of logistic regression functions (logit functions). Th...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Using the neural network to build a model for assessing borrowers in the field of online microfinance</h1><div class="post__text post__text-html js-mediator-article">  Currently, to build a scoring model, the de facto standard in the financial industry is the use of logistic regression functions (logit functions).  The essence of the method is reduced to finding such a linear combination of initial data (predictors), which as a result of the logit-transformation will be the most likely to make predictions. <br><br>  A practical disadvantage of the method is the need for long-term data preparation for building a model (about a week of specialist work).  In real life conditions of a microfinance company, a set of data on borrowers is constantly changing, various data providers are connected and disconnected, loan generations change - the preparation stage becomes a bottleneck. <br><br>  Another disadvantage of logit functions is related to their linearity ‚Äî the effect of each individual predictor on the final result is uniform over the entire set of predictor values. <br>  Models based on neural networks are devoid of these shortcomings, but are rarely used in the industry - there are no reliable methods for evaluating retraining, a great influence of ‚Äúnoisy‚Äù values ‚Äã‚Äãin the source data. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Below we will show how using various methods of optimizing a model based on neural networks, we can get a better prediction result compared to models based on logit functions. <br><br><a name="habracut"></a><br><h2>  1. Statement of the problem of simplifying the structure of a mathematical model and its solution using nonsmooth regularization methods (by the example of a linear model) </h2><br><h3>  1.1 Statement of the problem of building a model </h3><br>  Most applied research has the goal of establishing a natural relationship between some measurable quantity and several factors. <br><p><math> </math> $$ display $$ E (y / x) = f (x) = f (x, w), \ \ mathit {w \ theta} \ in W \ subset R ^ m, \ x \ in \ Omega \ subset R ^ n, \ \ \ \ (1) $$ display $$ </p><br>  Where <math> </math> $ inline $ E (y / x) $ inline $   - average value of the observed value <math> </math> $ inline $ y $ inline $   dependent on variables <math> </math> $ inline $ x, W $ inline $   and <math> </math> $ inline $ \ Omega $ inline $   - admissible sets of parameters <math> </math> $ inline $ w $ inline $   and <math> </math> $ inline $ x $ inline $   .  Dependency recovery is based on observation data. <br><p><math> </math> $$ display $$ D = \ {\; (x ^ i, y_i) | x ^ i \ in R ^ n, \; \; \; \; y_i \ in R ^ 1; \ i = 1, .. ., N \}. \ \ \ \ (2) $$ display $$ </p><br>  Parameter Estimates <math> </math> $ inline $ w $ inline $   can be obtained, for example, by the method of least squares <br><p><math> </math> $$ display $$ w = \ text {arg} \ underset {\ theta \ in \ Theta} {\ text {min}} \; \; E (w, D), \ \ E (w, D) = \ overset N {\ underset {i = 1} {\ sum}} [y_i-f (x ^ i, w)] ^ 2. \ \ \ \ (3) $$ display $$ </p><br><h3>  1.2.  Linear model </h3><br>  In the problem of constructing a linear model, it is required from the data D to build a model of the following type (estimate its unknown parameters <math> </math> $ inline $ w $ inline $   ) <br><p><math> </math> $$ display $$ f (x, w) = w_0 + \ sum _ {i = 1} ^ mw_ix_i, \ \ \ \ (4) $$ display $$ </p><br>  Where <math> </math> $ inline $ x_ {j (i)} $ inline $   - vector components <math> </math> $ inline $ x \ in R ^ n $, $ w = ([w_0, w_i, i = 1, ..., m]) $ inline $   - a set of unknown parameters that must be estimated using the least squares method (3), <math> </math> $ inline $ m $ inline $   - number of informative vector components <math> </math> $ inline $ x \ in R ^ n $ inline $   involved in the model, the n-dimension of the vector <math> </math> $ inline $ x $ inline $   . <br><br><h3>  1.3.  Logit models </h3><br>  Logit model has the form <p><math> </math> $$ display $$ f (x, w) = \; \ varphi (s), \ \ \ \ (5) $$ display $$ </p>  Where <p><math> </math> $$ display $$ s (x, w) = w_0 + \ sum _ {i = 1} ^ mw_ix_i, \ \ \ \ (6) $$ display $$ </p>  and the activation function can be set by one of the following types <p><math> </math> $$ display $$ \ varphi (s) = \ frac {s} {1+ | s |}, \ \ \ varphi '(s) = \ frac {1} {(1+ | s |) ^ 2}; \ \ \ (7) $$ display $$ </p><p><math> </math> $$ display $$ \ varphi (s) = \ frac 1 {1+ \ text {exp} (- s)}, \ \ \ varphi '(s) = \ varphi (s) (1- \ varphi (s) ), \ \ \ \ (8) $$ display $$ </p><p><math> </math> $$ display $$ \ varphi (s) = s, \ \ \ \ \ varphi '(s) = 1. \ \ \ \ (9) $$ display $$ </p><br>  The last of the functions is linear.  Along with (7) - (8), it can be used to compare the quality of approximation with (7) - (8). <br><br><h3>  1.4.  Two-layer sigmoidal neural network (with one hidden layer) </h3><br>  In the problem of approximation by a network of direct distribution, it is required according to <math> </math> $ inline $ D $ inline $   train the two-layer sigmoid neural network (NS) of the following type (estimate its unknown parameters <math> </math> $ inline $ w $ inline $   ) <br><p><math> </math> $$ display $$ f (x, w) = w_0 ^ {(2)} + \ sum _ {i = 1} ^ mw_i ^ {(2)} \; \ varphi \; (\ sum _ {j = 1 } ^ nx_j \; w _ {{\ text {ij}}} ^ {(1)} \; \; + w _ {\ mathit {i0}} ^ {(1)}), \ \ \ \ (10) $ $ display $$ </p><br>  Where <math> </math> $ inline $ x_j \; $ inline $   - vector components <math> </math> $ inline $ x \ in R ^ n $ inline $   , <math> </math> $ inline $ w = ([w_0 ^ {(2)}, w_i ^ {(2)}, i = 1, ..., m]; $ inline $ <br><math> </math> $ inline $ \; \; \; [w _ {\ mathit {i0}} ^ {(1)}, (w _ {{\ text {ij}}} ^ {(1)}, \; \; j = 1 , ..., n)] \;, \; i = 1, ..., m) $ inline $   - a set of unknown parameters that must be estimated using the least squares method (3), <math> </math> $ inline $ \ varphi (s) $ inline $   - activation function of the neuron, <math> </math> $ inline $ m $ inline $   - the number of neurons <math> </math> $ inline $ n $ inline $   - vector dimension <math> </math> $ inline $ x $ inline $   . <br><br><h3>  1.5.  Sigmoid neural network activation functions </h3><br>  We present the activation functions of the sigmoid species and their derivatives, which we will use: <br><p><math> </math> $$ display $$ \ varphi (s) = \ frac {s} {1+ | s |}, \ \ \ \ \ varphi '(s) = \ frac {1} {(1+ | s |) ^ 2 }; \ \ \ \ (11) $$ display $$ </p><br><p><math> </math> $$ display $$ \ varphi (s) = \ frac {1} {1+ \ text {exp} (- s)}, \ \ \ \ \ varphi '(s) = \ varphi (s) (1- \ varphi (s)). \ \ \ \ (12) $$ display $$ </p><br><p><math> </math> $$ display $$ \ varphi (s) = s, \ \ \ \ \ varphi '(s) = 1. \ \ \ \ (13) $$ display $$ </p><br><br><h3>  1.6.  Input Preprocessing </h3><br>  The main purpose of data preprocessing is to maximize the entropy of the input data.  When all values ‚Äã‚Äãof a variable are the same, then it does not carry information.  And, on the contrary, if the values ‚Äã‚Äãof a variable are uniformly distributed over a given interval, then its entropy is maximum. <br><br>  To transform the components of variables in order to increase the degree of uniformity of the components of a variable, use the logit model formula <br><p><math> </math> $$ display $$ x_i ^ + = 0.5- \ frac {1} {1+ \ text {exp} (- (x_i- \ bar x_i) / \ sigma _i)}, \; \; \; \; \; \; \; \; \; \ sigma _i = [(x_i- \ bar x_i) ^ 2 / N] ^ {0.5} \ \ \ \ (14) $$ display $$ </p><br><br><h3>  1.7.  Suppression of redundant variables and smoothing </h3><br>  To suppress redundant variables, prior training should be done by minimizing <math> </math> $ inline $ w $ inline $   quadratic error and non-smooth smoothing functional <br><p><math> </math> $$ display $$ E _ {\ Omega} \ left (\ alpha, w, D \ right) = \ underset {x, y \ in D} {\ sum} \ left (yf (x, w) \ right) ^ 2+ \ alpha \ Omega (w), \ \ \ \ (15) $$ display $$ </p><br><p><math> </math> $$ display $$ \ Omega \ left (w \ right) = \ underset {i \ in {\ text {Iw}}} {\ sum} | w_i | ^ {\ gamma}, \ \ 0 &lt;\ gamma &lt;1 , \ \ \ \ (16) $$ display $$ </p><br>  Where <math> </math> $ inline $ \ alpha $ inline $   - regularization parameter, <math> </math> $ inline $ {\ text {Iw}} $ inline $   - set of numbers of array variables <math> </math> $ inline $ w $ inline $   - on which regularization is carried out.  Functional <math> </math> $ inline $ \ Omega \ left (w \ right) $ inline $   designed to suppress redundant model variables <math> </math> $ inline $ f (x, w) $ inline $   .  Therefore, the solution will contain a set of components close to zero, which must be eliminated using special algorithms. <br><br><h2>  2. Smoothing Functionals for Smoothing and Suppressing Excess Variables </h2><br><h3>  2.1.  Smooth regularization </h3><br>  Derivatives of a functional like (8) <p><math> </math> $$ display $$ \ Omega \ left (w \ right) = \ overset n {\ underset {i = 1} {\ sum}} | w_i | ^ {\ gamma}, \ \ \ \ 0 &lt;\ gamma &lt;1 \ \ \ \ (17) $$ display $$ </p><br>  have the following form <br><p><math> </math> $$ display $$ \ frac {\ partial \ Omega \ left (w \ right)} {\ partial w_i} = \ frac {\ beta \; {\ text {sign}} (w_i)} {| w_i | ^ { 1- \ gamma}}, \ \ i = 1, ..., n, \ \ 0 &lt;\ gamma &lt;1. \ \ \ \ (18) $$ display $$ </p><br>  With <math> </math> $ inline $ w_i \ rightarrow 0 $ inline $   they will be arbitrarily large.  This means that the angles of the stars - level surfaces degenerate into needles, which slows down the rate of convergence of minimization methods and leads to emergency premature stops. <br>  The level lines of the functional (10) (the level lines of a star-like type) are shown in Figure 1. <br><br><img src="https://img-fotki.yandex.ru/get/368796/455254629.0/0_1c0419_763dab02_orig" alt="Figure 1. Functional level lines"><br>  In fig.  1 shows the interaction of two functionals (the main and smoothing) and the directions of their gradients and the resulting gradient. <br><br><h3>  2.2.  A special case of nonsmooth regularization (Occam's razor) </h3><br>  Consider (8) subject to <math> </math> $ inline $ \ gamma = 1 $ inline $ <br><p><math> </math> $$ display $$ \ Omega \ left (w \ right) = \ overset n {\ underset {i = 1} {\ sum}} | w_i |. \ \ \ \ (19) $$ display $$ </p><br>  Derivatives (10) have the following form <br><p><math> </math> $$ display $$ \ frac {\ partial \ Omega \ left (w \ right)} {\ partial w_i} = {\ text {sign}} (w_i), \ i = 1, ..., n. \ \ \ \ (20) $$ display $$ </p><br>  The level surfaces have the form of rectangles symmetrically located relative to zero and rotated by 45 degrees.  Function (10) is not smooth. <br><br><h3>  2.3.  Smooth regularized bounded derivatives </h3><br>  In the following functional, we will get rid of the presence of angles degenerating into needles. <br><p><math> </math> $$ display $$ \ Omega \ left (w \ right) = \ overset n {\ underset {i = 1} {\ sum}} (| w_i | + \ varepsilon) ^ {\ gamma}, \ \ \ \ \ \ varepsilon&gt; 0, \ \ \ \ 0 &lt;\ gamma &lt;1 \ \ \ \ (21) $$ display $$ </p><p><math> </math> $$ display $$ \ frac {\ partial \ Omega \ left (w \ right)} {\ partial w_i} = \ frac {\ beta \; {\ text {sign}} (w_i)} {(| w_i | + \ varepsilon) ^ {1- \ gamma}}, \ \ i = 1, ..., n, \ \ 0 &lt;\ gamma &lt;1. \ \ \ \ (22) $$ display $$ </p><br>  The disadvantage (10) is heterogeneous sensitivity to the parameter <math> </math> $ inline $ \ varepsilon $ inline $   with variations of the orders of the estimated parameters <math> </math> $ inline $ w $ inline $   for various neural networks. <br><br><h3>  2.4.  Smooth homogeneous regularization with bounded derivatives </h3><br>  In the next functional, we will get rid of the heterogeneity in the parameters <math> </math> $ inline $ w $ inline $ <br><p><math> </math> $$ display $$ \ Omega \ left (w \ right) = \ overset n {\ underset {i = 1} {\ sum}} (| w_i | + \ varepsilon \ overset n {\ underset {i = 1} { \ sum}} | w_i |) ^ {\ gamma} = \ overset n {\ underset {i = 1} {\ sum}} v_i ^ {\ gamma} = \ overset n {\ underset {i = 1} {\ sum}} f_i, \ \ varepsilon&gt; 0, \ \ varepsilon \ approx 10 ^ {- 4}, \ 0 &lt;\ gamma &lt;1, \ \ \ \ (23) $$ display $$ </p><p><math> </math> $$ display $$ \ frac {\ partial \ Omega \ left (w \ right)} {\ partial w_k} = \ frac {\ beta \ sign (w_k)} {(| w_k | + \ varepsilon \ overset n {\ underset {i = 1} {\ sum}} | w_k |) ^ {1- \ gamma}} + \ overset n {\ underset {i = 1} {\ sum}} \ frac {\ varepsilon \ beta \ sign ( w_k)} {(| w_i | + \ varepsilon \ overset n {\ underset {i = 1} {\ sum}} | w_i |) ^ {1- \ gamma}} = \\ = \ gamma \ sign (w_k) \ left [\ frac {f_k} {v_k} + \ varepsilon \ overset n {\ underset {i = 1} {\ sum} \ frac {f_i} {v_i} \ right] = \ gamma \; {sign} ( w_k) \ left [\ frac 1 {v_k ^ {1- \ gamma}} + \ varepsilon \ overset n {\ underset {i = 1} {\ sum}} \ frac {1_i} {v_i ^ {1- \ gamma }} \ right], \ k = 1, ..., n.  \ \ \ \ (24) $$ display $$ </p><br>  Convert (12) <br><p><math> </math> $$ display $$ \ Omega \ left (w \ right) = \ overset n {\ underset {i = 1} {\ sum}} (| w_i | + \ varepsilon \ overset n {\ underset {i = 1} { \ sum}} | w_i |) ^ {\ gamma} = \ left (\ overset n {\ underset {i = 1} {\ sum}} | w_i | \ right) ^ {\ gamma} \ overset n {\ underset {i = 1} {\ sum}} \ left (\ frac {| w_i |} {\ overset n {\ underset {i = 1} {\ sum}} | w_i |} + \ varepsilon \ right) ^ {\ gamma}, \ 0 &lt;\ gamma &lt;1 \ \ \ \ (25) $$ display $$ </p><br>  We introduce the normalized variables <br><p><math> </math> $$ display $$ z_i = \ frac {| w_i |} {\ overset n {\ underset {i = 1} {\ sum}} | w_i |}, \ \ z_i \ in [0, \; \;  1], \ \ \ overset n {\ underset {i = 1} {\ sum}} | z_i | = 1. \ \ \ \ (26) $$ display $$ </p><br>  Then (16) will take the form <br><p><math> </math> $$ display $$ \ Omega \ left (w \ right) = \ left (\ overset n {\ underset {i = 1} {\ sum}} | w_i | \ right) ^ {\ gamma} \ overset n {\ underset {i = 1} {\ sum}} \ left (\ frac {| w_i |} {\ overset n {\ underset {i = 1} {\ sum}} | w_i |} + \ varepsilon \ right) ^ { \ gamma} = \ left (\ overset n {\ underset {i = 1} {\ sum}} | w_i | \ right) ^ {\ gamma} \ overset n {\ underset {i = 1} {\ sum}} (z_i + \ varepsilon) ^ {\ gamma}, \ \ 0 &lt;\ gamma &lt;1, \ \ (27) $$ display $$ </p><br>  Denote the structure of the function.  Here is the first factor <math> </math> $ inline $ \ left (\ overset n {\ underset {i = 1} {\ sum}} | w_i | \ right) ^ {\ gamma} $ inline $   is a homogeneous function of degree <math> </math> $ inline $ \ gamma $ inline $   and displays the overall growth of the function.  The second factor in (16) is <br>  homogeneous function of zero degree and determines the behavior of the function depending on the structure of the proportions between the variables. <br><br>  We denote the properties of the functional (16), which determine its effectiveness. <br><ol><li>  Level surfaces form similar shapes relative to the origin.  The latter means the independence of the regularization of the scale of variables. </li><li>  The multiplier of the total growth of the function is a concave function, which determines the presence of extremes on the coordinate axes and therefore determines the properties of the possibility of deleting variables. </li><li>  The degree of concavity is set by parameter <math> </math> $ inline $ \ beta $ inline $   which can be chosen optimally on the basis of a preliminary computational experiment and further when calculating on this type of networks does not change </li><li>  The structure of the corner points is determined by the parameter <math> </math> $ inline $ \ varepsilon $ inline $   which can be chosen optimally on the basis of a preliminary computational experiment and further when calculating on this type of networks does not change </li></ol><br>  The following functions will be considered in order to have the properties we have identified, which are necessary to eliminate redundant variables. <br><br><h3>  2.5.  Quadratic regularization (Tikhonov regularization) </h3><br>  Derivatives of a quadratic function <br><p><math> </math> $$ display $$ \ Omega \ left (w \ right) = \ overset n {\ underset {i = 1} {\ sum}} w_i ^ 2 \ \ \ \ (28) $ display $$ </p><br>  have the following form <br><p><math> </math> $$ display $$ \ frac {\ partial \ Omega \ left (w \ right)} {\ partial w_i} = 2w_i, i = 1, ..., n. \ \ \ \ (29) $$ display $$ </p><br>  It does not allow solving the problem of eliminating redundant variables, since it does not have property 2. <br><br><h2>  3. Results of numerical research </h2><br>  Logit-models and sigmoidal neural networks with non-smooth uniform regularization and Tikhonov's quadratic regularization were investigated on test and real data. <br><br><h3>  3.1.  Research on real data of various models </h3><br>  Recovery of various dependencies <math> </math> $ inline $ f (x, w) $ inline $   made on the basis of observation data <br><p><math> </math> $$ display $$ D = \ {\; (x ^ i, y_i) | x ^ i \ in R ^ n, \; \; \; \; y_i = \ {0,1 \}; \; \; \; \; i = 1, ..., N \} \ \ \ \ (1) $$ display $$ </p><br>  where as quantities <math> </math> $ inline $ y_i $ inline $   used default characteristics <math> </math> $ inline $ (y_i = 1) $ inline $   or lack of default <math> </math> $ inline $ (y_i = 0) $ inline $   .  Estimates of unknown model parameters <math> </math> $ inline $ w $ inline $   produced by the least squares method <br><p><math> </math> $$ display $$ w = \ text {arg} \ \ underset {\ theta \ in \ Theta} {\ text {min}} \; \; E (w, D), \ \ \ \ E (w, D ) = \ overset N {\ underset {i = 1} {\ sum}} [y_i-f (x ^ i, w)] ^ 2. \ \ \ \ (2) $$ display $$ </p><br>  Conducted pre-processing of input data.  The main purpose of data preprocessing is to maximize the entropy of the input data.  When all values ‚Äã‚Äãof a variable are the same, then it does not carry information.  And, on the contrary, if the values ‚Äã‚Äãof a variable are uniformly distributed on a given interval, then its entropy is maximum. <br>  To transform the components of variables in order to increase the degree of uniformity of the components of a variable, use the logit-model formula <br><p><math> </math> $$ display $$ x_i ^ + = 0.5- \ frac 1 {1+ \ text {exp} (- (x_i- \ bar x_i) / \ sigma _i)}, \; \; \; \; \; \; \; \; \; \ sigma _i = [(x_i- \ bar x_i) ^ 2 / N] ^ {0.5}. \ \ \ \ (3) $$ display $$ </p><br>  The quality of the models was evaluated on the basis of the AUC characteristic, which determines the area under <br>  ROC curve. <br>  The error curve or ROC curve is a graphical characteristic of the quality of a binary classifier, the dependence of the proportion of true positive classifications on the proportion of false positive classifications when the threshold of the decision rule is varied. <br>  The advantage of the ROC curve is its invariance with respect to the price ratio of the error of type I and II. <br>  The area under the AUC ROC-curve (Area Under Curve) is an aggregated characteristic of the quality of the classification, independent of the price ratio of errors.  The higher the AUC value, the ‚Äúbetter‚Äù the classification model.  This indicator is often used for comparative analysis of several classification models. <br><br><h3>  3.2.  The study of Logit-models with different types of regularization </h3><br>  Logit model <br><p><math> </math> $$ display $$ f (x, w) = \; \ varphi (s), \\\\ s (x, w) = w_0 + \ sum _ {i = 1} ^ mw_ix_i, \ \ \ \ \ (4) $$ display $$ </p><br>  used with three kinds of activation function <br><p><math> </math> $$ display $$ \ varphi (s) = s, \ \ \ \ \ varphi '(s) = 1. \ \ \ \ (5) $$ display $$ </p><br><p><math> </math> $$ display $$ \ varphi (s) = \ frac s {1+ | s |}, \ \ \ \ \ varphi '(s) = \ frac {1} {(1+ | s |) ^ 2}; \ \ \ \ (6) $$ display $$ </p><br><p><math> </math> $$ display $$ \ varphi (s) = \ frac {1} {1+ \ text {exp} (- s)}, \ \ \ \ \ varphi '(s) = \ varphi (s) (1- \ varphi (s)), \ \ \ \ (7) $$ display $$ </p><br>  which we will denote respectively LIN, ABS and EXP.  Model coefficients were found by minimizing the function. <br><p><math> </math> $$ display $$ E _ {\ Omega} \ left (\ alpha, w, D \ right) = \ underset {x, y \ in D} {\ sum} \ left (yf (x, w) \ right) ^ 2+ \ alpha \ Omega (w) \ \ \ \ (8) $$ display $$ </p><br>  As <math> </math> $ inline $ \ Omega (w) $ inline $   The quadratic Tikhonov regularization function was used. <br><p><math> </math> $$ display $$ \ Omega _2 \ left (w \ right) = \ overset n {\ underset {i = 1} {\ sum}} w_i ^ 2 \ \ \ \ (9) $$ display $$ </p><br>  and non-smooth homogeneous function with non-smooth regularization <br><p><math> </math> $$ display $$ \ Omega _ {\ gamma} \ left (w \ right) = \ overset n {\ underset {i = 1} {\ sum} (| w_i | + \ varepsilon \ overset n {\ underset { i = 1} {\ sum}} | w_i |) ^ {\ gamma}, \ \ \ varepsilon&gt; 0, \ \ \ varepsilon \ approx 10 ^ {- 6}, \ \ \ \ 0 &lt;\ gamma &lt;1 \ \ \ \ (10) $$ display $$ </p><br>  The regularization algorithm was attended by 2 stages.  Some initial value was selected. <math> </math> $ inline $ \ alpha _0 $ inline $   , and on subsequent iterations <math> </math> $ inline $ \ alpha _ {k + 1} = 2 \ alpha _k $ inline $   it was a doubling <math> </math> $ inline $ \ alpha _ {k + 1} = 2 \ alpha _k $ inline $   .  With such values, the model was calculated and the variables were removed with excessively small coefficients.  At each iteration, a model with some small value was also calculated. <math> </math> $ inline $ \ alpha _ {\ text {min}} $ inline $   .  This method involves smoothing and deleting variables for large regularization parameters. <math> </math> $ inline $ \ alpha _k $ inline $   and free model building at small values.  Models with small regularization parameters can be useful in the assumption that the variables remaining after deletion are significant for the construction of the model. <br>  The following table shows the results of calculations of the model, the number of variables is nx = 254. <br><br><img src="https://img-fotki.yandex.ru/get/478076/455254629.0/0_1c04d6_e3747979_orig" alt="image"><br><br>  AUC_O - AUC on the training set <br>  AUC_T - AUC on the test set <br><br><h3>  3.3.  Conclusions study on the real data Logit-models </h3><br>  The best variants of models with quadratic regularization are obtained by means of a scenario with preliminary removal of a part of the model coefficients with large regularization parameters with subsequent calculation of model parameters with small regularization coefficients.  Such scenarios require large regularization parameters, which can lead to the removal of significant components of the model. <br><br>  The optimal model with non-smooth optimization was obtained for small values ‚Äã‚Äãof the regularization parameters, which allows us to conclude that there is a simultaneous effect of removing weak variables and smoothing the rest of the variables. <br><br>  A comparison of the average AUC_O and AUC_T models indicates that more efficient models are obtained on the basis of nonsmooth optimization. <br><br>  Average results for logit models <br><br><img src="https://img-fotki.yandex.ru/get/516848/455254629.0/0_1c04d7_963b9ad2_orig" alt="image"><br><br><h3>  3.4.  The study of neural network models with different types of regularization </h3><br>  Two-layer sigmoidal neural networks (with one hidden layer) were built.  In the problem of approximation by a network of direct propagation, it is required, according to data D, to train the following two-layer sigmoidal neural network (NN) (estimate its unknown parameters <math> </math> $ inline $ w $ inline $   ) <br><p><math> </math> $$ display $$ f (x, w) = w_0 ^ {(2)} + \ sum _ {i = 1} ^ mw_i ^ {(2)} \; \ varphi \; (\ sum _ {j = 1 } ^ nx_j \; w _ {{\ text {ij}}} ^ {(1)} \; \; + w _ {\ mathit {i0}} ^ {(1)}), \ \ \ \ (11) $ $ display $$ </p><br>  Where <math> </math> $ inline $ x_j $ inline $   - vector components <math> </math> $ inline $ x \ in R ^ n $ inline $   , <math> </math> $ inline $ w = ([w_0 ^ {(2)}, w_i ^ {(2)}, i = 1, ..., m]; [w _ {\ mathit {i0}} ^ {(1)}, (w _ {{\ text {ij}}} ^ {(1)}, \; \; j = 1, ..., n)] \;, \; i = 1, ..., m) $ inline $   - a set of unknown parameters that must be estimated using the least squares method (3), <math> </math> $ inline $ \ varphi (s) $ inline $   - activation function of the neuron, <math> </math> $ inline $ m $ inline $   - the number of neurons <math> </math> $ inline $ n $ inline $   - vector dimension <math> </math> $ inline $ x $ inline $   . <br>  The neural network model was used with two types of activation function. <br><p><math> </math> $$ display $$ \ varphi (s) = \ frac {s} {1+ | s |}, \ \ varphi '(s) = \ frac {1} {(1+ | s |) ^ 2}; \ \ \ \ (12) $$ display $$ </p><br><p><math> </math> $$ display $$ \ varphi (s) = \ frac {1} {1+ \ text {exp} (- s)}, \ \ \ \ \ varphi '(s) = \ varphi (s) (1- \ varphi (s)) \ \ \ \ (13) $$ display $$ </p><br>  which we will denote respectively LIN, ABS and EXP.  Model coefficients were found by minimizing the function. <br><p><math> </math> $$ display $$ E _ {\ Omega} \ left (\ alpha, w, D \ right) = \ underset {x, y \ in D} {\ sum} \ left (yf (x, w) \ right) ^ 2+ \ alpha \ Omega (w) \ \ \ \ (14) $$ display $$ </p><br>  As <math> </math> $ inline $ \ Omega (w) $ inline $   The quadratic Tikhonov regularization function was used. <br><p><math> </math> $$ display $$ \ Omega _2 \ left (w \ right) = \ overset n {\ underset {i = 1} {\ sum}} w_i ^ 2 \ \ \ \ (15) $$ display $$ </p><br>  and non-smooth homogeneous function with non-smooth regularization <br><p><math> </math> $$ display $$ \ Omega _ {\ gamma} \ left (w \ right) = \ overset n {\ underset {i = 1} {\ sum} (| w_i | + \ varepsilon \ overset n {\ underset { i = 1} {\ sum}} | w_i |) ^ {\ gamma}, \ varepsilon&gt; 0, \ \ varepsilon \ approx 10 ^ {- 6}, \ \ 0 &lt;\ gamma &lt;1 \ \ \ \ (16 ) $$ display $$ </p><br>  The regularization algorithm was attended by 2 stages.  Some initial value was selected. <math> </math> $ inline $ \ alpha _ {00} $ inline $   , and on subsequent iterations <math> </math> $ inline $ \ alpha _ {k + 1} = 2 \ alpha _k $ inline $   it was a doubling <math> </math> $ inline $ \ alpha _ {k + 1} = 2 \ alpha _k $ inline $   .  With such values, the model was calculated and the variables were removed with excessively small coefficients.  At each iteration, a model with some small value was also calculated. <math> </math> $ inline $ \ alpha _ {\ text {min}} $ inline $   .  This method involves smoothing and deleting variables for large regularization parameters. <math> </math> $ inline $ \ alpha _k $ inline $   and free model building at small values.  Models with small regularization parameters can be useful in the assumption that the variables remaining after deletion are significant for the construction of the model. <br><br>  Results for neural network without fixing centers.  The following tables show the results of calculations of models, the number of variables which <math> </math> $ inline $ nx $ inline $   = 254. <br><br><img src="https://img-fotki.yandex.ru/get/478076/455254629.0/0_1c04d8_2090eddd_orig" alt="image"><br><br>  Results for neural network with fixing centers. <br><br><img src="https://img-fotki.yandex.ru/get/874316/455254629.0/0_1c04d9_fc4f5d05_orig" alt="image"><br><br>  AUC_O - AUC on the training set <br>  AUC_T - AUC on the test set <br><br><h3>  3.5.  Conclusions of the research on real data of neural network models </h3><br>  The best variants of models with quadratic regularization are obtained by means of a scenario with preliminary removal of a part of the model coefficients with large regularization parameters with subsequent calculation of model parameters with small regularization coefficients.  Such scenarios require large regularization parameters, which can lead to the removal of significant components of the model. <br><br>  The optimal model with non-smooth optimization was obtained for small values ‚Äã‚Äãof the regularization parameters, which allows us to conclude that there is a simultaneous effect of removing weak variables and smoothing the rest of the variables. <br><br>  A comparison of the average AUC_O and AUC_T models indicates that more efficient models are obtained on the basis of nonsmooth optimization. <br><br>  The second conclusion is that the preliminary fixation of the working areas of the neurons has a positive effect on obtaining a more efficient neural network model.  Fixing neurons at the first stage does not allow the work areas to leave the data area, thereby leaving all the neurons working. <br><br>  Average results for neural networks without fixing centers. <br><br><img src="https://img-fotki.yandex.ru/get/516848/455254629.0/0_1c04da_372d644d_orig" alt="image"><br><br>  Average results for neural networks with fixing centers. <br><br><img src="https://img-fotki.yandex.ru/get/478076/455254629.0/0_1c04db_9775faa9_orig" alt="image"><br><br>  AUC_O - AUC on the training set <br>  AUC_T - AUC on the test set <br><br>  As our practice has shown, sigmoid neural networks with one inner layer can be successfully used to predict a borrower's default and show better results than models based on logistic regression functions.  The drawbacks of models based on neural networks are successfully overcome by preliminary regularization of the input data and the execution of training the model with fixation of the working areas of the neurons. </div><p>Source: <a href="https://habr.com/ru/post/340924/">https://habr.com/ru/post/340924/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../340914/index.html">How to evaluate whether marketing activities will bring profit to the store</a></li>
<li><a href="../340916/index.html">Blockchain eyes of the developer</a></li>
<li><a href="../340918/index.html">Optimization of Viola and Jones method for Elbrus platform</a></li>
<li><a href="../340920/index.html">Automation of work with current customers. Account Management. Choose a CRM system. Part 3</a></li>
<li><a href="../340922/index.html">We explain the modern JavaScript dinosaur</a></li>
<li><a href="../340926/index.html">Web application on Node and Vue, part 2: components, forms, routes</a></li>
<li><a href="../340928/index.html">Under the hood Ethereum Virtual Machine. Part 1 - Solidity basics</a></li>
<li><a href="../340932/index.html">REST in the real world and the practice of hypermedia</a></li>
<li><a href="../340934/index.html">Autumn changes on Habr√© and Geektimes</a></li>
<li><a href="../340936/index.html">Agile in Russia - 82.9% of companies report that they use at least something from Agile. First poll results</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>