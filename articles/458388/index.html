<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Deep (Learning + Random) Forest and parsing articles</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="We continue to talk about the conference on statistics and machine learning AISTATS 2019. In this post we will discuss articles about deep models from...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Deep (Learning + Random) Forest and parsing articles</h1><div class="post__text post__text-html js-mediator-article"><p>  We continue to talk about the conference on statistics and machine learning AISTATS 2019. In this post we will discuss articles about deep models from tree ensembles, mix regularization for highly sparse data and time-effective approximation of cross-validation. </p><br><p><img src="https://habrastorage.org/webt/n-/uv/xj/n-uvxjud1se0puoearqtlott2de.jpeg"></p><a name="habracut"></a><br><h2 id="algoritm-glubokiy-les-an-exploration-to-non-nn-deep-models-based-on-non-differentiable-modules">  Deep Forest Algorithm: An Exploration to Non-NN Deep Models based on Non-Differentiable Modules </h2><br><p>  Zhi-Hua Zhou (Nanjing University) <br>  ‚Üí <a href="http://www.aistats.org/0-AISTATS2019-slides-zhi-hua_zhou.pdf">Presentation</a> <br>  ‚Üí <a href="https://arxiv.org/abs/1702.08835">Article</a> <br>  Implementations - below </p><br><p>  A professor from China spoke about the tree ensemble, which the authors call the first in-depth training on non-differentiable modules.  This may seem too loud a statement, but this professor and his H-index 95 are invited speakers, this fact makes it possible to take the statement more seriously.  The basic theory of Deep Forest has been developed a long time ago, the original article was already in 2017 (almost 200 citations), but the authors write libraries and improve the speed algorithm every year.  And now, it seems, they have reached the level when this beautiful theory can finally be applied in practice. </p><br><p>  <em>General view of the Deep Forest architecture</em> <br><img src="https://habrastorage.org/webt/4k/7q/1q/4k7q1qlzs5rixw4itr5luctdpuq.jpeg"></p><br><p>  <strong>Prerequisites</strong> </p><br><p>  Deep models, which now mean deep neural networks, are used to capture complex dependencies in data.  Moreover, it turned out that increasing the number of layers is more efficient than increasing the number of units on each layer.  But neural networks have their disadvantages: </p><br><ul><li>  It takes a lot of data to not retrain, </li><li>  It takes a lot of computing resources to learn in a reasonable time. </li><li>  Too many hyperparameters that are difficult to configure optimally. </li></ul><br><p>  In addition, elements of deep neural networks are differentiable modules that are not necessarily effective for each task.  Despite the complexity of neural networks, conceptually simple algorithms, like a random forest, often work better or not much worse.  But for such algorithms you need to manually construct features, which is also difficult to do optimally. </p><br><p>  Already, researchers have noticed that the ensembles at Kaggle: "very poverfull to do the practice", and inspired by the words of Scholl and Hinton about the fact that differentiation is the weakest side of Deep Learning, they decided to create an ensemble of trees with DL properties. </p><br><p>  <em>Slide ‚ÄúHow to make a good ensemble‚Äù</em> <br><img src="https://habrastorage.org/webt/8w/cb/z9/8wcbz9ml-7qidb5ii4-meqcinec.jpeg"></p><br><p>  The architecture was derived from the properties of the ensembles: the elements of the ensembles should not be very poor in quality and different. </p><br><p>  GcForest consists of two stages: Cascade Forest and Multi-Grained Scanning.  Moreover, so that the cascade is not retrained, it consists of 2 types of trees - one of which is absolutely random trees that can be used on unpartitioned data.  The number of layers is determined within the algorithm by cross-validation. <br><img src="https://habrastorage.org/webt/qv/co/-b/qvco-br5vregwj-rrxn3bxnyfeq.jpeg"></p><br><p>  <em>Two types of trees</em> <br><img src="https://habrastorage.org/webt/mc/kp/ia/mckpiaiavjyh9hawcxhvtbusego.jpeg"></p><br><p>  <strong>results</strong> </p><br><p>  In addition to the results on standard datasets, the authors tried to apply gcForest on transactions of the Chinese payment system for fraud searches and got F1 and AUC much higher than those of LR and DNN.  These results are only in the presentation, but the code to run on some standard datasets is on Git. </p><br><p><img src="https://habrastorage.org/webt/y3/kf/gy/y3kfgytp_qawqyskwmvrrumzdna.jpeg"></p><br><p>  <em>Shutter Algorithm Results.</em>  <em>mdDF is optimal Margin Distribution Deep Forest, a variant of gcForest</em> </p><br><p><img src="https://habrastorage.org/webt/e1/oh/wq/e1ohwqrilda60nmdnosaa_ye4yk.jpeg"></p><br><p>  Pros: </p><br><ul><li>  Few hyperparameters, the number of layers is configured automatically within the algorithm. </li><li>  Default options are set to work well on many tasks. </li><li>  Adaptive complexity of the model, on small data - a small model </li><li>  No need to set features </li><li>  It works in comparable quality with deep neural networks, and sometimes better </li></ul><br><p>  Minuses: </p><br><ul><li>  Not accelerated on the GPU </li><li>  In the pictures plays DNNs </li></ul><br><p>  Neural grids have a gradient damping problem, and deep woods have a problem of ‚Äúdiversity vanishing‚Äù.  Since this is an ensemble, the more ‚Äúdifferent‚Äù and ‚Äúgood‚Äù elements to use - the higher the quality.  The problem is that the authors have already reviewed almost all the classical approaches (sampling, randomization).  Until new fundamental research appears in the ‚Äúdifferences‚Äù topic, it will be difficult to improve the quality of deep forest.  But now it is possible to improve the speed of calculations. </p><br><p>  <strong>Reproducible results</strong> </p><br><p>  XGBoost won such a cool gain on the tabular data, and wanted to reproduce the result.  I took Adults datasets and applied GcForestCS (slightly accelerated version of GcForest) with parameters from the authors of the article and XGBoost with default parameters.  In the example that the authors had, the categorical features were already pre-processed somehow, but not how.  In the end, I used CatBoostEncoder and another metric - ROC AUC.  The results were statistically different - XGBoost won.  The running time of XGBoost is insignificant, and for gcForestCS - 20 minutes each, each cross-validation on 5 folds.  On the other hand, the authors tested the algorithm on different datasets, and the parameters for this dataset were adjusted to their pre-processing features. </p><br><p>  Code can be found <a href="https://github.com/Julia-chan/Habr/blob/master/DeepForest_Demo.ipynb">here</a> . </p><br><p>  <strong>Implementations</strong> </p><br><p>  ‚Üí <a href="https://github.com/kingfengji/gcForest">Official code from the authors of the article</a> <br>  ‚Üí <a href="http://lamda.nju.edu.cn/code_gcForestCS.ashx">Official improved version, works faster, but there is no documentation</a> <br>  ‚Üí <a href="https://github.com/pylablanche/gcForest">Implementation easier</a> </p><br><h2 id="pclasso-the-lasso-meets-principal-components-regression">  PcLasso: the lasso meets principal components regression </h2><br><p>  J. Kenneth Tay, Jerome Friedman, Robert Tibshirani (Stanford University) </p><br><p>  ‚Üí <a href="https://arxiv.org/pdf/1810.04651.pdf">Article</a> <br>  ‚Üí <a href="http://www.aistats.org/0-AISTATS2019-slides-robert_tibshirani.pdf">Presentation</a> <br>  ‚Üí <a href="https://cran.r-project.org/web/packages/pcLasso/vignettes/pcLasso.html">Example of use</a> </p><br><p>  In early 2019, J. Kenneth Tay, Jerome Friedman and Robert Tibshirani from Stanford University proposed a new method of teaching with a teacher, especially suitable for sparse data. </p><br><p>  The authors of the article solved the problem of analyzing data on the study of gene expression, which are described in Zeng &amp; Breesy (2016).  Target is the mutational status of the p53 gene, which regulates gene expression in response to various cellular stress signals.  The aim of the study is to identify predictors that correlate with the mutational status of p53.  The data consists of 50 lines, 17 of which are classified as normal and the remaining 33 are mutations in the p53 gene.  In accordance with the analysis in Subramanian et al.  (2005) 308 sets of genes that are between 15 and 500 are included in this analysis.  These gene kits contain a total of 4301 genes and are available in the grpregOverlap R package.  When expanding data to handle overlapping groups, 13,237 columns go out.  The authors of the article applied the pcLasso method, which helped to improve the model results. </p><br><p>  <em>In the picture we see an increase in AUC when using ‚ÄúpcLasso‚Äù</em> <br><img src="https://habrastorage.org/webt/ok/p6/mg/okp6mgex-l9p49vcz5gedg8xa5o.jpeg"></p><br><p>  <strong>The essence of the method</strong> </p><br><p> Method combines <img src="https://habrastorage.org/getpro/habr/post_images/5dc/ac4/8c9/5dcac48c935fba11622a03af9e77e5a0.svg" alt="l_1">  -regularization with <img src="https://habrastorage.org/getpro/habr/post_images/966/980/a89/966980a893368f21382de225cb386c40.svg" alt="l_2">  , which narrows the vector of coefficients to the main components of the matrix of features.  They called the proposed method "lasso core components" ("pcLasso" available on R).  The method can be especially powerful if the variables are pre-grouped (the user chooses what and how to group).  In this case, pcLasso compresses each group and gets a solution in the direction of the main components of this group.  In the process of decision, the selection of significant groups among the existing ones is also performed. </p><br><p>  We present the diagonal matrix of the singular decomposition of a centered matrix of attributes <img src="https://habrastorage.org/getpro/habr/post_images/fb1/4b2/485/fb14b2485daa4b8f13252981a33df4e5.svg" alt="X">  in the following way: </p><br><p>  Our singular decomposition of the centered matrix X (SVD) is represented as <img src="https://habrastorage.org/getpro/habr/post_images/14c/c86/30c/14cc8630ccb53f188b649534d553f50a.svg" alt="X = UDV ^ T">  where <img src="https://habrastorage.org/getpro/habr/post_images/9cc/f8d/c83/9ccf8dc8359156f127521cb415e6274a.svg" alt="D">  - diagonal matrix consisting of singular values.  In this form <img src="https://habrastorage.org/getpro/habr/post_images/966/980/a89/966980a893368f21382de225cb386c40.svg" alt="l_2">  -regularization can be represented: <br><img src="https://habrastorage.org/getpro/habr/post_images/4ea/4d2/ce8/4ea4d2ce8a0cf78e5a1a14896a85562f.svg" alt="\ beta ^ T VZV ^ T \ beta">  Where <img src="https://habrastorage.org/getpro/habr/post_images/be9/efa/777/be9efa77761b1343076071d07add5798.svg" alt="Z">  - diagonal matrix containing the function of the squares of singular values: <img src="https://habrastorage.org/getpro/habr/post_images/d2d/a33/738/d2da3373821e795009e8d90770e49d4e.svg" alt="Z_ {11} = f_1 (d_1 ^ 2, d_2 ^ 2, ..., d_m ^ 2), ..., Z_ {22} = f_2 (d_1 ^ 2, d_2 ^ 2, ..., d_m ^ 2)">  . </p><br><p>  In general, in <img src="https://habrastorage.org/getpro/habr/post_images/966/980/a89/966980a893368f21382de225cb386c40.svg" alt="l_2">  -regularization <img src="https://habrastorage.org/getpro/habr/post_images/8f7/f30/bca/8f7f30bca3b83d1fae9b893be813dfca.svg" alt="Z_ {jj} = 1">  for all <img src="https://habrastorage.org/getpro/habr/post_images/c73/07a/1bf/c7307a1bf9d387db2abbcfc4215797c1.svg" alt="j">  that matches <img src="https://habrastorage.org/getpro/habr/post_images/5cd/b0d/c2c/5cdb0dc2ca0528c525d7b3e8e2087b4b.svg" alt="\ beta ^ T \ beta">  .  They propose to minimize the following functionality: </p><br><p><img src="https://habrastorage.org/webt/6l/fj/lv/6lfjlv9m-zy8qfhcvrcqcymuuxa.jpeg"></p><br><p>  Here <img src="https://habrastorage.org/getpro/habr/post_images/9cc/f8d/c83/9ccf8dc8359156f127521cb415e6274a.svg" alt="D">  - matrix of differences of diagonal elements <img src="https://habrastorage.org/getpro/habr/post_images/cd5/b9d/369/cd5b9d369847628d3a9d6c8f0ae13b33.svg" alt="d_1 ^ 2-d_1 ^ 2, d_1 ^ 2-d_2 ^ 2, ..., d_1 ^ 2-d_m ^ 2">  .  In other words, we control the vector. <img src="https://habrastorage.org/getpro/habr/post_images/818/11b/0b2/81811b0b26d4b7e534b439a41d4eb61f.svg" alt="\ beta">  also with the help of the hyperparameter <img src="https://habrastorage.org/getpro/habr/post_images/2cb/bcb/347/2cbbcb347a44c276c1095ac5bb3f8242.svg" alt="\ theta">  . <br>  Transforming this expression, we get the solution: </p><br><p><img src="https://habrastorage.org/webt/vf/qs/6b/vfqs6b8fnqo3bmlacyr5j4fpigs.jpeg"></p><br><p>  But the main "feature" of the method, of course, is the ability to group data, and based on these groups, select the main components of the group.  Then we rewrite our solution in the form: </p><br><p><img src="https://habrastorage.org/webt/9l/ij/wc/9lijwc3_kvwtvxalszzyt4zq4l4.jpeg"></p><br><p>  Here <img src="https://habrastorage.org/getpro/habr/post_images/3c3/734/188/3c3734188be1abf75e52eba52cfab3c8.svg" alt="\ beta_k">  - vector subvector <img src="https://habrastorage.org/getpro/habr/post_images/818/11b/0b2/81811b0b26d4b7e534b439a41d4eb61f.svg" alt="\ beta">  corresponding to group k, <img src="https://habrastorage.org/getpro/habr/post_images/164/2f2/a8f/1642f2a8f6c2f835030641cfd56a3f04.svg" alt="d_k = (d_ {k1}, ..., d_ {kmk})">  - singular values <img src="https://habrastorage.org/getpro/habr/post_images/fba/468/1a6/fba4681a68b5bc1c9cb46cc6f926e7d4.svg" alt="X_k">  arranged in descending order, and <img src="https://habrastorage.org/getpro/habr/post_images/fc1/def/468/fc1def46820876c24d818173637b0cc7.svg" alt="D_ {d_ {k1} ^ 2-d_ {kj} ^ 2}">  - diagonal matrix <img src="https://habrastorage.org/getpro/habr/post_images/599/342/dd7/599342dd7a36b9fc56c92200a163241b.svg" alt="d_ {k1} ^ 2-d_ {kj} ^ 2, j = 1,2, ..., m_k"></p><br><p>  Some comments on the solution of the target functional: </p><br><ol><li><p>  The objective function is convex, and the non-smooth component is separable.  Consequently, it can be effectively optimized using gradient descent. <br>  The approach is to fix multiple values. <img src="https://habrastorage.org/getpro/habr/post_images/2cb/bcb/347/2cbbcb347a44c276c1095ac5bb3f8242.svg" alt="\ theta">  (including zero, respectively, getting the standard <img src="https://habrastorage.org/getpro/habr/post_images/5dc/ac4/8c9/5dcac48c935fba11622a03af9e77e5a0.svg" alt="l_1">  -regularization), and then optimize: <img src="https://habrastorage.org/webt/uz/bf/eo/uzbfeori9kupj8b05x46dcj6iri.jpeg">  picking up <img src="https://habrastorage.org/getpro/habr/post_images/737/77b/299/73777b29907648e5b08f01b47d9ec807.svg" alt="\ lambda">  .  Accordingly, the parameters <img src="https://habrastorage.org/getpro/habr/post_images/2cb/bcb/347/2cbbcb347a44c276c1095ac5bb3f8242.svg" alt="\ theta">  and <img src="https://habrastorage.org/getpro/habr/post_images/737/77b/299/73777b29907648e5b08f01b47d9ec807.svg" alt="\ lambda">  are selected for cross-validation. </p><br></li><li><p>  Parameter <img src="https://habrastorage.org/getpro/habr/post_images/2cb/bcb/347/2cbbcb347a44c276c1095ac5bb3f8242.svg" alt="\ theta">  hard to interpret.  In the software (package "pcLasso"), the user himself sets the value of this parameter, which belongs to the interval [0,1], where 1 corresponds to <img src="https://habrastorage.org/getpro/habr/post_images/2cb/bcb/347/2cbbcb347a44c276c1095ac5bb3f8242.svg" alt="\ theta">  = 0 (lasso). </p><br></li></ol><br><p>  In practice, varying the values <img src="https://habrastorage.org/getpro/habr/post_images/2cb/bcb/347/2cbbcb347a44c276c1095ac5bb3f8242.svg" alt="\ theta">  = 0.25, 0.5, 0.75, 0.9, 0.95, and 1, you can cover a large range of models. </p><br><p>  <em>The algorithm itself is as follows</em> <br><img src="https://habrastorage.org/webt/l-/3x/si/l-3xsipork2hzeh7ws1bg1hz86o.jpeg"></p><br><p>  This algorithm is already written in R, if you wish, you can already use it.  The library is called 'pcLasso'. </p><br><h2>  A Swiss Army Infinitesimal Jackknife </h2><br><p>  Ryan Giordano (UC Berkeley);  William Stephenson (MIT);  Runjing Liu (UC Berkeley); <br>  Michael Jordan (UC Berkeley);  Tamara Broderick (MIT) </p><br><p>  ‚Üí <a href="https://arxiv.org/pdf/1806.00550.pdf">Article</a> <br>  ‚Üí <a href="https://github.com/rgiordan/AISTATS2019SwissArmyIJ/blob/master/simple_examples/ExponentialExample.ipynb">Code</a> </p><br><p>  The quality of machine learning algorithms is often evaluated by multiple cross-validation (cross-validation or bootstrap).  These methods are powerful, but slow on large data sets. </p><br><p>  In this paper, colleagues use linear approximation of scales, producing results that work faster.  This linear approximation is known in the statistical literature as the ‚Äúinfinitesimal jackknife‚Äù.  It is mainly used as a theoretical tool for proving asymptotic results.  The results of the paper are applicable regardless of whether the weights and data are stochastic or deterministic.  As a consequence, this approximation consistently evaluates the true cross-validation for any fixed k. </p><br><p>  <em>Handing Paper Award to the author of the article</em> <br><img src="https://habrastorage.org/webt/3n/1a/-k/3n1a-kygdmkbs0drfjdg38b9z5g.jpeg"></p><br><p>  <strong>The essence of the method</strong> </p><br><p>  Consider the problem of estimating an unknown parameter. <img src="https://habrastorage.org/getpro/habr/post_images/a05/b78/797/a05b78797b1ca72f2aa9678a0ed2bd2c.svg" alt="\ theta \ in \ Omega _ {\ theta} \ subset R ^ {D}">  where <img src="https://habrastorage.org/getpro/habr/post_images/796/bd0/0b5/796bd00b5d620acc0b5d095e7181cc1f.svg" alt="\ Omega _ {\ theta}">  - compact, and the size of our dataset - <img src="https://habrastorage.org/getpro/habr/post_images/a1e/39d/a1c/a1e39da1c84981d7264baa207047222a.svg" alt="N">  .  Our analysis will be conducted on a fixed set of data.  We define our assessment <img src="https://habrastorage.org/getpro/habr/post_images/a7f/7c9/e68/a7f7c9e68900aa1d825bec8a0b230041.svg" alt="\ theta \ in \ Omega _ {\ theta}">  in the following way: </p><br><ol><li>  For each <img src="https://habrastorage.org/getpro/habr/post_images/610/43d/b1b/61043db1ba350ce3be60cd5de6acc601.svg" alt="n = 1,2 ..., N">  let's set <img src="https://habrastorage.org/getpro/habr/post_images/ac0/eba/e25/ac0ebae250d885c0d494bd7269b95bd2.svg" alt="g_n">  ( <img src="https://habrastorage.org/getpro/habr/post_images/2cb/bcb/347/2cbbcb347a44c276c1095ac5bb3f8242.svg" alt="\ theta">  ) - function from <img src="https://habrastorage.org/getpro/habr/post_images/1e6/a8d/113/1e6a8d113fc00ca31a2f2f415caf68a0.svg" alt="\ Omega _ {\ theta} \ subset R ^ {D}"></li><li><img src="https://habrastorage.org/getpro/habr/post_images/966/0c2/daf/9660c2daf87000168540ec76b6a6277b.svg" alt="\ omega_n">  - a real number, and <img src="https://habrastorage.org/getpro/habr/post_images/215/ad4/e66/215ad4e667bc59c2156cec39aab8cee4.svg" alt="\ omega">  - vector consisting of <img src="https://habrastorage.org/getpro/habr/post_images/966/0c2/daf/9660c2daf87000168540ec76b6a6277b.svg" alt="\ omega_n"></li></ol><br><p>  Then <img src="https://habrastorage.org/getpro/habr/post_images/6a9/3ae/7fa/6a93ae7fa13250a0934159bc03dadcab.svg" alt="\ hat {\ theta}">  can be represented as: </p><br><p><img src="https://habrastorage.org/webt/vo/zo/bg/vozobgjy_dzwjry6nvhth83u3-u.jpeg"></p><br><p>  Solving this optimization problem using the gradient method, we assume that the functions are differentiable and we can calculate the Hessian.  The main problem that we solve is the computational costs associated with the assessment <img src="https://habrastorage.org/getpro/habr/post_images/e8c/821/be2/e8c821be2f762d8bb547f05ae94e61e0.svg" alt="\ hat {\ theta} ÃÇ (\ omega)">  for all <img src="https://habrastorage.org/getpro/habr/post_images/52a/3c9/78c/52a3c978c806fce5d2652266fe0331e5.svg" alt="\ omega‚ààW">  .  The main contribution of the authors of the article is to calculate the estimate <img src="https://habrastorage.org/getpro/habr/post_images/ff5/4e0/476/ff54e047607afe18029166a0ae9a59a6.svg" alt="\ hat {\ theta} _1 = \ hat {\ theta} _1 (1 _ {\ omega})">  where <img src="https://habrastorage.org/getpro/habr/post_images/c84/8a6/645/c848a664538a2b56073eac881ae7b84f.svg" alt="1_ \ omega = (1,1, ..., 1)">  .  In other words, our optimization will depend only on derivatives <img src="https://habrastorage.org/getpro/habr/post_images/89b/ca7/102/89bca71025cf975b53aacf1ad0ff64a6.svg" alt="g_n (\ theta)">  , which we assume exist, and are the Hessian: </p><br><p><img src="https://habrastorage.org/webt/tb/zb/t2/tbzbt2u2q_bdqidfjxfl9ywqesc.jpeg"></p><br><p>  Next, we define an equation with a fixed point and its derivative: <br><img src="https://habrastorage.org/webt/hj/8f/x3/hj8fx3broftye-ssmq4mwpkvaui.jpeg"></p><br><p>  It is worth noting here that <img src="https://habrastorage.org/getpro/habr/post_images/020/24f/557/02024f5575bbb0f75129f84000861a85.svg" alt="G (\ theta ÃÇ (\ omega), w) = 0">  , because <img src="https://habrastorage.org/getpro/habr/post_images/e8c/821/be2/e8c821be2f762d8bb547f05ae94e61e0.svg" alt="\ hat {\ theta} (\ omega)">  - solution for <img src="https://habrastorage.org/getpro/habr/post_images/127/3e8/d59/1273e8d59cd9fe1bc495c0fe42ead51c.svg" alt="\ frac {1} {N} \ sum_ {n = 1} ^ {N} \ omega_n g_n (\ theta) = 0">  .  Also define: <img src="https://habrastorage.org/getpro/habr/post_images/e5c/728/d80/e5c728d8088c24a2a058580fbd634ff0.svg" alt="H_1 = H (\ hat {\ theta} _1,1_ \ omega)">  and matrix weights like: <img src="https://habrastorage.org/getpro/habr/post_images/ee5/640/c49/ee5640c4942ad884a88ff0a94bba2680.svg" alt="\ Delta \ omega = \ omega-1_ \ omega \ in R ^ {n}">  .  In that case, when <img src="https://habrastorage.org/getpro/habr/post_images/b02/e5d/e8f/b02e5de8ffd34b2281c78bd1d23652b2.svg" alt="H_1">  has an inverse matrix, we can use the implicit function theorem and the 'chain rule': </p><br><p><img src="https://habrastorage.org/webt/c7/iv/x2/c7ivx2dadeupxwlo2hzgmxslrxe.jpeg"></p><br><p>  This derivative allows us to form a linear approximation <img src="https://habrastorage.org/getpro/habr/post_images/e8c/821/be2/e8c821be2f762d8bb547f05ae94e61e0.svg" alt="\ hat {\ theta} ÃÇ (\ omega)">  through <img src="https://habrastorage.org/getpro/habr/post_images/aa8/dff/8a8/aa8dff8a8813dd22aba9d71f138fb320.svg" alt="\ hat {\ theta} _1">  which looks like this: </p><br><p><img src="https://habrastorage.org/webt/lc/rc/5h/lcrc5hirn1act9jl8lp2jakjpsk.jpeg"></p><br><p>  Because <img src="https://habrastorage.org/getpro/habr/post_images/a62/585/0b2/a625850b2699424e397f9efa34d9d1af.svg" alt="\ hat {\ theta} _ {IJ}">  depends only on <img src="https://habrastorage.org/getpro/habr/post_images/aa8/dff/8a8/aa8dff8a8813dd22aba9d71f138fb320.svg" alt="\ hat {\ theta} _1">  and <img src="https://habrastorage.org/getpro/habr/post_images/9f0/adb/baf/9f0adbbaf3be258e3dd58dc990302df7.svg" alt="\ Delta \ omega">  and not on solutions for any other values. <img src="https://habrastorage.org/getpro/habr/post_images/215/ad4/e66/215ad4e667bc59c2156cec39aab8cee4.svg" alt="\ omega">  , accordingly, there is no need to recalculate anew and find new values ‚Äã‚Äãof œâ.  Instead, the SLN (system of linear equations) must be solved. </p><br><p>  <strong>results</strong> </p><br><p>  In practice, this significantly reduces the time compared to cross-validation: <br><img src="https://habrastorage.org/webt/sw/dr/6-/swdr6-j8t7pqcdwf_96705qs1tg.jpeg"></p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/458388/">https://habr.com/ru/post/458388/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../458374/index.html">TJBOT as an illustration of IBM Watson services</a></li>
<li><a href="../458376/index.html">Not a regular programming language</a></li>
<li><a href="../458378/index.html">Use Avocode for site layout. Overview for beginners. Bonus - register a 30-day trial period</a></li>
<li><a href="../458382/index.html">Why do we teach this?</a></li>
<li><a href="../458384/index.html">HP 3D Structured Light Scanner Pro S3 3D Scanner Review and Testing</a></li>
<li><a href="../458390/index.html">Ceph - from ‚Äúon the knee‚Äù to ‚Äúproduction‚Äù part 2</a></li>
<li><a href="../458394/index.html">Securing wireless protocols using LoRaWAN as an example</a></li>
<li><a href="../458396/index.html">How I made it easy to develop on Vue.js with server-side rendering</a></li>
<li><a href="../458398/index.html">Remote hygiene or telepathy benefits</a></li>
<li><a href="../4584/index.html">Rambler turned off the TV</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>