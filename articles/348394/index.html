<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Notes on deploying Ruby on Rails Deployment on Google Cloud Kubernetes Engine</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I use Google Cloud with Kubernetes Engine for 2 months. In fact, it did not take me a month to put everything in my head, but it took as much again to...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Notes on deploying Ruby on Rails Deployment on Google Cloud Kubernetes Engine</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/4y/zs/09/4yzs09jwu4hmrlsm5mh6yikhree.png"><br>  I use Google Cloud with Kubernetes Engine for 2 months.  In fact, it did not take me a month to put everything in my head, but it took as much again to deal with some troubles. </p><br><p>  TL; DR: Google does a pretty good job, so AWS is not relaxing.  If you know AWS well, I would advise testing Google Cloud.  Perhaps because of muscle memory, I would be more comfortable with AWS, but I studied Google Cloud and Kubernetes and am confident in most of my scenarios. </p><br><p>  I am not an expert, so accept my words with a bit of skepticism.  Google Cloud and Kubernetes are one of those topics that I really want to talk about, but I can‚Äôt always find the right words and hope you get the right idea about the proposed solutions. </p><br><p>  The purpose of the article is to preserve some fragments and thoughts for further use.  So keep in mind that this is not a walkthrough.  At first I intended to write a guide, but then I realized that it was almost like writing a whole book, so not this time. </p><a name="habracut"></a><br><p>  To succeed with something like Google Cloud and Kubernetes, you must have enough experience.  If you have never installed Linux From Scratch, if you have never performed server optimization, if you do not like server-side server components, do not attempt a real production deployment.  Your safest bet will still be on Heroku. </p><br><p>  You should be the kind of person who likes to tinker (as in my <a href="http://www.akitaonrails.com/linux">previous</a> blogs). </p><br><p>  I don't know everything, but I know enough.  To begin with, I had to understand what I needed.  It is important to state your needs before attempting to write the first YAML file.  Planning is crucial. </p><br><p>  Here is what I needed: </p><br><ul><li>  Scalable level of web applications, where I could perform both sliding updates (for zero <strong>updates idle</strong> ), and automatic and manual horizontal scaling of servers. </li><li>  Mounted permanent storage <strong>with</strong> automatic snapshots / backups. </li><li>  Managed reliable database (Postgresql) with automatic backup and <strong>simple replication</strong> to read-only instances. </li><li>  Managed storage solution for secrets (such as ENV Heroku support).  Never store a production configuration in source code. </li><li>  Docker image support without the need to create a custom infrastructure for deployment. </li><li>  Static external IP addresses for integration requiring a fixed IP address. </li><li>  SSL termination so that I can connect to CloudFlare (CDN is mandatory but not sufficient. In 2018, we will need DDoS protection). </li><li>  Enough security by default, that is, theoretically everything is blocked until I decide to open it. </li><li>  High availability in various regions and data center zones. </li></ul><br><p>  A simple demo web application is easy to deploy.  But I didn‚Äôt want a demo, I wanted a solution for long-term production. </p><br><p>  Some problems for newbies: </p><br><ul><li>  The documentation is very <em>extensive</em> and you will find <em>almost</em> everything if you know what you are looking for.  Also keep in mind that Azure and AWS implement Kubernetes with certain differences, so some documents do not apply to the Google Cloud and vice versa. </li><li>  There are many special features in alpha, beta and stable stages.  The documentation seems to be understandable, but most of the textbooks, which are just a couple of months old, can work differently than expected (Kubernetes 1.8.4-gke among them). </li><li>  There is a whole set of words that refer to the same concept.  You need to get used to the vocabulary. </li><li>  Imagine you are playing Lego.  Details can be interfered, differently combined, but it is very easy to spoil everything.  This means that you can create your own configuration that will meet your requirements.  But you can not just copy it. </li><li>  You can do <em>almost</em> everything through YAML files and the command line, but reusing a configuration (for working and intermediate environments, for example) is not a trivial task.  There are third-party tools that deal with parameterizable and reusable YAML bits, but I would do it all manually.  Never, never try automated patterns in an infrastructure without knowing exactly what they are doing. </li><li> You have two heavy command line tools: <code>gcloud</code> and <code>kubectl</code> . </li></ul><br><p>  Let me remind you once again: this is <strong>not a</strong> walkthrough.  I annotate a few steps. </p><br><h2 id="masshtabiruemyy-veb-uroven-samo-veb-prilozhenie">  Scalable web tier (web application itself) </h2><br><p>  The first thing you need is a complete 12-factors app. </p><br><p>  Whether it is Ruby on Rails, Django, Laravel, Node.js (whatever), this should be an application that is not shared and does not depend on writing anything on the local file system.  An application that you can easily disable and launch instances independently.  There should be no sessions in local memory or in local files (I prefer to avoid session proximity).  There is no file upload to the local file system (if necessary, you need to mount external persistent storage), always prefer to send binary streams to managed storage services. <br>  You have to have a <a href="https://tomanistor.com/blog/cache-bust-that-asset/">proper pipeline that caches through fingerprint assets</a> (whether you like it or not, Rails still has the best of off-the-shelf solutions in Asset Pipeline). <br>  Instruct the application, add the <a href="https%25253A%25252F%25252Frpm.newrelic.com%25252Fauth%25252Fnewrelic%25252Fcallback%2526state%253Db808c050ec556df4999af44894111e2dc5bf576326497339">New Relic RPM</a> and <a href="https://rollbar.com/">Rollbar</a> . <br>  2018, you do not want to deploy your own code using SQL injection (or any other input), uncontrolled eval around your code, there is no room for CSRF or XSS, etc. Go ahead, buy a license for Brakeman Pro and add it to CI conveyor.  I can wait‚Ä¶ <br>  Since this is not a tutorial, I assume that you can register with Google Cloud and configure the project, your region and zone. <br>  It took me some time to understand the primary structure in Google Cloud: </p><br><ul><li>  You start with a <a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects">project</a> that serves as a cover for everything you need for your application. </li><li>  Then you create <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture">‚Äúclusters‚Äù</a> .  For example, you might have a production, staging cluster, a web cluster, or a split service cluster for non-network materials. </li><li>  The cluster has a <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture">‚Äúcluster master‚Äù</a> , which is the controller of everything else (the <code>gcloud</code> and <code>kubectl</code> interact with the API interfaces). </li><li>  The cluster has many <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture">‚Äúnode instances‚Äù</a> , correct ‚Äúmachines‚Äù (or, more precisely, VM instances). </li><li>  Each cluster also has at least one <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools">‚Äúnode pool‚Äù</a> (‚Äúdefault pool‚Äù), which is a set of node instances with the same configuration and the same <a href="https://cloud.google.com/compute/docs/machine-types">‚Äúmachine type‚Äù</a> . </li><li>  Finally, each node instance runs one or more ‚Äúcontainers‚Äù, which are simple containers, such as LXC.  Here is where your app really is. <br>  Example of creating a cluster: </li></ul><br><pre> <code class="hljs tex">gcloud container clusters create my-web-production <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--enable-cloud-logging <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--enable-cloud-monitoring <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--machine-type n1-standard-4 <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--enable-autoupgrade <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--enable-autoscaling --max-nodes=5 --min-nodes=2 <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--num-nodes 2</code> </pre> <br><p>  As I already mentioned, the cluster creates a <code>default-pool</code> with <a href="https://cloud.google.com/compute/docs/machine-types">machine type</a> <code>n1-standard-4</code> .  Select the CPU / RAM combination you need for the application.  The type I chose has 4 vCPUs and 15 GB of RAM. </p><br><p>  By default, it starts at 3 nodes, so I first chose 2, but automatically scaled to 5 (you can update it later if needed, but make sure there is room for initial growth).  You can continue to add additional pools for instances of different sized nodes, say, for Sidekiq workers, to perform intensive background processing.  Then create a separate pool of nodes with a different machine type for your set of instances, for example: </p><br><pre> <code class="hljs tex">gcloud container node-pools create large-pool <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--cluster=my-web-production <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--node-labels=pool=large <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--machine-type=n1-highcpu-8 <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--num-nodes 1</code> </pre> <br><p>  This other pool manages 1 <code>n1-highcpu-8</code> node, which has 8 vCPUs with 7.2 GB of RAM.  More processors, less memory.  You have a <code>highmem</code> category, which is smaller than a CPU with a much larger memory.  Again, you need to know what you want. </p><br><p>  The important point here is <code>--node-labels</code> - how I will map the deployment for choosing between node pools (in this case, between the default pool and the large pool). </p><br><p>  When you create a cluster, you must issue the following command to get your credentials: </p><br><p> <code>gcloud container clusters get-credentials my-web-production</code> </p> <br><p>  The command sets <code>kubectl</code> .  If you have more than one cluster ( <code>my-web-production</code> <br>  and <code>my-web-staging</code> ), you need to be careful with <code>get-credentials</code> for the correct cluster, otherwise you can run an intermediate deployment on the production cluster. </p><br><p>  Since this is confusing, I changed my ZSH PROMPT to always see which cluster I come across.  I adapted from <a href="https://github.com/superbrothers/zsh-kubectl-prompt">zsh-kubectl-prompt</a> : </p><br><p><img src="https://habrastorage.org/webt/iw/op/r_/iwopr_ul7rzmpasp26dw8_jcyv4.png"></p><br><p>  Since you will have many clusters in a large application, I highly recommend adding this PROMPT to your shell. </p><br><p>  How do you deploy the app in sub-instances of the nodes? </p><br><p>  You must have a Docker file in the application's project repository to create a Docker image.  This is one example of a Ruby on Rails application: </p><br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> ruby:<span class="hljs-number"><span class="hljs-number">2.4</span></span><span class="hljs-number"><span class="hljs-number">.3</span></span> ENV RAILS_ENV production ENV SECRET_KEY_BASE xpto RUN curl -sL https://deb.nodesource.com/setup_8.x | bash - RUN apt-<span class="hljs-keyword"><span class="hljs-keyword">get</span></span> <span class="hljs-keyword"><span class="hljs-keyword">update</span></span> &amp;&amp; apt-<span class="hljs-keyword"><span class="hljs-keyword">get</span></span> install -y nodejs postgresql-client cron htop vim <span class="hljs-keyword"><span class="hljs-keyword">ADD</span></span> Gemfile* /app/ WORKDIR /app RUN gem <span class="hljs-keyword"><span class="hljs-keyword">update</span></span> bundler <span class="hljs-comment"><span class="hljs-comment">--pre RUN bundle install --without development test RUN npm install ADD . /app RUN cp config/database.yml.prod.example config/database.yml &amp;&amp; cp config/application.yml.example config/application.yml RUN RAILS_GROUPS=assets bundle exec rake assets:precompile</span></span></code> </pre> <br><p>  In the Google Cloud Web Console, you will find the <a href="https://cloud.google.com/container-registry/">‚ÄúContainer Registry‚Äù</a> , which is the Private Docker Registry. </p><br><p>  Add the remote URL to your local configuration as follows: </p><br><p> <code>git remote add gcloud https://source.developers.google.com/p/my-project/r/my-app</code> </p> <br><p>  Now you can <code>git push gcloud master</code> .  I recommend adding <a href="https://cloud.google.com/container-builder/docs/running-builds/automate-builds">triggers</a> to tag images.  I add 2 triggers: one to mark it <code>latest</code> and another to mark it with a random version number.  You will need this later. </p><br><p>  After adding the registry repository as a remote in your git configuration ( <code>git remote add</code> ), click on it.  It should begin to create a Docker image with the appropriate tags that you have configured using triggers. </p><br><p>  Make sure your Ruby on Rails application does not have anything in initializers that require a connection to the database because it is not available.  You can get stuck when the Docker build completes with an error due to the fact that <code>assets:precompile</code> - the task has loaded an initializer that accidentally calls the model, and this triggers <code>ActiveRecord :: Base</code> triggers to attempt to connect. </p><br><p>  Make sure that the Ruby version in the Dockerfile is the same as the version in the Gemfile, otherwise it will also fail. </p><br><p>  Have you noticed the weird <code>config/application.yml</code> .  This is from <a href="https://github.com/laserlemon/figaro">figaro</a> .  I recommend to simplify the setting of the ENV variable in the system.  I don‚Äôt like Rails secrets, and this isn‚Äôt very friendly to deployment systems after Heroku made ENV vars ubiquitous.  Stick to ENV vars.  Kubernetes will thank you for it. </p><br><p>  Now you can override any environment variable from the Kubernetes and yaml deployment file.  Now is the time to set an example.  You can call it <code>deploy / web.yml</code> or whatever it suits you.  And, of course, check it out in the source code repository. </p><br><pre> <code class="hljs mel">kind: Deployment apiVersion: apps/v1beta1 metadata: name: web spec: strategy: type: RollingUpdate rollingUpdate: maxSurge: <span class="hljs-number"><span class="hljs-number">1</span></span> maxUnavailable: <span class="hljs-number"><span class="hljs-number">1</span></span> minReadySeconds: <span class="hljs-number"><span class="hljs-number">10</span></span> replicas: <span class="hljs-number"><span class="hljs-number">2</span></span> template: metadata: labels: app: web spec: containers: - <span class="hljs-keyword"><span class="hljs-keyword">image</span></span>: gcr.io/my-project/my-app:latest name: my-app imagePullPolicy: Always ports: - containerPort: <span class="hljs-number"><span class="hljs-number">4001</span></span> command: [<span class="hljs-string"><span class="hljs-string">"passenger"</span></span>, <span class="hljs-string"><span class="hljs-string">"start"</span></span>, <span class="hljs-string"><span class="hljs-string">"-p"</span></span>, <span class="hljs-string"><span class="hljs-string">"4001"</span></span>, <span class="hljs-string"><span class="hljs-string">"-e"</span></span>, <span class="hljs-string"><span class="hljs-string">"production"</span></span>, <span class="hljs-string"><span class="hljs-string">"--max-pool-size"</span></span>, <span class="hljs-string"><span class="hljs-string">"2"</span></span>, <span class="hljs-string"><span class="hljs-string">"--min-instances"</span></span>, <span class="hljs-string"><span class="hljs-string">"2"</span></span>, <span class="hljs-string"><span class="hljs-string">"--no-friendly-error-pages"</span></span> <span class="hljs-string"><span class="hljs-string">"--max-request-queue-time"</span></span>, <span class="hljs-string"><span class="hljs-string">"10"</span></span>, <span class="hljs-string"><span class="hljs-string">"--max-request-time"</span></span>, <span class="hljs-string"><span class="hljs-string">"10"</span></span>, <span class="hljs-string"><span class="hljs-string">"--pool-idle-time"</span></span>, <span class="hljs-string"><span class="hljs-string">"0"</span></span>, <span class="hljs-string"><span class="hljs-string">"--memory-limit"</span></span>, <span class="hljs-string"><span class="hljs-string">"300"</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">env</span></span>: - name: <span class="hljs-string"><span class="hljs-string">"RAILS_LOG_TO_STDOUT"</span></span> value: <span class="hljs-string"><span class="hljs-string">"true"</span></span> - name: <span class="hljs-string"><span class="hljs-string">"RAILS_ENV"</span></span> value: <span class="hljs-string"><span class="hljs-string">"production"</span></span> # ... obviously reduced the many ENV vars <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> brevity - name: <span class="hljs-string"><span class="hljs-string">"REDIS_URL"</span></span> valueFrom: secretKeyRef: name: my-<span class="hljs-keyword"><span class="hljs-keyword">env</span></span> key: REDIS_URL - name: <span class="hljs-string"><span class="hljs-string">"SMTP_USERNAME"</span></span> valueFrom: secretKeyRef: name: my-<span class="hljs-keyword"><span class="hljs-keyword">env</span></span> key: SMTP_USERNAME - name: <span class="hljs-string"><span class="hljs-string">"SMTP_PASSWORD"</span></span> valueFrom: secretKeyRef: name: my-<span class="hljs-keyword"><span class="hljs-keyword">env</span></span> key: SMTP_PASSWORD # ... this part below is mandatory <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> Cloud SQL - name: DB_HOST value: <span class="hljs-number"><span class="hljs-number">127.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span> - name: DB_PASSWORD valueFrom: secretKeyRef: name: cloudsql-db-credentials key: password - name: DB_USER valueFrom: secretKeyRef: name: cloudsql-db-credentials key: username - <span class="hljs-keyword"><span class="hljs-keyword">image</span></span>: gcr.io/cloudsql-docker/gce-proxy:latest name: cloudsql-proxy command: [<span class="hljs-string"><span class="hljs-string">"/cloud_sql_proxy"</span></span>, <span class="hljs-string"><span class="hljs-string">"--dir=/cloudsql"</span></span>, <span class="hljs-string"><span class="hljs-string">"-instances=my-project:us-west1:my-db=tcp:5432"</span></span>, <span class="hljs-string"><span class="hljs-string">"-credential_file=/secrets/cloudsql/credentials.json"</span></span>] volumeMounts: - name: cloudsql-<span class="hljs-keyword"><span class="hljs-keyword">instance</span></span>-credentials mountPath: /secrets/cloudsql readOnly: true - name: ssl-certs mountPath: /etc/ssl/certs - name: cloudsql mountPath: /cloudsql volumes: - name: cloudsql-<span class="hljs-keyword"><span class="hljs-keyword">instance</span></span>-credentials secret: secretName: cloudsql-<span class="hljs-keyword"><span class="hljs-keyword">instance</span></span>-credentials - name: ssl-certs hostPath: path: /etc/ssl/certs - name: cloudsql emptyDir:</code> </pre> <br><p>  I will explain an example: </p><br><ul><li>  <code>kind</code> and <code>apiVersion</code> - important, follow the documentation, it may change.  This is what is called <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a> .  There used to be a replication controller (you will find it in old textbooks), but it is no longer used.  Recommendation: use <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a> . </li><li>  Call things by their first name, you have a <code>metadata:name</code> with the <code>web</code> .  Notice the <code>spec:template:metadata:labels</code> , where I put each block labeled <code>app: web</code> .  You will need it to be able to select items later in the ‚ÄúService‚Äù section. </li><li>  I have a <code>spec:strategy</code> in which we set up a rolling update. </li><li>  <code>spec:replicas</code> announces how many pods I want.  You will have to manually calculate the machine type of the pool of nodes, and then split the server resources for each application. </li><li>  Image Docker, which we created, with the "last" tag.  You refer to it in the <code>spec: template: spec: container: image</code> . </li><li>  I use Passenger with production configuration (read <a href="https://www.phusionpassenger.com/library/config/reference/">Phusion documentation</a> ) </li><li>  In spec: template: spec: container: env section, I can redefine the VAR ENV with real production secrets.  I can encode values ‚Äã‚Äãor use this device: </li></ul><br><pre> <code class="hljs pgsql">- <span class="hljs-type"><span class="hljs-type">name</span></span>: "SMTP_USERNAME" valueFrom: secretKeyRef: <span class="hljs-type"><span class="hljs-type">name</span></span>: my-env key: SMTP_USERNAME</code> </pre> <br><p>  This is a link to the ‚Äúsecret‚Äù repository, which I called my-env.  And this is how you create your own: </p><br><pre> <code class="hljs cs">kubectl create secret generic my-env \ --<span class="hljs-keyword"><span class="hljs-keyword">from</span></span>-literal=REDIS_URL=redis:<span class="hljs-comment"><span class="hljs-comment">//foo.com:18821 \ --from-literal=SMTP_USERNAME=foobar</span></span></code> </pre> <br><p>  Read the documentation, since you can download text files, and not declare everything from the command line. </p><br><p>  As I said, I would prefer to use a managed service for the database.  You can upload your Docker image, but I do not recommend it.  The same applies to other services like databases, such as Redis, Mongo.  If you are using AWS, keep in mind: <a href="https://cloud.google.com/sql/docs/postgres/">Google Cloud SQL</a> is similar to RDS. </p><br><p>  After creating an instance of PostgreSQL, you will not be able to access it directly from the web application.  You have a template for the second Docker image, <a href="https://cloud.google.com/sql/docs/mysql/connect-kubernetes-engine">‚ÄúCloudSQL Proxy‚Äù</a> . </p><br><p>  To do this, you must first create a service account: </p><br><pre> <code class="hljs pgsql">gcloud <span class="hljs-keyword"><span class="hljs-keyword">sql</span></span> users <span class="hljs-keyword"><span class="hljs-keyword">create</span></span> proxyuser host <span class="hljs-comment"><span class="hljs-comment">--instance=my-db --password=abcd1234</span></span></code> </pre> <br><p>  After creating a PostgreSQL instance, you will be prompted to load JSON credentials.  Save them somewhere.  I hope I do not have to remind you of the need for a strong password.  Need to create additional secrets: </p><br><pre> <code class="hljs javascript">kubectl create secret generic cloudsql-instance-credentials \ --<span class="hljs-keyword"><span class="hljs-keyword">from</span></span>-file=credentials.json=<span class="hljs-regexp"><span class="hljs-regexp">/home/my</span></span>self/downloads/my-db<span class="hljs-number"><span class="hljs-number">-12345.</span></span>json kubectl create secret generic cloudsql-db-credentials \ --<span class="hljs-keyword"><span class="hljs-keyword">from</span></span>-literal=username=proxyuser --<span class="hljs-keyword"><span class="hljs-keyword">from</span></span>-literal=password=abcd1234</code> </pre> <br><p>  They are mentioned in this part of the deployment: </p><br><pre> <code class="hljs mel">- <span class="hljs-keyword"><span class="hljs-keyword">image</span></span>: gcr.io/cloudsql-docker/gce-proxy:latest name: cloudsql-proxy command: [<span class="hljs-string"><span class="hljs-string">"/cloud_sql_proxy"</span></span>, <span class="hljs-string"><span class="hljs-string">"--dir=/cloudsql"</span></span>, <span class="hljs-string"><span class="hljs-string">"-instances=my-project:us-west1:my-db=tcp:5432"</span></span>, <span class="hljs-string"><span class="hljs-string">"-credential_file=/secrets/cloudsql/credentials.json"</span></span>] volumeMounts: - name: cloudsql-<span class="hljs-keyword"><span class="hljs-keyword">instance</span></span>-credentials mountPath: /secrets/cloudsql readOnly: true - name: ssl-certs mountPath: /etc/ssl/certs - name: cloudsql mountPath: /cloudsql volumes: - name: cloudsql-<span class="hljs-keyword"><span class="hljs-keyword">instance</span></span>-credentials secret: secretName: cloudsql-<span class="hljs-keyword"><span class="hljs-keyword">instance</span></span>-credentials - name: ssl-certs hostPath: path: /etc/ssl/certs - name: cloudsql emptyDir:</code> </pre> <br><p>  You must add the database name (my-db in our case) in the -instance command. </p><br><p>  By the way, <code>gce-proxy:latest</code> refers to version 1.09, when version 1.11 already existed.  The new version has created a headache breakage of the connection and the extension of the waiting time.  So I went back to a later version, 1.09, and everything worked out.  So not everything is new well.  In infrastructure, it is better to stick to stability. </p><br><p>  It would be possible to make sure that it was not a problem.  You may want to read this thread on the subject. </p><br><p>  You can download a separate instance of CloudSQL instead of having it in each pane so that the pods can connect to only one proxy.  I recommend reading <a href="https://github.com/GoogleCloudPlatform/cloudsql-proxy/issues/49">this article</a> . </p><br><p>  Nothing seems to be affected.  Therefore, you need to expose the pods through <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Node Port Service</a> .  Let's create the file <code>deploy/web-svc.yamlfile</code> . </p><br><pre> <code class="hljs pgsql">apiVersion: v1 kind: Service metadata: <span class="hljs-type"><span class="hljs-type">name</span></span>: web-svc spec: sessionAffinity: <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> ports: - port: <span class="hljs-number"><span class="hljs-number">80</span></span> targetPort: <span class="hljs-number"><span class="hljs-number">4001</span></span> protocol: TCP <span class="hljs-keyword"><span class="hljs-keyword">type</span></span>: NodePort selector: app: web</code> </pre> <br><p>  That is why I emphasized the importance of <code>spec:template:metadata:labels</code> .  We will use it in <code>spec:selector</code> to select the correct containers. </p><br><p>  Now we can expand these 2 pods as follows: </p><br><pre> <code class="hljs pgsql">kubectl <span class="hljs-keyword"><span class="hljs-keyword">create</span></span> -f deploy/web.yml kubectl <span class="hljs-keyword"><span class="hljs-keyword">create</span></span> -f deploy/web-svc.yml</code> </pre> <br><p>  You can see that the pods are created using <code>kubectl get pods --watch</code> . </p><br><h2 id="balansirovka-nagruzki">  Load balancing </h2><br><p>  In many textbooks, pods are provided through another service called <a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/">Load Balancer</a> .  I'm not sure how well he behaves under pressure, whether he has SSL termination, and so on. I decided to power up with <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress</a> Load Balancer using the <a href="https://www.nginx.com/products/nginx/kubernetes-ingress-controller/">NGINX controller</a> . </p><br><p>  First of all, I decided to create for it a separate pool of nodes, for example: </p><br><pre> <code class="hljs tex">gcloud container node-pools create web-load-balancer <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--cluster=my-web-production <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--node-labels=role=load-balancer <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--machine-type=g1-small <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--num-nodes 1 <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--max-nodes 3 --min-nodes=1 <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--enable-autoscaling</code> </pre> <br><p>  Once the <code>large-pool</code> example is created, take care to add <code>--node-labels</code> to install the controller instead of the <code>default-pool</code> .  You need to know the instance name of the node, and we can do it like this: </p><br><pre> <code class="hljs pgsql">$ gcloud compute instances list <span class="hljs-type"><span class="hljs-type">NAME</span></span> <span class="hljs-type"><span class="hljs-type">ZONE</span></span> MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS gke-my-web-production-<span class="hljs-keyword"><span class="hljs-keyword">default</span></span>-pool<span class="hljs-number"><span class="hljs-number">-123</span></span><span class="hljs-number"><span class="hljs-number">-123</span></span> us-west1-a n1-standard<span class="hljs-number"><span class="hljs-number">-4</span></span> <span class="hljs-number"><span class="hljs-number">10.128</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span> <span class="hljs-number"><span class="hljs-number">123.123</span></span><span class="hljs-number"><span class="hljs-number">.123</span></span><span class="hljs-number"><span class="hljs-number">.12</span></span> RUNNING gke-my-web-production-<span class="hljs-keyword"><span class="hljs-keyword">large</span></span>-pool<span class="hljs-number"><span class="hljs-number">-123</span></span><span class="hljs-number"><span class="hljs-number">-123</span></span> us-west1-a n1-highcpu<span class="hljs-number"><span class="hljs-number">-8</span></span> <span class="hljs-number"><span class="hljs-number">10.128</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.2</span></span> <span class="hljs-number"><span class="hljs-number">50.50</span></span><span class="hljs-number"><span class="hljs-number">.50</span></span><span class="hljs-number"><span class="hljs-number">.50</span></span> RUNNING gke-my-web-production-web-<span class="hljs-keyword"><span class="hljs-keyword">load</span></span>-balancer<span class="hljs-number"><span class="hljs-number">-123</span></span><span class="hljs-number"><span class="hljs-number">-123</span></span> us-west1-a g1-small <span class="hljs-number"><span class="hljs-number">10.128</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.3</span></span> <span class="hljs-number"><span class="hljs-number">70.70</span></span><span class="hljs-number"><span class="hljs-number">.70</span></span><span class="hljs-number"><span class="hljs-number">.70</span></span> RUNNING</code> </pre> <br><p>  Save it: </p><br><p> <code>export LB_INSTANCE_NAME=gke-my-web-production-web-load-balancer-123-123</code> </p> <br><p>  You can manually reserve the external IP address and give it a name: </p><br><pre> <code class="hljs pgsql">gcloud compute addresses <span class="hljs-keyword"><span class="hljs-keyword">create</span></span> ip-web-production \ <span class="hljs-comment"><span class="hljs-comment">--ip-version=IPV4 \ --global</span></span></code> </pre> <br><p>  Suppose that it generated the reserved IP address "111.111.111.111".  Save it: </p><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">export</span></span> LB_ADDRESS_IP=$(gcloud compute addresses list | grep <span class="hljs-string"><span class="hljs-string">"ip-web-production"</span></span> | awk <span class="hljs-string"><span class="hljs-string">'{print </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$3</span></span></span><span class="hljs-string">}'</span></span>)</code> </pre> <br><p>  Link an address with a load balancer instance: </p><br><pre> <code class="hljs perl">export LB_INSTANCE_NAT=$(gcloud compute instances describe $LB_INSTANCE_NAME | <span class="hljs-keyword"><span class="hljs-keyword">grep</span></span> -A3 networkInterfaces: | tail -n1 | awk -F<span class="hljs-string"><span class="hljs-string">': '</span></span> <span class="hljs-string"><span class="hljs-string">'{print $2}'</span></span>) gcloud compute instances <span class="hljs-keyword"><span class="hljs-keyword">delete</span></span>-access-config $LB_INSTANCE_NAME \ --access-config-name <span class="hljs-string"><span class="hljs-string">"$LB_INSTANCE_NAT"</span></span> gcloud compute instances add-access-config $LB_INSTANCE_NAME \ --access-config-name <span class="hljs-string"><span class="hljs-string">"$LB_INSTANCE_NAT"</span></span> --address $LB_ADDRESS_IP</code> </pre> <br><p>  Add the rest of the Ingress Deployment configuration.  This may take a long time, but basically we will follow the pattern.  Let's start by defining another web application called <code>default-http-backend</code> .  It will be used to respond to HTTP requests if web containers are unavailable for any reason.  Let's call it <code>deploy / default-web.yml</code> : </p><br><pre> <code class="hljs pgsql">apiVersion: extensions/v1beta1 kind: Deployment metadata: <span class="hljs-type"><span class="hljs-type">name</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">default</span></span>-http-backend spec: replicas: <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">template</span></span>: metadata: labels: app: <span class="hljs-keyword"><span class="hljs-keyword">default</span></span>-http-backend spec: terminationGracePeriodSeconds: <span class="hljs-number"><span class="hljs-number">60</span></span> containers: - <span class="hljs-type"><span class="hljs-type">name</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">default</span></span>-http-backend # <span class="hljs-keyword"><span class="hljs-keyword">Any</span></span> image <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> permissable <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> long <span class="hljs-keyword"><span class="hljs-keyword">as</span></span>: # <span class="hljs-number"><span class="hljs-number">1.</span></span> It serves a <span class="hljs-number"><span class="hljs-number">404</span></span> page at / # <span class="hljs-number"><span class="hljs-number">2.</span></span> It serves <span class="hljs-number"><span class="hljs-number">200</span></span> <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> a /healthz endpoint image: gcr.io/google_containers/defaultbackend:<span class="hljs-number"><span class="hljs-number">1.0</span></span> livenessProbe: httpGet: <span class="hljs-type"><span class="hljs-type">path</span></span>: /healthz port: <span class="hljs-number"><span class="hljs-number">8080</span></span> scheme: HTTP initialDelaySeconds: <span class="hljs-number"><span class="hljs-number">30</span></span> timeoutSeconds: <span class="hljs-number"><span class="hljs-number">5</span></span> ports: - containerPort: <span class="hljs-number"><span class="hljs-number">8080</span></span> resources: limits: cpu: <span class="hljs-number"><span class="hljs-number">10</span></span>m memory: <span class="hljs-number"><span class="hljs-number">20</span></span>Mi requests: cpu: <span class="hljs-number"><span class="hljs-number">10</span></span>m memory: <span class="hljs-number"><span class="hljs-number">20</span></span>Mi</code> </pre> <br><p>  No need to change anything.  You are now familiar with the deployment pattern.  We need to unmask the template via NodePort, so let's add <code>deploy/default-web-svc.yml</code> : </p><br><pre> <code class="hljs pgsql">kind: Service apiVersion: v1 metadata: <span class="hljs-type"><span class="hljs-type">name</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">default</span></span>-http-backend spec: selector: app: <span class="hljs-keyword"><span class="hljs-keyword">default</span></span>-http-backend ports: - protocol: TCP port: <span class="hljs-number"><span class="hljs-number">80</span></span> targetPort: <span class="hljs-number"><span class="hljs-number">8080</span></span> <span class="hljs-keyword"><span class="hljs-keyword">type</span></span>: NodePort</code> </pre> <br><p>  Again, do not change anything.  The following 3 files are important parts.  First, we create the NGINX load balancer, let's call it <code>deploy / nginx.yml</code> : </p><br><pre> <code class="hljs pgsql">apiVersion: extensions/v1beta1 kind: Deployment metadata: <span class="hljs-type"><span class="hljs-type">name</span></span>: nginx-ingress-controller spec: replicas: <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">template</span></span>: metadata: labels: k8s-app: nginx-ingress-lb spec: # hostNetwork makes it possible <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> use ipv6 <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> preserve the source IP correctly regardless <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> docker <span class="hljs-keyword"><span class="hljs-keyword">configuration</span></span> # however, it <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> a hard dependency <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> the nginx-ingress-controller itself <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> it may cause issues <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> port <span class="hljs-number"><span class="hljs-number">10254</span></span> already <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> taken <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> the host # that said, since hostPort <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> broken <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> CNI (https://github.com/kubernetes/kubernetes/issues/<span class="hljs-number"><span class="hljs-number">31307</span></span>) we have <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> use hostNetwork <span class="hljs-keyword"><span class="hljs-keyword">where</span></span> CNI <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> used hostNetwork: <span class="hljs-keyword"><span class="hljs-keyword">true</span></span> terminationGracePeriodSeconds: <span class="hljs-number"><span class="hljs-number">60</span></span> nodeSelector: <span class="hljs-keyword"><span class="hljs-keyword">role</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">load</span></span>-balancer containers: - args: - /nginx-ingress-controller - "--default-backend-service=$(POD_NAMESPACE)/default-http-backend" - "--default-ssl-certificate=$(POD_NAMESPACE)/cloudflare-secret" env: - <span class="hljs-type"><span class="hljs-type">name</span></span>: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - <span class="hljs-type"><span class="hljs-type">name</span></span>: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace image: "gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.5" imagePullPolicy: <span class="hljs-keyword"><span class="hljs-keyword">Always</span></span> livenessProbe: httpGet: <span class="hljs-type"><span class="hljs-type">path</span></span>: /healthz port: <span class="hljs-number"><span class="hljs-number">10254</span></span> scheme: HTTP initialDelaySeconds: <span class="hljs-number"><span class="hljs-number">10</span></span> timeoutSeconds: <span class="hljs-number"><span class="hljs-number">5</span></span> <span class="hljs-type"><span class="hljs-type">name</span></span>: nginx-ingress-controller ports: - containerPort: <span class="hljs-number"><span class="hljs-number">80</span></span> <span class="hljs-type"><span class="hljs-type">name</span></span>: http protocol: TCP - containerPort: <span class="hljs-number"><span class="hljs-number">443</span></span> <span class="hljs-type"><span class="hljs-type">name</span></span>: https protocol: TCP volumeMounts: - mountPath: /etc/nginx-ssl/dhparam <span class="hljs-type"><span class="hljs-type">name</span></span>: tls-dhparam-vol volumes: - <span class="hljs-type"><span class="hljs-type">name</span></span>: tls-dhparam-vol secret: secretName: tls-dhparam</code> </pre> <br><p>  Notice that <code>nodeSelector</code> creates the node label that we added when we created the new node pool. </p><br><p>  You may want to work with labels, the number of replicas.  It is important to note that the volume I called <code>tls-dhparam-vol</code> is mounted here.  This is <code>Diffie Hellman Ephemeral Parameters</code> .  We generate it: </p><br><pre> <code class="hljs javascript">sudo openssl dhparam -out ~<span class="hljs-regexp"><span class="hljs-regexp">/documents/</span></span>dhparam.pem <span class="hljs-number"><span class="hljs-number">2048</span></span> kubectl create secret generic tls-dhparam --<span class="hljs-keyword"><span class="hljs-keyword">from</span></span>-file=<span class="hljs-regexp"><span class="hljs-regexp">/home/my</span></span>self/documents/dhparam.pem kubectl create secret generic tls-dhparam --<span class="hljs-keyword"><span class="hljs-keyword">from</span></span>-file=<span class="hljs-regexp"><span class="hljs-regexp">/home/my</span></span>self/documents/dhparam.pem</code> </pre> <br><p>  Please note that I am using version 0.9.0-beta_5 for the controller image.  It works well, no problems so far.  But watch out for release notes and for new versions and do your own testing. </p><br><p>  Again, let's unmask this Ingress controller through a load balancing service.  Let's call it <code>deploy / nginx-svc.yml</code> : </p><br><pre> <code class="hljs pgsql">apiVersion: v1 kind: Service metadata: <span class="hljs-type"><span class="hljs-type">name</span></span>: nginx-ingress spec: <span class="hljs-keyword"><span class="hljs-keyword">type</span></span>: LoadBalancer loadBalancerIP: <span class="hljs-number"><span class="hljs-number">111.111</span></span><span class="hljs-number"><span class="hljs-number">.111</span></span><span class="hljs-number"><span class="hljs-number">.111</span></span> ports: - <span class="hljs-type"><span class="hljs-type">name</span></span>: http port: <span class="hljs-number"><span class="hljs-number">80</span></span> targetPort: http - <span class="hljs-type"><span class="hljs-type">name</span></span>: https port: <span class="hljs-number"><span class="hljs-number">443</span></span> targetPort: https selector: k8s-app: nginx-ingress-lb</code> </pre> <br><p>  Remember the static external IP we reserved above and saved to <code>LB_INGRESS_IP ENV var</code> ?  You need to include it in the <code>spec: loadBalancerIP</code> section. This is also the IP address that you will add as a ‚Äúrecord‚Äù in your DNS service (say, mapping your www.my-app.com.br with CloudFlare). </p><br><p>  Now we can create our own configuration Ingress, let's create <code>deploy / ingress.yml</code> as follows: </p><br><pre> <code class="hljs pgsql">apiVersion: extensions/v1beta1 kind: Ingress metadata: <span class="hljs-type"><span class="hljs-type">name</span></span>: ingress annotations: kubernetes.io/ingress.<span class="hljs-keyword"><span class="hljs-keyword">class</span></span>: "nginx" nginx.org/ssl-services: "web-svc" kubernetes.io/ingress.<span class="hljs-keyword"><span class="hljs-keyword">global</span></span>-static-ip-<span class="hljs-type"><span class="hljs-type">name</span></span>: ip-web-production ingress.kubernetes.io/ssl-redirect: "true" ingress.kubernetes.io/rewrite-target: / spec: tls: - hosts: - www.my-app.com.br secretName: cloudflare-secret rules: - host: www.my-app.com.br http: paths: - <span class="hljs-type"><span class="hljs-type">path</span></span>: / backend: serviceName: web-svc servicePort: <span class="hljs-number"><span class="hljs-number">80</span></span></code> </pre> <br><p>  So we connected the NodePort service created for the web modules with the nginx login controller and added SSL termination via <code>spec: tls: secretName</code> .  How to create it?  First, you must purchase an SSL certificate using CloudFlare as an example. </p><br><p>  When you purchase, the provider must provide you with the secret download files (keep them safe! The shared folder with Dropbox is not safe!).  Then you must add it to the infrastructure as follows: </p><br><pre> <code class="hljs pgsql">kubectl <span class="hljs-keyword"><span class="hljs-keyword">create</span></span> secret tls cloudflare-secret \ <span class="hljs-comment"><span class="hljs-comment">--key ~/downloads/private.pem \ --cert ~/downloads/fullchain.pem</span></span></code> </pre> <br><p>  Now that we have edited a lot of files, we can deploy the entire load balancer package: </p><br><pre> <code class="hljs pgsql">kubectl <span class="hljs-keyword"><span class="hljs-keyword">create</span></span> -f deploy/<span class="hljs-keyword"><span class="hljs-keyword">default</span></span>-web.yml kubectl <span class="hljs-keyword"><span class="hljs-keyword">create</span></span> -f deploy/<span class="hljs-keyword"><span class="hljs-keyword">default</span></span>-web-svc.yml kubectl <span class="hljs-keyword"><span class="hljs-keyword">create</span></span> -f deploy/nginx.yml kubectl <span class="hljs-keyword"><span class="hljs-keyword">create</span></span> -f deploy/nginx-svc.yml kubectl <span class="hljs-keyword"><span class="hljs-keyword">create</span></span> -f deploy/ingress.yml</code> </pre> <br><p>  This NGINX Ingress configuration is based on a <a href="https://zihao.me/post/cheap-out-google-container-engine-load-balancer/">Zihao Zhang blog post</a> .  There are also examples in <a href="">the cubernet incubator repository</a> .  You can check it out. </p><br><p>  If you did everything right, <a href="https://www.my-app-com.br/">https://www.my-app-com.br</a> should download your web application.  You can check the time on the first byte (TTFB) via CloudFlare as follows: </p><br><pre> <code class="hljs perl">curl -vso /dev/null -w <span class="hljs-string"><span class="hljs-string">"Connect: %{time_connect} \n TTFB: %{time_starttransfer} \n Total time: %{time_total} \n"</span></span> https:<span class="hljs-regexp"><span class="hljs-regexp">//www</span></span>.my-app.com.br</code> </pre> <br><p>  If you have a slow TTFB: </p><br><pre> <code class="hljs perl">curl --resolve www.my-app.com.br:<span class="hljs-number"><span class="hljs-number">443</span></span>:<span class="hljs-number"><span class="hljs-number">111.111</span></span>.<span class="hljs-number"><span class="hljs-number">111.111</span></span> https:<span class="hljs-regexp"><span class="hljs-regexp">//www</span></span>.my-app.com.br -svo /dev/null -k -w <span class="hljs-string"><span class="hljs-string">"Connect: %{time_connect} \n TTFB: %{time_starttransfer} \n Total time: %{time_total} \n"</span></span></code> </pre> <br><p>  TTFB should be around 1 second or less.  Otherwise, it means that there is an error in the application.  It is necessary to check the types of instances of machine nodes, the number of workers loaded on one module, the version of the CloudSQL proxy, the version of the NGINX controller, etc. This is a trial and error procedure.  Subscribe to <a href="https://loader.io/">Loader</a> or <a href="https://www.webpagetest.org/">Web Page Test</a> for understanding. </p><br><h2 id="rolling-obnovlenie">  Rolling update </h2><br><p>  Now that everything is working, how do I perform the Rolling Update update, which I mentioned at the beginning?  First, run <code>git push</code> to the Container registry repository and wait until the Docker image is created. </p><br><p>  Remember, I made the trigger put an image with a random version number?  Let's use (you can see it in the Build History list in the Google Cloud Container Registry): </p><br><pre> <code class="hljs sql">kubectl <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> image deployment web my-app=gcr.io/my-<span class="hljs-keyword"><span class="hljs-keyword">project</span></span>/my-app:<span class="hljs-number"><span class="hljs-number">1238471234</span></span>g123f534f543541gf5 <span class="hljs-comment"><span class="hljs-comment">--record</span></span></code> </pre> <br><p>  You must use the same name and image that is declared in <code>deploy/web.yml</code> .  A rolling update, the addition of a new module, and then the completion of the block will begin until all of them are updated without any downtime for users. </p><br><p>  Rolling updates should be performed carefully.  For example, if a database migration is required for deployment, you should add a maintenance window (you need to do it in the middle of the night, when the traffic volume is low).  Thus, you can run the migration command as follows: </p><br><pre> <code class="hljs pgsql">kubectl <span class="hljs-keyword"><span class="hljs-keyword">get</span></span> pods # <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> <span class="hljs-keyword"><span class="hljs-keyword">get</span></span> a pod <span class="hljs-type"><span class="hljs-type">name</span></span> kubectl exec -it my-web<span class="hljs-number"><span class="hljs-number">-12324</span></span><span class="hljs-number"><span class="hljs-number">-121312</span></span> /app/bin/rails db:migrate # you can <span class="hljs-keyword"><span class="hljs-keyword">also</span></span> bash <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> a pod <span class="hljs-keyword"><span class="hljs-keyword">like</span></span> this, but remember that this <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> an ephemeral container, so file you edit <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-keyword"><span class="hljs-keyword">write</span></span> there disappear <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> the next <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span>: kubectl exec -it my-web<span class="hljs-number"><span class="hljs-number">-12324</span></span><span class="hljs-number"><span class="hljs-number">-121312</span></span> bash</code> </pre> <br><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> To redistribute everything without resorting to a rolling upgrade, you need to do the following: </font></font></p><br><pre> <code class="hljs sql">kubectl <span class="hljs-keyword"><span class="hljs-keyword">delete</span></span> -f deploy/web.yml &amp;&amp; kubectl <span class="hljs-keyword"><span class="hljs-keyword">apply</span></span> -f deploy/web.yml</code> </pre> <br><h2 id="bonus-avtosnimki"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Bonus: Auto Shoot </font></font></h2><br><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">On my list ‚ÄúI want‚Äù there was an item - to have permanent mounted storage with automatic backup / snapshots. </font><font style="vertical-align: inherit;">Google Cloud provides half of this. </font><font style="vertical-align: inherit;">To connect to the modules, you can create permanent disks, but without the function of automatic backup. </font><font style="vertical-align: inherit;">However, the repository has an API for creating a snapshot manually.</font></font></p><br><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Create a new SSD disk and format it: </font></font></p><br><pre> <code class="hljs pgsql">gcloud compute disks <span class="hljs-keyword"><span class="hljs-keyword">create</span></span> <span class="hljs-comment"><span class="hljs-comment">--size 500GB my-data --type pd-ssd gcloud compute instances list</span></span></code> </pre> <br><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The last command allows you to copy the instance name of the node. </font><font style="vertical-align: inherit;">Suppose it is </font></font><code>gke-my-web-app-default-pool-123-123</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">We will attach a disk </font></font><code>my-data</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">to it:</font></font></p><br><pre> <code class="hljs perl">gcloud compute instances attach-disk gke-<span class="hljs-keyword"><span class="hljs-keyword">my</span></span>-web-app-default-pool-<span class="hljs-number"><span class="hljs-number">123</span></span>-<span class="hljs-number"><span class="hljs-number">123</span></span> --disk <span class="hljs-keyword"><span class="hljs-keyword">my</span></span>-data --device-name <span class="hljs-keyword"><span class="hljs-keyword">my</span></span>-data gcloud compute ssh gke-<span class="hljs-keyword"><span class="hljs-keyword">my</span></span>-web-app-default-pool-<span class="hljs-number"><span class="hljs-number">123</span></span>-<span class="hljs-number"><span class="hljs-number">123</span></span></code> </pre> <br><p>   ssh  .        <code>sudo lsblk</code> ,     500 , ,  <code>/ dev / sdb</code> . ,    ,    ! </p><br><pre> <code class="hljs pgsql">sudo mkfs.ext4 -m <span class="hljs-number"><span class="hljs-number">0</span></span> -F -E lazy_itable_init=<span class="hljs-number"><span class="hljs-number">0</span></span>,lazy_journal_init=<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-keyword"><span class="hljs-keyword">discard</span></span> /dev/sdb</code> </pre> <br><p>       SSH   : </p><br><pre> <code class="hljs haskell"><span class="hljs-title"><span class="hljs-title">gcloud</span></span> compute instances detach-disk gke-my-web-app-<span class="hljs-keyword"><span class="hljs-keyword">default</span></span>-pool-123-123 <span class="hljs-comment"><span class="hljs-comment">--disk my-data</span></span></code> </pre> <br><p>        ,    yaml : </p><br><pre> <code class="hljs kotlin">spec: containers: - image: ... name: my-app volumeMounts: - name: my-<span class="hljs-keyword"><span class="hljs-keyword">data</span></span> mountPath: /<span class="hljs-keyword"><span class="hljs-keyword">data</span></span> # readOnly: <span class="hljs-literal"><span class="hljs-literal">true</span></span> # ... volumes: - name: my-<span class="hljs-keyword"><span class="hljs-keyword">data</span></span> gcePersistentDisk: pdName: my-<span class="hljs-keyword"><span class="hljs-keyword">data</span></span> fsType: ext4</code> </pre> <br><p>     CronJob  <code>deploy / auto-snapshot.yml</code> : </p><br><pre> <code class="hljs pgsql">apiVersion: batch/v1beta1 kind: CronJob metadata: <span class="hljs-type"><span class="hljs-type">name</span></span>: auto-<span class="hljs-keyword"><span class="hljs-keyword">snapshot</span></span> spec: schedule: "0 4 * * *" concurrencyPolicy: Forbid jobTemplate: spec: <span class="hljs-keyword"><span class="hljs-keyword">template</span></span>: spec: restartPolicy: OnFailure containers: - <span class="hljs-type"><span class="hljs-type">name</span></span>: auto-<span class="hljs-keyword"><span class="hljs-keyword">snapshot</span></span> image: grugnog/google-cloud-auto-<span class="hljs-keyword"><span class="hljs-keyword">snapshot</span></span> command: ["/opt/entrypoint.sh"] env: - <span class="hljs-type"><span class="hljs-type">name</span></span>: "GOOGLE_CLOUD_PROJECT" <span class="hljs-keyword"><span class="hljs-keyword">value</span></span>: "my-project" - <span class="hljs-type"><span class="hljs-type">name</span></span>: "GOOGLE_APPLICATION_CREDENTIALS" <span class="hljs-keyword"><span class="hljs-keyword">value</span></span>: "/credential/credential.json" volumeMounts: - mountPath: /credential <span class="hljs-type"><span class="hljs-type">name</span></span>: editor-credential volumes: - <span class="hljs-type"><span class="hljs-type">name</span></span>: editor-credential secret: secretName: editor-credential</code> </pre> <br><p>     ,              IAM &amp; admin  Google Cloud,     JSON , ,    : </p><br><pre> <code class="hljs pgsql">kubectl <span class="hljs-keyword"><span class="hljs-keyword">create</span></span> secret generic editor-credential \ <span class="hljs-comment"><span class="hljs-comment">--from-file=credential.json=/home/myself/download/my-project-1212121.json</span></span></code> </pre> <br><p>   :     cron.   ¬´0 4 *¬ª ,          4  . </p><br><p>     <a href="https://github.com/grugnog/google-cloud-auto-snapshot"> </a>  . </p><br><p>    ,    ,      .     Kubernetes,   Deployment, Service, Ingress,     ReplicaSet, DaemonSet   . </p><br><p>   ,    multi-region High Availability,   . </p><br><p> : <a href="http://www.akitaonrails.com/2018/01/09/my-notes-about-a-production-grade-ruby-on-rails-deployment-on-google-cloud-kubernetes-engine">My Notes about a Production-grade Ruby on Rails Deployment on Google Cloud Kubernetes Engine</a> </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/348394/">https://habr.com/ru/post/348394/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../348382/index.html">Is it true that PM does something useful and what it takes to become</a></li>
<li><a href="../348384/index.html">Pygest # 22. Releases, articles, interesting projects, packages and libraries from the world of Python [January 18, 2018 - February 4, 2018]</a></li>
<li><a href="../348386/index.html">What really happened to vista</a></li>
<li><a href="../348390/index.html">Web typography: create tables for reading, not for beauty</a></li>
<li><a href="../348392/index.html">The obvious benefit: how and why to use the service approach beyond IT (Part 2)</a></li>
<li><a href="../348396/index.html">How I hacked the bitcoin mining pool</a></li>
<li><a href="../348398/index.html">Product Design Digest, January 2018</a></li>
<li><a href="../348400/index.html">Conceptual sorting in C ++ 20</a></li>
<li><a href="../348402/index.html">Nemesida WAF: intellectual protection against brute-force attacks</a></li>
<li><a href="../348404/index.html">How zadolbat all colleagues: collect requirements for CRM</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>