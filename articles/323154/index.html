<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Microservices: experience of use in a loaded project</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="At the HighLoad ++ 2016 conference, M-Tech development manager Vadim Madison spoke about growth from a system for which a hundred microservices seemed...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Microservices: experience of use in a loaded project</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/4b7/be2/933/4b7be2933a7c3f6e17e14a4f3546833f.jpg"><br><br>  <em>At the <a href="http://www.highload.ru/">HighLoad ++</a> 2016 conference, M-Tech development manager Vadim Madison spoke about growth from a system for which a hundred microservices seemed like a huge number to a loaded project, where a couple of thousand microservices are routine.</em> <br><br>  The topic of my report is how we launched microservices in production on a fairly loaded project.  This is a kind of aggregated experience, but since I work at M-Tech, let me tell you a few words about who we are. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In short, we are engaged in video delivery - give the video in real time.  We are a video platform for "NTV-Plus" and "Match TV".  These are 300 thousand simultaneous users who resort in 5 minutes.  This is 300 terabytes of content that we give per hour.  This is such an interesting task.  How does this all serve? <br><br>  What is this story about?  This is about how we grew up, how the project developed, how some rethinking of some of its parts, of some kind of interaction took place.  Anyway, this is about project scaling, because this is all - in order to withstand even more load, provide customers with even more functionality and not fall, not lose key characteristics.  In general, to make the client satisfied.  Well, a little bit about what path we have gone.  Where we started. <br><a name="habracut"></a><br><img src="https://habrastorage.org/getpro/habr/post_images/810/3bd/175/8103bd175215816dba6f4ddd0286aa41.jpg"><br><br>  This is the starting point, some starting point, when we had 2 servers in the Docker cluster.  Then the databases were run in the same cluster.  Something of this kind in our infrastructure was not.  The infrastructure was minimal. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/dfe/ea0/383/dfeea038320688a0e8e0d267b0e7d2b4.jpg"><br><br>  If you look at what was in our core infrastructure, then this is Docker and TeamCity as a system for delivering code, assemblies, and so on. <br><br>  The next milestone - what I call the middle of the road - was a fairly serious growth of the project.  When we got 80 servers already.  When we built a separate dedicated cluster for databases on special machines.  When we began to move to a distributed repository based on CEPH.  When we began to think that it might be time to reconsider how our services interact with each other, and came close to the fact that it was time for us to change our monitoring system. <br><br>  Well, actually what we have come to now.  There are already hundreds of servers in the Docker cluster ‚Äî hundreds of running microservices.  Now we have come to the conclusion that we are starting to divide our system into certain service subsystems at the level of data buses, at the level of logical separation of systems.  When these microservices became too much, we began to split up the system in order to better serve it, to understand it better. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/876/1b9/163/8761b9163e84a424f456dc11c9b18aba.jpg"><br><br>  Now on the screen you see the scheme.  This is one small piece of our system.  This is the thing that allows you to cut video.  I showed a similar scheme six months ago on RIT ++.  Then green microservices were, in my opinion, 17 pieces.  Now there are 28 of them. If you look at it, this is 1/20 of our system.  You can imagine the approximate scale. <br><br><h1>  Details </h1><br>  One of the interesting points is the transport between our services.  Classically begin with the fact that transport should be as efficient as possible.  We also thought about it, decided that protobuf is our everything. <br><br>  It looked like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/96f/556/0e5/96f5560e5f3c034100b587375b6ed426.jpg"><br><br>  The request via Load Balancer comes to front-line microservices.  These are either Frontend, or services that provide the API directly, they worked through JSON.  And to the internal services, the requests went already through protobuf. <br><br>  By itself, protobuf is such a pretty good thing.  It really gives a certain compactness in the messaging.  Now there are already fairly fast implementations that allow serializing and deserializing data with minimal overhead.  It can be considered a conditionally typed query. <br><br>  But if you look at the section of microservices, it is noticeable that between services you get some semblance of a proprietary protocol.  As long as you have 1, 2 or 5 services, you can safely release a console utility for each microservice, which will allow you to access a specific service and check what it returns.  If he blunted something - pull him and see.  This somewhat complicates the work with these services from the point of view of support. <br><br>  Before a certain stage, this was not a serious problem - there were not so many services.  Plus, guys from Google released gRPC.  We looked at what, in principle, for our purposes at that time he was doing everything we needed.  We slowly migrated to it.  And bams - one more thing appeared in our stack. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/aa1/8fa/9d3/aa18fa9d3d7a9163e81fed4b49eb0d52.jpg"><br><br>  Here is also quite an interesting detail in the implementation.  This thing is based on HTTP / 2.  This is the thing that really works out of the box.  If your environment is not very dynamic, if your instances do not change, do not move around the cars often enough, then this is, in general, a good thing.  Moreover, at the moment there is support for a bunch of languages ‚Äã‚Äã- both server and client. <br><br>  Now if you look at this in the context of microservices.  On the one hand, the thing is good, but on the other - it is a thing in itself.  So much so that when we began to standardize our logs in order to aggregate them in a single system, we were faced with the fact that you cannot get logs directly from the gRPC. <br><br>  As a result, we came to the conclusion that we wrote our own logging system, slipped it into gRPC.  She made parsing of the messages issued via gRPC with us, brought them to our mind, and then we could put it into our logging system normally.  And plus the situation when you first describe the service and the types of this service, then compile them, increases the dependence of the services among themselves.  For microservices this is a certain problem, the same as a certain complexity of versioning. <br><br>  As you probably already guessed, in the end we came to the conclusion that we began to think about JSON.  And we ourselves did not believe for a long time that after some kind of compact, conditionally binary protocol, we suddenly return to JSON until we came across an article from the DailyMotion guys who wrote about the same thing: ‚ÄúDamn, we also know how to cook JSON, they know how to cook everything in the world, why do we create additional difficulties for ourselves? ‚Äù <br><br><img src="https://habrastorage.org/getpro/habr/post_images/572/14d/e48/57214de489d8d8a5673c406a3d7ccbda.jpg"><br><br>  As a result, we gradually began to migrate from gRPC to JSON in some of its implementation.  That is, yes, we left HTTP / 2, we took fast enough implementations to work with JSON. <br><br>  Got all those buns that we have.  We can contact our service via cURL.  Our testers use Postman, and they are fine too.  At any stage of working with these services, everything became simple.  This is one thing that, on the one hand, is a controversial decision, and on the other hand, it really helps with maintenance. <br><br>  By and large, if you look at JSON, then the only real disadvantage that you can present to it right now is the lack of compactness of this description.  Those 30%, which, as statistically stated, are the difference between the same MessagePack or something else, in fact, according to our measurements, the difference is not so big, and it is not always so critical when we talk about a supported system. <br><br>  Plus, with the transition to JSON, we received additional buns.  Such as, for example, protocol versioning.  At some point, the situation began to emerge in us, that through the same protobuf we describe some kind of new version of the protocol.  Accordingly, customers, consumers of this particular service, must also move to it.  It turns out that if you have several hundred services, even 10% of them must move.  This is a large cascade effect.  You have changed in one service, and 10 more need to be redone. <br><br>  As a result, we began to develop a situation where the developer of this service released the fifth, sixth, seventh version, and in fact the production load is still on the fourth, because developers of related services have their deadlines and priorities.  They simply do not have the ability to constantly rebuild the service, move to a new version of the protocol.  Actually it turned out that new versions are released, but they are not in demand.  But the bugs in the old versions, we have to implement some obscure ways.  This complicated support. <br><br>  As a result, we came to the conclusion that we stopped producing versions of the protocols.  We fixed a certain basic version, within the framework of which we can add some properties, but in some very limited limits.  And consumer services began to use JSON-scheme. <br><br>  This is how it looks like: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a24/e0a/0ec/a24e0a0ec9814e0d349f74f3354b7183.jpg"><br><br>  Instead of 1, 2 and 3, we have version 1 and the scheme that applies to it. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/768/0b7/59c/7680b759cb85388fbdc635bb2402b078.jpg"><br><br>  Here is a typical response from one of our services.  This is Content Manager.  He gave information about the broadcast.  Here, for example, the scheme of one of the consumers. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/caf/0bd/bd2/caf0bdbd276b3cb9d56a7ff4f6f9001f.jpg"><br><br>  Here the most interesting line is the bottom one, where we have the required block.  If we look, we will see that this service actually needs only 4 fields from all this data - id, content, date, status.  If you really apply this scheme, then as a result the consumer service needs only this data. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7d8/96b/dff/7d896bdff6de0658963cc0f677b05f3d.jpg"><br><br>  They really are in every version, in every variation of the first version of the protocol.  This simplified the move to new versions.  We began to release new releases, and the migration of consumers to them has greatly simplified. <br><br>  The next, important moment that arises when we talk about microservices, and, in general, about any system.  Just in microservices it feels stronger and faster.  These are situations when the system becomes unstable. <br><br>  When you have a chain of calls for 1-2 services, then there are no special problems.  You don‚Äôt see any global difference between a monolithic and a distributed application.  But when your chain grows to 5-7, at some stage something fell off.  You really do not know why it fell off, what to do about it.  Debugging is quite difficult.  If, at the level of a monolithic application, you turned on the debugger, just went through the steps and found this error, then you have such factors as network instability, unstable operation under load, and something else.  And such things - in such a distributed system, with a bunch of such nodes - become very noticeable. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bc7/839/de9/bc7839de9a7caa0fe3228532f8c7b488.jpg"><br><br>  Then, at the beginning, we went the classic way.  We decided to monitor everything, to understand what was breaking and where, to try to somehow deal with it promptly.  We began to send metrics from our microservices, collect them into a single database.  We, through Diamond, began to collect data on the machines, what happens to them through cAdvisor.  We began to collect information on Docker-containers, merge it all into InfluxDB and build dashboards in Grafana. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/851/adf/91a/851adf91ac8b4a010b77217b671f8ee6.png"><br><br>  And here we have 3 more bricks in our infrastructure, which is gradually growing. <br><br>  Yes, we began to understand more what is happening here.  We began to respond more quickly to the fact that something fell apart.  But it did not stop falling apart. <br><br>  Because, oddly enough, the main problem of the microservice architecture is precisely the fact that you have services that are unstable.  That works, it does not work, and the reasons for this can be a lot.  Up to the fact that your service is overloaded, and you send an additional load on it, it goes down for a while.  After some time due to the fact that he does not serve everything, the load from him decreases, and he begins to serve again.  Such a leapfrog leads to the fact that such a system is very difficult to maintain and to understand what is wrong with it. <br><br>  As a result, we gradually came to the conclusion that it is better for this service to fall, than it jumps here and there.  This understanding led us to the fact that we began to change our approach to how we implement our services. <br><br>  The first of the important points.  We began to introduce in each of our service restrictions on incoming requests.  Each service we have become aware of how much it is able to serve customers.  How does he know this, I will tell you a little later.  He stops accepting all those requests that are above or above this limit.  He gives honest 503 Service Unavailable.  The one who addresses to it, understands that it is necessary to choose other note - this is unable to serve. <br><br>  Thus, we reduce the request time in case something is wrong with the system.  On the other hand, we increase its stability. <br><br>  Second moment.  If rate limiting is on the side of the destination service, then the second pattern that we began to introduce everywhere is Circuit Breaker.  This is a pattern that we, roughly speaking, implement on the client. <br><br>  Service A, he has as possible points of appeal, for example, 4 instances of service B. So he went to the registry, said: "Give me the addresses of these services."  Got them 4 pieces.  Went to the first, he replied that everything is ok.  Service marked "yes", you can go to him.  According to Round Robin, he scatters appeals.  Went to the second, he did not answer for the right time.  Everything, we bany him for some time and go to the next.  That one, for example, returns an incorrect version of the protocol with us - no matter why.  He bans him too.  Going to the fourth. <br><br>  The result is 50% of the services, they are really able to help him serve the client.  To these two he will go.  Those two that for some reason did not suit him, he bans for some time. <br><br>  This allowed us to seriously increase the stability of the work as a whole.  There is something wrong with the service - we shoot it off, an alert is raised that the service has been shot off, and we understand further what could be wrong. <br><br>  In response to the introduction of the Circuit Breaker pattern, we have another thing in our infrastructure - Hystrix. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2da/b94/48d/2dab9448dd63a0b209dab4a431f2ae23.jpg"><br><br>  The guys from Netflix not only realized the support of this pattern, but also did it visually how to understand if something was wrong with your system: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a9b/ab2/db0/a9bab2db0a8458e3d26a6abeec1d0272.png"><br><br>  Here the size of this circle shows how much traffic you have relative to others.  The color indicates how good or bad the system is.  If you have a green circle, then probably you are fine.  If red - not so rosy. <br><br>  Something like this, when you have to completely shoot the service.  A switch worked on it. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/088/304/e19/088304e19ec16dfe41b061b734726b51.png"><br><br>  We have ensured that our system has become more or less stable.  We have each service had at least two instances, so that we could switch by shooting one or the other.  But this did not give us an understanding of what was happening with our system.  If we have somewhere along the way fell off during the execution of the request, then how to understand it? <br><br>  Here is the standard query: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/43e/eed/c4b/43eeedc4b1516778abd98f70943aaa25.jpg"><br><br>  Such a chain of execution.  From the user came a request for the first service, then for the second, from the second he split the branch to the third and fourth. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7e6/678/606/7e6678606f7ebebaf226b3da47cec56c.jpg"><br><br>  Wham, and we have one of the branches disappeared.  Really wonder why.  When we faced this situation and began to figure out what to do here, how we can improve the visibility of the situation, we came across such a thing as Appdash.  This is a trace service. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/01b/292/2cc/01b2922cc7b8083541759948bff9e7e0.jpg"><br><br>  It looks like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/416/f72/5ac/416f725ac00123ad0a7b687911340fb8.jpg"><br><br>  I must say, it was exactly the thing to try, to understand, or it was.  It was easiest to implement it into our system, because by that moment we had moved quite tightly to Go.  Appdash had a ready connection library.  We looked at that, yes, this thing helps us, but the implementation itself is not very convenient for us. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/565/cd2/b5e/565cd2b5e033e316293d63747a2f4920.jpg"><br><br>  Then, instead of Appdash, we got a Zipkin.  This is the thing that the guys from Twitter did.  It looks like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f00/56b/d50/f0056bd50e15a15db0010ba64d9aea03.jpg"><br><br>  It seems to me a little more visually.  Here we see that we have a certain number of services.  We see how our request passes through this chain, we see what part of this service eats up in each of the services.  On the one hand, we see a certain general time and division by services, on the other - no one bothers us to add here the same information about what is happening inside the service. <br><br>  That is, some kind of payload, access to the database, reading out something from the file system, accessing the caches - this can all be added here and see what could be the most time added to this request in your request.  The thing that allows us to do this probros is a through TraceID.  I will continue to talk about him a little bit. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/463/bbd/750/463bbd750edb6825a68f21138dc810ae.png"><br><br>  This is how we began to understand what is happening in a particular request, why it suddenly falls for a particular client.  All is well and suddenly someone has something wrong.  We began to see a certain basic context and understand what is happening with the service. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cbe/f42/3f8/cbef423f81db54493e0651a62e03559e.png"><br><br>  Not so long ago, a certain standard was developed for the tracing system.  Just some agreement between the main suppliers of trace systems on how to implement the client API and client libraries so that this implementation can be made as simple as possible.  Now there is an implementation through Opentracing for almost all major languages.  You can safely use. <br><br>  We learned to understand which of the services suddenly did not allow us to serve the client.  We see that some of the parts blunted, but it is not always clear why.  The context is insufficient. <br><br>  We have logging.  Yes, this is a fairly standard thing, this is ELK.  Maybe in our small variation. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0a6/d39/ab7/0a6d39ab7c3516cba6b85db8cb56380d.png"><br><br>  We do not collect directly through the heap forward as Logstash.  We first send it to Syslog, with the help of Syslog we aggregate it on collecting machines.  From there already through forward we put in ElasticSearch and in Kibana.  Relatively standard stuff.  What is the trick? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/20e/04f/bd0/20e04fbd000d9413102ad01616c843a6.png"><br><br>  In that, wherever it is possible, where we really understand that it really refers to this particular request, we began to add the TraceID that I showed on the screen with Zipkin to these logs. <br><br>  As a result, we see in the logs on Kashana Dashboard the full context of execution for a specific user.  Obviously, if the service got into prod, then it is conditionally already working.  He passed autotests, testers have already looked at him, if necessary.  It should work.  If it does not work in any particular situation, then, apparently, there were some prerequisites.  These prerequisites in this detailed log, which we see with such filtering on a specific trace for a particular request, help to understand much more quickly what exactly is wrong in this situation.  As a result, we have seriously reduced the time to understand the causes of the problem. <br><br>  The next interesting point.  We have entered a dynamic debug mod.  In principle, we now have not such a wild amount of logs - about 100-150 gigabytes, I do not remember the exact figure.  But it is in the basic logging mode.  If we wrote at all super-detail, it would be terabytes.  To process them would be insanely expensive. <br><br>  Therefore, when we see that we have some kind of problem, we go to specific services, enable debug mod on them through the API and watch what happens.  Sometimes we first see what happens.  Sometimes we shoot off a service that creates a problem with us, without turning it off, we turn on the debug mod on it and then we already understand what was wrong with it. <br><br>  As a result, this rather seriously helps us in terms of the ELK stack, which is quite voracious.  On some critical services, we additionally do error aggregation.  That is, the service itself understands that it is a very critical error, which is moderately critical, and resets it all into Sentry. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ee4/6fd/b74/ee46fdb749a941dc448966f3ef323213.png"><br><br>  She cleverly enough knows how to aggregate these errors, reduce them by certain metrics, and make filters on basic things.  On a number of services we use it.  And we started using it from the time when we had monolithic applications, which we still have.  Now we enter on some services on the microservice architecture. <br><br>  The most interesting thing.  How do we scale this whole kitchen?  Here you need to tell some introductory.  For each of our car that serves the project, we treat as a black box. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a15/d17/f23/a15d17f23d1b26f96753f31be3f46536.png"><br><br>  We have an orchestration system.  We started with Nomad.  But no, in fact, we started with Ansible, with our scripts.  At some point this began to be missed.  By that time, there was already some version of Nomad.  We looked, she bribed us with its simplicity.  We decided that this is the thing that we can move to now. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d34/1a6/a1e/d341a6a1ede53258470c18f58db2856f.png"><br><br>  Along the way, Consul appeared with her as a registry for service discovery.  Also Vault, in which we store the secret data: passwords, keys, all secret that can not be stored in Git. <br><br>  Thus, it turned out that all cars became conditionally identical.  There is a Docker on the machine, there is a Consul agent on it, a Nomad agent.  This, by and large, is a finished machine, which can be taken and copied one-on-one, at the right moment to put into operation.  When they become unnecessary, can be decommissioned.  Moreover, if you have a cloud, then you can prepare the machine in advance, turn it on at peak times.  And when the load fell back, turn off.  This is quite a serious savings. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d34/1a6/a1e/d341a6a1ede53258470c18f58db2856f.png"><br><br>  At some point, Nomad we outgrew.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We moved to Kubernetes, and Consul began to play the role of a central configuration system for our services with all that it implies. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We have come to the fact that we have some kind of stack in order to automatically scale.</font></font> How do we do it? <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">First step. </font><font style="vertical-align: inherit;">We have imposed some limits on three characteristics: memory, processor, network. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/94c/8bf/b79/94c8bfb79be4ce624a6e9f62b2fd4a61.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We recorded three gradations in each of these quantities. </font><font style="vertical-align: inherit;">Cut some bricks.</font></font> As an example: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d42/583/158/d4258315831321b20f5ab3c2518c476f.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">R3-C2-N1. We have limited a certain service, gave it quite a bit of network, a little more processor and a lot of memory. There is some gluttonous service. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We introduce mnemonics exactly, because we can dynamically twist enough values ‚Äã‚Äãin a wide range already in our system, which we call the decision service. At the moment, these values ‚Äã‚Äãare approximately as follows: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/37c/620/38a/37c62038a4f7f664501b5255c3c8a0e9.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In fact, we still have C4, R4, but these are values ‚Äã‚Äãthat are completely beyond the scope of these standards. They are negotiated separately.</font></font><br>  It looks like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c25/4e1/a0b/c254e1a0bf9f9d792df260606e2d299c.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The next preparatory stage. We look at what type of scalability for this service. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The easiest is when your service is completely independent. You can linearly rivet this service. 2 times more users came - you launched 2 times more instances. You are fine again. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The second type is when your scalability depends on external resources. Roughly speaking, this service is included in the database. The base has a certain opportunity to serve a certain number of customers. You must take this into account. Either you need to understand when the degradation of the system begins and you cannot add more instances, or simply somehow understand how much you can already rest on it now.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And the third, most interesting option is when you are limited to some kind of external system. As an example - external billing. You know that he will not serve more than 500 requests. And even though you are launching 100 of your services, it‚Äôs still 500 requests to billing, and hello! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We must also take these limits into account. So we realized what type of service we have, we put the corresponding tag and then we look at how it works in our pipeline. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/67e/525/6c2/67e5256c265ad9dc695c887051cd572e.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">By default, we have assembled on the CI server, and have launched some unit tests. We have passed integration tests on the test environment, testers have checked something with us. Then we went to load testing in pre-production.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/fe3/f41/5a2/fe3f415a2c56bd758412baaca95036f9.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If we have a service of the first type, then we take an instance, run it in this isolated environment and give it the maximum load. We make several rounds, we take the minimum number from the received values. We put it in InfluxDB and say that this is the limit that is in principle possible for this service.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If we have a service of the second type, then here we run these instances in increment in some quantity, until we see that the degradation of the system has begun. We appreciate how fast or slow it is. Here we draw conclusions, if we know any particular load on our systems, is that enough at all? Is there a stock we need? If it is not there, then at this stage we set an alert and do not release this service in production. We are saying to the developers: ‚ÄúGuys, you either need to shard something or else introduce some kind of toolkit that would allow for a more linear scaling of this service.‚Äù</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/124/aa4/47b/124aa447bc6be92ceb0e975fc8eec300.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If we are talking about a third type of service, then we know its limit, we launch one copy of our service, in the same way we give the load and see how much this service can serve. If we know, for example, that the limit of the same billing is 1000 requests, 1 instance serves 200, then we understand that 5 instances are the maximum that can correctly serve it. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">All this information in InfluxDB we saved. Decision service appears. It looks 2 borders: upper and lower. By moving beyond the upper boundary, he realizes that you need to add instances, and maybe even machines for these instances. The reverse is also true. When the load drops (night), we don‚Äôt need so many machines, we can reduce the number of instances on some services, turn off the machines and thereby save a little money.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The general scheme looks like this: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/213/00a/30e/21300a30e34b8c947a04af3c79387081.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Each service through its metrics regularly says what is the current load on it. She goes to the same InfluxDB. When the Decision service sees that for this particular version of this particular instance we are approaching the threshold, it already gives the Nomad or Kubernetes command to add new instances. Perhaps he is initiating a new service in the cloud before that, perhaps he is doing some other preparatory work. But the bottom line is that it initiates the need to raise a new instance.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If it is clear that we will soon reach the limit for some limited services, then he raises the appropriate alert. Yes, we conditionally cannot do anything with it except to save the queue or something else, but at least we know that we may soon have such a problem and can already begin to prepare for it. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This is what concerns scaling in some common things. And all this parsley with a bunch of services, it eventually led to the fact that we looked at another such thing a little sideways - this is Gitlab CI.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/6f8/b4c/c68/6f8b4cc6886221218ee75156f16af324.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Traditionally, we collected our services through TeamCity. At some point, we realized that we have one template for all services, because each service is unique, he knows how to roll himself into a container. It became quite difficult to produce these projects here, there were a lot of them. And to describe it in a yml-file and put it together with the service itself turned out to be quite convenient. Therefore, we are gradually introducing this thing, for a little while, but the prospects are interesting. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Well, actually, what I would like to say to myself when we started all this stuff. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Firstly, this is what if we are talking about the development of microservices, then the first thing I would advise is to </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">start right away with some kind of orchestration system</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Let it be as simple as the same Nomad that you run with the command</font></font><code>nomad agent -dev</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and get a complete orchestration system, immediately with a raised Consul, with Nomad himself and with this whole kitchen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This makes it clear that you are working in a kind of black box. You are trying to immediately move away from being tied to a specific machine, tied to the file system on a specific machine. Something like this, it immediately rebuilds your thinking. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And, of course, right at the development stage, you should have it laid out that </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">each service has at least two instances</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , otherwise you will not be able to easily shoot off any problematic services, some things that make it difficult for you. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The next moment is, of course, some architectural things. As part of microservices, one of the most important of these things is the </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">message bus.</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A classic example: you have a user registration. How to make it the easiest way? To register, you need to create an account, create a user in billing, you need to make him an avatar and something else. Here you have a certain number of services, you receive a request to some such super-service, and he is already starting to scatter requests to all his wards. As a result, every time he knows more and more about the services he needs to pull in order to complete the registration.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It is much easier, safer and more effective to do it differently. Leave 1 service that makes registration. He has registered a user. Then you throw the event into this shared bus ‚ÄúI registered the user, the ID is such and such, the minimum information‚Äù. And it will receive all the services for which this information is useful. One will go to the account in the billing will do, the other welcome letter will send. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As a result, your system will lose such a rigid connectivity. You will not have such super-services that know about everything and about everyone. It actually makes it very easy to operate with such a system. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Well, what I mentioned. </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No need to try to repair these services.</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. If you have a problem with any particular instance, try to localize it, transfer traffic to other, maybe, just raised instances. And then understand what is wrong. The viability of the system will greatly improve. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Naturally, in order to understand what is happening with your system, how effective it is, </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">you need to collect metrics</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Here is an important point: if you don‚Äôt understand a metric, if you don‚Äôt know how to use it, if it doesn‚Äôt tell you anything, then you don‚Äôt need to collect it. Because at some point, these metrics become a billion. You spend a lot of CPU time just to choose what you need, spend tons of time filtering what you don't need. It lies dead weight.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">You understand that you need some kind of metric - start collecting it. </font><font style="vertical-align: inherit;">Something is not needed - do not collect. </font><font style="vertical-align: inherit;">This greatly simplifies the handling of this data, because it becomes really very fast a lot of them. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If you see some kind of problem, then you don‚Äôt need to do something for everyone. </font><font style="vertical-align: inherit;">In most cases, the </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">system itself must somehow respond</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">You really need an alert only in that situation when it requires some kind of your action. </font><font style="vertical-align: inherit;">If you don‚Äôt need to run something there in the middle of the night, it means that this is not an alert, but some kind of warning that you have taken note of and in some conditionally-regular mode will be able to process. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Well, actually everything.</font></font> Thank. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/MBZtcNgDXzU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br> <em><a href="http://www.highload.ru/2016/abstracts/2302.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Microservices: experience of use in a loaded project</font></font></a></em> </div><p>Source: <a href="https://habr.com/ru/post/323154/">https://habr.com/ru/post/323154/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../323138/index.html">How effective are the next-generation information security solutions?</a></li>
<li><a href="../323142/index.html">React or Vue? Choosing a library for front-end development</a></li>
<li><a href="../323144/index.html">Build a cache with efficient multithreaded access</a></li>
<li><a href="../323148/index.html">ReactDoc - the first open source solution of the program "Common Frontal System"</a></li>
<li><a href="../323152/index.html">Some techniques of working in Bitrix on SQL and BASH</a></li>
<li><a href="../323156/index.html">Interactive UX prototype: parsing on a real example</a></li>
<li><a href="../323158/index.html">RandLib. Library of probability distributions in C ++ 17</a></li>
<li><a href="../323160/index.html">Why do I need Refresh Token if there is Access Token?</a></li>
<li><a href="../323162/index.html">Virtualization and Cloud Performance Testing</a></li>
<li><a href="../323164/index.html">Modern javascript or how to make your redux module ready for reuse</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>