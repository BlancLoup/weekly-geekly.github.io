<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Building drbd mirrors on Proxmox-3.0</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this article I want to describe how-to create drbd mirroring on Proxmox 3.0 host machines. Combining machines in the proxmox cluster makes sense to...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Building drbd mirrors on Proxmox-3.0</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/storage2/04c/701/370/04c701370a4a210fdd13e1780e1259e0.png">  In this article I want to describe how-to create drbd mirroring on Proxmox 3.0 host machines.  Combining machines in the proxmox cluster makes sense to these operations - although in general there is no difference. <br>  The main difference of this material from many, spread out on the Internet, is that we are doing a drbd partition not on the second physical disk connected second, but on the lvm partition within the only available disk. <br>  The question of the expediency of such actions is rather controversial - whether drbd will be faster on a ‚Äúraw‚Äù disk or not, but in any case, this is a 100% tested version.  In the piggy bank so to speak.  And working with a ‚Äúraw‚Äù disk is just a special case of this manual. <br><a name="habracut"></a><br><br>  Actually, when installing Proxmox 3.0 (just like its predecessor 2.0), it does not bother with partitioning issues and it breaks everything up itself considering only the overall disk size and memory size.  We get a partition / pve / data that takes up most of the disk and is visible in Proxmox as local storage.  That is due to him and will be actions. <br><br>  1. We update packages to current <br> <code>#aptitude update &amp;&amp; aptitude full-upgrade</code> <br> <br>  2. Install the necessary packages <br> <code>#aptitude install drbd8-utils</code> <br> <br>  3. Free up space for a new section. <br>  Unmount / dev / pve / data (also known as / var / lib / vz).  All the following actions of step 3 can be done only on the unmounted resource - accordingly, before this we extinguish all the VMs that use the local storage on this node.  The rest can not touch if very necessary. <br> <code>#umount /dev/pve/data</code> <br> <br>  3.1.  Decrease / dev / pve / data. <br>  In principle, several of the following steps can be replaced with commands. <br> <code>#lvresize -L 55G /dev/mapper/pve-data</code> <br> <code>#mkfs.ext3 /dev/pve/data</code> <br> <br>  Well, or a little more detailed.  And in my opinion a little more correctly. <br> <code>#lvremove /dev/pve/data</code> <br> <code>#lvcreate -n data -l 55G pve</code> <br> <code>#mkfs.ext3 /dev/pve/data</code> <br> <br>  But at the same time we lose everything that is in the local storage.  If Proxmox is freshly installed (which is generally recommended for such kind of manipulations), then we go to step 4. If there is a question to save the data in the local repository, then we act differently. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      3.2.  Decrease / dev / pve / data without losing information. <br>  I assume that local is less than 50G.  If you have a different situation, simply change the "new size" in the teams. <br><br> <code>#umount /dev/pve/data</code> <br> <br>  Mandatory check, without it resize2fs will not work <br> <code>#e2fsck -f /dev/mapper/pve-data</code> <br> <code>e2fsck 1.42.5 (29-Jul-2012)</code> <br> <code>Pass 1: Checking inodes, blocks, and sizes</code> <br> <code>Pass 2: Checking directory structure</code> <br> <code>Pass 3: Checking directory connectivity</code> <br> <code>Pass 4: Checking reference counts</code> <br> <code>Pass 5: Checking group summary information</code> <br> <code>/dev/mapper/pve-data: 20/53223424 files (0.0% non-contiguous), 3390724/212865024 blocks</code> <br> <br>  Compress the file system to 50G.  If this step is skipped, then with a probability of 90% after lvresize we will get a broken system.  Moreover, the number is intentionally a little smaller than the resulting section.  With a margin. <br> <code>#resize2fs /dev/mapper/pve-data 50G</code> <br> <code>resize2fs 1.42.5 (29-Jul-2012)</code> <br> <code>Resizing the filesystem on /dev/mapper/pve-data to 13107200 (4k) blocks.</code> <br> <code>The filesystem on /dev/mapper/pve-data is now 13107200 blocks long.</code> <br> <br> <code>#e2fsck -f /dev/mapper/pve-data</code> <br> <br>  Compressing the / pve / data section directly to 55G <br> <code>#lvresize -L 55G /dev/mapper/pve-data</code> <br> <code>WARNING: Reducing active logical volume to 55.00 GiB</code> <br> <code>THIS MAY DESTROY YOUR DATA (filesystem etc.)</code> <br> <code>Do you really want to reduce data? [y/n]: y</code> <br> <code>Reducing logical volume data to 55.00 GiB</code> <br> <code>Logical volume data successfully resized</code> <br> <br>  We occupy the system all the available space.  In principle, if your ‚Äústock‚Äù at the previous step is not large, then this can be done.  Why save on matches?  ;) <br> <code>#resize2fs /dev/mapper/pve-data</code> <br> <code>resize2fs 1.42.5 (29-Jul-2012)</code> <br> <code>Resizing the filesystem on /dev/mapper/pve-data to 14417920 (4k) blocks.</code> <br> <code>The filesystem on /dev/mapper/pve-data is now 14417920 blocks long.</code> <br> <br>  Return / dev / pve / data system. <br> <code>#mount /dev/pve/data</code> <br> <br>  4. Creating a partition for drbd <br>  Enjoying free space.  We are convinced that all previous steps have given that which is not necessary.  Ie free space on the / dev / sda2 section <br> <code>#pvdisplay</code> <br> <code>--- Physical volume ---</code> <br> <code>PV Name /dev/sda2</code> <br> <code>VG Name pve</code> <br> <code>PV Size 931.01 GiB / not usable 0</code> <br> <code>Allocatable yes</code> <br> <code>PE Size 4.00 MiB</code> <br> <code>Total PE 238339</code> <br> <code>Free PE 197891</code> <br> <code>Allocated PE 40448</code> <br> <code>PV UUID 6ukzQc-D8VO-xqEK-X15T-J2Wi-Adth-dCy9LD</code> <br> <br>  Create a new partition for all free space. <br> <code>#lvcreate -n drbd0 -l 100%FREE pve</code> <br> <code>Logical volume "drbd" created</code> <br> <br>  5. Preparing drbd configuration file <br> <code>#nano /etc/drbd.d/r0.res</code> <br>  resource r0 { <br><br>  startup { <br>  wfc-timeout 120; <br>  degr-wfc-timeout 60; <br>  become-primary-on both; <br>  } <br><br>  net { <br>  cram-hmac-alg sha1; <br>  shared-secret "proxmox"; <br>  allow-two-primaries; <br>  after-sb-0pri discard-zero-changes; <br>  after-sb-1pri discard-secondary; <br>  after-sb-2pri disconnect; <br>  } <br>  syncer { <br>  rate 30M; <br>  } <br>  on p1 { <br>  device / dev / drbd0; <br>  disk / dev / pve / drbd; <br>  address 10.1.1.1:7788; <br>  meta-disk internal; <br>  } <br>  on p2 { <br>  device / dev / drbd0; <br>  disk / dev / pve / drbd; <br>  address 10.1.1.2:7788; <br>  meta-disk internal; <br>  } <br>  } <br><br>  Some recommend setting the wfc-timeout parameter to 0. Its meaning is that if at the start we don‚Äôt see the drbd neighbor, it will reboot after wfc-timeout seconds for a second attempt.  0 - means to disable such an action. <br>  Rate 30M - transfer limit between drbd hosts.  The value corresponds to 1G connection.  Recommended as 30% of the actual bandwidth between hosts.  In the example below, on ‚Äútest rabbits‚Äù the bandwidth on a 100M connection is about 11Mb / s, ie the rate should be reduced to 3M.  With a 10G connection between the hosts, it obviously makes sense to increase. <br><br>  6. Creating meta data and launching drbd partition. <br> <code>#modprobe drbd</code> <br> <br> <code>#drbdadm create-md r0</code> <br> <code>md_offset 830015008768</code> <br> <code>al_offset 830014976000</code> <br> <code>bm_offset 829989642240</code> <br> <br> <code>Found some data</code> <br> <br> <code>==&gt; This might destroy existing data! &lt;==</code> <br> <br> <code>Do you want to proceed?</code> <br> <code>[need to type 'yes' to confirm] yes</code> <br> <br> <code>Writing meta data...</code> <br> <code>initializing activity log</code> <br> <code>NOT initialized bitmap</code> <br> <code>New drbd meta data block successfully created.</code> <br> <code>Success</code> <br> <br> <code>#drbdadm up r0</code> <br> <br>  You can see the result like this: <br> <code>#cat /proc/drbd</code> <br> <code>version: 8.3.13 (api:88/proto:86-96)</code> <br> <code>GIT-hash: 83ca112086600faacab2f157bc5a9324f7bd7f77 build by root@sighted, 2012-10-09 12:47:51</code> <br> <code>0: cs:WFConnection ro:Secondary/Unknown ds:UpToDate/DUnknown C r----s</code> <br> <code>ns:0 nr:0 dw:0 dr:0 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:b oos:810536760</code> <br> <br>  7. Prepare the second host <br>  Steps 1-6 indulge in a second host machine.  An important point (!).  The size of the drbd partition should be identical on both hosts. <br><br>  8. Synchronization. <br>  Take one of the hosts (it does not matter which one).  Call it before complete synchronization primary.  The second, respectively, secondary.  After full synchronization, they will become equivalent - this is the mode we set. <br><br> <code>#drbdadm -- --overwrite-data-of-peer primary r0</code> <br> <br> <code># cat /proc/drbd <br></code> <code>version: 8.3.13 (api:88/proto:86-96)</code> <br> <code>GIT-hash: 83ca112086600faacab2f157bc5a9324f7bd7f77 build by root@sighted, 2012-10-09 12:47:51</code> <br> <code>0: cs:WFConnection ro:Primary/Unknown ds:UpToDate/DUnknown C r----s</code> <br> <code>ns:0 nr:0 dw:0 dr:664 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:b oos:810536760</code> <br> <br>  then on both hosts <br> <code>#drbdadm down r0</code> <br> <code>#service drbd start</code> <br> <br>  On primary, the result will look like this: <br> <code>Starting DRBD resources:[ d(r0) s(r0) n(r0) ]..........</code> <br> <code>***************************************************************</code> <br> <code>DRBD's startup script waits for the peer node(s) to appear.</code> <br> <code>- In case this node was already a degraded cluster before the</code> <br> <code>reboot the timeout is 60 seconds. [degr-wfc-timeout]</code> <br> <code>- If the peer was available before the reboot the timeout will</code> <br> <code>expire after 120 seconds. [wfc-timeout]</code> <br> <code>(These values are for resource 'r0'; 0 sec -&gt; wait forever)</code> <br> <code>To abort waiting enter 'yes' [ 18]:</code> <br> <code>.</code> <br> <br>  On secondary: <br><br> <code>Starting DRBD resources:[ d(r0) s(r0) n(r0) ]..........</code> <br> <code>***************************************************************</code> <br> <code>DRBD's startup script waits for the peer node(s) to appear.</code> <br> <code>- In case this node was already a degraded cluster before the</code> <br> <code>reboot the timeout is 60 seconds. [degr-wfc-timeout]</code> <br> <code>- If the peer was available before the reboot the timeout will</code> <br> <code>expire after 120 seconds. [wfc-timeout]</code> <br> <code>(These values are for resource 'r0'; 0 sec -&gt; wait forever)</code> <br> <code>To abort waiting enter 'yes' [ 14]:</code> <br> <code>0: State change failed: (-10) State change was refused by peer node</code> <br> <code>Command '/sbin/drbdsetup 0 primary' terminated with exit code 11</code> <br> <code>0: State change failed: (-10) State change was refused by peer node</code> <br> <code>Command '/sbin/drbdsetup 0 primary' terminated with exit code 11</code> <br> <code>0: State change failed: (-10) State change was refused by peer node</code> <br> <code>Command '/sbin/drbdsetup 0 primary' terminated with exit code 11</code> <br> <code>.</code> <br> <br>  Errors \ delays are related to the fact that we restarted everything at the same time.  In a normal situation, the launch looks simple: <br><br> <code>#service drbd start</code> <br> <code>Starting DRBD resources:[ d(r0) s(r0) n(r0) ].</code> <br> <br> <code># cat /proc/drbd</code> <br> <code>version: 8.3.13 (api:88/proto:86-96)</code> <br> <code>GIT-hash: 83ca112086600faacab2f157bc5a9324f7bd7f77 build by root@sighted, 2012-10-09 12:47:51</code> <br> <code>0: cs:SyncSource ro:Primary/Primary ds:UpToDate/Inconsistent C r-----</code> <br> <code>ns:199172 nr:0 dw:0 dr:207920 al:0 bm:11 lo:1 pe:24 ua:65 ap:0 ep:1 wo:b oos:810340664</code> <br> <code>[&gt;....................] sync'ed: 0.1% (791348/791536)M</code> <br> <code>finish: 19:29:01 speed: 11,532 (11,532) K/sec</code> <br> <br>  here we see that disk synchronization has begun. <br>  Let's start monitoring this process and go for a walk.  Depending on the size of the disk and the connection speed between the hosts, we can walk from a couple of hours to days ... <br><br> <code>#watch ‚Äìn 1 ‚Äúcat /proc/drbd‚Äù</code> <br> <br>  And we are waiting for the cherished 100% <br><br> <code>cs:SyncSource ro:Primary/Primary ds:UpToDate/ UpToDate</code> <br> <br>  9. Creating lvm volume group <br>  The process is long, so let's continue on the primary host. <br> <code>#vgcreate drbd-0 /dev/drbd0</code> <br> <code>No physical volume label read from /dev/drbd0</code> <br> <code>Writing physical volume data to disk "/dev/drbd0"</code> <br> <code>Physical volume "/dev/drbd0" successfully created</code> <br> <code>Volume group "drbd-0" successfully created</code> <br> <br><br>  10. Connecting a group in Proxmox <br><img src="http://habrastorage.org/storage2/f88/156/8e0/f881568e0a8464592607e056d6905f0c.jpg"><br>  Select the Data Center Storage section in the Proxmox GUI.  Add.  Type - LVM, arbitrary ID is just a name.  The partition group drbd-0, + enable, + is publicly available. <br>  Pay attention to the highlights.  drbd-0 is the group created in step 9. <br>  Well, public access is set so that Proxmox does not try to copy the images of the host machine disks during the migration process. <br><br>  11. Everything. <br>  After waiting for the synchronization to finish, you can create machines by selecting drbd as the image-disk storage, transfer them from the host to the host in the cluster without losing contact with the virtual machine to service the host machine.  In general, everything is ready to build a <a href="http://pve.proxmox.com/wiki/High_Availability_Cluster">High Availability Cluster - Proxmox</a> <br><br><habracut></habracut></div><p>Source: <a href="https://habr.com/ru/post/187660/">https://habr.com/ru/post/187660/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../187646/index.html">SMS and e-mail service for Russian Post and EMS parcels</a></li>
<li><a href="../187648/index.html">2027 mm business class - from the first flight to the moon to cloud computing</a></li>
<li><a href="../187650/index.html">Plug - home cloud storage</a></li>
<li><a href="../187654/index.html">Cache secrets, or how to spend 1000 cycles for 10 teams</a></li>
<li><a href="../187658/index.html">What are the types of project managers?</a></li>
<li><a href="../187662/index.html">Poll. Would you like to participate in a project like Mars One?</a></li>
<li><a href="../187664/index.html">Hierarchical data. In search of the optimal solution</a></li>
<li><a href="../187666/index.html">The dual education system in Germany - what it is and what it is eaten with</a></li>
<li><a href="../187668/index.html">Golang daemon</a></li>
<li><a href="../187672/index.html">New book Koflera. Linux Installation, configuration, administration</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>