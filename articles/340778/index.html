<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Method of formalized models as an alternative to neural networks</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The dominance of statistical methods, especially neural networks, pushes me to this stuffing - yes, I will classify them in this way. On the one hand,...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Method of formalized models as an alternative to neural networks</h1><div class="post__text post__text-html js-mediator-article"><p>  The dominance of statistical methods, especially neural networks, pushes me to this stuffing - yes, I will classify them in this way.  On the one hand, I have nothing against them, but at the same time, there is a clear bias, sometimes even neural networks are almost identical with the vague concept of artificial intelligence, although is it worse than SVM, HMM, etc.  In the matter of processing natural languages, I have always been a supporter of linguistic methods as opposed to statistical ones, but their essential drawback is felt - the complexity of manual modeling compared to machine learning.  Or maybe machine learning is also applicable to linguistic methods? </p><a name="habracut"></a><br><p>  I do not undertake to answer this question in this description, but I propose some reflections.  I note that the main idea came to me a long time ago and was framed in the form of patent No. 2392660, half of which has almost expired.  But what if we slightly expand the solution in this patent? </p><br><h3 id="chto-mne-pridumalos-prezhde">  What I thought up before </h3><br><p>  A patent describes a semantic model of a natural language, but some modification allows you to create a semantic model of images.  So if you select a common part, then the potential scope expands. </p><br><p>  I will begin with the previously invented semantic model.  In this case, it is a generalized semantic, but not ontological.  But at the same time multilevel, approximately as words, sentences, paragraphs, chapters, etc., only with other units for a natural language and other units for other cases, but more on that later.  So, one level includes (quote from previously published articles): </p><br><ul><li>  a set of connecting elements for this level (always finite); </li><li>  many types of atomic elements (always finite); </li><li>  set of one-place operators (always finite); </li><li>  the set of restrictions for the compatibility of binding elements, atomic elements and one-place operators (always finite); </li><li>  the sets of each type of atomic elements (finite or infinite); </li><li>  sets of classifiers for each of the sets of connecting elements (always finite) and each of the types of atomic elements (finite or infinite). </li></ul><br><p>  Now a little more about what it is, on the example of natural languages.  Since the same phrase can be expressed in different words, the division into words does not make sense in the semantic model.  The situation is similar with complex sentences, but it makes sense to select simple ones;  clauses - their well-established name.  Consider the semantic model inside the clause. </p><br><p>  The concepts of lexemes and semantic classes are also known.  Often, lexemes correspond to words, but in general, the ratio is many to many.  The lexemes themselves are polysemantic, but the semantic class captures a specific value.  I will not do a review of markup methods of semantic classes now, if necessary, I will leave for later - a lot has already been written on this topic.  For the lexemes with semantic classes themselves there are different names, here let them be called denotations. </p><br><p>  There is a need to separate them (denotation) into two types: objects and predicates.  The first ones themselves have a certain meaning, the second ones require the addition of other denotates, here we will call them (complementary denotates) actants. </p><br><p>  Denotats still do not solve the problem of the possibility of expressing meaning in different words, so we divide them into basic and composite ones, in more detail about the criteria in the preface to the <a href="https://ru.wikipedia.org/wiki/%25D0%2590%25D0%25BF%25D1%2580%25D0%25B5%25D1%2581%25D1%258F%25D0%25BD,_%25D0%25AE%25D1%2580%25D0%25B8%25D0%25B9_%25D0%2594%25D0%25B5%25D1%2580%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25BA%25D0%25BE%25D0%25B2%25D0%25B8%25D1%2587">"New Explanatory Dictionary of Synonyms"</a> and other works by the same author.  It is the division of the composite denotations on the base to solve this problem.  Actually, at this level, i.e. within simple sentences, the set of basic predicates will be the set of connecting elements. </p><br><p>  Denotaty objects in pure text, ie, without drawings, diagrams, formulas, listings, etc., will be of two types - discrete and continuous, the second - mostly directly numbers.  The sets of basic denotation objects will, of course, be endless. </p><br><p>  A little running ahead, it should be noted that the composite denotations include differential and characteristic components ‚Äî we use their name here, in this case they are the only one-place operators.  They make it possible to distinguish logical (complete) negation from linguistic, where negation extends only to differential components. </p><br><p>  In this case, classifiers will refer only to object denotations, and they will be the semantic classes mentioned earlier.  It is assumed the use of parallel hierarchies and the presence of the upper universal class.  Hierarchies themselves can begin with any semantic classes, but they belong to the universal semantic class completely.  Restrictions on compatibility will apply only to composite predicate denotations (see below) and for specific actants include many combinations of semantic classes and hierarchies.  That is, for a particular actant, the restriction includes a potentially infinite set of semantic classes, expressed by a finite set of combinations of semantic classes and hierarchies.  In this case, the actant can only be an object belonging to one of the appropriate semantic classes.  The actants of the basic predicates of the always universal semantic class. </p><br><p>  The sets of composite elements (also quoted from previously published articles): </p><br><ul><li>  the set of closed (non-binding) constituent elements (always infinite); </li><li>  a set of connecting components (always infinite); </li><li>  the set of restrictions on the use of atomic elements in binding components by classifiers (finite or infinite); </li><li>  a set of rules for replacing elements that are part of a component (always finite) </li></ul><br><p>  The binding elements are also predicate denotates, but in this case are composite.  Non-dwelling elements can be both composite objects, or extend to the entire clause or fragments corresponding to the phrases, the main requirement is the indication of all the actors. </p><br><p>  Each composite predicate, combining basic predicates and objects, can impose restrictions on the subsets of the semantic classes of its actants in the direction of detailing - this set of restrictions is, of course, infinite. </p><br><p>  It is advisable to present a composite predicate in conjunctive normal form, where the disjunctive members have a single operator, whether they are differential and characteristic.  In accordance with them, linguistic negation may lead to a logical one.  In addition, it is assumed the possibility of a general logical simplification, for example, the exclusion of identical objects united by a logical operator.  The second group of substitution rules will relate to basic predicates, such as the possibility of excluding opposing actions, for example, verbalization of the noun and substantivization of the verb in the meaning of action.  The third group of rules is simultaneously to basic predicates and objects, implying the possibility of excluding joining predicates for actants of a composite predicate when an object corresponds to a given set of semantic classes. </p><br><p>  A few words about the other sets.  For the Clause level, I find it difficult to drill down on these sets, only the fact that they will constitute a natural language text is obvious.  In this case, the composite object denotates of the level described above, which express clauses completely, will be the basic objects of this level. </p><br><p>  For images, different approaches are all the more possible, atomic elements can be numerical sets for filter characteristics or texels in the case of textures, in other cases there are also options.  Some possible solutions for the remaining semantic models will follow. </p><br><h3 id="chto-teper">  What now </h3><br><p>  Next, it makes sense to return to the original question of the application of machine learning.  It is clear that compiling a dictionary of objects, semantic classes, etc. manually is extremely laborious.  But modern methods of machine learning in the case of natural languages ‚Äã‚Äãare able to solve the problem not only of word matching, but also of statistical detection of denotations, which in the case of a neural network remain a black box.  At the same time, other statistical methods, such as clustering, provide clear results. </p><br><p>  The infinite sets, of course, are such only in theory, in practice this means that machine learning is designed to constantly update their composition.  Since this is a semantic model, not an ontological one, maintaining relevance is also one of the tasks. </p><br><p> Now some thoughts on the feasibility of implementation.  Take, for example, the semantic vectors offered by Yandex, and recall the notion of a <a href="http://www.ruscorpora.ru/">corpus of texts</a> .  Suppose there is a certain corpus of queries in the context of Yandex algorithms and a set of <a href="https://habrahabr.ru/company/yandex/blog/336094/">certain semantic matrices of</a> documents, although I myself do not fully understand what kind of details are in all details of the data structure.  But, I suppose, there is data on the words included in documents and queries, which means that it is possible to compare the presence of words in documents and queries with classical search tools, and to compare relevance based on semantic vectors.  What exactly to use - words, N-grams or other models for clustering - is a rather open question, but one way or another it is possible to distinguish semantic classes based on cluster analysis. </p><br><p>  One more statistical model needs to be obtained in the form of a directed asymmetric graph of connections between words, i.e., to what extent one word can replace another in relevant queries.  The base denotates will obviously have a pronounced asymmetrical weight of the arcs.  The specific algorithm for traversing the resulting graph to identify the base denotates is rather an open question, especially considering the large computational complexity. </p><br><p>  The options mentioned use the previously proposed neural networks as an intermediate link, but if we consider that many clustering algorithms are iterative in themselves, then simplification is definitely possible.  For now I‚Äôll leave some questions open. </p><br><h3 id="v-chyom-smysl">  What's the point </h3><br><p>  A few words about why all this may be needed.  By and large, these are the typical advantages of an explicit model over the "black box" of the neural network.  Firstly, it is the possibility of optimization.  If the search is based on the neural network, then the denotations are without clear boundaries, which makes it difficult to optimize the equivalent of any step, be it the markup of semantic classes or the comparison of basic denotations.  Secondly, when the existing concepts are updated, in the case of a neural network, the set of changes remains all the more uncertain from the point of view of interpretation, which makes it difficult to debug in the case of retraining.  Thirdly, if the search task is replaced, for example, with the reference task, the complexity of identifying that part of the neural network that deals directly with the search task after the initial semantic analysis also remains.  If I am mistaken - correct. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/340778/">https://habr.com/ru/post/340778/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../340768/index.html">Monitoring git clone and git push events on a local GitLab server</a></li>
<li><a href="../340770/index.html">Introduction to Beautiful Capi, C ++ wrapper creation tool for C ++ libraries</a></li>
<li><a href="../340772/index.html">Evolutionary Computing: Learning to Walk a Stool</a></li>
<li><a href="../340774/index.html">My rules are a good design system. Components and interface architecture in Figma</a></li>
<li><a href="../340776/index.html">Isomorphic React JS + Spring Boot application</a></li>
<li><a href="../340780/index.html">Programming for 3CX in C #: use the 3CX Call Control API in the Call Flow Designer development environment</a></li>
<li><a href="../340782/index.html">Fake cryptocurrency trading applications from Google Play were stolen by data</a></li>
<li><a href="../340784/index.html">Yandex. Blitz. 12 algorithmic problems of the qualifying round and their analysis</a></li>
<li><a href="../340786/index.html">EdHack 2017 results - AR and VR in education</a></li>
<li><a href="../340788/index.html">How to integrate with Cisco security solutions? Overview of two dozen APIs available to all</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>