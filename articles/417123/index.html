<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Spark Streaming and Kafka Integration</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello colleagues! We remind you that not so long ago we published a book about Spark , and right now the last proofreading book about Kafka is passing...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Spark Streaming and Kafka Integration</h1><div class="post__text post__text-html js-mediator-article">  Hello colleagues!  We remind you that not so long ago we published a <a href="https://www.piter.com/collection/all/product/effektivnyy-spark-masshtabirovanie-i-optimizatsiya">book about Spark</a> , and right now the last proofreading <a href="">book about Kafka</a> is passing. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/it/cf/nj/itcfnjaffoo8apwikyd7_yfym5s.jpeg"></div><br>  We hope these books will be successful enough to continue the topic - for example, to translate and publish literature on Spark Streaming.  We wanted to offer a translation about the integration of this technology with Kafka today. <br><a name="habracut"></a><br>  <b>1. Justification</b> <br><br>  Apache Kafka + Spark Streaming is one of the best combinations for creating real-time applications.  In this article we will discuss in detail the details of such integration.  In addition, we consider the example of Spark Streaming-Kafka.  Then we discuss the ‚Äúapproach with the recipient‚Äù and the option of directly integrating Kafka and Spark Streaming.  So, let's proceed to the integration of Kafka and Spark Streaming. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/webt/8x/jl/cp/8xjlcpzhwdwi4w2g87iifbquvr0.jpeg"><br><br>  <b>2. Integration of Kafka and Spark Streaming</b> <br><br>  When integrating Apache Kafka and Spark Streaming, there are two possible approaches to the configuration of Spark Streaming for receiving data from Kafka - i.e.  two approaches to integrating Kafka and Spark Streaming.  First, you can use the Kafka Recipients and Kafka high-level API.  The second (newer) approach is work without Recipients.  For both approaches, there are different programming models that differ, for example, in terms of performance and semantic guarantees. <br><br><img src="https://habrastorage.org/webt/91/mn/sk/91mnsklu_81q0nx9aadnjgya4fc.png"><br><br>  Consider these approaches in more detail. <br><br>  <i><b>a.</b></i>  <i><b>Recipient Based Approach</b></i> <br><br>  In this case, the data is received by the Recipient.  So, using the high-level consumption API provided by Kafka, we implement the Recipient.  Further, the data is stored in the Spark Contractors.  Then in Kafka - Spark Streaming, tasks are launched within which the data are processed. <br><br>  However, using this approach, there is still a risk of data loss in the event of a failure (with the default configuration).  Consequently, it will be necessary to additionally include an advance recording log in Kafka - Spark Streaming in order to prevent data loss.  Thus, all data received from Kafka is synchronously stored in the forward write log in the distributed file system.  That is why even after a system failure all data can be recovered. <br><br>  Next, we consider how to use this approach with the use of recipients in an application with Kafka - Spark Streaming. <br><br>  <i>i.</i>  <i>Binding</i> <br><br>  Now we associate our streaming application with the following artifact for Scala / Java applications, using the project definitions for SBT / Maven. <br><br><pre><code class="java hljs">groupId = org.apache.spark artifactId = spark-streaming-kafka-<span class="hljs-number"><span class="hljs-number">0</span></span>-<span class="hljs-number"><span class="hljs-number">8_2.11</span></span> version = <span class="hljs-number"><span class="hljs-number">2.2</span></span>.0</code> </pre> <br>  However, when deploying our application, we will have to add the aforementioned library and its dependencies, this is needed for Python applications. <br><br>  <i>ii.</i>  <i>Programming</i> <br><br>  Next, create the <code>DStream</code> input stream by importing <code>KafkaUtils</code> into the streaming application code: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.streaming.kafka._ val kafkaStream = KafkaUtils.createStream(streamingContext, [ZK quorum], [consumer group id], [per-topic number of Kafka partitions to consume])</code> </pre> <br>  In addition, using the createStream variants, you can specify key classes and value classes, as well as corresponding classes for decoding them. <br><br>  <i>iii.</i>  <i>Deployment</i> <br><br>  As with any Spark application, the spark-submit command is used to launch.  However, the details are slightly different in Scala / Java applications and in Python applications. <br><br>  Moreover, using <code>‚Äìpackages</code> you can add <code>spark-streaming-Kafka-0-8_2.11</code> and its dependencies directly to <code>spark-submit</code> , this is useful for applications in Python, where it is impossible to manage projects using SBT / Maven. <br><br><pre> <code class="java hljs">./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-<span class="hljs-number"><span class="hljs-number">0</span></span>-<span class="hljs-number"><span class="hljs-number">8_2.11</span></span>:<span class="hljs-number"><span class="hljs-number">2.2</span></span>.0 ...</code> </pre> <br>  You can also download the Maven <code>spark-streaming-Kafka-0-8-assembly</code> JAR archive from the Maven repository.  Then add it to <code>spark-submit</code> with - <code>jars</code> . <br><br>  <i>b.</i>  <i>Direct approach (without recipients)</i> <br><br>  After the approach with the use of recipients a new approach was developed - ‚Äúdirect‚Äù.  It provides reliable end-to-end guarantees.  In this case, we periodically request Kafka about the offset of the read data (offsets) for each topic / section, and not organize the delivery of data through recipients.  In addition, the size of the read fragment is determined, it is necessary for the proper processing of each packet.  Finally, a simple consuming API is used to read data ranges from Kafka with given offsets, especially when data processing tasks are started.  The whole process is like reading files from the file system. <br><br>  Note: This feature appeared in Spark 1.3 for Scala and the Java API, as well as in Spark 1.4 for the Python API. <br><br>  Now let's discuss how to apply this approach in our streaming application. <br>  The Consumer API is described in more detail at the following link: <br><br>  <a href="https://data-flair.training/blogs/kafka-consumer/">Apache Kafka Consumer |</a>  <a href="https://data-flair.training/blogs/kafka-consumer/">Examples of Kafka Consumer</a> <br><br>  i.  Binding <br><br>  However, this approach is supported only in applications on Scala / Java.  With the following artifact, build the SBT / Maven project. <br><br><pre> <code class="java hljs">groupId = org.apache.spark artifactId = spark-streaming-kafka-<span class="hljs-number"><span class="hljs-number">0</span></span>-<span class="hljs-number"><span class="hljs-number">8_2.11</span></span> version = <span class="hljs-number"><span class="hljs-number">2.2</span></span>.0</code> </pre> <br>  <i>ii.</i>  <i>Programming</i> <br><br>  Next, import KafkaUtils and create an input <code>DStream</code> in the streaming application code: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.streaming.kafka._ val directKafkaStream = KafkaUtils.createDirectStream[ [key <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class">], [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">value</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">class</span></span></span><span class="hljs-class">], [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">key</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">decoder</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">class</span></span></span><span class="hljs-class">], [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">value</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">decoder</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">class</span></span></span><span class="hljs-class">] ]( </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">streamingContext</span></span></span><span class="hljs-class">, [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">map</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">of</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Kafka</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">parameters</span></span></span><span class="hljs-class">], [</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">set</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">of</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">topics</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">to</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">consume</span></span></span><span class="hljs-class">])</span></span></code> </pre> <br>  In the Kafka parameters, you will need to specify either <code>metadata.broker.list</code> or <code>bootstrap.servers</code> .  Consequently, by default we will consume data starting from the last offset in each Kafka section.  However, if you want the reading to start from the smallest fragment, then in the Kafka parameters you need to set the configuration option <code>auto.offset.reset</code> . <br><br>  Moreover, by working with the <code>KafkaUtils.createDirectStream</code> variants, you can start reading at an arbitrary offset.  Then we will do the following, which will allow us to gain access to the Kafka fragments consumed in each packet. <br><br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">//      ,        var offsetRanges = Array.empty[OffsetRange] directKafkaStream.transform { rdd =&gt; offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges rdd }.map { ... }.foreachRDD { rdd =&gt; for (o &lt;- offsetRanges) { println(s"${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}") } ... }</span></span></code> </pre> <br>  If we want to organize Kafka monitoring based on Zookeeper with the help of special tools, then we can update Zookeeper ourselves with their help. <br><br>  <i>iii.</i>  <i>Deployment</i> <br><br>  The deployment process in this case resembles the deployment process in the version with the recipient. <br><br>  <b>3. Advantages of the direct approach</b> <br><br>  The second approach to integrating Spark Streaming with Kafka wins the first for the following reasons: <br><br>  <b><i>a.</i></b>  <b><i>Simplified concurrency</i></b> <br><br>  In this case, you do not need to create multiple Kafka input streams and merge them.  However, Kafka - Spark Streaming will create as many RDD segments as there will be Kafka consumption segments.  All these Kafka data will be read in parallel.  Therefore, we can say that we will have a one-to-one correspondence between the Kafka and RDD segments, and this model is clearer and easier to configure. <br><br>  <i><b>b.</b></i>  <i><b>Efficiency</b></i> <br><br>  In order to completely eliminate data loss during the first approach, information was required to be stored in the log of the leading record, and then replicated.  In fact, this is inefficient, since the data is replicated twice: for the first time by Kafka himself, and in the second - by the proactive write log.  In the second approach, this problem is eliminated, since there is no recipient, and, therefore, the forward write log is not needed either.  If we have provided for a sufficiently long storage of data in Kafka, then you can restore messages directly from Kafka. <br><br>  <b><i>with.</i></b>  <b><i>Exactly-Once Semantics</i></b> <br><br>  In principle, we used the high-level Kafka API on the first approach to store consumed read fragments in the Zookeeper.  However, this is the way to consume data from Kafka.  Suppose, at the same time, data losses are reliably excluded, there is a small probability that with some failures, individual records can be consumed twice.  It's all about the inconsistency between the mechanism for reliable data transfer in Kafka - Spark Streaming and reading fragments occurring in Zookeeper.  Therefore, in the second approach, we use a simple Kafka API, which does not require resorting to Zookeeper.  Here, the read fragments are tracked in Kafka - Spark Streaming, for this are used control points.  In this case, the inconsistency between Spark Streaming and Zookeeper / Kafka is eliminated. <br><br>  Therefore, even in the event of a bounce, Spark Streaming receives each entry strictly once.  Here we need to ensure that our output operation, in which data is stored in external storage, is either an idempotent or atomic transaction, in which both the results and the displacements are stored.  This is exactly how exactly-once semantics is achieved when deriving our results. <br><br>  Although, there is one drawback: the offsets in Zookeeper are not updated.  Therefore, Kafka-based Zookeeper monitoring tools do not allow tracking progress. <br>  However, we can still apply to offsets, if the processing is arranged by this method - we apply to each package and update Zookeeper ourselves. <br><br>  That's all that we wanted to tell about the integration of Apache Kafka and Spark Streaming.  Hope you enjoyed it. </div><p>Source: <a href="https://habr.com/ru/post/417123/">https://habr.com/ru/post/417123/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../417111/index.html">Online conferences: streaming vs webinar</a></li>
<li><a href="../417113/index.html">3D-printer of the Italian in Russia: Raise3D N1 Dual - modeling and prototyping</a></li>
<li><a href="../417115/index.html">Flutter.io bury or burn?</a></li>
<li><a href="../417117/index.html">Reverse engineering NES emulator in GameCube game</a></li>
<li><a href="../417119/index.html">Pagination in Vue.js</a></li>
<li><a href="../417125/index.html">RTC Meetup .Net: welcome to first meeting</a></li>
<li><a href="../417127/index.html">Tesla has signed an agreement to build Gigafabrika 3 in China</a></li>
<li><a href="../417129/index.html">Mind universe</a></li>
<li><a href="../417131/index.html">How to already feel transactions in MongoDB</a></li>
<li><a href="../417135/index.html">Unity3D: how to know the degree of illumination of the point of the scene?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>