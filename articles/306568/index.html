<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Learning machines - fun stuff: modern face recognition with deep learning</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Have you noticed that Facebook has gained an uncanny ability to recognize your friends in your photos? In the old days, Facebook marked your friends i...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Learning machines - fun stuff: modern face recognition with deep learning</h1><div class="post__text post__text-html js-mediator-article">  Have you noticed that Facebook has gained an uncanny ability to recognize your friends in your photos?  In the old days, Facebook marked your friends in photos only after you clicked on the corresponding image and entered your friend's name through the keyboard.  Now after you upload your photo, Facebook notes any notes to you that <em>looks like magic</em> : <br><a name="habracut"></a><br><img src="https://habrastorage.org/getpro/habr/post_images/8a5/d32/e28/8a5d32e28a48735638a54e64623be6a0.gif"><br><br>  Facebook automatically marks people in your photos that you noted once before.  I can not decide for myself, it is useful or scary! <br><br>  This technology is called face recognition.  Facebook's algorithms can recognize the faces of your friends after you have marked them only a couple of times.  This is an amazing technology: Facebook is able to recognize faces <a href="https://research.facebook.com/publications/deepface-closing-the-gap-to-human-level-performance-in-face-verification/">with an accuracy of 98%</a> - almost the same as a person! 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Consider how modern facial recognition works!  However, simply recognizing your friends would be too easy.  We can go to the frontier of this technology in order to solve a more complicated task - let's try to distinguish <a href="https://en.wikipedia.org/wiki/Will_Ferrell">Will Ferrell</a> (a famous actor) from <a href="https://en.wikipedia.org/wiki/Chad_Smith">Chad Smith</a> (a famous rock musician)! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/897/f33/6cd/897f336cd0a5bb495e51b2fe14c24336.jpg"><br>  One of these people is Will Ferrell.  The other is Chad Smith.  I swear - these are different people! <br><br><h1>  How to use machine learning for a very difficult problem </h1><br>  Until now, in <a href="https://medium.com/%40ageitgey/machine-learning-is-fun-80ea3ec3c471">parts 1</a> , <a href="https://medium.com/%40ageitgey/machine-learning-is-fun-part-2-a26a10b68df3">2</a> and <a href="https://medium.com/%40ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721">3,</a> we used machine learning to solve isolated problems with only one step - <a href="https://medium.com/%40ageitgey/machine-learning-is-fun-80ea3ec3c471">estimating the cost of a house</a> , <a href="https://medium.com/%40ageitgey/machine-learning-is-fun-part-2-a26a10b68df3">creating new data based on existing data</a> and <a href="https://medium.com/%40ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721">determining whether an image contains an object</a> .  All these problems can be solved by choosing one machine learning algorithm, entering data and obtaining a result. <br><br>  But face recognition is actually a sequence of several related problems: <br><br>  1. First, you need to look at the image and find all the faces on it. <br><br>  2. Secondly, it is necessary to focus on each face and determine that, despite the unnatural turn of the face or poor lighting, it is the same person. <br><br>  3. Thirdly, it is necessary to highlight the unique characteristics of the face that can be used to distinguish it from other people - for example, the size of the eyes, the elongation of the face, etc. <br><br>  4. In conclusion, it is necessary to compare these unique characteristics of the face with the characteristics of other people known to you in order to determine the name of the person. <br><br>  The human brain does all this automatically and instantly.  In fact, people recognize faces <em>extremely well</em> and, ultimately, see faces in everyday objects: <br><br><img src="https://habrastorage.org/files/d49/12d/788/d4912d788324436198d66d8840b5060d.jpg"><br><br>  Computers are incapable of such a high level of generalization ( <em>at least for the time being ...</em> ), so you have to teach them every step in the process separately. <br><br>  It is necessary to build a <em>pipeline</em> on which we will find a solution at each step of the face recognition process separately and transfer the result of the current step to the next.  In other words, we will combine several machine learning algorithms into one chain: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5b6/672/06e/5b667206e945d5dd400a24a6c6b655db.gif"><br>  How basic facial recognition can work <br><br><h1>  Face detection - step by step </h1><br>  Let's solve this problem consistently.  At each step, we will learn about the new machine learning algorithm.  I'm not going to explain each individual algorithm completely, so as not to turn this article into a book, but you will learn the basic ideas of each of the algorithms and learn how you can create your own face recognition system in Python using <a href="https://cmusatyalab.github.io/openface/">OpenFace</a> and <a href="http://dlib.net/">dlib</a> . <br><br><h3>  Step 1. Finding all the faces. </h3><br>  The first step on our assembly line is <em>face detection</em> .  It is clear that you need to select all the faces in the photo before trying to recognize them! <br><br>  If you have used a photograph in the past 10 years, then you probably have seen how the detection of faces works: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/083/216/067/0832160679db8adcebd6596541fcd73e.png"><br><br>  Face detection is a great thing for cameras.  If the camera can automatically detect faces, then you can be sure that all faces will be in focus before the picture is taken.  But we will use it for another purpose - to find the image areas that need to be transferred to the next stage of our pipeline. <br><br>  Face detection became the mainstream in the early 2000s, when Paul Viola and Michael Jones invented a <a href="https://en.wikipedia.org/wiki/Viola%25E2%2580%2593Jones_object_detection_framework">way to detect faces</a> that were fast enough to work on cheap cameras.  However, now there are much more reliable solutions.  We are going to use the <a href="http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf">method opened in 2005</a> - a histogram of directional gradients ( <strong>HOG for</strong> short). <br><br>  To detect faces in the image, we will make our image black and white, because  Color data is not needed for face detection: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b8a/c36/00c/b8ac3600c5cd20663c8d9e84ed0ae79d.jpg"><br><br>  Then we look at each pixel in our image sequentially.  For each individual pixel, its immediate surroundings should be considered: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6eb/92f/1f7/6eb92f1f756ee20e22597273e4973039.gif"><br><br>  Our goal is to highlight how dark the current pixel is compared to the pixels directly adjacent to it.  Then we draw an arrow indicating the direction in which the image becomes darker: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/50a/8f7/87c/50a8f787c3fa378d19401739f86ad096.gif"><br>  When considering this single pixel and its closest neighbors, it can be seen that the image darkens upwards to the right. <br><br>  If you repeat this process for <strong>each individual pixel</strong> in the image, then, ultimately, each pixel will be replaced by an arrow.  These arrows are called the <em>gradient</em> , and they show the flow from light to dark throughout the image: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/09f/8e2/f60/09f8e2f609aa642b225e09b0e142d0d2.gif"><br><br>  It may seem that the result is something random, but there is a very good reason for replacing pixels with gradients.  When we analyze the pixels directly, then the dark and bright images of the same person will have very different pixel intensity values.  But if we consider only the <em>direction of</em> change in brightness, then both the dark and the bright images will have exactly the same idea.  This greatly facilitates the solution of the problem! <br><br>  But maintaining the gradient for each individual pixel gives us a way of carrying too much detail.  We, ultimately, <a href="https://en.wiktionary.org/wiki/see_the_forest_for_the_trees">do not see the forest because of the trees</a> .  It would be better if we could just see the main stream of light / dark at a higher level, thus considering the basic structure of the image. <br><br>  To do this, divide the image into small squares of 16x16 pixels in each.  In each square, it is necessary to calculate how many gradient arrows show in each main direction (i.e., how many arrows are directed up, up-right, right, etc.).  Then the considered square in the image is replaced by an arrow with the direction prevailing in this square. <br><br>  In the end result, we turn the original image into a very simple representation that shows the basic structure of the face in a simple form: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8c6/66d/6d1/8c666d6d1818c90842584e912631efe3.gif"><br>  The original image is converted to a HOG representation, demonstrating the main characteristics of the image, regardless of its brightness. <br><br>  To detect faces on this HOG image, all that is required of us is to find such a part of the image that most closely resembles the well-known HOG structure obtained from the group of people used for training: <br><br><img src="https://habrastorage.org/files/1c8/cb8/cd6/1c8cb8cd6b344d84a196e4b82d9a4068.jpg"><br><br>  Using this method, you can easily find faces on any image: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6d8/683/746/6d8683746e25e5502d332c08ef517ef5.jpg"><br><br>  If you want to perform this step yourself using Python and dlib, then <a href="https://gist.github.com/ageitgey/1c1cb1c60ace321868f7410d48c228e1">there is a program</a> that shows how to create and view HOG images. <br><br><h3>  Step 2. Location and mapping of faces </h3><br>  So, we have highlighted the faces in our image.  But now the problem appears: the same person, viewed from different directions, looks completely different for the computer: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/36a/1ad/d77/36a1add77bcdad8757a3d08144fcd6a8.png"><br>  People can easily see that both images belong to actor Will Ferrell, but computers will treat them as the faces of two different people. <br><br>  To take this into account, we try to transform each image so that the eyes and lips are always in the same place of the image.  Comparison of individuals in the next steps will be greatly simplified. <br><br>  For this, we use an algorithm called <strong>‚Äúestimation of anthropometric points‚Äù</strong> .  There are many ways to do this, but we are going to use the <a href="http://www.csc.kth.se/~vahidk/papers/KazemiCVPR14.pdf">approach proposed in 2014 by Wahid Kacemi and Josephine Sullivan</a> . <br><br>  The basic idea is that 68 specific points ( <em>marks</em> ) are present on each face ‚Äî the protruding part of the chin, the outer edge of each eye, the inner edge of each eyebrow, etc.  Then the machine learning algorithm is tuned to search for these 68 specific points on each face: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d0f/149/d6e/d0f149d6e382db17069afd10a823eba1.png"><br>  68 anthropometric points we have on each face <br><br>  Below is the result of the location of 68 anthropometric points on our test image: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/dd8/f4f/74c/dd8f4f74c7041dd57fbfc3e94a5f5e6f.jpg"><br>  PROFESSIONAL BOARD FOR BEGINNERS: the same method can be used to put your own version of a 3D face filter into a real-time person in Snapchat! <br><br>  Now that we know where the eyes and the mouth are, we will simply rotate, scale and <a href="https://en.wikipedia.org/wiki/Shear_mapping">shift the</a> image so that the eyes and mouth are centered as best as possible.  We will not introduce any unusual 3D deformations, as they may distort the image.  We will only do basic image transformations, such as rotation and scaling, which preserve parallel lines (so-called <a href="https://en.wikipedia.org/wiki/Affine_transformation">affine transformations</a> ): <br><br><img src="https://habrastorage.org/files/d4a/792/758/d4a792758d3943fcae4ec69ef1fbd06e.jpg"><br><br>  Now, regardless of how the face is turned, we can center the eyes and mouth so that they are approximately in the same position in the image.  This will make the accuracy of our next step much higher. <br><br>  If you have a desire to try this step yourself using Python and dlib, then there is a <a href="https://gist.github.com/ageitgey/ae340db3e493530d5e1f9c15292e5c74">program for finding anthropometric points</a> and a <a href="https://gist.github.com/ageitgey/82d0ea0fdb56dc93cb9b716e7ceb364b">program for converting an image based on these points</a> . <br><br><h3>  Step 3. Encoding faces </h3><br>  Now we come to the essence of the problem - the very distinction of persons.  This is where the fun begins! <br><br>  The simplest approach to recognizing faces is a direct comparison of the unknown person found in step 2 with all those already marked.  If we find an already-marked person, very similar to our unknown, it will mean that we are dealing with the same person.  Sounds like a very good idea, isn't it? <br><br>  In fact, with this approach, there is a huge problem.  A site like Facebook with billions of users and trillions of photos cannot cycle through each previously marked person in a rather cyclical way, comparing it with each newly uploaded picture.  It would take too much time.  It is necessary to recognize faces in milliseconds, not hours. <br><br>  We need to learn how to extract some basic characteristics from each person.  Then we could get these characteristics from an unknown person and compare them with the characteristics of famous people.  For example, you can measure each ear, determine the distance between the eyes, the length of the nose, etc.  If you have ever watched a television series about the work of the Las Vegas forensic laboratory staff ( <a href="https://en.wikipedia.org/wiki/CSI:_Crime_Scene_Investigation">‚ÄúCSI: a crime scene‚Äù</a> ), then you know what it is about: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/99b/777/db9/99b777db97965934db670efdf6bf13ca.gif"><br>  Like in the movies!  So it seems to be true! <br><br><h3>  The most reliable method to measure face </h3><br>  Well, but what characteristics should be obtained from each person to build a database of famous people?  Ear sizes?  Nose length?  Eye color?  Anything else? <br><br>  It turns out that the characteristics that seem obvious to us humans (for example, eye color) do not make sense for a computer analyzing individual pixels in an image.  The researchers found that the most appropriate approach is to enable the computer to determine the characteristics that need to be collected.  In-depth training allows better than people can do to identify parts of the face that are important for its recognition. <br><br>  The solution is to train the deep convolutional neural network ( <a href="https://medium.com/%40ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721">this is exactly what we did in release 3</a> ).  But instead of learning the network to recognize graphic objects, as we did last time, we are now going to teach it to create 128 characteristics for each face. <br><br>  The learning process is valid when considering 3 face images at the same time: <br><br>  1. Download the training face image of a famous person. <br><br>  2. Download another face image of the same person. <br><br>  3. Upload another person's face image. <br><br>  Then the algorithm considers the characteristics that it currently creates for each of the three images indicated.  It slightly adjusts the neural network so that the characteristics created by it for images 1 and 2 are a little closer to each other, and for images 2 and 3 - a little further. <br><br>  <strong>A single ‚Äústructured‚Äù learning step:</strong> <br><br><img src="https://habrastorage.org/files/ea9/1e9/6aa/ea91e96aa6a646a0a37902d4d4ac0224.jpg"><br><br>  After repeating this step millions of times for millions of images of thousands of different people, the neural network is able to reliably create 128 characteristics for each person.  Any ten different images of the same person will give roughly the same characteristics. <br><br>  Specialists in training machines call these 128 characteristics of each person a <strong>set of characteristics (features)</strong> .  The idea of ‚Äã‚Äãconverting complex source data, such as, for example, an image, to a list of computer-generated numbers, turned out to be extremely promising in teaching machines (in particular, for translations).  This approach for individuals that we use <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/1A_089.pdf">was proposed in 2015 by researchers from Google</a> , but there are many similar approaches. <br><br><h3>  The encoding of our face image </h3><br>  The training process of the convolutional neural network in order to output sets of facial characteristics requires a large amount of data and high computer performance.  Even on an expensive <a href="http://www.nvidia.com/object/tesla-supercomputing-solutions.html">NVidia Telsa video card, it</a> takes <a href="https://twitter.com/brandondamos/status/757959518433243136">about 24 hours of</a> continuous training to get good accuracy. <br><br>  But if the network is trained, then it is possible to create characteristics for any person, even for the one that has never been seen before!  Therefore, this step is required only once.  Fortunately for us, the good people at <a href="https://cmusatyalab.github.io/openface/">OpenFace</a> have already done this and have <a href="https://github.com/cmusatyalab/openface/tree/master/models/openface">provided access to several trained networks</a> that we can immediately use.  Thanks to <a href="http://bamos.github.io/">Brandon Amos</a> and the team! <br><br>  As a result, all that is required of us is to take our face images through their pre-trained network and get 128 characteristics for each person.  Below are the specifications for our test image: <br><br><img src="https://habrastorage.org/files/587/078/653/58707865344c47e9b341a03343989229.JPG"><br><br>  But what specific parts of the face do these 128 numbers describe?  It turns out that we have no idea about this.  But in fact it does not matter to us.  We should be concerned only with the fact that the network gives out approximately the same numbers, analyzing two different images of the same person. <br><br>  If there is a desire to try to perform this step independently, then OpenFace <a href="">provides a Lua script</a> that creates the characteristics of all the images in the folder and writes them to a csv file.  You can run it <a href="https://gist.github.com/ageitgey/ddbae3b209b6344a458fa41a3cf75719">as shown</a> . <br><br><h3>  Step 4. Finding the name of the person after encoding the face </h3><br>  The last step is actually the easiest in the whole process.  All that is required of us is to find a person in our database of famous persons who has the characteristics closest to those of our test image. <br><br>  This can be done using any basic machine learning classification algorithm.  No special deep learning techniques are required.  We will use a simple linear <a href="https://en.wikipedia.org/wiki/Support_vector_machine">SVM classifier</a> , but many other classification algorithms can be applied. <br><br>  We are only required to train a classifier who can take the characteristics of a new test image and tell which famous person has the best fit.  The work of such a classifier takes milliseconds.  The result of the classifier is the name of the person! <br><br>  Let's test our system.  First of all, I trained the classifier using feature sets from approximately 20 images of Will Ferrell, Chad Smith and Jimmy Fallon: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c43/f85/8e3/c43f858e379f01e7f69ce79abade0cdd.jpg"><br>  Oh, these amazing pictures for learning! <br><br>  Then I drove the classifier on each frame of the famous Youtube video, where on the Jimmy Fallon show <a href="https://www.youtube.com/watch%3Fv%3DEsWHyBOk2iQ">Will Ferrell and Chad Smith pretend to be each other</a> : <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bbf/414/0db/bbf4140db82be482546000312b6df06b.gif"><br><br>  It worked!  And look how great it worked for people from many different directions - even in profile! <br><br><h1>  Independent execution of the whole process </h1><br>  Consider the required steps: <br><br>  1. Process the image using the HOG algorithm to create a simplified version of the image.  In this simplified image, find the area that is most similar to the created HOG-representation of the face. <br><br>  2. Determine the position of the face by setting the main anthropometric points on it.  After positioning these anthropometric points, use them to transform the image in order to center the eyes and mouth. <br><br>  3. Pass the centered face image through a neural network trained in characterization of the face.  Save the resulting 128 characteristics. <br><br>  4. After reviewing all persons whose characteristics were removed earlier, identify the person whose facial characteristics are closest to those obtained.  It is done! <br><br>  Now that you know how it all works, review the instructions from the very beginning to the end of how to conduct the entire face recognition process on your own computer using <a href="https://cmusatyalab.github.io/openface/">OpenFace</a> : <br><br><h2>  Before you start </h2><br>  Make sure you have Python, OpenFace and dlib installed.  You can <a href="https://cmusatyalab.github.io/openface/setup/">install them manually</a> or use a pre-configured container image in which everything is already installed: <br><br><pre><code class="hljs dos">docker pull bamos/openface docker run -p <span class="hljs-number"><span class="hljs-number">9000</span></span>:<span class="hljs-number"><span class="hljs-number">9000</span></span> -p <span class="hljs-number"><span class="hljs-number">8000</span></span>:<span class="hljs-number"><span class="hljs-number">8000</span></span> -t -i bamos/openface /bin/bash <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /root/openface</code> </pre> <br><br>  Newbie professional advice: if you use Docker on OSX, you can make the OSX / Users / folder visible inside the container image, as shown below: <br><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">docker</span></span> run -v /Users:/host/Users -p <span class="hljs-number"><span class="hljs-number">9000</span></span>:<span class="hljs-number"><span class="hljs-number">9000</span></span> -p <span class="hljs-number"><span class="hljs-number">8000</span></span>:<span class="hljs-number"><span class="hljs-number">8000</span></span> -t -i bamos/openface /bin/bash cd /root/openface</code> </pre> <br><br>  Then you can exit to all your OSX files inside the container image on / host / Users / ... <br><br><pre> <code class="hljs mel"><span class="hljs-keyword"><span class="hljs-keyword">ls</span></span> /host/Users/</code> </pre> <br><br><h3>  Step 1 </h3><br>  Create a folder called <code>./training-images/</code> in the openface folder. <br><br><pre> <code class="hljs dos"><span class="hljs-built_in"><span class="hljs-built_in">mkdir</span></span> training-images</code> </pre> <br><br><h3>  Step 2 </h3><br>  Create a subfolder for each person you want to recognize.  For example: <br><br><pre> <code class="hljs dos"><span class="hljs-built_in"><span class="hljs-built_in">mkdir</span></span> ./training-images/will-ferrell/ <span class="hljs-built_in"><span class="hljs-built_in">mkdir</span></span> ./training-images/chad-smith/ <span class="hljs-built_in"><span class="hljs-built_in">mkdir</span></span> ./training-images/jimmy-fallon/</code> </pre> <br><br><h3>  Step 3 </h3><br>  Copy all the images of each person in the appropriate subfolders.  Make sure there is only one face on each image.  You do not need to crop the image around the face.  OpenFace will do this automatically. <br><br><h3>  Step 4 </h3><br>  Run openface scripts from the openface root directory: <br><br>  Position detection and alignment must first be performed: <br><br><pre> <code class="hljs mel">./util/<span class="hljs-keyword"><span class="hljs-keyword">align</span></span>-dlib.py ./training-images/ <span class="hljs-keyword"><span class="hljs-keyword">align</span></span> outerEyesAndNose ./aligned-images/ --<span class="hljs-keyword"><span class="hljs-keyword">size</span></span> <span class="hljs-number"><span class="hljs-number">96</span></span></code> </pre> <br><br>  As a result, a new subfolder <code>./aligned-images/</code> will be created with a cropped and aligned version of each of your test images. <br><br>  Then create views from aligned images: <br><br><pre> <code class="hljs coffeescript">.<span class="hljs-regexp"><span class="hljs-regexp">/batch-represent/main.lua -outDir ./generated-embeddings/</span></span> -data .<span class="hljs-regexp"><span class="hljs-regexp">/aligned-images/</span></span></code> </pre> <br><br>  The subfolder <code>./generated-embeddings/</code> will contain a csv-file with sets of characteristics for each image. <br><br>  Train your face detection model: <br><br><pre> <code class="hljs pgsql">./demos/classifier.py train ./<span class="hljs-keyword"><span class="hljs-keyword">generated</span></span>-embeddings/</code> </pre> <br><br>  A new file will be created with the name <code>./generated-embeddings/classifier.pk</code> .  This file contains the SVM model that will be used to recognize new faces. <br><br>  From now on, you will have a working face recognizer! <br><br><h3>  Step 5. We recognize the faces! </h3><br>  Take a new picture with an unknown face.  Pass it through the classifier script, such as the following: <br><br><pre> <code class="hljs pgsql">./demos/classifier.py infer ./<span class="hljs-keyword"><span class="hljs-keyword">generated</span></span>-embeddings/classifier.pkl your_test_image.jpg</code> </pre> <br><br>  You should get something like this warning: <br><br><pre> <code class="hljs javascript">=== <span class="hljs-regexp"><span class="hljs-regexp">/test-images/</span></span>will-ferrel<span class="hljs-number"><span class="hljs-number">-1.</span></span>jpg === Predict will-ferrell <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> <span class="hljs-number"><span class="hljs-number">0.73</span></span> confidence.</code> </pre> <br><br>  Here, if you wish, you can customize the python script <code>./demos/classifier.py</code> . <br><br>  Important notes: <br><br>  ‚Ä¢ If the results are not satisfactory, then try adding a few more images for each person in step 3 (especially images from different directions). <br><br>  ‚Ä¢ This script will always issue a warning, even if it does not know this face.  In real use, you need to check the degree of confidence and remove warnings with a low confidence level, since they are most likely incorrect. </div><p>Source: <a href="https://habr.com/ru/post/306568/">https://habr.com/ru/post/306568/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../306558/index.html">Is it beneficial for Apple to be an ally of Google in the fight for the online advertising market?</a></li>
<li><a href="../306560/index.html">Cloud storage: new API functions</a></li>
<li><a href="../306562/index.html">The story ‚ÄúNIICHOSHI. Duty night "</a></li>
<li><a href="../306564/index.html">Ruby Ecosystem (on Rails) with a bitter aftertaste, or ‚ÄúHow we love to poke PHP‚Äù</a></li>
<li><a href="../306566/index.html">How we made the converter and player for CinemaDNG to CUDA</a></li>
<li><a href="../306570/index.html">Low cost VDS hosting in Russia and its technical support</a></li>
<li><a href="../306572/index.html">iOS 10: Notification Content Extension</a></li>
<li><a href="../306574/index.html">Study: What will happen if you tell users how you target advertising?</a></li>
<li><a href="../306580/index.html">Docker Volume plugin for Nutanix on AOS 4.7</a></li>
<li><a href="../306582/index.html">Rust: & and ref in patterns</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>