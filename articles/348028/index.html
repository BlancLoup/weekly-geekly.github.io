<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Convolutional neural network, part 2: training in the error back-propagation algorithm</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the first part , the structure, topology, activation functions and training set were considered. In this part I will try to explain how the convolu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Convolutional neural network, part 2: training in the error back-propagation algorithm</h1><div class="post__text post__text-html js-mediator-article">  <a href="https://habrahabr.ru/post/348000/">In the first part</a> , the structure, topology, activation functions and training set were considered.  In this part I will try to explain how the convolutional neural network is trained. <br><br><h2>  Training convolutional neural network </h2><br>  At the initial stage, the neural network is untrained (unconfigured).  In a general sense, learning is the sequential presentation of an image to the input of a neural network, from the training set, then the resulting answer is compared with the desired output, in our case it is 1 - the image represents a person, minus 1 - the image represents a background (not the person), the difference between the expected The answer and the result is the result of the error function (delta error).  Then this delta needs to be extended to all connected neurons of the network. <br><a name="habracut"></a><br>  Thus, the training of the neural network is reduced to minimizing the error function by adjusting the weights of the synaptic connections between neurons.  The error function is the difference between the received answer and the desired one.  For example, a face image was submitted to the input, suppose that the output of the neural network was 0.73, and the desired result is 1 (because the face image), we find that the network error is the difference, that is, 0.27.  Then the weights of the output layer of the neurons are adjusted in accordance with the error.  For neurons of the output layer, their actual and desired output values ‚Äã‚Äãare known.  Therefore, setting up link weights for such neurons is relatively simple.  However, the setting is not so obvious for neurons of the previous layers.  For a long time, the algorithm for propagating errors across hidden layers was not known. <br><br><h3>  Error Propagation Algorithm </h3><br>  For learning the described neural network, the backpropagation algorithm was used.  This method of teaching a multilayer neural network is called the generalized delta rule.  The method was proposed in 1986 by Rumelhart, McCleland and Williams.  This marked a revival of interest in neural networks, which began to fade away in the early 70s.  This algorithm is the first and the main practically applicable for training multilayer neural networks. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      For the output layer, the adjustment of the weights is intuitive, but for the hidden layers there has been no known algorithm for a long time.  The weights of the hidden neuron should vary in direct proportion to the error of those neurons with which the neuron is associated.  That is why the reverse propagation of these errors through the network allows you to correctly adjust the weights of the connections between all layers.  In this case, the magnitude of the error function is reduced and the network is trained. <br><br>  The basic relations of the back propagation method are obtained with the following notation: <br><br><img src="https://habrastorage.org/webt/_j/mi/fx/_jmifx2svjxa4guckq03t2a0_94.png"><br><br>  The magnitude of the error is determined by the formula 2.8 root-mean-square error: <br><br><img src="https://habrastorage.org/webt/2s/hf/fv/2shffvhtctajve39dbhgravkkwe.png"><br><br>  The non-activated state of each neuron j for the image p is written as a weighted sum using the formula 2.9: <br><br><img src="https://habrastorage.org/webt/vf/h5/b7/vfh5b7keq77_cxnakznyhpse8tm.png"><br><br>  The output of each neuron j is the value of the activation function. <br><br><img src="https://habrastorage.org/webt/za/wa/q8/zawaq8lygfxzqxxqwvuur0_wmrk.png">  which puts the neuron in an activated state.  As an activation function, any continuously differentiable monotonic function can be used.  The activated state of the neuron is calculated by the formula 2.10: <br><br><img src="https://habrastorage.org/webt/gs/os/ti/gsostiadp2q7los8_ktjr_usw44.png"><br><br>  As a method of minimizing the error, the method of gradient descent is used, the essence of this method comes down to finding the minimum (or maximum) of the function due to movement along the gradient vector.  To search for a minimum, the movement must be carried out in the direction of the antigradient.  Gradient descent method in accordance with Figure 2.7. <br><br><img src="https://habrastorage.org/webt/ut/fa/cq/utfacqfshnmdv0aa_q3b8o8dvpc.png"><br><br>  The gradient of the loss function is a vector of partial derivatives, calculated by the formula 2.11: <br><br><img src="https://habrastorage.org/webt/zt/z7/v8/ztz7v8lzbnz8s-cce76uno_rmmi.png"><br><br>  The derivative of the error function for a particular image can be written by the chain rule, formula 2.12: <br><br><img src="https://habrastorage.org/webt/dk/dk/l_/dkdkl_r7xumoixf87kavsech9we.png"><br><br>  Neuron error <img src="https://habrastorage.org/webt/bq/s-/zl/bqs-zl6nrw2g0_pny858vlctwww.png">  usually written as a symbol Œ¥ (delta).  For the output layer, the error is defined explicitly, if we take the derivative of formula 2.8, we get <b>t</b> minus <b>y</b> , that is, the difference between the desired and the obtained output.  But how to calculate the error for hidden layers?  To solve this problem, an algorithm for back propagation of an error was invented.  Its essence lies in the sequential calculation of errors of hidden layers using the error values ‚Äã‚Äãof the output layer, i.e.  error values ‚Äã‚Äãpropagate through the network in the opposite direction from the output to the input. <br><br>  Error Œ¥ for the hidden layer is calculated by the formula 2.13: <br><img src="https://habrastorage.org/webt/cv/b0/c2/cvb0c2s_azob5h-dyrdw4k0-4ws.png"><br><br>  The error propagation algorithm is reduced to the following stages: <br><ul><li>  direct signal propagation over the network, calculating the state of neurons; </li><li>  calculating the error value Œ¥ for the output layer; </li><li>  backward propagation: sequentially from the end to the beginning for all hidden layers we calculate Œ¥ by the formula 2.13; </li><li>  update the weights of the network to the previously calculated error Œ¥. </li></ul><br>  The algorithm for back propagation of an error in a multilayer perceptron is shown below: <br><img src="https://habrastorage.org/files/627/6e1/d36/6276e1d365ba4f8497cd41fb110d7619.gif"><br>  Up to this point, the cases of error propagation through the perceptron layers, that is, the output and hidden, have been considered, but in addition to them, there are subsample and convolutional networks in the convolutional neural network. <br><br><h3>  Calculation of errors on the subsample layer </h3><br>  The calculation of the error on the subsample layer is presented in several variants.  The first case, when the subsampling layer is in front of a fully connected one, then it has neurons and connections of the same type as in the fully connected layer, respectively, calculating Œ¥ error is no different from calculating Œ¥ of the hidden layer.  The second case, when the subsampling layer is in front of the convolutional layer, the calculation of Œ¥ occurs by reverse convolution.  To understand convolution back, you first need to understand ordinary convolution and the fact that the sliding window on the feature map (during direct signal propagation) can be interpreted as a normal hidden layer with connections between neurons, but the main difference is that these connections are shared, that is, one connection with a specific weight value may be in several pairs of neurons, and not just one.  Interpretation of the convolution operation in the usual multilayer form in accordance with Figure 2.8. <br><br><img src="https://habrastorage.org/webt/bo/gu/l_/bogul_qdvvftwf-bfuv_aseoe28.png"><br>  Figure 2.8 - Interpretation of a convolution operation into a multi-layered view, where links with the same color have the same weight.  The subsample map is marked in blue, the synaptic nucleus is multicolored, and the resulting convolution is orange. <br><br>  Now that the convolution operation is presented in its usual multi-layered form, it can be intuitively understood that the deltas are calculated in the same way as in the hidden layer of a fully connected network.  Accordingly, having previously calculated deltas of the convolutional layer, one can calculate the deltas of the subsampling, in accordance with Figure 2.9. <br><img src="https://habrastorage.org/webt/2w/sh/48/2wsh489btuthgck5zdkurlub7nq.png"><br>  Figure 2.9 - Calculation of Œ¥ sub-sampling layer due to Œ¥ convolutional layer and the core <br><br>  <b>Reverse convolution</b> is the same method of calculating deltas, only in a slightly tricky way, consisting in rotating the nucleus 180 degrees and sliding the process of scanning a convolutional delta map with modified edge effects.  In simple words, we need to take the core of the convolutional map (following the subsampling layer) to rotate it 180 degrees and do the usual convolution using previously calculated convolutional map deltas, but so that the scan window extends beyond the map.  The result of the operation of the inverse convolution in accordance with Figure 2.10, the cycle of the passage of the inverse convolution in accordance with Figure 2.11. <br><img src="https://habrastorage.org/webt/ag/sl/v7/agslv77c1vibvzm9benpve1z9x8.png"><br>  Figure 2.10 - The result of the operation of the reverse convolution <br><br><img src="https://habrastorage.org/webt/t8/d1/ut/t8d1ut61l5dzsl8y5i2a00wtbtw.png"><br>  Figure 2.11 - A rotated core 180 degrees scans a convolutional map <br><br><h3>  Calculation of the error on the convolutional layer </h3><br>  Usually the upcoming layer after the convolutional is a subsampling, so our task is to calculate the deltas of the current (convolutional) layer due to the knowledge of the deltas of the subsampling layer.  In fact, the delta error is not calculated, but copied.  With the direct propagation of the signal, the neurons of the subsampling layer were formed by a non-overlapping scan window on the convolutional layer, during which the neurons with the maximum value were selected. <br><br><h2>  Conclusion </h2><br>  By presenting the convolution operation in the usual multilayer form (Figure 2.8), one can intuitively understand that the deltas are calculated in the same way as in the hidden layer of a fully connected network. <br><br><h2>  Sources </h2><br>  <a href="http://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/">Error propagation algorithm for convolutional neural network</a> <br><br>  Reverse error propagation in convolutional layers <br>  <a href="https://grzegorzgwardys.wordpress.com/2016/04/22/8/">one</a> and <a href="https://becominghuman.ai/back-propagation-in-convolutional-neural-networks-intuition-and-code-714ef1c38199">two</a> <br><br>  <a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">Back propagation of error in perceptron</a> <br><br>  You can also read the Makarenko's dissertation in the RSL: ALGORITHMS AND PROGRAM CLASSIFICATION SYSTEM </div><p>Source: <a href="https://habr.com/ru/post/348028/">https://habr.com/ru/post/348028/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../348018/index.html">Even in Java 9, ArrayList can still (and should) be improved.</a></li>
<li><a href="../348020/index.html">Blockchain: Networking, Signature Verification and Student Assignment, Part 2</a></li>
<li><a href="../348022/index.html">Game development under NES in C. Chapters 1-3. From introduction to Hello World</a></li>
<li><a href="../348024/index.html">SecurityWeek 2: army of clones, Google hunts for ghosts, Blizzard patches</a></li>
<li><a href="../348026/index.html">Installing Linux without .ISO and virtualization</a></li>
<li><a href="../348032/index.html">Mobility Express - when they decided to migrate and scale the wireless network, but as always, there is no money</a></li>
<li><a href="../348034/index.html">Meeting Room Little Helper</a></li>
<li><a href="../348036/index.html">Software sound synthesis on early personal computers. Part 1</a></li>
<li><a href="../348038/index.html">WSTester - JS library for testing web services with WebSocket</a></li>
<li><a href="../348040/index.html">How Atlassian has built a $ 10 billion business. Part 2</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>