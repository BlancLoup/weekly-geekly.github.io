<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Augmentation (augmentation, ‚Äúblowing up‚Äù) of data for training a neural network using printed characters as an example</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="On Habr√© there are already many articles devoted to pattern recognition by methods of learning machines, such as neural networks, support vector machi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Augmentation (augmentation, ‚Äúblowing up‚Äù) of data for training a neural network using printed characters as an example</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/files/1b6/dbd/0a9/1b6dbd0a96b342d491ef96c986261e86.jpeg"></div><br>  On Habr√© there are already many articles devoted to pattern recognition by methods of learning machines, such as neural networks, support vector machines, random trees.  All of them require a significant number of examples for training and setting parameters.  Creating a training and test database of images of adequate volume for them is a very non-trivial task.  And this is not about the technical difficulties of collecting and storing a million images, but about the perennial situation when you have one and a half pictures in the first stage of system development.  In addition, it should be understood that the composition of the training base can affect the quality of the resulting recognition system more than all other factors.  Despite this, in most articles this important development stage is completely omitted. <br><br>  If it is interesting to you to learn about all this - welcome under kat. <br><a name="habracut"></a><br>  Before creating a database of examples of images and training a neural network, it is necessary to specify a technical problem.  It is clear that the recognition of handwriting, the emotions of a human face, or the location of a photograph is a completely different task.  It is also clear that the architecture of the used neural network will be influenced by the choice of platform: in the ‚Äúcloud‚Äù, on the PC, on the mobile device ‚Äî the available computing resources differ by orders of magnitude. <br><br>  Further it becomes more interesting.  Recognizing high-resolution images from cameras or blurry images from a webcam without autofocus will require completely different data for training, testing and validation.  This is hinted at by ‚Äútheorems about the absence of free data‚Äù.  That is why freely distributed educational database of images (for example, [ <a href="http://yann.lecun.com/exdb/mnist/index.html">1</a> , <a href="http://www.image-net.org/index">2</a> , <a href="http://bias.csr.unibo.it/fvc2000/databases.asp">3</a> ]), are excellent for academic research, but are almost always inapplicable in real-world tasks due to their ‚Äúgeneralization‚Äù. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The more accurately the training sample approximates the population of images that will be input to your system, the higher the maximum achievable quality of the result will be.  It turns out that a correctly composed training sample is the most concrete specification of the technical task!  For example, if we want to recognize printed characters in a photograph taken with a mobile device, the base of examples should contain photographs of documents from different sources with different lighting, taken from different models of phones and cameras.  All this complicates the collection of the required number of examples for learning the recognizer. <br><br>  Let us now consider several possible ways to prepare a sample of images to create a recognition system. <br><br><h5>  <b>Creating learning examples from natural images.</b> </h5><br>  Examples for learning from natural images are based on real data.  Their creation consists of the following steps: <br><ol><li>  Collection of graphic data (photographing objects of interest, removing the video stream from the camera, highlighting part of the image on the web page). </li><li>  Filtering - verification of images for a number of requirements: a sufficient level of illumination of objects on them, the presence of the necessary object, etc. </li><li>  Preparation of tools for markup (writing your own or optimizing the finished). </li><li>  Marking (selection of quadrangles, familiarity, areas of interest of the image). </li><li>  Assignment of a tag to each image (letter or name of an object in the image). </li></ol><br>  These operations require significant expenditure of working time, and, accordingly, a similar way of creating a training base is very expensive.  In addition, it is necessary to collect data in different conditions - lighting, phone models, cameras from which the survey is being taken, various sources of documents (printing houses), etc. <br>  All this complicates the collection of the required number of examples for learning the recognizer.  On the other hand, according to the results of training the system on such data, one can judge its effectiveness in real conditions. <br><br><h5>  <b>Creating learning examples from artificial images.</b> </h5><br>  Another approach to creating training data is their artificial generation.  You can take several templates / ‚Äúideal‚Äù examples (for example, font sets) and create the necessary number of examples for learning with the help of various distortions.  The following distortions can be used: <br><ol><li>  Geometric (affine, projective, ...). </li><li>  Brightness / color. </li><li>  Replacing the background. </li><li>  Distortions characteristic of the problem being solved: glare, noise, blur, etc. </li></ol><br>  Examples of image distortions for character recognition tasks: <br>  <u>Shifts:</u> <br><img src="https://habrastorage.org/files/e13/541/cad/e13541cad4bf45eebe3bba94fb928ef7.png"><img src="https://habrastorage.org/files/f1c/3b8/5b8/f1c3b85b8abf4dadad3e4a5f7f17e55d.png"><img src="https://habrastorage.org/files/036/a4e/f0d/036a4ef0d7c846c4bfea45bde74afb37.png"><img src="https://habrastorage.org/files/042/45a/ae6/04245aae6da3421486c38903e7a433e2.png"><img src="https://habrastorage.org/files/a97/b76/6a7/a97b766a7de941c7a93df8a7018d3370.png"><img src="https://habrastorage.org/files/fa1/6ce/44b/fa16ce44bb6d4038a1d21e26c602d471.png"><img src="https://habrastorage.org/files/c95/da7/b98/c95da7b98e7742d697b85feb650556c6.png"><img src="https://habrastorage.org/files/6e3/94b/9b0/6e394b9b04ed48e8b2ab84f68cf31f26.png"><br>  <u>Turns:</u> <br><img src="https://habrastorage.org/files/460/c30/257/460c302577d24221854476f6ae194269.png"><img src="https://habrastorage.org/files/6c6/348/0c6/6c63480c6abf4a2b8b55bdeff1bd5d4d.png"><img src="https://habrastorage.org/files/eda/71c/fdc/eda71cfdc293406c99028b5b6c15e010.png"><img src="https://habrastorage.org/files/95f/e58/5aa/95fe585aaf4a4e9dbf942bb20fd45b16.png"><img src="https://habrastorage.org/files/a1c/2d6/776/a1c2d677670945c4a8a913026047365a.png"><img src="https://habrastorage.org/files/23c/0eb/74f/23c0eb74fab84d1b9a99b7fe2f580c25.png"><img src="https://habrastorage.org/files/686/494/e72/686494e72d5b46a786b22aae6c10ef29.png"><img src="https://habrastorage.org/files/d19/de4/f5f/d19de4f5f89b4a7284f7be3b09b470be.png"><br>  <u>Additional lines on images:</u> <br><img src="https://habrastorage.org/files/9d4/aff/ec7/9d4affec7adc442fba60e89e2a316ac8.png"><img src="https://habrastorage.org/files/ae7/3b5/6b1/ae73b56b14644f898de15e9160d3f202.png"><img src="https://habrastorage.org/files/1de/014/b40/1de014b409894d4ba5496022c313e983.png"><img src="https://habrastorage.org/files/149/089/c90/149089c904ae4684ac51d7c09316cda4.png"><img src="https://habrastorage.org/files/641/a8c/0f3/641a8c0f3c2e4339ade90a8814e052ee.png"><img src="https://habrastorage.org/files/148/b7f/e9e/148b7fe9edbb4b8ab114db0db57d9d38.png"><img src="https://habrastorage.org/files/6f5/5b5/040/6f55b504042f46fa81237fc841ee2045.png"><img src="https://habrastorage.org/files/16c/bc6/753/16cbc6753745495cbdfdc5c8fd462128.png"><br>  <u>Glare:</u> <br><img src="https://habrastorage.org/files/2cc/6da/e94/2cc6dae94cea438981c61f86a42245de.png"><img src="https://habrastorage.org/files/8c0/61d/011/8c061d0111ab4d75bcd14edf1a4bdf0a.png"><img src="https://habrastorage.org/files/98b/1b5/a95/98b1b5a95ddf479ca305a088cfae4482.png"><img src="https://habrastorage.org/files/e36/5e7/9ba/e365e79ba61f4c91a7184f0854c15f64.png"><img src="https://habrastorage.org/files/c9e/53a/d64/c9e53ad64dc94b988a839cab3ef57ddd.png"><img src="https://habrastorage.org/files/3dd/9e7/037/3dd9e7037c964ce38b7ee398f3455c11.png"><img src="https://habrastorage.org/files/d21/065/63c/d2106563c9df475cab723e2d2cb3a875.png"><img src="https://habrastorage.org/files/733/d26/bb7/733d26bb732440a0a7376835e1108f08.png"><br>  <u>Defocus:</u> <br><img src="https://habrastorage.org/files/eca/93b/9a1/eca93b9a199a493ca1241b765f1ffca9.png"><img src="https://habrastorage.org/files/ba5/fcf/e4e/ba5fcfe4ee02491c9a5fe762e0f90a03.png"><img src="https://habrastorage.org/files/0c2/d68/d80/0c2d68d8076d4923b78720d64dd7c056.png"><img src="https://habrastorage.org/files/d5c/a06/192/d5ca06192ca347689f57993f40387ad3.png"><img src="https://habrastorage.org/files/4f3/bc5/97a/4f3bc597ac9843f28d576dbe302ac33c.png"><img src="https://habrastorage.org/files/208/979/543/20897954327f485e87028690070fa681.png"><img src="https://habrastorage.org/files/16a/886/1bf/16a8861bf3424ab9a617e8ce780daa74.png"><img src="https://habrastorage.org/files/757/4c0/c0a/7574c0c0a6494c19a49c6584d0bb710a.png"><br>  <u>Compression and stretching along the axes:</u> <br><img src="https://habrastorage.org/files/6e3/8d0/31d/6e38d031d096408eb6c45f8431d1793f.png"><img src="https://habrastorage.org/files/f15/dc1/a08/f15dc1a089d14ef3ad5d0b652e009cc2.png"><img src="https://habrastorage.org/files/33f/328/074/33f32807485349ee808ea1f8b996c656.png"><img src="https://habrastorage.org/files/fec/9e5/c36/fec9e5c3658342eeaf61da3292694f7b.png"><img src="https://habrastorage.org/files/ac4/181/1fa/ac41811fac9b4287b6143dc3ac6ac792.png"><img src="https://habrastorage.org/files/735/042/b8b/735042b8b5284da387cc29c2373f2f57.png"><img src="https://habrastorage.org/files/d83/9b5/3a2/d839b53a22c246c597f6bfe18f7c79e3.png"><img src="https://habrastorage.org/files/72b/d49/011/72bd490115614a74acd44422a0e389ce.png"><br><br>  You can generate distortions using image libraries [ <a href="http://effbot.org/zone/pil-index.htm">1</a> , <a href="http://opencv.org/">2</a> , <a href="http://www.imagemagick.org/script/index.php">3</a> ] or special programs that allow you to create whole artificial documents or objects. <br><br>  This approach does not require a large amount of human resources and is relatively cheap because it does not require markup and data collection ‚Äî the entire process of creating an image database is determined by the choice of the algorithm and parameters. <br>  The main disadvantage of this method is the weak relationship of the quality of the system on the generated data with the quality of work in real conditions.  In addition, the method requires large computing power to create the required number of examples.  The choice of distortions used to create a database for a specific task also constitutes a certain complexity. <br>  Below is an example of creating a fully artificial base. <br><br>  The initial set of font character images: <br><img src="https://habrastorage.org/files/ed3/fea/508/ed3fea5088c74a389db778b8b391ccec.png"><img src="https://habrastorage.org/files/cf1/9e0/d7e/cf19e0d7e8c24959bc0d3b0dd7e1f70b.png"><img src="https://habrastorage.org/files/9cf/6cf/a91/9cf6cfa912614139ac585ff22129186c.png"><img src="https://habrastorage.org/files/41a/f87/c7a/41af87c7afe3452f92ac5699eaf6015c.png"><img src="https://habrastorage.org/files/533/25c/031/53325c03152c40939f18ca041849a3c2.png"><img src="https://habrastorage.org/files/b16/6eb/02f/b166eb02fbb64a139fbea8e3adeca0b6.png"><img src="https://habrastorage.org/files/f96/52e/316/f9652e316ea441ef9369a1d69088bd21.png"><img src="https://habrastorage.org/files/9b4/262/8ff/9b42628ffb784d2c9fce534e9951c06e.png"><br><br>  Examples of backgrounds: <br><img src="https://habrastorage.org/files/a90/9d7/f5d/a909d7f5d69d4dcc8cc00bb7fbf1fb25.png"><br><br><img src="https://habrastorage.org/files/8ce/00e/6e3/8ce00e6e3792497980abe70692010e45.png"><br><br><img src="https://habrastorage.org/files/b00/c13/c5c/b00c13c5cb294e72903eaa0268690c07.png"><br><br><img src="https://habrastorage.org/files/552/c40/48c/552c4048ccee4e218ffc9606e2dbad19.png"><br><br>  Examples of images without distortion: <br><img src="https://habrastorage.org/files/68b/3b9/fc9/68b3b9fc94474a2289ff738ffa90544a.png"><img src="https://habrastorage.org/files/70c/3b3/86f/70c3b386f3904dbdb3a6633dcafa4597.png"><img src="https://habrastorage.org/files/57d/134/eb6/57d134eb6cde45a6bc3c8bf6757ddc41.png"><img src="https://habrastorage.org/files/ea9/24f/d2e/ea924fd2e0d344488dd7acc2371b4488.png"><img src="https://habrastorage.org/files/714/c0a/cf6/714c0acf67554297be9105fbb26b27a5.png"><img src="https://habrastorage.org/files/6e9/796/e3b/6e9796e3b27545c787f325afac0cff64.png"><img src="https://habrastorage.org/files/50d/8e8/fbe/50d8e8fbeec440a188177d84d1a91c42.png"><img src="https://habrastorage.org/files/162/e04/1ac/162e041acbe146ee857a5a7314337190.png"><br><br>  Adding small distortions: <br><img src="https://habrastorage.org/files/8c2/91b/7c5/8c291b7c5d1c454ca0a87f6d728db9af.png"><img src="https://habrastorage.org/files/3ad/556/7d2/3ad5567d2e0149dea8f521deba1daf99.png"><img src="https://habrastorage.org/files/fbf/f18/a7f/fbff18a7f7804d9487f8c216162a7245.png"><img src="https://habrastorage.org/files/45f/75d/600/45f75d600e7140bb832d91732d7b5c9c.png"><img src="https://habrastorage.org/files/33c/2af/d8f/33c2afd8f71744b5973cc88f492b7bb5.png"><img src="https://habrastorage.org/files/1ac/342/7b0/1ac3427b08974570a433c81833e4592b.png"><img src="https://habrastorage.org/files/fdd/3a6/0f1/fdd3a60f178a4a028a7133e253480b9c.png"><img src="https://habrastorage.org/files/f6d/783/832/f6d7838324f74e0d8a002e0f322eae99.png"><br><br><h5>  <b>Creation of artificial teaching examples generated from natural images.</b> </h5><br>  The logical continuation of the previous method is the generation of artificial examples using real data instead of templates and the original "ideal" examples.  By adding distortion, you can achieve a significant improvement in the performance of the recognition system.  In order to understand exactly which distortions should be applied, some of the real data should be used for validation.  They can be used to evaluate the most common types of errors and add images with appropriate distortions to the training base. <br><br>  This way of creating teaching examples contains the advantages of both of the above approaches: it does not require high material costs and allows you to create a large number of examples needed for the training of the recognizer. <br><br>  Difficulties can be caused by careful selection of the ‚Äúblow-up‚Äù parameters of the training sample from the initial examples.  On the one hand, the number of examples should be sufficient for the neural network to learn to recognize even noisy examples, on the other hand, it is necessary that the quality of other types of complex images does not fall <br><br><h5>  <b>Comparison of the quality of learning of a neural network using examples from natural images, completely artificial and generated using natural ones.</b> </h5><br>  Let's try to create a neural network on images of symbols MRZ.  Machine-Readable Zone (MRZ - Machine-Readable Zone) is a part of an identity document, made in accordance with international recommendations, enshrined in the <a href="http://www.icao.int/publications/pages/publication.aspx%3Fdocnum%3D9303">document Doc 9303 - Machine Readable Travel Documents of the International Civil Aviation Organization</a> .  You can read more about MRZ recognition problems in <a href="http://habrahabr.ru/company/smartengines/blog/259649/">our other article</a> . <br><br>  MRZ Example: <br><img src="https://habrastorage.org/files/a8a/f31/c8e/a8af31c8e6dc4f5fa0539238005ebde3.png"><br><br>  MRZ contains 88 characters.  We will use 2 characteristics of the quality of the system: <br><ul><li>  The percentage of characters recognized in error. </li><li>  the percentage of fully correctly recognized zones (MRZ is considered fully correctly recognized if all the characters in it are recognized correctly). </li></ul><br>  In the future, the neural network is supposed to be used on mobile devices, where computing power is limited, so the grids used will have a relatively small number of layers and weights. <br>  For the experiments, 800'000 examples of characters were collected, which were divided into 3 groups: 200'000 examples for training, 300'000 examples for validation and 300'000 examples for testing.  Such a partition is unnatural, since most of the examples are ‚Äúwasted‚Äù (validation and testing), but it makes it possible to best show the advantages and disadvantages of various methods. <br>  For a test sample, the distribution of examples of different classes is close to real and looks like this: <br>  Class name (symbol): number of examples <br>  0: 22416 1: 17602 2: 13746 3: 8115 4: 8587 5: 9383 6: 8697 7: 8082 8: 9734 9: 8847 <br>  &lt;: 110438 A: 12022 B: 1834 C: 3891 D: 2952 E: 7349 F: 3282 G: 2169 H: 3309 I: 6737 <br>  J: 934 K: 2702 L: 4989 M: 6244 N: 7897 O: 4515 P: 4944 Q: 109 R: 7717 S: 5499 T: 3730 <br>  U: 4224 V: 3117 W: 744 X: 331 Y: 1834 Z: 1246 <br><br>  When learning only in natural examples, the average symbolic error value in 25 experiments was 0.25%, i.e.  the total number of incorrectly recognized symbols was 750 images out of 300,000. For practical application, this quality is unacceptable, since the number of correctly recognized zones in this case is 80%. <br><br>  Consider the most common types of errors that a neural network makes. <br>  Examples of incorrectly recognized images: <br><img src="https://habrastorage.org/files/acc/a0b/687/acca0b687d7647b2a94263f72614655a.png"><img src="https://habrastorage.org/files/97c/41f/b34/97c41fb3445e4369954a06391ad7b70e.png"><img src="https://habrastorage.org/files/ef5/dbf/717/ef5dbf7177654bc682c7d33e8cccd1b4.png"><img src="https://habrastorage.org/files/77d/e59/a31/77de59a314984ec9b154b59058fae07e.png"><img src="https://habrastorage.org/files/c14/2e0/248/c142e024853c4e1d940a2e5528261ea9.png"><img src="https://habrastorage.org/files/632/6f3/943/6326f394353d4b5ea7b4ae2791d94868.png"><img src="https://habrastorage.org/files/5e2/ccd/8f9/5e2ccd8f9c164d3eb418348e090adcb1.png"><img src="https://habrastorage.org/files/d78/0ef/0eb/d780ef0eb5e9406484f249033f391d6f.png"><img src="https://habrastorage.org/files/4fa/e6d/79a/4fae6d79aae945f1b7b2e164b040c0c5.png"><img src="https://habrastorage.org/files/0d0/86f/ce0/0d086fce0ed44badaedba4f4b1616d64.png"><img src="https://habrastorage.org/files/ca0/c72/2ed/ca0c722ed4d648c69f64845393400eb0.png"><img src="https://habrastorage.org/files/1ef/b03/23e/1efb0323ec644d9fa932288d0cb870bd.png"><img src="https://habrastorage.org/files/eef/4b0/488/eef4b0488e8f4f939a83aaaefc063ad5.png"><img src="https://habrastorage.org/files/7e8/b21/409/7e8b2140966248bf8661cf4421c896f6.png"><img src="https://habrastorage.org/files/eb1/01c/47a/eb101c47a8de4c2e8b8ca7cc61f4ffad.png"><img src="https://habrastorage.org/files/b61/7c3/6de/b617c36deba743b4959775e293269cda.png"><img src="https://habrastorage.org/files/d7d/503/c85/d7d503c85a00450080a3ae732c0f4d8c.png"><img src="https://habrastorage.org/files/f7e/6f0/a94/f7e6f0a945cf4342a9d317b289cec88f.png"><br><br>  The following types of errors can be distinguished: <br><ul><li>  Errors on non-centered images. </li><li>  Errors on rotated images. </li><li>  Errors on images with lines. </li><li>  Errors on images with highlights. </li><li>  Errors in difficult cases. </li></ul><br>  The most common errors table: <br>  (format Original symbol, number of errors, with which symbols the network most often confuses this symbol and how many times) <br>  Original symbol: '0', number of errors: 437 <br>  'O': 419, 'U': 5, 'J': 4, '2': 2, '1': 1 <br>  Original symbol: '&lt;', number of errors: 71 <br>  '2': 29, 'K': 6, 'P': 6, '4': 4, '6': 4 <br>  Original symbol: '8', number of errors: 35 <br>  'B': 10, '6': 10, 'D': 4, 'E': 2, 'M': 2 <br>  Original symbol: 'O', number of errors: 20 <br>  '0': 19, 'Q': 1 <br>  Original symbol: '4', number of errors: 19 <br>  '6': 5, 'N': 3, '¬°': 2, 'A': 1, 'D': 1 <br>  Original symbol: '6', number of errors: 18 <br>  'G': 4, 'S': 4, 'D': 3, 'O': 2, '4': 2 <br>  Original symbol: '1', number of errors: 17 <br>  'T': 6, 'Y': 5, '7': 2, '3': 1, '6': 1 <br>  Original symbol: 'L', number of errors: 14 <br>  'I': 9, '4': 4, 'C': 1 <br>  Original symbol: 'M', number of errors: 14 <br>  'H': 7, 'P': 5, '3': 1, 'N': 1 <br>  Original symbol: 'E', number of errors: 14 <br>  'C': 5, 'I': 3, 'B': 2, 'F': 2, 'A': 1 <br><br>  We will gradually add various types of distortion corresponding to the most common types of errors in the training sample.  The number of added ‚Äúdistorted‚Äù images must be varied and selected based on the reverse response of the validation sample. <br><br>  We act according to the following scheme: <br><div style="text-align:center;"><img src="https://habrastorage.org/files/7f5/1b2/912/7f51b291260847648bc9960b55dc25c9.png"></div><br>  For example, the following was done for this task: <br><ol><li>  Adding distortion type ‚Äúshift‚Äù corresponding to the error on the ‚Äúnon-centered‚Äù image. </li><li>  Conducting a series of experiments: training of several neural networks. </li><li>  Quality assessment on the test sample.  MRZ recognition quality increased by 9%. </li><li>  Analysis of the most frequent recognition errors on the validation sample. </li><li>  Adding images with additional lines to the training base. </li><li>  Again conducting a series of experiments. </li><li>  Testing.  The recognition quality of the MRZ on the test sample increased by 3.5%. </li></ol><br>  Such ‚Äúiterations‚Äù can be repeated many times - until the required quality is achieved or until quality ceases to grow. <br>  In this way, the recognition quality was obtained in 94.5% of correctly recognized zones.  Using post-processing (Markov models, finite automata, N-gram and vocabulary methods, etc.), one can get a further increase in quality. <br><br>  When using training only on artificial data in the considered task, it was possible to achieve only the quality in 81.72% of correctly recognized zones, while the main problem is the difficulty of selecting the distortion parameters. <br><table><tbody><tr><th>  Data type for training </th><th>  Percentage of correctly recognized MRZ </th><th>  Character error </th></tr><tr><td>  Natural Images </td><td>  80.78% </td><td>  0.253% </td></tr><tr><td>  Natural images + images with shifts </td><td>  89.68% </td><td>  0.13% </td></tr><tr><td>  + images with additional lines </td><td>  93.19% </td><td>  0.1% </td></tr><tr><td>  + rotated images </td><td>  95.50% </td><td>  0.055% </td></tr><tr><td>  Artificial Images </td><td>  78.53% </td><td>  0.29% </td></tr></tbody></table><br><h5>  <b>Conclusion</b> </h5><br>  In conclusion, I would like to note that in each specific case it is necessary to choose your own algorithm for obtaining training data.  If the source data is completely absent, it is necessary to generate a sample artificially.  If real data is easy to get, you can use a training sample created only from them.  And if the real data is not very much, or there are rare errors, the best way is to blow up a set of natural images.  In our experience, this latter case occurs most frequently. </div><p>Source: <a href="https://habr.com/ru/post/264677/">https://habr.com/ru/post/264677/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../264667/index.html">Our million, year and first eight years</a></li>
<li><a href="../264669/index.html">Docker in the bank. Video from the lecture by Alexander Tarasov from Alfa-Bank</a></li>
<li><a href="../264671/index.html">Preparing graphics for iOS applications</a></li>
<li><a href="../264673/index.html">What does the IaaS market statistics say?</a></li>
<li><a href="../264675/index.html">Practical tips for improving HTML and JavaScript performance</a></li>
<li><a href="../264679/index.html">HP ProLiant DL360: Gen5 or Gen6? Server for 1C for 10-15 people</a></li>
<li><a href="../264681/index.html">Working with forms in Webix UI</a></li>
<li><a href="../264683/index.html">Manage dependencies in iOS applications correctly: Typhoon Tips & Tricks</a></li>
<li><a href="../264685/index.html">Fast mail server on t2.micro with EC2 running CentOS 7</a></li>
<li><a href="../264687/index.html">How to increase the speed of 1C 100 times by direct access to MSSQL</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>