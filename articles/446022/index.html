<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How I did NOT scan the Belarusian Internet</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Foreword 
 This article is not quite similar to those that were published earlier about scanning the Internet of certain countries, because I did not ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How I did NOT scan the Belarusian Internet</h1><div class="post__text post__text-html js-mediator-article"><h2>  Foreword </h2><br>  This article is not quite similar to those that were published earlier about scanning the Internet of certain countries, because I did not pursue the goals of mass scanning of a specific segment of the Internet to open ports and the presence of the most popular vulnerabilities due to the fact that this is against the law. <br><br>  I rather had a slightly different interest - try to identify all relevant sites in the BY domain zone using different methods, determine the stack of technologies used, through services like Shodan, VirusTotal, etc. Perform passive reconnaissance on IP and open ports and collect some other useful information in the appendage. information for the formation of some general statistics on the level of security regarding sites and users. <br><a name="habracut"></a><br><h2>  Introductory and our tools </h2><br>  The plan at the very beginning was simple - contact the local registrar for a list of currently registered domains, then check everything for availability and start exploring functioning sites.  In reality, everything turned out to be much more complicated - naturally, no one wanted to provide this kind of information, except for the official statistics page of current registered domain names in the BY zone (about 130 thousand domains).  If there is no such information, then you will have to collect it yourself. <br><br><img src="https://habrastorage.org/webt/yh/pz/4w/yhpz4wvpvleq-f25o5fp5wwt_2u.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      According to the toolkit, everything is actually quite simple - we are looking in the direction of open source, something can always be added, finished some minimal crutches.  From the most popular, the following tools were used: <br><br><ul><li>  <a href="https://github.com/urbanadventurer/WhatWeb">Whatweb</a> </li><li>  curl </li><li>  dig </li><li>  <a href="https://github.com/EnableSecurity/wafw00f">wafw00f</a> </li><li>  API third-party services ( <a href="https://www.virustotal.com/ru/documentation/public-api/">VirusTotal</a> , <a href="https://developers.google.com/safe-browsing/v4/">Google SafeBrwosing</a> , <a href="https://developer.shodan.io/">Shodan</a> , <a href="https://github.com/vulnersCom/api">Vulners</a> ) </li></ul><br><br><h2>  Beginning of activities: Starting point </h2><br>  As an introductory one, as I have already said, domain names ideally suited, but where can I get them?  We need to start with something simpler, in this case, we can use IP addresses, but then again - with reverse loops it is not always possible to catch all the domains, and when collecting hostnames - not always the correct domain.  At this stage, I began to think about possible scenarios for collecting this kind of information, again - the fact that our budget is $ 5 for renting a VPS was still taken into account, the rest should be free of charge. <br><br><h3>  Our potential sources of information: </h3><br><ul><li>  IP addresses ( <a href="https://lite.ip2location.com/belarus-ip-address-ranges">ip2location</a> site) </li><li>  Search domains by the second part of the email address (but where to get them? We will understand below) </li><li>  Some registrars / hosting providers may provide us such information in the form of sub-domains. </li><li>  Subdomains and their subsequent reverse (Sublist3r and Aquatone can help here) </li><li>  Brutfors and manual input (long, dreary, but possible, although I did not use this option) </li></ul><br>  I'll run a little ahead and say that with this approach I managed to collect about 50 thousand unique domains and sites, respectively (I didn't manage to process everything).  If I continued to actively gather information, then in less than a month of work my conveyor would probably have mastered the entire base, or most of it. <br><br><h3>  Getting down to business </h3><br>  In previous articles, information about IP addresses was taken from the IP2LOCATION site, I didn‚Äôt stumble upon these articles (for all the actions took place much earlier), but I also came to this resource.  However, in my case, the approach was different - I decided not to take the base to my place and not to extract information from CSV, but decided to monitor the changes directly on the site, on an ongoing basis and as the main base from which all subsequent scripts will take goals - made a table with IP addresses in different formats: CIDR, ‚Äúfrom‚Äù and ‚Äúto‚Äù list, country mark (just in case), AS Number, AS Description. <br><br><img src="https://habrastorage.org/webt/mx/s4/wj/mxs4wjpxueytkcd15srf7alswf8.png"><br><br>  The format is not the most optimal, but for the demo and one-time action I was quite satisfied, and in order not to seek support information like ASN on a permanent basis, I decided to log it in addition.  To get this information, I turned to the <a href="https://api.iptoasn.com/">IpToASN</a> service, they have a convenient API (with restrictions), which in fact you just need to integrate to yourself. <br><br><div class="spoiler">  <b class="spoiler_title">Code for parsing ip</b> <div class="spoiler_text"><pre><code class="php hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">ipList</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ $ch = curl_init(); curl_setopt($ch, CURLOPT_URL, <span class="hljs-string"><span class="hljs-string">"https://lite.ip2location.com/belarus-ip-address-ranges"</span></span>); curl_setopt($ch, CURLOPT_HEADER, <span class="hljs-number"><span class="hljs-number">0</span></span>); curl_setopt($ch, CURLOPT_USERAGENT,<span class="hljs-string"><span class="hljs-string">'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.13) Gecko/20080311 Firefox/2.0.0.13'</span></span>); curl_setopt($ch, CURLOPT_RETURNTRANSFER, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>); curl_setopt($ch, CURLOPT_SSL_VERIFYPEER, <span class="hljs-keyword"><span class="hljs-keyword">false</span></span>); curl_setopt($ch, CURLOPT_SSL_VERIFYHOST, <span class="hljs-keyword"><span class="hljs-keyword">false</span></span>); $ipList = curl_exec($ch); curl_close ($ch); preg_match_all(<span class="hljs-string"><span class="hljs-string">"/(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\&lt;\/td\&gt;\s+\&lt;td\&gt;\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})/"</span></span>, $ipList, $matches); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> $matches[<span class="hljs-number"><span class="hljs-number">0</span></span>]; } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">iprange2cidr</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">($ipStart, $ipEnd)</span></span></span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (is_string($ipStart) || is_string($ipEnd)){ $start = ip2long($ipStart); $end = ip2long($ipEnd); } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>{ $start = $ipStart; $end = $ipEnd; } $result = <span class="hljs-keyword"><span class="hljs-keyword">array</span></span>(); <span class="hljs-keyword"><span class="hljs-keyword">while</span></span>($end &gt;= $start){ $maxSize = <span class="hljs-number"><span class="hljs-number">32</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> ($maxSize &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>){ $mask = hexdec(iMask($maxSize - <span class="hljs-number"><span class="hljs-number">1</span></span>)); $maskBase = $start &amp; $mask; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>($maskBase != $start) <span class="hljs-keyword"><span class="hljs-keyword">break</span></span>; $maxSize--; } $x = log($end - $start + <span class="hljs-number"><span class="hljs-number">1</span></span>)/log(<span class="hljs-number"><span class="hljs-number">2</span></span>); $maxDiff = floor(<span class="hljs-number"><span class="hljs-number">32</span></span> - floor($x)); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>($maxSize &lt; $maxDiff){ $maxSize = $maxDiff; } $ip = long2ip($start); array_push($result, <span class="hljs-string"><span class="hljs-string">"$ip/$maxSize"</span></span>); $start += pow(<span class="hljs-number"><span class="hljs-number">2</span></span>, (<span class="hljs-number"><span class="hljs-number">32</span></span>-$maxSize)); } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> $result; } $getIpList = ipList(); <span class="hljs-keyword"><span class="hljs-keyword">foreach</span></span>($getIpList <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> $item) { $cidr = iprange2cidr($ip[<span class="hljs-number"><span class="hljs-number">0</span></span>], $ip[<span class="hljs-number"><span class="hljs-number">1</span></span>]); }</code> </pre> <br></div></div><br>  After we dealt with IP, we need to get rid of our entire database through reverse lookup services, alas, without any restrictions - this is impossible, except for money. <br><br>  From the services that are great for this and are easy to use, I want to point out as many as two: <br><br><ol><li>  VirusTotal - limit but the frequency of accessing from a single API key </li><li>  Hackertarget.com (their API) - limit on the number of hits from a single IP </li></ol><br>  Bypassing the limits we got the following options: <br><br><ul><li>  In the first case, one of the scenarios is to withstand timeouts of 15 seconds, for a total of 4 hits per minute, which can greatly affect our speed and in this situation, using 2-3 such keys will be useful, and I would also recommend using to proxy and change user-agent. </li><li>  In the second case, I wrote a script for automatic parsing of the proxy database based on publicly available information, their validation and subsequent use (but later withdrew from this option because, in fact, VirusTotal was enough) </li></ul><br>  Go ahead and smoothly go to email addresses.  They can also be a source of useful information, but where are they to be collected?  It did not take long to find a solution, since  in our segment of personal sites, users keep few, and in the bulk of this organization - then we will suit specialized sites like online store catalogs, forums, conditional marketplaces. <br><br>  For example, a cursory inspection of one of these sites showed that so many users add their email directly to their public profile and accordingly - this case can be carefully parsed for later use. <br><br><div class="spoiler">  <b class="spoiler_title">One of the parsers</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/env python3 import sys, threading, time, os, urllib, re, requests, pymysql from html.parser import HTMLParser from urllib import request from bs4 import BeautifulSoup # HEADERS CONFIG headers = { 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11.9; rv:42.0) Gecko/20200202 Firefox/41.0' } file = open('dat.html', 'w') def parseMails(uid): page = 'https://profile.onliner.by/user/'+str(uid)+'' cookie = {'onl_session': 'YOUR_SESSION_COOOKIE_HERE'} r = requests.get(page, headers = headers, cookies = cookie) data = BeautifulSoup(r.text) userinfo = data.find_all('dl', {'class': 'uprofile-info'}) find_email = [] for item in userinfo: find_email += str(item.find('a')) get_mail = ''.join(find_email) detect_email = re.compile(".+?&gt;(.+@.+?)&lt;/a&gt;").search(get_mail) file.write("&lt;li&gt;('"+detect_email.group(1)+"'),&lt;/li&gt;") for uid in range(1, 10000): t = threading.Thread(target=parseMails, args=(uid,)) t.start() time.sleep(0.3)</span></span></code> </pre><br></div></div><br>  I will not go into details of the parsing of each of the sites, it is more convenient to guess the user ID somewhere by searching, somewhere easier to parse the site map, get information from the company pages from it, and then collect addresses from them.  After collecting addresses, it remains for us to perform several simple operations at once sorted by domain zone, retaining tails for ourselves and driving away to eliminate duplicates on the existing base. <br><br>  At this stage, I believe that with the formation of a skoup, we can finish and proceed to exploration.  Exploration, as we already know, can be of two types - active and passive, in our case - the most relevant will be the passive approach.  But then again, simply accessing a site on port 80 or 443 without a malicious load and exploiting vulnerabilities is quite a legitimate action.  Our interest is server responses to a single request, in some cases there may be two requests (redirection from http to https), in more rare cases, as many as three (when using www). <br><br><h2>  Intelligence service </h2><br>  Using such information as a domain, we can collect the following data: <br><br><ul><li>  DNS records (NS, MX, TXT) </li><li>  Answer Headers </li><li>  Determine the technology stack used </li><li>  Understand what protocol the site is running </li><li>  Try to identify open ports (based on Shodan / Censys) without direct scanning </li><li>  Try to identify vulnerabilities based on the correlation of information from Shodan / Censys with the Vulners base </li><li>  Is it in the malicious Google Safe Browsing database? </li><li>  Collect email addresses by domain, as well as match already found and check on Have I Been Pwned, in addition - linking to social networks </li><li>  A domain is in some cases not only the face of the company, but also the product of its activity, email addresses for registering with services, etc., respectively - you can search for information that is associated with them on resources like GitHub, Pastebin, Google Dorks (Google CSE ) </li></ul><br>  You can always go ahead and use as a variant masscan or nmap, zmap, setting them up in advance through Tor with launching at random time or even from several instances, but we have different goals and the name implies that I didn‚Äôt do direct scans. <br><br>  We collect DNS records, check the possibility of amplifying queries and configuration errors like AXFR: <br><br><div class="spoiler">  <b class="spoiler_title">Example of collecting NS server records</b> <div class="spoiler_text"><pre> <code class="bash hljs">dig ns +short <span class="hljs-variable"><span class="hljs-variable">$domain</span></span> | sed <span class="hljs-string"><span class="hljs-string">'s/\.$//g'</span></span> | awk <span class="hljs-string"><span class="hljs-string">'{print $1}'</span></span></code> </pre> <br></div></div><br>  Example of collecting MX records (see NS, just replace 'ns' with 'mx' <br><br><div class="spoiler">  <b class="spoiler_title">Check on AXFR (there are many solutions here, here is another kostylny, but not a secret, used to view the output)</b> <div class="spoiler_text"><pre> <code class="php hljs"> $digNs = trim(shell_exec(<span class="hljs-string"><span class="hljs-string">"dig ns +short $domain | sed 's/\.$//g' | awk '{print $1}'"</span></span>)); $ns = explode(<span class="hljs-string"><span class="hljs-string">"\n"</span></span>, $digNs); <span class="hljs-keyword"><span class="hljs-keyword">foreach</span></span>($ns <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> $target) { $axfr = trim(shell_exec(<span class="hljs-string"><span class="hljs-string">"dig -t axfr $domain @$target | awk '{print $1}' | sed 's/\.$//g'"</span></span>)); $axfr = preg_replace(<span class="hljs-string"><span class="hljs-string">"/\;/"</span></span>, <span class="hljs-string"><span class="hljs-string">""</span></span>, $axfr); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(!<span class="hljs-keyword"><span class="hljs-keyword">empty</span></span>(trim($axfr))) { $axfr = preg_replace(<span class="hljs-string"><span class="hljs-string">"/\;/"</span></span>, <span class="hljs-string"><span class="hljs-string">""</span></span>, $axfr); $res = json_encode(explode(<span class="hljs-string"><span class="hljs-string">"\n"</span></span>, trim($axfr)));</code> </pre> <br></div></div><br><div class="spoiler">  <b class="spoiler_title">DNS Amplification Check</b> <div class="spoiler_text"><pre> <code class="bash hljs">dig +short test.openresolver.com TXT @<span class="hljs-variable"><span class="hljs-variable">$dns</span></span></code> </pre> <br>  In my case, the NS servers were taken from the database, by the end of the variable, there you can substitute just any server as a matter of fact.  Regarding the correctness of the results of this service - I can not be sure that everything there works 100% straight and the results are always valid, but I hope that most of the results are real. <br></div></div><br>  If for any purpose we need to save the full-fledged final URL to the site, for this I used cURL: <br><br><pre> <code class="bash hljs">curl -I -L <span class="hljs-variable"><span class="hljs-variable">$target</span></span> | awk <span class="hljs-string"><span class="hljs-string">'/Location/{print $2}'</span></span></code> </pre> <br>  He himself goes over the whole redirect and displays the final one, i.e.  current site URL.  In my case, this was extremely useful when later using a tool like WhatWeb. <br><br>  Why do we use it?  In order to determine the OS used, the web server, the CMS site, some headers, additional modules like JS / HTML libraries / frameworks, as well as the site title on which you can later try to filter to the same by activity. <br><br>  In this case, a very convenient option would be to export the results of the work in XML format for subsequent parsing and import into the database if there is a goal to process everything later. <br><br><pre> <code class="bash hljs">whatweb --no-errors https://www.mywebsite.com --<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>-xml=results.xml</code> </pre> <br>  For myself, I did the output on the basis of JSON and it was already added to the database. <br><br>  Speaking of headers, you can do almost the same thing with ordinary cURLs, performing a query like this: <br><br><pre> <code class="bash hljs">curl -I https://www.mywebsite.com</code> </pre> <br>  In the headers to independently catch information about the CMS and web servers using regular expressions for example. <br><br>  In addition to the stack from the useful, we can also highlight the ability to collect information about open ports using Shodan and then using the data ‚Äî perform a check on the Vulners database using their API (links to services are given in the header).  Of course, with accuracy in this situation there may be problems, yet this is not a direct scan with manual validation, but a banal ‚Äújuggling‚Äù with data from third-party sources, but it's better than that at all. <br><br><div class="spoiler">  <b class="spoiler_title">PHP function for Shodan</b> <div class="spoiler_text"><pre> <code class="php hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">shodanHost</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">($host)</span></span></span><span class="hljs-function"> </span></span>{ $ch = curl_init(); curl_setopt($ch, CURLOPT_URL, <span class="hljs-string"><span class="hljs-string">"https://api.shodan.io/shodan/host/"</span></span>.$host.<span class="hljs-string"><span class="hljs-string">"?key=&lt;YOUR_API_KEY&gt;"</span></span>); curl_setopt($ch, CURLOPT_HEADER, <span class="hljs-number"><span class="hljs-number">0</span></span>); curl_setopt($ch, CURLOPT_USERAGENT,<span class="hljs-string"><span class="hljs-string">'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.13) Gecko/20080311 Firefox/2.0.0.13'</span></span>); curl_setopt($ch, CURLOPT_RETURNTRANSFER, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>); curl_setopt($ch, CURLOPT_SSL_VERIFYPEER, <span class="hljs-keyword"><span class="hljs-keyword">false</span></span>); curl_setopt($ch, CURLOPT_SSL_VERIFYHOST, <span class="hljs-keyword"><span class="hljs-keyword">false</span></span>); $shodanResponse = curl_exec($ch); curl_close ($ch); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> json_decode($shodanResponse); }</code> </pre> <br></div></div><br><div class="spoiler">  <b class="spoiler_title">An example of such a comparative analysis # 1</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/ha/fv/7q/hafv7qlzupcsueqn_qwe_0qyo7y.png"><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Example # 2</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/9x/0c/jx/9x0cjxkltuscladnh9rd4nshsga.png"><br></div></div><br>  Yes, since I started talking over the API, Vulners has limitations and the most optimal solution would be to use their Python script, everything will work fine there without twisting and spinning, in the case of PHP it ran into some minor difficulties (again - add. timeouts saved the situation). <br><br>  One of the last tests - we will study the information on the firewall used with the help of such a script as ‚Äúwafw00f‚Äù.  When testing this wonderful tula I noticed one interesting thing, it was not always the first time to determine the type of firewall used. <br><br>  To see what types of firewalls can potentially determine wafw00f, you can enter the following command: <br><br><pre> <code class="bash hljs">wafw00f -l</code> </pre> <br>  To determine the type of firewall - wafw00f analyzes the server response headers after sending a standard request to the site, if this attempt is not enough, it forms an additional simple test request and if this is not enough again, the third method operates with data after the first two attempts . <br><br>  Because  For statistics, we don‚Äôt need the answer as a matter of fact, we cut off all unnecessary with a regular expression and leave only the name firewall: <br><br><pre> <code class="php hljs">/is\sbehind\sa\s(.+?)\n/</code> </pre> <br>  Well, as I wrote earlier, in addition to information about the domain and website, information about email addresses and social networks was also updated in a passive mode: <br><br><div class="spoiler">  <b class="spoiler_title">Statistics for email defined based on domain</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/f-/lq/fj/f-lqfjqylimkrb3l8ham2yxo8f4.png"><br></div></div><br><div class="spoiler">  <b class="spoiler_title">An example of determining the binding of social networks to email address</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/an/pw/dq/anpwdqniobeopkdhnyq5pd5ejgk.png"><br></div></div><br>  The easiest was to deal with the validation of addresses on Twitter (2 ways), with Facebook (1 way) in this regard it turned out to be a little more difficult because of a slightly more complicated system of generating a real user session. <br><br><h2>  We turn to the dry statistics </h2><br><h4>  DNS statistics </h4><br><img src="https://habrastorage.org/webt/uk/md/m1/ukmdm1eoizu6sjvxaf7nld8kdgi.png"><br><br>  <b>Provider - how many sites</b> <br>  ns1.tutby.com: 10899 <br>  ns2.tutby.com: 10899 <br>  ns1.neolocation.com: 4877 <br>  ns2.neolocation.com: 4873 <br>  ns3.neolocation.com: 4572 <br>  ns1.activeby.net: 4231 <br>  ns2.activeby.net: 4229 <br>  u1.hoster.by: 3382 <br>  u2.hoster.by: 3378 <br><br>  Unique DNS detected: 2462 <br>  Unique MX (mail) servers: 9175 (except for popular services, there are a sufficient number of administrators who use their own mail services) <br>  Affected DNS Zone Transfer: 1011 <br>  Subject to DNS Amplification: 531 <br>  Few CloudFlare fans: 375 (based on the NS records used) <br><br><h4>  CMS statistics </h4><br><img src="https://habrastorage.org/webt/oz/zp/0k/ozzp0kkl6kof-gomjiesmgoju_y.png"><br><br>  <b>CMS - Quantity</b> <br>  WordPress: 5118 <br>  Joomla: 2722 <br>  Bitrix: 1757 <br>  Drupal: 898 <br>  OpenCart: 235 <br>  DataLife: 133 <br>  Magento: 32 <br><br><ul><li>  Potentially vulnerable WordPress installations: 2977 </li><li>  Potentially vulnerable Joomla installations: 212 </li><li>  Using the Google SafeBrowsing service, it turned out to identify potentially dangerous or infected sites: about 10,000 (at different times, someone fixed, someone apparently broke, the statistics are not entirely objective) </li><li>  About HTTP and HTTPS - less than half of the sites of the volume found use the latter, but taking into account that my database is not complete, but only 40% of the total, it is quite possible that most of the sites from the second half can and communicate via HTTPS . </li></ul><br><h4>  Firewall statistics: </h4><br><img src="https://habrastorage.org/webt/1o/pf/qc/1opfqck9bluf4aa2xvjt2aivaks.png"><br><br>  <b>Firewall - Number</b> <br>  ModSecurity: 4354 <br>  IBM Web App Security: 126 <br>  Better WP Security: 110 <br>  CloudFlare: 104 <br>  Imperva SecureSphere: 45 <br>  Juniper WebApp Secure: 45 <br><br><h4>  Web server statistics </h4><br><img src="https://habrastorage.org/webt/k1/ru/ut/k1ruutqumvvmoism9bafmdp8oko.png"><br><br>  <b>Web Server - Number</b> <br>  Nginx: 31752 <br>  Apache: 4042 <br>  IIS: 959 <br><br>  Outdated and potentially vulnerable Nginx installations: 20966 <br>  Outdated and potentially vulnerable Apache installations: 995 <br><br>  Despite the fact that the leader in domains and hosting as a whole, we are hoster.by, we can make an Open contact, but it‚Äôs true in terms of the number of sites on the same IP: <br><br><img src="https://habrastorage.org/webt/mq/gu/wj/mqguwjm8_1ggvfo_o5vam20tjto.png"><br><br>  <b>IP Sites</b> <br>  93.84.119.243: 556 <br>  93.125.99.83: 399 <br>  193.232.92.25: 386 <br><br><img src="https://habrastorage.org/webt/h0/us/nk/h0usnka_tsv0yqqa5jb8vd5wy_k.png"><br><br>  According to email, I decided not to pull the detailed statistics at all, not to sort by domain zone, but rather the interest was to look at the location of users for specific vendors: <br><br><ul><li>  On the service TUT.BY: 38282 </li><li>  On Yandex service (by | ru): 28127 </li><li>  On Gmail service: 33452 </li><li>  Linked to Facebook: 866 </li><li>  Linked to Twitter: 652 </li><li>  Appeared in leaks according to HIBP information: 7844 </li><li>  Passive intelligence helped to identify more than 13 thousand email addresses </li></ul><br>  As you can see, the overall picture is quite positive, especially pleased with the active use of nginx by hosting providers.  Perhaps this is more due to the popular type of hosting among popular users. <br><br>  From the fact that I didn‚Äôt like it very much - there are a sufficient number of medium-sized hosting providers who had errors like AXFR, outdated versions of SSH and Apache were used, and some other small problems.  Here, of course, more light on the situation could shed a survey with the active phase, but at the moment, due to our legislation, it seems to me that it is impossible, and it‚Äôs not particularly desirable to register for such cases in the pest ranks. <br><br>  The picture on the email as a whole is quite rosy, if you can call it that.  Oh yeah, where the TUT.BY provider is indicated - it meant the use of a domain, since  This service is based on Yandex. <br><br><h2>  Conclusion </h2><br>  As a conclusion, I can say one thing - even with the available results, you can quickly understand that there is a large amount of work for professionals who are engaged in cleaning sites from viruses, setting up WAF and configuring / completing different CMS. <br><br>  Well, seriously, as in the previous two articles, we see that problems exist at absolutely different levels in absolutely all segments of the Internet and countries, and some of them even appear when studying the issue remotely, without using offensive techniques, t. e.  operating with publicly available information for the collection of which special skills and is not required. </div><p>Source: <a href="https://habr.com/ru/post/446022/">https://habr.com/ru/post/446022/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../445988/index.html">Own temporary mail: telegram bot</a></li>
<li><a href="../445992/index.html">Nauchpop on minimum salary: optical illusions</a></li>
<li><a href="../445998/index.html">How to make friends Progress OpenEdge and Oracle DBMS</a></li>
<li><a href="../446000/index.html">What is wrong with Yandex.Music? UX / UI parsing</a></li>
<li><a href="../446006/index.html">Intel - we sound in a new way</a></li>
<li><a href="../446024/index.html">Installing and configuring the Ripple node</a></li>
<li><a href="../446026/index.html">Why SvelteJS is probably the best framework for new web developers.</a></li>
<li><a href="../446028/index.html">Building blocks distributed applications. Zero approximation</a></li>
<li><a href="../446030/index.html">Startups in the field of anti-aging biotechnology, which will be relevant in 2019</a></li>
<li><a href="../446036/index.html">Oracle Application Express. Applications and Pages</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>