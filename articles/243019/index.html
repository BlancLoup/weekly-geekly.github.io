<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>NDFS - Nutanix Distributed File System, the "foundation" of Nutanix</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nutanix is ‚Äã‚Äãa ‚Äúconverged platform 2.0‚Äù, if under ‚Äúconverged platform 1.0‚Äù we consider the initial experiments on the mechanical connection of storage...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>NDFS - Nutanix Distributed File System, the "foundation" of Nutanix</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/553/49a/07e/55349a07e664549ca27ba222752bfd2a.jpg" alt="image"><br><br>  Nutanix is ‚Äã‚Äãa ‚Äúconverged platform 2.0‚Äù, if under ‚Äúconverged platform 1.0‚Äù we consider the initial experiments on the mechanical connection of storages and servers into a single product and some conditionally common architecture undertaken in <a href="http://habrahabr.ru/company/muk/blog/165475/">VCE (EMC + Cisco) Vblock</a> , <a href="http://habrahabr.ru/company/netapp/blog/181744/">NetApp FlexPod</a> or <a href="http://habrahabr.ru/company/advanserv/blog/205332/">Dell VRTX</a> .  Unlike those listed, Nutanix is ‚Äã‚Äãno longer just wired in a 19 "rack together server, network and storage, integration is much deeper and more interesting. <br><br>  But we will start with the basics, with what lies in the foundation of Nutanix as an architecture and solution, with Nutanix Ditributed File System, NDFS. <br><a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      As the name implies, <b>NDFS</b> is a distributed, clustered file system. <br><br>  One of the most important problems for the operation of a cluster distributed file system, which the developer has to solve, is the problem of working with so-called metadata. <br>  What is ‚Äúmetadata‚Äù, and how do they differ from the ‚Äúdata‚Äù itself? <br>  Any data that we store on the storage system always has ancillary data associated with it.  This is, for example, the file name, the path to this file, that is, where it is located in the storage structure, its attributes, size, creation time, modification time, access time, some bit attributes, such as ‚Äúfor read, as well as access rights, from the simplest, then deployed ACL.  These data are not our own stored data, but they are associated with it, and provide access to the data.  These additional data are called ‚Äúmetadata‚Äù.  As a rule, access to data begins with access to the metadata stored in the file system, and already with their help the OS can begin to work with the data. <br>  The problem begins when there is more than one who wants to work with the data.  With one - everything is simple.  I counted the path to the file, read the necessary piece, changed it, wrote it down, if necessary - corrected the metadata.  But what to do, when at the time, while one process, on one cluster node, wrote its changes to the file, this file was read and another process started to make its changes from another node? <br><br><blockquote>  One of the already classic mistakes of beginning system administrators is to mount an iSCSI from the storage system to two or more servers of its LUN (physically it is quite possible), format it in some NTFS, and then try to read and write to it from different servers.  Who has already passed this - knows what happens next.  Who else did not come across it - I tell.  Everything goes well, while only one server writes to such a ‚Äúdisk‚Äù.  As soon as the second starts recording, the file system immediately (more or less fatally) crumbles.  Why?  Because NTFS is not a clustered file system, and knows nothing about the situation when it can simultaneously write two different servers to the same file system. <br></blockquote><br><br>  The file system should monitor and ‚Äúdispatch‚Äù access to data from different nodes of the cluster.  The easiest way to do this is to assign a single node, the ‚Äúold-age file system,‚Äù so that all metadata changes are made through it, and it operated with them exclusively, and already it would respond to requests for metadata changes, blocking some, and allowing others.  So works, for example, <a href="https://ru.wikipedia.org/wiki/Lustre_(%25D1%2581%25D0%25B5%25D1%2582%25D0%25B5%25D0%25B2%25D0%25B0%25D1%258F_%25D1%2584%25D0%25B0%25D0%25B9%25D0%25BB%25D0%25BE%25D0%25B2%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B8%25D1%2581%25D1%2582%25D0%25B5%25D0%25BC%25D0%25B0)">Luster</a> or <a href="https://ru.wikipedia.org/wiki/Network_File_System">pNFS</a> .  Unfortunately, this means that operations on metadata become, sooner or later, a bottleneck for scaling.  When all operations with metadata pass through exactly one server, then one day it becomes a bottleneck.  Not immediately, not always, but once and for all. <br><br>  For example, those who tried to install multiple (hundreds) virtual machines on one LUN in a VMFS partition (and VMFS is a classic cluster file system with locks using SCSI Locks), they know that with a large number of metadata operations, LUN With VMFS, it sometimes sometimes ‚Äúslows down‚Äù unpleasantly.  A ‚Äúmetadata operation‚Äù is, for example, rebooting a VM, resizing a VMDK, creating, deleting and expanding a VMFS datastore on a LUN, creating templates or deploying a VM from a template, all of which cause LUN blocking and a brief I / O suspension, for all vm on it.  Partially this problem is being fought with Atomic Test and Set locking, shifted to the storage system, if it supports VAAI, but this, all the same, often solves the problem only partially. <br><br>  Therefore, the question of creating a good, truly widely scalable cluster system is, often, a question of creating a highly scalable metadata management scheme. <br>  When creating NDFS, NoSQL <a href="https://ru.wikipedia.org/wiki/Apache_Cassandra">Apache Cassandra</a> database was used to store the file system metadata in the cluster. It was originally developed inside Facebook and then sent to OpenSource.  It is not the first time it is used in this quality, because it scales well, but almost always, along with the problem of scaling, the problem of database consistency across the cluster scale also arises.  Therefore, the developers of Nutanix were faced with the task of ensuring not just distribution (for example, Cassandra works distantly in a Faculty, on clusters of thousands of nodes), but also in consistency.  For Facebook, it‚Äôs not too big a problem that your photos in the photoset will not update instantly on all nodes of the cluster, but for Nutanix the consistency of the storage metadata is critical.  Therefore, so to speak, vanilla-Cassandra has been substantially reworked to provide the necessary consistency of storage in the cluster, which was lacking in the initial implementation of the base. <br><br>  At the same time, due to the distribution of Cassandra in the system, there is no ‚Äúmain node‚Äù, the failure of which can affect the performance of the cluster as a whole.  The metadata base is distributed across the cluster and there is no ‚Äúmain node‚Äù, ‚Äúfile system head‚Äù, which solely controls access to metadata, Nutanix does not. <br><br>  How does the work and access to the disks in the Nutanix cluster occur? <br>  At the ‚Äúlarge-block‚Äù level, this work looks like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9b6/bbe/598/9b6bbe598f3f75b41bae16ee9e423e83.png" alt="image"><br><br>  Unexpected to many is that Nutanix does not use RAID technology to ensure high availability of data on disks.  We are so accustomed to RAID on disks in enterprise systems that the statement about ‚Äúno RAID‚Äù often causes confusion.  ‚ÄúBut what about the necessary reliability of disk storage?‚Äù That's right, there is no RAID.  But, nevertheless, data redundancy is ensured, and it is provided in a manner that we have called RAIN - Redundant Array of Independent Nodes.  The redundancy of the stored data is ensured by the fact that, at the logical level, each recordable data block is recorded not only on the disks of the cluster node to which it is intended, but also simultaneously and synchronously on the disks of another (or two, as an option) another node . <br><br>  What does the failure of the RAID?  First of all, it is faster and more flexible recovery in the event of a disk loss, and, consequently, reliability is higher, because for the duration of recovery in a classic RAID, its reliability is reduced, and, besides, the performance of disk operations also decreases, since the RAID group is busy with its own internal processes of reading and writing blocks for recovery. <br>  Secondly, it is much more flexible data placement.  The file system knows where and what data blocks are written, and can recover exactly what is needed at the moment, or write as it is optimal for data, as it completely controls the entire process of writing data to physical disks, in the case of a RAID controller logically separated from the level of OS operations. <br><br>  How is access to the disks organized? <br>  First, you remember that Nutanix is ‚Äã‚Äãa platform for the hypervisor.  All tasks within the Nutanix server revolve over baremetal hypervisor, for example ESXi, MS Hyper-V or Linux KVM. <br><br>  Among virtual machines there is one - special, it is called CVM, or Controller VM.  This is the so-called ‚Äúvirtual appliance‚Äù, within which the entire kitchen of forming and supporting the Nutanix file system works.  Physically, CVM is a virtual machine under Centos Linux, with many of our services, for example, Apache Cassandra, NoSQL-storage of file system metadata, I have already mentioned above, and besides this there is a diverse zoo of processes that provides everything that Nutanix is able.  In general, if you really sharpen, then Nutanix - this is the very CVM it is, this is its heart and brain, and the main intellectual property. <br>  But inside the hypervisor, it's just one big virtual machine with a lot of special processes in it. <br><br>  This virtual machine, as seen in the diagram, passes through the I / O traffic of the virtual machines to their virtual disks.  Physical disks are found for the OS of virtual machines not only ‚Äúbehind the hypervisor‚Äù, but also behind the CVM.  And already the CVMs of all the cluster nodes included in the common cluster create from the physical disks, from separate SSD and HDD, directly buried in them, the common space.  Create and give it to the hypervisor, which already sees the shared storage.  The CVM stores this repository in the form most convenient for this hypervisor.  For example, for VMware ESXi, this will be a ‚Äúvirtual‚Äù NFS stack, for 2012R2 Hyper-V, it will be SMB3 protocol stack, and for KVM, iSCSI. <br>  For each hypervisor (and now the three listed are supported), we now have our own CVM, which is installed in the hypervisor during the initial installation of the cluster. <br><br>  The process that provides I / O on selected protocols, between physical disks on the one hand, and the VM on the hypervisor on the other, is called Stargate, and the process that distributes tasks across a cluster of nodes, including all Map-Reduce tasks, such as balancing load on nodes, scrabbing (online integrity check) of disks - Curator.  Prism is the management interface, including the GUI, and Zookeper (also a product from the Apache lab) stores and maintains the cluster configuration. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8db/45c/8c8/8db45c8c8d7d75f853f147a8e624b60e.png" alt="image"><br><br>  Since, as I mentioned above, Nutanix does not use RAID, and lays out the data blocks on the disks on its own, this gives it greater flexibility.  For example, you can see SSD drives in the diagram.  They store the so-called hot tier, that is, those data blocks that are actively being accessed, read or modified.  On the hot tier also get recorded on disc blocks.  There they remain until they are driven out by cold tier, to HDD SATA due to inactivity.  Moreover, since the ‚Äúunfolding‚Äù of the blocks on the disks is handled by the Nutanix kitchen itself in the CVM, it completely controls where and how, what and for how long the blocks will lie. <br><br>  Combining the nodes of the Nutanix cluster into a single cluster, we get the following structure: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d54/1ac/b45/d541acb45b45e4525eb9f150020302c8.png" alt="image"><br><br>  Directly on top of the physical disks is the data storage.  The data is stored in the form of extents, that is, sequentially distributed, addressable block chains, and groups of these extents.  As an addressable storage, it was decided to use the well-developed ext4 file system, from which only the function of storing and addressing extents is used.  All the logic of working with metadata, as I mentioned above, is brought up to the level of Nutanix itself. <br>  The diagram below shows the yellow physical SSD and SATA HDDs, the green one is NDFS, consisting of ext4 as an extent store data and cluster metadata storage in Cassandra, and finally, there are data blocks of guest OS VM file systems on top of that NTFS, ext3.  XFS, or whatever. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0e3/269/c64/0e3269c643b8ee63dacc4ef0ac683d06.png" alt="image"><br><br>  In the next posts, I plan to continue the ‚Äútechnical‚Äù part of the story about what happens in Nutanix ‚Äúunder the hood‚Äù, for example, in more detail about how RAIN works and fault tolerance, how deduplication and compression work, how a distributed cluster is implemented and how replication works data for DR, how to make backups, and much more interesting. <br><br>  If you have just connected to our hub, then you will most likely be interested in reading other Nutanix publications on Habrahabr: <br>  <a href="http://habrahabr.ru/company/nutanix/blog/240859/">http://habrahabr.ru/company/nutanix/blog/240859/</a> <a href="http://habrahabr.ru/company/nutanix/blog/240859/"><br></a> <br>  In addition, those wishing to delve into the topic would like to recommend a blog, in which you can also find a lot of technical articles about how Nutanix works under the hood. <br>  <a href="http://blog.in-a-nutshell.ru/%3F%3Dhh1">http://blog.in-a-nutshell.ru</a> </div><p>Source: <a href="https://habr.com/ru/post/243019/">https://habr.com/ru/post/243019/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../24301/index.html">Competitions robots!</a></li>
<li><a href="../243011/index.html">Software renderer - 1: materiel</a></li>
<li><a href="../243013/index.html">Enthusiast designed a homemade 8-bit processor in Minecraft</a></li>
<li><a href="../243015/index.html">At any given time about the third part of Internet users are on-line.</a></li>
<li><a href="../243017/index.html">10 lessons on how to increase productivity, which I learned by working 90 hours a week for a month</a></li>
<li><a href="../243021/index.html">Vibration of the XboxOne gamepad for Unity3d</a></li>
<li><a href="../243023/index.html">Why Spritz has become so popular over the past few weeks.</a></li>
<li><a href="../243025/index.html">New on VPS Search: Notepad Domains and Notepad Servers</a></li>
<li><a href="../243027/index.html">Wargaming Developers Contest: Winners Announcement</a></li>
<li><a href="../24303/index.html">Web Tattoo</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>