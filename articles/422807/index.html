<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>AliceVision: command line photogrammetry</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Do you need to automate a huge number of photogrammetric scans? Then I have good news for you. 


 The video shows an open source Meshroom photogramme...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>AliceVision: command line photogrammetry</h1><div class="post__text post__text-html js-mediator-article">  Do you need to automate a huge number of photogrammetric scans?  Then I have good news for you. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/v_O6tYKQEBA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  The video shows an open source <i>Meshroom</i> photogrammetry program.  This project in various forms has been around for quite some time, but recently developers have released binaries, so you can just download and use them.  The video demonstrates the use of GUI for loading images, processing them, changing parameters, etc.  I recommend you to try this program in action. <br><br>  But I am interested in full automation.  If you have a scanner with 100 or more scans per day, then you need a fully automated solution for batch processing of these files.  This post is a guide and / or tutorial for solving this task. <br><a name="habracut"></a><br>  For a start, it is important to understand that <i>Meshroom</i> is not a giant, monolithic project.  In fact, the processing itself is performed by separate C ++ programs running from the command line, and the <i>Meshroom</i> is a thin Python padding program that executes the corresponding calls.  Therefore, instead of using <i>Meshroom,</i> we will apply these programs directly.  Note that full sources are available, so you can link libraries directly. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <i>Meshroom</i> has one more convenient feature: when performing each operation, its command is displayed in the terminal.  Therefore, in order to create the steps of this process, I simply worked with the <i>Meshroom</i> and looked at the teams.  Then I looked into the code to change some parameters.  In addition, it seems that when you launch Meshroom, you can order it to collect a set of images from the command line, but I prefer not to connect these steps. <br><br><h2>  Preparation and installation </h2><br>  <strong>0: Requirements</strong> <br><br>  <i>Meshroom</i> / <i>AliceVision</i> will not run on every platform.  Some steps require CUDA, so you need an NVIDIA GPU to build depth maps.  Unfortunately, it is impossible to use CPU fallback (transferring GPU functions to the CPU), otherwise the program would work fine on Windows and Linux.  The instructions in this article are for Windows, but with minimal changes you can tweak them under Linux. <br><br>  <strong>1: Download the Meshroom release</strong> <br><br>  <a href="">Meshroom 2018.1.0</a> <br><br>  The first thing you need to do is install <i>Meshroom</i> .  Select the folder from which you want to do the work, and then download the latest version.  There are binary files of all dependencies in the zip file. <br><br>  If you are drawn to adventure, you can try building a program yourself.  The release dynamic-link libraries work fine (/ MD), but I had to hack cmake files to create debug builds and / or static-link builds.  If you will build a program for Windows, then <strong>EXTREMELY</strong> recommend using VCPKG. <br><br>  <strong>2: Download data</strong> <br><br>  <a href="https://github.com/alicevision/dataset_monstree">alicevision / dataset_monstree</a> <br><br>  Obviously, the whole point of photogrammetry software is to process your own images, but for a start, I suggest using images that are guaranteed to fit.  They will allow you to find the sources of the problems if something goes wrong.  Fortunately, the developers have released a set of images for their test tree. <br><br>  <strong>3: Download the <i>run_alicevision.py</i> script</strong> <br><br>  <a href="">run_alicevision.zip</a> <br><br>  This is the script that we will use.  Just download the zip file and unzip it into a working folder. <br><br>  <strong>4: Install Python</strong> <br><br>  <a href="https://www.python.org/download/releases/2.7/">https://www.python.org/download/releases/2.7/</a> <br><br>  Install Python if you have not done so already.  Yes, I'm still writing code for Python 2.7.0.  The easiest way to install <i>Windows X86-64 MSI Installer</i> from releases. <br><br>  <strong>5: Install Meshlab (optional)</strong> <br><br>  <a href="http://www.meshlab.net/">Meshlab</a> <br><br>  As an optional step, you must also install <i>MeshLab</i> .  In fact, it will not be necessary for processing, but at several stages the data is output in the PLY point files.  They cannot be loaded into <i>Maya</i> , so I use <i>MeshLab</i> to view them. <br><br>  After unpacking all files, the folder should look like this (except for the <i>build_files</i> folder, which is generated by scripts): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bfd/ee8/853/bfdee88535c3e87f5a77c729e9fddcb9.png"></div><br><br>  Here is the following: <br><br><ul><li>  <strong>build_files: the</strong> files we have compiled. </li><li>  <strong>dataset_monstree-master:</strong> source images </li><li>  <strong>Meshroom-2018.1.0:</strong> <i>Meshroom</i> / <i>AliceVision binaries</i> . </li><li>  <strong>Everything else: the</strong> scripts to run them, which are taken from <a href="">run_alicevision.zip</a> . </li></ul><br><h2>  Launch AliceVision </h2><br>  Now it's time to <i>take a</i> closer look at <i>run_alicevision.py</i> <br><br>  The python file takes 5 arguments: <br><br> <code>python run_alicevision.py &amp;ltbaseDir&amp;gt &amp;ltimgDir&amp;gt &amp;ltbinDir&amp;gt &amp;ltnumImages&amp;gt &amp;ltrunStep&amp;gt</code> <br> <br><ol><li>  <strong>baseDir</strong> : the folder in which you want to place temporary files. </li><li>  <strong>imgDir</strong> : folder containing source images.  In our case, <i>IMG_1024.JPG</i> (and others). </li><li>  <strong>binDir</strong> : a folder containing <i>AliceVision executables</i> , for example <i>aliceVision_cameraInit.exe</i> . </li><li>  <strong>numImages</strong> : the number of images in <strong>imgDir</strong> , in our case 6. Of course, you can recognize this number automatically, but the goal was to create as simple a python script as possible, so you need to specify this number yourself. </li><li>  <strong>runStep</strong> : the operation to be performed. </li></ol><br>  To summarize: we start with 6 images that look like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/29f/777/05f/29f77705fac634cceb67d813ec2cee6c.png"></div><br>  Using the python script <i>run_alicevision.py,</i> we are going to create the following folder structure: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/91b/6f3/707/91b6f37070d53f55a030219b44d72c0f.png"></div><br>  And in the <i>11_Texturing</i> folder <i>there</i> will be a ready-made model opened in <i>Meshlab</i> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/979/62e/06a/97962e06a4b6f86d8fd29dbdf233c45d.jpg"></div><br>  Each of these folders is one of the stages.  We can either run them in turn using the <i>run_monstree_runXX.bat</i> files, or use <i>run_monstree_all.bat</i> to collect them all at once. <br><br>  That's all.  Now you can run the file <i>run_monstree_all.bat</i> , or perform one step at a time.  You can look at the script to understand its work.  For those who want to be able to customize the processing pipeline, I have prepared an introduction to the individual steps. <br><br>  <strong>00_CameraInit</strong> <br><br>  The first step will generate an SFM file.  SFM files are JSON files that store camera size, sensor information, found 3d points (observations), distortion factors, and other information.  The original SFM file in this folder will contain only sensor information and select default values ‚Äã‚Äãfrom the local sensor database.  Subsequent steps will create SFM files containing full matrices of camera external parameters, points, etc. <br><br>  You may need to customize this step.  If you are using a 4-camera setup, but take 10 shots of an object rotating on a turntable, then an SFM file with 40 images, but with just 4 different sensor calibrations, comes in handy.  This is the main reason why I like the structure of <i>AliceVision</i> .  It is easy to customize batch operations (for example, generating your own SFM file) without suffering, with customizing other software elements that you shouldn‚Äôt touch. <br><br>  <strong>01_FeatureExtraction</strong> <br><br>  The next stage extracts the features from the images, as well as the descriptors of these features.  It will change the file extension depending on the type of feature being extracted. <br><br>  <strong>02_ImageMatching</strong> <br><br>  <i>02_ImageMatching</i> is a post-processing step that determines which of the images is logical to compare with each other.  If you have a set of 1000 images, then a rough search of all 1000 images to match all 1000 images will require 1 million pairs.  This may take a long time (actually half the time, but you understand the principle).  Stage <i>02_ImageMatching</i> cuts these pairs. <br><br>  <strong>03_FeatureMatching</strong> <br><br>  <i>03_FeatureMatching</i> finds matches between images using feature descriptors.  The txt files generated by it do not need an explanation. <br><br>  <strong>04_StructureFromMotion</strong> <br><br>  So, this is the first serious stage.  Based on the <i>04_StructureFromMotion</i> correspondences, <i>it</i> calculates camera positions as well as internal camera parameters.  Note that the term ‚ÄúStructure From Motion‚Äù is used as a generic term for calculating camera positions.  If you have a setup for photogrammetry of 10 synchronized cameras, then ‚ÄúStructure From Motion‚Äù is used to bind them, even if nothing really moves. <br><br>  By default, <i>Meshroom</i> stores all calculated data as an <i>Alembic</i> file, but I prefer to store it in an SFM file.  This stage creates intermediate data to ensure that cameras are properly linked.  At the output, the script creates PLY files that can be viewed in <i>Meshlab</i> .  The following files are important: <br><br><ul><li>  <strong>bundle.sfm:</strong> SFM file with all observations. </li><li>  <strong>cameras.fm:</strong> an SFM file with data of the <strong>associated</strong> cameras only. </li><li>  <strong>cloud_and_poses.ply:</strong> points found and cameras. </li></ul><br><br>  Here is the <i>cloud_and_poses.ply</i> file.  Green dots are cameras.  I believe that this format is best suited for checking the absence of gross errors in the binding of cameras.  If an error occurs somewhere, you can go back and change the features, matches, or SFM parameters. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1cc/1fd/9fa/1cc1fd9fabcc181bd231cbf261d3c0f6.jpg"></div><br><br>  <strong>05_PrepareDenseScene</strong> <br><br>  The main objective of <i>05_PrepareDenseScene</i> is to eliminate image distortions.  It generates EXR images without distortion, so that subsequent stages of calculating depths and projections do not need to perform back-to-back conversions from the distortion function.  Images look like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c42/b58/3d3/c42b583d37d4e7eacf1d9f9118454096.jpg"></div><br><br>  It should be noted that you will see black areas.  Subsequent stages of <i>AliceVision</i> do not use a real camera matrix.  Instead, we pretend that the camera has a new matrix without distortion, and <i>05_PrepareDenseScene</i> deforms the original image under this fictional matrix.  Since this new virtual sensor is larger than the current sensor, some areas will be empty (black). <br><br>  <strong>06_CameraConnection</strong> <br><br>  Strictly speaking, this stage violates the principle of our work process.  All stages were designed so that each folder becomes a completely unique separate stage.  However, <i>06_CameraConnection</i> creates the <i>camsPairsMatrixFromSeeds.bin</i> file in the <i>05_PrepareDenseScene</i> folder, because this file must be in the same folder as the image without distortion. <br><br>  <strong>07_DepthMap</strong> <br><br>  This is the longest stage of <i>AliceVision</i> : generating depth maps.  It creates a depth map for each image as an EXR file.  I set it up to make it easier to see.  You can see a small ‚Äútongue‚Äù protruding from the tree. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0e1/333/ff8/0e1333ff8b1f58bd48c5fb6102b2fba0.jpg"></div><br>  Since this stage takes a lot of time, there is a parameter that allows us to launch groups of different cameras as different separate commands.  Therefore, if you have 1000 cameras, you can create depth maps for groups of cameras on different farm machines.  Or you can break up the execution of work into small groups so that if one machine fails, you do not need to repeat the whole process again. <br><br>  <strong>08_DepthMapFilter</strong> <br><br>  Original depth maps will not be fully consistent.  Some depth maps will need to see areas covered by other depth maps.  Step <i>08_DepthMapFilter</i> isolates such areas and enforces depth consistency. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7e7/456/6ac/7e74566ac815acdd3a3287d281931d87.jpg"></div><br>  <strong>09_Meshing</strong> <br><br>  This is the first stage in which the mesh is directly generated.  There may be some minor problems with the mesh that can be solved with ... <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/206/f86/0e8/206f860e8c42f1d095727927043564e6.jpg"></div><br>  <strong>10_MeshFiltering</strong> <br><br>  Stage <i>10_MeshFiltering</i> receives the <i>09_Meshing</i> mesh and improves it.  It performs the following operations: <br><br><ul><li>  Smooths the mesh. </li><li>  Eliminates large triangles. </li><li>  Keeps the largest mesh, but removes all the others. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/634/ee8/c73/634ee8c731347eb889630d0dd51c5aef.jpg"></div><br>  Some of these operations in certain cases are not always desirable, so if necessary, the parameters can be configured. <br><br>  <strong>11_Texturing</strong> <br><br>  Final stage.  <i>11_Texturing</i> creates UV and projects textures.  And at this stage everything ends! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/979/62e/06a/97962e06a4b6f86d8fd29dbdf233c45d.jpg"></div><br>  The last trick you can do with <i>Meshlab</i> : you can drag and drop different OBJ and PLY files as layers. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a82/d84/32e/a82d8432e8095d19b4f651894b5ff155.jpg"></div><br>  In my example, there is a layer for both the finished mesh and the points / chambers of the SFM.  Sometimes the smoothing phase of the mesh can be slightly more aggressive than needed, so it‚Äôs helpful to compare the source and smooth meshes.  If the mesh looks broken, it‚Äôs convenient to use sfm data from PLY and meshes from OBJ to track problems in the pipeline. <br><br>  <strong>Thanks</strong> <br><br>  This post would not be complete without a huge thanks to the <i>AliceVision</i> and <i>OpenMVG</i> development teams.  The source of inspiration was the <i>libmv</i> project.  This project was the predecessor of <i>OpenMVG</i> , which is a repository of engineers / researchers of computer vision for the development of new algorithms.  <i>AliceVision</i> is a fork of <i>OpenMVG</i> , created specifically to turn these algorithms into a separate solution in the form of a finished product. <br><br>  <i>AliceVision / Meshroom</i> is a large, ambitious open-source project.  His main achievement is the achievement of the final line with such a serious project, and we owe him very much.  We are also obliged to thank the <i>OpenMVG</i> (and <i>libmv</i> ) team, whose fundamental work allowed us to create <i>AliceVision</i> . <br><br>  Finally, I want to say special thanks to Microsoft for <i>VCPKG</i> .  <i>VCPKG</i> is a package manager that greatly simplifies the assembly of large open-source projects for Windows.  A few years ago I tried to build under Windows <i>OpenMVG</i> .  It did not end very well.  So when I heard about <i>AliceVision a</i> few months ago, I tried to compile it, but I failed even with simpler things.  Then I tried <i>VCPKG</i> , and it all worked right away.  It is difficult to quantify the advantage of using a project such as <i>VCPKG</i> , but it really helped the open-source ecosystem under Windows. <br><br>  <a href="https://github.com/alicevision">github.com/alicevision</a> <br><br>  <a href="https://github.com/openMVG/openMVG">github.com/openMVG/openMVG</a> <br><br>  <a href="https://github.com/libmv/libmv">github.com/libmv/libmv</a> <br><br>  <a href="https://github.com/Microsoft/vcpkg">github.com/Microsoft/vcpkg</a> </div><p>Source: <a href="https://habr.com/ru/post/422807/">https://habr.com/ru/post/422807/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../422797/index.html">How we made a small-sized cloud DVR from a regular IP camera</a></li>
<li><a href="../422799/index.html">How Microsoft hid the whole server and how to find it</a></li>
<li><a href="../422801/index.html">Understand RBAC in Kubernetes</a></li>
<li><a href="../422803/index.html">Storage Cost Calculator, or How We Opened the Black Box</a></li>
<li><a href="../422805/index.html">Quick unsubscribe from mailings in Mail.Ru Mail</a></li>
<li><a href="../422809/index.html">My address is not a house or a street: what will be the address of the XXI century</a></li>
<li><a href="../422811/index.html">Reaching Heaven: Managing Cloud Purchases with SAP Ariba</a></li>
<li><a href="../422815/index.html">Zextras will participate in RedHat Forum Russia 2018</a></li>
<li><a href="../422817/index.html">IFA 2018 Future Electronics Show in Berlin: How It Was</a></li>
<li><a href="../422819/index.html">Means GOSPKA. Translate terminology</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>