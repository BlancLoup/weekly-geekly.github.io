<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Profiling of hybrid cluster applications MPI + OpenMP</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Libraries that implement the MPI standard (Message Passing Interface) are the most popular mechanism for organizing computations on a cluster. MPI all...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Profiling of hybrid cluster applications MPI + OpenMP</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/d55/c79/a28/d55c79a28e7d48718011d0f2796621fc.png"><br><br>  Libraries that implement the MPI standard (Message Passing Interface) are the most popular mechanism for organizing computations on a cluster.  MPI allows you to transfer messages between nodes (servers), but no one bothers to run multiple MPI processes on one node, realizing the potential of several cores.  So often HPC applications are written, it's easier.  And while the number of cores on one node was small, there were no problems with the ‚Äúpure MPI‚Äù approach.  But today the number of cores goes to dozens, if not hundreds, for co-processors Intel Xeon-Phi.  And in such a situation, the launch of dozens of processes on one machine becomes not entirely effective. <br><br>  The fact is that MPI processes communicate through a network interface (albeit implemented through shared memory on the same machine).  This entails excessive copying of data through multiple buffers and increased memory consumption. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      For parallel computing inside a single machine with shared memory, threads and task distribution between them are much better suited.  Here the most popular in the world of HPC is the OpenMP standard. <br><br>  It would seem - well, we use OpenMP inside the node, and MPI for inter-node communications.  But not everything is so simple.  Using two frameworks (MPI and OpenMP) instead of one not only carries the additional complexity of programming, but also does not always give the desired performance gain ‚Äî at least not immediately.  We still have to decide how to distribute the calculations between MPI and OpenMP, and, perhaps, solve problems specific to each level. <br><br>  In this article I will not describe the creation of hybrid applications - information is not difficult to find.  We will look at how you can analyze hybrid applications using Intel Parallel Studio tools, choosing the optimal configuration and eliminating bottlenecks at different levels. <br><a name="habracut"></a><br>  For the tests, we will use NASA Parallel Benchmark: <br><ul><li>  CPU: Intel Xeon processor E5-2697 v2 @ 2.70GHz, 2 sockets, 12 cores in each. </li><li>  OS: RHEL 7.0 x64 </li><li>  Intel Parallel Studio XE 2016 Cluster Edition </li><li>  Compiler: Intel Compiler 16.0 </li><li>  MPI: Intel MPI library 5.1.1.109 </li><li>  Workload: NPB 3.3.1, ‚ÄúCG - Conjugate Gradient, irregular memory access and communication‚Äù module, class B </li></ul><br>  The benchmark is already implemented hybrid and allows you to configure the number of MPI processes and OpenMP streams.  It is clear that for inter-node communications there is no alternative to MPI (within our application).  The intrigue is that run on one node - MPI or OpenMP. <br><br><h1>  MPI Performance Snapshot </h1><br>  We have 24 cores at our disposal.  Let's start with the traditional approach - only MPI.  24 MPI process, 1 thread each.  To analyze the program, we will use the new tool released in the latest version of Intel Parallel Studio - MPI Performance Snapshot.  Just add the ‚Äú-mps‚Äù key to the mpirun startup line: <br><pre><code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">source</span></span> /opt/intel/vtune_amplifier_xe/amplxe-vars.sh <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> /opt/intel/itac/9.1.1.017/intel64/bin/mpsvars.sh --vtune mpirun -mps ‚Äìn 24 ./bt-mz.B.24 mps -g stats.txt app_stat.txt _mps</code> </pre> <br>  The first two lines set the right environment, the third runs the program with MPS profiling.  The last line forms the report in html format.  Without -g, the report will be displayed on the console - convenient for viewing directly on the cluster, but more beautiful in HTML: <br><br><img src="https://habrastorage.org/files/1a8/fc7/bd5/1a8fc7bd50a545eeab6d38275475ec1a.png"><br><img src="https://habrastorage.org/files/75f/7c2/4c6/75f7c24c6a7248a5b9d555f2bf1dcc0b.png"><br><br>  MPS provides top-level performance evaluation.  The overhead of its launch is extremely small, you can quickly make an assessment of the application, even on a large scale (32000 processes tested). <br><br>  First we look at the shares of MPI time and Computation time.  We have 32% of the time spent on MPI, almost all due to the imbalance of the load - some processes wait while others count.  In the blocks on the right, an estimate is given - MPI time is labeled HIGH - too much to spend on communication.  There is also a reference to another tool - Intel Trace Analyzer and Collector (ITAC), for detailed analysis of MPI problems.  About OpenMP no problems highlighted, which is not surprising, because we actually turned it off. <br><br>  MPS also considers hardware performance metrics: GFPLOS, CPI and the ‚ÄúMemory Bound‚Äù metric - an overall assessment of memory performance.  And still memory consumption (for one MPI process) - maximum and average. <br><br><h1>  Intel Trace Analyzer and Collector </h1><br>  MPS showed that the main problem in the ‚Äú24x1‚Äù configuration in MPI.  To find out the reasons, we collect the ITAC profile: <br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">source</span></span> /opt/intel/itac/9.1.1.017/intel64/bin/itacvars.sh mpirun -trace -n 24 ./bt-mz.B.24</code> </pre><br>  Open the track in ITAC GUI - I used the Windows version.  The graph of the Quantitative Timeline clearly shows that the proportion of MPI is large, and communications are distributed with some cyclical nature.  The uppermost graph shows periodic bursts of MPI activity: <br><br><img src="https://habrastorage.org/files/77a/2d2/270/77a2d227044242ef9ab4fa93188c0608.png"><br><br>  If you select several such bursts on the Event Timeline scale, you can observe that the communications are unevenly distributed.  Processes with ranks 0-4 are considered more, and those with ranks 15-23 are waiting more.  Load imbalance is evident: <br><br><img src="https://habrastorage.org/files/6c1/8fc/db5/6c18fcdb5236462ab718df99c3b56bbf.png"><br><br>  On the Message Profile graph, you can evaluate which processes are exchanging messages and where communication is the longest: <br><br><img src="https://habrastorage.org/files/855/ffb/381/855ffb381ea24b4d8ed0e411ae85366a.png"><br><br>  For example, messages between processes with ranks 17 and 5, 16 and 0, 18 and 7, and so on pass longer than others.  By increasing the Event Timeline even more, you can click on the black line at rank 17 and see the transfer details - from whom to whom, message size, send and receive calls: <br><br><img src="https://habrastorage.org/files/c7c/d9d/dca/c7cd9ddcafde4433a653d6a1806bd547.png"><br><br>  The Performance Assistant panel describes specific problems found by the tool in a selected region.  For example, "late reference": <br><br><img src="https://habrastorage.org/files/7c1/77e/ecc/7c177eecc3d6450392b77830a3cad62c.png"><br><br>  The imbalance in MPI can be caused not only by shortcomings in the communication scheme, but also by problems in useful calculations ‚Äî when some processes are considered slower than others.  If we are interested in what this application is spending time inside of any of the processes, and what the problem may be, ITAC can generate a command line to run Intel VTune Amplifier for this rank (for example, on the 2nd): <br><br><img src="https://habrastorage.org/files/2bc/eb2/fcd/2bceb2fcd8e64857b735bf21f8e501ba.png"><br><img src="https://habrastorage.org/files/e33/a60/a68/e33a60a6894b4b7c8a93e36c2becbb59.png"><br><br>  But back to the VTune Amplifier later.  And in general, ITAC gives a lot of opportunities for a detailed study of MPI communications, but our task now is to choose the optimal balance between OpenMP and MPI.  And for this, it is not necessary to immediately correct the MPI communication at 24 ranks - you can try other options first. <br><br><h1>  Other options </h1><br><img src="https://habrastorage.org/files/961/f99/3af/961f993af7eb4b359dd70ffbffb91b34.png"><br><img src="https://habrastorage.org/files/75c/652/b1c/75c652b1c31846ffbb1dd85e1d28e846.png"><br><br>  So, in an empirical way, it turned out that 12x2 and 6x4 distributions work better than others.  Even 2 OpenMP threads per process are significantly faster than 2 MPI processes.  However, with an increase in the number of threads, the running time starts to grow again: 2x12 is even worse than ‚Äúpure MPI‚Äù, and 1x24 makes no sense even to give.  And the blame for the imbalance of work, which is poorly distributed over a large number of OpenMP streams.  Option 2x12 has as much as 30% imbalance. <br><br>  Here we may well stop, because  A compromise of 12x2 or 6x4 is quite acceptable.  But you can dig deeper - to investigate what the problem with OpenMP scaling. <br><br><h1>  VTune Amplifier </h1><br>  Intel VTune Amplifier XE is perfect for a detailed analysis of OpenMP problems, which <a href="http://habrahabr.ru/company/intel/blog/248979/">we have already written</a> in detail about. <br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">source</span></span> /opt/intel/vtune_amplifier_xe/amplxe-vars.sh mpirun -gtool <span class="hljs-string"><span class="hljs-string">"amplxe-cl -c advanced_hotspots -r my_result:1"</span></span> -n 24 ./bt-mz.B.24</code> </pre><br>  To run analyzers such as VTune Amplifier and Intel Advisor XE, it has become very convenient to use the gtool option syntax (only in Intel MPI).  It is embedded in the launch line of the MPI application, allowing you to run the analysis only on selected processes - in our example, only for rank 1. <br><br>  Let's look at the profile of option ‚Äú2 MPI processes, 12 OpenMP streams‚Äù.  In one of the most costly parallel cycles, 0.23 seconds out of 1.5 goes to imbalance.  Further in the table it can be seen that the type of dispatch is static, the work is not redistributed.  In addition, there are only 41 iterations in the loop, and 10-20 iterations in adjacent cycles.  Those.  with 12 threads, each will get only 3-4 iterations.  Apparently, this is not enough for effective load balancing. <br><br><img src="https://habrastorage.org/files/809/eda/a06/809edaa06f7243c988920e1debbd3fac.png"><br><br>  With 2-4 streams, each of them gets more work, and the relative time of active waiting caused by imbalance is reduced.  As evidenced by the ‚Äú6x4‚Äù profile - imbalance is much lower: <br><br><img src="https://habrastorage.org/files/51c/e57/f97/51ce57f97f174687896e25debd9cf977.png"><br><br>  In addition, the MPI version appeared in the Intel VTune Amplifier 2016 version - the ‚ÄúMPI Communication Spinning‚Äù column and the yellow marking on the timeline.  You can run the VTune profile for several processes at once on one node, and watch MPI spinning along with OpenMP metrics in each of them: <br><br><img src="https://habrastorage.org/files/56d/a6a/025/56da6a025831479f9448adcd4ecd9339.png"><br><img src="https://habrastorage.org/files/f63/3cc/85b/f633cc85b50a47068ec38f70dec9dc75.png"><br><br><h1>  Intel Advisor XE </h1><br>  Going down through parallelism levels, from cluster scale (MPI), to single node streams (OpenMP) we get to parallelism on data within one stream - vectorization based on SIMD instructions.  Here, too, there can be serious potential for optimization, however, it was not for nothing that we got to it last - first you need to solve problems at the MPI and OpenMP levels, since  there is potentially more to win.  About Advisor not so long ago there were two posts (the <a href="http://habrahabr.ru/company/intel/blog/255731/">first</a> and the <a href="http://habrahabr.ru/company/intel/blog/257309/">second</a> ), so here I will limit myself to the launch line: <br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">source</span></span> /opt/intel/advisor_xe/advixe-vars.sh mpirun -gtool <span class="hljs-string"><span class="hljs-string">"advixe-cl -collect survey --project-dir ./my_proj:1"</span></span> -n 2 ./bt-mz.2</code> </pre><br>  Then we analyze the vectorization of the code, as we wrote earlier.  Advisor is an important part of the ecosystem analysis of cluster MPI programs.  In addition to a deep study of code vectorization, Advisor helps to prototype multi-threaded execution and checks memory access patterns. <br><h1>  Summary </h1><br>  Intel Parallel Studio offers four tools to analyze the performance of hybrid HPC applications: <br><ul><li>  MPI Performance Snapshot (cluster level) - quick assessment of efficiency, minimum overhead, profiling up to 32000 MPI processes, quick assessment of MPI and OpenMP imbalance, overall performance evaluation (GFLOPS, CPI). </li><li>  Intel Trace Analyzer and Collector (cluster level) - a detailed study of MPI, identification of communication patterns, localization of specific bottlenecks. </li><li>  Intel VTune Amplifier XE (single node level) - a detailed profile with source code and stacks, imbalances and other OpenMP problems, analysis of cache and memory usage, and much more. </li><li>  Intel Advisor XE (single node level) - analysis of the use of vector instructions and identifying the causes of their inefficiency, prototyping of multi-threaded execution, analysis of memory access patterns. </li></ul><br><img src="https://habrastorage.org/files/11f/d72/e7c/11fd72e7ce9944de97d22bb3632209ee.png"></div><p>Source: <a href="https://habr.com/ru/post/266409/">https://habr.com/ru/post/266409/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../266393/index.html">HighLoad ++ is a highly loaded dish.</a></li>
<li><a href="../266397/index.html">RailsClub 2015: Interview with Andrei Kumanyaev</a></li>
<li><a href="../266399/index.html">Automatically building kernel modules with DKMS</a></li>
<li><a href="../266401/index.html">The queue of events in the card game + Angular basics</a></li>
<li><a href="../266407/index.html">World undocumented React.js. Context</a></li>
<li><a href="../266411/index.html">Using the Java native library on application servers</a></li>
<li><a href="../266413/index.html">Microsoft Useful Resources for IT Professionals</a></li>
<li><a href="../266415/index.html">Protection of corporate applications: how to become a developer of PT Application Firewall</a></li>
<li><a href="../266417/index.html">Release it! Software design and design for those who care</a></li>
<li><a href="../266419/index.html">Organization of monitoring the work of employees in a small office</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>