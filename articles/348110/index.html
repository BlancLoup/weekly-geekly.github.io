<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Visualization of data for moviegoers: scrap movie recommendations and make interactive graph</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Once I stumbled upon an interactive map lastfm and decided to definitely make a similar project for films. Under the cut there is a story about how to...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Visualization of data for moviegoers: scrap movie recommendations and make interactive graph</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/t_/sj/6r/t_sj6rr8nqlj5jtch3kgqjvow9y.png" align="right" width="300"><br><p>  Once I stumbled upon <a href="http://sixdegrees.hu/last.fm/interactive_map.html">an interactive map lastfm</a> and decided to definitely make a similar project for films.  Under the cut there is a story about how to collect data, build a graph and create your own interactive demo using the data from a film search and imdb as an example.  We will look at the Scrapy scraping framework, go over large graph visualization techniques and deal with the tools for interactively displaying large graphs in the browser. </p><a name="habracut"></a><br><h1 id="1-sbor-dannyh-scrapy">  1. Data collection: Scrapy </h1><br><p>  As a data source, I chose a film search.  However, later it turned out that this is very small and I scraped IMDb.  To make a graph, for each movie you need to know the list of recommended films.  If you search, you can find enough parsers for movie search and all sorts of unofficial api, but nowhere is there any way to get recommendations.  IMDb openly shares its dataset, but there are no recommendations there either.  Therefore, there is only one choice left: to write your spider. </p><br><p>  On Habr√© there are already several articles about scraping, so I‚Äôll skip the review of possible approaches.  In a nutshell: if you are writing in python and do not want to write your framework, use <a href="https://scrapy.org/">Scrapy</a> .  Almost everything you need is already provided for. </p><br><p>  Scrapy is really very powerful and at the same time very simple tool.  The entry threshold is quite low, but at the same time Scrapy easily scales to projects of any size and complexity.  It really contains everything you need.  From tools directly parsing and HTTP requests, processing and saving the received elements, to managing the project work, including ways to bypass the block, pause and resume scraping, etc. </p><br><p> Creating a project begins with the <code>scrapy startproject mycoolproject</code> , after which you get a ready-made structure with the templates of the necessary elements and the files of the minimum working configuration.  To make a working project out of this, it‚Äôs enough to describe how to parse the page ‚Äî that is, create a spider and put it in the <code>spiders</code> folder inside the project, and describe exactly what information you want to extract ‚Äî that is, inherit your class from the <code>scrapy.item</code> class in the <code>scrapy.item</code> script.  Thus, you can make a fully working project in less than an hour.  There are built-in tools for saving the results: for example, writing to csv or json, but it is better to use an external database if the project is not for five minutes.  Behavior associated with the processing of results, including the preservation is specified in <code>pipelines.py</code> .  The last important file remains - <code>settings.py</code> whose purpose is clear from the title.  Here you can set the project configuration related, for example, using a proxy, timing between requests and many others. </p><br><p>  And so, in steps: </p><br><ul><li>  We look in the articles <a href="https://habrahabr.ru/post/308660/">once</a> , <a href="https://habrahabr.ru/post/115710/">twice</a> and in the <a href="https://docs.scrapy.org/en/latest/">documentation</a> how to create a project for Scrapy.  By analogy, we create our own class for items. </li></ul><br><div class="spoiler">  <b class="spoiler_title">items.py for movie searching</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> scrapy <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">MovieItem</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(scrapy.Item)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-string"><span class="hljs-string">'''Movie scraped info'''</span></span> movie_id = scrapy.Field() name = scrapy.Field() like = scrapy.Field() genre = scrapy.Field() date = scrapy.Field() country = scrapy.Field() director = scrapy.Field()</code> </pre> </div></div><br><ul><li>  We are looking for the elements we need on the page and get them xpath.  This can be done, for example, through chrome: right-click on an element, select the inspect element, again click the right mouse button in the code, and look for the copy -&gt; xpath item.  For debugging, you can run scrapy-shell and transfer the page url to it: <code>scrapy-shell https://www.kinopoisk.ru/film/518214/</code> .  You will have a response object instantiated from which you can get the necessary elements. </li></ul><br><div class="spoiler">  <b class="spoiler_title">Like this:</b> <div class="spoiler_text"><pre> <code class="hljs ruby">$scrapy-shell <span class="hljs-symbol"><span class="hljs-symbol">https:</span></span>/<span class="hljs-regexp"><span class="hljs-regexp">/www.kinopoisk.ru/film</span></span><span class="hljs-regexp"><span class="hljs-regexp">/sakhar-i-korica-1915-201125/</span></span> $response.xpath(<span class="hljs-string"><span class="hljs-string">'//span[@itemprop="director"]/a/span/text()'</span></span>).extract_first() <span class="hljs-string"><span class="hljs-string">' '</span></span></code> </pre> </div></div><br><ul><li>  We adjust processing of the received objects.  I chose to save the record in the sqlite database, because it is very simple. </li><li>  We check that everything works by setting the <code>CLOSESPIDER_PAGECOUNT=5</code> parameter at startup to limit the number of requests. </li><li>  To battle!  Create a directory to save intermediate results, for example <code>crawls1</code> .  Run the spider with the <code>scrapy crawl myspider -s JOBDIR=crawls1</code> parameter <code>scrapy crawl myspider -s JOBDIR=crawls1</code> : now, if something goes wrong, we can restart the spider from the same place where it ended.  The relevant section in the <a href="https://doc.scrapy.org/en/latest/topics/jobs.html">documentation</a> . </li></ul><br><h5 id="11-obhod-ogranicheniya-na-chislo-zaprosov">  1.1 Bypass restrictions on the number of requests. </h5><br><p>  Film search banned me at the stage of debugging a spider, when I sent every few minutes a pack of 5 requests with a timeout of 1 second.  To circumvent the restrictions, there are many options.  For scrapie, ready-made examples of using a torus, randomly searching a proxy from the list, or connecting to paid rotating proxy services are easy to do.  Since we have a ‚Äúweekend project‚Äù, I chose rotating proxy - the fastest option to implement, although I have to connect to a paid service.  How it works: you connect to a specific ip: port of your proxy provider, and at the output you get a new ip for each request.  From the scrapy side, you need to add one line in your project's settings.py file and in each request, pass a parameter for the ip: port pair. </p><br><div class="spoiler">  <b class="spoiler_title">In code, it looks like this:</b> <div class="spoiler_text"><p>  We are looking for the appropriate section in settings.py and add a line to it: </p><br><pre> <code class="python hljs">DOWNLOADER_MIDDLEWARES = { <span class="hljs-string"><span class="hljs-string">'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware'</span></span>:<span class="hljs-number"><span class="hljs-number">543</span></span> }</code> </pre> <br><p>  Then in each request of your spider: </p><br><pre> <code class="hljs rust">scrapy.Request(url=url, callback=<span class="hljs-keyword"><span class="hljs-keyword">self</span></span>.parse, meta={<span class="hljs-symbol"><span class="hljs-symbol">'proxy</span></span><span class="hljs-string"><span class="hljs-string">':'</span></span>http:<span class="hljs-comment"><span class="hljs-comment">//YOU_RPROXY_IP:PORT'})</span></span></code> </pre> </div></div><br><p>  The author of the project that inspired me started with Nightwish and then went around lastfm wide recommendations, like a tree - so it turned out to be a connected graph.  My approach was similar.  From a film search, you can get movies by id, which apparently just corresponds to the sequence number of the movie on the site.  If you just take all the id idling up, it will not work out very well, because most of the films do not have recommendations and will be single points that will turn into noise on the map.  ID of fresh films have about 500000 order - this is quite a lot for visual drawing, so let's start with the list of the top 250 films and we will iteratively bypass the lists of recommendations of each movie. </p><br><p>  I expected to get about 100,000 films, but after the night of scraping it turned out that the spider stopped at ~ 12,600.  The recommendations in the film search on this end.  As it was mentioned at the beginning, I climbed on IMDb for new data.  Scrapping IMDb was even easier.  A couple of hours to rewrite the finished project and a new spider is ready for launch.  After two or three days of crawling (8 requests per second so as not to become impudent), the spider stopped gathering 173+ thousands of films.  The full code of the spiders can be viewed on a githaba: film <a href="https://github.com/iggisv9t/kinoposik-spider">search</a> and <a href="https://github.com/iggisv9t/imdb-spider">IMDb</a> . </p><br><h1 id="2-vizualizaciya">  2. Visualization </h1><br><p>  On the one hand, graph visualization tools are a whole zoo.  On the other hand, when it comes to very large graphs, this zoo suddenly scatters somewhere.  I have chosen for myself two tools for such cases: these are sfdp from <a href="https://www.graphviz.org/">graphviz</a> and <a href="https://gephi.org/">gephi</a> .  SFDP is a CLI utility with a wide range of parameters, capable of drawing graphs per million nodes, but in our case it is not the most convenient tool, because we need to control the styling process.  For cases like ours, Gephi is great - this is an application with a graphical interface and a universal set of styling, for almost every taste. </p><br><p>  Exporting data for a graph is done using a simple python script.  I usually use the dot format because it is very simple, which is called "human readable".  Initially, the format is intended for use in graphviz, but is now supported by many other applications for working with graphs. </p><br><p>  <strong>Format description</strong> <br>  At the beginning we write the header of the <code>digraph kinopoisk {\n</code> and do not forget to write a closing bracket at the end of the file <code>}</code> .  In each line we describe the edges of the graph <code>node1 -&gt; node2;</code>  and close the list.  Description of the format here: <a href="https://www.graphviz.org/doc/info/lang.html">official docks</a> and <a href="https://en.wikipedia.org/wiki/DOT_(graph_description_language)">simple examples in wikipedia</a> . </p><br><div class="spoiler">  <b class="spoiler_title">Sample Explain File</b> <div class="spoiler_text"><pre> <code class="hljs erlang-repl">digraph sample { <span class="hljs-number"><span class="hljs-number">1</span></span> -&gt; <span class="hljs-number"><span class="hljs-number">2</span></span>; <span class="hljs-number"><span class="hljs-number">1</span></span> -&gt; <span class="hljs-number"><span class="hljs-number">3</span></span>; <span class="hljs-number"><span class="hljs-number">5</span></span> -&gt; <span class="hljs-number"><span class="hljs-number">4</span></span> [weight=<span class="hljs-string"><span class="hljs-string">"5"</span></span>]; <span class="hljs-number"><span class="hljs-number">4</span></span> [shape=<span class="hljs-string"><span class="hljs-string">"circle"</span></span>]; }</code> </pre> <br><p>  <code>digraph</code> - means that we declare a directed graph.  If not directed, then we just write a <code>graph</code> .  <code>sample</code> is the name of our graph (optional).  Each line contains edges or vertices.  If the edges are not directed, then instead of <code>-&gt;</code> we write <code>--</code> .  You can declare edge or vertex parameters in square brackets.  In this example, we set the edge between nodes 5 and 4 a weight equal to five, and the top 4 forms a circle.  The names of the vertices do not have to be denoted by numbers, they can be strings.  For more examples and parameters see the documentation.  In our case, the possibilities described above are quite enough. </p></div></div><br><h4 id="21-sravnenie-ukladok">  2.1 Comparison of styling </h4><br><p>  For large graphs, gephi has two reasonable options: OpenOrd and ForceAtlas 2. OpenOrd is a very fast approximate algorithm, but has few adjustable parameters.  ForceAtlas is similar to other classic force-directed algorithms, gives more accurate results, is very flexible in setting, but you have to pay for it with time.  Below are examples of the work of both algorithms on a graph that represents a grid. </p><br><img src="https://habrastorage.org/webt/bw/6y/ki/bw6ykigdmafr0zzxftlnht2zlbc.png" width="300" title="OpenOrd.  But quickly"><img src="https://habrastorage.org/webt/_i/ku/nb/_ikunbo7pkrhyydubkrvqmxtc6y.png" width="300" title="ForceAtlas.  Nice but long"><br>  Model graph-grid.  Left OpenOrd, right ForceAtlas. <br><br><br><p>  You might think that OpenOrd should not be used at all if there is time to wait for a more accurate result.  In fact, it is not uncommon for graphs to happen when ForceAtlas assembles all the nodes in one tight lump, and OpenOrd shows at least some structure. </p><br><p>  To speed up the process, I used OpenOrd as an initial approximation and then ‚Äúsmeared‚Äù the graph using ForceAtlas.  In order to have at least something clear on the image, you need to eliminate the overlapping of nodes on each other.  For this, it is convenient to use the Yifan Hu piling ‚Äî to smear the clusters a little, and noverlap, to completely eliminate the overlap.  On the elimination of the overlay in the graph of kinopoisk, the night was gone, imdb could not be managed for the whole weekend. </p><br><p><img src="https://habrastorage.org/webt/zd/q7/_k/zdq7_ke4quez0knl0nm5omryy-u.png" title="Film search"><img src="https://habrastorage.org/webt/ma/cb/4w/macb4wylip5w5cvdbrwyvyx_cns.png" title="IMDb"></p><br><h1 id="3-eksport-rezultatov-interaktivnaya-karta">  3. Export Results: Interactive Map </h1><br><p>  Gephi can export images to svg, png and many other formats.  But with big data come great difficulty.  Only one beautiful picture is not enough.  We want to see the names of the films and how they are related.  If we draw the labels of the nodes, then with such a number of them we will get a completely unreadable loop of letters.  There are options to use SVG and scale it until something becomes visible, or to draw only the most important tags.  But there is a better option, on which I decided to focus.  Making an interactive map. </p><br><img src="https://habrastorage.org/webt/jn/b0/pe/jnb0ped6ndgj1bbwis_knite_rw.gif" width="500"><br><br><h5 id="obzor-instrumentov">  Tool overview: </h5><br><p>  <strong>sigma.js</strong> <br>  The first option, one of the simplest and at the same time the most obvious, is a plugin for gephi with export to the <a href="http://sigmajs.org/">sigma.js</a> template.  On the gif above it just.  Install the plugin via the gephi menu, after which we have a new export menu item in the file tab.  We fill in the form, export and get ready-made working visualization.  Simple and powerful.  The result can be seen <a href="https://iggisv9t.github.io/kinopoisk/index.html">here</a> .  Disadvantage: on large graphs, the browser barely copes. </p><br><p>  <strong>gefx-js</strong> <br>  The next option is even simpler than the previous one and is generally very similar.  <a href="https://github.com/raphv/gexf-js">gefx-js</a> - you just need to export your project from gephi to gexf format and put it in a folder with a template.  Is done.  The disadvantage is exactly the same as in the previous case.  Moreover, if using sigmajs I could view the imdb graph at least locally, then with gefx-js it simply did not boot. </p><br><p>  <strong>openseadragon</strong> <br>  For cases when you need to show a very large picture there is a <a href="https://openseadragon.github.io/">seadragon</a> .  The principle is exactly the same as when rendering geographic maps: when scaling, new tiles are loaded that correspond to the current zoom and viewport.  That is exactly what the author of the project that inspired me did.  Lack of one: interactivity at least.  It is impossible to select nodes, it is difficult to see where the edges go.  It is impossible to "look behind" the overlap of nodes and edges. </p><br><p>  <strong>shinglejs</strong> <br>  But what if you made something like a mixture of the previous versions, so that when scaled, the graph was loaded with tiles, but not with pictures, but as in the first case, with interactivity of nodes and edges?  The finished solution was literally a miracle, it is <a href="https://www.ayalpinkus.nl/shinglejs/">shinglejs</a> . <br>  Pros: You can render very (very, very) large graphs in the browser, while maintaining interactivity. <br>  Cons: Not as beautiful as sigmajs, data preparation is not trivial. </p><br><div class="spoiler">  <b class="spoiler_title">Screen count in with shinglejs</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/zf/wk/ay/zfwkayulhwdl6p_1im96apxxwta.png"><br>  Not so beautiful, but very bright <br></div></div><br><p>  To visualize the imdb graph, I chose the last option.  There was no choice in general.  The result can be seen <a href="https://iggisv9t.github.io/imdb/index.html">here</a> , and then a little bit about how to prepare the data for such visualization. </p><br><p>  <strong>Export data to shinglejs:</strong> <br>  As I have already said, exporting data in the latter case is not very simple, so I will give an example of how to unload a graph from gephi for shinglejs. </p><br><ul><li>  Exporting a graph from gephi in gdf format is probably the only easy way to get the coordinates of nodes in a table.  The file structure is this: first there is a table with a description of nodes, then a table with a description of edges. </li><li>  We read the file and we get from it the description of tops and edges.  I did this with pandas, then chopped the data frame into two: edges and nodes.  About working with <a href="https://habrahabr.ru/company/ods/blog/322626/">pandas</a> . </li><li>  We change the names of the columns in accordance with the <a href="https://www.ayalpinkus.nl/shinglejs/">shinglejs</a> docks and export them to json.  Shinglejs does not support direct export of flowers, but for each node you can specify "communities" and color them already.  Therefore, the film ratings are unloaded as community tags. </li><li>  In the source code of the main page, do not forget to specify a list of colors for communities.  To color a node from the list of colors, the element with the number is taken which is calculated as follows: <code>id  %  </code> . </li><li>  We glue the files into one.  I did this through bash: <code>cat start imdbnodes.json middle imdbedges.json end &gt; imdbdata.json</code> , after creating the <code>start, middle, stop</code> files with the contents of " <code>{"nodes":</code> ", " <code>, "relations":</code> " and " <code>}</code> , respectively . </li><li>  Further, according to the instructions from the <a href="https://www.ayalpinkus.nl/shinglejs/">office.</a>  <a href="https://www.ayalpinkus.nl/shinglejs/">the site</a> </li><li>  Do not forget to create a bitmap and put it in the folder with the graph data, otherwise at a great distance you will see only a few nodes or nothing at all.  The author of the project, it seems, did not specify this detail, but by default the example will try to load the files <code>image_2400.jpg</code> and <code>image_1200.jpg</code> , and not npm as it may seem after building the default project. </li></ul><br><h1 id="4-interesnye-nablyudeniya">  4. Interesting observations </h1><br><p>  On the column lastfm there is an obvious clustering associated with the countries of origin of musical groups, for example, Japanese pop and rock, Greek metal, etc. Exactly the same thing happens with movies.  Korean, Turkish, Japanese and Brazilian films are very clearly separated.  In imdb, a large cluster of cartoons stands out far from the main mass.  On both graphs, a cluster of films about superheroes from comics is going very tightly.  It seems to be obvious, but nevertheless, unexpectedly, that bad films gather in one big cloud.  There are separate clusters of music videos, children‚Äôs youtube blogs and fan movies around the Harry Potter universe. </p><br><h1 id="5-chto-eschyo-mozhno-delat-s-etimi-dannymi">  5. What else can you do with this data? </h1><br><p>  I am sure that readers will be able to come up with and make on the obtained data many more interesting projects.  Such ideas immediately come to my mind: </p><br><ul><li>  Cluster and compile thematic collections.  I worked perfectly DBSCAN almost from the first call.  (An example will be next) </li><li>  Make your own recommendation systems </li><li>  Collect interesting statistics about films in general </li><li>  Of course, expand your movie library. </li></ul><br><h3 id="51-dbscan">  5.1 DBSCAN </h3><br><p>  There are many special methods for graph clustering, and all of them are worthy of individual articles.  I used the method for the graphs not intended as an experiment.  The reasoning was as follows: once the graph has visually decomposed into clouds of similar films, the areas where films are especially close together can be found using <a href="https://en.wikipedia.org/wiki/DBSCAN">DBSCAN</a> .  Let's see what this method does, without going into deep details.  The name DBSCAN stands for density-based scan, that is, using this method, we merge points that are quite close to each other.  This is formalized through two main hyperparameters - this is the radius in which we are looking for neighbors for each point and the minimum number of neighbors. </p><br><p>  <strong>1. Get the coordinates.</strong> <br>  To do this, export our graph from gephi in gdf format.  We read the file as a csv with pandas: </p><br><pre> <code class="python hljs">data = pd.read_csv(<span class="hljs-string"><span class="hljs-string">'./kinopoisk.gdf'</span></span>) <span class="hljs-comment"><span class="hljs-comment"># gdf  -     #   ,     # pandas     ,      #   nan          data_nodes = data[data['y DOUBLE'].apply(lambda x: not np.isnan(x))]</span></span></code> </pre> <br><p>  Let's draw now and see what it looks like. </p><br><pre> <code class="python hljs">plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">7</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>)) plt.scatter(data[<span class="hljs-string"><span class="hljs-string">'x DOUBLE'</span></span>].values, data[<span class="hljs-string"><span class="hljs-string">'y DOUBLE'</span></span>].values, marker=<span class="hljs-string"><span class="hljs-string">'.'</span></span>, alpha=<span class="hljs-number"><span class="hljs-number">0.3</span></span>);</code> </pre> <br><img src="https://habrastorage.org/webt/5m/md/ou/5mmdou3_gnxdq_d3pdmnpolodds.png"><br><p>  Great, DBSCAN should handle this. </p><br><p>  <strong>2. Clustered.</strong> <br>  We select the parameters and look at the distribution of cluster sizes.  No serious work was planned, so I evaluated the quality "by eye". </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.cluster <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> DBSCAN coords = data_nodes[[<span class="hljs-string"><span class="hljs-string">'x DOUBLE'</span></span>, <span class="hljs-string"><span class="hljs-string">'y DOUBLE'</span></span>]].values dbscan = DBSCAN(eps=<span class="hljs-number"><span class="hljs-number">70</span></span>, min_samples=<span class="hljs-number"><span class="hljs-number">5</span></span>, leaf_size=<span class="hljs-number"><span class="hljs-number">30</span></span>, n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>) labels = dbscan.fit_predict(coords) plt.hist(labels, bins=<span class="hljs-number"><span class="hljs-number">50</span></span>);</code> </pre> <br><img src="https://habrastorage.org/webt/mg/kt/wf/mgktwfooysrp4uzdksgtv2g_cy0.png"><br><p>  Cluster size distribution </p><br><p>  Let's color our dots in the colors of the clusters and see how the result is similar to the truth. </p><br><pre> <code class="python hljs">plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> l <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> set(labels): coordsm = coords[labels == l] plt.scatter(coordsm[:,<span class="hljs-number"><span class="hljs-number">0</span></span>], coordsm[:,<span class="hljs-number"><span class="hljs-number">1</span></span>], marker=<span class="hljs-string"><span class="hljs-string">'.'</span></span>, alpha=<span class="hljs-number"><span class="hljs-number">0.3</span></span>);</code> </pre> <br><img src="https://habrastorage.org/webt/5t/vp/zj/5tvpzjz8jazfouvawzb39iufcla.png"><br><p>  Looks like what we wanted to get. </p><br><p>  Let's try to get a cluster of a movie in the form of a list.  I did not take care of making a convenient way to get the list, so this time without a code.  Below is a list of films that fall into one cluster with the "real ghouls."  In my opinion not bad. </p><br><div class="spoiler">  <b class="spoiler_title">Table with movies</b> <div class="spoiler_text"><table><thead><tr><th>  movie_id </th><th>  name </th><th>  date </th><th>  genre </th><th>  country </th><th>  director </th></tr></thead><tbody><tr><td>  271695 </td><td>  Third planet from the sun </td><td>  1996-01-09 </td><td>  fantasy </td><td>  USA </td><td>  Terry Hughes </td></tr><tr><td>  663135 </td><td>  Neighbors </td><td>  2012-09-26 </td><td>  comedy </td><td>  USA </td><td>  Chris Koch </td></tr><tr><td>  277375 </td><td>  Aliens </td><td>  1997-11-07 </td><td>  a cartoon </td><td>  France </td><td>  Jim Gomez </td></tr><tr><td>  81845 </td><td>  Sweeney Todd, Demon Barber of Fleet Street </td><td>  2007-12-03 </td><td>  musical </td><td>  USA </td><td>  Tim Burton </td></tr><tr><td>  445196 </td><td>  Hands and legs for love </td><td>  2010-10-29 </td><td>  thriller </td><td>  Great Britain </td><td>  John landis </td></tr><tr><td>  271878 </td><td>  Red hotel </td><td>  2007-12-05 </td><td>  comedy </td><td>  France </td><td>  Gerard Kravchik </td></tr><tr><td>  3609 </td><td>  Plunkett and Maclaine </td><td>  1999-01-22 </td><td>  action movie </td><td>  Great Britain </td><td>  Jake scott </td></tr><tr><td>  183497 </td><td>  Bourke and Hare </td><td>  1972-02-03 </td><td>  horrors </td><td>  Great Britain </td><td>  Vernon Sewell </td></tr><tr><td>  3482 </td><td>  Doctor and Devils </td><td>  1985-10-04 </td><td>  horrors </td><td>  Great Britain </td><td>  Freddie Francis </td></tr><tr><td>  2528 </td><td>  Values ‚Äã‚Äãof the Addams Family </td><td>  1993-11-19 </td><td>  fantasy </td><td>  USA </td><td>  Barry Sonnenfeld </td></tr><tr><td>  503578 </td><td>  Lullaby </td><td>  2010-02-12 </td><td>  fantasy </td><td>  Poland </td><td>  Julius Makhulsky </td></tr><tr><td>  87404 </td><td>  Red Tavern </td><td>  1951-10-19 </td><td>  comedy </td><td>  France </td><td>  Claude Otan-Lara </td></tr><tr><td>  5293 </td><td>  Addams Family </td><td>  1991-11-22 </td><td>  fantasy </td><td>  USA </td><td>  Barry Sonnenfeld </td></tr><tr><td>  18089 </td><td>  Body Snatchers </td><td>  1945-02-16 </td><td>  horrors </td><td>  USA </td><td>  Robert Wise </td></tr><tr><td>  271846 </td><td>  Seller of the dead </td><td>  2008-10-10 </td><td>  horrors </td><td>  USA </td><td>  Glenn MacQuade </td></tr><tr><td>  272111 </td><td>  Freshly buried </td><td>  2007-09-09 </td><td>  drama </td><td>  Canada </td><td>  Chaz Thorn </td></tr><tr><td>  34186 </td><td>  Elvira: Master of Darkness </td><td>  1988-09-30 </td><td>  comedy </td><td>  USA </td><td>  James Signorelli </td></tr><tr><td>  818981 </td><td>  Real ghouls </td><td>  2014-01-19 </td><td>  comedy </td><td>  New Zealand </td><td>  Jemaine clement </td></tr><tr><td>  8421 </td><td>  Edward Scissorhands </td><td>  1990-12-06 </td><td>  fantasy </td><td>  USA </td><td>  Tim Burton </td></tr><tr><td>  5622 </td><td>  Sleepy Hollow </td><td>  1999-11-17 </td><td>  horrors </td><td>  USA </td><td>  Tim Burton </td></tr><tr><td>  2389 </td><td>  Beatlejus </td><td>  1988-03-29 </td><td>  fantasy </td><td>  USA </td><td>  Tim Burton </td></tr></tbody></table></div></div><br>  This approach seems to me interesting because we get a list of similar films that are not necessarily linked by direct recommendations, and are not even necessarily achievable in a small number of steps when traversing the graph.  That is, so you can find a movie that will fall if you simply click on similar movies directly on the site. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      PS <br>  Thanks to all the friends who were ready to answer my questions, to all movie lovers - new discoveries, and to the quality data datasaentists! </div><p>Source: <a href="https://habr.com/ru/post/348110/">https://habr.com/ru/post/348110/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../348098/index.html">Why is it important to check that the malloc function returned</a></li>
<li><a href="../348100/index.html">Indian programmers, cookies from England and the Caucasus: the history of technical support department</a></li>
<li><a href="../348102/index.html">Object Recognition with PowerAI Vision</a></li>
<li><a href="../348106/index.html">Office 365. An example of working with the Microsoft Graph API in Angular5 using ADAL JS. ADAL JS vs MSAL JS</a></li>
<li><a href="../348108/index.html">How to destroy the Internet?</a></li>
<li><a href="../348112/index.html">Wolfram Language (Mathematica) Virtual Textbook, 5th Edition</a></li>
<li><a href="../348116/index.html">‚ÄúProgrammer pragmatist. The journey from the apprentice to the master ": briefly about the main thing (part one)</a></li>
<li><a href="../348118/index.html">Expanding Ansible functionality with modules</a></li>
<li><a href="../348120/index.html">Managing Internet modules Laurent from RouterOS MikroTik</a></li>
<li><a href="../348122/index.html">Issue # 9: IT training - current issues and challenges from leading companies</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>