<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Southernmost data center</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="When we hear the word ‚Äúsouth‚Äù in conversation, our consciousness involuntarily draws sunny pictures, salty breeze, flocks of sea birds, palm trees. Th...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Southernmost data center</h1><div class="post__text post__text-html js-mediator-article">  When we hear the word ‚Äúsouth‚Äù in conversation, our consciousness involuntarily draws sunny pictures, salty breeze, flocks of sea birds, palm trees.  This article will talk about the southernmost data center in the world, but, despite the seasonal abundance of the sun, you will not find resort-women walking around in a bikini.  At the Amundsen-Skot polar station, which was located at 89 degrees 59 minutes and 24 seconds south latitude, a data center Ice Cube data center was created and successfully operates to maintain the operation of the neutrino observatory.  About the tasks of this IT site and about the conditions in which it is necessary to maintain the efficiency of the equipment, and will be discussed later in the article. <br><br><img src="https://habrastorage.org/files/aab/cd3/61a/aabcd361a9b24b9487de7955c9269f0d.jpg"><a name="habracut"></a><br><br>  The data center at the laboratory "Ice Cube" has at its disposal more than 1,200 cores and about 3 petabytes of memory.  In fact, the entire computational potential of the data center is aimed at serving the needs of the neutrino observatory, which is located almost at the southernmost point of our Earth.  Neutrino detectors are stacked in rows under more than a kilometer of Antarctic ice, and their task is to detect bursts of neutrino radiation, which comes to us from deep space with echoes of large-scale destructive events, thus helping us to study both the mysterious dark matter and the physics itself of radiation components. neutrino. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/files/0af/04f/2a2/0af04f2a2ef843718e6fbcec0be993bb.jpg"><br><br>  The process of launching and maintaining the operational state of the IT infrastructure in one of the most severe and remote places throughout the world was marked by a number of problems and tests that IT engineers had simply not yet encountered. <br><br>  The path to the Amundsen-Scott polar station can be compared with a journey to another planet: ‚ÄúThis is a world in which only two colors dominate: white and blue,‚Äù says an employee of the station Gitt.  ‚ÄúIn the summer, you catch yourself thinking that two o'clock in the afternoon here lasts for days, which in itself brings new and very interesting sensations into your perception of a person‚Äôs normal life cycle,‚Äù says Barnet, colleague Gitta.  At the same time, winter in these parts is characterized by a long night, which is illuminated only by the Moon and a rare phenomenon - colorful auroras (auroras). <br><br><img src="https://habrastorage.org/files/a03/98a/a4c/a0398aa4c46d463794176dc598d3320b.jpg"><br><br><h4>  Staff </h4><br>  In total, the station is not more than 150 people, among them IT professionals make up only a small part.  The work of specialists is designed so that most of them do not hibernate at the station, the WIPAC IT team (Wisconsin IceCube Particle Astrophysics Center), which serves the data center, is located on a plot of ice covered with a few kilometers thick ice only in the summer months.  For the period of the short polar summer, the technicians fulfill all the plans, after which they leave the white continent. <br><br><img src="https://habrastorage.org/files/870/c1f/3ef/870c1f3ef07d459aa2ebc1c17abf647b.jpg"><br><br>  The rest of the year, the station‚Äôs infrastructure is managed remotely from the University of Wisconsin.  That part of the IT team, which remains for the winter, is usually chosen not so much for its intellectual capabilities, as for physical ones.  ‚ÄúThe main skill that the wintering officer should master is the use of satellite phone,‚Äù jokes Ralf Auer, IT manager at the polar station.  ‚ÄúWhen an emergency situation arises, our task is to collect as much information as possible and solve the problem remotely,‚Äù Ralph says without a smirk. <br><br>  ‚ÄúThe wintering team is responsible for the‚Äú iron ‚Äùcomponent of the complex to support the data center equipment in working condition, however, the routine IT work also does not avoid them,‚Äù says Barnet.  The result of this streamlined system is that Auer, Barnet, and other team members get a complete understanding of what is happening to them far from the equipment.  After analyzing their data, hibernating team members get clear guidelines for the actions they need to take.  ‚ÄúAll this can be compared with the situation in which experienced air traffic controllers from the radio room would control the flight of the aircraft, on board of which only flight attendants from the crew were left‚Äù, notes Barnet with irony.  As follows from the above, in order to simplify the already difficult stay of the University staff in the arms of the Antarctic winter, the main work on maintaining the data center is carried out at a distance, and this is done through all available communication channels. <br><br>  ‚ÄúThe polar station may receive additional support from IT specialists in case of emergency, but these people no longer belong to the staff of our university.  In the summer peaks of activity of university staff at the station no more than 5-8 people, in the winter period no more than 4-5, ‚Äùsays Gitt.  ‚ÄúHelp can come from McMurdo Station, which is the main base of support for the polar observatory.  In the peaks of the summer season, there may be up to 35-40 scientists involved in various projects, ‚ÄùGitt continued. <br><br><img src="https://habrastorage.org/files/ec0/ab3/7d0/ec0ab37d0c1d41bca36ec6a07e4891eb.jpg"><br><br><h4>  Connection </h4><br>  The most reliable communication channel available at such an observatory's distance from any civilization was satellite Internet.  The Iridium satellite network has become the connecting thread on which such a heavy burden of responsibility has hung.  The data transfer rate of such a connection is only 2400 bits per second.  The data transfer process was extremely optimized, the maximum number of connections, including multiplex connections to the Iridium system, was reserved on the satellite, and the best compression technique for the transmitted information minimizes its volume. <br><br><img src="https://habrastorage.org/files/cf2/a05/2c6/cf2a052c6cf34a6590b8e1b2deddd206.jpg"><br><br>  ‚ÄúSometimes, the communication channel for data exchange with our polar data center is served by regular sleds that are attached to the tractor.  Carefully packed data carriers are transported to our support base, ‚Äùsays Gitt, who has repeatedly participated in organizing caravans ensuring that they are laid across a frozen continent. <br><br>  The most high-speed access to the worldwide network is the system provided by NASA - the Tracking and Data Relay Satellite System (TDRSS).  The data transfer rate in it is 150 Mbps.  The main problem of this communication channel is its only partial daily availability, which is only 10 hours.  Also located at the station at the South Pole for 8 hours, the former meteorological satellite launched back in 1978 is available, but its data transfer rate is only 1 Mbit / s. <br><br><h4>  Physical inaccessibility of the data center </h4><br>  Delivery of personnel to the station and ensuring the vital activity of personnel is one of the most difficult tasks.  ‚ÄúThe specificity that we need to take into account is that the station is closed, and moreover, due to climatic conditions, it is physically not available from March to October,‚Äù says Barnet.  - ‚ÄúAny work that we need to do for the correct functioning of the infrastructure, we can only do from November to January.  And this requires perfect logistics planning.  Since even the polar summer weather component has always been decisive for those places, even with the most excellent plan for optimizing and branching IT infrastructure, you will sit on a chair waiting for the end of the storm, because the necessary cargo can not corny. ‚Äù <br><br>  Located at Amundsen-Scott station, the observatory with its data center serves as the final point of the supply route, 15,000 km long.  All that is sent to the station from the United States enters Christchurch (New Zealand), after which it is loaded on board the C-17 airship, specially equipped for landing on ice, which keeps its way to the McMurdo polar station.  At the station, the equipment is transferred to an LC-130 aircraft, specially equipped with a ski landing gear and an accelerator on a missile rocket, and the last 1,250 kilometers are already covered on it. <br><br><img src="https://habrastorage.org/files/88a/5ed/c5a/88a5edc5a13f4d91af15322ce0d05c95.jpg"><br><br>  The consequence of this removal of the data center from civilization has become the need to always have in stock a good reserve of spare parts.  ‚ÄúIf your hard drive fails, or the network card stops responding, there is no guarantee that a new unit will be delivered to you from the‚Äú greater land ‚Äùfor a week or two, which is why reserve the station for the entire IT infrastructure is vital‚Äù - especially noted Barnet. <br><br>  ‚ÄúThe regular delivery time of the equipment that has failed in the summer season to our polar station is 30 days, but even such deadlines do not always comply,‚Äù said Barnet. <br><br><h4>  The problem of heat server racks </h4><br>  It would seem that there should be no problems with the cooling of the servers at the pole, but even here, not all glory to God.  Cooling in the coldest place on Earth is a really big problem.  ‚ÄúIn such a harsh environment, you must be very careful when operating the system to remove heat from the server racks,‚Äù says Gitt.  ‚ÄúThe intake of cold air comes straight from the environment.  There were such days when we simply could not open the valves of the ventilation system, cooling the data center of the system, they were completely covered with ice.  Other situations could have caused the equipment to overheat, as the environment was too hot, while the air conditioning system was designed to work at much lower temperatures. ‚Äù <br><br><img src="https://habrastorage.org/files/48b/1eb/61b/48b1eb61b2b649c1b58c8b38c7024a3c.jpg"><br><br>  ‚Äú150 cars produce quite a lot of heat, and in order to get rid of it, you can‚Äôt just open the door to the street, the equipment will be damaged in minutes by a huge temperature drop.‚Äù <br><br>  What would happen to this, temperature control in the data center should be very flexible.  It was simply impossible to get a ready-made solution of the ventilation system for such latitudes, the designers did many things at their own peril and risk, and after the system was installed, it was repeatedly refined.  There were many problems in the most unexpected directions.  ‚ÄúAfter replacing part of the servers in the racks to more compact ones, the air flow changed, which cooled the equipment, as a result - we had to change the layout of all the racks somewhat,‚Äù Barnet mentioned. <br><br>  To get in the room +18 Celsius, when outside can rage from -40 to -75, we must try very hard.  The Ice Cuba team manages the flow of air into the room by controlling the amount of incoming air through the air intake, which periodically freezes, which creates major problems for data center employees. <br><br><img src="https://habrastorage.org/files/240/332/7a8/2403327a81d54c06add5aec249667602.jpg"><br><br><h4>  Antarctica - the continent of extremes </h4><br>  Another difficulty in the functioning of the data center in the polar Antarctic environment was humidity, or rather its absence.  The air at the south pole is extremely dry, the low temperature and the remoteness of the base almost a thousand kilometers from the nearest coast and led to this effect.  The overdried air is not only negative for people, it also does not always have a positive effect on equipment.  For film media, dry air has become a real disaster.  After the failure of network equipment due to excessively intensive wear of the insulating parts of the boards, it was decided to specially moisten the air in some rooms.  According to reviews of employees who are on duty at the station, this changed the situation for the better. <br><br>  Another problem for the data center was power supply.  There are no power stations at the South Pole, there are no redundant power lines.  All that employees of the polar data center can operate with are two generators that, in addition to powering the data center, feed the entire base, which, respectively, carries additional risks.  Switching between generators takes time.  And to predict this time is quite difficult. <br><br>  Problems of functioning of the data center at every step.  Even the fact that access to the data center premises during the summer season is physically not always possible, says a lot.  The server room is a separate building at the polar station, and in order to get from the residential modules to it, you need to overcome a short distance down the street, and natural conditions do not always allow this, and employees have to wait several days for the weather to improve. <br><br><img src="https://habrastorage.org/files/1c9/c57/f25/1c9c57f259e14ab39e676bdddef5b0fd.jpg"><br><br>  But, despite all the difficulties that people working at the station face every day, and maybe even thanks to them, the members of the Ice Cube team, Auer and Barnet, think that their work in ensuring the performance of the southernmost data center is really cool. .  ‚ÄúWhen you tell a person who is knowledgeable in IT that you are going to launch a data center with about 150 servers at the South Pole, while the latter‚Äôs uptime is maintained at 99.5%, that‚Äôs just great,‚Äù Auer said. </div><p>Source: <a href="https://habr.com/ru/post/244327/">https://habr.com/ru/post/244327/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../244315/index.html">Connect ("Russia") seminars in Moscow - for those who want to understand what has happened over the past couple of months in the Microsoft ecosystem</a></li>
<li><a href="../244317/index.html">Useful and pleasant for the developer in Mojolicious</a></li>
<li><a href="../244319/index.html">Modeling an object as a whole and as a composition</a></li>
<li><a href="../244321/index.html">Asternic CDR Reports. Listening to calls in FreePBX with access restrictions</a></li>
<li><a href="../244325/index.html">10 languages ‚Äã‚Äãinto which to translate your mobile game</a></li>
<li><a href="../244329/index.html">Arachnidium self-made framework for testing web and mobile applications. Part 2. A bit about the settings</a></li>
<li><a href="../244331/index.html">Is there a future for bid managers in contextual advertising?</a></li>
<li><a href="../244337/index.html">Validation and verification of system requirements</a></li>
<li><a href="../244339/index.html">All-over-IP Forum in Moscow: reporting and impressions</a></li>
<li><a href="../244343/index.html">Future [absence] of Yandex browser interfaces</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>