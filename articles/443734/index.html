<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Neural networks have a surprisingly simple image classification strategy.</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Convolutional neural networks do an excellent job with the classification of distorted images, unlike people 


 In this article, I will show why adva...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Neural networks have a surprisingly simple image classification strategy.</h1><div class="post__text post__text-html js-mediator-article"><h3>  Convolutional neural networks do an excellent job with the classification of distorted images, unlike people </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/2e6/a7c/4c8/2e6a7c4c8f0ba811e57fa24193375289.jpg"><br><br>  In this article, I will show why advanced deep neural networks can perfectly recognize distorted images and how it helps to uncover the surprisingly simple strategy used by neural networks to classify natural photos.  These discoveries, <a href="https://openreview.net/pdf%3Fid%3DSkfMWhAqYQ">published</a> in ICLR 2019, have many implications: first, they demonstrate that it is much easier to find an ‚Äú <a href="https://ru.wikipedia.org/wiki/ImageNet">ImageNet</a> ‚Äù solution than was thought.  Secondly, they help us create more interpretable and understandable systems for classifying images.  Third, they explain several phenomena observed in modern convolutional neural networks (SNA), for example, their tendency to search for textures (see our other <a href="https://openreview.net/forum%3Fid%3DBygh9j09KX">work</a> in ICLR 2019 and the corresponding <a href="https://blog.usejournal.com/why-deep-learning-works-differently-than-we-thought-ec28823bdbc">blog entry</a> ) and ignoring the spatial arrangement of parts of an object. <br><a name="habracut"></a><br><h2>  Good old models "bag of words" </h2><br>  In the good old days, before the emergence of deep learning, the recognition of natural images was quite simple: we define a set of key visual features (‚Äúwords‚Äù), determine how often each visual feature is found in an image (‚Äúbag‚Äù), and classify an image based on these numbers  Therefore, such models in computer vision are called ‚Äúbag of words‚Äù (bag-of-words or BoW).  For example, suppose that we have two visual features, the human eye and the pen, and we want to classify images into two classes, ‚Äúpeople‚Äù and ‚Äúbirds‚Äù.  The simplest BoW model would be this: for each eye found on the image, we increase the evidence in favor of the ‚Äúperson‚Äù by 1. Conversely, for each feather, we increase the evidence in favor of the ‚Äúbird‚Äù by 1. Which class is gaining more evidence, so it will be. <br><br>  A convenient feature of such a simple BoW model is the interpretability and clarity of the decision-making process: we can definitely check which features of the image speak in favor of a particular class, the spatial integration of features is very simple (compared to the nonlinear integration of features in deep neural networks) just understand how the model makes its decisions. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Traditional BoW models were extremely popular and worked well before the onslaught of deep learning, but quickly went out of fashion due to their relatively low efficiency.  But are we sure that neural networks use a fundamentally different decision-making strategy from BoW? <br><br><h2>  Depth Interpretable Network with Features Bag (BagNet) </h2><br>  To test this assumption, let us combine the interpretability and clarity of BoW models with the effectiveness of neural networks.  The strategy looks like this: <br><ul><li>  Divide the image into small pieces qx q. </li><li>  We skip the pieces through the neural network to get evidence of class membership (logits) for each piece. </li><li>  We summarize the evidence for all the pieces to obtain a solution at the level of the entire image. </li></ul><br><br><img src="https://habrastorage.org/getpro/habr/post_images/474/6f5/363/4746f53632bd9e3e7477de9ecb76d396.png"><br><br>  To implement such a strategy in the simplest way, we take the standard ResNet-50 architecture and replace almost all 3x3 convolutions with 1x1 convolutions.  As a result, each hidden element in the last convolutional layer ‚Äúsees‚Äù only a small part of the picture (that is, their field of perception is much smaller than the size of the image).  So we avoid the imposed markup of the image and come as close as possible to the standard SNA, while applying a pre-planned strategy.  We call the resulting BagNet-q architecture, where q denotes the size of the perceptual field of the uppermost layer (we tested the model with q = 9, 17, and 33).  BagNet-q works about 2.5 longer than ResNet-50. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b5e/4b6/1c4/b5e4b61c431e103ac9a6ec3fe4b02894.jpg"><br><br>  The efficiency of BagNet on the data from the ImageNet database is impressive even when using small pieces: 17 √ó 17 pixels are enough to reach the efficiency of AlexNet, and 33 √ó 33 pixels are enough to reach 87% accuracy by entering top-5.  Efficiency can be increased by placing the 3x3 convolutions more carefully and adjusting the hyperparameters. <br><br>  This is our first main result: ImageNet can be solved using only a set of small features of images.  Distant spatial relationships of parts of a composition, such as the shape of objects or the interaction between parts of an object, can be completely ignored;  they are completely unnecessary to solve the problem. <br><br>  BagNet's remarkable feature is the transparency of their decision making system.  For example, you can find out what features of the images will be the most characteristic for a given class.  For example, tench, a large fish, is usually recognized by the image of fingers on a green background.  Why?  Because most of the photos in this category are fisherman, holding a tench as his trophy.  And when BagNet incorrectly recognizes the image as a line, it usually happens because somewhere in the photo there are fingers on a green background. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/175/197/727/175197727cc15da939117f5c2502d82c.jpg"><br>  <i>The most characteristic parts of the images.</i>  <i>The top row in each cell corresponds to the correct recognition, and the bottom one - to distracting fragments that led to incorrect recognition.</i> <br><br>  We also get an accurate ‚Äúheat map‚Äù that shows which parts of the image contributed to the decision. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/89b/510/f5b/89b510f5b5d816a2a1d8590f4487abf6.jpg"><br>  <i>Heat maps are not approximations, they accurately show the contribution of each part of the image.</i> <br><br>  BagNet's demonstrate that it is possible to obtain high accuracy of working with ImageNet only on the basis of weak statistical correlations between local features of images and the category of objects.  If this is enough, then why would standard neural networks like ResNet-50 explore something fundamentally different?  Why would ResNet-50 study complex large-scale relationships of the type of object's shape, if the abundance of local image features is enough to solve the problem? <br><br>  To test the hypothesis that modern SNS adhere to a strategy similar to the work of the simplest BoW networks, we tested different networks ‚Äî ResNet, DenseNet, and VGG on the following ‚Äúsigns‚Äù of BagNet: <br><ul><li>  The solutions are independent of the spatial shuffling of the image features (this can only be verified on VGG models). </li><li>  Modifications of different parts of the image should not depend on each other (in the sense of their influence on class membership). </li><li>  Errors made by standard SNS and BagNet should be similar. </li><li>  Standard SNA and BagNet should be sensitive to similar features. </li></ul><br><br>  In all four experiments, we found a surprisingly similar behavior of the SNS and BagNet.  For example, in the last experiment, we show that BagNet is most sensitive (if they, for example, overlap) to the same image locations as the SNS.  In fact, heat maps (spatial sensitivity maps) of BagNet better predict the sensitivity of DenseNet-169 than heat maps obtained by such attribution methods like DeepLift (counting heat maps for DenseNet-169 directly).  Of course, the SNA does not exactly repeat the behavior of BagNet, but some deviations demonstrate.  In particular, the deeper the networks become, the larger the dimensions of the features become and the further the dependencies extend.  Therefore, deep neural networks are indeed an improvement compared to BagNet models, but I do not think that the basis of their classification is somehow changing. <br><br><h2>  We go beyond the classification of BoW </h2><br>  Observing the decision making of the SNA in the style of BoW strategies can explain some of the strange features of the SNA.  First, it explains why the SNS is so strongly <a href="https://openreview.net/forum%3Fid%3DBygh9j09KX">tied to textures</a> .  Secondly, why the SNS is not sensitive to the <a href="https://www.sciencedirect.com/science/article/pii/S095943881730065X">mixing of</a> parts of the image.  This may even explain the existence of competitive stickers and competitive perturbations: confusing signals can be placed anywhere in the image, and the SNS will still certainly catch this signal, regardless of whether it fits the rest of the image. <br><br>  In fact, our work shows that when recognizing images, the SNA use a lot of weak statistical laws and do not go on to integrate parts of the image at the object level, as people do.  The same is likely true for other tasks and sensory modalities. <br><br>  We need to carefully plan our architectures, tasks, and training methods to overcome the tendency to use weak statistical correlations.  One approach is to translate the distortion of SNA learning from small local features to more global ones.  The other is to remove or replace those features that the neural network should not rely on, which we did in another <a href="https://openreview.net/forum%3Fid%3DBygh9j09KX">publication</a> for ICLR 2019, using style transfer preprocessing to eliminate the texture of the natural object. <br><br>  One of the biggest problems, however, remains the classification of images: if there are enough local features, there is no incentive to study the real "physics" of the natural world.  We need to restructure the task so as to encourage the models to study the physical nature of objects.  To do this, you will most likely have to go beyond purely observational learning to the correlation of input and output data so that models can extract cause-and-effect relationships. <br><br>  Together, our results suggest that the SNS can follow an extremely simple classification strategy.  The fact that such a discovery can be made in 2019 emphasizes how little we still understand the internal features of the work of the deep neural networks.  Lack of understanding does not allow us to develop fundamentally improved models and architectures that reduce the gap between human and machine perception.  Deepening understanding will allow us to discover ways to narrow this gap.  This can be extremely useful: trying to move the SNA towards the physical properties of objects, we suddenly achieved <a href="https://openreview.net/forum%3Fid%3DBygh9j09KX">resistance to the noise of the</a> human level.  I look forward to the emergence of a large number of other interesting results on our way to the development of the SNA, truly comprehending the physical and causal nature of our world. </div><p>Source: <a href="https://habr.com/ru/post/443734/">https://habr.com/ru/post/443734/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../443718/index.html">Fantasy game with 300 thousand races</a></li>
<li><a href="../443722/index.html">Design and naming of queues</a></li>
<li><a href="../443726/index.html">Features settings Palo Alto Networks: SSL VPN</a></li>
<li><a href="../443728/index.html">Google Plus stops working. So what?</a></li>
<li><a href="../443730/index.html">Ctrl-Alt-Del: scheduled obsolescence of programmers</a></li>
<li><a href="../443736/index.html">Setting up from scratch the Network UPS Tools UPS Management Service (NUT) to manage a locally connected UPS</a></li>
<li><a href="../443740/index.html">The first release of an open source product test testing tool.</a></li>
<li><a href="../443746/index.html">The gaming market, trends and forecasts - a great analytics from App Annie</a></li>
<li><a href="../443748/index.html">Presentation of Tesla Model Y - what to expect and where to look</a></li>
<li><a href="../443752/index.html">Kotlin as the future of Android application development</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>