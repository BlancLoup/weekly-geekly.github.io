<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Heading "We read articles for you." October - November 2017</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! By tradition, we present to your attention a dozen reviews of research papers from members of the Open Data Science community from the #arti...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Heading "We read articles for you." October - November 2017</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/6l/28/-z/6l28-ziaduu_9kfqn22_qexv5ds.png"></p><br><p>  Hi, Habr!  By tradition, we present to your attention a dozen reviews of research papers from members of the Open Data Science community from the #article_essense channel.  Want to get them before everyone else - join the <a href="http://ods.ai/">ODS</a> community! </p><br><p>  Articles are selected either from personal interest, or because of the proximity to the ongoing competition.  We remind you that the descriptions of the articles are given without changes and in the form in which the authors posted them in the #article_essence channel.  If you want to offer your article or you have any suggestions - just write in the comments and we will try to take everything into account in the future. </p><br><p>  Articles for today: </p><a name="habracut"></a><br><ol><li>  <a href="https://habr.com/ru/company/ods/blog/343822/">Combining Number Of Neural Networks Into One</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/343822/">AutoEncoder by Forest</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/343822/">Name-Entity Recognition</a> <em>(NLP) of the</em> <a href="https://habr.com/ru/company/ods/blog/343822/">Hybrid LSTM-CRF</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/343822/">Densely Connected Convolutional Networks</a> <em>(CV)</em> </li><li>  <a href="https://habr.com/ru/company/ods/blog/343822/">Dual Path Networks</a> <em>(CV)</em> </li><li>  <a href="https://habr.com/ru/company/ods/blog/343822/">A Large Self-Annotated Corpus for Sarcasm</a> <em>(NLP)</em> </li><li>  <a href="https://habr.com/ru/company/ods/blog/343822/">Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms</a> <em>(CV)</em> </li><li>  <a href="https://habr.com/ru/company/ods/blog/343822/">Dynamic Routing Between Capsules</a> <em>(Hinton's capsules)</em> </li><li>  <a href="https://habr.com/ru/company/ods/blog/343822/">DeepXplore: Automated Whitebox Testing of Deep Learning Systems</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/343822/">One-Shot Learning for Semantic Segmentation</a> <em>(CV)</em> </li><li>  <a href="https://habr.com/ru/company/ods/blog/343822/">Field-aware Factorization Machines in a Real-World Online Advertising System</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/343822/">The Marginal Value of Adaptive Gradient Methods in Machine Learning</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/343822/">FractalNet: Ultra-Deep Neural Networks without Residuals</a> <em>(CV)</em> </li></ol><br><div class="spoiler">  <b class="spoiler_title">Links to past collections of the series:</b> <div class="spoiler_text"><ul><li>  <a href="https://habrahabr.ru/company/ods/blog/339094/">September 2017</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/336624/">August 2017</a> </li></ul></div></div><br><hr><br><h3 id="1-combining-infinity-number-of-neural-networks-into-one">  1. Combining Infinity Number Of Neural Networks Into One </h3><br><p>  Article authors: Bo Tian, ‚Äã‚Äã2016 <br>  <a href="">‚Üí Original article</a> <br>  Review author: <a href="https://habrahabr.ru/users/belerafonl/" class="user_link">BelerafonL</a> </p><br><p>  The idea is to add Gaussian noise to the weights of the neurons to get something like a drop-out.  Only if the dropout usually acts on the outputs of the nodes, then the noise is added to the weights.  But this is not the main thing, but the fact that the author finds an analytical solution for network output with an endless ensemble of such networks with a different combination of noise samples in the scales.  Those.  The desired noise level of the scale (sigma) is specified, and the solution found shows what the network would produce if it were to sample the output of such a noisy network for a long time and to average the result. </p><br><p>  The author for the problem of separating two spirals on a plane shows how the solution of a conventional network differs from that.  He draws an activation card, and for a normal network, and it turns out to be rugged and torn, and to solve it, it is smooth and exactly follows the shape of the spirals. <br>  Unfortunately, the author does not check the proposed method for other tasks and uses the network with only one hidden layer.  In addition, he uses the LMA optimization method (Levenberg ‚Äì Marquardt Algorithm), but says that you can use any other, including the usual backprop. </p><br><p>  The article‚Äôs premise is simple - an endless ensemble of networks differing by the amount of noise in the scales perfectly generalizes and finds the best minimum, besides, the noise level of the scales can be changed as the network is trained, and then the desired accuracy / generalization ratio can be found.  And since the solution is found analytical, the computational cost of this is almost none. </p><br><p>  There are a lot of formulas in the article, why I did not understand the applicability of the method for practical complex problems, are there any pitfalls.  Therefore, I ask more experienced researchers to look at and comment on the article, it is painfully beautiful to get the result of such regularization. </p><br><p>  Also there is an author's note on <a href="https://stats.stackexchange.com/a/249243">stackexchange</a> where he <a href="https://stats.stackexchange.com/a/249243">briefly</a> explains the concept. </p><br><hr><br><h3 id="2-autoencoder-by-forest">  2. AutoEncoder by Forest </h3><br><p>  Article authors: Zhou Z, Feng J, 2017 <br>  <a href="http://arxiv.org/abs/1709.09018">‚Üí Original article</a> <br>  <a href="https://habrahabr.ru/users/dumbris/" class="user_link">Reviewer</a> : <a href="https://habrahabr.ru/users/dumbris/" class="user_link">Dumbris</a> </p><br><p>  In the article AutoEncoder by Forest, the authors of Zhou Z, Feng J proposed an alternative method of building auto-encoders based on an ensemble (Random Forest, Gradient boosted tree) of trees.  They called the proposed method <em>eForest</em> . </p><br><p>  According to the authors' experiments, eForest shows the best results on the MNIST and CIFAR-10 tasks, in comparison with auto-encoders built on the basis of the Multilayer Perceptron and Convolutional Neural Network.  And in the task of recovering text on the IMDB dataset, eForest exceeded MLP accuracy 200 times. </p><br><p><img src="https://habrastorage.org/webt/s0/8b/qp/s08bqp_-0upps-s8grz2egz5vv8.png"></p><br><p>  <em>The main advantages of the eForest method:</em> </p><br><ul><li>  <em>Accuracy</em> : reconstruction error is lower than that of NN-based auto-encoders </li><li>  <em>Efficiency</em> : learning on a multi-core CPU is faster than NN using a GPU (but, while the decoding phase in eForest is slow) </li><li>  <em>damage-tolerable</em> - the model works upon partially erasing the encoded representation.  In experiments with the text even for the remaining 10%, the model was able to show an acceptable result. </li><li>  <em>reusable</em> eForest-trained auto-encoders can be used on other data from the same domain domain. </li></ul><br><p>  <em>How does the proposed method work</em> </p><br><p> The autoencoder has two main functions encoding and decoding. <br>  <em>Coding</em>  To obtain an encoded representation, you need to construct a forest <code>F</code> trees <code>T</code> and save the indexes of sheets from all trees for each <code>x</code> .  Thus, for input <code>x</code> we obtain a vector of long T. </p><br><p><img src="https://habrastorage.org/webt/0g/jy/pf/0gjypffi4mybe0utjfdoypoekqe.png"></p><br><p>  <em>Decoding</em> - having a forest <code>F</code> and sheet indexes for all <code>x</code> you can restore the path in the tree from sheet to top.  The authors write this way in the form of a conjunction of logical expressions taken from the nodes of the decision tree.  Example <code>(x1=&gt;0)^(x2=&gt;5)^:(x3==RED)^:(x1&gt;2:7)^:(x4 == NO)</code> .  Such an expression they call RULE. </p><br><p>  Many RULEs for a particular sample from all <code>T</code> trees can be simplified, to one rule, which Zhou Z, Feng J call: Maximal-Compatible Rule (MCR).  The authors claim, in the form of a theorem, that all original samples will be located within the regions defined by the MCR. </p><br><p>  According to the rules recorded in the MCR, you can generate data that will be similar to the original.  The article has a description of the algorithms in pseudo-code format. </p><br><p>  Judging from Table 4, the procedure for constructing MCR and decoding takes much longer, compared to the decoding phase in NN.  But the authors hope to optimize this moment in the future. </p><br><p>  At the beginning of the year, there was a more general paper from these same authors: <a href="http://arxiv.org/abs/1702.08835">Deep Forest: Towards</a> . </p><br><hr><br><h3 id="3-application-of-a-hybrid-bi-lstm-crf-model-to-the-task-of-russian-named-entity-recognition">  3. Application of a Hybrid Bi-LSTM-CRF Named Entity Recognition </h3><br><p>  Authors of the article: Anh LT Arkhipov M. Y, Burtsev M. S, 2017 <br>  <a href="http://arxiv.org/abs/1709.09686">‚Üí Original article</a> <br>  <a href="https://habrahabr.ru/users/dumbris/" class="user_link">Reviewer</a> : <a href="https://habrahabr.ru/users/dumbris/" class="user_link">Dumbris</a> </p><br><p>  The first publication in the project <em>iPavlov</em> .  According to the authors - <abbr title="State of the art">SOTA</abbr> for <abbr title="Named Entity Recognition">NER</abbr> in Russian texts. </p><br><p>  <em>Datasets</em> </p><br><p>  For the experiments, Gareev's datasets (top cited articles from Yandex News), Person-1000 and FactRuEval were used.  As a baseline, a <em>Bi-LSTM network</em> was taken <em>with a Conditional Random Fields (CRF)</em> layer.  The use of CRF gives a significant increase in accuracy compared to pure Bi-LSTM on the NER task. </p><br><p>  <em>Network architecture, experiments</em> </p><br><p>  At the input for each word its word level and char level representation is calculated, both representations are concatenated into one vector and fed to the input Bi-LSTM.  After Bi-LSTM, the CRF layer assigns to each word a tag by which it can be determined whether the word is a person‚Äôs name or an organization‚Äôs name. </p><br><p><img src="https://habrastorage.org/webt/ca/lz/zj/calzzjqj7aoa25dnqfytbbg1mi0.png"></p><br><p>  During the experiments, extensions of the well-known <a href="http://neuroner.com/">NeuroNER</a> architecture were also investigated using the highway network.  With the <em>highway network, the</em> network gets the opportunity to learn to ‚Äúturn off‚Äù some of the inner layers (carry gate), skipping low-level views of the input data closer to the output layers.  This mechanism is implemented using the sigmoid layer.  It turns out that the network itself can manage the complexity of the architecture, depending on the input data. </p><br><p><img src="https://habrastorage.org/webt/jc/xi/x2/jcxix2wzgpnjuzsl2emhpjmpvag.png"></p><br><p>  However, in practice it was found that the use of properly prepared word embeddings gives an increase in accuracy in combination with Bi-LSTM-CRF.  For training embeddings, News by Kutuzov and Lenta corps were used. </p><br><p>  <em>results</em> </p><br><p>  The most accurate was the model Bi-LSTM-CRF + word embedding, created on the basis of Russian news corpses.  The combination of NeuroNER + highway network also performed well.  One of the promising directions is the use of <em>character level CNN</em> architecture, instead of LSTM.  In the framework of this work it has not been investigated. </p><br><hr><br><h3 id="4-densely-connected-convolutional-networks">  4. Densely Connected Convolutional Networks </h3><br><p>  Authors: Gao Huang, Zhuang Liu, Kilian Q. Weinberger, Laurens van der Maaten, 2016 <br>  <a href="https://arxiv.org/abs/1608.06993">‚Üí Original article</a> <br>  <a href="https://habrahabr.ru/users/n01z3/" class="user_link">Reviewed by</a> : <a href="https://habrahabr.ru/users/n01z3/" class="user_link">N01Z3</a> </p><br><p>  A relatively fresh idea of ‚Äã‚Äãthe architecture, which passed somehow unnoticed, but showed in <a href="http://ods.ai/">our Chata the</a> best single model performance on <a href="https://www.kaggle.com/c/planet-understanding-the-amazon-from-space">Kaggle Amazon from Space</a> . </p><br><p><img src="https://habrastorage.org/webt/kq/my/ee/kqmyeesltw5k-vfd8pgj9w8jqgo.png"></p><br><p>  So the guys offered the Dense Block.  Each base element is BN -&gt; ReLU -&gt; Conv.  And each basic element prokidyvaet its features to all the following elements in the block.  Features are concatenated, not aggregated, so the number of channels grows linearly in the number of filters (the authors call this Growth rate).  But this is offset by the fact that the basic elements themselves are narrower.  Also, in order to reduce the increase in the number of channels after each Dense Block, a basic element is inserted with convolutions (1, 1) and the number of filters is two times less than the channels at this point in the main branch.  And then the usual Pooling with Stride 2. </p><br><p>  Everything.  And then waving hands goes on, that the features are more competently re-used, that due to this, everything is effectively considered.  Even in this case, the dropout in convolutions looks somehow more meaningful.  Also in the following papers, the guys showed that it is possible to get an evaluation classification already after the first blocks and thus not to drive the inference further in the case of confident predictions. </p><br><hr><br><h3 id="5-dual-path-networks">  5. Dual Path Networks </h3><br><p>  Authors of the article: Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan, Jiashi Feng, 2017 <br>  <a href="https://arxiv.org/abs/1707.01629">‚Üí Original Article</a> <a href="https://github.com/cypw/DPNs">‚Üí Source Code</a> <br>  <a href="https://habrahabr.ru/users/n01z3/" class="user_link">Reviewed by</a> : <a href="https://habrahabr.ru/users/n01z3/" class="user_link">N01Z3</a> </p><br><p>  Another rethinking of the connection between convolutional and recurrent networks.  For starters, the authors liked Resnet and Densnet.  The first is the essence of the recurrent network deployed along the main brunch, and the second is the high order recurrent sit (HORNN) (a series of pictures 1).  And the guys decided to merge both ideas into one.  This is how DPN was born. </p><br><p>  To understand how they combined, a series of pictures 2 </p><br><p><img src="https://habrastorage.org/webt/cc/3_/j8/cc3_j8t_uzbgt9gjccdyuerqgqo.png"></p><br><p>  a) Classic Resnet - fixed-size main brunch, features added <br>  b) normal Densenet - the main brunch grows, features are konkatiniruyutsya <br>  c) conventional Densenet - equivalent circuit.  Rethinking in the sense that after all there is a fixed-size basic brunch from the entrance to the DenseBlock and the auxiliary one that is growing. <br>  d) the latest Dual Path block - there is a fixed-size brunch, there is a brunch that is growing.  Features are taken from each, run through a layer of convolutions, are separated and docked in each brunch.  In the fixed brunch - folded, in the growing - concatenated <br>  e) the latest Dual Path block - equivalent Dual Path circuit. </p><br><p>  Each input and output block uses convolutions (1.1) to match the channels.  And the main convolutions (3.3) are used not ordinary, but Separable.  That is, in fact, not Resnet, but ResNeXt is taken as the base. </p><br><hr><br><h3 id="6-a-large-self-annotated-corpus-for-sarcasm">  6. A Large Self-Annotated Corpus for Sarcasm </h3><br><p>  Authors of the article: Mikhail Khodak, Nikunj Saunshi, Kiran Vodrahalli, 2017 <br>  <a href="https://arxiv.org/abs/1704.05579">‚Üí Original article</a> <br>  <a href="https://habrahabr.ru/users/yorko/" class="user_link">Reviewed by</a> : <a href="https://habrahabr.ru/users/yorko/" class="user_link">yorko</a> </p><br><p>  Recognizing sarcasm is a very non-trivial task, which is not something that grids, and people do not always cope.  If a person with his 10 ^ 10 neurons sometimes asks, ‚ÄúWait a minute ... and you don‚Äôt troll me by chance?‚Äù, Then you can imagine that recognizing sarcasm is really one of the greatest machine learning challenges. </p><br><p>  Dudes have marked the biggest currently dataset on sarcasm - 1.3 million sarcastic statements with Reddit.  In addition, there are still half a billion non-sarcastic phrases lying around, so that you can practice imbalanced classification at your leisure. </p><br><p>  The guys immediately stated that their main contribution is dataset, and not the methodology for identifying sarcasm.  But dataset an order of magnitude more previous and much better, according to them, than from Twitter, where people put the tags #sarcasm or #irony themselves.  On Reddite, users also notice that they are tagged with the ‚Äú/ s‚Äù tag, and the authors acknowledge that this markup is also noisy. </p><br><p><img src="https://habrastorage.org/webt/kd/1_/np/kd1_npiquksizvrlyfp4nkyh3qa.png"></p><br><p>  It is curious that the dudes did not collect messages going to the thread <em>after</em> sarcastic approval.  Type there goes a trash-trolling, sarcasm respond to sarcasm, and the markup is too noisy turns out. </p><br><p>  The authors assessed the quality of the resulting markup by selecting 500 random sarcastic and ordinary messages and checking the markup on their own.  They fixed 1% False Positive Rate (when the regular message came with the label ‚Äúsarcasm‚Äù) and 2% False Negative.  In the case of balanced learning, these are norms, but if you study with a bunch of non-sarcasm (99.75%), then 2% False Negative Rate is a bit too much. </p><br><p>  I'd add: I figured the error matrix for the markup with Reddit and the assessor markup, with such an imbalance the accuracy (precision) of the markup with Reddit is very low: only 11% of cases assessors confirm sarcasm, if Reddit has come to be sarcasm.  This raises questions: why is it so much better than the rest, of the same Twitter?  The authors argue that it‚Äôs still better: reddit comes up with sarcasm much more often (~ 1% vs. 0.002% on Twitter) and generally the message on Reddite is obviously longer.  Well, ok ... persuaded </p><br><p>  Further, the authors report several baselines in the task of determining sarcasm on Reddit's data.  The task is formulated as follows: there is a message to read and all comments to it.  It is necessary to determine which of the comments carry sarcasm.  8.4 million such messages were collected (as I understood, the object is the original message + comments to it), sarcasm - 28% (balanced the sample).  Then they took one message from the thread with or without sarcasm, and on the basis of GloVe-embeddings, sarcastic phrases too similar to each other (avalanche-like yeast throwing shit) were cut off. </p><br><p>  Three logistic regressions were built, the signs are very simple: Bag of Words, Bag of bigrams and GloVe-embeddings (Amazon product corpus, dimension - 1600, document presentation (messages) - simple averaging of individual words).  They also planted poor assessors to mark sarcasm - there were 5 people, they marked out 100 messages, it would be a little bit difficult, it would be possible to strain a mechanical Turk (Amazon Mechanical Turk), because writing a scientific article is such a profitable business! </p><br><p><img src="https://habrastorage.org/webt/pz/t-/3u/pzt-3uxcktsfiexqpz6lmjwkywu.png"></p><br><p>  It turned out that the bigrams gave the best result among the three models (~ 76% accuracy in politics and 71% in the other subreddits), but the human marking is certainly better (83% and 82% on average, 85% and 92% ensemble 5 assessors, majority voting).  The authors conclude that embeddingdings played worse, but it is possible, with their help, we will learn to convey the context in which sarcasm is applied. </p><br><p>  In general, it is difficult for both people and algorithms to detect sarcasm.  I expect progress in this task on the part of neural networks - we must somehow learn to properly take into account the context in which sarcasm is used, that is, correctly encode this context into a hidden representation, as it is now done in conditioned recurrent meshes.  The experiments in this article are so-so, but the fact that the guys posted a big dataset of sarcasm is definitely great!  I'm afraid to imagine what will happen when cars really learn to feel sarcasm better than people. </p><br><hr><br><h3 id="7-fashion-mnist-a-novel-image-dataset-for-benchmarking-machine-learning-algorithms">  7. Fashion-MNIST: a Novel Image for Benchmarking Machine Learning Algorithms </h3><br><p>  Authors of the article: Han Xiao, Kashif Rasul, Roland Vollgraf, 2017 <br>  <a href="https://arxiv.org/abs/1708.07747">‚Üí Original article</a> <br>  Review author: Ivan Bendyna </p><br><p>  The guys from Zalando also burn from the phrase "state-of-the-art on MNIST", so they made their Fashion-mnist, which completely repeats the structure of the original MNIST.  It also has 10 classes (clothes and shoes: shirt, t-shirt, sneakers, ...), 28x28 pixels, 60,000 train, 10,000 test.  This is the main idea of ‚Äã‚Äãdataset - you can simply replace the URL and check the quality on a slightly more complex dataset.  <abbr title="State of the art">SOTA</abbr> top-1 error is about 3.5%, which is an order of magnitude larger than the error on MNIST. </p><br><p><img src="https://habrastorage.org/webt/yy/n2/mq/yyn2mq12fq4qyjisubistgfaq2q.png"></p><br><p>  There is a benchmark of the main classifiers from <a href="http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/">sklearn</a> , top results can be found in <a href="https://github.com/zalandoresearch/fashion-mnist">Readme.md</a> .  Also there you can find the result of a person who is only 0.835, which is not surprising in principle if you look at the pictures with your eyes. </p><br><p>  Dataset prepared for products from the site zalando, the procedure for creating dataset well described in the work.  Dataset is already in <a href="https://github.com/pytorch/vision">pytorch</a> . </p><br><p>  I also learned from the article that there is an extended dataset <a href="https://www.nist.gov/itl/iad/image-group/emnist-dataset">EMNIST</a> (62 classes of handwritten characters) </p><br><p>  My thoughts: </p><br><ul><li>  Pictures are made from photos of clothes, that is closer in 2d-structure to ordinary photos than handwritten text </li><li>  The main drawback of MNIST has not gone away - good methods may not show improvements on such small pictures. </li><li>  Probably, this dataset will not be able to reach accuracy 0.99, since the border between some classes (for example, shirt and t-shirt) is rather arbitrary, and if it exists, it can disappear with decreasing size and translation into b / w image </li><li>  It‚Äôs not a fact that it can be a replacement, but now all those who evaluated quality on MNIST can check it for fashion mnist for free (in terms of developer time) </li><li>  Also a good alternative for homework for students who usually do not have the capacity to run on ImageNet </li></ul><br><hr><br><h3 id="8-dynamic-routing-between-capsules">  8. Dynamic Routing Between Capsules </h3><br><p>  Authors: Sara Sabour, Nicholas Frosst, <em>Geoffrey E Hinton</em> , 2017 <br>  <a href="https://arxiv.org/abs/1710.09829">‚Üí Original article</a> <br>  Reviewer: Ruslan Grimov </p><br><p>  This article reveals the internal structure of the capsules and describes the routing mechanism - a way to transfer the output from the capsule in the current layer only to certain capsules in the upper layer.  They call this the construction of the Parse tree, where each node is associated with one capsule. </p><br><p>  Result of the article: building an encoder / decoder trained in MNIST and calling for studying the capsules further.  Encoder - CapsNet network, consists of a usual convolutional layer and two layers with capsules, one of which is convolutional (see below).  A decoder is just a fullconnected network.  The code supplied to the decoder is 10 capsules with 16 neurons each.  One by one. </p><br><p>  The capsule is a separate group of neurons on the layer.  They are not connected to each other inside the capsule, but the output of each neuron depends on the output of the others in the capsule (see formulas below).  A separate capsule is responsible for one object.  Each neuron in the capsule learns to represent some property of the object.  At the end of the article there will be a picture where they show how the shape of numbers changes as the properties change (it looks like these neurons really know something). </p><br><p>  Neurons are in the same position, but in different capsules of one layer they are responsible for different properties of objects (as far as I understood).  The very presence of an object on the scene is determined by the length of the output vector of the capsule (the root of the sum of the squares of neuron activities in the capsule).  Respectively.  if there are two identical objects on the stage, then the properties will be mixed. </p><br><p>  The dimension of the capsules grows from the input layer of the network to the output.  At the same time, information from the ‚Äúpositional‚Äù is transformed into a rate-coded (‚Äúrate-coded‚Äù).  Simple convolutional layers at the beginning of the network are considered as capsules having one neuron. </p><br><p>  <em>Capsules</em> </p><br><p>  In the article, everything is repelled by capsules.  Therefore, further all actions are described in relation to capsules, and not to individual neurons.  That is, not input / output of the neuron and weights between the neurons, but input / output and weights between the capsules are considered.  We simply mean that a capsule is a vector of neurons. </p><br><div class="spoiler">  <b class="spoiler_title">Formulas</b> <div class="spoiler_text"><p>  j is the capsule index in the current layer, i is in the underlying layer (the one closer to the entrance). <br>  Capsule input (vector of dimension m) s [j] = sum over all i from c [i, j] <em>U [i, j], where</em> <em><br></em>  <em>c [i, j] - a certain coefficient of connectedness (scalar) between the capsules of the two layers - the result of the routing operation (see below).</em>  <em>For a capsule of i, the sum over all j from c [i, j] = 1. That is, one capsule from the underlying layer distributes its ‚Äúinfluence‚Äù on the overlying unevenly.</em>  <em>Before the routing cycles, all c [i, j] are equal.</em> <em><br></em>  <em>U [i, j] - the article is called prediction (vector of dimension m) U [i, j] = W [i, j]</em> u [i], where W [i, j] is the weight of connections between the neurons of the capsules i and j ( dimension nxm), u [i] - output capsules i (vector dimension n).  U [i, j] will also be needed later for routing. <br>  Next, a nonlinear activation function is applied to s [j], which leads the vector s [j] to a vector v [j] of the same dimension but long less than 1 (this is the moment where the neurons of one capsule influence each other, otherwise special connections between by themselves inside the capsule they are not connected).  The elements of this vector after the last cycle of the routing are the outputs of the neurons of the capsule j. </p></div></div><br><p>  <em>Routing</em> </p><br><p>  The length of the output vector of the capsule is less than 1. And once the length of the output vector of the capsules is normalized, it is possible to find out which of the capsules of the overlying layer are more ‚Äúresponsive‚Äù and transfer data from the underlying capsule only to those capsules for which the inner product U [i, j] and v [j] more.  Actually this is the construction of the parsing tree (as I understood it). </p><br><p>  The parsing tree between the l and (l + 1) layers is built on the fly (again when submitting each image). <br>  To build this tree, special weights b [i, j] (scalar) between the capsules of the two layers are used (they are used in calculating that c [i, j] - coefficient of connectivity, but they are not permanently stored, when the new input to the network is reset to zero). </p><br><p>  c [i, j] are calculated as exp (b [i, j]) / (sum over k from exp (b [i, k])), where k is the number of groups in the overlying layer.  Those.  this is softmax. </p><br><div class="spoiler">  <b class="spoiler_title">Algorithm such</b> <div class="spoiler_text"><pre> <code class="hljs pgsql"><span class="hljs-number"><span class="hljs-number">1</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">procedure</span></span> ROUTING(U[i,j], r, l) //prediction (  ?), - ,   <span class="hljs-number"><span class="hljs-number">2</span></span>:         <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> <span class="hljs-keyword"><span class="hljs-keyword">all</span></span> capsule i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> layer l <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> capsule j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> layer (l + <span class="hljs-number"><span class="hljs-number">1</span></span>): b[i,j] = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">3</span></span>:         <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> r iterations <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span>:                    <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> <span class="hljs-keyword"><span class="hljs-keyword">all</span></span> capsule i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> layer l <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> capsule j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> layer (l + <span class="hljs-number"><span class="hljs-number">1</span></span>):  c[i, j] <span class="hljs-number"><span class="hljs-number">5</span></span>:                    <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> <span class="hljs-keyword"><span class="hljs-keyword">all</span></span> capsule j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> layer (l + <span class="hljs-number"><span class="hljs-number">1</span></span>):  s[j] <span class="hljs-number"><span class="hljs-number">6</span></span>:                    <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> <span class="hljs-keyword"><span class="hljs-keyword">all</span></span> capsule j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> layer (l + <span class="hljs-number"><span class="hljs-number">1</span></span>):  v[j] <span class="hljs-number"><span class="hljs-number">7</span></span>:                    <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> <span class="hljs-keyword"><span class="hljs-keyword">all</span></span> capsule i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> layer l <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> capsule j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> layer (l + <span class="hljs-number"><span class="hljs-number">1</span></span>): b[i,j] = b[i,j] + U[i, j]*v[j] <span class="hljs-number"><span class="hljs-number">8</span></span>:         <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> v[j]</code> </pre> <br><p>  After the last iteration, the value of v [j] is the output of the neurons of the capsule j. </p></div></div><br><p>  <em>Error function</em> </p><br><p>  Maximize the length of the output vector of the capsule, which should be activated on the current image and minimize those that should not.  Namely, we calculate the sum over all k from 0 to 9 </p><br><p>  L [k] = T <em>max (0, 0.9 - || v [k] ||) ^ 2 + 0.5</em> (1 - T) * max (0, || v [k] || - 0.1) ^ 2 <br>  where T = 1 in the presence of the digit k in the picture and is equal to 0 in the absence. </p><br><p>  In addition, we add a pixel-by-pixel error of the picture recovery by the decoder (see below), but they are added with a very small coefficient of 0.0005. </p><br><p>  <em>CapsNet architecture</em> </p><br><p><img src="https://habrastorage.org/webt/9r/1y/n5/9r1yn5t7hmg9yru8r2ajqhbmjlu.png"></p><br><p>  The encoder consists of: </p><br><ol><li>  A normal convolutional layer with 256 cores 9x9, step 1 and ReLU as an activation function. </li><li>  Layer "primary capsule", a layer with capsules, convolutional.  32 cores 9x9 of 8D capsules (8 neurons in the capsule) and in step 2. Each such capsule sees 81x256 neurons from the previous layer.  In total, the second layer has [32, 6, 6] capsule exits, each of which consists of 8 neurons.  Each capsule in the grid [6, 6] shares its weights with the others (it‚Äôs not quite clear how c [i, j] is calculated here, what is k? 32 or 32x6x6?). </li><li>  Layer "DigitCaps" consists of 10 capsules 16D, each capsule is connected to all capsules of the previous layer. </li></ol><br><p>  Routing is used only between the second and third layer.  All b at initialization is 0. </p><br><p>  The decoder consists of three fullyconnected layers with 512, 1024 and 784 neurons and ReLU as activation.  That is, the decoder takes data from those 10x16 neurons of the last encoder layer and produces a 28x28 picture. </p><br><p>  <em>results</em> </p><br><p>  To determine which digit is active, they selected the longest vector from the last layer.  Without network ensembles and data augmentation (except offset by 2px), a 0.25% error on networks with 3 layers was reached. </p><br><p>  We trained our network only on pure MNIST until it reached an error of 99.23%.     affNIST,  79% accuracy.       CNN ‚Äî   66%. </p><br><p> <em>     </em> </p><br><p>           (    ,   )   ‚àí0.25, 0.25     .    .         (, ,  ,       , ,     ). </p><br><p><img src="https://habrastorage.org/webt/4p/tk/gg/4ptkgggru8imi8evizdzmhmxkn4.png"></p><br><p> <em>.      MNIST</em> </p><br><p>     .     .              (         ).          . </p><br><p> PS   :                .   : </p><br><ol><li>       ‚Äî  </li><li>           </li><li>            c[i, j],         . </li><li>         ,          </li></ol><br><hr><br><h3 id="9-deepxplore-automated-whitebox-testing-of-deep-learning-systems"> 9. DeepXplore: Automated Whitebox Testing of Deep Learning Systems </h3><br><p>  : Kexin Pei, Yinzhi Cao, Junfeng Yang, Suman Jana, 2017 <br>  <a href="http://www.cs.columbia.edu/~suman/docs/deepxplore.pdf">‚Üí Original article</a> <br>  : <a href="https://habrahabr.ru/users/arseny_info/" class="user_link">Arseny_Info</a> </p><br><p>   ,   -  DL ,        . </p><br><p>  Since   ,   software testing       code coverage  ,     ‚Äî  ,   -,       . </p><br><p><img src="https://habrastorage.org/webt/cy/g_/be/cyg_bebgafoxqzabjon_qxdqdim.png"></p><br><p>  : </p><br><ul><li>   ; </li><li>   gradient ascent  ,      :  <br> (1)   , <br> (2)           ()  </li><li>    domain constraints (,    ,       [0,255]) </li><li>        -; </li></ul><br><p><img src="https://habrastorage.org/webt/r1/yz/ft/r1yzftsycvxecuwuezfp4b6ztsa.png"></p><br><p>   , -       .    TF + Keras. </p><br><p>  -  : </p><br><ul><li>        ; </li><li>        ; </li><li>   ‚Äú‚Äù    . </li></ul><br><p>      :   (MNIST, Imagenet, Driving)    (Contagio/Virustotal, Drebin).         (  !),      ( ,  ,    )  . </p><br><hr><br><h3 id="10-one-shot-learning-for-semantic-segmentation"> 10. One-Shot Learning for Semantic Segmentation </h3><br><p>  : Amirreza Shaban, Shray Bansal, Zhen Liu, Irfan Essa, Byron Boots, 2017 <br>  <a href="https://arxiv.org/abs/1709.03410">‚Üí Original article</a> <br>  : Vadzim Piatrou </p><br><p>    ,           . </p><br><p>      .          ,   .    4096  .      -   .    4096   1     4096   ,    .           . </p><br><p><img src="https://habrastorage.org/webt/kp/y8/fo/kpy8fovseqjewdv0_bqds0npgtg.png"></p><br><p>      PASCAL VOC 2012        .       -              .          -     . </p><br><p>   (mIoU)       40%,          (31-33%).      2       (PSPNet, 83%). </p><br><hr><br><h3 id="11-field-aware-factorization-machines-in-a-real-world-online-advertising-system"> 11. Field-aware Factorization Machines in a Real-world Online Advertising System </h3><br><p>  : Yuchin Juan, Damien Lefortier, Olivier Chapelle, 2017 <br>  <a href="https://arxiv.org/abs/1701.04099">‚Üí Original article</a> <br>  : Fedor Shabashev </p><br><p> Factorization machines       CTR,       ,        . </p><br><p>    ,         . </p><br><p> ,       ,     . </p><br><p>   ,       ,    factorization machines.       factorization machine    ,              (   naive). </p><br><p>             CTR. <br>   factorization machines -            ,  .   ,          . </p><br><p>   , ,   ,                (   pre-mature). <br>          . </p><br><hr><br><h3 id="12-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning"> 12. The Marginal Value of Adaptive Gradient Methods in Machine Learning </h3><br><p>  : Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, Benjamin Recht, 2017 <br>  <a href="https://arxiv.org/abs/1705.08292">‚Üí Original article</a> <br>  : <a href="https://habrahabr.ru/users/belerafonl/" class="user_link">BelerafonL</a> </p><br><p>      : SGD, SGD+momentum, RMSProp, AdaGrad, Adam.     ‚Äì   (RMSProp, Adam, AdaGrad)    ,   SGD     .           ,    ,     ..     SGD        ,    ‚Äì    ,    . </p><br><p>       SGD  ,     .      .    ,        .  -           .                 .  ,       SGD  .   ,   ,         learning rate   decay,     ,     ,    SGD. , ,      ,    ,  ,  ‚Ä¶ </p><br><hr><br><h3 id="13-fractalnet-ultra-deep-neural-networks-without-residuals"> 13. FractalNet: Ultra-Deep Neural Networks without Residuals </h3><br><p>  : Gustav Larsson, Michael Maire, Gregory Shakhnarovich, 2016 <br>  <a href="https://arxiv.org/abs/1605.07648">‚Üí Original article</a> <br>  : <a href="https://habrahabr.ru/users/belerafonl/" class="user_link">BelerafonL</a> </p><br><p> ,   .    FractalNet: Ultra-Deep Neural Networks without Residuals.           ,      ResNet  DenseNet.     ,      , , ,  .      . </p><br><p><img src="https://habrastorage.org/webt/mi/c8/xc/mic8xcbrvask1vmzxcws3heicv4.png"></p><br><p>   ‚Äì     .  :         .       ,  .         ,  ,   .  ,      ,   .       ResNet? </p><br><p>    ,       Join (  ).  ,     , , ,   - .     .      ,        .     ,      . ,    ,      ,     </p><br><p><img src="https://habrastorage.org/webt/jz/pk/wf/jzpkwfphcy2mohmko4pgsu1hfoe.png"></p><br><p>         ,           .  Those.    (  )    ,       ,  . </p><br><p>  What does this give?   ,        , ..         ,   ,   ,   ,   ,     .  Those.   student-teacher. </p><br><p> ,    ResNet,   ,     residual ,  FractalNet     ,      .  ResNet   residual  ,      ‚Äì     , residual    .  FractalNet    , ,           . </p><br><p><img src="https://habrastorage.org/webt/nz/xb/ay/nzxbay8l5-mbbnqzn072jvqub9u.png"></p><br><p>    , ,   ,    ,    !    ResNet,    residual  . </p><br><p>     :           ¬´   ¬ª, ..    .  student-teacher       ,     .    160  (  )      .         ResNet    (, ResNet       ).     ‚Äì , ,   ¬´¬ª          ,   residual.   ,       ,      ,     .    ¬´¬ª  ‚Äì . </p><br><p>       : </p><br><p><img src="https://habrastorage.org/webt/by/za/wd/byzawdm2ei8hup_diesrxcpk4yk.png"></p><br><p>    ,      ¬´¬ª ‚Äî ..  col#4   ,       , ..   .     ,       , .. .  ,             ,   ¬´¬ª,  ,       ,      . </p><br><p>      ,         ResNet,     .    ‚Äì  ,   ,       ,   .  Those.    3,  ‚Äì -  .   ,     ‚Äì  ¬´¬ª      ,         . </p><br><p> ,    ,     10    ,    . </p><br><hr><br><p>            ,    <a href="https://nips.cc/">NIPS</a> .  ,      <br> <a href="https://www.youtube.com/watch%3Fv%3D0sdy1Maabgs">    NIPS 2017</a> . </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/343822/">https://habr.com/ru/post/343822/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../343810/index.html">Neural network to identify individuals embedded in the smartphone</a></li>
<li><a href="../343812/index.html">DotNext 2018 Piter Release Notes</a></li>
<li><a href="../343816/index.html">We collect user activity in JS and ASP</a></li>
<li><a href="../343818/index.html">TypeScript: tslib library</a></li>
<li><a href="../343820/index.html">Technical diary: half a year developing mobile PvP</a></li>
<li><a href="../343824/index.html">MySQL and partitioning</a></li>
<li><a href="../343826/index.html">Russia's first mitap on Apache Ignite, December 12</a></li>
<li><a href="../343828/index.html">Writing a simple Linux kernel module</a></li>
<li><a href="../343830/index.html">Dependency injection in .Net Mark Siman 1 - Dependencies between application layers</a></li>
<li><a href="../343832/index.html">Designing a system for reading data from input devices (Part Two)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>