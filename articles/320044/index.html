<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Caching data in web applications. Using memcached</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Yuri Krasnoshchek (Delphi LLC, Dell) 
 I'll tell you a little about caching. Caching is, in general, not very interesting, you take it and cache it, s...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Caching data in web applications. Using memcached</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/2ae/adc/ec0/2aeadcec005af2e24f5840d5e420201b.jpg"><br><br><h2>  Yuri Krasnoshchek (Delphi LLC, Dell) </h2><br>  I'll tell you a little about caching.  Caching is, in general, not very interesting, you take it and cache it, so I‚Äôll tell you about memcached, quite intimate details. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/055/3d8/f68/0553d8f688cfbaf1603c4937f7098cbc.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      About caching, let's start by asking you to develop a factory for the production of rhomonium torsometers.  This is a standard task, the main thing is to make a boring person and say: "Well, we will apply a standard scheme for the development of a factory." <br><a name="habracut"></a><br>  In general, it is closer to factory production, i.e.  where did the problem go from?  The factory works very quickly, it produces our torsyometers.  And to calibrate each device, you need a pure romonium, which you have to fly somewhere far away and, accordingly, while we are in the process of extracting this omonium, the devices lie uncalibrated, and, in fact, all production stops.  Therefore, we are building a warehouse near the factory.  But it is not free. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/54d/79c/c00/54d79cc00eb023c5ba0e8bf2e579949e.png"><br><br>  Go to the terminology.  Just for us to talk about caching in one language there is a fairly well-established terminology. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8fe/507/317/8fe50731780d90fca8cf289fe99595af.png"><br><br>  The source is called <em>origin</em> ;  warehouse volume, i.e.  cache size - <em>cache</em> <em>size</em> ;  when we go to the warehouse for a sample of the desired form, and the storekeeper issues what we asked for - this is called <em>cache</em> <em>hit</em> , and if he says: ‚ÄúThere is no such thing‚Äù, this is called <em>cache miss</em> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/160/ef1/f49/160ef1f49549d35e8774e45acffecf1f.png"><br><br>  The data - our omonium - has <em>freshness</em> , literally - it is freshness.  Fresheness is used everywhere.  Once the data loses its natural freshness, they become <em>stale</em> <em>data</em> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/666/82a/6f7/66682a6f788f0916b844d22d73db4a4a.png"><br><br>  The process of validating data for validity is called <em>validation</em> , and the moment when we say that the data is invalid, and throwing it out of stock is called <em>invalidation</em> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/955/6d2/bb0/9556d2bb0153eac24ca8966f09def347.png"><br><br>  Sometimes such an insulting situation happens when we don‚Äôt have enough space, but we‚Äôd better keep a fresh omonium, so we find the oldest one by some criteria and throw it away.  This is called <em>eviction</em> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fae/4d6/5a8/fae4d65a84fa9c97d36aa42d56e3b5c7.png"><br><br>  The scheme is such that from our browser to the backend there is a mass of links in the chain.  Question: where to cache?  In fact, it is necessary to cache absolutely everywhere.  And whether you like it or not, the data will be cached, i.e.  rather, the question is how can we influence it? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/38e/b38/28d/38eb3828dd4de7f8bb576ba357470134.png"><br><br>  First you need to find good data for caching.  Due to the fact that we are doing some kind of cache, we have one more intermediate link in the chain, and it is not necessary that the cache speeds up something.  If we pick up the data badly, then, at best, it will not affect the speed, and at worst, it will also slow down the process of the entire production, the entire system. <br><br>  It makes no sense to cache data that changes frequently.  You need to cache data that is used frequently.  The size of the data matters, i.e.  if we decide to cache a blu-ray movie in memory, it's great, we will quickly get it out of memory, but, most likely, we will have to transfer it later over the network, and it will be very slow.  Such a large amount of data is not commensurate with the speed of delivery, i.e.  we have no reason to keep such data in memory.  We can keep the disk, but we need to compare the speed. <br><br>  By the way, you can google ‚Äúprogramming latencies‚Äù.  There, the site is very well given all the standard delays, for example, the access speed in the CPU cache, the sending speed of the Round Trip package in the data center.  And when you design, you pretend something, it is good to look, it shows very clearly what time it takes and compared to what it takes. <br><br>  These are ready-made recipes for caching: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c47/352/87b/c4735287b6d0676bc8d3ba90bfcc05fa.png"><br><br>  This is the relevant HTTP Headers: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/97f/a74/1c2/97fa741c244053d21643676e517e1843.png"><br><br>  I'll tell you a little about some, but, in general, you should read about it.  Basically, the only thing on the web is how we can influence caching ‚Äî this is by correctly installing these headers. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a3b/1eb/694/a3b1eb6942ff3d07ee5b35bcc512ad4c.png"><br><br>  Expires used before.  We set freshness for our data, we literally say: ‚ÄúEverything, this content is valid until such a number‚Äù.  And now this header should be used, but only as a fallback, because  there is a newer header.  Again, this chain is very long, you can get on some kind of proxy that only this header, Expires, understands. <br><br>  The new header, which is now responsible for caching, is Cache-Control: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/222/1f9/b27/2221f9b27a093b4f4df64bdee6e17d07.png"><br><br>  Here you can immediately specify both freshness, and the validation mechanism, and invalidation mechanism, indicate whether it is public data or private, how to cache them ... <br><br>  By the way, no-cache is very interesting.  By name, it is obvious that we say: cache everywhere, please cache as much as you like, if we say no-cache.  But every time when we use some data from this content, for example, we have a form, and we submit in this form, we say that in any case, all your cached data is not relevant, you need to recheck them. <br><br>  If we want, in general, to turn off caching for content, then we say "no-store": <br><br><img src="https://habrastorage.org/getpro/habr/post_images/529/f65/ed0/529f65ed0595afe8a11ae7c7d642b2e9.png"><br><br>  These ‚Äúno-cache‚Äù, ‚Äúno-store‚Äù are very often used for forms of authentication, i.e.  we don‚Äôt want to cache unauthenticated users so that it‚Äôs not strange that they don‚Äôt see too much or there‚Äôs no misunderstanding.  And, by the way, about this Cache-Control: no-cache ... If, say, the Cache-Control header is not supported, then its behavior can be simulated.  We can take header Expires and set the date to some past. <br><br>  These all headers, including even Content-Length, are relevant for the cache.  Some cache proxies may simply not even cache if there is no Content-Length. <br><br>  Actually, we come to memcached, to the cache on the side of the backend. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/621/193/adc/621193adc6802bb21a6eaf6c18ab2eac.png"><br><br>  Again, we can cache in different ways, i.e.  we got some data from the database, do something in the code with them, but, in fact, this is a cache, we just got them to reuse it many times.  We can use in the code some component, framework.  This component is needed for caching, because we need to have reasonable limits on our product.  It all starts with the fact that some kind of engineer comes and says: "Explain to me the requirements for your product."  And you have to tell him that this will be so much RAM, so much disk space, such a predicted amount of growth for the application ... Therefore, if we cache something, we want to have limitations.  Suppose the first constraint we can easily provide is the number of items in the cache.  But if we have elements of different sizes, then we want to close it with frames of a fixed amount of memory.  Those.  we say any cache size is the most important limit, the most important boundary.  We use a library that can do this thing. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2fc/855/797/2fc85579779148d792b7ad74bc24bae5.png"><br><br>  Well, or we use a separate caching service, generally stand-alone.  Why do we need some kind of separate caching service?  Most often, the backend is not something monolithic, one process.  We have some disparate processes, some scripts, and if we have a separate caching service, then the entire backend infrastructure can see this cache, use data from it.  It's great. <br><br>  The second point is that we have the opportunity to grow.  For example, we put one service, we are running out of cash, we put another service.  Naturally, this is not free, i.e.  ‚ÄúAnd today we decided to scale‚Äù cannot happen.  It is necessary to plan such things in advance, but a separate caching service provides such an opportunity. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/88b/44c/e82/88b44ce82fa2fbbd9c0180396fc436c4.png"><br><br>  The cache still gives us availability practically for free.  Suppose we have some data in the cache, and we are trying to get this data from the cache.  We have something somewhere falls, and we pretend that nothing has fallen, we give the data from the cache.  It may, at this time, be re-lifted somehow, and there will be even availability. <br><br>  Actually, we got to memcached.  Memcached is a typical noSQL. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/220/ac1/b92/220ac1b92780f94f8c066aa61708f066.png"><br><br>  Why is noSQL for caching good? <br><br>  By structure.  We have a regular hash table, i.e.  we get low latency.  In the case of memcached and similar key-value storage, this is not just low latency, but in big-O notation, we have the complexity of most operations - it is a constant of one.  And therefore we can say that we have some kind of temporary constraint.  For us, for example, the request takes no more than 10 ms., I.e.  you can even negotiate a contract based on these latency.  It's good. <br><br>  Most of the time we cache anything - pictures mixed with CCS with JS, some fragments of forms of the primer, something else.  It is not clear what data, and the key-value structure allows you to store it quite easily.  We can start a notation that we have an account.300.avatar - this is a picture, and it works there.  300 is the account ID in our case. <br><br>  The important moment is that the storage code itself is simplified if we have key-value noSQL, because the worst thing that can happen is that we somehow corrupt or lose data.  The less code that works with data, the less chance of spoiling, so a simple cache with a simple structure is good. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/129/dad/f67/129dadf67fc3443ad945aa68867ea36a.png"><br><br>  About memcached key-value.  You can specify expiration with data.  Work is supported in a fixed amount of memory.  You can set 16-bit flags with a value arbitrarily - they are transparent to memcached, but more often you will work with memcached from some client, and most likely, this client has already captured these 16 bits for itself, i.e.  he uses them somehow.  There is such an opportunity. <br><br>  memcached can work with wiki support;  when we run out of space, we push out the oldest data, add the newest ones.  Or we can say: ‚ÄúDo not delete any data‚Äù, then when adding new data it will return an error out of memory - this is the ‚Äú-M‚Äù checkbox. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5ab/78b/46f/5ab78b46faac4b3463f00c47a8fe7613.png"><br><br>  There is no structured uniform documentation for memcached, it is best to read the protocol description.  Basically, if you type the memcached protocol on Google, this will be the first link.  The protocol describes not only the formats of commands - sending, what we send, what comes in response ... It describes that this command, it will behave like this and that, that is,  there are some corner cases. <br><br>  Short on commands: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3e6/f63/6ba/3e6f636ba1fd7015e9a737de7b389bd6.png"><br><br>  <em>get</em> - get data; <br><br>  <em>set /</em> <em>add /</em> <em>delete /</em> <em>replace</em> - how we write this data, ie: <br><br><ul><li>  <em>set</em> is to save, add new or replace, <br></li><li>  <em>add</em> is only to add if there is no such key <br></li><li>  <em>delete</em> - delete; <br></li><li>  <em>replace</em> - replace only if there is such a key, otherwise - an error. <br></li></ul><br>  It is in an environment where we have a shard.  When we have a cluster, this does not guarantee us any consistency.  But with one instance, consistency can be supported by these commands.  More or less, you can build such constraints. <br><br>  <em>prepend /</em> <em>append</em> - this is what we take and in front of our data we insert some piece or after our data we insert some piece.  They are not very efficiently implemented inside memcached, i.e.  you will still be allocated a new piece of memory, functionally, there is no difference between them and <em>set</em> . <br><br>  We can save the data, specify some kind of expiration and then we can touch this data with the touch command, and we extend the life specifically for this key, i.e.  he will not go away. <br><br>  There are increment and decrement commands - <em>incr / decr</em> .  It works as follows: you store a number as a string, then it says <em>incr</em> and you give it a value.  It summarizes.  Decrement is the same, but subtracts.  There is an interesting point, for example, 2 - 3 = 0, from the point of view of memcached, i.e.  it automatically handles underflood, but it does not allow us to make a negative number, in any case, zero will return. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/043/71d/d72/04371dd72a4168a40a39bfb8bc05ff54.png"><br><br>  The only command with which you can create some consistency is <em>cas</em> (this is the atomic operation compare and swap).  We compare two values, if these values ‚Äã‚Äãare the same, then we replace the data with new ones.  The value we are comparing is a global counter inside <br>  memcached, and each time we add data there, this counter is incremented, and our key-value pair gets some value.  Using the <em>gets</em> command, we get this value and then in the <em>cas</em> command we can use it.  This team has all the same problems that ordinary atoms have, i.e.  You can make a lot of raise conditionals interesting, especially since memcached has no guarantees on the order of command execution. <br><br>  Have memcached key "-C" - it turns off the <em>cas</em> .  Those.  what's happening?  This counter disappears from the key-value pair, if you add the key "-C", then you save 8 bytes, because it is a 64-bit counter on each value.  If your values ‚Äã‚Äãare small, the keys are small, then this can be a significant savings. <br><br>  How to work with memcached effectively? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e74/bc0/138/e74bc01385f6a4645ac7ea7b7781ea93.png"><br><br>  She is designed to work with multiple sessions.  Sets are hundreds.  Those.  starts from hundreds.  And the fact is that in terms of RPS - request per second - you will not squeeze much out of memcached using 2-3 sessions, i.e.  in order to swing it, you need a lot of connections.  Sessions should be long-playing, because the creation of a session inside memcached is a rather expensive process, so you hooked once and that's all, this session should be held. <br><br>  Requests need to batch'it, i.e.  we have to send requests in batches.  For the get-command, we have the opportunity to transfer a few keys, this should be used.  Those.  we say get and key-key-key.  For the rest of the teams there is no such possibility, but we can still do batch, i.e.  we can form a request from ourselves, locally, on the client side using several commands, and then send this request entirely. <br><br>  memcached is multithreaded, but it is not very well multithreaded.  It has a lot of locks inside, rather ad-hoc, so more than four threads cause very strong content inside.  I don‚Äôt need to believe, it‚Äôs necessary to recheck everything myself, it‚Äôs necessary to do some experiments on a live system, but a very large number of threads will not work.  It is necessary to play, to pick up some optimal number with the key "-t". <br><br>  memcached supports UDP.  This is the patch that was added to memcached by facebook.  The way Facebook uses memcached - they make all the sets, i.e.  all modification of data on TCP, and get they make over UDP.  And it turns out, when the amount of data is substantially large, then UDP gives a serious gain due to the fact that the packet size is smaller.  They manage to pump more data through the grid. <br><br>  I told you about incr / decr - these commands are ideal for storing backend statistics. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f42/94c/bea/f4294cbea198259252f857547ddcf5d3.png"><br><br>  Statistics in HighLoad is an irreplaceable thing, i.e.  you can‚Äôt understand what, how, where a specific problem comes from, if you don‚Äôt have statistics, because after half an hour of work ‚Äúthe system behaves strangely‚Äù and that‚Äôs all ... To add specifics, for example, every thousandth query will be sent, we need some kind of then statistics.  The more statistics there are, the better.  And even, in principle, in order to understand that we have a problem, we need some kind of statistics.  For example, the backend gave a page for 30 ms, started over 40, it is impossible to distinguish with a glance, but with us the performance sank by a quarter - this is terrible. <br><br>  Memcached also supports statistics by itself, and if you already use memcached in your infrastructure, then memcached statistics is part of your statistics, so you need to look in there, you need to look there to understand whether the backend is using the cache correctly, or if it caches data well . <br><br>  The first is that each team has hits and misses.  When we turned to the cache, and gave us the data, I got a hit on this command.  For example, we made the delete key, we will have delete hits 1, so for each command.  Naturally, it is necessary that hits be 100%, there is no misses at all.  Must watch.  Suppose we can have a very high miss ratio.  The most banal reason - we just climb the wrong data.  There may be such an option that we allocated little memory for the cache, and we constantly reuse the cache, i.e.  we added some data, added, added, at some moment there the first data fell out of the cache, we got behind it, they are no longer there.  We climbed after the others, they are not there either.  And it is spinning like this.  Those.  it is necessary either to reduce the load on memcached from the backend side, or you can increase the amount of memory that we allow to use with the ‚Äú-m‚Äù parameter. <br><br>  Evictions is a very important moment.  The situation I am talking about will be visible from the fact that the evictions rate will be very high.  This is the quantity when the valid data is not exploded, i.e.  they are fresh, good are thrown out of the cache, we then have a growing number of evictions. <br><br>  I said that you need to use batch'i.  How to choose batch size?  There is no silver bullet, it is necessary to select it experimentally.  Everything depends on your infrastructure, on the network you use, on the number of instances and other factors.  But when we have a very large batch ... Imagine the situation that we are performing a batch, and all the other things are standing and waiting until the batch is executed.  This is called starvation - fasting, i.e.  when the rest of the team go hungry and wait until one fatty one is fulfilled.  To avoid this, there is a mechanism inside memcached that interrupts the execution of batch by force.  It is implemented quite rudely, there is a key ‚Äú-R‚Äù, which says how many teams can execute one connection in a row.  By default, this value is 20. And you, when you look at the statistics, if your conn_yields stat is somehow very high, it means that you use batch more than memcached can chew, and it has to forcefully switch the context of this connection.  Here you can either increase the size of the batch with the ‚Äú-R‚Äù key, or not use such batch from the backend. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/164/c03/a73/164c03a737d465d618faa4806499c479.png"><br><br>  I also said that memcached discards the oldest data from memory.  So, I lied.  In fact, it is not.  Inside memcached has its own memory manager to work effectively with this memory to eject these atoms.  It is designed in such a way that we have slabs (literally "stub").  This is a well-established term in programming memory managers for a piece of memory, i.e.  we just have some big piece of memory, which, in turn, is divided into pages.  Pages inside memcached by MB, so you can not create there data "key-value" of more than one MB.  This is a physical limitation - memcached cannot create data more than one page.  And, as a result, all the pages are beaten by chunks, this is what you see in the picture at 96, 120 - they are of a certain size.  Those.  chunks of 96 MB, then 120 bits, with a factor of 1.25, from 32 to 1 MB.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Within this piece there is a doubly linked list. </font><font style="vertical-align: inherit;">When we add some new value, memcached looks at the size of this value (this is the key + value + expiration + flags + system information that memcached is needed (about 24-50 bytes)), chooses the size of this chunk and adds our doubly-connected list data. </font><font style="vertical-align: inherit;">She always adds data to the head. </font><font style="vertical-align: inherit;">When we access some data, memcached removes them from the doubly linked list and again throws it in the head. </font><font style="vertical-align: inherit;">Thus, the data that is little used, crawl in the tail, and eventually they are deleted.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If we don‚Äôt have enough memory, then memcached will start removing memory from the end. The list recently used mechanism works within one chunk, i.e. these lists are allocated for a certain size, it is not a fixed size - this is the range from 96 to 120 will fall into the 120th chunk, etc. We can‚Äôt influence this mechanism from the memcached side, only from the backend side we need to select this data.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/380/8bd/555/3808bd55504b4e0c53d5bb256c5b320b.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">You can see the statistics for these slabs. The easiest way to watch memcached statistics is that the protocol is completely text-based, and we can Telnet to connect, type stats, Enter, and it will dump the ‚Äúsheet‚Äù. Similarly, we can type stats slabs, stats items - this is basically similar information, but stats slabs gives a picture that is more blurred in time, there are such stats - what happened, what happened during the entire period while memcached worked, and stat items - there is more about what we have now, how much is what. In principle, both of these things need to look, we must take into account.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/1ad/cdf/54a/1adcdf54a2dc06ab89d69d52e93774f9.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Here we are getting close to scaling. Naturally, we installed another memcached server - great. What do we do? Somehow you have to choose. Either we, on the client side, decide which server to join and why. If we have availability, then everything is simple - we wrote it down there, we wrote it down here, we read it from somewhere, it doesn‚Äôt matter, you can use Round Robin as you wish. Either we set up some kind of broker, and for the backend we get that it looks like a memcached instance, but actually a cluster is hidden behind this broker. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What is a broker used for? To simplify the backend infrastructure. For example, we need to transport servers from a data center to a data center, and all clients should know about it. Or we can do a hack behind this broker, and for the backend everything will pass transparently.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But latency is growing. 90% of requests are network round trip, i.e. memcached inside it processes the request for ms - this is very fast, and data travels over the network for a long time. When we have a broker, we have another link, i.e. still longer. If the client immediately knows which cluster of memcached he should go to, then he will get the data quickly. And, actually, how does the client know which cluster of memcached to go to? We take, consider the hash from our key, take the remainder of dividing this hash by the instance number in memcached and go to this cluster - the simplest solution. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But we have added another cluster to the infrastructure, which means that now we need to crash the entire cache, because it has become inconsistent, not valid, and to recount everything again - this is bad. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/286/fa4/ba6/286fa4ba637ba5e06a8c9e06ee221ac9.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">For this there is a mechanism - consistent hashing ring.</font></font> Those.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">what are we doing? </font><font style="vertical-align: inherit;">We take hash values, all possible hash values, for example int32, take all possible values ‚Äã‚Äãand place it as if on the watch face.</font></font> So.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">we can configure - say, hashes from such and such go to this cluster. </font><font style="vertical-align: inherit;">We configure rangie and configure clusters that are responsible for these range. </font><font style="vertical-align: inherit;">Thus, we can shuffle the server as you like, i.e. </font><font style="vertical-align: inherit;">we will need to change this ring in one place, regenerate it, and the servers, clients or router, broker - they will have a consistent idea of ‚Äã‚Äãwhere the data lie.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I would also like to say a little about data consistency. </font><font style="vertical-align: inherit;">As soon as we have a new link, as soon as we cache somewhere, we have duplicate data. </font><font style="vertical-align: inherit;">And we have a problem so that this data is consistent. </font><font style="vertical-align: inherit;">Because there are many such situations - for example, we write data to the cache - local or remote, then we go and write this data to the database, at this moment the base drops, we have lost connection with the base. </font><font style="vertical-align: inherit;">In fact, according to logic, we do not have this data, but at the same time, customers read them from the cache - this is a problem. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">memcached is poorly suited to the consistancy of some solutions, </font><font style="vertical-align: inherit;">This is more of an availability solution, but at the same time, there are some possibilities with a cas'om to do something about it.</font></font><br><br><h3>  Contacts </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">" </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cachelot@cachelot.io</font></font></a> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> " </font></font><a href="http://cachelot.io/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">http://cachelot.io/</font></font></a> <br><br><blockquote> <font color="gray">  ‚Äî            <a href="http://junior.highload.ru/%3Futm_source%3Dhabr%26utm_medium%3Dmedia%26utm_campaign%3Dpast.articles%26utm_content%3Dcommon">HighLoad++ Junior</a> . <br><br>          -     <a href="http://highload.guide/%3Futm_source%3Dhabr%26utm_medium%3Dmedia%26utm_campaign%3Dpast.articles%26utm_content%3Dcommon">HighLoad.Guide</a> ‚Äî     , , , .       30  . ! <br><br>     ‚Äî      " <a href="http://ritfest.ru/"> -</a> ",     ,  <strong>HighLoad++ Junior</strong> .</font> </blockquote></div><p>Source: <a href="https://habr.com/ru/post/320044/">https://habr.com/ru/post/320044/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../320032/index.html">Hardware or software write blockers - which is more reliable?</a></li>
<li><a href="../320034/index.html">Is React Native good?</a></li>
<li><a href="../320036/index.html">Installation and basic configuration of nginx and php-fpm for developing projects locally in Ubuntu 16.04</a></li>
<li><a href="../320040/index.html">Firebase: farewell to illusions</a></li>
<li><a href="../320042/index.html">Grant Proposal course: how to find the right words to attract attention and money to your development</a></li>
<li><a href="../320046/index.html">Authorization in Laravel, through social networks (Ulogin). Simple, flexible and efficient.</a></li>
<li><a href="../320048/index.html">Summary building information model: a practical lesson on OpenBIM technology</a></li>
<li><a href="../320052/index.html">Android Shortcuts Review</a></li>
<li><a href="../320054/index.html">Conducting research in modern conditions</a></li>
<li><a href="../320056/index.html">New quick start with PHPixie: we build quote-by-commit commit</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>