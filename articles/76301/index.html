<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>About information search, finding the best ways to view search results and much more</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The task of finding the best ways to view the search results is my main topic for the candidate work. Today I want to share the intermediate results o...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>About information search, finding the best ways to view search results and much more</h1><div class="post__text post__text-html js-mediator-article">  The task of finding the best ways to view the search results is my main topic for the candidate work.  Today I want to share the intermediate results of the research, as well as the applications and SDKs that were used in the work. <br><br>  The decision to write this article was made after viewing the seminar from the cycle ‚ÄúInformation Search and Data Analysis‚Äù on the topic ‚ÄúSemantic Analysis of Texts Using Wikipedia‚Äù, the speaker of which was Maxim Grinev, Associate Professor, Senior Lecturer of the Department of System Programming, Head of the Institute of Scientific and Practical Information. <br><br>  You can <a href="">view a report</a> , <a href="http://narod.ru/disk/14697525000/M_Grinev_Semanticheskii_analiz_tekstov_s_ispol%2527zovaniem_Vikipedii.avi.html">download a report</a> or <a href="http://clubs.ya.ru/4611686018427420323/">view a schedule of other reports</a> . <br><a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  Brief scientific conclusions from the seminar </h3><br>  <i>Below will be briefly described the contents of the seminar and the main results obtained.</i> <br><br>  The report considered new approaches and methods of semantic search, principles for assessing semantic proximity based on data from the English-language Wikipedia. <br><br>  The main principles used in the report: the term (which is described by the corresponding article) may have several meanings, thus it is necessary to single out the most relevant article.  The term (article) contains references to other terms (articles) both within the main text and in the see see, links, etc. blocks. <br><br>  The semantic distance between articles A and B can be calculated by counting the number of common articles referenced by articles A and B: <br><br><img src="http://msug.vn.ua/cfs-file.ashx/__key/CommunityServer.Components.UserFiles/00.00.00.21.00/semantic_5F00_relatedness.gif"><br><br>  <i>Picture 1</i> <br><br>  Semantic proximity is a key point in the methods described below.  I want to mention that the SimRank method [1], described last year, was declared not successful. <br><br>  <i>From the author:</i> <i>in addition to semantic proximity, the <a href="http://habrahabr.ru/blogs/algorithm/65944/">method of shingles</a> or <a href="http%253A%252F%252Fru.wikipedia.org%252Fwiki%252F%2525D0%25259A%2525D1%252580%2525D0%2525B8%2525D1%252582%2525D0%2525B5%2525D1%252580%2525D0%2525B8%2525D0%2525B9_%2525D1%252581%2525D0%2525BE%2525D0%2525B3%2525D0%2525BB%2525D0%2525B0%2525D1%252581%2525D0%2525B8%2525D1%25258F_%2525D0%25259F%2525D0%2525B8%2525D1%252580%2525D1%252581%2525D0%2525BE%2525D0%2525BD%2525D0%2525B0%26amp%3Brct%3Dj%26amp%3Bq%3D%25D0%25BA%25D1%2580%25D0%25B8%25D1%2582%25D0%25B5%25D1%2580%25D0%25B8%25D0%25B9%2B%2B%25D0%259F%25D0%25B8%25D1%2580%25D1%2581%25D0%25BE%25D0%25BD%25D0%25B0%26amp%3Bei%3Dd_wKS9y7HIrymwOKzeimCg%26amp%3Busg%3DAFQjCNF9VxkMRXBh6eLCsSAeBIbmRsXXXA%26amp%3Bsig2%3DydQq0hW7pHOAOT9ykLpCCg">the Pearson xi-square test</a> can be used to determine the distance between two web documents.</i>  <i>Also on this issue is my published article "Method of assessing the similarity of web pages" [2], which describes a generalized method for assessing the similarity of web pages based on some semantic features.</i> <br><br>  Then it was the extraction of keywords for a given term and the construction of the so-called.  community (communities) or semantic graphs [3]: <br><br><img src="http://msug.vn.ua/cfs-file.ashx/__key/CommunityServer.Components.UserFiles/00.00.00.21.00/keywords_5F00_graph.gif"><br><br>  <i>Figure 2</i> <br><br>  The essence of these graphs is that the terms (articles) that belong to a single community are included in a certain general category.  Those.  The classical problem of classifying texts is solved.  This category can be computed by defining a generic ‚Äúparent‚Äù category, which includes selected terms.  To define communities, the clustering method is used (which does not require specifying the number of clusters and cluster size); communities with a small rank are discarded. <br><br>  An example of a real semantic graph: <br><br><img src="http://msug.vn.ua/cfs-file.ashx/__key/CommunityServer.Components.UserFiles/00.00.00.21.00/semantic_5F00_graph.gif"><br><br>  <i>Figure 3</i> <br><br>  In the course of the research, it turned out that ‚Äúgood‚Äù communities have a much greater rank than the others - less relevant. <br><br><img src="http://msug.vn.ua/cfs-file.ashx/__key/CommunityServer.Components.UserFiles/00.00.00.21.00/community_5F00_rank.gif"><br><br>  <i>Figure 4</i> <br><br>  This approach allows you to filter out non-primary content (top, bottom, see also), since the communities derived from the terms of these blocks will have a small rank, and, accordingly, will be eliminated during the calculations. <br><br><h3>  Comments and Comments </h3><br>  When I looked at the report, I had a feeling of deja vu, since I do many things in my work and some of the results have a strong relationship with it. <br><br>  First of all, I want to focus on some of the weaknesses of the described techniques and methods: <br><ul><li>  All methods are tied to Wikipedia.  This means that, a priori, we believe that the range of knowledge is limited only to Wikipedia and this knowledge is absolute; </li><li>  Wikipedia is a strictly structured resource, i.e.  erroneous classification of knowledge inside it is practically absent.  This fact greatly simplifies the pre-processing of texts in classical problems of classification and clustering (in fact, this stage is skipped, since all the work is done before the analysis begins, and it is known that the quality of pre-processing is key when analyzing large data arrays); </li><li>  Wikipedia has a single structure (layout), which makes it easy to clear an article of unimportant information with very high accuracy; </li></ul><br><br>  <i>Below I will describe my own work in the context of the methods described above.</i> <br><br><h3>  Advanced Clustering Method <br></h3><br>  The method is a refinement of the classic k-means algorithm, which is simple in terms of implementation, but not accurate.  There are two reasons why this algorithm is not exact: the algorithm is sensitive to the choice of starting points and the number of clusters.  Thus, if unreliable data will be submitted to the input, the quality of the result will be desired. <br><br>  In his work ‚ÄúThe clustering method based on clusters distributed according to the normal law‚Äù [4], it was proposed to check the distribution law of objects within the clusters.  If the cluster is distributed according to a certain law, then we leave it, if not - divide it into two subsidiaries and the verification process continues until all clusters are distributed according to a certain law or when we exceed the limit by the number of clusters.  Thus we solve the problem of the number of clusters.  The problem of choosing the initial points is solved by specifying the most separated points within the larger cluster as initial centers.  On the test data, the method showed 95% accuracy. <br><br><h3>  The importance of information blocks sites <br></h3><br>  When we talk about a modern web page, we mean not only the main content for which, we actually came, but also a lot of additional information on the sides, below, etc.  These information blocks can have different purposes - lists of links, statistics, related articles, advertising.  It is clear that the importance of this content is much less than the main one. <br><br>  A method was developed to purify web pages from information noise [5], about which I <a href="http://habrahabr.ru/blogs/data_mining/66221/">wrote in general terms earlier</a> [6].  I will dwell on an important point - the procedure for determining the ‚Äúimportant‚Äù blocks is based on a fuzzy clustering method, and simple words are searched for blocks with maximum numerical estimates (which differ sharply from others).  In fact, the same approach was used to define ‚Äúgood‚Äù communities, which Maxim Grinev spoke about (see Figure 4). <br><br>  The prototype is available for download at the <a href="http://smartbrowser.codeplex.com/">codeplex</a> site: <br><br><img src="http://i3.codeplex.com/Project/Download/FileDownload.aspx?ProjectName=smartbrowser&amp;amp;DownloadId=65022" width="800" height="362"><br><br>  Let's take another close look at Figure 3 - a graph of the relationship of terms.  In fact, this is nothing but a graph of the relationship of web pages that are responsible for a specific term. <br><br>  We developed a similar system for assessing the relationships between sites, but in our case we use search engines as a data source (for us there is no fundamental difference with which search engine to take the results). <br><br>  A real example of how the system works on the Microsoft request can be found below: <br><br><img src="http://msug.vn.ua/cfs-file.ashx/__key/CommunityServer.Components.UserFiles/00.00.00.21.00/site_5F00_graph.gif"><br><br>  <i>Figure 5 (in this case, a shallow link analysis is selected)</i> <br><br>  If you look closely, you can also see some of the "community", which indicate membership in different categories.  Accordingly, by selecting keywords (or other properties of each web page), you can cluster the search results in runtime.  Note that this works for the entire web, not just for Wikipedia. <br><br><h3>  Finding the best ways to view search results </h3><br>  Now I come to the most interesting part, since all of the above is not in itself of great importance. <br><br>  Once we have a relationship graph, information about each web page (Google PageRank, the distance between web pages, the "weight" of the page), we can find the best way to view the search results on the graph.  Those.  in other words, we‚Äôll get a non-linear list of sites ranked according to a certain algorithm (weight), but a set of recommendations on the order <b>in which to search the web search results</b> [7] <b>.</b> <br><br>  To achieve the goal, we use a modified ant algorithm that simulates user behavior, namely, random transitions during surfing.  Each path is estimated by a certain formula and at the end we get the optimal path (the amount of information, the amount of duplicate information and several other parameters are taken into account). <br><br>  In addition, the user can select: <br><ul><li>  depth of analysis </li><li>  maximum (optimal) number of sites </li><li>  choose what is important for him - the speed of execution (i.e. requests like ‚Äúwhat is wikipedia‚Äù), or a deep analysis of a certain issue (for example, ‚ÄúAfrican economies‚Äù) </li><li> Of course, he can save all this in his personal settings, thus reconfigure the algorithm for himself <br></li></ul><br><br><h3>  findings </h3><br>  Thus, the considered methods and algorithms allow to gain knowledge not only in Wikipedia, but also for the entire web.  The ideas and methods presented at the seminar and received by us, on the whole, coincide, in some ways even surpass them.  Among the shortcomings I can name the impossibility of testing methods on large amounts of data, which Yandex has for its research and the need to work on formalities, and not on the essence of the problems. <br><br>  I hope this article will help assess the state of affairs in the field of information retrieval. <br><br>  PS I can not fail to mention the developed <a href="http://extracting.codeplex.com/">Data Extracting SDK</a> , with which almost all applications that were used for research were written. <br><br>  PSS If something is not clear or there is a desire to get acquainted with some methods (ideas) in more detail - write in the comments, I will try to answer them. <br><br><br>  [1] Dmitry Lizorkin, Pavel Velikhov, Maxim Grinev, Denis Turdakov Accuracy Estimate and Optimization Techniques for SimRank Computation, VLDB 2008 <br>  [2] <a href="http%253A%252F%252Fwww.nbuv.gov.ua%252FPortal%252Fnatural%252Foeiet%252F2008_2%252F16pdf%252F10.pdf%26amp%3Brct%3Dj%26amp%3Bq%3D%25D0%25B2%25D1%2596%25D0%25B4%25D1%2581%25D1%2582%25D0%25B0%25D0%25BD%25D1%258C%2B%25D0%25BC%25D1%2596%25D0%25B6%2B%25D0%25B4%25D0%25BE%25D0%25BA%25D1%2583%25D0%25BC%25D0%25B5%25D0%25BD%25D1%2582%25D0%25B0%25D0%25BC%25D0%25B8%2B%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BA%25D0%25BE%25D0%25B2%25D0%25B5%25D1%2586%25D1%258C%25D0%25BA%25D0%25B8%25D0%25B9%2B%26amp%3Bei%3DMvsKS4TKGJycmAODl-HDCg%26amp%3Busg%3DAFQjCNH-3ZRdp47gdm8xQxWdQCtrQqYI1Q%26amp%3Bsig2%3Db-Bdj4df5vSxPkGBTIlmLA">Method of assessing the similarity of web pages</a> <br>  [3] Maria Grineva, Maxim Grinev, Dmitry Lizorkin.Extracting Key Terms From Noisy and Multitheme Documents.  WWW2009: 18th International World Wide Web Conference <br>  [4] Krakovetsky O. Yu.  The method of clustering on the basis of clusters, which follows a normal law // International Science and Technology Journal "Information Technology Technology and Computer Engineering" ‚Ññ1 (11).  - 2008 p.  - p.56-60. <br>  [5] <a href="http%253A%252F%252Fwww.nbuv.gov.ua%252FPortal%252Fnatural%252Foeiet%252F2008_2%252F16pdf%252F10.pdf%26amp%3Brct%3Dj%26amp%3Bq%3D%25D0%25B2%25D1%2596%25D0%25B4%25D1%2581%25D1%2582%25D0%25B0%25D0%25BD%25D1%258C%2B%25D0%25BC%25D1%2596%25D0%25B6%2B%25D0%25B4%25D0%25BE%25D0%25BA%25D1%2583%25D0%25BC%25D0%25B5%25D0%25BD%25D1%2582%25D0%25B0%25D0%25BC%25D0%25B8%2B%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BA%25D0%25BE%25D0%25B2%25D0%25B5%25D1%2586%25D1%258C%25D0%25BA%25D0%25B8%25D0%25B9%2B%26amp%3Bei%3DMvsKS4TKGJycmAODl-HDCg%26amp%3Busg%3DAFQjCNH-3ZRdp47gdm8xQxWdQCtrQqYI1Q%26amp%3Bsig2%3Db-Bdj4df5vSxPkGBTIlmLA">Method of clearing web pages of information noise</a> <br>  [6] V.M.  Dubovoy, O. Yu.  Krakovetsky, OV  The rush.  Factor analysis of social status of information blocks of websites // News of Vinnytsia Polytechnic Institute.  - 2008. - ‚Ññ6.  - c. 103-107 <br>  [7] <a href="http://conf.vstu.vinnica.ua/ies/2008/txt/dubovoy_metod_pobudovy.pdf">Volodymyr Dubovoy, Oleksandar Krakovetsky, Olga Glon. The method of obtaining optimal values ‚Äã‚Äãwill look at the results of the web-work on the basis of heuristic algorithms.</a> </div><p>Source: <a href="https://habr.com/ru/post/76301/">https://habr.com/ru/post/76301/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../76273/index.html">Coworking in Barnaul</a></li>
<li><a href="../76274/index.html">Fittings in the ears</a></li>
<li><a href="../76275/index.html">Launch Chromium OS in Virtual Box</a></li>
<li><a href="../76277/index.html">Google reader favicon</a></li>
<li><a href="../76299/index.html">Land for peasants, factories for workers, tablets for Internet users</a></li>
<li><a href="../76302/index.html">Search plugin for htmlbook.ru</a></li>
<li><a href="../76303/index.html">Mobile Technology Conference November 27 in Minsk. Program</a></li>
<li><a href="../76304/index.html">The problem of the removal of customers by employees of the company</a></li>
<li><a href="../76305/index.html">Find good people</a></li>
<li><a href="../76309/index.html">Parallelization of long operations</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>