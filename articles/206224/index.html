<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Live migration in OpenStack</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dynamic migration is the transfer of a working instance from one compute node to another. Being extremely popular among cloud service administrators, ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Live migration in OpenStack</h1><div class="post__text post__text-html js-mediator-article">  Dynamic migration is the transfer of a working instance from one compute node to another.  Being extremely popular among cloud service administrators, this feature is mainly used to provide zero downtime when servicing the cloud, and can also be useful for maintaining operability, since  allows you to move running instances from a heavily loaded compute node to a less loaded one. <a name="habracut"></a><br><br>  Dynamic migration planning should take place early in the planning and design of an OpenStack deployment.  Consider the following: <br>  ‚Ä¢ Today, not all hypervisors support migration to OpenStack, so it‚Äôs best to check in the <a href="https://wiki.openstack.org/wiki/HypervisorSupportMatrix">HypervisorSupportMatrix</a> list whether your hypervisor supports dynamic migration.  The hypervisors that support this feature currently include, for example, KVM, QEMU, XenServer / XCP, and HyperV. <br><br>  ‚Ä¢ In a standard Openstack deployment, each compute node locally manages its instances in a dedicated directory (for example, / var / lib / nova / instances /), but for dynamic migration this folder must be stored centrally and shared by all compute nodes.  Therefore, the availability of a file system or storage of shared data blocks is an important requirement for performing dynamic migration.  For such data storage, you must correctly configure a distributed file system like GlusterFS or NFS and run it before starting the migration.  SAN storage protocols such as Fiber Channel (FC) and iSCSI can also be used for shared memory. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      ‚Ä¢ To grant access rights to files when using centralized storage in shared memory, you need to make sure that the UID and GID of the Compute user (nova) are the same on the controller node and on all compute nodes (assuming that the shared memory is located on the controller node).  In addition, the UID and GID identifiers of libvirt-qemu must also be the same on all compute nodes. <br><br>  ‚Ä¢ It is important to set vncserver_listen = 0.0.0.0 so that the vnc server can accept connections from all compute nodes regardless of where the instances are running.  If this value is not specified, then there may be problems with access to transferred instances through vnc, since  The IP address of the target compute node will not match the IP address of the computational source node. <br><br>  The instructions below will enable dynamic migration when deploying a multi-mode OpenStack platform using the KVM hypervisor running on Ubuntu 12.04 LTS, and the NFS file system.  This guide assumes that a deployed and running multi-mode platform is already configured using an automated installation system such as Mirantis Fuel.  The guide is based on testing using the following components: a cloud controller node, a network node using a Neutron network connection service, and two compute nodes. <br><br>  I draw your attention to the fact that the guide does not deal with security aspects during dynamic migration.  Study this important question yourself and do not take these instructions as a ready-made guide to action from a security point of view. <br>  The manual includes two stages: the first is the NFS deployment procedure;  the second is a demonstration of dynamic migration. <br><br><h4>  Part 1: Deploying NFS File System </h4><br>  The cloud controller node is the NFS server.  The goal is to ensure the sharing of / var / lib / nova / instances by all the compute nodes of your Openstack cluster.  The directory contains the libvirt KVM disk image files for the instances hosted on this compute node.  If you do not use the cloud in a shared storage environment, this directory will be unique for all compute nodes.  Note that if your instances are already running in the cloud before the dynamic migration configuration, you need to take precautions to avoid blocking these instances. <br><br>  Do the following on the NFS server / controller node: <br>  1. Install the NFS server. <br>  root @ vmcon-mn: ~ # apt-get install nfs-kernel-server <br><br>  2. IDMAPD provides the functionality of the client and the NFSv4 core server by translating user and group IDs into their names and vice versa.  Change / etc / default / nfs-kernel-server and set the value of the specified option = yes.  This file must be the same on both the client and the NFS server. <br>  NEED_IDMAPD = yes # only needed for Ubuntu 11.10 and earlier <br><br>  3. Make sure file /etc/idmapd.conf contains the following: <br>  [Mapping] <br><br>  Nobody-User = nobody <br>  Nobody-Group = nogroup <br><br>  4. To share / var / lib / nova / instances, add the following to / etc / exports: <br>  192.168.122.0/24(rw,fsid=0,insecure,no_subtree_check,async,no_root_squash) <br><br>  Where 192.168.122.0/24 is the network address of the compute nodes (usually the target data network) for your OpenStack cluster. <br><br>  5. Set the next value of the 'execute' bit in the shared directory so that qemu can use images inside directories when exporting them to compute nodes. <br>  root @ vmcom1-mn: ~ # chmod o + x / var / lib / nova / instances <br><br>  6. Restart the services. <br>  root @ vmcon-mn: ~ # service nfs-kernel-server restart <br>  root @ vmcon-mn: ~ # /etc/init.d/idmapd restart <br><br>  Perform the following steps on each compute node: <br>  1. Install the NFS client services. <br>  root @ vmcom1-mn: ~ # apt-get install nfs-common <br><br>  2. Change / etc / default / nfs-common and set the value of the specified option = yes. <br>  NEED_IDMAPD = yes # only needed for Ubuntu 11.10 or earlier <br><br>  3. Mount the shared file system from the NFS server. <br>  mount NFS-SERVER: / var / lib / nova / instances / var / lib / nova / instances <br>  Where NFS-SERVER is the host name / IP address of the NFS server. <br><br>  4. In order not to type it again after each reboot, add the following line to / etc / fstab: <br>  nfs-server: / / var / lib / nova / instances nfs auto 0 0 <br><br>  5. Check all compute nodes and make sure that the permissions are set to the following values.  This indicates that the correct permissions have been set on the controller node using the above chmod + x command. <br>  root @ vmcom1-mn: ~ # ls -ld / var / lib / nova / instances / <br>  drwxr-xr-x 8 nova nova 4096 Oct 3 12:41 / var / lib / nova / instances / <br><br>  6. Ensure that the exported directory can be mounted and that it is mounted. <br>  root @ vmcom1-mn # mount ‚Äìa -v <br>  root @ vmcom1-mn: ~ # df -k <br>  Filesystem 1K-blocks Used Available Use% Mounted on <br>  / dev / vda1 6192704 1732332 4145800 30% / <br>  udev 1991628 4 1991624 1% / dev <br>  tmpfs 800176 284 799892 1% / run <br>  none 5120 0 5120 0% / run / lock <br>  none 2000432 0 2000432 0% / run / shm <br>  cgroup 2000432 0 2000432 0% / sys / fs / cgroup <br>  vmcon-mn: / var / lib / nova / instances 6192896 2773760 3104512 48% / var / lib / nova / instances <br><br>  Check if the last line matches the one in the list.  It means that the export of / var / lib / nova / instances from the NFS server was correct.  If this line does not exist, your NFS file system may not work correctly and you need to fix them before continuing. <br><br>  7. Update libvirt settings.  Modify /etc/libvirt/libvirtd.conf.  To see all the possible options, see the <a href="http://libvirt.org/remote.html">libvirtd</a> settings. <br>  before: #listen_tls = 0 <br>  after: listen_tls = 0 <br><br>  before: #listen_tcp = 1 <br>  after: listen_tcp = 1 <br><br>  add: auth_tcp = "none" <br><br>  8. Modify /etc/init/libvirt-bin.conf. <br>  before: exec / usr / sbin / libvirtd -d <br>  after: exec / usr / sbin / libvirtd -d -l <br><br>  -l - short for listen <br><br>  9. Modify / etc / default / libvirt-bin. <br>  before: libvirtd_opts = "-d" <br>  after: libvirtd_opts = "-d -l" <br><br>  10. Restart libvirt.  After executing this command, make sure that the restart of libvirt was successful. <br>  $ stop libvirt-bin &amp;&amp; start libvirt-bin <br>  $ ps -ef |  grep libvirt <br><br><h5>  Other settings </h5><br>  You can skip the steps below if the dynamic migration was planned from the very beginning and the basic requirements are met as indicated in the introduction.  These actions are performed to ensure that the nova UID and GID are the same on the controller node and on all compute nodes.  In addition, the UID and GID identifiers of libvirt-qemu on all compute nodes must be the same.  To do this, you must manually change the GID and UID to unify them on the compute nodes and the controller. <br>  Do the following: <br>  1. Check the nova ID value on the controller node, then do the same on all compute nodes: <br>  [root @ vmcon-mn ~] # id nova <br>  uid = 110 (nova) gid = 117 (nova) groups = 117 (nova), 113 (libvirtd) <br><br>  2. Now that we know the nova UID and GID values, we can change them on all compute nodes, as shown below: <br>  [root @ vmcom1-mn ~] # usermod -u 110 nova <br>  [root @ vmcom1-mn ~] # groupmod -g 117 nova <br><br>  Repeat these steps for all compute nodes. <br><br>  3. Do the same for libvirt-qemu, but remember that the controller node does not have this user, because the controller does not start the hypervisor.  Ensure that all compute nodes have the same UID and GID for user libvirt-qemu. <br><br>  4. Since we changed the UID and GID of the nova and libvirt-qemu users, we need to make sure that this is reflected in all the files owned by these users.  To do this, do the following.  Stop the nova-api and libvirt-bin services on the compute node.  Replace with new UID and GID values ‚Äã‚Äãin all files owned by the nova user and the nova user group, respectively.  For example: <br>  [root @ vmcom1-mn ~] #service nova-api stop <br>  [root @ vmcom1-mn ~] #service libvirt-bin stop <br>  [root @ vmcom1-mn ~] #find / -uid 106 -exec chown nova {} \;  # note the 106 here is the old nova uid before the change <br>  [root @ vmcom1-mn ~] #find / -uid 104 -exec chown libvirt-qemu {} \;  # note the 104 here is the old nova uid before the change <br>  [root @ vmcom1-mn ~] # find / -gid 107 -exec chgrp nova {} \;  #note the 107 here is the old nova uid before the change <br>  [root @ vmcom1-mn ~] #find / -gid 104 -exec chgrp libvirt-qemu {} \;  #note the 104 here is the old nova uid before the change <br>  [root @ vmcom1-mn ~] #service nova-api restart <br>  [root @ vmcom1-mn ~] #service libvirt-bin restart <br><br><h4>  <i>Part 2: Dynamic migration of the OpenStack virtual machine.</i> </h4><br>  After the OpenStack cluster and the NFS file system are configured properly, you can proceed with the live migration.  Perform the following steps on the controller node: <br>  1. Check running instance IDs. <br>  nova list <br>  root @ vmcon-mn: ~ # nova list <br>  + -------------------------------------- + ------ + --- ----- + ------------------------ + <br>  |  ID |  Name |  Status |  Networks | <br>  + -------------------------------------- + ------ + --- ----- + ------------------------ + <br>  |  0bb04bc1-5535-49e2-8769-53fa42e184c8 |  vm1 |  ACTIVE |  net_proj_one = 10.10.1.4 | <br>  |  d93572ec-4796-4795-ade8-cfeb2a770cf2 |  vm2 |  ACTIVE |  net_proj_one = 10.10.1.5 | <br>  + -------------------------------------- + ------ + --- ----- + ------------------------ + <br><br>  2. Check which compute nodes the instances run on. <br>  nova-manage vm list <br>  root @ vmcon-mn: ~ # nova-manage vm list <br><br>  instance node type state launched image kernel ramdisk project user zone index <br>  vm1 vmcom2-mn m1.tiny active 2013-10-03 13:33:52 b353319f-efef-4f1a-a20c-23949c82abd8 419303e31d40475a9c5b7d554b28a22f cd516c290d4e437d8605b411af4108fe4f4a4f cd516c290d4e437d8605b411af4108a4f cf5c7904a4f4a4f cd516c290d4e437d8605b411a4f4a4f4f4a4f cf5c7b4a4f4a4f cd516c290d4e47a4f <br>  vm2 vmcom1-mn m1.tiny active 2013-10-03 13:34:33 b353319f-efef-4f1a-a20c-23949c82abd8 419303e31d40475a9c5b7d554b28a22f cd516c290d4e437d8605b411af4108a4f4f4f4f4a4f cd516c290d4e437d8605b411af4108af <br><br>  We see that the vm1 virtual machine is running on compute node 2 (vmcom2-mn), and vm2 is running on node 1 (vmcom1-mn). <br><br>  3. Perform a live migration.  We will transfer vm1 with id 0bb04bc1-5535-49e2-8769-53fa42e184c8 (obtained from the above nova list) running on compute node 2 to compute node 1 (see the nova-manage command in the vm list above), vmcom1-mn .  Please note that this is an administrative function, therefore, as a rule, you must first export the variables or the source file with the admin credentials. <br>  root @ vmcon-mn: ~ # export OS_TENANT_NAME = admin <br>  root @ vmcon-mn: ~ # export OS_USERNAME = admin <br>  root @ vmcon-mn: ~ # export OS_PASSWORD = admin <br>  root @ vmcon-mn: ~ # export OS_AUTH_URL = " <a href="http://10.0.0.51/">10.0.0.51</a> : 5000 / v2.0 /" <br>  root @ vmcon-mn: ~ # nova live-migration 0bb04bc1-5535-49e2-8769-53fa42e184c8 vmcom1-mn <br><br>  If successful, the nova live-migration command returns nothing. <br><br>  4. Verify that the migration is complete by running: <br>  root @ vmcon-mn: ~ # nova-manage vm list <br><br>  instance node type state launched image kernel ramdisk project user zone index <br>  vm1 vmcom1-mn m1.tiny active 2013-10-03 13:33:52 b353319f-efef-4f1a-a20c-23949c82abd8 419303e31d40475a9c5b7d554b28a22f cd516c290d4e437d8605b411af4108a4f4f4f4a4f cd516c290d4e437d8605b411af4108a4f cd5c290d4e437d4604b4d44a4f4f4a4f cd516c290d4e437d4604b4d44a4f4f4a4f cd5ccd904f4a4f <br>  vm2 vmcom1-mn m1.tiny active 2013-10-03 13:34:33 b353319f-efef-4f1a-a20c-23949c82abd8 419303e31d40475a9c5b7d554b28a22f cd516c290d4e437d8605b411af4108a4f4f4f4f4a4f cd516c290d4e437d8605b411af4108af <br><br>  We see that both instances now work on the same node. <br><br><h4>  <i>Conclusion</i> </h4><br>  Live migration is important to ensure zero downtime when servicing the OpenStack cloud when you need to stop some of the compute nodes.  The above steps for sharing data and migrating a running instance were performed to demonstrate dynamic migration on the OpenStack Grizzly cloud running on Ubuntu 12.04 OS using the NFS file system. <br><br>  Original article <a href="http://www.mirantis.com/blog/tutorial-openstack-live-migration-with-kvm-hypervisor-and-nfs-shared-storage/">in English</a> </div><p>Source: <a href="https://habr.com/ru/post/206224/">https://habr.com/ru/post/206224/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../206212/index.html">LG A290 TriSim. One year later</a></li>
<li><a href="../206216/index.html">$ 7M investment in symfony and sensiolabs</a></li>
<li><a href="../206218/index.html">We build in polls for users of Android applications</a></li>
<li><a href="../206220/index.html">Designing a notification system for web applications</a></li>
<li><a href="../206222/index.html">DirectWrite support added to Chrome Canary</a></li>
<li><a href="../206230/index.html">Filtering SharePoint lists, address bar settings</a></li>
<li><a href="../206234/index.html">Tasks for interviews in Yandex</a></li>
<li><a href="../206236/index.html">Per capita e-rubbish - an interactive map shows the reality of each country</a></li>
<li><a href="../206238/index.html">Distributing servers: December contest</a></li>
<li><a href="../206242/index.html">Developers from Google have made the Amiga 500 emulator for Chrome</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>