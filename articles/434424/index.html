<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>JAVA SOUND API basics</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! I present to your attention the translation of the article "Java Sound, Getting Started, Part 1, Playback" . 

 Sound in JAVA, part one, Sta...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>JAVA SOUND API basics</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr!  I present to your attention the translation of the article <a href="https://www.developer.com/java/other/article.php/1572251">"Java Sound, Getting Started, Part 1, Playback"</a> . <br><br><h3>  Sound in JAVA, part one, Start.  Sound reproduction </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/1JZnj4eNHXE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  This is the first in a series of eight lessons that will fully familiarize you with the Java Sound API. <br><a name="habracut"></a><br>  What is sound in human perception?  This is the feeling that we experience when the change in air pressure is transmitted to tiny touch areas inside our ears. <br><br>  And the main purpose of creating Sound API, respectively, to provide you with the means to write code that will help in transferring pressure waves to the ears to the right subject at the right time. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Types of sound in Java: <br><br><ol><li>  The Java Sound API supports two basic types of audio (sound). </li><li>  Sound digitized and recorded directly as a file. </li><li>  Record as a MIDI file.  Very remote, but similar to a musical notation, where musical instruments are played in the desired sequence. </li></ol><br>  These types are quite different in their essence and we will concentrate on the first, since in most cases we deal with sound, which either needs to be digitized and recorded from an external source to a file or, vice versa, to reproduce the sound previously recorded from such a file. <br><br><h3>  Preview </h3><br>  The Java Sound API is based on the concept of <i>lines and mixers.</i> <br><br>  Further: <br>  We will describe the physical and electrical characteristics of the analog audio representation as applied to the <i>audio mixer</i> . <br><br>  We turn to the scenario of the beginning rock band, which in this case uses six microphones and two stereo speakers.  This is what we need to understand how the audio mixer works. <br><br>  Next we look at a number of Java Sound themes for programming, such as lines, mixers, formats for audio data, and more. <br><br>  We will understand the connections existing between the SourceDataLine, Clip, Mixer, AudioFormat objects and create a simple program that plays audio. <br><br>  Below we give an example of this program, which you can use to record and then play the recorded sound. <br><br>  In the following, we will provide a full explanation of the software code used for this purpose.  But not completely in this lesson. <br><br><h3>  Sample code and its consideration </h3><br>  <b>Physical and electrical characteristics of analog sound</b> <br><br>  The goal of our lesson is to introduce you to the basics of Java programming using the Java Sound API. <br><br>  The Java Sound API is based on the concept of an audio mixer, which is a device commonly used when playing sound almost anywhere: from rock concerts to listening to CDs at home.  But before embarking on a detailed explanation of the audio mixer, it will be useful to get acquainted with the physical and electrical characteristics of the analog sound itself. <br><br>  <i>Look at pic.</i>  <i>one</i> <br><br><img src="https://habrastorage.org/webt/ez/tu/sq/eztusq7byax0l9nu-5r6vj3vkxe.gif"><br><br>  Vasya Pupyrkin pushes speech. <br><br>  This drawing shows Vasya, who gives a speech using a system known as wide address.  Such a system typically includes a microphone, amplifier, and loudspeaker.  The purpose of this system is to strengthen Vasya‚Äôs voice so that it can be heard even in a large crowd. <br><br>  <b>Oscillations in the air</b> <br><br>  Briefly, when Vasya speaks, his vocal cords cause air particles to oscillate in his own larynx.  This leads to the appearance of sound waves, which in turn, cause the membrane of the microphone to oscillate and then turn into electrical oscillations of a very small amplitude in accuracy, which simulate the original Vasiny sound vibrations.  An amplifier, as its name implies, amplifies these electrical oscillations.  Then they get to the loudspeaker, which performs the inverse transformation of amplified electrical oscillations already into very amplified sound waves, but which nevertheless exactly repeat the same waves born in the vocal cords of Vasi Pupyrkin. <br><br>  <b>Dynamic microphone</b> <br><br>  Now look at pic.  2, which shows a schematic of a microphone device called dynamic. <br><br><img src="https://habrastorage.org/webt/hz/1v/ui/hz1vui2-yqnq4cg3xdpi5iy-1w0.gif"><br>  <i>Fig.</i>  <i>2 Diagram of a dynamic microphone</i> <br><br>  <b>Sound vibrations affect the membrane</b> <br><br>  The pressure of sound vibrations affects the flexible membrane inside the microphone.  This causes the membrane to vibrate, while the vibrations of the membrane repeat the vibrations of the sound waves. <br><br>  <b>Moving coil</b> <br><br>  The coil is wound from a thin wire attached to the membrane of the microphone.  As the membrane oscillates, the coil reciprocates in the magnetic field of the core made of a strong permanent magnet.  And how else did Faraday set up, while an electric current arises in the coil. <br><br>  <b>The electrical signal repeats the shape of the sound waves.</b> <br><br>  Thus, from a very weak current induced in the coil, an alternating electrical signal is obtained that follows the shape of the sound waves that act on the membrane of the microphone.  Further, this signal in the form of alternating voltage is fed to the input of the amplifier with  one. <br><br>  <b>Speaker</b> <br><br>  In essence, the principle of operation of the loudspeaker repeats the device of a dynamic microphone, only turned on in the opposite direction.  <i>(Naturally, in this case, the winding wires are much thicker, and the membrane is much larger to ensure operation with an amplified signal)</i> <i><br></i> <br><br><img src="https://habrastorage.org/webt/0e/ec/4x/0eec4xwyiyp2icsx69azgymv78c.gif"><br><br>  Fluctuations in the loudspeaker membrane affect air particles and create powerful sound waves.  The form of these waves exactly repeats the form of sound waves of much lower intensity, created by Vasin's vocal cords.  But the intensity of the new waves is now enough for sound vibrations from Vasya to reach the ears of people standing even in the back rows of a large crowd. <br><br>  <b>Rock concert</b> <br><br>  By this time, you may wonder, what does all this have to do with the Java Sound API?  But wait a little longer, we are leading the way to the basics of the audio mixer. <br><br>  The scheme described above was quite simple.  It consisted of Vasya Pupyrkin, a single microphone, amplifier and loudspeaker.  Now consider the scheme with Fig.  4, which presents a scene prepared for a rock concert by an aspiring musical group. <br><br><img src="https://habrastorage.org/webt/jh/zh/qo/jhzhqouio0xa25axcr164jch4du.gif"><br><br>  <b>Six microphones and two loudspeakers</b> <br><br>  In Fig.  4 six microphones located on the stage.  Two loudspeakers (speakers) are placed on the sides of the stage.  When a concert begins, performers sing or play music in each of the six microphones.  Accordingly, we will have six electrical signals that must be individually amplified and then fed to both speakers.  In addition to this, the performers can use various sound effects, such as reverb, which also need to be translated into electrical signals before they are sent to the loudspeakers. <br><br>  Two speakers on the sides of the stage are designed to create the effect of stereo sound.  That is, the electrical signal coming from the microphone located on the stage on the right should fall into the speaker located also on the right.  Similarly, the signal from the microphone on the left must be fed to the loudspeaker located to the left of the scene.  But the electrical signals from other microphones located closer to the center of the stage should already be transmitted to both speakers in appropriate proportions.  And two microphones right in the center should broadcast their signal to both speakers equally. <br><br>  <b>Audio mixer</b> <br><br>  The task above is just performed by an electronic device called an audio mixer. <br><br>  <b>Audio line (channel)</b> <br><br>  Although the author is not an expert in audio mixers, in his modest understanding, a typical audio mixer has the ability to receive at the input a certain number of electrical signals independent of each other, each of which represents a source audio signal or a line <i>(channel).</i> <br><br>  (The concept of the audio channel will become very important when we begin to deal in detail with the Java Sound API. <br><br>  <b>Independent processing of each audio channel</b> <br><br>  In any case, the standard audio mixer has the ability to amplify each audio line independently of the other channels.  Also, the mixer usually has the ability to overlay sound effects, such as, for example, reverb on any of the audio lines.  In the end, the mixer, as its name implies, can mix all individual electrical signals in the output channels as it is set, so as to control the contribution of each audio line to the output channels. (This control is usually called pan or panning - distribution in space). <br><br>  <b>Returning to stereo sound</b> <br><br>  Thus, in the diagram with Fig.  4, the audio mixer sound engineer has the ability to combine the signals from six microphones to receive two output signals, each of which is transmitted to its own loudspeaker. <br><br>  For successful operation, the signal from each microphone must be given in an appropriate proportion, depending on the physical location of the microphone on the stage.  (By changing the panning, a qualified sound engineer can change the contribution of each microphone if necessary, for example, if the lead vocalist moves during a concert around the stage). <br><br>  <b>Time to return to the world of programming</b> <br><br>  Let's now go back from the physical world to the world of programming.  According to Sun: <i>‚ÄúJava Sound does not imply special hardware configuration;</i>  <i>it is designed to allow various audio components to be installed into the system and be accessible to the user through the API.</i>  <i>Java Sound supports standard input and output functionality from a sound card (for example, for recording and playing audio files), as well as the ability to mix multiple audio streams. ‚Äù</i> <br><br>  <b>Mixers and Channels</b> <br><br>  As already mentioned, Java Sound API is built on the concept of mixers and channels.  If we move from the physical world to the programming world, then Sun writes the following regarding the mixer: <br><br>  <i>‚ÄúA mixer is an audio device with one or more channels.</i>  <i>But a mixer that really mixes an audio signal must have several input channels of source sources and at least one output target channel. ‚Äù</i> <br><br>  Input lines can be instances of classes with SourceDataLine objects, and output lines can be TargetDataLine.  The mixer can also receive a pre-recorded and looped sound as input, defining its input source channels as instances of class objects implementing the Clip interface. <br><br>  Line interface channel. <br><br>  Sun reports the following from the Line interface: ‚Äú <i>Line is an element of a digital audio pipeline such as an audio input or output port, mixer, or route of audio data to or from a mixer.</i>  <i>Audio data passing through the channel can be mono or multichannel (for example, stereo).</i>  <i>... The channel can have Controls, such as gain, pan and reverb. "</i> <br><br>  <b>Combining terms together</b> <br><br>  So, the above quotes from Sun meant the following terms <br><br>  SourceDataLine <br>  TargetDataLine <br>  Port <br>  Clip <br>  Controls <br><br>  <i>Fig.</i>  <i>5 shows an example of using these terms to build a simple audio output program.</i> <br><br><img src="https://habrastorage.org/webt/e1/5r/gh/e15rghejgy0b2reeciyvircdvua.gif"><br><br>  <b>Program script</b> <br><br>  From the program point of view  5 shows a Mixer object received with one Clip object and two SourceDataLine objects. <br><br>  <b>What is Clip</b> <br><br>  Clip is an object at the input of a mixer, the contents of which does not change with time.  In other words, you load audio data into a Clip object before you lose it.  Clip object audio content may be played one or more times.  You can loop Clip and then the content will play again and again. <br><br>  <b>Input stream</b> <br><br>  The SourceDataLine object, on the other hand, is a stream object at the input of the mixer.  This type of object can receive a stream of audio data and send it to the mixer in real time.  The required audio data can be obtained from various sources, such as audio files, a network connection, or a memory buffer. <br><br>  <b>Different types of channels</b> <br><br>  Thus, the Clip and SourceDataLine objects can be viewed as input channels for the Mixer object.  Each of these input channels can have its own: pan, gain and reverb. <br><br>  <b>Playing audio content</b> <br><br>  In such a simple system, the Mixer reads data from input lines, uses control to mix input signals, and provides output stream to one or more output channels, such as a speaker, line-out, headphone jack, and so on. <br><br>  Listing 11 shows a simple program that captures audio data from a microphone port, stores this data in memory, and then plays it through the speaker port. <br><br>  We will discuss only capture and playback.  Most of the above program accounts for creating a window and a graphical interface for the user in order to be able to control the recording and playback.  We will not discuss this part as going beyond the goal.  But then we will look at capturing and playing data.  We will discuss replay in this lesson, and capture in the following.  Along the way, we illustrate the use of an audio channel with the Java Sound API. <br><br>  The captured data is stored in the ByteArrayOutputStream object. <br><br>  The data capture code fragment provides for reading audio data from a microphone and storing them as a ByteArrayOutputStream object. <br><br>  The method called playAudio, which starts in Listing 1, plays audio data that was captured and stored in a ByteArrayOutputStream object. <br><br><pre><code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">playAudio</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">try</span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span> audioData[] = byteArrayOutputStream. toByteArray(); InputStream byteArrayInputStream = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ByteArrayInputStream( audioData);</code> </pre> <br>  <i>Listing 1</i> <br><br>  <b>We start with the standard code</b> <br><br>  The program fragment in Listing 1 is not really related to Java Sound. <br><br>  Its purpose is to: <br><br><ul><li>  Convert previously saved data into an array of type byte. </li><li>  Get the input stream for a byte array of data. </li></ul><br>  We need this to make the audio data available for later playback. <br><br>  <b>Go to the Sound API</b> <br><br>  The code line in Listing 2 is already related to the Java Sound API. <br><br><pre> <code class="java hljs"> AudioFormat audioFormat = getAudioFormat();</code> </pre><br>  <i>Listing 2</i> <br><br>  Here we briefly touch on the topic, which will be discussed in detail in the next lesson. <br><br>  <b>Two independent formats</b> <br><br>  Most often we are dealing with two independent formats for audio data. <br><br>  The format of the file (any) that contains audio data (in our program it is not there yet, since the data is stored in memory) <br><br>  The format of the presented audio data is in itself. <br><br>  <b>What is the audio format?</b> <br><br>  Here is what Sun writes about this: <br><br>  <i>‚ÄúEach data channel has its own audio format associated with its data stream.</i>  <i>The format (an instance of AudioFormat) determines the byte order of the audio stream.</i>  <i>The format parameters can be the number of channels, the sampling frequency, the quantization width, the encoding method, etc. The conventional coding methods can be linear PCM pulse modulation and its varieties. ‚Äù</i> <br><br>  <b>Byte sequence</b> <br><br>  The original audio data is a byte sequence of binary data.  There are various options for how to order and interpret this sequence.  We are not going to deal with all these options in detail, but we will discuss a little the audio format that we use here in our program. <br><br>  <b>Small retreat</b> <br><br>  Here we leave the playAudio method for now and look at the getAudioFormat method from Listing 2. <br><br>  <i>The complete getAudioFormat method is shown in Listing 3.</i> <br><br><pre> <code class="java hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> AudioFormat </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getAudioFormat</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> sampleRate = <span class="hljs-number"><span class="hljs-number">8000.0F</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> sampleSizeInBits = <span class="hljs-number"><span class="hljs-number">16</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> channels = <span class="hljs-number"><span class="hljs-number">1</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">boolean</span></span> signed = <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">boolean</span></span> bigEndian = <span class="hljs-keyword"><span class="hljs-keyword">false</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> AudioFormat( sampleRate, sampleSizeInBits, channels, signed, bigEndian); }<span class="hljs-comment"><span class="hljs-comment">//end getAudioFormat</span></span></code> </pre><br>  <i>Listing 3</i> <br><br>  In addition to the declaration of initialized variables, the code in Listing 3 contains one executable expression. <br><br>  <b>AudioFormat object</b> <br><br>  The getAudioFormat method creates and returns an instance of an object of the AudioFormat class.  Here is what Sun writes about this class: <br><br>  <i>‚ÄúThe AudioFormat class defines the specific ordering of data in an audio stream.</i>  <i>Referring to the fields of the AudioFormat object, you can get information on how to correctly interpret the bits in a binary data stream. ‚Äù</i> <br><br>  <b>We use the simplest constructor</b> <br><br>  The AudioFormat class has two types of constructors (we take the most trivial).  For this constructor, the following parameters are required: <br><br><ul><li>  Sample rate or sample rate per second (Available values: 8000, 11025, 16000, 22050 and 44100 samples per second) </li><li>  Data bit width (8 and 16 bits are available per sample) </li><li>  The number of channels (one channel for mono and two for stereo) </li><li>  Signed or unsigned data that is used in the stream (for example, the value changes from 0 to 255 or from -127 to +127) </li><li>  Byte order Big-endian or little-endian.  (if you transfer 16-bit values ‚Äã‚Äãby byte stream, it is important to know which byte comes first - the younger or the oldest, since both variants are encountered). </li></ul><br>  As you can see in Listing 3, in our case, we used the following parameters for an instance of an AudioFormat object. <br><br><ul><li>  8000 counts per second </li><li>  16 data size </li><li>  data sign </li><li>  order little-endian </li></ul><br>  By default, data is encoded linear PCM. <br><br>  The constructor we used creates an instance of the AudioFormat object using the linear pulse code modulation and parameters specified above (We will return to linear PCM and other encoding methods in the following lessons) <br><br>  <b>Returning to the playAudio method again</b> <br><br>  Now that we understand how the format of audio data in Java sound works, let's go back to the playAudio method.  As soon as we want to play the available audio data, we need an object of class AudioInputStream.  We will get an instance of it in Listing 4. <br><br><pre> <code class="java hljs"> audioInputStream = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> AudioInputStream( byteArrayInputStream, audioFormat, audioData.length/audioFormat. getFrameSize());</code> </pre><br>  <i>Listing 4</i> <br><br>  <b>Parameters for the AudioInputStream constructor</b> <br><br><ul><li>  The constructor for the AudioInputStream class requires the following three parameters: </li><li>  The stream on which the instance of the AudioInputStream object will be based (as we see for this purpose we are served by an instance of the ByteArrayInputStream object created earlier) </li><li>  The audio data format for this stream (for this purpose, we have already created an instance of the AudioFormat object) </li><li>  Frame size (frame) for data in this stream (see the description below) </li><li>  The first two parameters are clear from the code in Listing 4. However, the third parameter is not so obvious in itself. </li></ul><br>  <b>Get frame size</b> <br><br>  As we see in Listing 4, the value of the third parameter is created using calculations.  This is just one of the attributes of the audio format, which we have not mentioned before, and it is called a frame. <br><br>  <b>What is a frame?</b> <br><br>  For simple linear PCM used in our program, the frame contains a set of samples for all channels at a given time. <br><br>  Thus, the frame size is equal to the size of the count in bytes multiplied by the number of channels. <br><br>  As you may have already guessed, a method called getFrameSize returns the frame size in bytes. <br><br>  <b>Frame size calculation</b> <br><br>  Thus, the length of audio data in a frame can be calculated by dividing the total number of bytes in the audio data sequence by the number of bytes in one frame.  This calculation is used for the third parameter in Listing 4. <br><br>  <b>Getting a SourceDataLine Object</b> <br><br>  The next part of the program that we will discuss is a simple audio output system.  As we can see from the diagram in Fig. 5, we will need a SourceDataLine object to solve this problem. <br><br>  There are several ways to get an instance of the SourceDataLine object, all of which are very intricate.  The code in Listing 5 gets and saves a reference to an instance of the SourceDataLine object. <br><br>  (Note that this code does not just create an instance of the SourceDataLine object. It gets it in a rather roundabout way.) <br><br><pre> <code class="java hljs"> DataLine.Info dataLineInfo = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> DataLine.Info( SourceDataLine.class, audioFormat); sourceDataLine = (SourceDataLine) AudioSystem.getLine( dataLineInfo);</code> </pre><br>  <i>Listing 5</i> <br><br>  What is a SourceDataLine object? <br><br>  About this Sun writes the following: <br><br>  <i>‚ÄúSourceDataLine is a data channel in which data can be recorded.</i>  <i>It works as an input for a mixer.</i>  <i>The application writes the byte sequence to SourceDataLine, which buffers the data and delivers it to its mixer.</i>  <i>The mixer can transmit the data processed by it for the next stage, for example, to the output port.</i> <i><br><br></i>  <i>Notice that the naming convention for such a pairing reflects the relationship between the channel and its mixer. ‚Äù</i> <br><br>  <b>GetLine method for class AudioSystem</b> <br><br>  One way to get an instance of a SourceDataLine object is to call the static getLine method from the AudioSystem class (We will have a lot to report on in the following lessons). <br><br>  The getLine method requires an input parameter of type Line.Info and returns a Line object that corresponds to the description in the already defined Line.Info object. <br><br>  <b>Another short retreat</b> <br><br>  Sun reports the following information about the Line.Info object: <br><br>  ‚ÄúThe channel has its own information object (an instance of Line.Info), which shows which mixer (if any) sends the mixed audio data as output directly to the channel, and which mixer (if any) receives the audio data as input directly from the channel.  Line variations can correspond to subclasses of Line.Info, which allows you to specify other types of parameters related to specific types of channels ‚Äù <br><br>  <b>DataLine.Info object</b> <br><br>  The first expression in Listing 5 creates a new instance of the DataLine.Info object, which is a special form (subclass) of the Line.Info object. <br><br>  There are several overloaded constructors for the DataLine.Info class.  We have chosen to use the simplest.  This constructor requires two parameters. <br><br>  <b>Class object</b> <br><br>  The first parameter is Class, which represents the class that we defined as SourceDataLine.class <br><br>  The second parameter determines the desired data format for the channel.  We use for it an instance of the AudioFormat object, which has already been defined earlier. <br><br>  <b>We are already there where necessary?</b> <br><br>  Unfortunately, we still do not have the SourceDataLine object we need.  So far we have an object that only represents information about the SourceDataLine object we need. <br><br>  <b>Getting a SourceDataLine Object</b> <br><br>  The second expression in Listing 5 finally creates and stores an instance of SourceDataLine that we need so much.  This is done by calling the static getLine method of the AudioSystem class and passing the dataLineInfo as a parameter.  (In the next lesson we will look at how to get a Line object, working directly with the Mixer object). <br><br>  The getLine method returns a reference to an object of type Line, which is the parent of SourceDataLine.  Therefore, a downward type cast is necessary here before the return value is stored as a SourceDataLine. <br><br>  <b>Prepare to use the SourceDataLine object</b> <br><br>  Once we have an instance of the SourceDataLine object, we need to prepare it for opening and running, as shown in Listing 6. <br><br><pre> <code class="java hljs"> sourceDataLine.open(audioFormat); sourceDataLine.start();</code> </pre><br>  <i>Listing 6</i> <br><br>  <b>Opening method</b> <br><br>  As you can see in Listing 6, we sent the AudioFormat object to the opening method for the SourceDataLine object. <br><br>  According to Sun, this is the method: <br><br>  <i>‚ÄúOpens a line (channel) with a previously defined format, allowing it to receive any system resources it requires and to be in working (operational) state‚Äù</i> <br><br>  <b>Opening state</b> <br><br>  There are some more that Sun writes about him in this thread. <br><br>  <i>‚ÄúOpening and closing a channel affects the allocation of system resources.</i>  <i>Successful opening of the channel ensures that all necessary resources are provided to the channel.</i> <i><br><br></i>  <i>Opening the mixer, which has its input and output ports for audio data, includes, among other things, enabling the platform hardware on which the operation takes place and initializing the necessary software components.</i> <i><br><br></i>  <i>Opening a channel, which is a route for audio data to or from a mixer, includes both its initialization and receiving not at all limitless mixer resources.</i>  <i>In other words, a mixer has a finite number of channels, so several applications with their own channel requirements (and sometimes even one application) must correctly share mixer resources) ‚Äù</i> <br><br>  <b>Calling the start method for a channel</b> <br><br>  According to Sun, calling the start method for a channel means the following: <br><br>  <i>‚ÄúThe channel is allowed to use I / O lines.</i>  <i>If an attempt is made to use an already operating line, the method does nothing.</i>  <i>But after emptying the data buffer, the line resumes I / O start, starting with the first frame, which it did not have time to process, after the buffer was fully loaded. "</i> <br><br>  In our case, of course, the channel did not stop.  Since we launched it for the first time. <br><br>  <b>Now we have almost everything we need.</b> <br><br>  At this point, we have received all the audio resources we need to play the audio data that we have previously recorded and stored in an instance of the ByteArrayOutputStream object.  (Recall that this object exists only in the computer‚Äôs RAM). <br><br>  <b>Run the threads</b> <br><br>  We will create and run the stream to play the audio.  The code in Listing 7 creates and starts this thread. <br><br>  (Do not confuse the start method call on this thread with the start method call in the SourceDataLine object from Listing 6. These are completely different operations) <br><br><pre> <code class="java hljs">Thread playThread = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Thread(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> PlayThread()); playThread.start(); } <span class="hljs-keyword"><span class="hljs-keyword">catch</span></span> (Exception e) { System.out.println(e); System.exit(<span class="hljs-number"><span class="hljs-number">0</span></span>); }<span class="hljs-comment"><span class="hljs-comment">//end catch }//end playAudio</span></span></code> </pre><br>  <i>Listing 7</i> <br><br>  <b>Simple code</b> <br><br>  A fragment of the program from Listing 7 is very simple, but it does show an example of multi-thread programming in Java.  If you don‚Äôt understand it, you‚Äôd better get acquainted with this topic in specialized Java training topics. <br><br>  Once the stream is started, it will work until all previously recorded audio data is played to the end. <br><br>  <b>New Thread object</b> <br><br>  The code in Listing 7 creates an instance of the Thread object (stream) belonging to the PlayThread class.  This class is defined as an internal class in our program.  Its description begins in Listing 8. <br><br><pre> <code class="java hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">PlayThread</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Thread</span></span></span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span> tempBuffer[] = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[<span class="hljs-number"><span class="hljs-number">10000</span></span>];</code> </pre><br>  <i>Listing 8</i> <br><br>  <b>Thread run method</b> <br><br>  With the exception of declaring a variable tempBuffer (which refers to an array of bytes), the full definition of this class is simply the definition of the run method.  As you should already know, calling the start method in a Thread object causes the run method of this object to execute. <br><br>  The run method for this thread starts in Listing 9 <br><br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">run</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">try</span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> cnt; <span class="hljs-comment"><span class="hljs-comment">//  //    -1 // while((cnt = audioInputStream. read(tempBuffer, 0, tempBuffer.length)) != -1){ if(cnt &gt; 0){ //   //    //    //   . sourceDataLine.write( tempBuffer, 0, cnt); }//end if }//end while</span></span></code> </pre><br>  <i>Listing 9</i> <br><br>  <b>The first part of the program fragment in the run method</b> <br><br>  The run method contains two important parts, the first of which is shown in Listing 9. <br><br>  In sum, it uses a loop to read the audio data from the AudioInputStream object and pass it to the SourceDataLine object. <br><br>  The data sent to the SourceDataLine object is automatically transferred to the default audio output.  This may be a built-in computer speaker or a line-out.  (We will learn how to determine the necessary sound devices in the following lessons).  The variable cnt and temp buffer buffer is used to control the flow of data between read and write operations. <br><br>  <b>Reading data from AudioInputStream</b> <br><br>  A read loop from the AudioInputStream object, reads the specified maximum number of data bytes from the AudioInputStream and places their byte array. <br><br>  <b>Return value</b> <br><br>  Next, this method returns the total number of bytes read, or -1, if the end of the recorded sequence was reached.  The number of bytes read is stored in the variable cnt. <br><br>  <b>Write cycle in SourceDataLine</b> <br><br>  If the number of bytes read is greater than zero, then a transition to the data writing cycle in SourceDataLine occurs.  In this loop, the audio data goes to the mixer.  Bytes are read from the byte array in accordance with their indices and written into the channel buffer. <br><br>  <b>When the input stream dries up</b> <br><br>  When the reading cycle returns -1, this means that all previously recorded audio data has ended and then the control is transferred to the program fragment in Listing 10. <br><br><pre> <code class="java hljs"> sourceDataLine.drain(); sourceDataLine.close(); }<span class="hljs-keyword"><span class="hljs-keyword">catch</span></span> (Exception e) { System.out.println(e); System.exit(<span class="hljs-number"><span class="hljs-number">0</span></span>); }<span class="hljs-comment"><span class="hljs-comment">//end catch }//end run }//   PlayThread</span></span></code> </pre><br>  <i>Listing 10</i> <br><br>  <b>Lock and wait</b> <br><br>  The code in Listing 10 calls the drain method for a SourceDataLine object so that the program can block and wait for the internal buffer to empty into the SourceDataLine.  When the buffer is empty, this means that the entire next portion is delivered to the sound output of the computer. <br><br>  <b>Close SourceDataLine</b> <br><br>  The program then calls the close method to close the channel, thus showing that all system resources used by the channel are now free.  Sun reports the following channel closure: <br><br>  <i>‚ÄúThe closure of the channel indicates that all the resources involved for this channel can be released.</i>  <i>To release resources, the application must close the channels, whether they are independently involved or not, as well as when the application ends.</i>  <i>Mixers are supposed to share system resources and can be closed and opened repeatedly.</i>  <i>Other channels may or may not support reopening after they have been closed.</i>  <i>In general, the mechanisms for opening lines vary according to different subtypes. "</i> <br><br>  <b>And now the end of the story</b> <br><br>  So here we gave an explanation of how our program uses the Java Sound API in order to ensure the delivery of audio data from the internal memory of the computer to the sound card. <br><br>  <b>Run the program</b> <br><br>  Now you can compile and run the program from Listing 11, which crowns the end of our lesson. <br><br>  <b>Capture and play audio data</b> <br><br>  The program demonstrates the ability to record data from a microphone and play them through the sound card of your computer.  Instructions for using it are very simple. <br><br>  Run the program.  A simple GUI GUI, shown in Figure 6, should appear on the screen. <br><br><img src="https://habrastorage.org/webt/lf/7l/ew/lf7lew65yqcqstjqvdvmptmhevy.gif"><br><br><ul><li>  Click the Capture button and record any sounds on the microphone. </li><li>  Click the Stop button to stop recording. </li><li>  Click the Playback button to play the recording through your computer's audio output. </li></ul><br>  If you do not hear anything, try increasing the sensitivity of your microphone or the volume of the speakers. <br><br>  The program keeps a record in the computer‚Äôs memory, so be careful.  If you try to save too much audio data, you may run out of memory. <br><br>  <b>Conclusion</b> <br><br><ul><li>  We found that the Java Sound API is based on the concept of channels and mixers. </li><li>  We received initial information about the physical and electrical characteristics of the analog sound, to then understand the audio mixer device. </li><li>  We used the amateur rock concert scenario using six microphones and two stereo speakers to describe the possibility of using an audio mixer. </li><li>  We discussed a number of Java Sound programming topics, including mixers, channels, data format, and more. </li><li>  We explained to ourselves the general interconnections between the SourceDataLine, Clip, Mixer, AudioFormat objects and the ports in a simple audio output program. </li><li>  We got acquainted with the program that allows us to initially record and then play audio data. </li><li>  We received a detailed explanation of the code used to play audio data previously recorded in the computer's memory. </li></ul><br>  <b>What's next?</b> <br><br>  In this lesson, we learned that the Java Sound API is based on the concept of mixers and channels.  However, the code we discussed did not explicitly include mixers.  The AudioSystem class provided us with static methods that make it possible to write audio processing programs without directly accessing the mixers.  In other words, these static methods take the mixers away from us to the background. <br><br>                .      ,   ,    ,     . <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> javax.swing.*; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> java.awt.*; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> java.awt.event.*; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> java.io.*; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> javax.sound.sampled.*; <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">AudioCapture01</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">JFrame</span></span></span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">boolean</span></span> stopCapture = <span class="hljs-keyword"><span class="hljs-keyword">false</span></span>; ByteArrayOutputStream byteArrayOutputStream; AudioFormat audioFormat; TargetDataLine targetDataLine; AudioInputStream audioInputStream; SourceDataLine sourceDataLine; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">static</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">main</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( String args[])</span></span></span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> AudioCapture01(); }<span class="hljs-comment"><span class="hljs-comment">//end main public AudioCapture01(){ final JButton captureBtn = new JButton("Capture"); final JButton stopBtn = new JButton("Stop"); final JButton playBtn = new JButton("Playback"); captureBtn.setEnabled(true); stopBtn.setEnabled(false); playBtn.setEnabled(false); captureBtn.addActionListener( new ActionListener(){ public void actionPerformed( ActionEvent e){ captureBtn.setEnabled(false); stopBtn.setEnabled(true); playBtn.setEnabled(false); //  //   //   Stop captureAudio(); } } ); getContentPane().add(captureBtn); stopBtn.addActionListener( new ActionListener(){ public void actionPerformed( ActionEvent e){ captureBtn.setEnabled(true); stopBtn.setEnabled(false); playBtn.setEnabled(true); //  //    stopCapture = true; } } ); getContentPane().add(stopBtn); playBtn.addActionListener( new ActionListener(){ public void actionPerformed( ActionEvent e){ //  //    playAudio(); } } ); getContentPane().add(playBtn); getContentPane().setLayout( new FlowLayout()); setTitle("Capture/Playback Demo"); setDefaultCloseOperation( EXIT_ON_CLOSE); setSize(250,70); setVisible(true); } //    //     //   ByteArrayOutputStream private void captureAudio(){ try{ //    audioFormat = getAudioFormat(); DataLine.Info dataLineInfo = new DataLine.Info( TargetDataLine.class, audioFormat); targetDataLine = (TargetDataLine) AudioSystem.getLine( dataLineInfo); targetDataLine.open(audioFormat); targetDataLine.start(); //     //    //   //    Thread captureThread = new Thread( new CaptureThread()); captureThread.start(); } catch (Exception e) { System.out.println(e); System.exit(0); } } //    // ,    //  ByteArrayOutputStream private void playAudio() { try{ //  //  byte audioData[] = byteArrayOutputStream. toByteArray(); InputStream byteArrayInputStream = new ByteArrayInputStream( audioData); AudioFormat audioFormat = getAudioFormat(); audioInputStream = new AudioInputStream( byteArrayInputStream, audioFormat, audioData.length/audioFormat. getFrameSize()); DataLine.Info dataLineInfo = new DataLine.Info( SourceDataLine.class, audioFormat); sourceDataLine = (SourceDataLine) AudioSystem.getLine( dataLineInfo); sourceDataLine.open(audioFormat); sourceDataLine.start(); //    //     //     //      Thread playThread = new Thread(new PlayThread()); playThread.start(); } catch (Exception e) { System.out.println(e); System.exit(0); } } //     //  AudioFormat private AudioFormat getAudioFormat(){ float sampleRate = 8000.0F; //8000,11025,16000,22050,44100 int sampleSizeInBits = 16; //8,16 int channels = 1; //1,2 boolean signed = true; //true,false boolean bigEndian = false; //true,false return new AudioFormat( sampleRate, sampleSizeInBits, channels, signed, bigEndian); } //===================================// //    //    class CaptureThread extends Thread{ byte tempBuffer[] = new byte[10000]; public void run(){ byteArrayOutputStream = new ByteArrayOutputStream(); stopCapture = false; try{ while(!stopCapture){ int cnt = targetDataLine.read( tempBuffer, 0, tempBuffer.length); if(cnt &gt; 0){ //     byteArrayOutputStream.write( tempBuffer, 0, cnt); } } byteArrayOutputStream.close(); }catch (Exception e) { System.out.println(e); System.exit(0); } } } //===================================// //   //     class PlayThread extends Thread{ byte tempBuffer[] = new byte[10000]; public void run(){ try{ int cnt; //     -1 while((cnt = audioInputStream. read(tempBuffer, 0, tempBuffer.length)) != -1){ if(cnt &gt; 0){ //    //   //    //    sourceDataLine.write( tempBuffer, 0, cnt); } } sourceDataLine.drain(); sourceDataLine.close(); }catch (Exception e) { System.out.println(e); System.exit(0); } } } //===================================// }//end outer class AudioCapture01.java</span></span></code> </pre><br> <i> 11</i> </div><p>Source: <a href="https://habr.com/ru/post/434424/">https://habr.com/ru/post/434424/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../434412/index.html">Removal of the rocket before launch on the East</a></li>
<li><a href="../434414/index.html">Iceberg</a></li>
<li><a href="../434416/index.html">Read on vacation. The best posts on our blog for 2018</a></li>
<li><a href="../434418/index.html">Faster, louder, brighter: the physics of the mating "dancing" of hummingbirds</a></li>
<li><a href="../434422/index.html">Internet unprofitable things</a></li>
<li><a href="../434426/index.html">Checklist: How to submit reports on the simplified taxation system for 2018</a></li>
<li><a href="../434428/index.html">We collect, repair and carry vintage digital watches</a></li>
<li><a href="../434430/index.html">IBM showed 8-bit analog phase memory chip</a></li>
<li><a href="../434432/index.html">Wii, Wai, Wai, Frond - ‚Äúdifficulties of translation‚Äù, or what lies behind the new platform SAS Viya (Frond)</a></li>
<li><a href="../434438/index.html">Excel performance in pure javascript is achievable</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>