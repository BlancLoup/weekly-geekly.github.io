<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Viola-Jones method (Viola-Jones) as a basis for facial recognition</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Although the method was developed and introduced in 2001 by Paul Viola and Michael Jones [1, 2], it is still fundamental at the time of writing my pos...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Viola-Jones method (Viola-Jones) as a basis for facial recognition</h1><div class="post__text post__text-html js-mediator-article">  Although the method was developed and introduced in 2001 by Paul Viola and Michael Jones [1, 2], it is still fundamental at the time of writing my post to search for objects in a real-time image [2].  Following the <a href="http://habrahabr.ru/blogs/artificial_intelligence/67937/">topic of the</a> Habrayuser <a href="https://habrahabr.ru/users/indalo/" class="user_link">Indalo</a> about this method, I tried to write a program that recognizes the emotion on my face, but, unfortunately, I did not see the missing theory and description of the work of some algorithms on Habr√© except for indicating their names.  I decided to put it all together in one place.  At once I will say that I successfully wrote my program according to the algorithms.  How did you get to tell about them below, it's up to you, dear Habrachiteli! <a name="habracut"></a><habracut><br><br>  So, immediately to the point. <br><br><h3>  Viola Jones Method Description </h3><br>  <b>The basic principles</b> on which the method is based are: <br><ul><li>  images are used in the <b><a href="http://habrahabr.ru/blogs/algorithm/102919/">integral representation</a></b> , which allows you to calculate quickly the necessary objects; </li><li>  the <b><a href="http://en.wikipedia.org/wiki/Haar-like_features">signs of Haar are used</a></b> , with the help of which the search for the necessary object takes place (in this context, the face and its features); </li><li>  use <b><a href="http://machinelearning.ru/wiki/index.php%3Ftitle%3D%25D0%2591%25D1%2583%25D1%2581%25D1%2582%25D0%25B8%25D0%25BD%25D0%25B3">boosting</a></b> (from the English. <b>boost</b> - enhancement, enhancement) to select the most appropriate signs for the desired object in this part of the image; </li><li>  all signs come to the input of the <b><a href="http://ru.wikipedia.org/wiki/%25D0%2597%25D0%25B0%25D0%25B4%25D0%25B0%25D1%2587%25D0%25B0_%25D0%25BA%25D0%25BB%25D0%25B0%25D1%2581%25D1%2581%25D0%25B8%25D1%2584%25D0%25B8%25D0%25BA%25D0%25B0%25D1%2586%25D0%25B8%25D0%25B8">classifier</a></b> , which gives the result ‚Äútrue‚Äù or ‚Äúfalse‚Äù; </li><li>  <b><a href="http://courses.graphicon.ru/main/vision/2011/lectures/7">feature cascades</a></b> are used to quickly drop windows where no face has been found. </li></ul><br>  Training of classifiers is very slow, but <b>face search results are very fast</b> , which is why this method of face recognition in the image was chosen.  Viola-Jones is one of the best in terms of performance indicators of recognition / speed.  Also, this detector has an extremely low probability of false detection of the face.  The algorithm even works well and recognizes facial features at a slight angle, up to about 30 degrees.  When the angle of inclination is more than 30 degrees, the percentage of detections drops sharply.  And it does not allow in the standard implementation to detect the rotated face of a person at an arbitrary angle, which greatly complicates or makes it impossible to use the algorithm in modern production systems, taking into account their growing needs. <br>  A detailed analysis of the principles on which the Viola-Jones algorithm is based is required.  This method generally looks for faces and facial features according to the general <i>principle of a scanning window</i> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  Scanning window principle </h4><br>  In general, the task of detecting a person‚Äôs face and facial features on a digital image looks like this: <ul><li>  there is an <i>image</i> that <i>has the desired objects</i> .  It is represented by a <i>two-dimensional matrix of pixels of size w * h</i> , in which each pixel has the value: <br>  - <code> 0  255</code> , if it is a black and white image; <br>  - <code> 0  255 <sup>3</sup></code> , if it is a color image (components R, G, B). </li><li>  as a result of its work, the algorithm should identify faces and their features and <i>mark them</i> - the search is performed in the <i>active area of ‚Äã‚Äãthe</i> image with <i>rectangular signs</i> , with the help of which the found face and its features are described: <br>  <code>rectangle <sub>i</sub> = {x,y,w,h,a},</code> (1.1) <br>  where x, y are the coordinates of the center of the i-th rectangle, w is the width, h is the height, a is the angle of inclination of the rectangle to the vertical axis of the image. </li></ul><br>  In other words, the <i>scanning window (scanning window) approach is</i> used for pictures and photographs: the image is scanned by the search window (the so-called scanning window), and then the classifier is applied to each position.  <i>The system of training</i> and selection of the most significant features is <i>fully automated</i> and does not require human intervention, so this approach works quickly. <br>  The task of finding and finding faces in an image with the help of this principle is often the next step on the way to recognizing characteristic features, for example, verifying a person by a recognized face or recognizing facial expressions. <br><br><h4>  Integral representation of images </h4><br>  In order to perform any actions with data, the <i>integral representation of</i> images [3] is used in the Viola-Jones method.  This representation is often used in other methods, for example, in wavelet transforms, SURF, and many other parsed algorithms.  The integral representation allows you to quickly calculate the <i>total brightness of</i> an arbitrary rectangle in a given image, no matter what the rectangle is, the calculation time is constant. <br>  The integral representation of the image is a <i>matrix that is the same size as the original image</i> .  Each element of it contains the <i>sum of the intensities of all pixels that are to the left and above the element</i> .  The elements of the matrix are calculated according to the following formula: <br><img src="https://habrastorage.org/storage1/207d1f71/71ccea79/90aa700a/7d44e488.jpg">  (1.2) <br>  where I (i, j) is the brightness of the pixel of the original image. <br>  Each element of the matrix L [x, y] is the sum of pixels in the rectangle from (0,0) to (x, y), i.e.  the value of each pixel (x, y) is equal to the sum of the values ‚Äã‚Äãof all pixels to the left and above the given pixel (x, y).  The matrix calculation takes linear time proportional to the number of pixels in the image, therefore the integral image is calculated in one pass. <br>  The calculation of the matrix is ‚Äã‚Äãpossible according to the formula 1.3: <br>  <code>L(x,y) = I(x,y) ‚Äì L(x-1,y-1) + L(x,y-1) + L(x-1,y)</code> (1.3) <br>  Using such an integral matrix, one can very quickly calculate the sum of the pixels of an arbitrary rectangle, of arbitrary area. <br>  Suppose that in the rectangle ABCD there is an object D of interest to us: <br><img src="https://habrastorage.org/storage1/3990dfd2/36aff5c3/68127624/a1ad9900.jpg"><br>  From the figure it is clear that the amount inside the rectangle can be expressed in terms of the sums and differences of adjacent rectangles by the following formula: <br>  <code>S(ABCD) = L(A) + L() ‚Äî L(B) ‚Äî L(D)</code> (1.4) <br>  An approximate miscalculation is shown in the figure below: <br><img src="https://habrastorage.org/storage1/02bb2ab0/b7862f7f/3692c736/a5722043.jpg"><br><br><h4>  Signs of haar </h4><br>  <i>The sign</i> is the mapping f: X =&gt; D <sub>f</sub> , where D <sub>f</sub> is the set of valid values ‚Äã‚Äãof the sign.  If the attributes f <sub>1</sub> , ..., f <sub>n are given</sub> , then the feature vector x = (f <sub>1</sub> (x), ..., f <sub>n</sub> (x)) is called the <i>feature description of the</i> object x ‚àà X. It is permissible to identify the feature descriptions with the objects themselves.  Moreover, the set X = D <sub>f1</sub> * ... * D <sub>fn</sub> is called the attribute space [1]. <br>  Signs are divided into the following types depending on the set D <sub>f</sub> : <br><ul><li>  binary sign, D <sub>f</sub> = {0,1}; </li><li>  nominal feature: D <sub>f</sub> - finite set; </li><li>  ordinal feature: D <sub>f</sub> - finite ordered set; </li><li>  quantitative attribute: D <sub>f</sub> - the set of real numbers. </li></ul><br>  Naturally, there are applied tasks with different types of signs, and not all methods are suitable for solving them. <br>  In the standard Viola-Jones method, the rectangular signs shown in the figure below are used, they are called <i>Haar primitives</i> : <br><img src="https://habrastorage.org/storage1/b5e22c69/33e407c1/fdfdda2c/821151e7.jpg"><br>  The advanced Viola-Jones method used in the OpenCV library uses additional features: <br><img src="https://habrastorage.org/storage1/d3c33a53/2f27fbf9/4e78708a/1c60fe94.jpg"><br>  The calculated value of such a feature will be <br>  <code>F = XY</code> , (1.5) <br>  where X is the sum of the brightness values ‚Äã‚Äãof the points to be closed by the <i>light part of the feature</i> , and Y is the sum of the brightness values ‚Äã‚Äãof the points to be closed by the <i>dark part of the feature</i> .  To calculate them, the concept of an integral image, discussed above, is used. <br>  The signs of Haar give a point value of the <i>brightness difference along the X and Y axis, respectively</i> . <br><br><h4>  Window scan </h4><br>  Visualization of the scanning window in the program: <br><img src="https://habrastorage.org/storage1/d32c9409/8ed7b9ea/69792813/07a1218d.jpg"><br>  The algorithm for scanning windows with signs looks like this: <br><ul><li>  there is a test image, a scan window is selected, the used features are selected; </li><li>  then the scanning window starts to move sequentially through the image in 1 cell increments of the window (for example, the size of the window itself is 24 * 24 cells); </li><li>  when scanning an image in each window, approximately 200,000 variants of the location of signs are calculated by changing the scale of the signs and their position in the scanning window; </li><li>  scanning is performed sequentially for different scales; </li><li>  it is not the image itself that is scaled, but the scanning window (the cell size is changed); </li><li>  all found signs get to the classifier, which "makes a verdict." </li></ul><br><img src="https://habrastorage.org/storage1/0a15a9b1/caf3a63a/8b280a3b/8215b0cc.jpg"><br>  In the search process to calculate all the signs on low-power desktop PCs is simply unrealistic.  Consequently, the classifier should respond <i>only to a specific, necessary subset of</i> all <i>features</i> .  It is quite logical that it is necessary to train the classifier in finding faces on this particular subset.  This can be done by teaching the computer <i>automatically</i> . <br><br><h4>  The machine learning model used in the algorithm </h4><br>  <b>Machine learning</b> is the process by which a module gains new knowledge.  There is a recognized definition for this process: <blockquote>  <i>‚ÄúMachine learning is a science that studies computer algorithms that automatically improve during work‚Äù (Michel, 1996)</i> </blockquote>  .  Below is the learning process of the machine: <br><img src="https://habrastorage.org/storage1/fb3ce4f0/bbcd9c4b/12e9dc93/58b4d6b5.jpg"><br>  This process is part of the concept and technology called <i><a href="http://ru.wikipedia.org/wiki/Data_mining">Data mining</a> (information extraction and data mining)</i> , which includes, besides Machine Learning, such disciplines as Database Theory, Artificial Intelligence, Algorithmization, Pattern Recognition, and others. <br>  Machine learning in the Viola-Jones method solves such a problem as <i>classification</i> . <br><br><h4>  Classifier Training in Viola-Jones Method </h4><br>  In the context of the algorithm, there are many objects (images), divided in some way into classes.  A finite set of images is given for which it is known which class they belong to (for example, this could be the ‚Äúfrontal position of the nose‚Äù class).  This set is called a <i>training</i> set.  Class affiliation of other objects is not known.  It is required to construct an algorithm capable of classifying an arbitrary object from the original set [4]. <br>  <b>To classify an object</b> means to indicate the number (or class name) to which the given object belongs. <br>  <b>Classification of an object</b> - the number or name of the class, issued by the classification algorithm as a result of its application to this particular object. <br>  <b>Classifier</b> - in classification problems, this is an approximating function that decides which particular class this object belongs to. <br>  <b>The training set</b> is a finite amount of data. <br>  In machine learning, the classification task is related to the <b>training</b> section <b>with the teacher</b> <i>when the classes are divided</i> .  Pattern recognition is essentially a classification of images and signals.  In the case of the Viola-Jones algorithm for face identification and recognition, the classification is <i>two-class</i> . <br>  <i>The</i> classification is as follows: <br>  There is X - a set in which the description of objects is stored, Y - a finite set of numbers belonging to classes.  There is a dependency between them - the map Y *: X =&gt; Y. The training set is represented by <code>X <sub>m</sub> = {(x <sub>1</sub> ,y <sub>1</sub> ), ‚Ä¶, (x <sub>m</sub> ,y <sub>m</sub> )}</code> .  The function f is constructed from the feature vector X, which gives the answer for any possible observation X and is able to classify the object x‚ààX.  This simple rule should work well on new data. <br><br><h4>  AdaBoost used in the algorithm and development </h4><br>  To solve the problem of this, so complex learning, there is a <i>booster</i> technology. <br>  Boosting - a set of methods that contribute to improving the accuracy of analytical models.  <i>An effective model that admits few classification errors</i> is called <i>‚Äústrong</i> . <i>‚Äù</i>  <i>‚ÄúWeak‚Äù</i> , on the contrary, <i>does not allow to reliably</i> divide classes or give accurate predictions, it makes a lot of mistakes in the work.  Therefore, <b>boosting</b> (from the English. Boosting - boosting, enhancing, improving) means literally <b>‚Äústrengthening‚Äù the ‚Äúweak‚Äù models</b> [5] - this is a procedure for sequentially building the composition of machine learning algorithms, when each next algorithm seeks to compensate for the disadvantages of the composition of all previous algorithms. <br>  The idea of ‚Äã‚Äãboosting was <i>proposed by Robert Shapir (Schapire)</i> in the late 90s [6], when it was necessary to find a solution to the problem of having a lot of bad (slightly different from random) learning algorithms, to get one good one.  The basis of this idea is the construction of a <i>chain (ensemble) of classifiers</i> [5, 6], which is called a <b>cascade</b> , each of which (except the first) <i>learns from the mistakes of the previous one</i> .  For example, one of the first boosting algorithms Boost1 used a cascade of 3 models, the first of which was trained on the entire data set, the second on a sample of examples, in half of which the first gave correct answers, and the third on examples where ‚Äúanswers‚Äù the first two went their separate ways.  Thus, there is a sequential processing of examples by a cascade of classifiers, and in such a way that the task for each subsequent one becomes more difficult.  The result is determined by simple voting: the example refers to the class that is issued by most of the cascade models. <br>  Busting is a <a href="http://ru.wikipedia.org/wiki/%25D0%2596%25D0%25B0%25D0%25B4%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25B0%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC"><i>greedy algorithm for</i></a> constructing a composition of algorithms (greedy algorithm) - this is an algorithm that at every step makes the locally best choice in the hope that the final solution will be optimal.  Boosting over decisive trees is considered one of the most effective methods in terms of classification quality.  In many experiments, an almost unlimited decrease in the error rate was observed on an independent test sample as the composition was increased.  Moreover, the quality of the test sample often continued to improve even after reaching the unmistakable recognition of the entire training sample.  This turned the notion that existed for a long time that in order to increase the generalizing ability it is necessary to limit the complexity of the algorithms.  Using the example of boosting, it became clear that <i>arbitrarily complex compositions</i> can have a good quality if properly tuned [5]. <br>  <i>Mathematically, the</i> boosting comes up like this: <br>  Along with the sets X and Y, an auxiliary set R, called <i>the estimate space</i> , is introduced.  We consider algorithms that have the form of a superposition a (x) = C (b (x)), where the function b: X ‚Üí R is called an <i>algorithmic operator</i> , the function C: R ‚Üí Y is the <i>decision rule</i> . <br>  Many classification algorithms have exactly this structure: first, the estimates of the object's belonging to the classes are calculated, then the decision rule translates these estimates into the class number.  The value of the assessment, as a rule, characterizes the degree of confidence of the classification. <br>  <i>Algorithmic composition</i> - algorithm a: X ‚Üí Y <br>  <code>a(x) = C(F(b <sub>1</sub> (x), . . . , b <sub>T</sub> (x)), x ‚àà X</code> (1.6) <br>  composed of algorithmic operators b <sub>t</sub> : X ‚Üí R, t = 1, ..., T, corrective operation F: R <sup>T</sup> ‚Üí R and decision rule C: R ‚Üí Y. <br>  <i>The basic algorithms</i> denote the functions a <sub>t</sub> (x) = C (b <sub>t</sub> (x)), and for a fixed decision rule C, the operators b <sub>t</sub> (x) themselves. <br>  Superpositions of the form F (b <sub>1</sub> , ..., b <sub>T</sub> ) are mappings from X to R, that is, again, algorithmic operators. <br>  In classification problems into two non-intersecting classes, the set of real numbers is usually used as a rating space.  Decision rules can have customizable parameters.  So, in the Viola-Jones algorithm, the <i>threshold decision rule is used</i> , where, as a rule, an operator is first constructed at a zero value, and then the optimal value is chosen.  The process of sequential learning of basic algorithms is probably used most often in the construction of compositions. <br>  <i>The stopping criteria</i> can be used different, depending on the specifics of the task, it is also possible to apply several criteria together: <ul><li>  built a specified number of basic algorithms T; </li><li>  Achieved accuracy on the training set; </li><li>  The achieved accuracy on the control sample has not been improved over the last few steps with a certain parameter of the algorithm. </li></ul><br>  The development of this approach was the development of a more advanced <b>AdaBoost</b> family of algorithms for boosting ( <i>adaptive boosting</i> ), proposed by Yoav Freund (Freund) and Robert Schapire (1999), which can use an arbitrary number of classifiers and produce training on one set examples, alternately applying them to different steps. <br>  The classification problem into two classes is considered, Y = {‚àí1, + 1}.  For example, the basic algorithms also return only two responses ‚àí1 and +1, and the decision rule is fixed: C (b) = sign (b).  The desired algorithmic composition is: <br><img src="https://habrastorage.org/storage1/25d028f4/488a6a73/ca9cb0a2/2ffdf635.jpg">  (1.7) <br>  The quality functional of a composition Q <sub>t</sub> is defined as the number of errors it makes in the training set: <br><img src="https://habrastorage.org/storage1/e5a55e91/a65faf70/c8b1ab2f/aead418d.jpg">  (1.8) <br>  where W <sup>l</sup> = (w <sub>1</sub> , ..., w <sub>l</sub> ) is the vector of object weights. <br>  To solve the AdaBoosting problem, we need an exponential approximation of the threshold loss function [z &lt;0], with the exponential E <sup>z</sup> = e <sup>-z</sup> (seen in the figure, which demonstrates the operation of AdaBoost below). <br>  So, the <i>general</i> adaptive gain algorithm, AdaBoost, looks like this: <br> <code>: <br> Y = {‚àí1,+1}, b <sub>1</sub> (x), . . . , b <sub>T</sub> (x)  ‚àí1  + 1, X <sup>l</sup> ‚Äì  . <br></code> <ul><li> <code>: <br> Y = {‚àí1,+1}, b <sub>1</sub> (x), . . . , b <sub>T</sub> (x)  ‚àí1  + 1, X <sup>l</sup> ‚Äì  . <br></code> </li> <li> <code>: <br> Y = {‚àí1,+1}, b <sub>1</sub> (x), . . . , b <sub>T</sub> (x)  ‚àí1  + 1, X <sup>l</sup> ‚Äì  . <br></code> </li> <li> <code>: <br> Y = {‚àí1,+1}, b <sub>1</sub> (x), . . . , b <sub>T</sub> (x)  ‚àí1  + 1, X <sup>l</sup> ‚Äì  . <br></code> </li> </ul> <code>: <br> Y = {‚àí1,+1}, b <sub>1</sub> (x), . . . , b <sub>T</sub> (x)  ‚àí1  + 1, X <sup>l</sup> ‚Äì  . <br></code> <br> <code>: <br> 1.   : <br> w <sub>i</sub> := 1/‚Ñì, i = 1, . . . , ‚Ñì; (1.9) <br>   t = 1, . . . , T,     : <br> 2 . <img src="http://habrastorage.org/storage1/54f66e3b/72490229/eeac5466/4066002f.jpg"> (1.10) <br> 2 . <img src="http://habrastorage.org/storage1/8a4f5e5a/50a200ee/fae2d9a1/37a58506.jpg"> (1.11) <br> 3.   .    .     <img src="http://habrastorage.org/storage1/ff6b06c2/6cc4d825/55c64a7c/0252819d.jpg"> ,  b <sub>t</sub>    ,      ,  b <sub>t</sub>   x <sub>i</sub> .  ,           ,       : <br> <img src="http://habrastorage.org/storage1/7d9d17a5/315d5fb5/dae2a3b0/7a340f5a.jpg"> (1.12) <br> 4.   : <br> <img src="http://habrastorage.org/storage1/7daf679d/8079c995/1788a69b/3617a12f.jpg"> (1.13) <br></code> <br>  The implementation of AdaBoost at the preparatory stages of the 2nd step, the 12th and 642nd is shown in the figure.  After constructing a certain number of basic algorithms (say, a couple dozen), it is necessary to analyze the distribution of object weights.  The objects with the largest weights w <sub>i</sub> may be noise emissions, which should be excluded from the sample, and then start building the composition again. <br><img src="https://habrastorage.org/storage1/13359e60/13e99777/bcd11389/8d9a4ca9.png"><br>  AdaBoost advantages: <br><ul><li>  good generalizing ability.  In real problems, compositions are almost always constructed that exceed the basic algorithms in quality.  Generalizing ability may improve as the number of basic algorithms increases; </li><li>  ease of implementation; </li><li>  own overhead expenses are small.  The time to build a composition is almost completely determined by the learning time of the basic algorithms; </li><li>  the ability to identify objects that are noise emissions.  These are the most "difficult" objects x <sub>i</sub> , for which in the process of increasing the composition, the weights w <sub>i</sub> take on the greatest values. </li></ul><br><br>  Cons AdaBoost: <br><ul><li>  There is retraining in the presence of a significant level of noise in the data.  The exponential loss function too much increases the weights of the ‚Äúmost difficult‚Äù objects, on which many basic algorithms are mistaken.  However, it is these objects that most often turn out to be noise emissions.  As a result, AdaBoost begins to tune in to noise, which leads to retraining.  The problem is solved by removing outliers or using less ‚Äúaggressive‚Äù loss functions.  In particular, the GentleBoost algorithm is used; </li><li>  AdaBoost requires fairly long training samples.  Other linear correction methods, in particular, bagging, are capable of building algorithms of comparable quality for smaller data samples; </li><li>  There is a construction of a non-optimal set of basic algorithms.  To improve the composition, you can periodically return to the previously built algorithms and train them again. </li><li>  Boosting can lead to the construction of cumbersome compositions consisting of hundreds of algorithms.  Such compositions exclude the possibility of meaningful interpretation, require large amounts of memory to store the basic algorithms and significant time spent on the calculation of classifications. </li></ul><br>  Nowadays, the approach of strengthening simple classifiers is a popular and probably the most effective method of classification due to high speed and operational efficiency and relative ease of implementation. <br><br><h4>  Principles of the decision tree in the developed algorithm </h4><br>  <i>A decision tree</i> is a tree, in the leaves of which there are values ‚Äã‚Äãof the objective function, and in the remaining nodes there are transition conditions (for example, there is a Smile on the Face), determining which of the edges to go.  If, for a given observation, the condition is true, then a transition is made along the left edge, if false, then the right edge [4].  For example, the tree is shown in the following figure: <br><img src="https://habrastorage.org/storage1/84c830e0/28d7da1e/1d8e26f4/dd7954d3.png"><br>  An example <i>algorithm for creating a</i> decision tree is shown below: <br> <code>function Node = _( {(x,y)} ) { <br> if {y}  <br> return _(y); <br> test = __( {(x,y)} ); <br> {(x0,y0)} = {(x,y) | test(x) = 0}; <br> {(x1,y1)} = {(x,y) | test(x) = 1}; <br> LeftChild = _( {(x0,y0)} ); <br> RightChild = _( {(x1,y1)} ); <br> return _(test, LeftChild, RightChild); <br> } <br> //  <br> function main() { <br> {(X,Y)} = __(); <br> TreeRoot = _( {(X,Y)} ); <br> } <br></code> <br>  The advantages of such decisive trees are visibility, ease of working with them, speed.  Also, they are easily applied to problems with many classes. <br><br><h4>  Cascade model of the developed algorithm </h4><br>  The algorithm for searching for faces from my point of view is this: <br>  1. Definition of weak classifiers for rectangular features; <br>  2. For each movement of the scanning window, a rectangular feature is calculated on each example; <br>  3. Choose the most appropriate threshold for each feature; <br>  4. The best signs and the best suitable threshold are selected; <br>  5. Re-weighted sample. <br><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A cascading model of strong classifiers</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> is essentially the same decision tree, where each tree node is designed to detect almost all images of interest and reject regions that are not images. In addition, the tree nodes are placed in such a way that the closer the node is to the root of the tree, the smaller number of primitives it consists of and thus requires less time to make a decision. This type of cascade model is well suited for image processing, in which the total number of detected images is small. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In this case, the method can quickly decide that the region does not contain an image, and proceed to the next one. An example of a cascade model of strong classifiers:</font></font><br><img src="https://habrastorage.org/storage1/e125cf2f/aff7f67f/adcaaae7/145644c4.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The complexity of learning such cascades is O (xyz), where x steps are used, y examples and z signs. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Further, the cascade is applied to the image: </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1. Work with ‚Äúsimple‚Äù classifiers - in this case, part of the ‚Äúnegative‚Äù windows is discarded; </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2. The positive value of the first classifier starts the second one, more adapted and so on; </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3. The negative value of the classifier at any stage leads to an immediate transition to the next scanning window, the old window is discarded; </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4. The chain of classifiers becomes more complex, so the errors become much smaller. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To train such a cascade will require the following steps:</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1. Set the error level values ‚Äã‚Äãfor each stage (they must first be quantified when applied to the image from the training set) - they are called </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">detection</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">false positive rates</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - it is necessary that the level of detection be high and the level of false positive rates low; </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2. Signs are added until the parameters of the calculated stage reach the set level, here such auxiliary stages are possible as:</font></font><br>  but.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Testing additional small training set; </font></font><br>  b.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The AdaBoost threshold is deliberately lowered in order to find more objects, but in this regard, the greatest possible number of inaccurate definitions of objects; </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3. If false positive rates remain high, then the next stage or layer is added; </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4. False detections in the current stage are used as negative already in the next layer or stage. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In a </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">more formal form,</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> the cascade training algorithm is given below:</font></font><br> <code>a)    f (      )  d (     ) <br> b)        F <sub>target</sub> <br> c) P ‚Äì    <br> d) N ‚Äì    <br> e) F <sub>0</sub> = 1,0; D <sub>0</sub> = 1,0; i = 0 <br> f) while ( F <sub>i</sub> &gt; F <sub>target</sub> ) <br> i = i+1; n <sub>i</sub> = 0; F <sub>i</sub> = F <sub>i-1</sub> <br> while (F <sub>i</sub> = f * F <sub>i-1</sub> ) <br> n <sub>i</sub> = n <sub>i</sub> + 1 <br> AdaBoost(P, N, n <sub>i</sub> ) <br>         F <sub>i</sub>  D <sub>i</sub> ; <br>    i- ,           d*D <sub>i</sub> -1 (   F <sub>i</sub> ) ; <br> g) N = √ò; <br> h)  F <sub>i</sub> &gt; F <sub>target</sub> ,       ,   ,       N. <br></code> <br><br><h4>  findings </h4><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The mechanism of operation of the Viola-Jones algorithm was considered in detail. </font><font style="vertical-align: inherit;">You can improve this method and modify it, which I achieved in my written program - this will be discussed in my next topic.</font></font><br><br><h5>  Bibliography: </h5><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1. P. Viola and MJ Jones, ‚ÄúRapid Object Detection using a Boosted Cascade of Simple Features‚Äù, IEEE Conf. on Computer Vision and Pattern Recognition (CVPR 2001), 2001 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2. P. Viola and MJ Jones, ‚ÄúRobust real-time face detection,‚Äù International Journal of Computer Vision, vol. 57, no. 2, 2004., pp.137‚Äì154 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3. R. Gonsales, R. Woods, ‚ÄúDigital Image Processing‚Äù, ISBN 5-94836-028-8, publishing house: Technosphere, Moscow, 2005. - 1072 p. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4. Mestetsky LM, ‚ÄúMathematical methods of pattern recognition‚Äù, Moscow State University, Moscow, 2002‚Äì2004., P. 42 - 44 </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">5. Jan Àá Sochman, JiÀár¬¥ƒ± Matas, AdaBoost, Center for Machine Perception, Czech Technical University, Prague, 2010</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">6. Yoav Freund, Robert E. Schapire, ‚ÄúA Short Introduction to Boosting‚Äù, Shannon Laboratory, USA, 1999., pp. </font><font style="vertical-align: inherit;">771-780 </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PS</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> For the appearance of this article, I thank those people who pushed me the day before yesterday, and I got 2 karma points, and therefore the opportunity to present this article to HabraUzers. </font><font style="vertical-align: inherit;">Have a great weekend everyone!</font></font></habracut></div><p>Source: <a href="https://habr.com/ru/post/133826/">https://habr.com/ru/post/133826/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../133820/index.html">Support for user ssh keys in the cloud</a></li>
<li><a href="../133821/index.html">API creation</a></li>
<li><a href="../133823/index.html">How to become a successful freelancer</a></li>
<li><a href="../133824/index.html">Person rating of the Russian market of customized web development Tagline-2011</a></li>
<li><a href="../133825/index.html">Wikileaks will publish documents about companies that create technology to spy on people</a></li>
<li><a href="../133827/index.html">Network game in NES (Dendy) implemented on Flash P2P</a></li>
<li><a href="../133828/index.html">Using OpenGL Shaders in QML</a></li>
<li><a href="../133830/index.html">Wireless Joys: Samsung CA750 Monitor Review</a></li>
<li><a href="../133835/index.html">We remove the Store button in the music player iOS 5</a></li>
<li><a href="../133836/index.html">Replacing the menu of calls in iOS with an alternative one</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>