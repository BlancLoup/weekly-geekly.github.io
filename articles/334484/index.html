<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>High-Availability Postgres Clustering</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A couple of months ago we moved from Amazon to our dedicated servers (Hetzner), one of the reasons for this was the high cost of RDS. The task is to s...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>High-Availability Postgres Clustering</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/web/67f/924/b3d/67f924b3d6d54a6b9b46cda3562fb1df.png"><br><br>  A couple of months ago we moved from Amazon to our dedicated servers (Hetzner), one of the reasons for this was the high cost of RDS.  The task is to set up and launch a master-slave cluster on dedicated servers.  After googling and reading the official documentation, it was decided to assemble my own decision of the highly available asynchronous Postgres cluster. <br><a name="habracut"></a><br>  <b>Goals</b> <br><br><ul><li>  Use as few tools and dependencies as possible. </li><li>  Striving for transparency, no magic! </li><li>  Do not use all-included combine type pg-pool, stolon etc. </li><li>  Use docker and his buns. </li></ul><br>  So, let's begin.  Actually, we will need Postgres itself and such a wonderful tool as <a href="http://repmgr.org/">repmgr</a> , which manages replication management and cluster monitoring. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The project is called pg-dock, consists of 3 parts, each part lies on the githaba, you can take them and modify as you like. <br><br><ul><li>  <a href="https://github.com/xcrezd/pg-dock-config">pg-dock-config is a</a> ready <a href="https://github.com/xcrezd/pg-dock-config">-</a> made set of configuration files, now 2 nodes are registered there, master slave. </li><li>  <a href="https://github.com/xcrezd/pg-dock">pg-dock</a> is engaged in packing configs and delivering them to nodes, in the right form and in the right place. </li><li>  <a href="https://github.com/xcrezd/pg-dock-base">pg-dock-base</a> is the base docker image that will be run on the nodes. </li></ul><br>  Let's take a detailed look at each part: <br><br>  <a href="https://github.com/xcrezd/pg-dock-config">pg-dock-config</a> <br>  The cluster configuration has the following structure <br><table><tbody><tr><td width="291"><img src="https://habrastorage.org/web/849/22c/b76/84922cb761a2478f9fee364da04c5884.png"><br></td><td>  Two nodes (n1, n2) are already registered in the repository, if you have more than a node, then simply create another folder with the name of the new node.  For each node its configuration files.  It seems to me that everything is quite simple, for example, the env folder is the environment variables that will be picked up by the docker-compose, the postgres folder, respectively, the postgres configs, etc. <br><br>  For example, the file pg-dock-conf / n1 / env / main <br><pre><code class="bash hljs">POSTGRES_USER=postgres POSTGRES_PASSWORD=postgres POSTGRES_DB=testdb PGDATA=/var/lib/postgresql/data HETZNER_USER=**** HETZNER_PASS=**** HETZNER_FAILOVER_IP=1.2.3.4 HETZNER_ACTIVE_SERVER_IP=5.6.7.8</code> </pre> <br>  This tells us that during the initial initialization of the postgres, the user postgres and the testdb database will be created.  Also, variables for the failover-ip script are written here that change the ip to a new master node if the old one is not available. <br><br>  pg-dock-conf / n1 / env / backup <br>  The environment variables for the interval backup of the database on s3 are picked up by the docker-compose when starting the service. <br></td></tr></tbody></table><br>  If we have shared configuration files, then in order not to duplicate them on the nodes, we will put them in the shared folder. <br><br>  Let's go through its structure: <br><br><ul><li>  <b>failover</b> <br>  In my case, there is a script for Hetzner failover-ip, which changes the ip to a new master.  In your case, it can be a keepalived script or something similar. </li><li>  <b>initdb</b> <br>  All initializing sql queries must be put in this folder. </li><li>  <b>ssh</b> <br>  Here are the keys to connect to another node, in our example, the keys on all the nodes are the same, so they are in the shared folder.  ÷º÷º Ssh need repmgr to do such manipulations as switchover itp </li><li>  <b>sshd</b> <br>  The ssh server configuration file, ssh will work on port 2222 for us not to intersect with the default port on the host (22) </li></ul><br>  <a href="https://github.com/xcrezd/pg-dock">pg-dock</a> <br>  Here the configuration for each node is actually packaged. <br><br>  The bottom line is to pack the node's configuration into the docker image, push it into the hub or your registry, and then do the update on the node. <br><table><tbody><tr><td width="235"><img src="https://habrastorage.org/web/26d/78e/ef2/26d78eef27cf44eabfce748125eacad9.png"><br></td><td>  To work there are basic operations, create a build config (build.sh), update the config on the node <br>  (update.sh) and run the cluster itself (docker-compose.yml) <br><br><ul><li>  <b>helpers</b> <br>  Support files for cluster operation </li><li>  <b>manage</b> <br>  Ready scripts that simplify your life, for example, cloning data from the wizard, to start the slave.  Restore backup from S3. </li></ul><br></td></tr></tbody></table><br>  At startup: <br><br><pre> <code class="bash hljs">PG_DOCK_NODE=n1 PG_DOCK_CONF_IMAGE=n1v1 ./build.sh docker images REPOSITORY TAG IMAGE ID CREATED SIZE n1v1 latest 712e6b2ace1a 6 minutes ago 1.17MB</code> </pre><br>  The pg-dock-conf / n1 configuration is copied to the pg-dock / pg-dock-conf-n1 folder, then the docker build starts with all the dependencies, the output is an image with the name n1v1 which stores our configuration for node n1. <br><br>  At startup: <br><br><pre> <code class="bash hljs">PG_DOCK_CONF_IMAGE=n1v1 ./update.sh</code> </pre> <br>  This will launch a container that will update all the configuration files on the host.  Thus, we can have several configuration images, rollback to different versions of it. <br><br>  <a href="https://github.com/xcrezd/pg-dock-base">pg-docker-base</a> <br>  The base docker image in which all packages for cluster operation are installed: repmgr, rsync, openssh-server, supervisor ( <a href="https://github.com/xcrezd/pg-dock-base/blob/master/Dockerfile">Dockerfile</a> ).  The image itself is based on the latest version of postgres 9.6.3, but you can use any other build.  Components run by supervisor from under postgres user.  We will run this image on our servers (rsync, openssh-server is required for repmgr to work). <br><br>  <b>Let's run the cluster!</b> <br>  <i>For convenience, in this article all the manipulations will be done with the help of the docker-machine.</i> <br><br>  We clone the pg-dock and pg-dock-conf projects into the working folder (for example lab) <br><br><pre> <code class="bash hljs">mkdir ~/lab &amp;&amp; <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> ~/lab git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/xcrezd/pg-dock git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/xcrezd/pg-dock-conf</code> </pre> <br>  We create nodes, group and the user of postgres (uid, gid has to be 5432 on a host and in the container) <br><br><pre> <code class="bash hljs">docker-machine create n1 docker-machine ssh n1 sudo addgroup postgres --gid 5432 docker-machine ssh n1 sudo adduser -u 5432 -h /home/postgres --shell /bin/sh -D -G postgres postgres <span class="hljs-comment"><span class="hljs-comment"># debian/ubuntu #sudo adduser --uid 5432 --home /home/postgres --shell /bin/bash --ingroup postgres --disabled-password postgres docker-machine create n2 docker-machine ssh n2 sudo addgroup postgres -g 5432 docker-machine ssh n2 sudo adduser -u 5432 -h /home/postgres --shell /bin/sh -D -G postgres postgres</span></span></code> </pre> <br>  Add ip node in / etc / hosts <br><br><pre> <code class="bash hljs">docker-machine ip n1 <span class="hljs-comment"><span class="hljs-comment">#192.168.99.100 docker-machine ip n2 #192.168.99.101 #   n1 docker-machine ssh n1 "sudo sh -c 'echo 192.168.99.100 n1 &gt;&gt; /etc/hosts'" docker-machine ssh n1 "sudo sh -c 'echo 192.168.99.101 n2 &gt;&gt; /etc/hosts'" #   n2 docker-machine ssh n2 "sudo sh -c 'echo 192.168.99.100 n1 &gt;&gt; /etc/hosts'" docker-machine ssh n2 "sudo sh -c 'echo 192.168.99.101 n2 &gt;&gt; /etc/hosts'"</span></span></code> </pre> <br>  If the IP of your machines differ from the IP in the article, then you need to add them to <br><br><ul><li>  pg-dock-config / n1 / postgres / pg_hba.conf </li><li>  pg-dock-config / n2 / postgres / pg_hba.conf </li></ul><br>  Create configuration images and immediately update them on nodes <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> pg-dock docker-machine use n1 PG_DOCK_NODE=n1 PG_DOCK_CONF_IMAGE=n1v1 ./build.sh PG_DOCK_CONF_IMAGE=n1v1 ./update.sh docker-machine use n2 PG_DOCK_NODE=n2 PG_DOCK_CONF_IMAGE=n2v1 ./build.sh PG_DOCK_CONF_IMAGE=n2v1 ./update.sh</code> </pre> <br>  Pay attention to the docker-machine use command ( <a href="">how to do it</a> ), each time it is used, we change the context of the client docker, that is, in the first case, all the docker manipulations will be on node n1 and then on n2. <br><br>  We start containers <br><br><pre> <code class="bash hljs">docker-machine use n1 PG_DOCK_NODE=n1 docker-compose up -d docker-machine use n2 PG_DOCK_NODE=n2 docker-compose up -d</code> </pre> <br>  docker-compose will also launch the pg-dock-backup container, which will make a periodic backup on s3. <br>  Now let's see where the files we need are stored: <br><table><tbody><tr><td>  <b>Files</b> <br></td><td>  <b>Host</b> <br></td><td>  <b>Container</b> <br></td></tr><tr><td>  DB <br></td><td>  / opt / pg-dock / data <br></td><td>  / var / lib / postgresql / data <br></td></tr><tr><td>  Logs <br></td><td>  / opt / pg-dock / logs <br></td><td>  / var / log / supervisor <br></td></tr><tr><td>  Configuration and Scripts <br></td><td>  / opt / pg-dock / scripts <br></td><td>  ** learn docker-compose.yml <br></td></tr></tbody></table><br>  Go ahead, set up a cluster <br><br><pre> <code class="bash hljs">docker-machine use n1 <span class="hljs-comment"><span class="hljs-comment">#    docker exec -it -u postgres pg-dock repmgr master register docker-machine use n2 #    n1 docker exec -it -u postgres -e PG_DOCK_FROM=n1 pg-dock manage/repmgr_clone_standby.sh #    docker exec -it -u postgres pg-dock repmgr standby register</span></span></code> </pre> <br>  That's it, the cluster is ready <br><br><pre> <code class="bash hljs">docker <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it -u postgres pg-dock repmgr cluster show Role | Name | Upstream | Connection String ----------+------|----------|-------------------------------------------- * master | n1 | | host=n1 port=5432 user=repmgr dbname=repmgr standby | n2 | n1 | host=n2 port=5432 user=repmgr dbname=repmgr</code> </pre> <br><br>  Let's check it out.  In the pg-dock-config / shared / tests folder we have such preparations for testing our cluster: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#   cat tests/prepare.sh CREATE TABLE IF NOT EXISTS testtable (id serial, data text); GRANT ALL PRIVILEGES ON TABLE testtable TO postgres; # 100000  cat tests/insert.sh insert into testtable select nextval('testtable_id_seq'::regclass), md5(generate_series(1,1000000)::text); #     cat tests/select.sh select count(*) from testtable;</span></span></code> </pre><br>  We create a test table, fill it with data and check if they are on the slave: <br><br><pre> <code class="bash hljs">docker-machine use n1 <span class="hljs-comment"><span class="hljs-comment">#      docker exec -it -u postgres pg-dock config/tests/prepare.sh #    docker exec -it -u postgres pg-dock config/tests/insert.sh INSERT 0 1000000 docker-machine use n2 #     n2 () docker exec -it -u postgres pg-dock config/tests/select.sh count --------- 1000000 (1 row)</span></span></code> </pre><br>  Profit! <br><br>  Now let's take a look at the master drop script: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#   docker-machine use n1 docker stop pg-dock #  repmgr   docker-machine use n2 docker exec -it pg-dock tailf /var/log/supervisor/repmgr-stderr.log #NOTICE: STANDBY PROMOTE successful</span></span></code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Full log</b> <div class="spoiler_text">  [2017-07-12 12:51:49] [ERROR] unable to connect to server: Connection refused <br>  Is the server running on host "n1" (192.168.99.100) and accepting <br>  TCP / IP connections on port 5432? <br><br>  [2017-07-12 12:51:49] [ERROR] connection to database failed <br>  Is the server running on host "n1" (192.168.99.100) and accepting <br>  TCP / IP connections on port 5432? <br><br>  [2017-07-12 12:51:49] [WARNING] connection to master has been lost, trying to recover ... 60 seconds before failover decision <br>  [2017-07-12 12:51:59] [WARNING] connection to master has been lost, trying to recover ... 50 seconds before failover decision <br>  [2017-07-12 12:52:09] [WARNING] connection to master has been lost, trying to recover ... 40 seconds before failover decision <br>  [2017-07-12 12:52:19] [WARNING] connection to master has been lost, trying to recover ... 30 seconds before failover decision <br>  [2017-07-12 12:52:29] [WARNING] connection to master has been lost, trying to recover ... 20 seconds before failover decision <br>  [2017-07-12 12:52:39] [WARNING] connection to master has been lost, trying to recover ... 10 seconds before failover decision <br>  [2017-07-12 12:52:49] [ERROR] unable to reconnect to master (timeout 60 seconds) ... <br>  [2017-07-12 12:52:54] [NOTICE] this site, promoting ... <br>  % Total% Received% Xferd Average Speed ‚Äã‚ÄãTime Time Time Current <br>  Dload Upload Total Spent Left Speed <br>  100 171 100 143 0 28 3 0 0:00:47 0:00:39 0:00:08 31 <br>  Did not connect to server: Connection refused <br>  Is the server running on host "n1" (192.168.99.100) and accepting <br>  TCP / IP connections on port 5432? <br><br>  NOTICE: promoting standby <br>  NOTICE: promoting server using '/usr/lib/postgresql/9.6/bin/pg_ctl -D / var / lib / postgresql / data promote' <br>  NOTICE: STANDBY PROMOTE successful <br></div></div><br>  Consider the status of the cluster: <br><br><pre> <code class="bash hljs">docker <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it -u postgres pg-dock repmgr cluster show Role | Name | Upstream | Connection String ---------+------|----------|-------------------------------------------- FAILED | n1 | | host=n1 port=5432 user=repmgr dbname=repmgr * master | n2 | | host=n2 port=5432 user=repmgr dbname=repmgr</code> </pre> <br>  Now the new master has n2, failover ip also points to it. <br>  Now let's return the old master as a new slave. <br><pre> <code class="bash hljs">docker-machine use n1 <span class="hljs-comment"><span class="hljs-comment">#  PG_DOCK_NODE=n1 docker-compose up -d #  #    n2 docker exec -it -u postgres -e PG_DOCK_FROM=n2 pg-dock manage/repmgr_clone_standby.sh #    docker exec -it -u postgres pg-dock repmgr standby register -F</span></span></code> </pre> <br>  Consider the status of the cluster: <br><br><pre> <code class="bash hljs">docker <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it -u postgres pg-dock repmgr cluster show Role | Name | Upstream | Connection String ---------+------|-----------|-------------------------------------------- * master | n2 | | host=n2 port=5432 user=repmgr dbname=repmgr standby| n1 | n2 | host=n1 port=5432 user=repmgr dbname=repmgr</code> </pre> <br>  Done!  And that's what we did to do;  We dropped the master, the automatic assignment of the slave by the new master worked, the failover IP was changed.  The system continues to function.  Then we reanimated node n1, made it a new slave.  Now for the sake of interest, we will make a swithover - that is, we will manually make n1 a master and n2 a slave, as it was before.  That's exactly what repmgr needs for ssh, the slave connects via ssh to the master and makes the necessary manipulations with scripts. <br><br>  switchover: <br><br><pre> <code class="bash hljs">docker-machine use n1 docker <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it -u postgres pg-dock repmgr standby switchover <span class="hljs-comment"><span class="hljs-comment">#NOTICE: switchover was successful</span></span></code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Full log</b> <div class="spoiler_text">  NOTICE: switching current node 1 to master server and current master to standby ... <br>  Warning: Permanently added '[n2]: 2222, [192.168.99.101]: 2222' (ECDSA) to the list of known hosts. <br>  NOTICE: 1 files copied to / tmp / repmgr-n2-archive <br>  NOTICE: current master has been stopped <br>  Did not connect to server: Connection refused <br>  Is the server running on host "n2" (192.168.99.101) and accepting <br>  TCP / IP connections on port 5432? <br><br>  NOTICE: promoting standby <br>  NOTICE: promoting server using '/usr/lib/postgresql/9.6/bin/pg_ctl -D / var / lib / postgresql / data promote' <br>  server promoting <br>  NOTICE: STANDBY PROMOTE successful <br>  NOTICE: Executing pg_rewind on old master server <br>  Warning: Permanently added '[n2]: 2222, [192.168.99.101]: 2222' (ECDSA) to the list of known hosts. <br>  Warning: Permanently added '[n2]: 2222, [192.168.99.101]: 2222' (ECDSA) to the list of known hosts. <br>  NOTICE: 1 files copied to / var / lib / postgresql / data <br>  Warning: Permanently added '[n2]: 2222, [192.168.99.101]: 2222' (ECDSA) to the list of known hosts. <br>  Warning: Permanently added '[n2]: 2222, [192.168.99.101]: 2222' (ECDSA) to the list of known hosts. <br>  NOTICE: restarting server using '/usr/lib/postgresql/9.6/bin/pg_ctl -w -D / var / lib / postgresql / data -m fast restart' <br>  pg_ctl: PID file "/var/lib/postgresql/data/postmaster.pid" does not exist <br>  Is server running? <br>  starting server anyway <br>  NOTICE: replication slot "repmgr_slot_1" deleted on node 2 <br>  NOTICE: switchover was successful <br></div></div><br>  Consider the status of the cluster: <br><br><pre> <code class="bash hljs">docker <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it -u postgres pg-dock repmgr cluster show Role | Name | Upstream | Connection String ----------+------|----------|-------------------------------------------- standby | n2 | | host=n2 port=5432 user=repmgr dbname=repmgr * master | n1 | | host=n1 port=5432 user=repmgr dbname=repmgr</code> </pre> <br><br>  That's it, the next time when we need to update the node's configuration, be it postgres, repmgr, or supervisor config, we just pack it and update it: <br><br><pre> <code class="bash hljs">PG_DOCK_NODE=n1 PG_DOCK_CONF_IMAGE=n1v1 ./build.sh PG_DOCK_CONF_IMAGE=n1v1 ./update.sh</code> </pre> <br>  After updating the new configuration: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#  postgres docker exec -it -u postgres pg-dock psql -c "SELECT pg_reload_conf();" #  supervisor docker exec -it -u postgres pg-dock supervisorctl reread #   docker exec -it -u postgres pg-dock supervisorctl restart foo:sshd</span></span></code> </pre> <br>  * A <i>nice bonus, the supervisor has a log rotation feature, so we don‚Äôt have to worry about it either.</i> <br>  * <i>Containers work directly through the host network, thereby avoiding network virtualization delays.</i> <br>  * I <i>recommend adding already existing production nodes in the docker-machine, this will greatly simplify your life.</i> <br><br>  Now let's touch on the topic of balancing requests.  I didn‚Äôt want to complicate (that is, use pg-pool, haproxy, stolon), so we will do balancing on the application side, thereby relieving ourselves of the responsibility for organizing the high availability of the balancer itself.  Our backends are written in ruby, so the choice fell on gem <a href="https://github.com/taskrabbit/makara">makara</a> .  Heme can separate requests for sample and data modification (insert / update / delete / alter), sample requests can be balanced between several nodes (slaves).  In case of failure of one of the nodes, heme can temporarily exclude him from the pool. <br><br>  Sample database.yml configuration file: <br><br><pre> <code class="hljs pgsql">production: adapter: <span class="hljs-string"><span class="hljs-string">'postgresql_makara'</span></span> makara: # the <span class="hljs-keyword"><span class="hljs-keyword">following</span></span> are <span class="hljs-keyword"><span class="hljs-keyword">default</span></span> <span class="hljs-keyword"><span class="hljs-keyword">values</span></span> blacklist_duration: <span class="hljs-number"><span class="hljs-number">5</span></span> master_ttl: <span class="hljs-number"><span class="hljs-number">5</span></span> master_strategy: failover sticky: <span class="hljs-keyword"><span class="hljs-keyword">true</span></span> connections: - <span class="hljs-keyword"><span class="hljs-keyword">role</span></span>: master <span class="hljs-keyword"><span class="hljs-keyword">database</span></span>: mydb host: <span class="hljs-number"><span class="hljs-number">123.123</span></span><span class="hljs-number"><span class="hljs-number">.123</span></span><span class="hljs-number"><span class="hljs-number">.123</span></span> port: <span class="hljs-number"><span class="hljs-number">6543</span></span> weight: <span class="hljs-number"><span class="hljs-number">3</span></span> username: &lt;%= ENV[<span class="hljs-string"><span class="hljs-string">'DATABASE_USERNAME'</span></span>] %&gt; <span class="hljs-keyword"><span class="hljs-keyword">password</span></span>: &lt;%= ENV[<span class="hljs-string"><span class="hljs-string">'DATABASE_PASSWORD'</span></span>] %&gt; - <span class="hljs-keyword"><span class="hljs-keyword">role</span></span>: slave <span class="hljs-keyword"><span class="hljs-keyword">database</span></span>: mydb host: <span class="hljs-number"><span class="hljs-number">123.123</span></span><span class="hljs-number"><span class="hljs-number">.123</span></span><span class="hljs-number"><span class="hljs-number">.124</span></span> port: <span class="hljs-number"><span class="hljs-number">6543</span></span> weight: <span class="hljs-number"><span class="hljs-number">7</span></span> username: &lt;%= ENV[<span class="hljs-string"><span class="hljs-string">'DATABASE_USERNAME'</span></span>] %&gt; <span class="hljs-keyword"><span class="hljs-keyword">password</span></span>: &lt;%= ENV[<span class="hljs-string"><span class="hljs-string">'DATABASE_PASSWORD'</span></span>] %&gt;</code> </pre><br>  Libraries in other languages ‚Äã‚Äã/ frameworks: <br>  ‚Üí <a href="https://laravel.com/docs/5.4/database">laravel</a> <br>  ‚Üí <a href="http://www.yiiframework.com/doc-2.0/guide-db-dao.html">Yii2</a> <br>  ‚Üí <a href="http://www.codeday.top/2017/01/08/6521.html">Node.js</a> <br><br><h3>  Conclusion </h3><br>  So, what we got in the end: <br><br><ul><li>  Self-sufficient master-standby cluster ready for battle. </li><li>  Transparency of all components, easy replaceability. </li><li>  Automatic failover in case of master failure (repmgr) </li><li>  Load balancing on the client, thereby removing the responsibility for the availability of the balancer itself </li><li>  The lack of a single point of failure, repmgr will run a script that will transfer the IP address to the new node, which was upgraded to the master in case of failure.  In the template there is a script for hetzner, but nothing prevents to add keepalived, aws elasticIp, drdb, pacemaker, corosync. </li><li>  Version control, the ability to do rollback in case of problems / ab testing. </li><li>  Ability to customize the system for themselves, add nodes, repmgr witness, for example, configuration flexibility and its changes. </li><li>  Periodic backup on S3 </li></ul><br>  In the next article I will tell you how to place pg-dock and PgBouncer on one node without losing high availability, thank you all for your attention! <br><br>  Recommendations for familiarization: <br><br><ul><li>  <a href="https://wiki.postgresql.org/wiki/Streaming_Replication">Streaming replication in postgres</a> </li><li>  <a href="https://github.com/2ndQuadrant/repmgr">Repmgr</a> </li><li>  <a href="http://supervisord.org/">Supervisord</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/334484/">https://habr.com/ru/post/334484/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../334472/index.html">Working with databases in the 3CX Call Flow Designer development environment</a></li>
<li><a href="../334474/index.html">Pygest # 14. Releases, articles, interesting projects from the world of Python [July 18, 2017 - July 31, 2017]</a></li>
<li><a href="../334476/index.html">Association of .NET communities</a></li>
<li><a href="../334478/index.html">Bluetooth mesh - the basic components of the network</a></li>
<li><a href="../334482/index.html">I invite you to summer open lectures on the gaming industry at VSBI</a></li>
<li><a href="../334486/index.html">We have long hands: 7 foreign sites where you can find remote work</a></li>
<li><a href="../334488/index.html">Build AngularJs Bundle Apps With Gulp</a></li>
<li><a href="../334490/index.html">Hackathon during working hours</a></li>
<li><a href="../334492/index.html">iRobot will sell data from IoT devices manufactured by the company</a></li>
<li><a href="../334498/index.html">How Chrome and Firefox agree to transfer two video streams</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>