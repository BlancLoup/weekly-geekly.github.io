<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Google tells how the "exponential" growth of AI changes the very nature of computing</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Google programmer Clif Young explains how the explosive development of depth learning algorithms coincides with the failure of Moore's law, the empiri...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">🔎</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">📜</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">⬆️</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">⬇️</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Google tells how the "exponential" growth of AI changes the very nature of computing</h1><div class="post__text post__text-html js-mediator-article"><h3>  Google programmer Clif Young explains how the explosive development of depth learning algorithms coincides with the failure of Moore's law, the empirical rule of computer chip progress for decades, and makes it develop fundamentally new computational schemes </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/f16/257/7ce/f162577cea1b28010ec340bfff65f307.jpg"><br><br>  The explosive development of AI and machine learning algorithms changes the very nature of computing - as they say in one of the largest AI practicing companies - on Google.  Google programmer Cliff Young spoke at the opening of the autumn microprocessor conference organized by the Linley Group, a popular symposium on computer chips held by a respected semiconductor analysis company. <br><br>  Yang said that the use of AI had moved into the “exponential phase” at the very moment when Moore’s law, the empirical rule of progress for computer chips, had been completely slowed down for decades. <br><a name="habracut"></a><br>  “The times are pretty nervous,” he said thoughtfully.  “Digital CMOS is slowing down, we see problems with the 10nm process at Intel, we see them at the 7nm process from GlobalFoundries, and at the same time as the development of depth learning, an economic inquiry appears.  CMOS, a complementary metal-oxide-semiconductor structure, is the most common material used to make computer chips. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      While classic chips hardly increase efficiency and productivity, requests from AI researchers are growing, said Young.  He gave a bit of statistics: the number of machine learning papers stored on the arXiv preprint site maintained by Cornell University doubles every 18 months.  And the number of internal projects focusing on AI in Google, he said, also doubles every 18 months.  The need for the number of floating-point operations needed for processing neural networks used in machine learning is growing even faster - it doubles every three and a half months. <br><br>  All of this growth in computational queries is being combined into “Mura's super law,” said Young, and he called this phenomenon “a bit intimidating” and “a little dangerous”, and “in order to worry about.” <br><br>  “Where did all this exponential growth come from?” In the field of AI, he asked.  “In particular, the thing is that in-depth training just works.  “In my career, I have long ignored machine learning,” he said.  “It was not obvious that these things would take off.” <br><br>  But then such breakthroughs quickly began to appear, such as pattern recognition, and it became clear that in-depth training was “incredibly effective,” he said.  “For most of the last five years, we were a company that put AI in the first place, and we redid a large part of AI-based businesses,” from search to advertising and much more. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a72/3ff/1e9/a723ff1e9203c6ee395dc5cd8d8bd296.jpg"><br><br>  The Google Brain project team, the leading AI research project, needs “giant machines,” said Young.  For example, neural networks are sometimes measured by the number of “weights” used in them, that is, the variables applied to a neural network and affect how it processes data. <br><br>  And if ordinary neural networks can contain hundreds of thousands or even millions of scales that need to be calculated, researchers from Google require themselves “tera-weight machines”, that is, computers capable of counting trillions of scales.  Because "every time we double the size of a neural network, we improve its accuracy."  The rule of AI development is becoming more and more. <br><br>  In response to requests from Google, they are developing their own line of chips for MO, the Tensor Processing Unit.  TPU and its like are needed, since traditional CPU and GPU graphics chips do not cope with the loads. <br><br>  "We held ourselves together for a very long time and said that Intel and Nvidia are great at creating high-performance systems," said Young.  “But we crossed this line five years ago.” <br><br>  TPU, after its first public appearance in 2017, caused hype by claims that it outperforms conventional chips in speed.  Google is already working on the third generation of TPU, using it in their projects and offering computer facilities on demand through the Google Cloud service. <br><br>  The company continues to manufacture TPU of all large and large sizes.  In its “stringed” configuration, the 1024 TPUs are jointly connected to a new type of supercomputer, and Google plans to continue to expand this system, according to Young. <br><br>  “We are creating giant multi-computers with tens of petabytes,” he said.  “We are moving tirelessly on progress in several directions at once, and terabyte-scale operations continue to grow.”  Such projects raise all the problems associated with the development of supercomputers. <br><br>  For example, Google engineers have adopted the tricks used in the legendary Cray supercomputer.  They combined a giant “matrix multiplication module”, a part of the chip that carries the main computational burden for neural networks, with a “general purpose vector module” and a “general purpose scalar module”, as was done in Cray.  “The combination of scalar and vector modules allowed Cray to overtake all in performance,” he said. <br><br>  Google has developed its own innovative arithmetic constructions for programming chips.  A certain way of representing real numbers called bfloat16 improves the efficiency of processing numbers in neural networks.  In colloquial speech, it is called “brain float number”. <br><br>  TPU uses the fastest memory chips, high bandwidth memory, or HBM [high-bandwidth memory].  He said that the demand for large amounts of memory in the training of neural networks is growing rapidly. <br><br>  “Memory during training is used more intensively.  People talk about hundreds of millions of scales, but they have their own problems when processing the activation of "variables of the neural network. <br><br>  Google also tweaks a way of programming neural networks that helps squeeze the most out of hardware.  “We are working on data and parallelism of the model” in such projects as “Mesh TensorFlow” - an adaptation of the TensorFlow software platform, “combining data and parallelism on pod scales”. <br><br>  Young did not disclose some technical details.  He noted that the company did not talk about internal connections, about how data travels around the chip - simply noted that "our connectors are gigantic."  He refused to cover this subject, which caused laughter in the audience. <br><br>  Young pointed to even more interesting areas of computation, which may soon be revealed to us.  For example, he suggested that calculations using analog chips, circuits that process input data in the form of continuous values ​​instead of zeros and ones can play an important role.  "Perhaps we will turn to the analog field, in physics there is a lot of interesting things related to analog computers and NVM memory." <br><br>  He also expressed hope for the success of the startups associated with the chips presented at the conference: “There are very cool startups here, and we need them to work, because the capabilities of digital CMOS are not limitless;  I want all these investments to shoot. ” </div><p>Source: <a href="https://habr.com/ru/post/429794/">https://habr.com/ru/post/429794/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../429782/index.html">The story of how we accelerated tests 12 times</a></li>
<li><a href="../429786/index.html">Fast Sin and Cos on Delphi Embedded ASM</a></li>
<li><a href="../429788/index.html">Another reason why Docker containers are slowing down</a></li>
<li><a href="../429790/index.html">Julia and the movement of a charged particle in an electromagnetic field</a></li>
<li><a href="../429792/index.html">Artificial intelligence, engaged in physics, can deduce the laws of imaginary universes</a></li>
<li><a href="../429796/index.html">How DeviceLock DLP prevents leakage of sensitive data on GitHub</a></li>
<li><a href="../429798/index.html">Sales of plug-in electric vehicles in the USA (with charts): October 2018</a></li>
<li><a href="../429800/index.html">Symfony Bundle for exporting statistics in Prometheus format</a></li>
<li><a href="../429804/index.html">Friendly WEB resource protection against brute force attacks</a></li>
<li><a href="../429808/index.html">Roscosmos may lose the largest order due to the FSB</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>