<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The winning decision of the ML Boot Camp III contest</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Good day! In this article, I would like to briefly tell you about the solution that brought me the first place in the ML Boot Camp III machine learnin...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The winning decision of the ML Boot Camp III contest</h1><div class="post__text post__text-html js-mediator-article">  Good day!  In this article, I would like to briefly tell you about the solution that brought me the first place in the ML Boot Camp III machine learning competition from mail.ru. <br><a name="habracut"></a><br>  I introduce myself, my name is Karachun Mikhail, I am also the winner of the previous contest mail.ru.  My past decision is described <a href="https://habrahabr.ru/company/mailru/blog/321016/">here</a> and sometimes I will refer to it. <br><br><h3>  Conditions </h3><br>  The participants of the competition were asked to predict the probability that the player who played the online game would leave it on the basis of a sample of data.  The sample contained some data on the activity of the players for two weeks, <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html">logloss</a> was selected as a metric, a detailed description of the problem on <a href="http://mlbootcamp.ru/sandbox/10/">the competition website</a> . <br><br><h3>  Introduction </h3><br>  Each machine learning competition has its own specifics, which depends on the data sample offered by the participants.  This can be a huge table of raw logs that need to be cleaned and converted into signs.  This can be a whole database with different tables from which you also need to generate features.  In our case, there was very little data (25k rows and 12 columns), they were free of gaps and errors.  Based on this, the following assumptions were made: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  It is more efficient to generate and sort hypotheses than to invent them, since the amount of data is very small. </li><li>  Most likely in the sample there is no one very cool hidden dependency that will eventually solve everything (killer feature). </li><li>  Most likely the struggle will be for the n-th decimal place and the top 10 will be very tight. </li></ul><br>  That is, it should have been a competition in which it is difficult to find a good dependence in the data and the best solution would consist more of an ensemble of models than of a single one.  So it happened. <br><br>  A small lyrical digression.  Most winning decisions of machine learning contests are not suitable for real-world systems.  To take at least the well-known case with Netflix, they paid $ 1 million for a solution that <a href="https://www.forbes.com/sites/ryanholiday/2012/04/16/what-the-failed-1m-netflix-prize-tells-us-about-business-advice/">they could not implement</a> .  Therefore, the final models in such contests remind me of one novel.  The second part of its name ‚ÄúModern Prometheus‚Äù is the one that brought people fire and knowledge.  The first part, of course, "Frankenstein". <br><br><h3>  Basic solution </h3><br>  As a base solution, I used xgboost which, after optimizing parameters through hyperopt, immediately gave the result on a 0.3825 leaderboard.  Next, I moved to feature engineering. <br><br>  For example, it was noticed that the distribution of many signs resemble logarithmic ones, so I added their logarithms (or rather log (x + 1)) to the base columns. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/a1f/e7f/d67/a1fe7fd67bb941f6bbdaee32a6e6817b.png" alt="image"></div><br>  Then I generated various functions from all possible combinations of the two columns and checked how they affect the result.  Within the framework of this task, it seemed to me pointless to invent some ‚Äúinterpretable‚Äù signs, because it would be much faster to simply sort them out, since the amount of data allows.  A good result, for example, was given by the differences of various signs, which made it possible to get 0.3819 on the same xgboost. <br><br>  About optimization of parameters and selection of columns can be read in <a href="https://habrahabr.ru/company/mailru/blog/321016/">my previous article</a> . <br><br><h3>  First trouble </h3><br>  When checking the solutions at the very beginning of the championship, it turned out that the change in the evaluation on the leaderboard is rather bad at the change in the local assessment.  When generating new columns, the logloss on the local cross-validation decreased steadily, while on the leaderboard it grew, and no changes in the parameters of the local partitioning of the sample helped to verify.  Watching the general chat of the competition, it was possible to notice that a lot of participants encountered this problem, regardless of the means and variances of local estimates.  This was another sign in favor of creating an ensemble of models. <br><br><h3>  More trees ... </h3><br>  On the same set of columns (at this point I got 70 of them), I created two more ensembles of gradient trees, but from other libraries, sklearn and <a href="https://github.com/Microsoft/LightGBM">lightgbm</a> .  The training took place as follows - at first I tuned each model separately for the best result, then I took the average, saved the results of all models and set up each in turn, but not for my own best result, but for the best result in an ensemble of three models.  This gave about 0.3817 on the leaderboard. <br><br><h3>  Regression </h3><br>  Despite the fact that even different implementations of the same algorithms, when averaged in an ensemble, give the best result, it is much more efficient to combine different algorithms together. <br>  So a logistic regression was added to the general ensemble.  The model was built in the same way as the past - we generate all possible signs, recursively select and leave the best ones.  Here I was disappointed.  The local regression performed better than xgboost!  The score on the leaderboard was much worse: 0.383.  I struggled with this for a long time, threw out the signs in which the distribution on the training and test samples were different, tried various methods of normalization, tried to break the signs into intervals - nothing helped.  But even with this result, adding regression to the ensemble turned out to be useful - the result is approximately 0.3816 <br><br><h3>  Neural networks </h3><br>  Since linear regression has shown a good result, it is worth trying neural networks.  I spent enough time on them, since the application of standard hyperoptimization algorithms to the structure of a neuron network gives a very weak result.  As a result, a good configuration was found which gave about as much on the leaderboard as the regression.  The <a href="https://keras.io/">keras</a> library was used for implementation, I will not give the structure here, you can find it in the final file, I will give just a small sample of code that helped me a lot.  At the cross-training it was clear that the result of the model strongly depends on the number of epochs of training.  You can reduce the learning rate - but it worsened the result, I did not succeed in adjusting the decay - the result was also worsened.  Then I just decided to change the learning rate once in the middle of training. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Callback <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> keras_clb <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">LearningRateClb</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(keras_clb)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">on_epoch_end</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, epoch, logs={})</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> epoch ==<span class="hljs-number"><span class="hljs-number">300</span></span>: self.model.optimizer.lr.set_value(<span class="hljs-number"><span class="hljs-number">0.01</span></span>)</code> </pre> <br>  The result after adding to the ensemble - the result is approximately 0.3815. <br><br><h3>  More models for God models </h3><br>  All the above models were built on approximately the same columns - generated on the basis of basic features and their logarithms.  I also tried to generate attributes based on the base columns and their square roots.  This also helped - two more ensembles of gradient trees were added.  Such a set became final: regression, neural network and five ensembles of gradient trees.  The result is approximately 0.3813 <br><br><h3>  Sample analysis </h3><br>  An important addition to the ensemble was a slight change in the overall result of the model before shipping.  It was found that in the training sample there are large groups of lines with exactly the same signs.  And since logloss urges us to optimize the probability, it would be logical to replace the model results on these groups with the average calculated on the training sample.  This was done for groups of more than 50 elements.  The result is approximately 0.3809 <br><br><h3>  Ideas that I refused </h3><br>  <i>Stacking</i>  With such a large number of models, it seems that you can think of a better function for averaging them than a simple arithmetic average.  You can, for example, put their results in another model.  I refused this idea because the local results did not coincide strongly with the leaderboard, and even the simplest type of stacking did not work - the weighted average. <br><br>  <i>Manual change of results.</i>  On the training sample met subgroups whose results were strictly 1 or strictly 0, for example, all players played 14 days.  Then of course it was worth remembering how badly the logloss punishes for such values; in the case of use, more than one hundred places could be lost. <br><br>  Until the last moment, among the models there was also a random forest, but in the end, based on the public score, I excluded it, although, as it turned out, the privat score showed the model with the best result. <br><br>  As a result, I really want to thank the organizers!  Here you can see the source code of the solution. </div><p>Source: <a href="https://habr.com/ru/post/324916/">https://habr.com/ru/post/324916/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../324906/index.html">The story of how I went to Google Next 17. A brief pressing on the announcements and the most important</a></li>
<li><a href="../324908/index.html">A brief history of javascript. Part 3</a></li>
<li><a href="../324910/index.html">Why do you need a manufacturer‚Äôs service for IT equipment and its cost estimate?</a></li>
<li><a href="../324912/index.html">PayPal Node.js</a></li>
<li><a href="../324914/index.html">We develop video chat between the browser and the mobile application</a></li>
<li><a href="../324918/index.html">Docker and detection of available resources inside the container</a></li>
<li><a href="../324920/index.html">How the CIA caused the rain: using Rain Maker to gather information from closed objects</a></li>
<li><a href="../324922/index.html">Threat Horizon 2017-2019 by the International Security Forum (executive executive)</a></li>
<li><a href="../324924/index.html">Cook ML Boot Camp III: Starter Kit</a></li>
<li><a href="../324926/index.html">We are friends of Angular with Google (Angular Universal)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>