<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Interprocedural analysis and optimization (I)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="One of the most interesting and important components of a modern optimizing compiler is interprocedural analysis and optimization. A good programming ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Interprocedural analysis and optimization (I)</h1><div class="post__text post__text-html js-mediator-article">  One of the most interesting and important components of a modern optimizing compiler is interprocedural analysis and optimization.  A good programming style and the need to divide work among developers dictate the need to partition a large project into separate modules, in which the main utilities are implemented as ‚Äúblack boxes‚Äù for all main users.  The implementation details, at best, are known to a couple or three specific developers, and sometimes, after years of age, they are not known to anyone at all.  (And what can you do - specialization, globalization).  Your code often contains calls to external functions whose bodies are defined in external files or libraries, and your knowledge of these functions is minimal.  In addition to this, when developing large projects, all sorts of global variables are generated, with the help of which components of a large project exchange valuable information, and in order to understand the work of your part of the code in case of any problems, it is necessary to shovel a lot of code.  Obviously, all this greatly complicates the work of the optimizing compiler.  What negative effects modularity generates and whether there are special tools in the compiler for overcoming them is a topic for a big conversation.  Now I will try to start such a conversation.  I will talk about some interesting features of the work of the optimizing compiler using the example of the Intel compiler.  I will be guided by OS Windows therefore the options of the compiler I bring characteristic for this platform.  Well, in order to make life easier for me, I will sometimes use IPA abbreviations for interprocedural analysis and IPO for interprocedural optimizations.  And I will begin with the story about the models of interprocedural analysis and the call graph. <br><a name="habracut"></a><br><br><h4>  The impact of modularity on performance </h4><br>  To understand what problems may arise from modularity, you need to look at what options exist for the compiler, how the compiler tries to solve certain problems, what are the restrictions on the analysis methods used by the compiler.  The compiler should work conservatively, i.e.  with complete distrust of the programmer and the program and to do this or that optimization only if its correctness is fully proved. <br><br>  The simplest version of the compiler is a one-pass compilation.  In this version of the work, the compiler processes in turn all the functions from the source file plus all the previous declarations.  Those.  the compiler sequentially parses the characters from the source file, fills the tables with different definitions of functions, sets descriptions of defined types and sequentially parses the bodies of all functions in the processed source file.  First, the compiler solves the problem of checking the program for compliance with the requirements of the language standard, then it can perform various optimizations - cyclic, scalar, optimization of the control flow graph, etc.  At the end of processing, the result is saved to the object file.  With such work successfully (and usually relatively quickly) the task is solved - to get a working application, but its optimal work is very far.  The reason for this is that the compiler has insufficient information about non-local objects for the function being processed.  Also, the compiler has very little information about called functions.  Does this situation affect the performance of the application and what are the problems associated with the use of function calls?  There is no need to go far for examples.  I will list some of the most important ones, in my opinion: <br><ol><li>  Problems with ambiguity resolution when working with memory.  For example, in my previous posts I discussed the problem of recognizing loops and the accompanying problem of ambiguity resolution when working with memory (alias analysis).  Those.  if the program has arguments, pointers and / or global variables, then there is the possibility that work with the same memory is going on through several different objects.  Such uncertainty prevents the compiler from recognizing cycles, creates putative dependencies that interfere with the execution of cycle optimizations, complicates data flow analysis, etc.  The compiler often tries to solve such problems by creating a multi-version code, i.e.  inserts into the runtime check code.  But this "makes the code heavier" and is good only for simple cases when the intended dependencies are few, and what if there are a lot of unresolved issues? </li><li>  When passed to a function, many properties of actual arguments that could have an impact on the quality of optimization are lost.  For example, information about the location of objects in memory affects the gain from vectorization.  If the actual argument is a constant, then dragging it into a function can also cause a simplification of the calculations performed.  Well, etc. </li><li>  Calling a function with unknown properties makes it difficult to analyze the data flow.  It becomes necessary to assume that this unknown call can change all the data to which, according to the language standard, it can reach.  (For example, global pointers and variables, objects, access to which is passed to the function through arguments).  This interferes with such optimizations as the pushing of constants, the removal of general subexpressions, and many others. </li><li>  Cycles calling unknown functions cannot be correctly classified (as cycles with a certain number of iterations).  This is because an unknown function may contain an exit from the program.  As a result, almost all cyclic optimizations are consistent. </li><li>  The function call itself is quite expensive.  It is necessary to prepare arguments, allocate space on the stack for storing local variables, load the instructions from the new address for execution.  Chances are that these addresses will not be in the cache.  If the body of the function is small, it may happen that the cost of its challenge is huge compared with the time of its work. </li><li>  With static linking, the program will contain redundant code ‚Äî unused functions from sources and libraries. </li></ol><br>  Probably, the list goes on. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      There is a desire to collect various properties of functions and then use them in the optimization and processing of each particular function.  Actually, this is the main idea of ‚Äã‚Äãinterprocedural analysis.  To conduct such an analysis, it is necessary to take into account the relationships of all the functions involved in the calculations with each other.  It is clear that in order to establish the properties of a certain function, it is necessary to analyze all the functions that it calls, etc.  For example, in some languages ‚Äã‚Äãthere are ‚Äúpure‚Äù functions (pure function) and you have got, use and calculate a similar attribute for all functions.  Pure function does not change its arguments and any global variables and does not display any information.  It is clear that this attribute for a function can be set only if all functions called within this function are pure. <br>  To reflect these mutual influences and relationships, the <a href="http://ru.wikipedia.org/wiki/%25D0%2593%25D1%2580%25D0%25B0%25D1%2584_%25D0%25B2%25D1%258B%25D0%25B7%25D0%25BE%25D0%25B2%25D0%25BE%25D0%25B2">call graph is used.</a>  Its vertices are the existing functions, and the faces are the challenges.  If the function foo calls bar, then the vertices corresponding to these functions are connected by an edge.  Since there may be several calls, the number of edges may be different, that is, the call graph is a multigraph. <br>  IPA analysis works with a static graph, i.e.  a graph reflecting all possible paths that can be covered during program execution.  Sometimes, when analyzing performance, it is necessary to analyze a dynamic call graph, i.e.  the real way in which control was transferred when a program was executed with some particular data set. <br>  In order to build a call graph, you must first go through all the functions and determine which calls are made in each function.  Then analyze the constructed graph and determine what features each function has.  To implement various interprocedural optimization and only after that to process each individual function.  With this order of work, it is necessary to at least twice process the body of each function ‚Äî this compilation method is called two-pass. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/af1/0fc/577/af10fc5777ca20e63a9b7e20439ede8e.png" alt="image"><br>  You can draw something like this for a two-pass and one-pass compilation. <br><br>  In the case when the programmer sets the option to include interprocedural optimizations, the compiler does lexical and syntactic analysis of each source file and, if the file conforms to the language standard, the compiler creates an internal representation and packs it into an object file.  When all the necessary files with internal representation are created, the compiler re-examines them all, collects information about each function, finds all calls inside functions and builds a call graph.  Analyzing the call graph, the compiler pulls through it various properties of functions, analyzes the properties of various objects, etc.  After that, interprocedural optimizations are made that change the call graph or properties of objects.  (I hope that after some time I will get together and talk about the most interesting optimizations in my opinion in more detail.) Many optimizations are applicable only if the compiler manages to prove that the executable file is being generated, and the call graph contains all the functions of the program.  Such optimizations are called whole program optimization.  Examples of such optimizations are the removal from the call graph of individual functions or parts of the call graph that cannot be reached from main (dead code elimination).  Another famous example is the modification of data structures.  Most interprocedural optimizations can be performed for both full and incomplete call graphs.  The most well-known interprocedural optimization is the substitution of the function body <a href="http://en.wikipedia.org/wiki/Inline_expansion">(inlining), which</a> substitutes the function body instead of calling it. <br>  After all interprocedural optimizations have been performed, all functions included in the final call graph are processed in turn by the optimizing part of the compiler (backend).  The difference with a single-pass compilation is that some additional information may be known about each function being called, for example, whether a function changes a particular object.  This additional information allows for optimization more efficiently. <br>  In fact, from my description it can be seen that the term two-pass optimization is a rather conditional term and in fact there can be more passes through the function body. <br><br>  So, if you use the compiler to create an object file, for example with the ‚Äìc option, then different entities will be created depending on the presence or absence of the ‚ÄìQipo option.  With -Qipo, an object file is a packaged internal representation, not a regular object file.  Such files will not understand the linker link and the utility for building lib libraries.  To work with such object files, you need to use their extended Intel analogues xilink and xilib.  A library created from object files with an internal view will contain sections with an internal view, which means that the utilities that it contains will be able to participate in inter-procedural analysis at the moment when such a library will link to the application. Intel compiler and -Qipo options.) It follows that in order to get the maximum benefit from using the Intel compiler and interprocedural analysis, you must use the libraries created  using the Intel compiler with -Qipo.  In order for interprocedural analysis to work with system functions, the compiler has information about these functions. <br><br>  By the way, if for some reason you mix flies and cutlets, i.e.  Since ordinary object files and files compiled with ‚ÄìQipo and causing an Intel compiler (icl) to process them, nothing terrible will happen.  The compiler will notice that you pass the internal representation to it and will enable interprocedural analysis / optimization automatically.  IPA and IPO will be executed for an incomplete call graph (i.e., a call graph with unknown functions), which will include all functions for which there is an internal representation.  The compiler will create object files and then call the linker to link normal object files to them.  But in the general case, an ‚Äúordinary‚Äù object file that is included in the program assembly with ‚ÄìQipo can disable the whole layer of optimizations, the whole program optimizations. <br><br><h4>  Call graph </h4><br>  It is clear that interprocedural optimization is not a gift.  Working with a call graph can be quite complex and resource intensive and much is determined by the size of the graph.  For the sake of interest, I looked at the well-known performance test from the suite for measuring the performance of CPU2006 447.dealII (this is a c ++ application) and outputted information about the number of vertices of the call graph before the start of interprocedural analysis.  It turned out that before the interprocedural analysis, there were approximately 320,000 different functions and approximately 380,000 different arcs connecting them.  Large projects may experience problems using two-pass compilation and interprocedural analysis. <br><br>  The Intel compiler implements two different models of interprocedural analysis, namely the single-file and multi-file model.  Single-file enabled by default, unless multi-file inter-procedural optimization mode is specified.  In the description of the options, it looks something like this: <br><br><pre><code class="dos hljs">/Qip[-] enable(DEFAULT)/disable single-file IP optimization within files /Qipo[n] enable multi-file IP optimization between files</code> </pre> <br><br>  Well, plus there are still two additional options, which I will mention later. <br><br><pre> <code class="dos hljs">/Qipo-c generate a multi-file object file (ipo_out.obj) /Qipo-S generate a multi-file assembly file (ipo_out.asm)</code> </pre><br><br>  Here I will allow myself to give a scheme of some abstract call graph, characteristic of the C-shny project, so that the <s>article would look more beautifully the</s> work of the compiler with the call tree for both methods looked more clearly. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/551/3f1/72f/5513f172f84fb57813405d20223ca3e2.png" alt="image"><br><br>  Formally, it can be said that c ‚ÄìQipo requires more resources, and ‚ÄìQip requires less resources, but also provides the worst quality of optimizations. <br>  The reality is not so simple, at least with resources.  Of course, in fact, call graphs for a single file are not subgraphs of the call graph for the entire project.  For example, the columns for different files may contain the same functions, which came from the description files and are local to each file.  This is especially true for large C ++ projects.  Such functions can be duplicated in call graphs for some files.  In my opinion, it can be conditionally assumed that these functions are a kind of analogue of weak functions.  Therefore, if we sum up the number of vertices for each individual graph with a single-file optimization model, then the number of vertices can be much larger than the number of vertices in the complete call graph.  In this case, with the single-file optimization model, all these weak functions go into processing and the compiler ‚Äúfiddles‚Äù with each of them so that the linker leaves one version of this function in the executable file or library.  Also, the amount of work during multi-file processing can be reduced by removing the subgraphs and functions that are unreachable from main.  If the function has been substituted into all its calls (and some more conditions have been fulfilled), then the function will not be processed by the optimizing part of the compiler, i.e.  the final executable file will not contain the body of this function.  For a single-file model, this is true only if the function has a static attribute.  That is, with a single-file IPA, a certain number of "dead" functions will get into the processing. <br>  In general, it may happen that in total the compiler will do much more work with ‚ÄìQip than with ‚ÄìQipo.  When compiling a program instead of one big call graph, you will have to handle many smaller graphs.  As a result, each single-file compilation will require much less resources than working with a full call graph, but the amount of resources spent on compiling all files can be much more than with a full call graph. <br>  I will illustrate this with the already mentioned benchmark 447.dealII from CPU2006.  I already wrote that in the full graph of calls to this application about 320,000 vertices.  Almost 120 files are involved in the compilation.  The total number of all vertices in the call graphs of all processed files is ~ 1500000 vertices (although in each vertex file there are less than 50,000).  Those.  5 times more than the number of all vertices in the full call graph.  I turned off all parallel compilation options and got that the program with ‚ÄìQipo compiles on my Nehalemam approximately 160s, while with ‚ÄìQip the compilation time is ~ 495s, i.e.  3 times more.  If you count the number of functions that are processed in the backend with ‚ÄìQipo, you get ~ 19,500 functions.  The total number of processed functions when compiling a project with ‚ÄìQip is ~ 55000.  Well, if you look at the size of the executable file, then for this application the file compiled with ‚ÄìQipo is almost a third smaller than the file compiled with ‚ÄìQip. <br>  The main advantage of single-file optimization is that in this case, when editing a single file, you do not need to do a complete rebuild of the entire application, but recompile just one file and link it with other object files.  In addition, working with ‚ÄìQip is easy to parallelize, but creating an application with ‚ÄìQipo is more difficult to parallelize.  Although the ‚ÄìQipo option may contain a numeric argument (for example, -Qipo2 implies operation in two streams), IPA is executed on the full call graph and has not yet come up with a good method for parallelizing this analysis, so this part of the work is done in one stream (or repeated in each stream) and, moreover, each stream receives into the load a lot of data collected by IPA.  Processing all functions in the backend can be efficiently distributed between threads, but due to the difficulties listed above, such automatic parallelization is less efficient than in the case of using a single-file model. <br><br>  By the way, I would like to mention here another useful option. <br>  If you suddenly decide to go the simplest way and compile your project with a single call to the Intel compiler, passing the compiler all the options and a list of all sources at once, then by default the compilation will be done in one stream and you need to apply a special option to speed up the compilation.  This option only affects compilation without multi-file IPA. <br><br><pre> <code class="dos hljs">/MP[&lt;n&gt;] create multiple processes that can be used to compile large numbers of source files <span class="hljs-built_in"><span class="hljs-built_in">at</span></span> the same <span class="hljs-built_in"><span class="hljs-built_in">time</span></span></code> </pre><br><br>  If the mentioned 447.deallI is compiled with different values ‚Äã‚Äãin ‚ÄìQipo, then the compile time will vary as follows: -Qipo1: 160s, -Qipo2: 136s, -Qipo4: 150s.  Those.  very soon the streams start interfering with each other. <br>  If you replace ‚ÄìQipo with ‚ÄìQip in the same icl call (or remove it altogether, -Qip will work by default) and add the / MP option, you get the following: / MP1: 495s;  / Mp2: 259s;  / Mp4: 145s;  / Mp8: 117s;  / MP16: 84s <br><br>  If we talk about performance, then multi-file IPA is much more efficient.  (Although there is always some likelihood of "grief from the mind")  It is easy to come up with a simple example that would demonstrate the advantages of a multi-file model in terms of the ability to use various optimizations thanks to additional information from the IPA. <br><br>  test_main1.c: <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;stdio.h&gt; #include &lt;stdlib.h&gt; #define N 10000 extern float calculate(float *a, float *b); int main() { float *a,*b,res; int i; a = (float*)malloc(N*sizeof(float)); b = (float*)malloc(N*sizeof(float)); for(i=0;i&lt;N;i++) { a[i] = i; b[i] = 1 - i; } res = calculate(a,b); printf("res = %f", res); }</span></span></span></span></code> </pre><br><br>  test1.c: <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;stdio.h&gt; #define N 10000 extern float calc(float a, float b); float calculate(float * restrict a,float * restrict b) { int i; float res; for(i=0;i&lt;N;i++) { res = calc(a[i],b[i]); } return res; }</span></span></span></span></code> </pre><br><br>  test2.c: <br><br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">float</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">calc</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> a,</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> b)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">return</span></span>(a+b); }</code> </pre><br><br><pre> <code class="dos hljs">icl -Feaaa.exe -Qparallel -Qpar_report3 test_main.c test1.c test2.c -Qstd=c99 icl -Feaaa.exe -Qipo -Qparallel -Qpar_report3 test_main.c test1.c test2.c -Qstd=c99</code> </pre><br><br>  Here, the ‚ÄìQparallel option is used, which includes the auto-parallelizer of the Intel compiler and ‚ÄìQpar_report3 - an option that gives information about which cycles were processed or rejected by the auto-parallelizer and why. <br><br>  Output without ‚ÄìQipo reports the following: <br><pre> <code class="dos hljs">... test_main.c procedure: main ..\test_main.c(<span class="hljs-number"><span class="hljs-number">14</span></span>): (col. <span class="hljs-number"><span class="hljs-number">1</span></span>) remark: LOOP WAS AUTO-PARALLELIZED test1.c procedure: calculate ..\test1.c(<span class="hljs-number"><span class="hljs-number">11</span></span>): (col. <span class="hljs-number"><span class="hljs-number">1</span></span>) remark: loop was <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> parallelized: existence of parallel dependence test2.c procedure: calc</code> </pre><br><br>  Conclusion with ‚ÄìQipo: <br><pre> <code class="dos hljs">test_main.c test1.c test2.c procedure: main C:\iusers\aanufrie\for_habrahabr\<span class="hljs-number"><span class="hljs-number">7</span></span>a\test_main.c(<span class="hljs-number"><span class="hljs-number">13</span></span>): (col. <span class="hljs-number"><span class="hljs-number">1</span></span>) remark: LOOP WAS AUTO-PARALLELIZED C:\iusers\aanufrie\for_habrahabr\<span class="hljs-number"><span class="hljs-number">7</span></span>a\test_main.c(<span class="hljs-number"><span class="hljs-number">17</span></span>): (col. <span class="hljs-number"><span class="hljs-number">8</span></span>) remark: LOOP WAS AUTO-PARALLELIZED</code> </pre><br><br>  From the output from ‚ÄìQipo, it is clear that the interprocedural analysis helped automate the parallelization of the loop. <br>  It is clear that in the case of ‚ÄìQipo, inlining also helps, but even if inlining is prohibited, then by analyzing the calc function, the compiler has the opportunity to prove the validity of auto-parallelization. <br><br>  What to do if for some reason a multi-file model is not applicable, for example, it takes a long time to build a program and this does not suit you?  You can enhance the usefulness of a single-file model by placing shared functions in a file.  The static attribute is very useful for functions.  If the function has this attribute, it means that IPA sees all the calls to this function during single-file operation, and if these calls have any peculiarities, then they can be extended inside the function.  If all calls to the static function have been substituted, then the function body itself can be deleted.  The same goes for static objects.  Interprocedural analysis believes that it sees all the examples of their use and can perform various optimizations with them.  In general, any restrictions on the scope of objects by a single file can help to make a better single-file interprocedural analysis. <br>  And what if it‚Äôs impossible to solve any problems within a single-file framework, since the important functionality is split between several files? <br>  The option ‚ÄìQipo-c provides an opportunity to organize some intermediate model of work, i.e.  to build the call graph not on the basis of functions from one or all files, but to divide all files into groups.  This is useful if your project is divided into several closely interrelated pieces.  (For example, such a project is our compiler, in which there are several components using common utilities and located in separate directories).  ‚ÄìQipo-c creates an object file with a fixed name ipo_out.obj and all the functions from the files listed in the compiler call fall into this file.  In the example above, the application could be compiled as follows: <br><br><pre> <code class="dos hljs">icl -Qipo-c -Qparallel -Qpar_report3 -Qstd=c99 test1.c test2.c icl test_main.c ipo_out.obj</code> </pre><br><br>  The output from the compilation of the first two files shows that in this case both auto-parallelization and in-lining will work, since the critical functions were combined in one call graph, which made it possible to do interprocedural analysis and optimization. <br><br>  This is the question, did any of the readers use such a compilation scheme, or at least heard about it?  There is an opinion that only 5% of developers are interested in the performance problem.  Often, the desire to provide the user with some additional levers to control the operation of the compiler rests on a simple question: ‚ÄúAnd who needs it at all, and who will use it?‚Äù <br><br>  By the way, you can add and remove some files from interprocedural analysis using the ‚Äúflies and cutlets‚Äù method: creating object files with and without Qipo.  In this case, the interprocedural analysis will be carried out on an incomplete tree obtained from the functions that are described in files compiled with ‚ÄìQipo.  In the example I described, this also works. <br><br>  Finally, the ‚ÄìQipo-S option is useful for those who want to upgrade or learn an assembler after interprocedural optimizations. <br><br>  With this, I will finish the story about the call graphs as the basis of IPO and IPA.  I hope to continue the story about interprocedural optimizations and I will try to answer all the questions. </div><p>Source: <a href="https://habr.com/ru/post/199112/">https://habr.com/ru/post/199112/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../199092/index.html">Accelerate the understanding of a commercial or technical text: how to stop being afraid to write simply</a></li>
<li><a href="../199094/index.html">Writing Hello World in assembly language under Windows RT using winapi</a></li>
<li><a href="../199102/index.html">How to find a developer in a garage startup: from personal experience</a></li>
<li><a href="../199104/index.html">The story of a graduate school in the United States. Part 4.1: What next?</a></li>
<li><a href="../199110/index.html">Continuous integration in xcode5</a></li>
<li><a href="../199114/index.html">Remote work is not "freelancing"</a></li>
<li><a href="../199116/index.html">We create the first application on NancyFX. Part Three Nancy modules</a></li>
<li><a href="../199124/index.html">iversity - online courses from the European platform MOOC</a></li>
<li><a href="../199128/index.html">We break iOS-application. Part 1</a></li>
<li><a href="../199130/index.html">Breaking the iOS application! Part 2</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>