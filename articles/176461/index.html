<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Probabilistic models: Bayesian networks</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this blog we have already talked a lot about what: there were brief descriptions of the main recommender algorithms ( problem statement , user-base...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Probabilistic models: Bayesian networks</h1><div class="post__text post__text-html js-mediator-article">  In this blog we have already talked a lot about what: there were brief descriptions of the main recommender algorithms ( <a href="http://habrahabr.ru/company/surfingbird/blog/139022/">problem statement</a> , <a href="http://habrahabr.ru/company/surfingbird/blog/139518/">user-based and item-based</a> , SVD: <a href="http://habrahabr.ru/company/surfingbird/blog/139863/">1</a> , <a href="http://habrahabr.ru/company/surfingbird/blog/140555/">2</a> , <a href="http://habrahabr.ru/company/surfingbird/blog/141959/">3</a> , <a href="http://habrahabr.ru/company/surfingbird/blog/143455/">4</a> ), several models for working with content ( <a href="http://habrahabr.ru/company/surfingbird/blog/150207/">naive Bayes</a> , <a href="http://habrahabr.ru/company/surfingbird/blog/150607/">LDA</a> , a <a href="http://habrahabr.ru/company/surfingbird/blog/172417/">review of text analysis methods</a> ), was a series of articles on cold start ( <a href="http://habrahabr.ru/company/surfingbird/blog/168733/">problem statement</a> , <a href="http://habrahabr.ru/company/surfingbird/blog/170081/">text mining</a> , <a href="http://habrahabr.ru/company/surfingbird/blog/174245/">tags</a> ), there was a mini-series about multi-armed bandits ( <a href="http://habrahabr.ru/company/surfingbird/blog/168611/">part 1</a> , <a href="http://habrahabr.ru/company/surfingbird/blog/169573/">part 2</a> ). <br><br>  In order to move further and put these and many other methods into a common context, we need to develop some kind of common base, to learn the language spoken by modern data processing methods, the language of <i>graphical probabilistic models</i> .  Today - the first part of this story, the most simple, with pictures and explanations. <br><br><img src="https://habrastorage.org/storage2/83d/4ea/356/83d4ea3564bf9d4b81a1d4308c42d3f4.jpg"><br><a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  Bayesian Output Tasks </h3><br>  First, <a href="http://habrahabr.ru/company/surfingbird/blog/150207/">let</a> me briefly remind you <a href="http://habrahabr.ru/company/surfingbird/blog/150207/">of what we have already said</a> .  The main tool of machine learning is the Bayes theorem: <br><img src="http://habrastorage.org/getpro/habr/post_images/138/e30/ccb/138e30ccb4c824996cfc08804c60b9fa.png" alt="image"><br>  Here D is the data, what we know, and Œ∏ are the parameters of the model that we want to train.  For example, <a href="http://habrahabr.ru/company/surfingbird/blog/139863/">in the SVD model,</a> data are the ratings that users put on products, and the parameters of the model are factors that we train for users and products. <img src="http://habrastorage.org/getpro/habr/post_images/3a1/310/83c/3a131083c486867287a21d6b223d457a.png" alt="image">  - this is what we want to find, the probability distribution of the parameters of the model <i>after</i> we have taken into account the data;  this is called <i>posterior probability</i> .  This probability, as a rule, cannot be directly found, and this is precisely where Bayes theorem is needed. <img src="https://habrastorage.org/getpro/habr/post_images/e5f/af9/944/e5faf9944934e0f374bfc36d11cfee38.png" alt="image">  - this is the so-called <i>likelihood</i> , the probability of the data, provided the model parameters are fixed;  it is usually easy to find; in fact, the design of the model usually consists in setting the likelihood function.  BUT <img src="https://habrastorage.org/getpro/habr/post_images/a7f/f23/018/a7ff23018d643ebd4a20be82edd6750d.png" alt="image">  - prior probability, it is a mathematical formalization of our intuition about the subject, a formalization of what we knew before, even before any experiments. <br><br>  Thus, we come to one of the main tasks of Bayesian inference: to find the a posteriori distribution on hypotheses / parameters: <br><img src="https://habrastorage.org/getpro/habr/post_images/7f3/a2e/ab9/7f3a2eab9505773286a05cd8786699f3.png" alt="image"><br>  and find the maximum a posteriori hypothesis <img src="https://habrastorage.org/getpro/habr/post_images/a79/78e/dca/a7978edca091d65cc59ffc84143ef47d.png" alt="image">  . <br><br><h3>  Complexisation factorization and Bayesian networks </h3><br>  But if everything is so simple - they multiplied two functions, maximized the result - what is the whole science about?  The problem is that the distributions that interest us are usually too complex to be maximized directly, analytically.  There are too many variables in them, too complex connections between variables.  But, on the other hand, they often have an additional structure that can be used, a structure in the form of <i>independence</i> ( <img src="https://habrastorage.org/getpro/habr/post_images/58a/c09/165/58ac0916585426b604b5363f5b26d67b.png" alt="image">  ) and <i>conditional independence</i> ( <img src="https://habrastorage.org/getpro/habr/post_images/2dc/7fe/c7a/2dc7fec7a147638092868502dabfe31c.png" alt="image">  ) some variables. <br><br>  Recall <a href="http://habrahabr.ru/company/surfingbird/blog/150207/">our conversation about naive Bayes</a> : the model produced a very large number of parameters, and we made an additional assumption about the conditional independence of attributes (words) subject to the condition: <br><img src="https://habrastorage.org/getpro/habr/post_images/7c8/de2/d30/7c8de2d301c1aa296a1da6b53d192084.png" alt="image"><br>  The result is a complex posterior distribution. <img src="https://habrastorage.org/getpro/habr/post_images/183/4ca/260/1834ca260491d8d0f614bc5dca7d2fd1.png" alt="image">  managed to rewrite as <br><img src="https://habrastorage.org/getpro/habr/post_images/bd4/c38/94e/bd4c3894e1c9f351b451e5c286b398ec.png" alt="image"><br>  and in this form it turned out to be much easier to cope with (to train the parameters of each small distribution separately, and then choose <i>v</i> , which gives the maximum of the product).  This is one of the simplest examples of decomposition (factorization) of a complex distribution into a product of simple ones.  Today and in subsequent texts we will be able to generalize this method and bring it to unsurpassed heights. <br><br>  So, let's look at one of the most convenient ways to represent large and complex probability distributions - Bayesian belief networks (Bayesian belief networks), which recently are often called simply <i>directed graphical models</i> (directed graphical models).  A Bayesian network is a directed graph without directed cycles (this is a very important condition!), In which the vertices correspond to variables in the distribution, and the edges connect ‚Äúconnected‚Äù variables.  Each node is assigned a conditional distribution of the node, provided its parents <img src="https://habrastorage.org/getpro/habr/post_images/9df/b7d/96c/9dfb7d96c8d999e0256c507d1c68533f.png" alt="image">  , and the Bayesian network graph means that a large joint distribution decomposes into a product of these conditional distributions. <br><br>  Here, for example, the graph corresponding to the naive Bayes: <br><img src="https://habrastorage.org/storage2/35f/4df/a1c/35f4dfa1ccf99e5a94cc030e6691e010.png" alt="naive"><br>  It corresponds to decomposition. <br><img src="https://habrastorage.org/getpro/habr/post_images/a96/b72/02d/a96b7202d5ca6cdc0868056dcd28b377.png" alt="image"><br>  <i>C has</i> no ancestors, so we take its unconditional (marginal) distribution, and each of <i>A <sub>i</sub></i> "grows" directly from <i>C</i> and is no longer connected with anyone. <br><br>  We already know that in this case all the attributes <i>A <sub>i are</sub></i> conditionally independent, provided that the category <i>C is</i> : <br><img src="https://habrastorage.org/getpro/habr/post_images/21a/e62/5c3/21ae625c356dcbde276a1a58df1954d1.png" alt="image"><br>  Let us now consider all the simplest variants of the Bayesian network and see what conditions of independence between variables they correspond to. <br><br><h3>  Bayesian networks with two and three variables: trivial cases </h3><br>  Let's start with a network of two variables, <i>x</i> and <i>y</i> .  There are only two options: either there is no edge between <i>x</i> and <i>y</i> , or there is.  If there is no edge, it simply means that <i>x</i> and <i>y are</i> independent, because such a graph corresponds to the decomposition <img src="https://habrastorage.org/getpro/habr/post_images/58a/c09/165/58ac0916585426b604b5363f5b26d67b.png" alt="image">  . <br><img src="https://habrastorage.org/storage2/e11/017/9f6/e110179f6f1c719f887829bcb2331673.png"><br><br>  And if there is an edge (let it go from <i>x</i> to <i>y</i> , it doesn't matter), we get the decomposition <img src="https://habrastorage.org/getpro/habr/post_images/386/021/245/3860212457d8a83b6b0c956da7e3e441.png" alt="image">  which literally by definition of conditional probability is always stupidly true for any distribution <img src="https://habrastorage.org/getpro/habr/post_images/741/f1d/322/741f1d32232f6d072f9a6dd7b4b2b228.png" alt="image">  .  Thus, a graph with two vertices with an edge does not give us new information. <br><img src="https://habrastorage.org/storage2/829/f33/5b6/829f335b64f9b147b79413a11d99967a.png"><br><br>  Now go to the networks of the three variables, <i>x</i> , <i>y</i> and <i>z</i> .  The easiest case is when there are no edges at all. <br><img src="https://habrastorage.org/storage2/403/1ea/dc3/4031eadc31f8d6620b771c2d0077ce61.png"><br>  As with two variables, this means that <i>x</i> , <i>y,</i> and <i>z are</i> simply independent: <img src="https://habrastorage.org/getpro/habr/post_images/69f/dc9/389/69fdc93893e6085962651eb5ede10e2d.png" alt="image">  . <br><br>  Another simple case is when <i>all</i> edges are drawn between variables. <br><img src="https://habrastorage.org/storage2/007/600/dde/007600dde16ce1c8045291f866f3e497.png"><br>  This case is also similar to the one discussed above;  for example, let the edges go from <i>x</i> to <i>y</i> and <i>z</i> , and also from <i>y</i> to <i>z</i> .  We get decomposition <br><img src="https://habrastorage.org/getpro/habr/post_images/263/ad0/e3a/263ad0e3ad2fa8f59b4ccb235bab11e7.png" alt="image"><br>  which, again, is always true for any joint distribution <img src="https://habrastorage.org/getpro/habr/post_images/92b/eb6/571/92beb6571b6cd6165e42fa3b1e70e149.png" alt="image">  .  In this formula it would be possible to select variables in any order, nothing would change.  Please note that directed loops in Bayesian networks are prohibited, and as a result, there are six options for how to draw all edges, not eight. <br><br>  These observations, of course, are generalized to the case of any number of variables: if the variables are completely unrelated to each other, they are independent, and the graph in which all edges are drawn (with arrows in any direction, but so that directional cycles do not work) , corresponds to decomposition, which is always true, and does not give us any information. <br><br><h3>  Bayesian networks with three variables: interesting cases </h3><br>  In this section, we will look at three more interesting cases - these will be the ‚Äúbricks‚Äù from which any Bayesian network can be made.  Fortunately, it is enough to consider graphs in three variables - everything else will be generalized from them.  In the examples below, I will somewhat sin against the truth and will intuitively interpret the edge, the arrow between the two variables, as ‚Äú <i>x</i> affects <i>y</i> ‚Äù, i.e.  essentially as a causal relationship.  In fact, this, of course, is not quite so - for example, you can often turn an edge without losing meaning (remember: in a column of two variables it did not matter in which direction the edge was drawn).  And in general, this is a difficult philosophical question - what are "cause-effect relationships" and why are they needed?  But for the reasoning on the fingers will come down. <br><br><h4>  Serial communication </h4><br>  Let's start with the serial communication between variables: <i>x</i> ‚Äúaffects‚Äù <i>y</i> , and <i>y</i> , in turn, ‚Äúaffects‚Äù <i>z</i> . <br><img src="https://habrastorage.org/storage2/cfd/6c8/d0b/cfd6c8d0b6ddc03327a46650e95ac013.png"><br>  This graph represents the decomposition <br><img src="http://habr.habrastorage.org/post_images/88e/a20/5b1/88ea205b156bc55320bb072206d933f7.png" alt="image"><br><br>  Intuitively, this corresponds to a consistent causal relationship: if you run around in the winter without a hat, you will catch a cold, and if you catch a cold, your temperature will rise.  It is obvious that <i>x</i> and <i>y</i> , as well as <i>y</i> and <i>z</i> are connected with each other, even the arrows are drawn directly between them.  Are <i>x</i> and <i>z</i> interconnected in such a network, are these variables dependent?  Of course!  If you run in the winter without a hat, the likelihood of high temperatures rises.  However, in such a network, <i>x</i> and <i>z</i> are connected only through <i>y</i> , and if we already know the value of <i>y</i> , <i>x</i> and <i>z</i> become independent: if you already know that you have caught a cold, it does not matter at all what caused it, the temperature will now rise (or not ) it is from a cold. <br><br>  Formally, this corresponds to the conditional independence of <i>x</i> and <i>z</i> under the condition <i>y</i> ;  let's check it out: <br><img src="https://habrastorage.org/getpro/habr/post_images/f4c/fe9/3e1/f4cfe93e181ed017d5c2427c6b662dfa.png" alt="image"><br>  where the first equality is the definition of conditional probability, the second is our decomposition, and the third is the application of the Bayes theorem. <br><br>  So, the consistent connection between the three variables tells us that the extreme variables are conditionally independent under the condition of the average.  Everything is very logical and quite straightforward. <br><br><h4>  Divergent link </h4><br>  The next possible option is a diverging relationship: <i>x</i> ‚Äúaffects‚Äù both <i>y</i> and <i>z</i> . <br><img src="https://habrastorage.org/storage2/626/517/676/6265176763668baaea0920e2bac6929f.png"><br>  This graph represents the decomposition <br><img src="https://habrastorage.org/getpro/habr/post_images/81b/92e/919/81b92e91996f460af7fddbde2b363477.png" alt="image"><br><br>  Intuitively, this corresponds to two consequences of the same reason: if you catch a cold, you may have a fever, and a cold can also begin.  As in the previous case, it is obvious that <i>x</i> and <i>y</i> , as well as <i>x</i> and <i>z are</i> dependent, and the question is the relationship between <i>y</i> and <i>z</i> .  Again, it is obvious that these variables are dependent: if you have a runny nose, this increases the likelihood that you have a cold, which means that the probability of a high temperature also rises. <br><br>  However, in such a network, like in the previous case, <i>y</i> and <i>z</i> are connected only through <i>x</i> , and if we already know the meaning of the common cause, <i>x</i> , <i>y</i> and <i>z</i> become independent: if you already know that you have caught a cold, the cold and temperature become independent.  Disclaimer: my medical examples are extremely conditional, in fact, I have no idea if runny nose and temperature are independent in life under the condition of a cold, and even I suspect that it is unlikely, because  many other diseases can also cause both of these symptoms together.  But in our conditional model in the universe there are no other illnesses besides the common cold. <br><br>  Formally, this corresponds to the conditional independence of <i>y</i> and <i>z</i> under the condition <i>x</i> ;  check it out even easier than serial communication: <br><img src="https://habrastorage.org/getpro/habr/post_images/b34/733/846/b347338462930d90c20ec1c124b55ed5.png" alt="image"><br><br>  So, the divergent relationship between the three variables tells us that the ‚Äúconsequences‚Äù are conditionally independent, provided that they have a ‚Äúcommon cause‚Äù.  If the reason is known, then the consequences become independent;  as long as the cause is unknown, the effects through it are related.  Again, while it seems to be nothing surprising. <br><br><h4>  Converging relationship </h4><br>  We have only one possible variant of the connection between the three variables: a convergent connection, when <i>x</i> and <i>y</i> together "influence" <i>z</i> . <br><img src="https://habrastorage.org/storage2/d92/2af/382/d922af382f9c9e028c00d8435a9b5f87.png"><br>  The decomposition here turns out to be: <br><img src="https://habrastorage.org/getpro/habr/post_images/c80/d26/a24/c80d26a24dd2d6604786fd8b3e3cf896.png" alt="image"><br><br>  This is a situation in which the same effect may have two different causes: for example, the temperature may be the result of a cold, or maybe poisoning.  Are cold and poisoning dependent?  Not!  In this situation, as long as the general effect is unknown, two reasons are not related to each other, and it is very easy to verify this formally: <br><img src="https://habrastorage.org/getpro/habr/post_images/51b/465/4a2/51b4654a2d6c18b36380f38c5b84fea8.png" alt="image"><br><br>  However, if the ‚Äúcommon effect‚Äù <i>z</i> becomes known, the situation changes.  Now the common causes of a certain consequence begin to influence each other.  This phenomenon is called ‚Äúexplaining away‚Äù in English: suppose you know that you have a fever.  This greatly increases the likelihood of both colds and poisoning.  However, if you now learn that you have been poisoned, the likelihood of a cold will decrease - the symptom "is already explained" by one of the possible causes, and the second becomes less likely. <br><br>  Thus, with a converging connection, the two ‚Äúcauses‚Äù are independent, but only as long as the meaning of their ‚Äúgeneral effect‚Äù is unknown;  if the general effect is signified, the causes become dependent. <br><br>  But there is, as Vassily Ivanovich said, one nuance: when there is a converging link on the way, it is not enough to look only at its ‚Äúgeneral effect‚Äù in order to determine independence.  In fact, even if there is no value for <i>z</i> , but one of its descendants has it (perhaps far enough), two reasons will still become dependent.  Intuitively, this is also easy to understand: for example, suppose we do not observe the actual temperature, but observe its descendant - the thermometer readings.  There probably are faulty thermometers in the world, so this is also some kind of probabilistic connection.  However, observing the thermometer readings in the same way makes the common cold and poisoning dependent. <br><br><h3>  Putting it all together </h3><br>  Sequential, converging and diverging connections are the three building blocks of which any acyclic directed graph consists.  And our reasoning is quite enough to summarize the results on conditional dependence and independence on all such graphs.  Of course, there is no time or place for formally proving general theorems, but the result is quite predictable - suppose you need to check in a large graph whether two vertices are independent (or even two sets of vertices).  To do this, you look at all the paths in the graph (without taking into account the arrows) connecting these two sets of vertices.  Each of these paths can be ‚Äúbroken‚Äù by one of the above constructions: for example, a sequential link will break if it has a meaning in the middle (the value of the variable is known), and a converging link will break if, on the contrary, there is no value (and not at the very top out of the way, nor in her descendants).  If, as a result, all the paths are broken, then the sets of variables are truly independent;  if not, no. <br><br>  So, today we have considered a simple, convenient and important representation of factorizations of complex distributions - Bayesian networks.  Next time we will look at another (but, of course, similar) graphical representation and begin to develop output algorithms, i.e.  solving Bayesian inference problems in complex probabilistic models. </div><p>Source: <a href="https://habr.com/ru/post/176461/">https://habr.com/ru/post/176461/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../176443/index.html">Excel 2013 has a spectacular 3D visualization of data on a map.</a></li>
<li><a href="../176449/index.html">Open data, Rosstat, Prosecutor's Office and all-all-all</a></li>
<li><a href="../176455/index.html">Setting up a cluster with several regions for cloud storage of objects with OpenStack Swift</a></li>
<li><a href="../176457/index.html">Twitter Bootstrap 3: Unofficial First Look</a></li>
<li><a href="../176459/index.html">Two-factor authentication on Citrix NetScaler</a></li>
<li><a href="../176463/index.html">Solving the problem with the encoding of data from MySQL to Symfony</a></li>
<li><a href="../176467/index.html">Dynamic Access Control: Reshaping Resource Management</a></li>
<li><a href="../176469/index.html">New Brief: Samsung Launches 128-Gbps 3-Bit MLC NAND Memory Based on 10-nm Process Technology</a></li>
<li><a href="../176473/index.html">Javascript Interface Abstract Notation</a></li>
<li><a href="../176477/index.html">How to make the best landing page: checklist of 50 points</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>