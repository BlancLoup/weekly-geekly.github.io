<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>NeurIPS: how to conquer the best ML conference</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="NeurIPS is a conference that is currently considered the most top event in the world of machine learning. Today I will tell you about my experience of...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>NeurIPS: how to conquer the best ML conference</h1><div class="post__text post__text-html js-mediator-article"><p>  <a href="https://neurips.cc/">NeurIPS</a> is a conference that is currently considered the most top event in the world of machine learning.  Today I will tell you about my experience of participating in NeurIPS contests: how to compete with the best academics in the world, take a prize place and publish an article. </p><br><img src="https://habrastorage.org/webt/hb/kq/-v/hbkq-vnd_xgxhvcixlo-u8b_pmk.jpeg"><a name="habracut"></a><br><hr><br><h1 id="v-chem-sut-konferencii">  What is the essence of the conference </h1><br><p>  NeurIPS supports the introduction of machine learning methods in various scientific disciplines.  About 10 <a href="https://nips.cc/Conferences/2018/CompetitionTrack">tracks are</a> launched annually to solve actual problems of the academic world.  According to the results of the competition, the winners perform at the conference with presentations, new developments and algorithms.  Most of all I am passionate about learning with reinforcements (Reinforcement Learning or RL), so for the second year I have been participating in RL contests dedicated to NeurIPS. </p><br><h1 id="pochemu-neurips">  Why NeurIPS </h1><br><img src="https://habrastorage.org/webt/ei/c2/us/eic2usvfs-brxmsjczvkvygpfwq.png"><br><br>  NeurIPS is primarily focused on science, not money.  By participating in contests, you do something really important, deal with actual problems. <br><p>  Secondly, this conference is a global event, scientists from different countries gather in one place, with each of which you can communicate. </p><br><p>  In addition, the entire conference is filled with the latest scientific achievements and state-of-the-art results, to know and follow which people from the field of data science is extremely important. </p><br><h1 id="kak-nachat">  How to start? </h1><br><p>  Starting to participate in such contests is quite simple.  If you understand so much in DL that you can <a href="https://github.com/Scitator/catalyst-examples/tree/master/finetune">train ResNet</a> - that's enough: register and go ahead.  There is always a public leaderboard on which you can soberly assess your level compared to other participants.  And if something is not clear ‚Äì‚Äì there are always channels in <a href="http://ods.ai/">slack</a> / discord / gitter / etc to discuss all the issues that arise.  If the topic is really ‚Äúyours‚Äù, then nothing will stop you from getting the coveted result ‚Äì‚Äì in all the contests in which I participated, all approaches and solutions were studied and implemented right in the course of the competition. </p><br><h1 id="neurips-na-primere-konkretnogo-keysa-learning-to-run">  NeurIPS on the example of a specific case: Learning to Run </h1><br><img src="https://habrastorage.org/webt/hu/ws/d5/huwsd5weqocxiuqv3hugygmfqea.jpeg"><br><br><h3 id="problematika">  Problematics </h3><br><p>  The gait of a person is the result of the interaction of muscles, bones, organs of sight and inner ear.  When the central nervous system is impaired, certain movement disorders can occur, including gait disturbance ‚Äì‚Äì abasia. <br>  Researchers from the <a href="https://nmbl.stanford.edu/">Stanford laboratory of neuromuscular biomechanics</a> decided to connect machine learning to the subject of treatment in order to be able to experiment and test their theories on a virtual skeleton model, and not on living people. </p><br><h3 id="postanovka-zadachi">  Formulation of the problem </h3><br><p>  The participants were given a virtual human skeleton (in the <a href="http://opensim.stanford.edu/">OpenSim</a> simulator), which had a prosthesis in place of one leg.  The task was to teach the skeleton to move in a certain direction with a given speed.  During the simulation, both the direction and speed could change. </p><br><img src="https://habrastorage.org/webt/od/vj/np/odvjnpxb7xogj5h_5ll85iokhp0.jpeg">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      To obtain a virtual skeleton control model, it was proposed to use reinforcement learning.  The simulator gave us some state of the skeleton S (a vector of ~ 400 numbers).  It was necessary to predict what action A should be performed (the forces of activation of the muscles of the legs ‚Äì‚Äì a vector of 19 numbers).  In the course of the simulation, the skeleton was given a reward R - as a kind of constant minus the penalty for deviation from a given speed and direction. <br><div class="spoiler">  <b class="spoiler_title">Pro training with reinforcements</b> <div class="spoiler_text"><p>  Reinforcement Learning (RL) is an area that deals with decision theory and the search for optimal behavior policies. </p><br><p>  Recall how to teach <del>  cat </del>  doggy new tricks.  Repeat some action, for doing the trick you give a snack, for failing - you don't.  A dog should understand and find the strategy of behavior (‚Äúpolicy‚Äù or ‚Äúpolicy‚Äù in terms of RL), which maximizes the number of received tastes. </p><br><p>  Formally, we have an agent (dog) who is trained in the history of interactions with the environment (man).  In this environment, assessing the actions of the agent, gives him a reward (snack) - the better the behavior of the agent, the greater the reward.  Accordingly, the task of the agent is to find a policy that maximizes well the reward for the whole time of interaction with the environment. </p><br><p>  Developing this topic further, rule-based solutions - software 1.0, when all the rules were set by the developer, supervised learning is the same software 2.0, when the system learns itself by the existing examples and finds data dependencies, reinforcement learning is a step further when the system itself learns to explore, experiment, and find the required dependencies in their solutions.  The further we go, the better we try to repeat the way a person learns. </p></div></div><br><h3 id="osobennosti-zadachi">  Task features </h3><br><p>  The task looks like a typical reinforcement training representative for tasks with a continuous space of action (RL for continuous action space).  It differs from the usual RL in that instead of choosing a specific action (pressing the joystick button), this action is required to be accurately predicted (there are infinitely many possibilities here). </p><br><p>  The basic approach to solving ( <a href="https://arxiv.org/abs/1509.02971">Deep Deterministic Policy Gradient</a> ) was invented in 2015, which for a long time by the standards of DL, the area continues to actively develop in application to robotics and real-world RL applications.  There is something to improve: robustness of approaches (not to break a real robot), sample efficiency (not to collect data from real robots for months) and other problems of RL (exploration vs exploitation trade-off, etc).  In this competition, a real robot was not given to us - just a simulation, but the simulator itself was 2000 times slower than Open Source analogues (on which everyone checks their RL algorithms), and therefore brought the problem of sample efficiency to a new level. </p><br><h3 id="etapy-sorevnovaniya">  Stages of the competition </h3><br><p>  The competition itself took place in three stages, during which the task and conditions were somewhat modified. </p><br><ul><li>  Stage 1: the skeleton learned to walk straight at a speed of 3 meters per second.  The task was considered completed if the agent will pass 300 steps. </li><li>  Stage 2: changing the speed and direction with a regular frequency.  The length of the distance increased to 1000 steps. </li><li>  Stage 3: the final decision had to be packaged in a docker image and sent for review.  In total it was possible to make 10 parcels. </li></ul><br><p>  The main quality metric was considered the total reward for the simulation, which showed how well the skeleton adhered to a given direction and speed throughout the distance. </p><br><p>  During the 1st and 2nd stages, the progress of each participant was displayed on the leaderboard.  The final decision was required to send in the form of a docker-image.  There were restrictions on work time and resources. </p><br><div class="spoiler">  <b class="spoiler_title">Coolstory: public leaderboard and RL</b> <div class="spoiler_text"><p>  Because of the availability of the leaderboard, no one shows their best model in order to give out ‚Äúa little more than usual‚Äù in the final round and surprise their rivals. </p></div></div><br><h6 id="pochemu-tak-vazhny-docker-obrazy">  Why docker images are so important </h6><br><p> Last year, there was a small incident in evaluating decisions in the very first round.  At that time, the test went through http-interaction with the platform, and the face of testing conditions was found.  It was possible to find out in which situations the agent was evaluated and to retrain him only under these conditions.  Which, of course, did not solve the real problem.  That is why it was decided to transfer the system submit to docker-images and launch on remote servers of the organizers.  <a href="https://dbrain.io/">Dbrain</a> uses the same system for calculating the result of competitions exactly from these considerations. </p><br><h1 id="klyuchevye-momenty">  Key points </h1><br><h3 id="komanda">  Team </h3><br><img src="https://habrastorage.org/webt/ty/ur/gp/tyurgpqbzb2zl2wimtzri0mnpwk.jpeg"><br><br>  The first thing that matters to the success of an entire enterprise is the team.  No matter how good you are (and how powerful your hands are) - participation in a team greatly increases the chances of success.  The reason is simple - a variety of opinions and approaches, double-checking hypotheses, the ability to parallelize the work and conduct more experiments.  All this is extremely important when solving new problems that you have to face. <br><p>  Ideally, your knowledge and skills should be at the same level and complement each other.  For example, this year I got our team on PyTorch, and I got some initial ideas for implementing a distributed agent training system. </p><br><p>  How to find a team?  First, you can join the ranks of <a href="http://ods.ai/">ods</a> and look for like-minded people there.  Secondly, for RL-fellows there is a separate chat in a telegram - the <a href="https://t.me/theoreticalrl">RL club</a> .  Thirdly, you can take a wonderful course from the ShAD - <a href="https://github.com/yandexdataschool/Practical_RL">Practical RL</a> , after which you will definitely get a couple of acquaintances. </p><br><p>  However, it is worth remembering the policy of ‚Äúsubmitting - or not‚Äù.  If you want to unite - first get your decision, zabmmit, appear on the leaderboard and show your level.  As practice shows, such teams are much more balanced. </p><br><h3 id="motivaciya">  Motivation </h3><br><p>  As I already wrote, if the theme is ‚Äúyours‚Äù, then nothing will stop you.  This means that the region does not just like you, but inspires you - you burn with it, you want to become the best in it. <br>  I met RL 4 years ago - during the passage of the <a href="http://ai.berkeley.edu/home.html">Berkeley 188x - Intro to AI</a> - and still do not cease to be surprised at the progress in this area. </p><br><h3 id="sistematichnost">  Systematic </h3><br><p>  Third, but just as important - you need to be able to do what you promised, to invest in the competition every day and just ... solve it.  Everyday.  No innate talent can compare with the ability to do something, even a little bit, but every day.  This is what motivation is needed for.  To succeed in this, I advise you to read <a href="https://www.amazon.com/Deep-Work-Focused-Success-Distracted/dp/1455586692">DeepWork</a> and <a href="https://towardsdatascience.com/ask-me-anything-session-with-a-kaggle-grandmaster-vladimir-i-iglovikov-942ad6a06acd">AMA ternaus</a> . </p><br><h3 id="time-management">  Time management </h3><br><p>  Another extremely important skill is the ability to distribute your strength and use your free time properly.  Combining fulltime work and participation in competitions is not a trivial task.  The most important thing in these conditions - do not burn out and withstand the entire load.  To do this, you need to properly manage your time, soberly assess your strength and do not forget to rest in time. </p><br><h3 id="overwork">  Overwork </h3><br><p>  At the final stage of the competition, there is usually a situation where in just a week you need to do more than just a lot, but A LOT.  For the sake of a better result, you need to be able to force yourself to sit down and make the last dash to the coveted prize. </p><br><div class="spoiler">  <b class="spoiler_title">Coolstory: deadline after deadline</b> <div class="spoiler_text"><p>  Because of what you may need to rework for the benefit of the competition?  The answer is quite simple - the transfer of deadlines.  At such competitions, organizers often cannot predict everything, which is why the easiest way out is to give participants more time.  This year the competition was extended 3 times: first for a month, then for a week and at the very last moment (24 hours before the deadline) - for another 2 days.  And if during the first two transfers it was necessary to simply organize the extra time, then on the last two days you just had to plow. </p></div></div><br><h3 id="theory">  Theory </h3><br><img src="https://habrastorage.org/webt/gf/rg/9q/gfrg9ql1ukvjmlbglwcizlfcpto.png"><br><p>  Among other things, do not forget about the theory - to be aware of what is happening in the region and be able to note the relevant.  For example, to solve last year, our team pushed away from the following articles: </p><br><ul><li>  <a href="https://arxiv.org/abs/1509.02971">Continuous control with deep reinforcement learning</a> - a basic article on deep reinforcement learning for problems with continuous action space. </li><li>  <a href="https://arxiv.org/abs/1706.01905">Parameter Space Noise for Exploration</a> - a study on adding noise to the weight of an agent to better study the environment.  By experience - one of the best techniques for exploration in RL. </li></ul><br><p>  This year they added another ‚Äúcouple‚Äù: </p><br><ul><li>  <a href="https://arxiv.org/abs/1707.06887">A Distributional Perspective on Reinforcement Learning</a> - a new look at predicting possible rewards.  Instead of simply predicting the average, the distribution of the future reward is calculated. </li><li>  <a href="https://arxiv.org/abs/1710.10044">Distributional Reinforcement Learning with Quantile Regression</a> is a continuation of the previous work, but already with ‚Äúquantizing‚Äù the distribution. </li><li>  <a href="https://arxiv.org/abs/1803.00933">Distributed Prioritized Experience Replay</a> - work from the direction of deep reinforcement learning at scale.  How to properly organize the architecture of the experiment to maximize the use of available resources and increase the speed of training agents. </li><li>  <a href="https://arxiv.org/abs/1804.08617">Distributed Distributional Deterministic Policy Gradients</a> - combining the three previous articles for tasks with a continuous space of action. </li><li>  <a href="https://arxiv.org/abs/1802.09477">The Addressing Function Approximation Error in Actor-Critic Methods</a> is an excellent job of increasing the robustness of RL agents.  I recommend reading. </li><li>  <a href="https://arxiv.org/abs/1805.08296">Data-Efficient Hierarchical Reinforcement Learning</a> - a development of a previous article on hierarchical reinforcement learning (HRL). </li></ul><br><div class="spoiler">  <b class="spoiler_title">Additional reading</b> <div class="spoiler_text"><ul><li>  <a href="https://arxiv.org/abs/1801.01290">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a> - the authors proposed a method of training stochastic policies for off-policy reinforced learning.  Thanks to this article, it became possible to train non-deterministic politicians even in tasks with a continuous action space. </li><li>  <a href="https://arxiv.org/abs/1804.02808">Latent Space Policies for Hierarchical Reinforcement Learning</a> - a continuation of the previous HRL article with multi-level stochastic policies. </li><li>  <a href="https://arxiv.org/abs/1802.06070">Diversity is All You Need: Learning Skills without a Reward Function</a> - The article contains an approach to learning a variety of random, low-level stochastic policies without any reward from the environment.  Subsequently, when we have set the reward function, the most correlating with the award can be used to teach high-level policies on top. </li><li>  <a href="https://arxiv.org/abs/1805.00909">Probabilistic Inference: Tutorial and Review</a> - a review of all kinds of maximum entropy reinforced learning learning methods from <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a> . </li></ul><br><p>  I also advise <a href="https://spinningup.openai.com/en/latest/spinningup/keypapers.html">OpenAI to compile articles</a> on reinforcement learning and its <a href="https://www.mendeley.com/community/openai-spinningup/">version for mendeley</a> .  And if you are interested in the topic of training with reinforcements - join the <a href="https://t.me/theoreticalrl">RL club</a> and <a href="https://t.me/rlpapers">RL papers</a> . </p></div></div><br><h3 id="practice">  Practice </h3><br><img src="https://habrastorage.org/webt/xp/g7/it/xpg7itebqdpi3cwex33uzrzkidg.jpeg"><br><br>  Knowledge of theory alone is not enough - it is important to be able to implement all these approaches in practice and establish the correct validation system for evaluating decisions.  For example, this year we learned that our agent does not cope well with some marginal cases only 2 days before the end of the competition.  Because of this, we did not have time to completely correct our model and literally got a few points to the coveted second place.  If we found it at least a week later - the result could be better. <br><div class="spoiler">  <b class="spoiler_title">Coolstory: episode III</b> <div class="spoiler_text"><p>  The averaged reward for 10 test episodes served as the final evaluation of the decision. </p><br><img src="https://habrastorage.org/webt/jq/bj/yc/jqbjyctkjettuu2bqd19xcahssk.png"><br><p>  The graph shows the results of testing our agent: 9 out of 10 episodes, our skeleton passed just fine (average - 9955.66), but one episode .... episode 3 was not given to him (award 9870).  It was this mistake that led to a fall in the total score to 9947 (-8 points). </p></div></div><br><h3 id="udacha">  Luck </h3><br><p>  And finally - do not forget about banal luck.  Do not think that this is a controversial point.  On the contrary, a little luck strongly contributes to continuous work on yourself: even if the probability of luck is only 10%, a person who tried to participate in the competition 100 times succeeds much more than someone who tried only 1 time and abandoned the idea. </p><br><h1 id="tuda-i-obratno-reshenie-proshlogo-goda--trete-mestohttpswwwcrowdaiorgchallengesnips-2017-learning-to-runwinners">  There and back: the decision of last year - the <a href="https://www.crowdai.org/challenges/nips-2017-learning-to-run/winners">third place</a> </h1><br><img src="https://habrastorage.org/webt/mq/lx/i_/mqlxi_alc8pt0acnzwoi8twb8oo.jpeg"><br><br>  Last year, our team - Mikhail Pavlov and I - participated in NeurIPS competitions for the first time and the main motivation was simply to participate in the first NeurIPS competition in reinforcement learning.  Then I just completed the <a href="https://github.com/yandexdataschool/Practical_RL">Practical RL</a> course at the SAD and wanted to test my skills.  As a result, we took the honorable third place, behind only nnaisene (Schmidhuber) and the university team from China.  At that time, our <a href="https://github.com/Scitator/Run-Skeleton-Run">decision</a> was ‚Äúpretty simple‚Äù and was based on Distributed DDPG with parameter noise ( <a href="https://arxiv.org/abs/1711.06922">publication</a> and <a href="https://www.youtube.com/watch%3Fv%3DNFazgKEiPyk">performance on ml</a> . <a href="https://www.youtube.com/watch%3Fv%3DNFazgKEiPyk">Trainings</a> ). <br><h1 id="reshenie-etogo-goda--trete-mestohttpswwwcrowdaiorgchallengesnips-2018-ai-for-prosthetics-challengeleaderboards">  The decision of this year - the <a href="https://www.crowdai.org/challenges/nips-2018-ai-for-prosthetics-challenge/leaderboards">third place</a> </h1><br><img src="https://habrastorage.org/webt/gf/qq/to/gfqqtoneh51dn47m3f7oicyqixk.jpeg"><br><p>  This year there have been a couple of changes.  First, simply there was no desire to participate in this competition, I wanted to win it.  Secondly, the <a href="https://www.crowdai.org/participants/jolly-roger">team has</a> also changed: Alexey Grinchuk, Anton Pechenko, and me.  To take and win did not work, but we again took 3rd place. <br>  Our solution will be officially presented at NeurIPS, and now we will limit ourselves to a small number of details.  Taking the decision of last year and the success of off-policy reinforcement learning of this year (articles above), we added to this a number of our own developments, which we will tell on NeurIPS, and got Distributed Quantile Ensemble Critic, with which we took the third place. </p><br><p>  All of our achievements ‚Äì‚Äì distributed learning system, algorithms, etc. will be published and available in <a href="https://github.com/Scitator/catalyst">Catalyst.RL</a> after NeurIPS. </p><br><div class="spoiler">  <b class="spoiler_title">Coolstory: big boys - big guns</b> <div class="spoiler_text"><p>  Our team confidently went to the 1st place throughout the entire competition.  However, the big guys had other plans - 2 weeks before the end of the competition, 2 big players came to the competition at once: FireWork (Baidu) and nnaisense (Schmidhuber).  And if nothing could be done with Chinese Google, we managed to fairly fight for second place with the Schmidhuber team for a long time, losing only with a minimum margin.  It seems to me pretty good for lovers. </p></div></div><br><h1 id="zachem-eto-vse">  Why is this all? </h1><br><ul><li>  Connections  Top researchers come to the conference with whom you can communicate live, which will not give any email correspondence. </li><li>  Publication.  If the decision takes a prize, then the team is invited to the conference (and maybe more than one) to present its decision and publish the article. </li><li>  Job offer and PhD.  Publication and prizes in such a conference significantly increase your chances of getting a position in such leading companies as OpenAI, DeepMind, Google, Facebook, Microsoft. </li><li>  Real world value.  NeurIPS is conducted to solve actual problems of academic and real world.  You can be sure that the results will not go to the table, but will really be in demand and will help to improve the world. </li><li>  Drive.  Solving such contests ... just interesting.  In the conditions of competition, you can come up with many new ideas, test different approaches - just to be the best.  And let's be honest, when else can you drive skeletons, play games and all this with a serious look and for the sake of science? </li></ul><br><div class="spoiler">  <b class="spoiler_title">Coolstory: visa and RL</b> <div class="spoiler_text"><p>  I strongly recommend not trying to explain to the American who checks you that you are going to the conference because you are training virtual skeletons to run in simulations.  Just go to the conference with a report. </p></div></div><br><h1 id="itogi">  Results </h1><br><p>  Participation in NeurIPS is an experience that is difficult to overestimate.  Do not be afraid of loud headlines - you just need to pull yourself together and start to decide. </p><br><p>  And go to <a href="https://github.com/Scitator/catalyst">Catalyst.RL</a> , what really. </p></div><p>Source: <a href="https://habr.com/ru/post/430712/">https://habr.com/ru/post/430712/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../430702/index.html">Very strange training</a></li>
<li><a href="../430704/index.html">How artificial intelligence technologies help Aviasales grow: seven examples</a></li>
<li><a href="../430706/index.html">New theory of evolution</a></li>
<li><a href="../430708/index.html">Tic Tac Toe "Without Borders"</a></li>
<li><a href="../430710/index.html">What to do if Black Friday is tomorrow, and your servers are not ready</a></li>
<li><a href="../430714/index.html">VMware buys Heptio - what does this mean for Kubernetes</a></li>
<li><a href="../430718/index.html">What objects should use cloud video surveillance?</a></li>
<li><a href="../430720/index.html">Intel RealSense D435i: a small update and a small historical excursion</a></li>
<li><a href="../430722/index.html">PHP performance: we plan, profile, optimize</a></li>
<li><a href="../430724/index.html">Conference DEFCON 21. DNS can be dangerous for your health. Part 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>