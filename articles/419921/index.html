<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to drop 10 million packets per second</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the company, our team for resisting DDoS attacks is called ‚Äúpacket packet droppers‚Äù. While all the other teams are doing cool things with traffic p...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to drop 10 million packets per second</h1><div class="post__text post__text-html js-mediator-article">  In the company, our team for resisting DDoS attacks is called ‚Äúpacket packet droppers‚Äù.  While all the other teams are doing cool things with traffic passing through our network, we have fun finding new ways to get rid of it. <br><br><img src="https://habrastorage.org/webt/s-/3y/un/s-3yun8pllqd7-e077fuxbk9jiw.png"><br>  <i>Photography: <a href="https://www.flickr.com/photos/beegee49">Brian Evans</a> , <a href="https://creativecommons.org/licenses/by-sa/2.0/">CC BY-SA 2.0</a></i> <br><br>  The ability to quickly discard packets is very important in countering DDoS attacks. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Dropping packets reaching our servers can be done on several levels.  Each method has its pros and cons.  Under the cut, we look at everything we tried. <br><a name="habracut"></a><br><blockquote>  <i>Translator's note: in the output of some of the commands presented, extra spaces have been removed to preserve readability.</i> </blockquote><h1>  Test pad </h1><br>  For ease of comparison, we will give you some numbers, however, you should not take them too literally, due to the artificiality of the tests.  We will use one of our Intel servers with a 10Gbps network card.  The remaining server characteristics are not so important, because we want to focus on the limitations of the operating system, not the hardware. <br><br>  Our tests will look like this: <br><br><ul><li>  We load from a huge number of small UDP packets, reaching 14 million packets per second; </li><li>  All this traffic is directed to one processor core of the selected server; </li><li>  We measure the number of packets processed by the core on one core processor. </li></ul><br>  Artificial traffic is generated in such a way as to create maximum load: a random IP address and port of the sender are used.  Here‚Äôs something like this in tcpdump: <br><br><pre><code class="bash hljs">$ tcpdump -ni vlan100 -c 10 -t udp and dst port 1234 IP 198.18.40.55.32059 &gt; 198.18.0.12.1234: UDP, length 16 IP 198.18.51.16.30852 &gt; 198.18.0.12.1234: UDP, length 16 IP 198.18.35.51.61823 &gt; 198.18.0.12.1234: UDP, length 16 IP 198.18.44.42.30344 &gt; 198.18.0.12.1234: UDP, length 16 IP 198.18.106.227.38592 &gt; 198.18.0.12.1234: UDP, length 16 IP 198.18.48.67.19533 &gt; 198.18.0.12.1234: UDP, length 16 IP 198.18.49.38.40566 &gt; 198.18.0.12.1234: UDP, length 16 IP 198.18.50.73.22989 &gt; 198.18.0.12.1234: UDP, length 16 IP 198.18.43.204.37895 &gt; 198.18.0.12.1234: UDP, length 16 IP 198.18.104.128.1543 &gt; 198.18.0.12.1234: UDP, length 16</code> </pre> <br>  On the selected server, all packets will be in one RX queue and, therefore, processed by one core.  We achieve this with hardware flow control: <br><br><pre> <code class="bash hljs">ethtool -N ext0 flow-type udp4 dst-ip 198.18.0.12 dst-port 1234 action 2</code> </pre><br>  Performance testing is a complex process.  When we prepared the tests, we noticed that the presence of active raw sockets has a negative impact on performance, so you need to make sure that no <code>tcpdump</code> is running before running the tests.  There is an easy way to check for bad processes: <br><br><pre> <code class="bash hljs">$ ss -A raw,packet_raw -l -p|cat Netid State Recv-Q Send-Q Local Address:Port p_raw UNCONN 525157 0 *:vlan100 users:((<span class="hljs-string"><span class="hljs-string">"tcpdump"</span></span>,pid=23683,fd=3))</code> </pre><br>  Finally, we disable Intel Turbo Boost on our server: <br><br><pre> <code class="hljs pgsql">echo <span class="hljs-number"><span class="hljs-number">1</span></span> | sudo tee /sys/devices/<span class="hljs-keyword"><span class="hljs-keyword">system</span></span>/cpu/intel_pstate/no_turbo</code> </pre> <br>  Despite the fact that Turbo Boost is a great thing and increases throughput by at least 20%, it significantly spoils the standard deviation in our tests.  With turbo enabled, the deviation reaches ¬± 1.5%, while without it only 0.25%. <br><br><img src="https://habrastorage.org/webt/ic/ik/jq/icikjqda_ztydjswe8xjq07u5sa.png"><br><br><h3>  Step 1. Dropping packets in the application </h3><br>  Let's start with the idea of ‚Äã‚Äãdelivering all the packages to the application and ignoring them there.  For the integrity of the experiment, make sure that iptables does not affect the performance: <br><br><pre> <code class="bash hljs">iptables -I PREROUTING -t mangle -d 198.18.0.12 -p udp --dport 1234 -j ACCEPT iptables -I PREROUTING -t raw -d 198.18.0.12 -p udp --dport 1234 -j ACCEPT iptables -I INPUT -t filter -d 198.18.0.12 -p udp --dport 1234 -j ACCEPT</code> </pre><br>  An application is a simple loop in which incoming data is immediately dropped: <br><br><pre> <code class="python hljs">s = socket.socket(AF_INET, SOCK_DGRAM) s.bind((<span class="hljs-string"><span class="hljs-string">"0.0.0.0"</span></span>, <span class="hljs-number"><span class="hljs-number">1234</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: s.recvmmsg([...])</code> </pre><br>  We have already prepared the <a href="">code</a> , run: <br><br><pre> <code class="bash hljs">$ ./dropping-packets/recvmmsg-loop packets=171261 bytes=1940176</code> </pre><br>  This solution allows the kernel to take only 175 thousand packets from the hardware queue, as was measured by the <code>ethtool</code> utilities and <a href="https://blog.cloudflare.com/three-little-tools-mmsum-mmwatch-mmhistogram/">our</a> <code>mmwatch</code> : <br><br><pre> <code class="bash hljs">$ mmwatch <span class="hljs-string"><span class="hljs-string">'ethtool -S ext0|grep rx_2'</span></span> rx2_packets: 174.0k/s</code> </pre><br>  Technically, the server comes to 14 million packets per second, however, one processor core cannot cope with such a volume.  <code>mpstat</code> confirms this: <br><br><pre> <code class="bash hljs">$ watch <span class="hljs-string"><span class="hljs-string">'mpstat -u -I SUM -P ALL 1 1|egrep -v Aver'</span></span> 01:32:05 PM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle 01:32:06 PM 0 0.00 0.00 0.00 2.94 0.00 3.92 0.00 0.00 0.00 93.14 01:32:06 PM 1 2.17 0.00 27.17 0.00 0.00 0.00 0.00 0.00 0.00 70.65 01:32:06 PM 2 0.00 0.00 0.00 0.00 0.00 100.00 0.00 0.00 0.00 0.00 01:32:06 PM 3 0.95 0.00 1.90 0.95 0.00 3.81 0.00 0.00 0.00 92.38</code> </pre><br><br>  As we can see, the application is not a bottleneck: CPU # 1 is used at 27.17% + 2.17%, while interrupt handling takes 100% on CPU # 2. <br><br>  Using <code>recvmessagge(2)</code> plays an important role.  After Specter‚Äôs vulnerability was discovered, system calls became even more expensive due to the <abbr title="Kernel page-table isolation">KPTI</abbr> and <abbr title="Return trampoline">retpoline</abbr> used in the kernel <abbr title="Return trampoline">.</abbr> <br><br><pre> <code class="hljs ruby">$ tail -n +<span class="hljs-number"><span class="hljs-number">1</span></span> /sys/devices/system/cpu/vulnerabilities/* ==&gt; <span class="hljs-regexp"><span class="hljs-regexp">/sys/devices</span></span><span class="hljs-regexp"><span class="hljs-regexp">/system/cpu</span></span><span class="hljs-regexp"><span class="hljs-regexp">/vulnerabilities/meltdown</span></span> &lt;== <span class="hljs-symbol"><span class="hljs-symbol">Mitigation:</span></span> PTI ==&gt; <span class="hljs-regexp"><span class="hljs-regexp">/sys/devices</span></span><span class="hljs-regexp"><span class="hljs-regexp">/system/cpu</span></span><span class="hljs-regexp"><span class="hljs-regexp">/vulnerabilities/spectre</span></span>_v1 &lt;== <span class="hljs-symbol"><span class="hljs-symbol">Mitigation:</span></span> __user pointer sanitization ==&gt; <span class="hljs-regexp"><span class="hljs-regexp">/sys/devices</span></span><span class="hljs-regexp"><span class="hljs-regexp">/system/cpu</span></span><span class="hljs-regexp"><span class="hljs-regexp">/vulnerabilities/spectre</span></span>_v2 &lt;== <span class="hljs-symbol"><span class="hljs-symbol">Mitigation:</span></span> Full generic retpoline, IBPB, IBRS_FW</code> </pre><br><br><h3>  Step 2. Killing conntrack </h3><br>  We specifically made such a load with different IP and sender port in order to load conntrack as much as possible.  The number of entries in the conntrack during the test strives for the maximum possible and we can verify this: <br><br><pre> <code class="bash hljs">$ conntrack -C 2095202 $ sysctl net.netfilter.nf_conntrack_max net.netfilter.nf_conntrack_max = 2097152</code> </pre><br>  Moreover, in <code>dmesg</code> you can also see conntrack shouts: <br><br><pre> <code class="bash hljs">[4029612.456673] nf_conntrack: nf_conntrack: table full, dropping packet [4029612.465787] nf_conntrack: nf_conntrack: table full, dropping packet [4029617.175957] net_ratelimit: 5731 callbacks suppressed</code> </pre><br>  So let's turn it off: <br><br><pre> <code class="bash hljs">iptables -t raw -I PREROUTING -d 198.18.0.12 -p udp -m udp --dport 1234 -j NOTRACK</code> </pre><br>  And restart the tests: <br><br><pre> <code class="bash hljs">$ ./dropping-packets/recvmmsg-loop packets=331008 bytes=5296128</code> </pre><br><br>  This allowed us to reach the level of 333 thousand packets per second.  Hooray! <br>  PS With the use of SO_BUSY_POLL we can reach as much as 470 thousand per second, however, this is a topic for a separate post. <br><br><h3>  Step 3. Berkeley packet filter </h3><br>  Go ahead.  Why do we need to deliver packages to the application?  Although this is not a common solution, we can bind the classic Berkeley packet filter to a socket by calling <code>setsockopt(SO_ATTACH_FILTER)</code> and configure the filter to drop packets even in the kernel. <br>  Prepare the <a href="">code</a> , run: <br><br><pre> <code class="bash hljs">$ ./bpf-drop packets=0 bytes=0</code> </pre><br>  Using a packet filter (the classic and advanced Berkeley filters give roughly the same performance) we get to about 512 thousand packets per second.  Moreover, dropping a packet during an interrupt frees the processor from having to wake up the application. <br><br><h3>  Step 4. iptables DROP after routing </h3><br>  Now we can drop packets by adding the following rule to iptables in the INPUT chain: <br><br><pre> <code class="bash hljs">iptables -I INPUT -d 198.18.0.12 -p udp --dport 1234 -j DROP</code> </pre><br>  Let me remind you that we have already disabled conntrack with the <code>-j NOTRACK</code> rule.  These two rules give us 608 thousand packets per second. <br><br>  Look at the numbers in iptables: <br><br><pre> <code class="bash hljs">$ mmwatch <span class="hljs-string"><span class="hljs-string">'iptables -L -v -n -x | head'</span></span> Chain INPUT (policy DROP 0 packets, 0 bytes) pkts bytes target prot opt <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> out <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> destination 605.9k/s 26.7m/s DROP udp -- * * 0.0.0.0/0 198.18.0.12 udp dpt:1234</code> </pre><br>  Well, not bad, but we can do better. <br><br><h3>  Step 5. iptabes DROP to PREROUTING </h3><br>  Faster technology is to drop packets before routing using this rule: <br><br><pre> <code class="bash hljs">iptables -I PREROUTING -t raw -d 198.18.0.12 -p udp --dport 1234 -j DROP</code> </pre> <br>  This allows us to discard a solid 1.688 million packets per second. <br><br>  In fact, this is a slightly surprising jump in performance.  I did not understand the reasons, maybe our routing is difficult, or maybe just a bug in the server configuration. <br><br>  In any case, raw iptables work much faster. <br><br><h3>  Step 6. nftables DROP </h3><br>  Now the iptables utility is already a bit old.  It was replaced by nftables.  Check out <a href="https://www.youtube.com/watch%3Fv%3D9Zr8XqdET1c">this video explaining</a> why nftables are top.  Nftables promise to be faster than gray iptables for a variety of reasons, including the rumor that retpoline slows down iptables a lot. <br><br>  But our article is still not about comparing iptables and nftables, so let's just try the quickest thing I could do: <br><br><pre> <code class="bash hljs">nft add table netdev filter nft -- add chain netdev filter input { <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> filter hook ingress device vlan100 priority -500 \; policy accept \; } nft add rule netdev filter input ip daddr 198.18.0.0/24 udp dport 1234 counter drop nft add rule netdev filter input ip6 daddr fd00::/64 udp dport 1234 counter drop</code> </pre><br>  Counters can be seen as: <br><br><pre> <code class="bash hljs">$ mmwatch <span class="hljs-string"><span class="hljs-string">'nft --handle list chain netdev filter input'</span></span> table netdev filter { chain input { <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> filter hook ingress device vlan100 priority -500; policy accept; ip daddr 198.18.0.0/24 udp dport 1234 counter packets 1.6m/s bytes 69.6m/s drop <span class="hljs-comment"><span class="hljs-comment"># handle 2 ip6 daddr fd00::/64 udp dport 1234 counter packets 0 bytes 0 drop # handle 3 } }</span></span></code> </pre><br>  The nftables input hook showed values ‚Äã‚Äãof about 1.53 million packets.  This is a little less than the PREROUTING chain in iptables.  But there is a mystery in this: theoretically, the nftables hook comes earlier than PREROUTING iptables and, therefore, should be processed faster. <br><br>  In our test, nftables are slightly slower than iptables, but still, nftables are cooler.  : P <br><br><h3>  Step 7. tc DROP </h3><br>  Somewhat unexpectedly, the tc (traffic control) hook occurs earlier than iptables PREROUTING.  tc allows us to select packets according to simple criteria and, of course, discard them.  The syntax is a bit unusual, so for the customization we suggest using <a href="">this script</a> .  And we need a rather complicated rule that looks like this: <br><br><pre> <code class="bash hljs">tc qdisc add dev vlan100 ingress tc filter add dev vlan100 parent ffff: prio 4 protocol ip u32 match ip protocol 17 0xff match ip dport 1234 0xffff match ip dst 198.18.0.0/24 flowid 1:1 action drop tc filter add dev vlan100 parent ffff: protocol ipv6 u32 match ip6 dport 1234 0xffff match ip6 dst fd00::/64 flowid 1:1 action drop</code> </pre><br>  And we can check it in action: <br><br><pre> <code class="bash hljs">$ mmwatch <span class="hljs-string"><span class="hljs-string">'tc -s filter show dev vlan100 ingress'</span></span> filter parent ffff: protocol ip pref 4 u32 filter parent ffff: protocol ip pref 4 u32 fh 800: ht divisor 1 filter parent ffff: protocol ip pref 4 u32 fh 800::800 order 2048 key ht 800 bkt 0 flowid 1:1 (rule hit 1.8m/s success 1.8m/s) match 00110000/00ff0000 at 8 (success 1.8m/s ) match 000004d2/0000ffff at 20 (success 1.8m/s ) match c612000c/ffffffff at 16 (success 1.8m/s ) action order 1: gact action drop random <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> none pass val 0 index 1 ref 1 <span class="hljs-built_in"><span class="hljs-built_in">bind</span></span> 1 installed 1.0/s sec Action statistics: Sent 79.7m/s bytes 1.8m/s pkt (dropped 1.8m/s, overlimits 0 requeues 0)</code> </pre><br>  The tc hook allowed us to drop up to 1.8 million packets per second on a single core.  It's fine! <br>  But we can even faster ... <br><br><h3>  Step 8. XDP_DROP </h3><br>  And finally, our strongest weapon: XDP - <a href="https://prototype-kernel.readthedocs.io/en/latest/networking/XDP/">eXpress Data Path</a> .  With the help of XDP, we can run the Berkeley extended packet filter (extended Berkley Packet Filter, eBPF) code directly in the context of the network driver and, most importantly, before allocating memory under <code>skbuff</code> , which promises us an increase in speed. <br><br>  Usually an XDP project consists of two parts: <br><br><ul><li>  eBPF downloadable code </li><li>  bootloader that puts code on the correct network interface </li></ul><br>  Writing your bootloader is a difficult task, so just use the <a href="https://cilium.readthedocs.io/en/latest/bpf/">new iproute2 chip</a> and load the code with a simple command: <br><br><pre> <code class="bash hljs">ip link <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> dev ext0 xdp obj xdp-drop-ebpf.o</code> </pre><br>  Ta-dam! <br><br>  The source code of the <a href="">downloadable eBPF program is available here</a> .  The program looks at such characteristics of IP packets as the UDP protocol, the sender's subnet and destination port: <br><br><pre> <code class="hljs swift"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (h_proto == htons(<span class="hljs-type"><span class="hljs-type">ETH_P_IP</span></span>)) { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (iph-&gt;<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">protocol</span></span></span><span class="hljs-class"> == </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">IPPROTO_UDP</span></span></span><span class="hljs-class"> &amp;&amp; (</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">htonl</span></span></span><span class="hljs-class">(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">iph</span></span></span><span class="hljs-class">-&gt;</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">daddr</span></span></span><span class="hljs-class">) &amp; 0</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">xFFFFFF00</span></span></span><span class="hljs-class">) == 0</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">xC6120000</span></span></span><span class="hljs-class"> // 198.18.0.0/24 &amp;&amp; </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">udph</span></span></span><span class="hljs-class">-&gt;</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">dest</span></span></span><span class="hljs-class"> == </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">htons</span></span></span><span class="hljs-class">(1234)) </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-type"><span class="hljs-type">XDP_DROP</span></span>; } }</code> </pre><br>  The XDP program should be compiled using modern clang, which can generate BPF bytecode.  After that we can download and test the functionality of the BFP program: <br><br><pre> <code class="bash hljs">$ ip link show dev ext0 4: ext0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 xdp qdisc fq state UP mode DEFAULT group default qlen 1000 link/ether 24:8a:07:8a:59:8e brd ff:ff:ff:ff:ff:ff prog/xdp id 5 tag aedc195cc0471f51 jited</code> </pre><br>  And then see the statistics in <code>ethtool</code> : <br><br><pre> <code class="bash hljs">$ mmwatch <span class="hljs-string"><span class="hljs-string">'ethtool -S ext0|egrep "rx"|egrep -v ": 0"|egrep -v "cache|csum"'</span></span> rx_out_of_buffer: 4.4m/s rx_xdp_drop: 10.1m/s rx2_xdp_drop: 10.1m/s</code> </pre><br>  Yu-hoo!  With XDP, we can drop up to 10 million packets per second! <br><br><img src="https://habrastorage.org/webt/fq/mf/e5/fqmfe5jgs1qylz7fpwryoeey0ag.png"><br>  <i>Photo: <a href="https://www.flickr.com/photos/afiler/">Andrew Filer</a> , <a href="https://creativecommons.org/licenses/by-sa/2.0/">CC BY-SA 2.0</a></i> <br><br><h3>  findings </h3><br>  We repeated the experiment for IPv4 and for IPv6 and prepared this diagram: <br><br><img src="https://habrastorage.org/webt/dl/sc/qa/dlscqatdfii2rihttytjnfvtfea.png"><br>  In general, it can be argued that our configuration for IPv6 is slightly slower.  But since IPv6 packets are somewhat larger, the difference in speed is expected. <br><br>  In Linux, there are many ways to filter packets, each with its own speed and complexity of configuration. <br><br>  To protect against DDoS, it is quite reasonable to give packets to the application and process them there.  A well-tuned application can perform well. <br><br>  For DDoS attacks with random or spoofed IPs, it may be useful to disable conntrack to get a small increase in speed, but beware: there are attacks against which conntrack is very useful. <br><br>  In other cases, it makes sense to add a Linux firewall as one of the ways to mitigate a DDoS attack.  In some cases it is better to use the "-t raw PREROUTING" table, since it is much faster than the filter table. <br><br>  For the most neglected cases, we always use XDP.  And yes, this is a very powerful thing.  Here is the chart as above, only with XDP: <br><br><img src="https://habrastorage.org/webt/o4/un/dm/o4undm-syfgavdxop6izkvpckyk.png"><br>  If you want to repeat the experiment, here is the <a href="">README, in which we have documented everything</a> . <br><br>  We use CloudFlare ... almost all of these techniques.  Some user space tricks are integrated into our applications.  The technique with iptables is found in our <a href="https://blog.cloudflare.com/meet-gatebot-a-bot-that-allows-us-to-sleep/">Gatebot</a> .  Finally, we replace our own core solution with XDP. <br><br>  Many thanks to <a href="https://twitter.com/JesperBrouer">Jesper Dangaard Brouer</a> for help with the work. </div><p>Source: <a href="https://habr.com/ru/post/419921/">https://habr.com/ru/post/419921/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../419911/index.html">Building orbits of celestial bodies by means of Python</a></li>
<li><a href="../419913/index.html">IKEA and smart home. Part 2</a></li>
<li><a href="../419915/index.html">Bonding and SSH server in initramfs</a></li>
<li><a href="../419917/index.html">Neural networks: implementation of the task about mushrooms on Tensor Flow and Python</a></li>
<li><a href="../419919/index.html">Version Control Inside SQL Server</a></li>
<li><a href="../419923/index.html">My time creativity, clock from motherboards</a></li>
<li><a href="../419925/index.html">Version control of individual files using GitHub Gist</a></li>
<li><a href="../419927/index.html">[DotNetBook] Exceptions: type system architecture</a></li>
<li><a href="../419929/index.html">[DotNetBook] Exception events and how to get StackOverflow and ExecutionEngineException from scratch</a></li>
<li><a href="../419931/index.html">[DotNetBook] Time for entertaining stories: exceptionally exceptional situations</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>