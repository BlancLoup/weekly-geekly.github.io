<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The pitfalls of backup and recovery of deduplicated data in a disaster recovery script</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Developing the theme of backup and recovery on the storage system with a new architecture, consider the nuances of working with deduplicated data in t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The pitfalls of backup and recovery of deduplicated data in a disaster recovery script</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/671/e3d/aba/671e3dabaed34e06ac6eed9d578a0895.jpg"><br><br>  Developing the theme of backup and recovery on the storage system with a new architecture, consider the nuances of working with deduplicated data in the disaster recovery script, where storage systems with their own deduplication are protected, namely, how this efficient storage technology can help or hinder data recovery. <br><br><a name="habracut"></a><br>  The previous article is here: <a href="http://habrahabr.ru/company/emc/blog/269933/">Pitfalls backup in hybrid storage systems</a> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  Introduction </h3><br>  Once the deduplicated data takes up less disk space, it is logical to assume that backup and recovery should take less time.  Indeed, why not backup / restore deduplicated data immediately in a compact deduplicated form?  Indeed, in this case: <br><br><ul><li>  Only unique data is placed in the backup. </li><li>  No need to reduplytirovat (rehydrate) data on a productive system. </li><li>  No need to dedup data back to IBS. </li><li>  Conversely, you can restore only those unique data that are necessary for reconstruction.  Nothing extra. </li></ul><br>  But if you look at the situation more closely, it turns out that not everything is so simple, the direct path is not always more efficient.  At least because in general purpose storage systems and backup storage systems different deduplication is used. <br><br><h3>  Deduplication in general purpose storage systems </h3><br>  Deduplication, as a method of eliminating redundant data and increasing storage efficiency, has been and remains one of the key areas of development in the storage industry. <br><br><img src="https://habrastorage.org/files/c5c/83a/330/c5c83a3303af4ddba1a9cc4cb954dccf.jpg"><br>  <i>The principle of deduplication.</i> <br><br>  In the case of productive data, deduplication is intended not only and not so much to reduce disk space, but to increase the speed of access to data due to their more dense placement on fast media.  In addition, deduplicated data is convenient for caching. <br><br>  One deduplitsirovanny block in the cache, on the top level of multi-level storage, or simply placed on the flash, can correspond to dozens or even hundreds of identical user data blocks, which previously took place on physical disks and had completely different addresses, and therefore could not be effectively cached <br><br>  Today, deduplication on general purpose storage is very effective and profitable.  For example: <br><br><ul><li>  On the flash system (All-Flash Array) you can put significantly more logical data than their raw capacity usually allows. </li><li>  When using hybrid systems, deduplication helps to identify ‚Äúhot‚Äù data blocks, since it only retains unique data.  And the higher the deduplication, the more calls to the same blocks, which means - the higher the efficiency of multi-level storage. </li></ul><br><img src="https://habrastorage.org/files/5b8/ca9/6b3/5b8ca96b34e2403b89f5538a22b6c901.png"><br>  <i>Efficiency of solving storage problems using a combination of deduplication and tiering.</i>  <i>In each embodiment, equal performance and capacity is achieved.</i> <br><br><h3>  Deduplication in storage backup </h3><br>  Initially, deduplication became widespread in these systems.  Due to the fact that data blocks of the same type are copied dozens, or even hundreds of times, into the CPK, substantial space savings can be achieved by eliminating redundancy.  At one time, this was the reason for the ‚Äúoffensive‚Äù on tape disk library systems for backup with deduplication.  The disk has heavily pressed the tape, because the cost of storing backups on disks has become very competitive. <br><br><img src="https://habrastorage.org/files/7c5/b24/78a/7c5b2478adbe4f52b5f3158cd4c77d44.jpg"><br>  <i>The advantage of deduplitsirovannogo backup disks.</i> <br><br>  As a result, even such adherents of tapes as Quantum began to develop disk libraries with deduplication. <br><br><h3>  What deduplication is better? </h3><br>  Thus, in the storage world at the moment there are two different ways of deduplication - in backup and in general-purpose systems.  The technologies they use are different - with variable and fixed blocks, respectively. <br><img src="https://habrastorage.org/files/a47/ccd/621/a47ccd6211d34302874d166900c7cc4a.jpg"><br>  <i>The distinction between the two methods of deduplication.</i> <br><br>  Fixed block deduplication is easier to implement.  It is well suited for data that needs regular access, so it is more commonly used in general purpose storage systems.  Its main disadvantage is the lesser ability to recognize identical data sequences in the general stream.  That is, two identical streams with a small offset will be perceived as completely different, and will not be deduplicated. <br><br>  Variable block deduplication can better recognize repetitions in a data stream, but for this it needs more processor resources.  In addition, it is unsuitable for providing block or multi-threaded access to data.  This is due to the storage structure of the deduplicated information: if to put it simply, it is also stored in variable blocks. <br><br>  With their tasks, both methods help to cope perfectly, but with unusual tasks, everything is much worse. <br><br>  Let's look at the situation that arises at the interface of the two technologies. <br><br><h3>  Problems backing up deduplicated data </h3><br>  The difference between the two approaches in the absence of coordinated interaction leads to the fact that if the storage system that stores already deduplicated data is backed up with deduplication, then the data is ‚Äúduplicated‚Äù each time, and then deduplicated back in the process of saving them to the backup system copying. <br><br>  For example, physically stored 10 TB of productive deduplicated data with a total ratio of 5: 1.  Then during the backup process, the following happens: <br><br><ul><li>  Not 10, but completely 50 TB are copied. </li><li>  A productive system in which the original data is stored will have to do the work of rehydrating (‚Äúreplicating‚Äù) the data in the opposite direction.  At the same time, it should provide productive applications and backup data flow.  That is, three simultaneous heavy processes that load system I / O buses, cache memory, and processor cores of both storage systems. </li><li>  The target backup system will have to deduplicate the data back. </li></ul><br>  In terms of use of processor resources - this can be compared with the simultaneous pressing of the gas and brake.  The question arises - can this be somehow optimized? <br><br><h3>  The problem of restoring deduplicated data </h3><br>  When restoring data to volumes with deduplication enabled, you will have to repeat the whole process in the opposite direction.  Not all storage systems have this process running on the fly, and many solutions use the ‚Äúpost process‚Äù principle.  That is, the data is first recorded on physical disks (even if on flash) as is, then analyzed, data blocks are compared, duplicates are detected, and only then is the cleaning performed. <br><br><img src="https://habrastorage.org/files/765/e1f/195/765e1f1958444e55a2e705687ed48eaf.jpg"><br>  <i>Comparison of in-line and Post-Process Dedupe.</i> <br><br>  This means that in the storage system during the first pass, there may potentially be not enough space to fully recover all non-duplicated data.  And then you have to do the restoration in several passes, each of which can take a lot of time, consisting of recovery time and deduplication time with the release of space on the general purpose storage system. <br><br>  This possible scenario relates not so much to recovering data from a data recovery (minimizing the risks of the Data loss class), but rather to recovering from a catastrophically large data loss (which is classified as a disaster, that is, Disaster).  However, to put it mildly, such Disaster Recovery is not optimal. <br><br>  In addition, in case of a catastrophic failure, it is not at all necessary to restore all the data at once.  It is enough to start only with the most necessary. <br><br>  As a result, the backup, which is designed to be a means of last resort, which is addressed when nothing else worked, does not work optimally in the case of deduplicating general-purpose storage systems. <br><br>  Why, then, do we need a backup from which, in the event of a disaster, one can recover only with great difficulty, and almost certainly not completely?  After all, there are replication tools built into the productive storage system (mirroring, snapshots) that do not have a significant impact on performance (for example, VNX Snapshots, XtremIO Snapshots).  <a href="http://denserov.com/2013/01/30/bu-vs-rep/">The answer to this question will be all the same</a> .  However, any normal engineer would try to somehow optimize and improve this situation. <br><br><h3>  How to combine two worlds? </h3><br>  The old organization of working with data during backup and restoration looks at least strange.  Therefore, many attempts were made to optimize the backup and restore deduplicated data, and a number of problems were solved. <br><br>  Here are just a few examples: <br><br><ul><li>  <a href="https://community.emc.com/thread/181581">Windows 2012 Deduplication with Networker</a> </li><li>  <a href="http://www.symantec.com/connect/forums/windows-server-2012-deduplication-and-backup-exec">Windows Server 2012 Deduplication and Backup Exec</a> </li><li>  <a href="https://technet.microsoft.com/en-us/library/hh831600.aspx">Backup and Restore Considerations for Deduplicated Volumes</a> </li><li>  <a href="https://msdn.microsoft.com/en-us/library/hh769304%2528v%3Dvs.85%2529.aspx">Backup and Restore of Data Deduplication-Enabled Volumes</a> </li></ul><br>  But these are just ‚Äúpatches‚Äù at the level of operating systems and individual isolated servers.  They do not solve problems at the general hardware level in SHD where it is really difficult for making. <br><br>  The fact is that in general purpose storage systems and in backup systems, different, specially developed deduplication algorithms are used - with fixed and variable blocks. <br><br>  On the other hand, it is not always necessary to do a full backup, and much less often a full restore.  It is not necessary to deduplicate and compress all productive data.  However, you need to remember the nuances.  Because the <a href="http://continuitysolutionsgroup.com/services">catastrophic loss of data has not been canceled</a> .  And to prevent them, standard industrial solutions have been developed, which should be provided for under the regulations.  So, if data cannot be recovered from a backup in normal time, then responsible people may cost a career. <br><br><img src="https://habrastorage.org/files/2ce/776/e99/2ce776e999494dbdb4ea999c86a1f2ea.jpg"><br><br>  Let's look at how to best prepare for this situation and avoid unpleasant surprises. <br><br>  <b>Backup</b> <br><br><ul><li>  If possible, use incremental backup and synthetic full copies.  In Networker, for example, this feature is available starting from version 8. </li><li>  Leave more time to full backup, considering the need to re-organize data.  Choose the time for the minimum utilization of system processors.  In the course of backups, it is better to watch the disposal of productive storage processors.  It is better that it does not exceed 70% at least on average during the backup period. </li><li>  Use deduplication meaningfully.  If the data does not deduplitsiruyutsya and do not huddle, then why waste processor power during backup?  If the system always deduplicates, then it should be powerful enough to handle all the work. </li><li>  Consider the processor power allocated for deduplication in storage.  This function is even found in entry-level systems that do not always cope with the simultaneous execution of all tasks. </li></ul><br>  <b>Full data recovery, Disaster Recovery</b> <br><br><ul><li>  Prepare a sane Disaster Recovery or Business Continuity Plan, taking into account the behavior of storage systems with deduplication.  Many vendors, including EMC, as well as system integrators, offer the services of such planning, because each organization has its own unique combination of factors affecting the process of restoring applications to work. </li><li>  If the general purpose storage system uses the post-process deduplication mechanism, then I would recommend to provide in it a free capacity buffer, in case of recovery from backup.  For example, the buffer size can be taken as 20% of the logical capacity of the deduplicated data.  Try to support this parameter at least on average. </li><li>  Look for opportunities to archive old data so that they do not interfere with quick recovery.  Even if deduplication is good and effective, do not wait for a crash, after which you will have to restore from the backup and completely deduplicate the volume to many dozens of TB.  All non-operational / historical data is better transferred to an online archive (for example, based on Infoarchive). </li><li>  On-the-fly data deduplication in general purpose storage systems has an advantage over post-process in terms of speed.  It can play a special role in recovering from a catastrophic loss. </li></ul><br>  These are some of my considerations regarding backing up and restoring deduplicated data.  I will be glad to hear here your feedback and opinions on this issue. <br><br>  And I must say that one interesting special case that requires separate consideration is not affected here yet.  So to be continued. <br>  <a href="http://denserov.com/">Denis Serov</a> </div><p>Source: <a href="https://habr.com/ru/post/270907/">https://habr.com/ru/post/270907/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../270889/index.html">The best reports of the DotNext 2015 Piter conference: Part 2 (Video inside)</a></li>
<li><a href="../270891/index.html">Incorrectly used mobile interface patterns</a></li>
<li><a href="../270895/index.html">Analysis of price changes in Russian online stores</a></li>
<li><a href="../270897/index.html">IBM Corporation opens the Bluemix blog on Habrahabr</a></li>
<li><a href="../270901/index.html">The digest of events from the world D ‚Ññ2</a></li>
<li><a href="../270909/index.html">Testing of domains or more than analyzing boundary values</a></li>
<li><a href="../270913/index.html">Objective-C What is actually a method and self? + runtime</a></li>
<li><a href="../270917/index.html">The digest of interesting materials from the world of web development and IT for the last week ‚Ññ185 (November 8 - 15, 2015)</a></li>
<li><a href="../270919/index.html">Let's encrypt: start of the public beta on December 3</a></li>
<li><a href="../270927/index.html">Quiz of 15 questions: do you know CSS well</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>