<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Recommended systems: user-based and item-based</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="So, last time we talked a little about what recommender systems are all about and what problems they face, and also about what the formulation of the ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Recommended systems: user-based and item-based</h1><div class="post__text post__text-html js-mediator-article">  So, last time we talked a little about what recommender systems are all about and what problems they face, and also about what the formulation of the collaborative filtering problem looks like.  Today I will talk about one of the simplest and most natural methods of collaborative filtering, from which research in this area began in the 90s.  The basic idea is very simple: how to understand whether Vasya will like the film ‚ÄúTractor drivers‚Äù?  You just need to find other users similar to Vasya, and see what ratings they put on ‚ÄúTractor drivers‚Äù.  Or on the other hand: how to understand whether Vasya likes the film ‚ÄúTractor Drivers‚Äù?  You just need to find other films similar to "Tractor Drivers" and see how Vasya evaluated them. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3c1/df9/dea/3c1df9dead1c6d225bb5cc2af35aff28.jpg"><br><a name="habracut"></a><br>  Let me remind you that we have a very sparse matrix consisting of ratings (likes, purchase facts, etc.) that users (matrix rows) assigned to products (matrix columns): <br><img src="https://habrastorage.org/getpro/habr/post_images/f62/51e/c9e/f6251ec9ee67f3241437207b258d0a90.png"><br>  Our task is to predict estimates. <img src="https://habrastorage.org/getpro/habr/post_images/4a0/b97/9de/4a0b979de77bada1030ca4bf6d717e7b.png">  knowing some of the ratings already placed in the matrix <img src="https://habrastorage.org/getpro/habr/post_images/b40/547/83b/b4054783bb4ea1af5b7f52f7e85bbb3d.png">  .  Our best prediction will be denoted by <img src="https://habrastorage.org/getpro/habr/post_images/e72/8c0/2a2/e728c02a262a0414a6f7bfb713501352.png">  .  If we can get these predictions, we just need to select one or more products for which <img src="https://habrastorage.org/getpro/habr/post_images/e72/8c0/2a2/e728c02a262a0414a6f7bfb713501352.png">  maximum. <br><br>  We mentioned two approaches: either look for similar users ‚Äî this is called ‚Äúuser-based collaborative filtering‚Äù, or look for similar products ‚Äî that is logical, called ‚Äúproduct-based recommendations‚Äù (item-based collaborative filtering).  Actually, the basic algorithm in both cases is clear. <br><ol><li>  Find how other users (products) in the database are similar to this user (product). </li><li>  It is estimated by other users (products) to predict what rating this user will give to this product, given the greater weight of those users (products) that are more similar to this one. </li></ol><br>  It remains only to understand how to do it all. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      First, you need to determine what ‚Äúsimilar‚Äù means.  I remind you that all we have is a vector of preferences for each user (rows of the R matrix) and a vector of user ratings for each product (columns of the R matrix).  First of all, we leave in these vectors only those elements for which we know the values ‚Äã‚Äãin both vectors, i.e.  Leave only those products that are rated by both users, or only those users who both rate this product.  As a result, we just need to determine how similar the two vectors of real numbers are.  This is, of course, a well-known problem, and its classical solution is to calculate the correlation coefficient: for two user preference vectors i and j, the Pearson correlation coefficient is <br><img src="https://habrastorage.org/getpro/habr/post_images/477/c03/bb3/477c03bb3961f88e1bc6203574e27df2.png"><br>  Where <img src="https://habrastorage.org/getpro/habr/post_images/ea8/1cb/31b/ea81cb31b883cc286cba3ec79a507146.png">  - average rating given by user i.  Sometimes they use the so-called "cosine similarity" using the cosine of the angle between the vectors: <br><img src="https://habrastorage.org/getpro/habr/post_images/752/11a/894/75211a894c127cde693822f29c832610.png"><br>  But in order for the cosine to work well, it is advisable to still first subtract the average for each vector, so in reality it is the same metric. <br><br>  For example, consider some evaluation matrix. <br><img src="https://habrastorage.org/storage2/adb/dbc/b85/adbdbcb85e0cd9ca59065d7425788b6d.png"><br><br>  We will leave the manual analysis of preferences to the reader, and calculate for the user-based recommendations the correlation between the preference vector of Vasya and the rest of the participants in the system. <br><img src="https://habrastorage.org/storage2/25b/43a/3fd/25b43a3fdeb2ab181c67762f8351339c.png"><br><br>  We have now given formulas for user-based recommendations.  In the item-based approach, the situation is similar, but there is one nuance: different users relate differently to the estimates, someone puts everyone in a row with five asterisks (websites like "like"), and someone, on the contrary, puts everything in two or three stars (often shakes "dizlike").  For the first user, a low rating (‚Äúdizlink‚Äù) will be much more informative than a high one, and for the second, the opposite.  In the user-based approach, the correlation coefficient automatically takes care of this.  And in the item-based recommendations, to take this into account, you can, for example, subtract the average rating of a particular user from each assessment, and then calculate the correlation or cosine of the angle between the vectors.  Then in the formula for the cosine we get <br><img src="https://habrastorage.org/getpro/habr/post_images/703/f2c/3d6/703f2c3d646a96f952131bf15a4b712c.png">  , <br>  Where <img src="https://habrastorage.org/getpro/habr/post_images/ea8/1cb/31b/ea81cb31b883cc286cba3ec79a507146.png">  denotes the average rating given by user i.  In our example, if you subtract the average from each vector of estimates, you get this: <br><img src="https://habrastorage.org/storage2/291/2ae/f0c/2912aef0c86d8582a7a88437a77bd93d.png"><br><br>  And then the correlation between the ratings vector of the film ‚ÄúTractorists‚Äù and the ratings of other films will be (note that a degenerate situation has developed with ‚ÄúOnce Upon a Tractor‚Äù because there were too few overlapping assessments) <br><img src="https://habrastorage.org/storage2/410/f86/828/410f86828592257d8b1c455244b48b20.png"><br><br>  These measures of similarity have their drawbacks and various variations on the theme, but for the illustration of the methods let us limit ourselves to them.  Thus, we have dealt with the first point of the plan;  Next in line is the second - how to use these similarity estimates (weights <img src="https://habrastorage.org/getpro/habr/post_images/b52/6c3/5f8/b526c35f880be479697260f22bfff7b5.png">  )? <br><br>  A simple and logical approach is to bring a new rating as the average rating of a given user plus deviations from the average ratings of other users weighed by these weights: <br><img src="https://habrastorage.org/getpro/habr/post_images/39e/00a/c9d/39e00ac9d0ba73af29d42eb56748359a.png">  . <br>  This approach is sometimes also called the GroupLens algorithm - this is how the grandfather of the GroupLens recommender systems worked (by the way, pardon the pun, I recommend <a href="http://movielens.umn.edu/">MovieLens</a> site] - serious collaborative filtering actually started with it once, but today it can please little-known films that you like) .  In the case of Vasya and Tractor Engineers, this method is expected to have a rating of about 3.9 (Petrovich let us down a little), so you can safely look. <br><br>  For item-based recommendations, everything is absolutely equivalent - you just need to find the weighted average of the products already rated by the user: <br><img src="https://habrastorage.org/getpro/habr/post_images/ff0/9b3/530/ff09b353057eba7078e1187095169480.png"><br>  The item-based method in our example assumes that Vasya will deliver to "Tractor Engineers" 4.4. <br><br>  Of course, theoretically all this is good, but in reality it is unlikely for each recommendation to summarize the data from millions of users.  Therefore, this formula is usually roughed up to k nearest neighbors - k users who are as similar as possible to this user and have already evaluated this product: <br><img src="https://habrastorage.org/getpro/habr/post_images/b16/f39/145/b16f391454c835fdbcc79dcd2f06dd89.png"><br><br>  It remains only to understand how to quickly find the nearest neighbors.  This is already beyond the scope of our discussion, so I‚Äôll just call two main methods: in small dimensions, you can use kd-trees (kd-trees, see, for example, <a href="http://citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.10.818%26rep%3Drep1%26type%3Dpdf">this introduction</a> ), and in large dimensions you can use locally sensitive hashing (locally sensitive). hashing, see, for example, <a href="http://www.slaney.org/malcolm/yahoo/Slaney2008-LSHTutorial.pdf">here</a> ). <br><br>  So, today we have considered the simplest recommender algorithms that evaluate the similarity of a user (product) to other users (other products), and then recommend, based on the opinion of the closest neighbors in this metric.  By the way, one should not think that these methods are completely outdated - for many problems they work perfectly.  However, in the next series, we turn to a more subtle analysis of the available information.  In particular, from the very beginning I persistently called the input data a ‚Äúrating matrix,‚Äù although it would seem that we never used the fact that it is really a matrix, and not just a set of vectors (either rows or columns, depending on the method).  Next time - use. <br><br>  PS For images with formulas, thanks to the <a href="http://mathurl.com/">mathURL</a> service.  Unfortunately, he does not understand Russian letters, so the tables about Vasya and Petrovich had to be made by hand. </div><p>Source: <a href="https://habr.com/ru/post/139518/">https://habr.com/ru/post/139518/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../139511/index.html">Functional and economic comparison of Russian cloud service providers</a></li>
<li><a href="../139513/index.html">Who, Why and How looking for startups</a></li>
<li><a href="../139515/index.html">Political spam about the rally on Pushkin Square "kills" users' computers</a></li>
<li><a href="../139516/index.html">Reading the book prevents ... the tablet on which you read it</a></li>
<li><a href="../139517/index.html">Unity3D distributes licenses to Android and iOS for free</a></li>
<li><a href="../139521/index.html">Alisher Usmanov became the richest Russian thanks to Facebook, Zynga and Groupon</a></li>
<li><a href="../139522/index.html">Convergent billing for three popular hypervisors</a></li>
<li><a href="../139523/index.html">Designing Interfaces for Small Children</a></li>
<li><a href="../139524/index.html">Ubuntu for Android - the first look at the symbiosis of operating systems</a></li>
<li><a href="../139525/index.html">A bunch of LightSquid + Active Directory</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>