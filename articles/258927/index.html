<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Salesforce Data Migration with Pentaho Data Integration</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr. 

 My name is Ilya Grebtsov, I work as a Java / JS Developer in DataArt. I want to share something useful with those who work with Salesforc...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Salesforce Data Migration with Pentaho Data Integration</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr. <br><br>  My name is Ilya Grebtsov, I work as a Java / JS Developer in DataArt.  I want to share something useful with those who work with Salesforce. <br><br>  In Salesforce, the task often arises of massively creating / changing / deleting a group of related records in several objects, analogs of tables in a relational database.  For example, frequently used standard objects of the Account (information about the client‚Äôs company), Contact (information about the client itself).  The problem is that when you save a Contact record, you must specify the Id of the associated Account record, that is, the account must exist at the time of adding the contact record. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In reality, connections can be even more complicated, for example, the Opportunity object refers to both Account and Contact.  Plus, links to any non-standard (custom) objects are possible.  In any case, the record by reference must be created before the record that refers to it. <br><br>  Consider solutions to this problem: <br><a name="habracut"></a><br><h3>  <b>Anonymous APEX</b> </h3><br>  You need to prepare an APEX script, then execute it in the Salesforce Developer Console.  In the script, related objects are filled in sequence.  In the example below, the test Account is inserted, then Contact.  When inserting Contact, the Account Id of the Account is used, which is obtained after inserting the Account. <br><blockquote>  Account <font color="#009900">[</font> <font color="#009900">]</font> accounts <font color="#339933">;</font> <br>  accounts.  <font color="#006633">add</font> <font color="#009900">(</font> <font color="#000000">new</font> Account <font color="#009900">(</font> <br>  <font color="#003399">Name</font> <font color="#339933">=</font> 'test' <br>  <font color="#009900">)</font> <font color="#009900">)</font> <font color="#339933">;</font> <br>  insert accounts <font color="#339933">;</font> <br><br>  Contact <font color="#009900">[</font> <font color="#009900">]</font> contacts <font color="#339933">;</font> <br>  contacts.  <font color="#006633">add</font> <font color="#009900">(</font> <font color="#000000">new</font> Contact <font color="#009900">(</font> <br>  AccountId <font color="#339933">=</font> accounts <font color="#009900">[</font> <font color="#cc66cc">0</font> <font color="#009900">]</font> .  <font color="#006633">Id</font> , <br>  FirstName <font color="#339933">=</font> 'test', <br>  LastName <font color="#339933">=</font> 'test' <br>  <font color="#009900">)</font> <font color="#009900">)</font> <font color="#339933">;</font> <br>  insert contacts <font color="#339933">;</font> <br></blockquote><br>  Pros: <br><ul><li>  Developer Console is always at hand, nothing else needs to be installed and configured. </li><li>  The script is written in the language APEX, close Salesforce-developers. </li><li>  It is easy to implement simple logic. </li></ul><br><br>  Minuses: <br><ul><li>  Salesforce Limits allow no more than 200 entries to be modified in this way. </li><li>  It is difficult to implement complex logic. </li><li>  The method is not suitable for data migration from outside Salesforce, all data must be already loaded. </li></ul><br><br>  Thus, the method is only suitable for small, simple, manual changes. <br><br><h3>  <b>Batch APEX</b> </h3><br>  When you need to make changes to the set of records that are already inside Salesforce, you can use Batch APEX.  Unlike the previous one, this method allows you to process up to 10,000 records, according to Salesforce Limits.  Batch is a custom class inherited from Database.Batchable, written in APEX. <br><br>  You can run the class manually from the Developer Console: <br><blockquote>  Database.  <font color="#006633">Batchable</font> <font color="#339933">&lt;</font> sObject <font color="#339933">&gt;</font> batch <font color="#339933">=</font> <font color="#000000">new</font> myBatchClass <font color="#009900">(</font> <font color="#009900">)</font> <font color="#339933">;</font> <br>  Database.  <font color="#006633">executeBatch</font> <font color="#009900">(</font> batch <font color="#009900">)</font> <font color="#339933">;</font> </blockquote><br>  Or create a Job, with which the process will start at a specific time. <br><br>  Thus, the method is suitable for large-scale data changes within Salesforce, but it is very laborious.  When deploying from a sandbox to a production class, like any other APEX code, it should be covered with a unit test. <br><br><h3>  <b>Data loader</b> </h3><br>  Data Loader is a standard Salesforce utility installed locally.  Allows you to process up to 5 million records.  Migration with the Data Loader is the best practice and the most popular method of processing a large number of records.  Uploading / uploading records is done using the Salesforce API. <br><br>  The utility allows you to select an object in Salesforce and export the data to a CSV file.  And vice versa, download the object from CSV to Salesforce. <br><br>  Processing existing data in Salesforce is as follows: <br><ol><li>  Upload required data from Salesforce to CSV files. </li><li>  Modify data in CSV files. </li><li>  Uploading data to Salesforce from CSV files. </li></ol><br><br>  Point 2 here is a bottleneck that cannot be implemented by the Data Loader itself.  It is necessary to create third-party procedures for processing CSV files. <br><br>  As an example, in order to insert several contact records, the data must go to the associated Account and Contact.  The action algorithm should be as follows: <br><ol><li>  Prepare a CSV file with a list of new Account entries.  Download to Salesforce using Data Loader.  The result is a list of Account IDs. </li><li>  Prepare a CSV file with a list of new Contact entries.  In it in the field AccountId you must specify the ID from the list obtained in step 1.  This can be done manually, or use any programming language. </li><li>  Download the received CSV with the Contacts list to Salesforce. </li></ol><br><br>  Thus, the method is suitable for large-scale data changes, both within Salesforce, and using external data.  But it is very laborious, especially if it is necessary to modify the records. <br><br><h3>  <b>Pentaho data integration</b> </h3><br>  Pentaho Data Integration, also known as Kettle, is a universal ETL utility.  Not a specialized Salesforce utility.  The set contains Salesforce Input- and Output-connection methods, which allows Salesforce to transparently process data as data from other sources: relational databases, SAP, XML files, CSV, and others. <br><br>  With Salesforce, the utility works through the Salesforce API, so it is possible to process up to 5 million records, as with the Data Loader.  Only in a more convenient way. <br><br>  The main distinguishing feature is the graphical interface.  The whole transformation is divided into separate simple steps: read the data, sort, join, write data.  Steps are displayed in the form of pictograms between which arrows are drawn.  Thus, it is clearly seen that where it comes from and where it comes from. <br><br>  There are at least two versions of the utility: paid with guaranteed support and free.  The free Community Edition (Apache License v2.0) can be downloaded at <a href="http://community.pentaho.com/">http://community.pentaho.com/</a> . <br><br>  Development of the transformation in the simplest case does not require programming skills.  But if you want, you can use steps that include routines written in Java or JavaScript. <br><br>  Features of data migration using Pentaho Data Integration should be covered in more detail.  Here I will describe my experience and difficulties encountered. <br><br>  Connection parameters to Salesforce should be specified in the Transformation Properties parameters.  Once made settings will be available in all steps, where necessary, in the form of variables. <br>  I recommend indicating: <br><ul><li>  The URL to connect to Salesforce. <br>  For productions, including Salesforce Developer Edition: <a href="">https://www.salesforce.com/services/Soap/u/21.0</a> <br>  For sandbox test environments: <a href="">https://test.salesforce.com/services/Soap/u/21.0</a> <br>  The version of the API (in this case 21.0) is modified as necessary. </li><li>  Login.  The user must have sufficient rights to connect through the API.  Ideally, this should be a System Administrator. </li><li>  Password.  It is important that in addition to the password itself, you must specify Security Token. </li></ul><br>  For security reasons, you can leave the login and password fields empty, in which case the Data Integration will prompt them when the transformation is started. <br><br><img src="https://habrastorage.org/files/b16/118/529/b161185299af4c11b9ee7d78c13773e5.jpg"><br><br>  Data sampling is carried out by the Salesforce Input step.  In the settings of this step, you need to specify the connection parameters, in this case, the variables created earlier are used.  Also select an object and a list of fields to select, or specify a specific query using the SOQL query language (similar to the SQL query language used in relational databases). <br><br><img src="https://habrastorage.org/files/f06/9f9/cab/f069f9cab88a496a80436287ca4588f3.jpg"><br><br>  Data insertion is performed using one of several Output steps: <br><ul><li>  Salesforce Insert </li><li>  Salesforce update </li><li>  Salesforce Upsert (combines insert and update: if there is a record, it will be updated, otherwise a new one is inserted) </li></ul><br>  It is also possible to delete entries using <br><ul><li>  Salesforce Delete. </li></ul><br><br>  As in the input step, you need to specify the connection parameters, in this case variable transformations are used.  Here there is a more fine-tuning - the parameters of time-out connections, after which the transformation will fail.  And the Salesforce-specific parameter Batch Size is the number of records transferred in one transaction.  Increasing the Batch Size slightly increases the speed of the transformation, but cannot be more than 200 (according to Salesforce restrictions).  In addition, if there are triggers performing additional data processing after insertion, unstable operation with a large Batch Size value is possible.  The default is 10. <br><br><img src="https://habrastorage.org/files/91c/429/ee8/91c429ee82884b1d835067df05611bab.jpg"><br><br>  These two steps fully cover the capabilities of the Data Loader utility.  Everything in between is the logic of data processing.  And it can be implemented directly in Pentaho Data Integration. <br><br>  For example, one of the most requested steps is the joining of two data streams.  That join from SQL which so does not suffice in SOQL.  Here he is. <br>  In the settings, it is possible to select the type: Inner, Left Outer, Right Outer, Full Outer - and specify connection keys. <br><br><img src="https://habrastorage.org/files/4f9/88b/631/4f988b6318234baaa4fe0e76305bdd1d.jpg"><br><br>  Mandatory requirement - input data for this step must be sorted by key fields.  In Data Integration, a separate Sorter step is used for this. <br><br>  Sorting is done in RAM, however, it is possible that it is not enough and the data will be stored in an intermediate file on the disk.  Most sorter settings are associated with this case.  Ideally, you should avoid swapping to disk: this is ten times slower than sorting in memory.  To do this, you need to adjust the parameter Sort size - specify the upper limit of the number of lines that can theoretically pass through the sorter. <br><br>  Also in the settings you can select one or more key fields, which will be sorted in ascending or descending order of values.  If the field type is string, it makes sense to specify case sensitivity.  The Presorted parameter shows that the rows are already sorted by this field. <br><br><img src="https://habrastorage.org/files/0a1/1d4/a3e/0a11d4a3e70a459d97bbd2722cf2971e.jpg"><br><br>  Join and Sorter form a bundle that occurs in almost every transformation. <br><br><img src="https://habrastorage.org/files/ad9/c05/720/ad9c05720a744bf68060a1b9e1fde876.jpg"><br><br>  Sorting and joining have the most effect on transformation performance.  It is necessary to avoid unnecessary sorting, if the data is already sorted a few steps earlier and their order after did not change.  But you need to be careful: if in Join the data comes unsorted, Data Integration does not interrupt the work and does not show an error, just the result will be incorrect. <br><br>  As key fields you should always choose a short field.  Data Integration allows you to select several key fields for sorting and joining, but the processing speed is significantly reduced.  As a workaround, it is better to generate a surrogate key, as a result there will be only one field for the connection.  In the simplest case, a surrogate key can be obtained by concatenating strings.  For example, to connect to the fields FirstName, LastName is better to connect via FirstName + '' + LastName.  If you go further, from the resulting line you can calculate the hash (md5, sha2).  Unfortunately, there is no built-in step in Data Integration to calculate the string hash, you can write it yourself using the User Defined Java Class. <br><br>  In addition to the above steps, Data Integration includes many others.  These are filters, switch, union, steps for processing rows, lookup to relational tables and web services.  And many others.  As well as two universal steps, allowing to execute code in Java or JavaScript.  I will not dwell on them in detail. <br><br>  The unpleasant feature of Data Integration with Salesforce is the slow speed of inserting records through the Salesforce API.  About 50 records per second (as with the standard Data Loader, accessing the web service itself is a slow operation), which makes it difficult to process thousands of lines.  Fortunately, in Data Integration it is possible to organize insertion into several streams.  There is no standard solution, here's what I could come up with: <br><br><img src="https://habrastorage.org/files/6fc/6dd/d25/6fc6ddd257fb45719b75fb9cf59df8e8.jpg"><br><br>  Here the JavaScript procedure generates a random stream number.  Next step Switch distributes the streams according to its number.  Four separate Salesforce Insert steps insert records, thus increasing the overall flow rate to 200 records per second.  Ultimately, all inserted records with a filled ID field are saved to a CSV file. <br><br>  Using parallel insertion, you can speed up data processing.  But it‚Äôs not possible to produce threads infinitely: according to the limitations of Salesforce, maybe no more than 25 open connections from one user. <br><br>  The resulting transformation can be immediately run on the local machine.  Progress is displayed in Step Metrics.  Here you can see what steps are working, how many records were read in this step and passed on.  As well as the speed of processing records at a particular step, which makes it easy to find the bottleneck of the transformation. <br><br><img src="https://habrastorage.org/files/ba7/882/898/ba7882898dd9491a8b841fb71d5e1e10.jpg"><br><br>  For regular transformations, Data Integration allows you to create a job that runs on a condition or schedule on a local machine or a dedicated server. <br><br>  Thanks for attention.  I hope Salesforce-developers will take such a useful tool in service. </div><p>Source: <a href="https://habr.com/ru/post/258927/">https://habr.com/ru/post/258927/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../258915/index.html">Innovations in Cordova 5.0</a></li>
<li><a href="../258917/index.html">Oberon is dead, long live Oberon! Part 1. Some like more active</a></li>
<li><a href="../258919/index.html">Accepting bank card payments in iOS apps</a></li>
<li><a href="../258921/index.html">First day of PHDays V: from SMS interception to satellite hacking</a></li>
<li><a href="../258925/index.html">Smart Cities and Big Data</a></li>
<li><a href="../258929/index.html">10 Tips to Improve Email Support</a></li>
<li><a href="../258931/index.html">Website based on a single HTML page</a></li>
<li><a href="../258933/index.html">How I learned to make the world better in HeadHunter</a></li>
<li><a href="../258935/index.html">Inventory and rationalization: we manage software assets of large companies. Experience using Microsoft Assessment and Planning Toolkit</a></li>
<li><a href="../258937/index.html">The benefits of 3d printers for medicine and rocket science: desktop FDM versus industrial metal (can 500 zerlings overwhelm a dark templar?)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>