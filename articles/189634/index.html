<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>LSI Nytro MegaRAID NMR8100-4i in action</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The client formulated a task in HOSTKEY for me - for 1 (one) server with 12 cores and 64GB of memory, you need about 3-4TB of space for virtual virtua...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>LSI Nytro MegaRAID NMR8100-4i in action</h1><div class="post__text post__text-html js-mediator-article">  The client formulated a task in <a href="http://www.hostkey.ru/">HOSTKEY</a> for me - for 1 (one) server with 12 cores and 64GB of memory, you need about 3-4TB of space for virtual virtual machines of the same type, but that would work just like on an SSD and at the same time meet 16000r per month.  They began to think there were several options: <br><ul><li>  We do everything on the SSD.  we take 6 500GB disks of the type Samsung 840 Pro eMLC, collect them into a stripe or even as cunning, add a couple of 3TB disks for the backup and ... do not go through the budget.  2U server, 8 port RAID controller and 12000r drives per unit are not allowed. </li><li>  We do everything on hard drives, take 12x300Gb SAS 15K and again do not go through either in the budget or in performance. </li><li>  We use a RAID controller with an external SSD cache - LSI CacheCade or Adaptec MaxCache.  The idea is better, but we need 4 disks of 2 TB in RAID10 - we have to take an 8-port controller and a 120 GB SSB eMLC or SLC for it.  What would have been access to the SSD, again we must take a 2U case.  The controller 8 ports + SSD costs about 37000r, we do not pass into the budget. </li><li>  We use the new LSI Nytro MegaRAID NMR8100-4i.  We take the 1U system, put the controller in it, put 4 disks of 2 TB in it in RAID10 and fit into both the performance and the budget.  Nitra costs 27000r, it already has 2 44GB disks, and SLC! </li></ul><br>  No sooner said than done, about 3 working days and 4 options on our table.  Let's see what he is capable of - not in advertising brochures, but in practice. <br><img src="https://habrastorage.org/storage2/922/a1a/fd5/922a1afd5cdfe48357766aeb82562c65.jpg"><br><a name="habracut"></a><br><br><h4>  Machine assembly: </h4><br>  1U Supermicro, 2xE5-2630 2.3GHz 12 cores, 16Gb DDR3 ECC Reg LV x4 = 64GB of memory (can be expanded to 256GB if desired), LSI Nytro MegaRAID NMR8100-4i, 4x2Tb WDC RE 7200rpm HDD (WD2000FYYZ) SATA3.  All components on a 3 year warranty. <br>  Monthly price with accommodation and 100M channel - 16000r. <br><br><img src="https://habrastorage.org/storage2/ed8/125/e53/ed8125e53223cdf85a7930df7873de4f.jpg">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  Specification of our Nitra: </h4><br>  8xPCIe 3.0 (E5 and E3 only), 4 SAS / SATA 6Gbps ports, two 64GB SLC SSDs with redundancy of 30% of the space, 1GB of cache per DDR3, all possible RAID levels, support for expanders and a battery can be supplied.  Sample price at the time of August 2013 - 27000r, 3 years warranty. <br>  Who wants to read the <a href="http://thessdreview.com/our-reviews/lsi-nytro-megaraid-nmr-8120-4i-application-acceleration-card-review/">full review</a> - attention, a lot of letters in English. <br><br><h4>  Tests </h4><br>  Let's see what our Nitra is capable of.  Watch will be under Windows 2012 - the most obvious.  First, we configure the necessary arrays in BIOS and see the controller settings - everything is flexibly configured in the most traditional way.  The appearance of the utility has not changed for about 15 years. <br><br><img src="https://habrastorage.org/storage2/470/997/d13/470997d13f9872bffedbe81cb78ae080.jpg"><br><br><h4>  RAID </h4><br>  RAID is configured separately from a pair of embedded SSDs of 44GB, a separate logical volume from the remaining disks.  You can make a mirror or a stripe - choose a stripe, especially since the documentation clearly states that if the disk fails, the system will remove it from the file itself.  On a logical volume, you can do any RAID - from 0 to 6. Everything is configured traditionally. <br><br><img src="https://habrastorage.org/storage2/746/935/36c/74693536cd85507ea9fc8fee56ba1b3e.jpg"><br><br><h4>  Ready setup </h4><br><br><img src="https://habrastorage.org/storage2/fbd/732/126/fbd732126454e7f9940f2ad1fe689640.jpg"><br><br>  We‚Äôll load the Windows 2012 server, and the Nitra comes with a flash drive with Megaraid Storage Manager - finally.  We put it, everything is visible from the box.  The software is normal, also did not change for 10 years. Everything is native, in its place.  Understandable behavior in case of accidents. <br><br><img src="https://habrastorage.org/storage2/f64/fab/b97/f64fabb970913c8d46518085dc8ea27f.jpg"><br><br>  It is very important that now we see the percentage of SSD wear and we can change the controller in a timely manner and take preventive measures.  On older controllers, if the SSD was in an array, then getting SMART from it or something else was unrealistic. <br><br><img src="https://habrastorage.org/storage2/ca4/3da/ba3/ca43daba3c6a7f00643a51cc3f71a889.jpg"><br><br><h4>  Run block device </h4><br>  We are compiling a 4TB volume - RAID10 and without marking it we are hitting HD Tune Pro to check for dry performance without the help of the OS and its caches. <br><br><h5>  Reading: </h5><br><br><img src="https://habrastorage.org/storage2/1be/4f3/3c1/1be4f33c1e190200f19f06f1462640a4.jpg"><br><br><img src="https://habrastorage.org/storage2/e1b/b94/707/e1bb9470731e46e9736083ea45a63ab2.jpg"><br><br>  A read run on the entire disk capacity gives the expected result - 200-250MB per second, about 100 IOPS, 10ms.  The cache is clearly empty - everything comes from the disk.  Not much for 10 RAID, should be thicker.  But we have another task, virtualization.  Reading performance is usually leveled by OS caches. <br><br><img src="https://habrastorage.org/storage2/744/a6e/057/744a6e0572a868aa361c66d42b7cf4f8.jpg"><br>  IOPS on random reads - nothing magical, all operations past the SSD - everything is predictably slow, as it should be from a pair of disks. <br><br><h5>  Record: </h5><br><br><img src="https://habrastorage.org/storage2/4c4/1f2/e5f/4c41f2e5f3aec1db4965c344648084ba.jpg"><br><br><img src="https://habrastorage.org/storage2/be7/210/68b/be721068b4d6b8b46149bfdc86961f36.jpg"><br><br>  here that is necessary, on average 266 Mb / s and access 0.17 ms.  Nitra writes to the SSD and then from it in large blocks to the disks.  In the beginning, 1GB of memory of the controller was clogged, then the 3Tb system had enough SSD supply and only the last 1TB was written directly to the disks without a cache, the disks did not have time. <br><br>  Let's see what is with the record - everything is gorgeous here.  In one turn, we see the full-time performance of the SSD - all the records go through it, the disks do not participate. <br><br><img src="https://habrastorage.org/storage2/bdd/81c/ba2/bdd81cba23842f661bb846cc964861f2.jpg"><br><br>  We have 14,000 IOPS on small blocks with a typical access time of 0.07 ms.  Fine!  If we commit transactions from the database, they will crash to disk instantly.  If the virtual machine decided to pop it up or put something on the disk, it will not affect the disk subsystem. <br><br><h4>  File system run </h4><br>  Now we create the file system and see how the OS will work with files and, in general, with the marked place.  All standard tests throughout are not particularly different from the block device tests.  Let's set a file on 10 GB and we will chase it. <br><br>  First run, the file is not yet in cache.  The first gig on the recording fails through memory at speeds of about 3 Gb / s, then it slows down to 1000 Mb / s, after 50 GB it probably does not fit into the SSD and starts writing to 250 Mb / s disks. <br><br><img src="https://habrastorage.org/storage2/112/d23/deb/112d23deb75d30abc047b9f93a906072.jpg"><br><br>  IOPS values ‚Äã‚Äãare removed from the SSD cache - one thread 4000 for reading and 10,000 for writing, great!  In 32 threads - 52000 for reading and 23000 for writing, apparently 32 threads are overkill.  Files of 512MB in size fall into the RAM cache of the controller and are distributed from there at a PCIe speed of 2.5 GB per second. <br><br><img src="https://habrastorage.org/storage2/43b/416/372/43b416372dafa3ff3f636b8318551bc0.jpg"><br><br>  The repetition of the operation shows visually how a file is distributed from a pair of SSDs - a flat shelf of 1000 MB per second - faster than RAID0 on a pair of integrated SSDs does not accelerate.  IOPS is not affected. <br><br><h4>  Linux </h4><br><br>  Let's try to boot in the fresh Tsenthos and check if our Nitra is visible without a tambourine.  The answer is correct, LSI is true for itself - everything can be seen immediately and without prompts. <br><br><img src="https://habrastorage.org/storage2/d4f/5eb/662/d4f5eb662eb7eab0303fe7ca14b9cc48.jpg"><br><br>  Let's test the regular benchmark, the first time is a dull picture, the second time is much more fun!  An average reading of 426 MB / s with a delay of about 10 ms, is permissible when testing the surface at 4 TB. <br><br><img src="https://habrastorage.org/storage2/0d0/4c6/3f5/0d04c63f583d4aa46aec5a6157c99836.jpg"><br><br>  We try as earlier LSI cards see our RAID10 - I took 4 different controllers such as LSI 9260 and Intel OEM - none of them could load the array configuration.  Perhaps 9266 will be able to read it, but there were none on hand.  In general, it is necessary to keep this in mind when forming the SPTA. <br><br><h4>  What should Nitra be used for? </h4><br><ul><li>  <b>For virtualization:</b> there are not so many moving fragments in VM disks.  The statistics on our cloud-based VPS nodes shows that each read operation accounts for 2-3 write operations that reach the disk subsystem.  During the day, 10% of the used space is overwritten and re-read from the strength of 5-6% of the total capacity.  To measure your metrics elementary - run iostat for 24 hours and you will learn everything about yourself. </li><li>  <b>For databases:</b> If the database does not fit entirely into memory, Nitra will be salvation.  Everything important with indexes will be on the SSD and in memory, all unimportant of the voluminous directories will be on the disk.  All write transactions will be in place instantly, there will be nothing left. </li></ul><br><h5>  What exactly should not be used: </h5><br><ul><li>  large media archives, video sites, streaming, backups and backup storage.  There will be no benefit - the cache will not work or will be too weak.  For websites, too, there is no special meaning - everything that is particularly important is perfectly placed in memory and distributed from there, and not important at the block level will not be cached. </li><li>  For ZFS and other file systems that want to see the disks themselves.  The cache will not understand the logic of the file system and will disappear. </li><li>  Under video surveillance, non-linear editing - long linear reads and writes is not about Nitra. </li></ul><br><h5>  Total </h5><br>  In conclusion, I can say this - the <a href="http://www.lsi.com/products/storagecomponents/Pages/NytroMegaRAIDNMR8100-4i.aspx">Nytro MegaRAID NMR 8100-4i</a> is available for order at <a href="http://www.hostkey.ru/">HOSTKEY</a> at a price of 1500r per month to the price of an ordinary 4-port RAID controller in Moscow and +30 Euro to custom servers in the Netherlands.  It is more profitable and more reliable to use pure SSD to build file systems and more reliably than building software filers with in-memory caches. <br><br>  Nitra is an excellent support for large database servers and intracorporate virtualization, when you can or need to do without a global storage system.  4-8 SAS / SATA drives + Nitra = SSD performance without SSD and RAID reliability. <br><br>  On the basis of Nitra, we will speed up the work of our virtualization cluster in the Netherlands in the very near future and I think we will significantly increase reliability and performance, which I will accomplish further. <br><br>  Author - CEO <a href="http://www.hostkey.ru/">HOSTKEY</a> </div><p>Source: <a href="https://habr.com/ru/post/189634/">https://habr.com/ru/post/189634/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../189618/index.html">Representation of numbers by the sum of two squares and elliptic curves</a></li>
<li><a href="../189624/index.html">First impressions: Windows Server 2012 R2 Hyper-V in all its glory</a></li>
<li><a href="../189626/index.html">Expert evaluation methods</a></li>
<li><a href="../189628/index.html">Nokia HERE LivingCities</a></li>
<li><a href="../189630/index.html">Development methodology on 1C-Bitrix - the experience of a fool</a></li>
<li><a href="../189644/index.html">The presumption of guilt on the Internet. When you need to prove that you are not a camel</a></li>
<li><a href="../189646/index.html">GALAXY S4 Active - a smartphone for a better life</a></li>
<li><a href="../189650/index.html">Do not panic (translation of the chapter in the book Passionate Programmer by Chad Fowler)</a></li>
<li><a href="../189652/index.html">An open letter to the ROI and the FID on a petition to cancel the 187-FZ (# Law against Internet)</a></li>
<li><a href="../189656/index.html">Implement last visited / recent pages in Django</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>