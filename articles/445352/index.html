<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Spark Structured Streaming Applications on Kubernetes. Experience FASTEN RUS</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Today I will tell you how we managed to solve the problem of porting Spark Structured Streaming Applications to Kubernetes (K8s) and implement CI stre...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Spark Structured Streaming Applications on Kubernetes. Experience FASTEN RUS</h1><div class="post__text post__text-html js-mediator-article">  Today I will tell you how we managed to solve the problem of porting <a href="https://spark.apache.org/docs/2.3.0/structured-streaming-programming-guide.html">Spark Structured Streaming Applications</a> to <a href="https://kubernetes.io/">Kubernetes</a> (K8s) and implement CI streaming. <br><a name="habracut"></a><br><h4>  <i><b>How it all began?</b></i> </h4><br>  Streaming is a key component of the FASTEN RUS BI platform.  Real-time data is used by the date analysis team to build operational reports. <br><br>  Streaming applications are implemented using <a href="https://spark.apache.org/docs/2.3.0/structured-streaming-programming-guide.html">Spark Structured Streaming</a> .  This framework provides a convenient API for data transformation, which meets our needs in terms of speed of refinement. <br><br>  The streams themselves went up on the <a href="https://aws.amazon.com/ru/emr/">AWS EMR</a> cluster.  Thus, when raising a new stream to the cluster, an ssh script was laid out on the Spark-Jobs submission, after which the application was launched.  And at first everything seemed to suit us.  But with the growing number of streams, the need for CI streaming implementation became more and more obvious, which would increase the autonomy of the analysis date command when launching applications to deliver data to new entities. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      And now we will look at how we managed to solve this problem by porting streaming to Kubernetes. <br><br><h4>  <i><b>Why Kubernetes?</b></i> </h4><br>  Kubernetes as a resource manager has been most responsive to our needs.  This is a warm-up without a downtime, and a wide range of CI implementation tools on Kubernetes, including Helm.  In addition, our team had sufficient expertise in implementing the CI pipelines on the K8s.  Therefore, the choice was obvious. <br><br><h4>  <i><b>How is the Kubernetes-based Spark application management model organized?</b></i> </h4><br><img src="https://habrastorage.org/webt/ms/rz/fv/msrzfvb4_7eqjzokg2cuvplcerm.png"><br><br>  Client launches spark-submit on K8s.  A pod application driver is created.  Kubernetes Scheduler binds pod to a cluster node.  The driver then sends a request to create pod's for running executors, scams are created and bound to the cluster nodes.  After that, a standard set of operations is performed with subsequent conversion of the application code into the DAG, decomposition into stages, breakdown into tasks and their execution on executors. <br><br>  This model works quite successfully with the manual start of Spark-applications.  However, the approach to running spark-submit outside the cluster did not suit us in terms of implementing CI.  It was necessary to find a solution that would allow Spark-jobs to be run (perform spark-submit) directly on the nodes of the cluster.  And here our model was fully met by the model Kubernetes Operator. <br><br><h4>  <i><b>Kubernetes Operator as a lifecycle management model Spark applications</b></i> </h4><br>  <a href="https://coreos.com/operators/">Kubernetes Operator</a> - the concept of managing statefull applications in Kubernetes, proposed by <a href="https://coreos.com/">CoreOS</a> , which involves the automation of operational tasks, such as deploying applications, restarting applications in the case of files, and updating application configurations.  One of the key patterns of Kubernetes Operator is CRD ( <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">CustomResourceDefinitions</a> ), which involves adding custom resources to the K8s cluster, which, in turn, allows you to work with these resources as native Kubernetes objects. <br><br>  Operator is a demon that lives in the cluster pod and responds to the creation / change of the state of a custom resource. <br><br>  Consider this concept in relation to the management of the life cycle of Spark-applications. <br><br><img src="https://habrastorage.org/webt/an/ei/gt/aneigtq-0jc8fhtryimgh3orfaw.png"><br><br>  The user executes the kubectl apply -f spark-application.yaml command, where spark-application.yaml is the Spark application specification.  Operator receives a Spark application object and executes spark-submit. <br><br>  As we can see, the Kubernetes Operator model involves managing the lifecycle of a Spark application directly in the Kubernetes cluster, which was a serious argument for this model in the context of solving our problems. <br><br>  As Kubernetes Operator for controlling streaming applications, it was decided to use a <a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator">spark-on-k8s-operator</a> .  This operator offers a fairly convenient API, and also has the flexibility to configure the Spark application restart policy (which is quite important in the context of support for streaming applications). <br><br><h4>  <i><b>CI implementation</b></i> </h4><br>  To implement CI streaming, <a href="https://docs.gitlab.com/ee/ci/">GitLab CI / CD was used</a> .  Spark applications are deployed on K8s using <a href="https://helm.sh/">Helm</a> . <br><br>  Pipeline itself involves 2 stages: <br><br><ul><li>  test - checks the syntax and renders the Helm-templates; </li><li>  deploy - deploying streaming applications to test (dev) and product (prod) environments. </li></ul><br>  Let us consider these stages in more detail. <br><br>  At the test stage, the Helm-template of Spark-applications (CRD - <a href="">SparkApplication</a> ) is <a href="">rendered</a> with media-specific values. <br><br>  The key sections of the Helm-template are: <br><ol><li>  spark: <br><ul><li>  version - Apache Spark version </li><li>  image - used Docker image </li></ul></li><li>  nodeSelector - contains a list (key ‚Üí value) corresponding to the labels of the pods. </li><li>  tolerations - specifies the list of Spark-application tolerance. </li><li>  mainClass - Spark application class </li><li>  applicationFile - the local path where the Spark application jar is located </li><li>  restartPolicy - Spark application restarts policy <br><ul><li>  Never - the completed Spark application is not restarted. </li><li>  Always - the completed Spark application is restarted regardless of the reason for the stop. </li><li>  OnFailure - Spark-application is restarted only in the case of file </li></ul></li><li>  maxSubmissionRetries - the maximum number of submit Spark-applications </li><li>  driver / executor: <br><ul><li>  cores - the number of cores allocated to the driver / executor </li><li>  instances (used only for configuration of executors) - the number of executors </li><li>  memory - the amount of memory allocated to the process driver / executor </li><li>  memoryOverhead - the amount of off-heap memory allocated to the driver / executor </li></ul></li><li>  streams: <br><ul><li>  name - the name of the streaming application </li><li>  arguments - arguments of the streaming application </li></ul></li><li>  sink - the path to datasets Data Lake on S3 </li></ol><br>  After the application template has been rendered, it is applied to the dev test environment using Helm. <br><br>  Worked out CI pipeline. <br><br><img src="https://habrastorage.org/webt/ah/za/br/ahzabrhmjpduar2y7ug3xzn90lu.png"><br><br>  After that, run the job deploy-prod - launch applications in production. <br><br>  We are convinced of the successful implementation of Joba. <br><br><img src="https://habrastorage.org/webt/md/-h/0e/md-h0e0u3mwccnncq7t2qqzaf5i.png"><br><br>  As we can see below, applications are running, while they are running in the RUNNING status. <br><br><img src="https://habrastorage.org/webt/ce/4i/cc/ce4iccuv7vkzintbor0tyjtae88.png"><br><br><h4>  <i><b>Conclusion</b></i> </h4><br>  The porting of Spark Structured Streaming Applications to K8s and the subsequent implementation of CI made it possible to automate the launch of streams for the delivery of data on new entities.  To raise the next stream, it is enough to prepare a Merge Request with a description of the configuration of the Spark application in the yaml-file of values ‚Äã‚Äãand when launching the deploy-prod data, data delivery to DWH ( <a href="https://aws.amazon.com/ru/redshift/">Redshift</a> ) / Data Lake (S3) is initiated.  This solution ensured the autonomy of the date analysis command when performing tasks related to adding new entities to the repository.  In addition, streaming porting to K8s and, in particular, management of Spark-applications using Kubernetes Operator spark-on-k8s-operator significantly increased the resiliency of streaming.  But about this in the next article. </div><p>Source: <a href="https://habr.com/ru/post/445352/">https://habr.com/ru/post/445352/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../445340/index.html">Increase network security by using a cloud analyzer</a></li>
<li><a href="../445344/index.html">OpenVox Unified Communications Platform</a></li>
<li><a href="../445346/index.html">How to write a bad API</a></li>
<li><a href="../445348/index.html">SNA Hackathon 2019: we complicate architecture - we simplify features</a></li>
<li><a href="../445350/index.html">Sonata - SIP provisioning server</a></li>
<li><a href="../445354/index.html">Finding objects in the pictures</a></li>
<li><a href="../445356/index.html">Overview of the Mobile section on the DUMP-2019: maximum application and useful in everyday work</a></li>
<li><a href="../445358/index.html">Organization of the event system in Unity - through the eyes of game designer</a></li>
<li><a href="../445360/index.html">5 typical tasks for interviews on JavaScript: analysis and solutions</a></li>
<li><a href="../445362/index.html">The book "Distributed Systems. Design Patterns ¬ª</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>