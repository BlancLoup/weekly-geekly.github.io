<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Save data and faith in humanity: large migration of ElasticSearch cluster</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this article, I continue to share my field experience with a logging system based on Heka and ElasticSearch. 


 This time the story goes about dat...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Save data and faith in humanity: large migration of ElasticSearch cluster</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/web/26c/f33/1d7/26cf331d7cd742e69e400595925410b0.png"></p><br><p>  <em>In this article, I continue to share my field experience with a logging system based on Heka and ElasticSearch.</em> </p><br><p>  This time the story goes <strong>about data migration between two clusters of ElasticSearch 2.2 and 5.2.2</strong> , which cost me a lot of nerves.  After all, 24 billion records were to be transported without breaking the already running system. </p><br><p>  <a href="https://habrahabr.ru/post/328018/">The last article</a> ended on the fact that the system is working, the logs are received and added to the ElasticSearch cluster, their viewing is available in real time through Kibana.  But the cluster was originally assembled with a significant margin from memory just for growth. <a name="habracut"></a></p><br><p>  If you refer to the <a href="https://www.elastic.co/guide/en/elasticsearch/guide/master/heap-sizing.html">official documentation of</a> ElasticSearch (hereinafter referred to simply as ES), then first of all you will see the strict warning ‚ÄúDon't cross 32 gb‚Äù.  The excess threatens to slump performance up to moments of complete standstill while the garbage collector performs a rebuild in the spirit of "stop the world".  The manufacturer's recommendation for memory on the server: 32 GB under heap (xms / xmx) and another 32 GB of free space under the cache.  Total 64 GB of physical memory per data-node. </p><br><p>  But what if there is more memory?  The official answer is all in the same documentation - put multiple copies of ES on the same host.  But such an approach seemed to me to be not quite correct, since there are no standard means for this.  Duplicate init scripts is the last century, so cluster virtualization with the placement of nodes in LXD containers seemed more interesting. </p><br><blockquote>  LXD (Linux Container Daemon) is the so-called ‚Äúcontainer legcaster‚Äù.  Unlike the "heavy" hypervisors, it does not contain hardware emulation, which allows to reduce the overhead of virtualization.  In addition, it has an advanced REST API, flexible configuration of the resources used, the ability to transfer containers between hosts, and other features more typical of classic virtualization systems. </blockquote><p><img src="https://habrastorage.org/web/02e/50b/647/02e50b6471024e379fb7f967a8ae3f6c.png"></p><br><p>  <em>This is the structure of the future cluster.</em> </p><br><p>  By the beginning of the work at hand was the following iron: </p><br><ul><li><p>  Four working ES data nodes in the old cluster: Intel Xeon 2x E5-2640 v3;  512 GB of RAM, 3x16TB of RAID-10. </p><br></li><li>  Two new empty servers similar to the previous item configuration. </li></ul><br><p>  As planned, <strong>each</strong> physical server will have two ES data nodes, a master node and a client node.  In addition, the server will house the log container with HAProxy and Heka pool installed to service the data nodes of this physical server. </p><br><h1 id="podgotovka-novogo-klastera">  Preparing a new cluster </h1><br><p>  First of all, you need to release one of the data-nodes - this server immediately goes to a new cluster.  The load on the remaining three will increase by 30%, but they will cope, which is confirmed by the download statistics for the last month.  Moreover, this is not for long.  Next, I give my sequence of actions for the regular output of the data node from the cluster. </p><br><p>  We remove the load from the fourth data node by prohibiting the placement of new indexes on it: </p><br><pre><code class="bash hljs">{ <span class="hljs-string"><span class="hljs-string">"transient"</span></span>: { <span class="hljs-string"><span class="hljs-string">"cluster.routing.allocation.exclude._host"</span></span>: <span class="hljs-string"><span class="hljs-string">"log-data4"</span></span> } }</code> </pre> <br><p>  Now we turn off the automatic rebalancing of the cluster during the migration in order not to create an extra load on the remaining data nodes: </p><br><pre> <code class="bash hljs">{ <span class="hljs-string"><span class="hljs-string">"transient"</span></span>: { <span class="hljs-string"><span class="hljs-string">"cluster.routing.rebalance.enable"</span></span>: <span class="hljs-string"><span class="hljs-string">"none"</span></span> } }</code> </pre> <br><p>  We compile a list of indices from the released data node, divide it into three equal parts and start moving shards to the remaining data nodes as follows (for each index and shard): </p><br><pre> <code class="bash hljs">PUT _cluster/reroute { <span class="hljs-string"><span class="hljs-string">"commands"</span></span> : [ { <span class="hljs-string"><span class="hljs-string">"move"</span></span> : { <span class="hljs-string"><span class="hljs-string">"index"</span></span> : <span class="hljs-string"><span class="hljs-string">"service-log-2017.04.25"</span></span>, <span class="hljs-string"><span class="hljs-string">"shard"</span></span> : 0, <span class="hljs-string"><span class="hljs-string">"from_node"</span></span> : <span class="hljs-string"><span class="hljs-string">"log-data4"</span></span>, <span class="hljs-string"><span class="hljs-string">"to_node"</span></span> : <span class="hljs-string"><span class="hljs-string">"log-data1"</span></span> } } }</code> </pre> <br><p>  When the transfer is completed, turn off the vacated node and do not forget to return the rebalancing back: </p><br><pre> <code class="bash hljs">{ <span class="hljs-string"><span class="hljs-string">"transient"</span></span>: { <span class="hljs-string"><span class="hljs-string">"cluster.routing.rebalance.enable"</span></span>: <span class="hljs-string"><span class="hljs-string">"all"</span></span> } }</code> </pre> <br><p>  If the network and the load on the cluster allow, then to speed up the process, you can increase the queue of simultaneously moving shards (by default, this number is equal to two) </p><br><pre> <code class="bash hljs">{ <span class="hljs-string"><span class="hljs-string">"transient"</span></span>: { <span class="hljs-string"><span class="hljs-string">"cluster"</span></span>: { <span class="hljs-string"><span class="hljs-string">"routing"</span></span>: { <span class="hljs-string"><span class="hljs-string">"allocation"</span></span>: { <span class="hljs-string"><span class="hljs-string">"cluster_concurrent_rebalance"</span></span>: <span class="hljs-string"><span class="hljs-string">"10"</span></span> } } } } }</code> </pre> <br><p>  While the old cluster gradually comes to life, we collect on the three existing servers a new one based on ElasticSearch 5.2.2, with separate LXD containers for each node.  The point is simple and well described in the documentation, so I omit the details.  If anything - ask in the comments, I will tell in detail. </p><br><p>  During the setup of the new cluster, I allocated the memory as follows: </p><br><ul><li><p>  Master nodes: 4 GB </p><br></li><li><p>  Client nodes: 8 GB </p><br></li><li><p>  Data nodes: 32 GB </p><br></li><li>  XMS everywhere is set to XMX. </li></ul><br><p>  This distribution was born after thinking about the documentation, viewing the statistics of the old cluster and applying common sense. </p><br><h1 id="sinhroniziruem-klastery">  Sync clusters </h1><br><p>  So, we have two clusters: </p><br><ol><li><p>  Old - three data nodes, each on an iron server. </p><br></li><li>  New, with six data nodes in LXD containers, two per server. </li></ol><br><p>  The first thing we do is enable traffic mirroring in both clusters.  At the Heka receiving pools (refer to the <a href="https://habrahabr.ru/company/yamoney/blog/328018/">previous article in the</a> cycle for a detailed description), we add another Output section for each processed service: </p><br><pre> <code class="bash hljs">[Service1Output_Mirror] <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> = <span class="hljs-string"><span class="hljs-string">"ElasticSearchOutput"</span></span> message_matcher = <span class="hljs-string"><span class="hljs-string">"Logger == 'money-service1''"</span></span> server = <span class="hljs-string"><span class="hljs-string">"http://newcluster.receiver:9200"</span></span> encoder = <span class="hljs-string"><span class="hljs-string">"Service1Encoder"</span></span> use_buffering = <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br><p>  After that, traffic will go in parallel to both clusters.  Considering that we store indexes with operational logs of components for no more than 21 days, we could stop at that.  After 21 days in the clusters will be the same data, and the old can be disabled and parsed.  But so long and so boring to wait.  Therefore, we proceed to the last and most interesting stage - the migration of data between clusters. </p><br><h1 id="perenos-indeksov-mezhdu-klasterami">  Transfer indexes between clusters </h1><br><p>  Since there is no official procedure for migrating data between ES clusters at the time of the project, and I don‚Äôt want to reinvent crutches - we use Logstash.  Unlike Heka, he can not only write data to ES, but also read it from there. </p><br><blockquote>  Judging by the comments on the <a href="https://habrahabr.ru/company/yamoney/blog/328018/">previous article</a> , many have formed the opinion that for some reason I do not like Logstash.  But after all, each tool is designed for its own tasks, and Logstash is the best fit for migration between clusters. </blockquote><p>  During the migration, it is useful to increase the size of the memory buffer for the indices from 10% by default to 40%, which are selected by the average amount of free memory on the working ES data-nodes.  You also need to turn off the update of the indexes on each data node, for which we add the following parameters to the date-node configuration: </p><br><pre> <code class="bash hljs">memory.index_buffer_size: 40% index.refresh_interval: -1</code> </pre> <br><p>  By default, the index is updated every second and thus creates an extra load.  Therefore, while none of the users is watching the new cluster, the update can be disabled.  At the same time, I created a default template for the new cluster, which will be used when generating new indexes: </p><br><pre> <code class="bash hljs">{ <span class="hljs-string"><span class="hljs-string">"default"</span></span>: { <span class="hljs-string"><span class="hljs-string">"order"</span></span>: 0, <span class="hljs-string"><span class="hljs-string">"template"</span></span>: <span class="hljs-string"><span class="hljs-string">"*"</span></span>, <span class="hljs-string"><span class="hljs-string">"settings"</span></span>: { <span class="hljs-string"><span class="hljs-string">"index"</span></span>: { <span class="hljs-string"><span class="hljs-string">"number_of_shards"</span></span>: <span class="hljs-string"><span class="hljs-string">"6"</span></span>, <span class="hljs-string"><span class="hljs-string">"number_of_replicas"</span></span>: <span class="hljs-string"><span class="hljs-string">"0"</span></span> } } } }</code> </pre> <br><p>  Using the template, we turn off replication for the duration of the migration, thereby reducing the load on the disk system. </p><br><p>  For Logstash we have the following configuration: </p><br><pre> <code class="bash hljs">input { elasticsearch { hosts =&gt; [ <span class="hljs-string"><span class="hljs-string">"localhost:9200"</span></span> ] index =&gt; <span class="hljs-string"><span class="hljs-string">"index_name"</span></span> size =&gt; 5000 docinfo =&gt; <span class="hljs-literal"><span class="hljs-literal">true</span></span> query =&gt; <span class="hljs-string"><span class="hljs-string">'{ "query": { "match_all": {} }, "sort": [ "@timestamp" ] }'</span></span>} } output { elasticsearch { hosts =&gt; [ <span class="hljs-string"><span class="hljs-string">"log-new-data1:9200"</span></span> ] index =&gt; <span class="hljs-string"><span class="hljs-string">"%{[@metadata][_index]}"</span></span> document_type =&gt; <span class="hljs-string"><span class="hljs-string">"%{[@metadata][_type]}"</span></span> document_id =&gt; <span class="hljs-string"><span class="hljs-string">"%{[@metadata][_id]}"</span></span>}} }</code> </pre> <br><p>  In the input section, we describe the source of the data, indicate to the system that the data needs to be taken in batches (bulk) of 5000 records, and select all records sorted by timestamp. </p><br><p>  In output, you must specify the destination for transferring the received data.  Pay attention to the descriptions of the following fields, which can be obtained from the old indexes: </p><br><ul><li><p>  <strong>document_type</strong> - the type (mapping) of the document, which is better to indicate when moving, so that the names of the created mappings in the new cluster coincide with the names in the old one - they are used in saved queries and dashboards. </p><br></li><li>  <strong>document_id</strong> - internal identifier of the entry in the index, which is a unique 20-character hash.  With its explicit transfer, two tasks are solved: firstly, we ease the load on the new cluster without requiring to generate an id for each of the billions of records, and secondly, if the process is interrupted, there is no need to delete the under-downloaded index, you can simply start the process again, and ES ignores entries with a matching id. </li></ul><br><p>  Logstash startup options: </p><br><pre> <code class="bash hljs">/usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/migrate.conf --pipeline.workers 8</code> </pre> <br><p>  The key parameters affecting the speed of migration are the size of the packs that Logstash will send to ES, and the number of simultaneously running processes (pipeline.workers) for processing.  There are no strict rules that would determine the choice of these values ‚Äã‚Äã- they were selected experimentally by the following method: </p><br><ul><li><p>  We choose a small index: for tests we used an index with 1 million multi-line (this is important) records. </p><br></li><li><p>  We start migration of this index by means of Logstash. </p><br></li><li><p>  We look at the <strong>thread_pool</strong> on the receiving data node, paying attention to the number of "rejected" records.  The growth of this value clearly indicates that ES does not have time to index incoming data - then the number of parallel processes Logstash should be reduced. </p><br></li><li>  If a sharp increase in ‚Äúrejected‚Äù records does not occur, we increase the number of bulk / workers and repeat the process. </li></ul><br><p>  After everything was prepared, lists of indexes for relocation were compiled, configurations were written, and warnings were sent about upcoming loads to the network infrastructure and monitoring departments, I started the process. </p><br><p>  In order not to sit and not restart the logstash process, after completing the migration of the next index, I did the following with the new configuration file: </p><br><ul><li><p>  The list of indexes for the move was divided into three approximately equal parts. </p><br></li><li><p>  In /etc/logstash/conf.d/migrate.conf left only the static part of the configuration: </p><br><pre> <code class="bash hljs">input { elasticsearch { hosts =&gt; [ <span class="hljs-string"><span class="hljs-string">"localhost:9200"</span></span> ] size =&gt; 5000 docinfo =&gt; <span class="hljs-literal"><span class="hljs-literal">true</span></span> query =&gt; <span class="hljs-string"><span class="hljs-string">'{ "query": { "match_all": {} }, "sort": [ "@timestamp" ] }'</span></span>} } output { elasticsearch { hosts =&gt; [ <span class="hljs-string"><span class="hljs-string">"log-new-data1:9200"</span></span> ] index =&gt; <span class="hljs-string"><span class="hljs-string">"%{[@metadata][_index]}"</span></span> document_type =&gt; <span class="hljs-string"><span class="hljs-string">"%{[@metadata][_type]}"</span></span> document_id =&gt; <span class="hljs-string"><span class="hljs-string">"%{[@metadata][_id]}"</span></span>}} }</code> </pre> <br></li><li><p>  I compiled a script that reads the names of the indexes from the file and invokes the logstash process, dynamically substituting the index name and node address for migration. </p><br></li><li>  All you need to run three instances of the script, one for each file: <strong>indices.to.move.0.txt</strong> , <strong>indices.to.move.1.txt</strong> and <strong>indices.to.move.2.txt</strong> .  After that, the data goes to the first, third and fifth data nodes. </li></ul><br><p>  The code for one of the script instances: </p><br><pre> <code class="bash hljs">cat /tmp/indices_to_move.0.txt | <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-variable"><span class="hljs-variable">$line</span></span> &gt; /tmp/0.txt &amp;&amp; /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/migrate.conf --pipeline.workers 8 --config.string <span class="hljs-string"><span class="hljs-string">"input {elasticsearch { index =&gt; \"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$line</span></span></span><span class="hljs-string">\" }} output { elasticsearch { hosts =&gt; [ \"log-new-data1:9200\" ] }}"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">done</span></span>;</code> </pre> <br><p>  To view the migration status, I had to assemble another script ‚Äúon my lap‚Äù and run the <strong>screen</strong> in a separate process (via <em>watch -d -n 60)</em> : </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash regex=$(cat /tmp/?.txt) regex="(($regex))" regex=$(echo $regex | sed 's/ /)|(/g') curl -s localhost:9200/_cat/indices?h=index,docs.count,docs.deleted,store.size | grep -P $regex |sort &gt; /tmp/indices.local curl -s log-new-data1:9200/_cat/indices?h=index,docs.count,docs.deleted,store.size | grep -P$regex | sort &gt; /tmp/indices.remote echo -e "index\t\t\tcount.source\tcount.dest\tremaining\tdeleted\tsource.gb\tdest.gb" diff --side-by-side --suppress-common-lines /tmp/indices.local /tmp/indices.remote | awk '{print $1"\t"$2"\t"$7"\t"$2-$7"\t"$8"\t"$4"\t\t"$9}'</span></span></code> </pre> <br><p>  The migration process took about a week.  And to be honest, I slept this week restlessly. </p><br><h1 id="posle-pereezda">  After moving </h1><br><p>  After the transfer of the indexes, it remains to do quite a bit.  One fine Saturday night, the old cluster was turned off and the DNS records were changed.  Therefore, all those who came to work on Monday saw the new pink-blue interface of the fifth Kibana.  While the staff got used to the updated color scheme and studied the new features, I continued to work. </p><br><p>  From the old cluster I took another server that had become vacant and put two containers on it with ES data nodes for the new cluster.  All the rest of the iron went to the reserve. </p><br><p>  The final structure turned out exactly what was planned on the first scheme: </p><br><ul><li><p>  Three master nodes. </p><br></li><li><p>  Three client nodes. </p><br></li><li><p>  Eight data nodes (two per server). </p><br></li><li>  Four log-receiver (HAProxy + Heka Pools, one for each server). </li></ul><br><p>  We transfer the cluster to production mode - we return the buffer parameters and the index update intervals: </p><br><pre> <code class="bash hljs">memory.index_buffer_size: 10% index.refresh_interval: 1s</code> </pre> <br><p>  The cluster quorum (taking into account three master nodes) is set to two: </p><br><pre> <code class="bash hljs">discovery.zen.minimum_master_nodes: 2</code> </pre> <br><p>  Next, we need to return the shard values, taking into account that we already have eight date-nodes: </p><br><pre> <code class="bash hljs">{ <span class="hljs-string"><span class="hljs-string">"default"</span></span>: { <span class="hljs-string"><span class="hljs-string">"order"</span></span>: 0, <span class="hljs-string"><span class="hljs-string">"template"</span></span>: <span class="hljs-string"><span class="hljs-string">"*"</span></span>, <span class="hljs-string"><span class="hljs-string">"settings"</span></span>: { <span class="hljs-string"><span class="hljs-string">"index"</span></span>: { <span class="hljs-string"><span class="hljs-string">"number_of_shards"</span></span>: <span class="hljs-string"><span class="hljs-string">"8"</span></span>, <span class="hljs-string"><span class="hljs-string">"number_of_replicas"</span></span>: <span class="hljs-string"><span class="hljs-string">"1"</span></span> } } } }</code> </pre> <br><p>  Finally, we select the right moment (all employees went home) and restart the cluster. </p><br><h1 id="nashardit-no-ne-smeshivat">  Nashardit, but do not mix </h1><br><p>  In this section, I want to focus on reducing the overall reliability of the system, which occurs when placing multiple ES data-nodes in the same iron server, and indeed with any virtualization. </p><br><p><img src="https://habrastorage.org/web/807/a98/955/807a989550b04394b5be6085be369f6f.png"></p><br><p>  <em>From the point of view of the ES cluster, everything is fine: the index is divided into shards by the number of data nodes, each shard has a replica, the primary and replica shards are stored on different nodes.</em> </p><br><p>  The sharding and replication system in ES increases both the speed and reliability of data storage.  But this system was designed taking into account the placement of one ES node on one server, when in case of problems with equipment only one ES data node is lost.  In the case of our cluster, <strong>two will fall</strong> .  Even taking into account the equal division of indices between all nodes and the presence of a replica for each shard, there is a situation when primary and replica of the same shard appear on two adjacent data nodes of the same physical server. </p><br><p>  Therefore, ES developers have proposed a tool to control the placement of the shard within a single cluster - <a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.4/allocation-awareness.html">Shard Allocation Awareness</a> (SAA).  This tool allows when placing a shard to operate not with data nodes, but with more global structures like servers with LXD containers. </p><br><p>  In the settings of <strong>each</strong> data node, you need to place the ES attribute that describes the physical server on which it is located: </p><br><pre> <code class="bash hljs">node.attr.rack_id: <span class="hljs-built_in"><span class="hljs-built_in">log</span></span>-lxd-host-N</code> </pre> <br><p>  Now you need to reload the nodes to apply the new attributes, and add the following code to the cluster configuration: </p><br><pre> <code class="bash hljs">{ <span class="hljs-string"><span class="hljs-string">"persistent"</span></span>: { <span class="hljs-string"><span class="hljs-string">"cluster"</span></span>: { <span class="hljs-string"><span class="hljs-string">"routing"</span></span>: { <span class="hljs-string"><span class="hljs-string">"allocation"</span></span>: { <span class="hljs-string"><span class="hljs-string">"awareness"</span></span>: { <span class="hljs-string"><span class="hljs-string">"attributes"</span></span>: <span class="hljs-string"><span class="hljs-string">"rack_id"</span></span> } } } } } }</code> </pre> <br><p>  And only in this order, because after enabling SAA, the cluster <strong>will not</strong> place shards on nodes without the specified attribute. </p><br><p>  By the way, a similar mechanism can be used for several attributes.  For example, if the cluster is located in several data centers and you do not want to move the shards back and forth between them.  In this case, the already familiar settings will look like this: </p><br><pre> <code class="bash hljs">node.attr.rack_id: <span class="hljs-built_in"><span class="hljs-built_in">log</span></span>-lxd-hostN node.attr.dc_id: datacenter_name</code> </pre> <br><pre> <code class="bash hljs">{ <span class="hljs-string"><span class="hljs-string">"persistent"</span></span>: { <span class="hljs-string"><span class="hljs-string">"cluster"</span></span>: { <span class="hljs-string"><span class="hljs-string">"routing"</span></span>: { <span class="hljs-string"><span class="hljs-string">"allocation"</span></span>: { <span class="hljs-string"><span class="hljs-string">"awareness"</span></span>: { <span class="hljs-string"><span class="hljs-string">"attributes"</span></span>: <span class="hljs-string"><span class="hljs-string">"rack_id, dc_id"</span></span> } } } } } }</code> </pre> <br><p>  It would seem that everything in this section is obvious.  But it is the obvious that takes off from the head first of all, so check it out separately - then after the move it will not be excruciatingly painful. </p><br><p>  The next article in the series will be devoted to two of my favorite topics ‚Äî monitoring and tuning an already built system.  <strong>Be sure to write in the comments if something from the already written or planned is particularly interesting and raises questions</strong> . </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/330952/">https://habr.com/ru/post/330952/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../330942/index.html">The story of how cognitive technologies help preserve karma</a></li>
<li><a href="../330944/index.html">[Administrator's abstract] Domains, addresses and Windows: mix, but do not shake</a></li>
<li><a href="../330946/index.html">‚ÄúWhen a critical crash occurs with databases, it always happens somewhat epically‚Äù - Ilya Kosmodemyansky</a></li>
<li><a href="../330948/index.html">How to ensure that learning in games is not annoying</a></li>
<li><a href="../330950/index.html">Online conference DEV Labs JavaScript. June 24</a></li>
<li><a href="../330956/index.html">Monitoring Nginx Plus in Zabbix</a></li>
<li><a href="../330958/index.html">Interface design of corporate BI tool for data mining</a></li>
<li><a href="../330960/index.html">AgeHack - the first online hackathon to extend life on the MLBootCamp platform</a></li>
<li><a href="../330964/index.html">FSTEC gives "good"</a></li>
<li><a href="../330966/index.html">Southeast Asian countries are switching to a hybrid storage model</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>