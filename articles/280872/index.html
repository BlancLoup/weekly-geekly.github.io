<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Active / Passive PostgreSQL Cluster using Pacemaker, Corosync</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Description 
 This article describes an example of setting up an Active / Passive cluster for PostgreSQL using Pacemaker, Corosync. The disk subsystem...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Active / Passive PostgreSQL Cluster using Pacemaker, Corosync</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/cb2/776/a29/cb2776a2971117d6517936e78f696e38.jpg" alt="image"><br><br><h5>  Description </h5><br>  This article describes an example of setting up an Active / Passive cluster for PostgreSQL using Pacemaker, Corosync.  The disk subsystem is a disk from a storage system (CSV).  The solution resembles Microsoft's Windows Failover Cluster. <br><br>  Technical details: <br>  <i>Operating System Version - CentOS 7.1</i> <i><br></i>  <i>The version of the package pacemaker - 1.1.13-10</i> <i><br></i>  <i>Package Version pcs - 0.9.143</i> <i><br></i>  <i>PostgreSQL Version 9.4.6</i> <i><br></i>  <i>As servers (2pcs) - iron servers 2 * 12 CPU / 94GB memory</i> <i><br></i>  <i>As CSV (Cluster Shared Volume) - Mid-Range Hitachi RAID 1 + 0 array</i> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h5>  Preparing Cluster Nodes </h5><br><a name="habracut"></a>  We edit / etc / hosts on both hosts and make each other's hosts visible by short names, for example: <br><pre><code class="bash hljs">[root@node1 ~]<span class="hljs-comment"><span class="hljs-comment"># cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 10.1.66.23 node1.local.lan node1 10.1.66.24 node2.local.lan node2</span></span></code> </pre> <br><br>  We also do the exchange between the servers via SSH keys and scatter the keys between the hosts. <br><br>  After that, you need to make sure that both servers see each other by short names: <br><pre> <code class="bash hljs">[root@node1 ~]<span class="hljs-comment"><span class="hljs-comment"># ping node2 PING node2.local.lan (10.1.66.24) 56(84) bytes of data. 64 bytes from node2.local.lan (10.1.66.24): icmp_seq=1 ttl=64 time=0.204 ms 64 bytes from node2.local.lan (10.1.66.24): icmp_seq=2 ttl=64 time=0.221 ms 64 bytes from node2.local.lan (10.1.66.24): icmp_seq=3 ttl=64 time=0.202 ms 64 bytes from node2.local.lan (10.1.66.24): icmp_seq=4 ttl=64 time=0.207 ms [root@node2 ~]# ping node1 PING node1.local.lan (10.1.66.23) 56(84) bytes of data. 64 bytes from node1.local.lan (10.1.66.23): icmp_seq=1 ttl=64 time=0.202 ms 64 bytes from node1.local.lan (10.1.66.23): icmp_seq=2 ttl=64 time=0.218 ms 64 bytes from node1.local.lan (10.1.66.23): icmp_seq=3 ttl=64 time=0.186 ms 64 bytes from node1.local.lan (10.1.66.23): icmp_seq=4 ttl=64 time=0.193 ms</span></span></code> </pre><br><br>  Installing Cluster Creation Packages <br>  We put the necessary packages on both hosts in order to build the cluster: <br><pre> <code class="bash hljs">yum install -y pacemaker pcs psmisc policycoreutils-python</code> </pre><br><br>  Then we start and turn on the pcs service: <br><pre> <code class="bash hljs">systemctl start pcsd.service systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> pcsd.service</code> </pre><br><br>  To manage the cluster, we need a special user, create it on both hosts: <br><pre> <code class="bash hljs">passwd hacluster Changing password <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> user hacluster. New password: Retype new password: passwd: all authentication tokens updated successfully. Pacemaker|Corosync</code> </pre><br>  To verify the authentication, from the first node, the following command must be executed: <br><pre> <code class="bash hljs">[root@node1 ~]<span class="hljs-comment"><span class="hljs-comment"># pcs cluster auth node1 node2 Username: hacluster Password: node1: Authorized node2: Authorized</span></span></code> </pre><br><br>  Next, we start our cluster and check the launch status: <br><pre> <code class="bash hljs">pcs property <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> stonith-enabled=<span class="hljs-literal"><span class="hljs-literal">false</span></span> pcs property <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> no-quorum-policy=ignore pcs cluster start --all pcs status --all</code> </pre><br><br>  The conclusion about the state of the cluster should be something like this: <br><br><pre> <code class="bash hljs">[root@node1 ~]<span class="hljs-comment"><span class="hljs-comment"># pcs status Cluster name: cluster_web WARNING: no stonith devices and stonith-enabled is not false Last updated: Tue Mar 16 10:11:29 2016 Last change: Tue Mar 16 10:12:47 2016 Stack: corosync Current DC: node2 (version 1.1.13-10.el7_2.2-44eb2dd) - partition with quorum 2 Nodes configured 0 Resources configured Online: [ node1 node2 ] Full list of resources: PCSD Status: node1: Online node2: Online Daemon Status: corosync: active/disabled pacemaker: active/disabled pcsd: active/enabled</span></span></code> </pre><br><cut><br><br>  Now we are going to configure the resources in the cluster. <br><br><h5>  CSV Setup </h5><br><br>  Go to the first host and configure LVM: <br><pre> <code class="bash hljs">pvcreate /dev/sdb vgcreate shared_vg /dev/sdb lvcreate -l 100%FREE -n ha_lv shared_vg mkfs.ext4 /dev/shared_vg/ha_lv</code> </pre><br><br>  The disk is ready.  Now we need to make it so that the auto-mount rule for LVM was not applied to the disk.  This is done by making changes in the /etc/lvm/lvm.conf file (activation section) on both hosts: <br><br><pre> <code class="bash hljs">activation {..... <span class="hljs-comment"><span class="hljs-comment">#volume_list = [ "vg1", "vg2/lvol1", "@tag1", "@*" ] volume_list = [ "centos", "@node1" ]</span></span></code> </pre><br><br>  Update initrams and reboot nodes: <br><pre> <code class="bash hljs">dracut -H -f /boot/initramfs-$(uname -r).img $(uname -r) shutdown -h now</code> </pre><br>  Adding resources to the cluster <br>  Now you need to create a group of resources in the cluster - a disk with the file system and IP. <br><br><pre> <code class="bash hljs">pcs resource create virtual_ip IPaddr2 ip=10.1.66.25 cidr_netmask=24 --group PGCLUSTER pcs resource create DATA ocf:heartbeat:LVM volgrpname=shared_vg exclusive=<span class="hljs-literal"><span class="hljs-literal">true</span></span> --group PGCLUSTER pcs resource create DATA_FS Filesystem device=<span class="hljs-string"><span class="hljs-string">"/dev/shared_vg/ha_lv"</span></span> directory=<span class="hljs-string"><span class="hljs-string">"/data"</span></span> fstype=<span class="hljs-string"><span class="hljs-string">"ext4"</span></span> force_unmount=<span class="hljs-string"><span class="hljs-string">"true"</span></span> fast_stop=<span class="hljs-string"><span class="hljs-string">"1"</span></span> --group PGCLUSTER pcs resource create pgsql pgsql pgctl=<span class="hljs-string"><span class="hljs-string">"/usr/pgsql-9.4/bin/pg_ctl"</span></span> psql=<span class="hljs-string"><span class="hljs-string">"/usr/pgsql-9.4/bin/psql"</span></span> pgdata=<span class="hljs-string"><span class="hljs-string">"/data"</span></span> pgport=<span class="hljs-string"><span class="hljs-string">"5432"</span></span> pgdba=<span class="hljs-string"><span class="hljs-string">"postgres"</span></span> node_list=<span class="hljs-string"><span class="hljs-string">"node1 node2"</span></span> op start timeout=<span class="hljs-string"><span class="hljs-string">"60s"</span></span> interval=<span class="hljs-string"><span class="hljs-string">"0s"</span></span> on-fail=<span class="hljs-string"><span class="hljs-string">"restart"</span></span> op monitor timeout=<span class="hljs-string"><span class="hljs-string">"60s"</span></span> interval=<span class="hljs-string"><span class="hljs-string">"4s"</span></span> on-fail=<span class="hljs-string"><span class="hljs-string">"restart"</span></span> op promote timeout=<span class="hljs-string"><span class="hljs-string">"60s"</span></span> interval=<span class="hljs-string"><span class="hljs-string">"0s"</span></span> on-fail=<span class="hljs-string"><span class="hljs-string">"restart"</span></span> op demote timeout=<span class="hljs-string"><span class="hljs-string">"60s"</span></span> interval=<span class="hljs-string"><span class="hljs-string">"0s"</span></span> on-fail=<span class="hljs-string"><span class="hljs-string">"stop"</span></span> op stop timeout=<span class="hljs-string"><span class="hljs-string">"60s"</span></span> interval=<span class="hljs-string"><span class="hljs-string">"0s"</span></span> on-fail=<span class="hljs-string"><span class="hljs-string">"block"</span></span> op notify timeout=<span class="hljs-string"><span class="hljs-string">"60s"</span></span> interval=<span class="hljs-string"><span class="hljs-string">"0s"</span></span> --group PGCLUSTER</code> </pre><br><br>  Please note that all resources have the same group. <br>  You should also remember to correct the cluster's dafault parameters: <br><pre> <code class="bash hljs">failure-timeout=60s migration-threshold=1</code> </pre><br><br>  In the end, you should see something like this: <br><br><pre> <code class="bash hljs">[root@node1 ~]<span class="hljs-comment"><span class="hljs-comment"># pcs status Cluster name: cluster_web Last updated: Mon Apr 4 14:23:34 2016 Last change: Thu Mar 31 12:51:03 2016 by root via cibadmin on node2 Stack: corosync Current DC: node2 (version 1.1.13-10.el7_2.2-44eb2dd) - partition with quorum 2 nodes and 4 resources configured Online: [ node1 node2 ] Full list of resources: Resource Group: PGCLUSTER DATA (ocf::heartbeat:LVM): Started node2 DATA_FS (ocf::heartbeat:Filesystem): Started node2 virtual_ip (ocf::heartbeat:IPaddr2): Started node2 pgsql (ocf::heartbeat:pgsql): Started node2 PCSD Status: node1: Online node2: Online Daemon Status: corosync: active/disabled pacemaker: active/disabled pcsd: active/enabled</span></span></code> </pre><br><br>  Check the status of the PostgreSQL service, on the host where the resource group: <br><br><pre> <code class="bash hljs">[root@node2~]<span class="hljs-comment"><span class="hljs-comment"># ps -ef | grep postgres postgres 4183 1 0 Mar31 ? 00:00:51 /usr/pgsql-9.4/bin/postgres -D /data -c config_file=/data/postgresql.conf postgres 4204 4183 0 Mar31 ? 00:00:00 postgres: logger process postgres 4206 4183 0 Mar31 ? 00:00:00 postgres: checkpointer process postgres 4207 4183 0 Mar31 ? 00:00:02 postgres: writer process postgres 4208 4183 0 Mar31 ? 00:00:02 postgres: wal writer process postgres 4209 4183 0 Mar31 ? 00:00:09 postgres: autovacuum launcher process postgres 4210 4183 0 Mar31 ? 00:00:36 postgres: stats collector process root 16926 30749 0 16:41 pts/0 00:00:00 grep --color=auto postgres</span></span></code> </pre><br><br><h5>  We check the performance </h5><br><br>  We simulate a drop in service on node2 and see what happens: <br><pre> <code class="bash hljs">[root@node2 ~]<span class="hljs-comment"><span class="hljs-comment"># pcs resource debug-stop pgsql Operation stop for pgsql (ocf:heartbeat:pgsql) returned 0 &gt; stderr: ERROR: waiting for server to shut down....Terminated &gt; stderr: INFO: PostgreSQL is down</span></span></code> </pre><br><br>  Checking status on node1: <br><br><pre> <code class="bash hljs">[root@node1 ~]<span class="hljs-comment"><span class="hljs-comment"># pcs status Cluster name: cluster_web Last updated: Mon Apr 4 16:51:59 2016 Last change: Thu Mar 31 12:51:03 2016 by root via cibadmin on node2 Stack: corosync Current DC: node2 (version 1.1.13-10.el7_2.2-44eb2dd) - partition with quorum 2 nodes and 4 resources configured Online: [ node1 node2 ] Full list of resources: Resource Group: PGCLUSTER DATA (ocf::heartbeat:LVM): Started node1 DATA_FS (ocf::heartbeat:Filesystem): Started node1 virtual_ip (ocf::heartbeat:IPaddr2): Started node1 pgsql (ocf::heartbeat:pgsql): Started node1 Failed Actions: * pgsql_monitor_4000 on node2 'not running' (7): call=48, status=complete, exitreason='none', last-rc-change='Mon Apr 4 16:51:11 2016', queued=0ms, exec=0ms PCSD Status: node1: Online node2: Online Daemon Status: corosync: active/disabled pacemaker: active/disabled pcsd: active/enabled</span></span></code> </pre><br><br>  As we can see, the service already feels great on node1. <br><br>  ToDO: make dependencies on resources within a group ... <br><br>  Literature: <br>  <a href="http://clusterlabs.org/">clusterlabs.org</a> </cut></div><p>Source: <a href="https://habr.com/ru/post/280872/">https://habr.com/ru/post/280872/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../280862/index.html">The most interesting of the vSphere 6 arsenal</a></li>
<li><a href="../280864/index.html">The near future of cloud technology and virtualization</a></li>
<li><a href="../280866/index.html">Firewalls Palo Alto Networks VM-Series</a></li>
<li><a href="../280868/index.html">IaaS-digest: 30 materials on the topic of virtualization</a></li>
<li><a href="../280870/index.html">Dokkur - the first PaaS from Russia</a></li>
<li><a href="../280875/index.html">Look at the post: Satya Nadella opens the Microsoft Envision conference.</a></li>
<li><a href="../280878/index.html">Digital security certificate: what is it for?</a></li>
<li><a href="../280880/index.html">According to Rambler.iOS # 6</a></li>
<li><a href="../280882/index.html">Rust through its founding principles</a></li>
<li><a href="../280884/index.html">4.04</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>