<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Scaling Zabbix</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Those who use or intend to use Zabbix on an industrial scale are always worried about the question: how much real data can Zabbiks ‚Äúdigest‚Äù before it ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Scaling Zabbix</h1><div class="post__text post__text-html js-mediator-article"><img align="left" src="https://habrastorage.org/storage3/24d/d65/0e0/24dd650e01d209be1138182b54f28d44.png" alt="Zabbix logo">  Those who use or intend to use Zabbix on an industrial scale are always worried about the question: how much real data can Zabbiks ‚Äúdigest‚Äù before it finally chokes and chokes?  Part of my recent work was about this issue.  The fact is that I have a huge network with more than 32,000 nodes and which can potentially be fully monitored by Zabbiks in the future.  The forum has long been discussing how to optimize Zabbix for work on a large scale, but, unfortunately, I still could not find a complete solution. <br><br>  In this article I want to show how I set up my system, capable of processing really a lot of data. <a name="habracut"></a>  So that you understand what this is about, here is just a picture of the system statistics: <br><br><img src="https://habrastorage.org/storage3/3a8/a6d/919/3a8a6d91935ccdd0528c692301c524d1.png" alt="image">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      To begin with, I would like to discuss what the item ‚ÄúRequired server performance, new values ‚Äã‚Äãper second (further NVPS) (Required response speed per second)‚Äù really means.  So, it does not correspond to how much real data gets into the system per second, but is a simple mathematical calculation of all active data elements taking into account polling intervals.  And then it turns out that Zabbix-trapper is not involved in the calculation.  In our network, trapper has been used quite actively, so let's see how much NVPS is really in the environment under consideration: <br><br><img src="https://habrastorage.org/storage3/c0a/038/cb9/c0a038cb96c971cc2a51e7450fe7a14f.png" alt="image"><br><br>  As shown in the graph, on average Zabbix processes about <b>9260</b> requests per second.  In addition, there were short bursts in the network up to <b>15000 NVPS</b> , which the server coped with without any problems.  Honestly, it's great! <br><br><h4>  Architecture </h4><br>  The first thing to understand is the architecture of the monitoring system.  Should Zabbix be fault tolerant?  Will one or two hours of downtime matter?  What are the consequences if the database falls?  What disks will be required for the base, and which RAID setup?  What bandwidth is needed between a Zabbix server and Zabbix proxy?  What is the maximum delay?  How to collect data?  Interrogate the network (passive monitoring) or listen to the network (active monitoring)? <br><br>  Let's look at each question in detail.  To be honest, I did not consider the network issue when deploying the system, which led to problems that were later difficult to diagnose.  So, here is a general scheme of the monitoring system architecture: <br><br><img src="https://habrastorage.org/storage3/fb0/780/273/fb0780273e15bbd59b880bb381bfb557.png" alt="image"><br><br><h4>  Iron </h4><br>  Accurately select the correct iron process is not easy.  The main thing that I did here was using SAN for data storage, since Zabbix‚Äôs database requires a lot of I / O disk system.  Simply put, the faster the disks at the database server, the more data Zabbiks can process. <br><br>  Of course, CPU and memory are also very important for MySQL.  A large amount of RAM allows Zabbiks to store frequently read data in memory, which naturally contributes to the speed of the system.  Initially, I planned for a database server with 64GB of memory, but everything works fine on 32GB so far. <br><br>  The servers on which the zabbix_server itself is installed should also have sufficiently fast CPUs, since it is necessary for it to calmly process hundreds of thousands of triggers.  12GB would be enough for the memory as there are not so many processes on the Zabbiks server itself (almost all monitoring goes through a proxy). <br><br>  Unlike the DBMS and zabbix_server, Zabbix proxies do not require serious hardware, so I used ‚Äúvirtual machines‚Äù.  Basically, active data elements are collected, so that proxies serve as data collection points, but they themselves practically do not interrogate anything. <br><br>  Here is the summary table that I used in my system: <br><table><tbody><tr><th>  Zabbix server </th><th>  Zabbix DB </th><th>  Zabbix proxies </th><th>  SAN </th></tr><tr><td>  HP ProLiant BL460c Gen8 <br>  12x Intel Xeon E5-2630 <br>  16GB memory <br>  128GB disk <br>  CentOS 6.2 x64 <br>  Zabbix 2.0.6 <br></td><td>  HP ProLiant BL460c Gen8 <br>  12x Intel Xeon E5-2630 <br>  32GB memory <br>  2TB SAN-backed storage (4Gbps FC) <br>  CentOS 6.2 x64 <br>  MySQL 5.6.12 <br></td><td>  VMware Virtual Machine <br>  4x vCPU <br>  8GB memory <br>  50GB disk <br>  CentOS 6.2 x64 <br>  Zabbix 2.0.6 <br>  MySQL 5.5.18 <br></td><td>  Hitachi Unified Storage VM <br>  2x 2TB LUN <br>  Tiered storage (with 2TB SSD) <br></td></tr></tbody></table><br><h4>  Fault tolerance Zabbix server </h4><br>  Let us return to the architectural issues that I voiced above.  In large networks, for obvious reasons, not working monitoring is a real disaster.  However, the Zabbiks architecture does not allow running more than one instance of the zabbix server process. <br><br>  So I decided to use Linux HA with Pacemaker and CMAN.  For basic configuration, please look at the <a href="http://clusterlabs.org/quickstart-redhat.html">RedHat 6.4 manual</a> .  Unfortunately, the instruction was changed from the moment I used it, but the end result should be the same.  After the basic configuration, I further configured: <br><br><ol><li>  Shared IP address (shared IP address) <br><ol><li>  In the case of a faylover, the IP address goes to the server that becomes active </li><li>  Since the common IP address is always used by the active Zabbix server, there are three advantages from here: <br><ul><li>  It's always easy to find which server is active. </li><li>  All connections from Zabbix server are always from the same IP (After setting the SourceIP = parameter in <a href="https://www.zabbix.com/documentation/2.0/manual/appendix/config/zabbix_server">zabbix_server.conf</a> ) </li><li>  All Zabbix proxies and Zabbix agents simply specify the total IP as the server. </li></ul><br></li></ol></li><li>  The zabbix_server process <br><ul><li>  in the case of a faylover, zabbix_server will be stopped on the old server and run on the new </li></ul><br></li><li>  Symlink for cron jobs <br><ol><li>  The symlink indicates the directory in which tasks are to be executed only on the active Zabbix server.  Crontab must have access to all tasks through this symlink. </li><li>  In the case of a faylover, the symlink is deleted on the old server and created on the new </li></ol></li><li>  crond <br><ul><li>  In the case of a faylover, the crond stops on the old server and runs on the new active server. </li></ul><br></li></ol>  Sample configuration file, as well as LSB init-script for zabbix-server can be downloaded <a href="">here</a> .  Do not forget to edit the parameters enclosed in the "&lt;&gt;".  In addition, the init script is written taking into account that all Zabbix files are in the same folder (/ usr / local / zabbix).  So correct the paths in the script if necessary. <br><br><h4>  Fault tolerance of the DBMS </h4><br>  Obviously, there is no benefit from the fault tolerance of servers with Zabbix-servers, if the database can fall at any time.  For MySQL there are a huge number of ways to create a cluster, I will tell you about the method that I used. <br><br>  I also used Linux HA with Pacemaker and CMAN and for the database.  As it turned out, it has a couple of excellent features for managing MySQL replication.  I use (used, see the ‚Äúopen problems‚Äù section) replication to synchronize data between the active (master) and standby (slave) MySQL.  For a start, just like for Zabbix server servers, we do the basic cluster configuration.  Then in the addition I set up: <br><br><ol><li>  Shared IP address (shared IP address) <br><ol><li>  In the case of a faylover, the IP address goes to the server that becomes active </li><li>  Since the common IP address is always used by the active Zabbix server, there are two advantages from here: <br><ul><li>  It's always easy to find which server is active. </li><li>  In the case of a file server, no action is required on the Zabbix server itself to indicate the address of the new active MySQL server. </li></ul><br></li></ol></li><li>  Common Slave IP Address <br><ol><li>  This IP address can be used when a read request to the database occurs.  Thus, the request can process the MySQL slave server, if available. </li><li>  any of the servers may have an additional address, it depends on the following: <br><ul><li>  if the slave server is available and the clock does not lag behind for more than 60 seconds, then the address will be with it </li><li>  In the opposite case, the address will be at the MySQL master server. </li></ul></li></ol></li><li>  mysqld <br><ul><li>  In the case of a faylover, the new MySQL server will become active.  If after that the old server comes back in operation, it will remain a slave for the already newly-made master. </li></ul></li></ol><br>  An example of the configuration file can be found <a href="http://blog.zabbix.com/wp-content/uploads/2013/07/pacemaker_mysql.txt">here</a> .  Do not forget to edit the pacemaker parameters enclosed in "&lt;&gt;".  You may also need to download another MySQL resource agent for use with pacemaker.  The link can be found in the MySQL cluster installation documentation with the pacemaker in the Percona github <a href="">repository</a> .  Also for every "fire case" copy is <a href="">here</a> . <br><br><h4>  Zabbix proxy </h4><br>  If for some reason you have not heard about Zabbix-proxy, then please urgently look in the <a href="https://www.zabbix.com/documentation/2.0/manual/distributed_monitoring/proxies">documentation</a> .  Proxies allow Zabbiks to distribute the monitoring load across multiple machines.  After that, each Zabbiks proxy already sends all collected data to the Zabbiks server. <br><br>  Working with Zabbiks proxy it is important to remember: <br><br><ol><li>  Zabbiks proxies are capable of handling very large amounts of data if they are properly configured.  For example, during tests, a proxy (let's call it Proxy A) processed 1500-1750 NVPS without any problems.  And this is a virtual machine with two virtual CPUs, 4GB of RAM and a SQLite3 database.  In this case, the proxy was on the same site with the server itself, so that the network latency could simply be ignored.  Also, almost everything that was collected was the active data elements of the Zabbiks agent. </li><li>  I mentioned earlier how important network latency is in monitoring.  So, this is true when it comes to large systems.  In fact, the amount of data that a proxy can send, without lagging behind, is directly dependent on the network. <br><br>  The graph below clearly shows how problems accumulate when network latency is not taken into account.  A proxy that does not have time: </li></ol><br><img src="https://habrastorage.org/storage3/f31/d85/44c/f31d8544cc0a8e12abd1a3f09672d3e8.png" alt="image"><br><br>  It seems clear enough that the queue of data for transmission should not increase.  The schedule refers to another Zabbiks-proxy (Proxy B), which is no different from Proxy A by hardware, but can transfer without problems only 500NVPS and not 1500NVPS, like Proxy A. The difference is that B is in Singapore and server in North America, and the delay between sites about 230ms.  This delay has a serious effect, given the way data is sent.  In our case, Proxy B can only send 1000 collected items to Zabbiks server every 2-3 seconds.  According to my observations, this is what happens: <br><br><ul><li>  Proxy connects to server </li><li>  Proxy maximum sends 1000 collected data item values ‚Äã‚Äãat a time </li><li>  Proxy closes connection </li></ul><br>  This procedure repeats as many times as required.  In the case of a large delay, this method has several serious problems: <br><br><ul><li>  Primary connection is very slow.  In my case, it occurs in 0.25 seconds.  Phew! </li><li>  Since the connection is closed after sending 1000 data elements, the TCP connection never lasts long enough to have time to use all the available bandwidth. </li></ul><br><h4>  Database performance </h4><br>  High database performance is key to the monitoring system, since absolutely all collected information goes there.  At the same time, given the large number of write operations to the database, disk performance is the first bottleneck you encounter.  I was lucky and I had SSD disks at my disposal, but still this is not a guarantee of fast base operation.  Here is an example: <br><br><ul><li>  Initially, I used MySQL 5.5.18 on the system.  At first, there were no visible performance problems, however, after 700-750, NVPS MySQL started to load the processor 100% and the system literally froze.  My further attempts to rectify the situation, twisting the parameters in the configuration file, activating large pages or partitioning, did not lead to anything.  My wife suggested a better solution: first upgrade to MySQL 5.6 and then figure it out.  To my surprise, a simple update solved all performance problems, which I could not win in 5.5.18.  Just in case, here is a copy of <a href="http://blog.zabbix.com/wp-content/uploads/2013/07/large_environment_mysql_config.txt">my.cnf</a> . </li></ul><br>  The graph shows the number of requests per second in the database: <br><img src="https://habrastorage.org/storage3/f81/ef6/7b9/f81ef67b9f973362e571567ded27ca94.png" alt="image"><br><br>  Note that the most ‚ÄúCom_update‚Äù requests.  The reason lies in the fact that each value obtained results in an Update to the ‚Äúitems‚Äù table.  Also, the database is basically write operations, so the MySQL query cache does not help.  In fact, it may even be detrimental to performance, given that it will constantly have to mark requests as invalid. <br><br>  Another issue for performance can be Zabbix Housekeeper.  In large networks, it is strongly recommended to disable.  To do this, set DisableHousekeeping = 1 in the config file.  It is clear that without Housekeeping, old data (data elements, events, actions) will not be deleted from the database.  Then removal can be organized through partitioning. <br><br>  However, one of the limitations of MySQL 5.6.12 is that partitioning cannot be used in tables with foreign keys and they are just present almost everywhere in the Zabbiks database.  But besides the history tables that we need.  Partitioning gives us two advantages: <br><br><ol><li>  All historical data tables are divided by day / week / month / etc.  may be in separate files, which allows you to further delete the data without any consequences for the database.  It is also very easy to understand how much data is collected for a certain period of time. </li><li>  After clearing the tables, InnoDB does not return space to the disk, keeping it for new data.  As a result, with InnoDB it is impossible to clear disk space.  In the case of partitioning, this is not a problem; space can be freed up by simply deleting old partitions. </li></ol><br>  About partitioning in Zabbiks it was already <a href="http://habrahabr.ru/post/120955/">written</a> on Habr√©. <br><br><h4>  Collect or listen </h4><br>  In Zabbiks, there are two methods for collecting data: active and passive: In the case of passive monitoring, Zabbiks the server itself polls the agents for Zabbiks, and in the case of the active one waits for Zabbix agents to connect to the server themselves.  <a href="https://www.zabbix.com/documentation/2.0/manual/config/items/itemtypes/trapper">Zabbix trapper</a> also falls under the active monitoring, since the initiation of sending remains on the side of the network node. <br><br>  The difference in performance can be serious when choosing one or another method as the main one.  Passive monitoring requires running processes on the Zabbiks server, which will regularly send a request to the Zabbiks agent and wait for a response, in some cases, waiting can drag on even up to a few seconds.  Now multiply this time by at least a thousand servers, and it becomes clear that the ‚Äúpolling‚Äù can take time. <br><br>  In the case of active monitoring of the polling processes, there is no server, and the server is in a state of waiting, when the agents themselves will start connecting to the Zabbix server in order to get a list of the data elements that need to be monitored. <br><br>  Further, the agent will start collecting data items based on the interval received from the server and send them; the connection will be opened only when the agent has something to send.  Thus, there is no need for verification prior to receiving data, which is present during passive monitoring.  Conclusion: active monitoring increases the speed of data collection, which is required in our large network. <br><br><h4>  Monitoring of Zabbiks himself </h4><br>  Without monitoring Zabbix itself, the effective operation of a large system is simply not possible - it is critically important to understand where the ‚Äúplug‚Äù will occur when the system refuses to accept new data.  Existing data elements for monitoring Zabbiks can be found <a href="https://www.zabbix.com/documentation/2.0/manual/config/items/itemtypes/internal">here</a> .  In versions 2.x of Zabbiks, they were kindly assembled into a template for monitoring Zabbix server, provided out of the box.  Use! <br><br>  One useful metric is the free space in History Write Cache (HistoryCacheSize in the server config file).  This parameter should always be close to 100%.  If the cache is full - this means that Zabbix does not have time to add incoming data to the database. <br><br>  Unfortunately, this parameter is not supported by Zabbix proxy.  In addition, in Zabbix, there is no data item indicating how much data is waiting to be sent to the Zabbix server.  However, this data element is easy to do yourself through a SQL query to the proxy database: <br><br> <code>SELECT ((SELECT MAX(proxy_history.id) FROM proxy_history)-nextid) FROM ids WHERE field_name='history_lastid'</code> <br> <br>  The request will return the required number.  If you have SQLite3 as a database for a Zabbix proxy, just add the following command as UserParameter in the Zabbix agent config file installed on the machine where the Zabbix proxy is spinning. <br><br> <code>UserParameter=zabbix.proxy.items.sync.remaining,/usr/bin/sqlite3 /path/to/the/sqlite/database "SELECT ((SELECT MAX(proxy_history.id) FROM proxy_history)-nextid) FROM ids WHERE field_name='history_lastid'" 2&gt;&amp;1 <br></code> <br><br>  Then just put the trigger, which will notify that the proxy can not cope: <br><br> <code>{Hostname:zabbix.proxy.items.sync.remaining.min(10m)}&gt;100000</code> <br> <br><h4>  Total statistics </h4><br>  Finally, I suggest the system boot charts.  Immediately I say that I do not know what happened on July 16 - I had to re-create all the proxy bases (SQLite at that time) to solve the problem.  Since then I transferred all proxies to MySQL and the problem did not recur.  The rest of the ‚Äúirregularities‚Äù of the graphs coincide with the time of the load testing.  In general, the graphs show that the iron used has a large margin of safety. <br><br><img src="https://habrastorage.org/storage3/3a3/737/bad/3a3737bad6445af99d1759c295d826e1.png" alt="image"><br><img src="https://habrastorage.org/storage3/87d/c5c/196/87dc5c196ebb0ce6f52788f2648fa149.png" alt="image"><br><img src="https://habrastorage.org/storage3/e9f/07b/9f8/e9f07b9f8535c77b4fa6e841e0484af7.png" alt="image"><br><img src="https://habrastorage.org/storage3/6f9/7ec/661/6f97ec66128216feadb8908315e6b836.png" alt="image"><br><img src="https://habrastorage.org/storage3/02c/1f4/db3/02c1f4db3446565cffdbde41d1f7c858.png" alt="image"><br><img src="https://habrastorage.org/storage3/285/c49/f46/285c49f46100092164fb448443bc38b6.png" alt="image"><br><br>  But the graphics from the database server.  Traffic increments every day correspond to the time taken to dump (mysqldump).  Also, the failure of July 16 on the query graph (qps) refers to the same problem that I described above. <br><br><img src="https://habrastorage.org/storage3/41a/4c7/888/41a4c7888ebaabc00a116baf0c7b9694.png" alt="image"><br><img src="https://habrastorage.org/storage3/f32/646/20b/f3264620b3036f5032d6fdad7c565178.png" alt="image"><br><img src="https://habrastorage.org/storage3/0ed/50d/805/0ed50d805df05fdba7cb04d6a89a30ce.png" alt="image"><br><img src="https://habrastorage.org/storage3/034/a22/cbe/034a22cbefdd3c6190751add2dc014d6.png" alt="image"><br><img src="https://habrastorage.org/storage3/733/19c/580/73319c5802547aae0afc7aea30b93242.png" alt="image"><br><br><h4>  Control </h4><br>  In total, the system uses 2 servers for Zabbix servers, 2 servers for MySQL, 16 virtual servers for Zabbix proxies, and thousands of servers monitored with Zabbix agents.  With so many hosts, making changes by hands was out of the question.  And the solution was a Git repository, to which all servers have access, and where I located all the configuration files, scripts, and everything else that needs to be distributed.  Next, I wrote a script that is invoked through the UserParameter in the agent.  After running the script, the server connects to the Git repository, downloads all the necessary files and updates, and then reboots the Zabbix agent / proxy / server if the config files have changes.  Updating has become no more difficult than running zabbix_get! <br><br>  Creating new network nodes manually via the web interface is also not our method, with so many servers.  Our company has CMDB, which contains information about all the servers and services they provide.  Therefore, another magic script collects information from the CMDB every hour and compares it with what is in Zabbix.  On the findings of this comparison, the script removes / adds / enables / disables hosts, creates a group host, adds templates to hosts.  All that remains to be done manually in this case is the implementation of a new type of trigger or data element.  Unfortunately, these scripts are strongly tied to our systems, so I can not share them. <br><br><h4>  Open problems </h4><br>  Despite all the efforts that I have put in, there remains one significant problem that I just have to solve.  The point is that when the system reaches 8000-9000NVPS, the backup MySQL database no longer keeps up with the main one, so in fact there is no fault tolerance. <br><br>  I have ideas how this problem can be solved, but I have not had time to implement it: <br><br><ul><li>  Use Linux-HA with DRBD for partitioning the database. </li><li>  LUN replication to a SAN with replication to another LUN </li><li>  Percona XtraDB cluster.  In version 5.6 is still unavailable, so this will have to wait (as I wrote, there were performance problems in MySQL 5.5) </li></ul><br><h4>  Links </h4><br><ul><li>  <a href="">Zip file with all downloads from the article</a> </li><li>  <a href="https://www.zabbix.com/forum/showthread.php%3Ft%3D25349">Large environment forum thread</a> </li><li>  <a href="https://www.zabbix.com/documentation/2.0/manual/appendix/config/zabbix_server">Zabbix server configuration documentation</a> </li><li>  <a href="https://www.zabbix.com/documentation/2.0/manual/distributed_monitoring/proxies">Zabbix proxy distributed monitoring documentation</a> </li><li>  <a href="https://www.zabbix.com/documentation/2.0/manual/appendix/items/activepassive">Zabbix active / passive item documentation</a> </li><li>  <a href="https://www.zabbix.com/documentation/2.0/manual/config/items/itemtypes/internal">Zabbix internal item documentation</a> </li><li>  <a href="http://blog.zabbix.com/monitoring-how-busy-zabbix-processes-are/">Zabbix blogpost on internal items</a> </li><li>  <a href="http://clusterlabs.org/quickstart-redhat.html">Pacemaker / CMAN quickstart guide</a> </li><li>  <a href="">MySQL Pacemaker configuration guide</a> </li><li>  <a href="http://dev.mysql.com/doc/refman/5.0/en/large-page-support.html">MySQL Large Pages</a> </li><li>  <a href="http://zabbixzone.com/zabbix/partitioning-tables/">Partitioning the Zabbix database</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/193472/">https://habr.com/ru/post/193472/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../193460/index.html">Automate monitoring: low-level detection</a></li>
<li><a href="../193464/index.html">Yealink and 3CX become strategic partners</a></li>
<li><a href="../193466/index.html">Competition within the team kills the team spirit</a></li>
<li><a href="../193468/index.html">Localization trends and localization of trends: a summary of Localization World-2013</a></li>
<li><a href="../193470/index.html">‚ÄúYour mom's combat boots‚Äù is a new feature in iOS7</a></li>
<li><a href="../193474/index.html">How sales are arranged in a mobile studio</a></li>
<li><a href="../193478/index.html">Google Chromecast Review: Analogue of Apple TV?</a></li>
<li><a href="../193480/index.html">Samsung at IFA 2013: exhibition results</a></li>
<li><a href="../193482/index.html">Dissatisfied with javascript speed? - Wait a year and a half, and it will pass!</a></li>
<li><a href="../193484/index.html">"Accelerate" the opening of a heavy site</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>