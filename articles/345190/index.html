<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>We generate fake news headlines in the style of Tapy.ru</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="We compare 2 approaches to text generation using neural networks: Char-RNN vs Word Embeddings + funny examples at the end. 

 When it becomes absolute...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>We generate fake news headlines in the style of Tapy.ru</h1><div class="post__text post__text-html js-mediator-article">  <i>We compare 2 approaches to text generation using neural networks: Char-RNN vs Word Embeddings + funny examples at the end.</i> <a name="habracut"></a><br><br>  When it becomes absolutely nothing to read, I don‚Äôt want to open the book, all the articles on Habr√© are read, all the notifications on the phone are processed, and even spam in the boxes is viewed, I open Lentu.ru.  My wife - a professional journalist - at such moments begins allergies, and it is clear why.  After the old team left Ribbon in 2014, the yellowing level of the publication went up, and the quality of the text and editing went down.  Over time, periodically continuing to read Lenta by inertia, I began to notice that the news headline models were repeated: ‚ÄúFound [insert pseudo-sensation]‚Äù, ‚ÄúPutin [did something]‚Äù, ‚ÄúUnemployed Muscovite [ <a href="https://meduza.io/shapito/2015/12/12/samyy-bogatyy-zlodey-strany-udivitelnye-priklyucheniya-bezrabotnogo-moskvicha">description of his adventures</a> ]‚Äù and etc.  This was the first introductory. <br><br>  The second introductory one - recently accidentally found a funny domestic equivalent of <a href="https://twitter.com/deepdrumpf">@DeepDrumph</a> (this is Twitter, in which the phrases generated by the neural network based on Trump's official twitter) are <a href="https://twitter.com/neuromzan">laid out</a> - <a href="https://twitter.com/neuromzan">@neuromzan</a> .  Unfortunately, the author stopped uploading new tweets and hid himself, but the description of the idea is preserved <a href="https://meduza.io/shapito/2016/08/05/neyroramzan-robot-kotoryy-uchilsya-pisat-na-instagrame-ramzana-kadyrova">here</a> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      And the idea came, why not do the same thing, but based on the headlines Tape.ru?  It can turn out not less amusing, considering the level of absurdity of some real headings on this resource. <br><br>  <i><b>Note</b> : it is the task of generating the subsequent text based on some introductory text that is considered here.</i>  <i>This is not the task of Text Summarization, when, for example, its title is generated based on the text of the news.</i>  <i>The text of the news in my case is not used at all.</i> <i><br></i> <br><h2>  Data </h2><br>  The prospect of downloading and parsing all the content of the Ribbon did not please me at all, and I began to search if anyone had already done it before me.  And I was lucky: just a few days before I had this idea, Ildar Gabdrakhmanov <a href="https://habrahabr.ru/users/ildarchegg/" class="user_link">ildarchegg</a> posted a <a href="https://habrahabr.ru/post/343838/">post</a> where he describes how he robbed the content of the Ribbon and shares the full archive.  In the end, he adds, ‚ÄúI hope that someone finds this data interesting and will be able to find use for them.‚Äù Of course!  I already have an application!  Thank you, Ildar!  Your efforts saved me a few days! <br><br>  So, we take this archive and pull out the articles for the period we need: from 04/01/2014 (when the old team left) to the present.  I will not dwell on preliminary data processing.  Who <a href="https://github.com/voice32/lenta_ai">cares</a> , in the <a href="https://github.com/voice32/lenta_ai">repository on Gitkhab</a> there is a separate laptop with a step by step description of the process. <br><br><h2>  The first attempt at training - Char-RNN </h2><br>  Judging by the above article, the author neuromzan'a used the architecture of Char-RNN, described by Andrei Karpaty (Andrej Karpathy) in his already become the legendary article " <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> ".  Let's try to solve our problem with this approach.  The implementation of Char-RNN for different languages ‚Äã‚Äãand frameworks can be easily found on the Internet.  I took <a href="https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py">this one</a> - it turns out to be quite compact on Keras.  Having modified it a bit to fit my needs (changed the hyperparameters and added saving the results and more informative output), I started the training. <br><br>  And the results did not please me.  The idea of ‚Äã‚ÄãChar-RNN is that the network learns to predict the next character based on the N previous ones.  That is, some source text is supplied as input to the network, and further text is generated character by character.  Therefore, in order to learn how to write relatively readable texts, the network must: <br><br><ol><li>  To learn how to separate sequences of characters (pseudo-words) by spaces and sometimes put an end; </li><li>  Learn to generate sequences of characters that resemble real words; </li><li>  Learn how to generate words similar to the real ones, taking into account the previous almost real words. </li></ol><br>  This is what happened with a dual-layer network with 64 LSTM cells trained on a limited set of headers (&lt;800).  In brackets - introductory text for the network, generate the following 150 characters: <br><br><pre><code class="hljs objectivec"><span class="hljs-number"><span class="hljs-number">1</span></span> : <span class="hljs-string"><span class="hljs-string">"[   .    ]                .               "</span></span>; <span class="hljs-number"><span class="hljs-number">2</span></span> : <span class="hljs-string"><span class="hljs-string">"[       ]     .            .       .         .    .  2      "</span></span>; <span class="hljs-number"><span class="hljs-number">3</span></span> : <span class="hljs-string"><span class="hljs-string">"[     .    ]      .  .  .    .  .  .  .   .  "</span></span>; <span class="hljs-number"><span class="hljs-number">4</span></span> : <span class="hljs-string"><span class="hljs-string">"[  . :  ,   ]    .       .                   "</span></span>; <span class="hljs-number"><span class="hljs-number">5</span></span> : <span class="hljs-string"><span class="hljs-string">"[ -   .   ] .  .      .  .    .   .      .    .  . "</span></span>; ... <span class="hljs-number"><span class="hljs-number">25</span></span> : <span class="hljs-string"><span class="hljs-string">"[    7,2. ]          .    -.     "</span></span>; ... <span class="hljs-number"><span class="hljs-number">50</span></span> : <span class="hljs-string"><span class="hljs-string">"[  - 2018   ]                    "</span></span></code> </pre> <br>  Further did not begin to train.  It turns out some nonsense.  Yes, this is similar to phrases where there are words separated by spaces and ending with dots.  Some ‚Äúwords‚Äù are even real words from the Russian language.  But it is impossible to read, and it is not at all like those beautiful and ridiculous remarks that were examples that inspired me.  Changes in hyperparameters had virtually no effect on quality.  Training in full dataset also did not greatly improve the results.  Perhaps these comrades trained the network for much longer, added more layers and made them wider, but I decided to try a different approach. <br><br><h2>  Alternative method - Word Embeddings </h2><br>  I remembered that a little less than a year ago, on the <a href="https://www.udacity.com/course/deep-learning-nanodegree-foundation--nd101">Udacity Deep Learning Foundation</a> course, I had a task to write a network that would generate a script for the Simpsons (more specifically, one scene in Mo‚Äôs tavern), taking the original scenarios of similar scenes from 25 seasons of the cartoon for learning .  In this case, another approach was used - Word Embeddings, when the text is generated not by character, but by words, but also on the basis of the distribution of probabilities of the occurrence of certain words given the previous words [1..N]. <br><br>  This approach has several significant advantages over Char-RNN: <br><br><ol><li>  Networks do not need to learn to generate words - they themselves are atomic elements themselves, respectively, the generated phrases will initially consist only of words from our body, there will be no real words; </li><li>  The words in the phrases are more consistent with each other, because for the set of words {"in", "on", "drove", "car", "car"} the sequence ["went", "on", ‚ÄúCar‚Äù] than [‚Äúdrove‚Äù, ‚Äúin‚Äù, ‚Äúcar‚Äù]. </li><li>  Higher learning speed, because  for the same case for Char-RNN, tokens will be characters, and for Word Embeddings - words.  The number of words in the text is obviously less than the characters, and for the machine that is in the first, that in the second case, the token is simply an index in the dictionary.  Accordingly, in one iteration, Word Embeddings will need to process far fewer tokens than Char-RNN, and it will take approximately proportionally less time. </li></ol><br>  Quick hypothesis testing on a limited dataset (the same 2 layers and 64 LSTM cells, generate 100 of the following tokens): <br><br><pre> <code class="hljs objectivec"> <span class="hljs-number"><span class="hljs-number">1</span></span>: <span class="hljs-string"><span class="hljs-string">"[]...................................................................................................."</span></span> ...  <span class="hljs-number"><span class="hljs-number">4</span></span>: <span class="hljs-string"><span class="hljs-string">"[]...  ..   ...... facebook........... .......... ............... .... ...... . ......... . ...........    ..."</span></span> ...  <span class="hljs-number"><span class="hljs-number">10</span></span>: <span class="hljs-string"><span class="hljs-string">"[].      .    . . ..     iphone8. ...  . -.    . .   .     ..  .   .     ..  facebook        . .     .  .  facebook . android. . . . "</span></span> ...  <span class="hljs-number"><span class="hljs-number">20</span></span>: <span class="hljs-string"><span class="hljs-string">"[].            .      .    .  .         .   12.     . ..  .    .  .      . . android   .    . -  . .    . .     "</span></span></code> </pre> <br>  So what happens here.  In the first era, the network sees that one of the most frequent tokens is a full stop.  ‚ÄúAnd let me put points everywhere, since she has such a high frequency,‚Äù she thinks, and gets a gigantic mistake.  Then she doubts for a couple of iterations, and on the 4th she decides that some words should be inserted between the points.  ‚ÄúWow!  The error has become smaller, ‚Äùthe network rejoices,‚Äú We ‚Äã‚Äãmust continue in this spirit, we will insert different words, but I really like the dots, so for now I will continue to put them. ‚Äù  And he continues to put dots, diluting them with words.  Gradually, she realizes that words should be put more often than dots, and some words should go close to each other, for example, she remembers some passages from the dataset: ‚Äúextend the government‚Äù, ‚Äúfulfill agreements‚Äù, ‚Äútwin eater‚Äù, etc.  By the 20th epoch, she already remembers rather long phrases, such as ‚Äúpublished footage of the dropout of military equipment from the air.‚Äù  And it shows that, in principle, the approach works, but on such a small set of data the network quickly retrains (even despite the dropout), and instead of unique phrases it produces memorized ones. <br><br>  Let's see what happens if we train it on the full set of headings: <br><br><pre> <code class="hljs css"> 1: "<span class="hljs-selector-attr"><span class="hljs-selector-attr">[]</span></span>      ..       .        .             .          .  .    .   .      .     .   .      .         .   10 " ...  5: "<span class="hljs-selector-attr"><span class="hljs-selector-attr">[]</span></span>  .   .       59 .        .         . <span class="hljs-selector-tag"><span class="hljs-selector-tag">-</span></span>        <span class="hljs-selector-tag"><span class="hljs-selector-tag">forbes</span></span>  .   10  .       300.           .     3     .   <span class="hljs-selector-tag"><span class="hljs-selector-tag">-</span></span>.         .  " ‚Ä¶  10: ‚Äú<span class="hljs-selector-attr"><span class="hljs-selector-attr">[]</span></span> .         .  .          <span class="hljs-selector-tag"><span class="hljs-selector-tag">tor</span></span>.  .     .         .          72 .     .    .           .              .     ‚Äù</code> </pre> <br>  Only 10 epochs were enough for loss to stop decreasing.  As you can see, in this case the network also memorized some rather long pieces, but at the same time, there are also many relatively original phrases.  You can often see how the network creates long phrases from other long ones, like this: ‚ÄúI called durov cattle and publicly erased a telegram‚Äù + ‚Äúa drunk Muscovite stole a tractor and got into an accident with a taxi‚Äù = ‚Äúa drunk Muscovite stole a tractor and publicly erased a telegram ". <br><br>  However, all the same, in most cases the phrases are not as beautiful as those of DeepDrumph and neuromzan.  What is wrong here?  Need to train longer, deeper and wider?  And then an insight came upon me.  No, these guys have not found a magical architecture, issuing beautiful texts.  They simply generate long texts, select potentially funny chunks and manually edit them!  The final word for the man - that's the secret! <br><br>  After some manual editing, you can get quite acceptable options: <br><br><ul><li>  ‚ÄúResponded.  Sands sale opportunity zammera "&gt;&gt;&gt;" Sands responded to the possibility of selling the post of deputy mayor " </li><li>  "Vice Speaker found man" &gt;&gt;&gt; "Vice Speaker found man" </li><li>  "Deputy Lebedev recognized as inappropriate the proposal of the offer of murder" &gt;&gt;&gt; "Deputy Lebedev recognized as inappropriate the offer of murder" </li></ul><br>  ‚Ä¶ and so on. <br><br>  There is another important point related to the Russian language.  It is much more difficult to generate phrases for the Russian language, because the words and phrases in the sentence need to be coordinated.  In English, of course, too, but to a much lesser extent.  Here is an example: <br>  "Car" &gt;&gt;&gt; "car" <br>  "By car" &gt;&gt;&gt; "by car" <br>  "I see the car" &gt;&gt;&gt; "see the car" <br><br>  That is, in English the word in different cases is the same token from the network point of view, and in Russian it is different due to different endings and other parts of the word.  Therefore, the results of the model in English look more believable than in Russian.  You can, of course, lemmatize words in a Russian corpus, but then the generated text will consist of such words, and here you can‚Äôt do without manual doping.  By the way, I tried to do this using the <a href="https://pymorphy2.readthedocs.io/en/latest/">pymorphy2</a> module, but the result, in my personal opinion, was even worse in some places, despite the fact that the number of unique tokens (words) after normalization decreased by more than 2 times.  After 20 epochs, the result was: <br><br><pre> <code class="hljs 1c">‚Äú[].     <span class="hljs-keyword"><span class="hljs-keyword"></span></span> . <span class="hljs-keyword"><span class="hljs-keyword"></span></span>  <span class="hljs-keyword"><span class="hljs-keyword"></span></span> .          .   .       . <span class="hljs-keyword"><span class="hljs-keyword"></span></span>  <span class="hljs-keyword"><span class="hljs-keyword"></span></span> .  . .  .      .  .  .     .     facebook  ..   .   .  .      ..    ‚Äù</code> </pre> <br>  You may also notice that the original meaning of the word is often lost.  For example, in the above passage, the name ‚ÄúSands‚Äù pymorphy normalized to the word ‚Äúsand‚Äù - a serious loss for the whole corpus. <br><br><h2>  findings </h2><br><ul><li>  Word Embeddings copes with text generation much better than Char-RNN; </li><li>  Other things being equal, the quality of the generation of English text is generally higher than that of Russian, due to the nature of the languages; </li><li>  When you hear that the AI ‚Äã‚Äãwrote the book (as it was the other day when it was announced that the <a href="https://www.newsru.com/cinema/14dec2017/potter.html">algorithm wrote the Harry Potter book</a> , divide this statement by 1000, because, most likely, the algorithm could only 1) generate the names and descriptions of the characters, or 2) generate a general outline of the story, or 3) generate a semi-absurd text, which was then long and stubbornly ruled and coordinated by the people-editors.  Or all this taken together.  But certainly, AI did not write a book from the first to the last word.  Not at the same level of development is our technology. </li></ul><br><h2>  Dessert </h2><br>  Well, in the end some pearls after manual edits, it seems to me, quite in the spirit of Lenta.ru: <br><br><ul><li>  Ministry of Finance refused to party in the Ryazan region </li><li>  Australian rejected iPhone8 on Android </li><li>  Irkutsk left without air </li><li>  Police of St. Petersburg against the world of St. Petersburg </li><li>  China began sales of debtors </li><li>  Ministry of Economic Development reported a gay partnership against sanctions </li><li>  Greece made a teddy bear </li><li>  Putin made peace with the museum and with Solzhenitsyn </li><li>  Naked tourist died in Budyonnovsk </li><li>  Doctors dissuaded Gorbachev from parting with problems in the slave trade. </li><li>  In Britain, the semi-finals of the Confederations Cup </li><li>  MP Lebedev found inappropriate to fine taxi </li><li>  In the Federation Council offered to choose the honorary Leopoldov </li><li>  Deputy Speaker of the Duma justified the proposal to prepare a coup d'√©tat </li><li>  Lithuanian oncologists in Moscow started treatment </li><li>  Central Bank will allocate money to repair the stone troll penis </li><li>  It is more difficult to decently joke about the new Moscow Region leading </li><li>  Drunk arrested provoked the State Duma </li><li>  Israel answered with acid in the face </li><li>  The head of the US Federal Reserve promised the absence of creams </li><li>  The problem of the barracks in Izhevsk decided through the bed </li><li>  On Yamal krechety first bred from Eurovision </li><li>  Moscow for the year will spend more Brazil </li><li>  Off the coast of Libya taxi from Ivanushek </li><li>  Published the death toll in the crash of a tourist vessel in the Rada </li><li>  Kim Kardashian accused of preparing another chemical attack </li><li>  The government has promised a lack of money for the internal affairs of Russia </li><li>  Thousand counselors prepared for work in Russia </li><li>  Breivik complained of air attack on his territory </li><li>  US demanded to give in the face Zhirkov </li><li>  Petersburg schizophrenic warned of a few days before blocking Telegram </li></ul><br>  The code with laptops and comments <a href="https://github.com/voice32/lenta_ai">is posted on Gitkhab</a> .  There is also a pre-trained network, a script for generating text (lenta_ai.py) and instructions for use.  In 99% of cases, you will receive meaningless sets of words, and only occasionally something interesting will come across.  Well, if you want to ‚Äújust see‚Äù, I launched a small <a href="https://lenta-ai.herokuapp.com/">web application on heroku</a> , where you can generate headers without running the code on your machine. </div><p>Source: <a href="https://habr.com/ru/post/345190/">https://habr.com/ru/post/345190/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../345178/index.html">UIKit performance optimization</a></li>
<li><a href="../345180/index.html">Helping food delivery: logo redesign and corporate identity development</a></li>
<li><a href="../345184/index.html">REST is a new SOAP</a></li>
<li><a href="../345186/index.html">Render clouds on mobile devices</a></li>
<li><a href="../345188/index.html">Today is the day of thick polar fox retail - the error of cash registers Shtrikh-M across the country</a></li>
<li><a href="../345192/index.html">Understand what happened with the course of Bitcoin</a></li>
<li><a href="../345194/index.html">Information security of bank non-cash payments. Part 2 - Typical IT infrastructure of the bank</a></li>
<li><a href="../345196/index.html">Attention! S in Ethereum stands for Security. Part 1. Blockchain things</a></li>
<li><a href="../345198/index.html">Dynamic identification of control objects</a></li>
<li><a href="../345200/index.html">How to deal with miners cryptocurrency in the corporate network</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>