<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>LinkedIn's brief history of scaling</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Translator's note : We at Latera are engaged in creating billing for telecom operators . We will write about the features of the system and the detail...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>LinkedIn's brief history of scaling</h1><div class="post__text post__text-html js-mediator-article">  <b>Translator's note</b> : <i>We at Latera are engaged in creating <a href="http://www.hydra-billing.ru/">billing for telecom operators</a> .</i>  <i>We will write about the features of the system and the details of its development in our blog on Habr√© (for example, to ensure fault tolerance), but you can also learn something interesting from the experience of other companies.</i>  <i>Today we bring to your attention an adapted translation of a <a href="https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin">note</a> by LinkedIn chief engineer Josh Klemm on the process of scaling social network infrastructure.</i> <br><br> <a href="http://habrahabr.ru/company/latera/blog/266447/"><img src="https://habrastorage.org/files/c74/b17/cc6/c74b17cc634445f48ec044ca312d4b50.png"></a> <br><br>  LinkedIn was <a href="https://ourstory.linkedin.com/">launched</a> in 2003 with the goal of creating and maintaining a network of business contacts and expanding job search opportunities.  In the first week, 2,700 people registered on the network.  After a few years, the number of products, customer base and server load have increased significantly. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Today, LinkedIn has more than 350 million users worldwide.  We check tens of thousands of web pages every second, every day.  <a href="http://blog.linkedin.com/2014/04/18/the-next-three-billion/">Mobile devices</a> now account for more than 50% of our traffic worldwide.  Users request data from our backend systems, which, in turn, process several million requests per second.  How did we achieve this? <a name="habracut"></a><br><br><h4>  Early years: Leo </h4><br>  We began to create a LinkedIn site in the same way as many other sites are being created today - in the form of a monolithic application that handles all tasks.  This application is called Leo.  It hosted web servlets that served all the web pages, took into account business logic, and connected to several LinkedIn databases. <br><br><img src="https://habrastorage.org/files/a3b/e7c/9bf/a3be7c9bfa9149279cdb344e5554f845.png"><br><br>  <i>Good old-fashioned website development is so easy and simple.</i> <br><br><h4>  User graph </h4><br>  In a social network, first of all, it is necessary to establish communication between its users.  To improve efficiency and productivity, we needed to create a system that would request data about the connections in the existing graph, while it itself was stored in the network memory.  With this new approach to the use of profiles, the system needed to be scaled independently of Leo.  So we had a separate system for our user graph, which we called Cloud: it became the first LinkedIn service.  We connected this separate service and the Leo application using RPC (Remote Procedure Call) technology implemented in Java. <br><br>  Around the same time, we thought about the need to create a search engine.  Our graph-based user service has begun to transmit information to a new search service based on <a href="https://lucene.apache.org/">Lucene</a> . <br><br><h4>  Database copies </h4><br>  As the site grew, Leo also grew: along with its significance and influence, the complexity of the application also increased.  To reduce the complexity led to the load distribution, as in the work there were several instances of Leo.  Nevertheless, the increased load influenced the work of the most important LinkedIn system - its user profile database. <br><br>  The simplest solution was the usual vertical scaling: we added more processors and memory to the system.  So we won a little time, but it was necessary to carry out further scaling.  The profiles database was responsible for both reading and writing data, so several child copies (Slave) of the database were created, which were synchronized with the main database (Master) using the very first version of <a href="http://data.linkedin.com/blog/2012/10/driving-the-databus">Databus</a> (now <a href="https://engineering.linkedin.com/data-replication/open-sourcing-databus-linkedins-low-latency-change-data-capture-system">open</a> source) .  These copies were designed to read all the data, and we built logic to determine when it is safer and more expedient to read data from a replica than from the main database. <br><br><img src="https://habrastorage.org/files/3c9/f71/a32/3c9f71a329d44c359d9fe5dba8da85b5.png"><br><br>  <i>* Due to the fact that the Master-Slave model was a medium-term solution, we decided to partition the databases</i> <br><br>  Our site attracted more and more traffic, in connection with which our monolithic Leo application began to hang frequently.  Finding and eliminating the cause, as well as debugging the application with the new code was not so easy.  High resiliency is very important for LinkedIn.  Therefore, it was clear that we need to ‚Äúkill Leo‚Äù and break it into many small services that have a number of functions and at the same time <a href="http://en.wikipedia.org/wiki/Service-oriented_architecture">do not store</a> information about the previous state. <br><br><img src="https://habrastorage.org/files/2bb/ebd/564/2bbebd5643a34aa8accf67d159cd2672.jpg"><br><br>  <i>‚ÄúKill Leo‚Äù was the motto of our company for several years ...</i> <br><br><h4>  Service Oriented Architecture </h4><br>  In the course of development, microservices began to be allocated for the communication of API-interfaces and business logic - an example is our platform for searching, communicating and creating profiles and groups.  Later, presentation areas were identified for areas such as recruitment and general profile.  Outside the Leo application, completely new services were created for new products.  Over time, in each functional area of ‚Äã‚Äãits own vertical stack. <br><br>  We installed front-end servers so that we could extract data from different domains, take into account the presentation logic and create HTML pages using JSP technology (JavaSever Pages).  In addition, we have developed services at the intermediate level, in order to use them to provide access to data through the API, and backend services for storing user data in order to provide reliable access to the database / databases.  By 2010, we already had more than 150 separate services.  Today their number exceeds 750. <br><br><img src="https://habrastorage.org/files/06e/297/5e2/06e2975e2d764f53beed7594a18dfb91.png"><br><br>  <i>LinkedIn example service oriented architecture</i> <br><br>  Since the states are not saved, scaling can be done by running new instances of any of the services and using a load balancer.  We began to actively monitor the load that each of the services can withstand, having previously allocated all the necessary resources and prepared means of monitoring performance. <br><br><h4>  Caching </h4><br>  The company experienced rapid growth and planned to expand further.  We knew that we could reduce the overall load by increasing the number of cache levels.  In many applications, intermediate cache levels have been introduced, such as in the cases of <a href="https://en.wikipedia.org/wiki/Memcached">Memcached</a> and <a href="https://en.wikipedia.org/wiki/Couchbase_Server">Couchbase</a> .  We also increased the cache of our databases and began to use the <a href="http://engineering.linkedin.com/tags/voldemort">Voldemort</a> repository with pre-calculated results when necessary. <br><br>  Over time, we have deleted many intermediate cache levels.  They stored data retrieved from different domains.  At first glance, caching seems to be an obvious way to reduce the load, but the complexity of the invalidation and the structure of the call graph is prohibitively high.  The closest location of the cache to the data stores reduces waiting time, allows horizontal scaling and reduces cognitive load. <br><br><h4>  Kafka </h4><br>  To bring growing data volumes together, LinkedIn has developed several types of pipelines for transferring data flow and organizing them in a queue.  For example, we needed to transfer data to the repository, send data packets between <a href="http://data.linkedin.com/projects/hadoop">Hadoop</a> workflows for analysis, collect logs of each service and various statistical indicators, such as the number of page views, create a queue system in its inMail messaging system, and issue relevant information about users immediately after updating their profiles. <br><br>  As the site expanded, more and more pipelines appeared.  The site needed scaling, therefore, each individual pipeline also needed scaling.  Something needed to be sacrificed.  As a result, we developed a distributed messaging system called <a href="http://blog.confluent.io/2015/02/25/stream-data-platform-1/">Kafka</a> .  It has become a universal conveyor based on the <a href="http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">principle of</a> storing the operations performed and taking into account the possibility of increasing speed and scalability.  Kafka provided almost instant access to any data warehouse, helped in working with vacancies with Hadoop, allowed us to carry out <a href="http://engineering.linkedin.com/analytics/real-time-analytics-massive-scale-pinot">real-time</a> analysis, significantly improved <a href="http://engineering.linkedin.com/samza/real-time-insights-linkedins-performance-using-apache-samza">monitoring of</a> our site and <a href="http://engineering.linkedin.com/52/autometrics-self-service-metrics-collection">the alert system</a> , and also made it possible to visually display call graphs and <a href="http://engineering.linkedin.com/distributed-service-call-graph/real-time-distributed-tracing-website-performance-and-efficiency">monitor</a> their changes .  Today, Kafka processes <a href="http://engineering.linkedin.com/kafka/kafka-linkedin-current-and-future">more than 500 billion requests</a> per day. <br><br><img src="https://habrastorage.org/files/c74/b17/cc6/c74b17cc634445f48ec044ca312d4b50.png"><br><br>  <i>Kafka as a universal data transmission tool</i> <br><br><h4>  Inversion </h4><br>  Scaling can be viewed from different points of view, including the organizational one.  In late 2011, LinkedIn launched an internal project called <a href="http://www.bloomberg.com/bw/articles/2013-04-10/inside-operation-inversion-the-code-freeze-that-saved-linkedin">InVersion</a> .  Thanks to him, we have suspended the development of individual functions: it allowed the entire company to focus on equipment, infrastructure, implementation and productivity of developers.  As a result, we have achieved a certain level of flexibility needed to develop new scalable products that we have today. <br><br><h4>  Our days: Rest.li </h4><br>  After we switched from Leo to a service-oriented architecture, the APIs of our teams, which were accessed via RPC in Java, turned out to be incompatible and closely related to the presentation layer.  Over time, the situation only worsened.  To solve the problem, we created a new API model and called it <a href="http://engineering.linkedin.com/architecture/restli-restful-service-architecture-scale">Rest.li.</a>  Rest.li was a step towards the data model-oriented architecture.  It was built on a single, stateless RESTful API model: the <a href="http://engineering.linkedin.com/restli/linkedins-restli-moment">whole company</a> could now use this model. <br><br>  As a result, our new APIs on JSON, which was used instead of HTTP, made it easier to work with non-Java client applications.  Today, most LinkedIn developers program in Java, but the company has a ton of clients in Python, Ruby, Node.js, and C ++, developed by both full-time employees and programmers from companies absorbed by LinkedIn. <br><br>  The refusal of RPC made us less dependent on presentation levels and saved us from compatibility issues with previous versions.  Plus, using <a href="https://github.com/linkedin/rest.li/wiki/Dynamic-Discovery">Dynamic Device Discovery (D2)</a> together with Rest.li allowed us to automate load balancing, device discovery and scaling of the client API of each service. <br><br>  Currently LinkedIn has more than 975 resources based on the Rest.li model, and our data centers handle more than 100 billion Rest.li requests per day. <br><br><img src="https://habrastorage.org/files/deb/226/036/deb2260361784a1eb0574bbb9abf2ca8.png"><br><br>  <i>Stack Rest.li R2 / D2</i> <br><br><h4>  Superblocks </h4><br>  Service-oriented architectures are ideal for separating domains and scaling services independently.  But there is a negative aspect.  Many of our applications process data of various types, thus making several hundred calls to the client side.  All of these appeals are usually reduced to the concepts of "call graph" and "branching."  For example, each access to a profile page causes not only profile data: these are photographs, contacts, groups, information about subscriptions, blog posts and the connectivity levels of our graph, recommendations and much more.  This graph was quite difficult to manage, and its complexity gradually increased. <br><br>  We used the superblock principle, which is to form a group of backend services with a single access to the API.  Due to this principle, the team of our specialists is engaged in block optimization, at the same time following the call graph of each client. <br><br><h4>  Multi-user data centers </h4><br>  Being an international company with a growing customer base, we need to scale up, involving <a href="https://www.linkedin.com/pulse/armen-hamstra-how-he-broke-linkedin-got-promoted-angel-au-yeung">more than one</a> data center in processing traffic.  We began to solve this problem several years ago: at first, two data centers began to deal with general customer profiles - in Los Angeles and Chicago. <br><br>  After testing them, we decided to work on improving our services in order to establish data replication, feedback from various sources, events of one-way data replication and connection of the user to a data center closer to him / her. <br><br>  Most of our databases are stored in <a href="http://engineering.linkedin.com/espresso/introducing-espresso-linkedins-hot-new-distributed-document-store">Espresso</a> - a modern multi-user data warehouse of our company.  It was built to meet the requirements of multi-user data centers.  Espresso provides Master-Master support and is responsible for more complex replication technologies. <br><br>  Having multiple data centers is extremely important for maintaining site operation and high resiliency.  We have to avoid the presence of even the slightest problems not only in each individual service, but also throughout the site.  Today, LinkedIn has three main data centers and several additional <a href="http://engineering.linkedin.com/performance/how-linkedin-used-pops-and-rum-make-dynamic-content-download-25-faster">points of presence</a> around the world. <br><br><img src="https://habrastorage.org/files/83e/039/903/83e039903bf243e397c8e0b8361cd197.png"><br><br>  <i>The location of various types of equipment LinkedIn as of 2015 (circles denote data centers, diamonds - points of presence)</i> <br><br><h4>  What else have we achieved? </h4><br>  As you understand, in practice, scaling is much more difficult.  During these few years, our development and engineering teams have done a titanic work, from which it is worthwhile to single out several separate projects. <br><br>  Many especially important systems of our company have their rich history of development in the process of scaling.  Separately, it is worth to highlight the <a href="http://engineering.linkedin.com/real-time-distributed-graph/using-set-cover-algorithm-optimize-query-latency-large-scale-distributed">graph of users</a> (our first service outside the Leo application), <a href="https://engineering.linkedin.com/search/did-you-mean-galene">search</a> (our second service), news feed, <a href="http://www.slideshare.net/manivannan57/LinkedIn-Communication-Architecture-Presentation-2%3Frelated%3D5">communication platform</a> and server application for displaying user profiles. <br><br>  We have developed an information infrastructure for long-term growth.  We first felt this after the creation of Databus and Kafka.  Then came <a href="https://engineering.linkedin.com/samza/real-time-insights-linkedins-performance-using-apache-samza">Samza</a> for streaming, <a href="http://engineering.linkedin.com/espresso/introducing-espresso-linkedins-hot-new-distributed-document-store">Espresso</a> and Voldemort as storage tools, Pinot for analyzing our systems and other user solutions.  Among other things, our equipment has become much better, so that developers can now independently use this infrastructure. <br><br>  We have developed an offline business process management system using <a href="http://data.linkedin.com/projects/hadoop">Hadoop</a> and <a href="http://engineering.linkedin.com/tags/voldemort">Voldemort</a> data warehouses in advance to provide information such as People You May Know, Similar Profiles, Successful Graduates, and Profile Location. <br><br>  We revised our approach to the frontend by adding client-side <a href="http://www.slideshare.net/brikis98/dustjs">templates</a> (‚Äú <a href="http://engineering.linkedin.com/profile/engineering-new-linkedin-profile">Profile Page</a> ‚Äù, ‚Äú <a href="http://engineering.linkedin.com/university/building-linkedin-university-pages">University Pages</a> ‚Äù).  They add interactivity to applications by requesting from the server only those objects that are partially or fully written in JSON.  In addition, the templates are in the cache network CDN (content delivery network) and the browser.  We also started using <a href="http://engineering.linkedin.com/frontend/new-technologies-new-linkedin-home-page">BigPipe</a> and the <a href="https://engineering.linkedin.com/play/composable-and-streamable-play-apps">Play</a> framework, changing our model from a streaming web server to <a href="https://engineering.linkedin.com/play/play-framework-async-io-without-thread-pool-and-callback-hell">non-blocking and asynchronous</a> . <br><br>  Over the application code, we introduced several levels of <a href="http://www.slideshare.net/thenickberry/reflecting-a-year-after-migrating-to-apache-traffic-server">proxy servers</a> using Apache Traffic Server and HAProxy for load balancing, data center connections, security, smart routing, server-side rendering, and much more. <br><br>  And on top of that, we continue to increase the performance of our servers, optimizing hardware, setting up an improved memory system, and using the latest Java runtime environment. </div><p>Source: <a href="https://habr.com/ru/post/266447/">https://habr.com/ru/post/266447/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../266433/index.html">Developing a metamodel using the Eclipse Modeling Framework (and a little about data modeling)</a></li>
<li><a href="../266435/index.html">How to learn to make games: useful resources</a></li>
<li><a href="../266437/index.html">Says Gartner: Overheated IoT and Practical Clouds</a></li>
<li><a href="../266441/index.html">Assistant program for mastering blind typing on a keyboard in Linux</a></li>
<li><a href="../266443/index.html">How do closures (under the hood) in JavaScript</a></li>
<li><a href="../266449/index.html">A little about the implementation of the puzzle "Soma Cubes" (Swift & SceneKit)</a></li>
<li><a href="../266453/index.html">As I wrote the rainy map</a></li>
<li><a href="../266455/index.html">Backstage at CyberSoft</a></li>
<li><a href="../266457/index.html">Making a Real Forex Robot Idea</a></li>
<li><a href="../266459/index.html">Old new pywinauto: automate Windows GUI in Python with install / uninstall example</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>