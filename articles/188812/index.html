<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Probabilistic models: struggle with cycles and variational approximations</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the fourth series of the cycle about graphical probabilistic models ( part 1 , part 2 , part 3 ) we will continue to talk about how to cope with co...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Probabilistic models: struggle with cycles and variational approximations</h1><div class="post__text post__text-html js-mediator-article">  In the fourth series of the cycle about graphical probabilistic models ( <a href="http://habrahabr.ru/company/surfingbird/blog/176461/">part 1</a> , <a href="http://habrahabr.ru/company/surfingbird/blog/177889/">part 2</a> , <a href="http://habrahabr.ru/company/surfingbird/blog/185622/">part 3</a> ) we will continue to talk about how to cope with complex factor graphs.  <a href="http://habrahabr.ru/company/surfingbird/blog/185622/">Last time</a> we studied the message transfer algorithm, which, however, works only in cases where the factor graph is a tree, and at each node you can easily recalculate the distribution by brute force.  What to do in really interesting cases, when there are large informative cycles in the graph, we will start discussing today - let's talk about a couple of relatively simple methods and discuss a very powerful, but difficult to use tool - variational approximations. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/568/1cd/bd8/5681cdbd8f68ff7e194a194b610aec4d.jpg"><br><a name="habracut"></a><br><br><h3>  Summary of the previous series </h3><br>  First, let us recall what a factor graph, which we discussed last time.  This is a bichromatic graph in which two types of vertices correspond to variables and functions.  Each function is associated with those variables on which it depends, and the graph itself defines the decomposition of a large function into a product of small ones.  Last time we examined in detail a very simple example of a factor graph - a linear chain of nodes <br><img src="https://habrastorage.org/getpro/habr/post_images/3fb/db5/810/3fbdb5810fdca23d2644103b49f5f31c.png" alt="image">  , <br>  which corresponds to decomposition <br><img src="https://habrastorage.org/storage2/3ee/cae/ac3/3eecaeac31307feedcc19f3a856f4891.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The message passing algorithm, the essence of which we saw in this example, works almost unchanged in any factor graph, which is a tree: messages are transmitted from the leaves to the tree, converge at the root of the tree (which can be any node), and then diverge back .  As a result, each vertex of the graph receives messages from all its neighbors, after which it can easily calculate the sum of the original expansion for all variables except itself (thereby calculating its own marginal distribution). <br><br>  However, in real life, not all useful probabilistic models are described by tree graphs.  We <a href="http://habrahabr.ru/company/surfingbird/blog/150607/">have already considered</a> one such example in our series of articles - this is the LDA model (latent Dirichlet allocation), which is very useful for automatic text analysis and looks like this in the Bayesian network: <br><img src="https://habrastorage.org/getpro/habr/post_images/5eb/d92/a73/5ebd92a738eeb5f628c0061564559736.png"><br>  Let me remind you that the model shown in this picture generates a new document of length <i>N</i> as follows: <br><ul><li>  choose vector <img src="https://habrastorage.org/getpro/habr/post_images/fbf/5b0/617/fbf5b061708fff69d170c3e2e37645bc.png">  - vector "degree of expression" of each topic in the document; </li><li>  for each of the <i>N</i> words <i>w</i> : <br><ul><li>  choose a topic <img src="https://habrastorage.org/getpro/habr/post_images/af2/163/d1e/af2163d1ecb7e905400d31b46e144a79.png">  by distribution <img src="https://habrastorage.org/getpro/habr/post_images/92f/665/7da/92f6657da9a33ba894fb8655730c8a24.png">  ; </li><li>  choose a word <img src="https://habrastorage.org/getpro/habr/post_images/3a3/1f3/4bf/3a31f34bf1e79e7297672d298bdb8200.png">  with probabilities given in Œ≤. </li></ul></li></ul><br><br>  The picture looks so simple because it uses so-called ‚Äúplates‚Äù (plates) - each rectangle actually implies several copies of everything that is depicted in it.  For example, if you fully draw the LDA factor graph, say, for two documents, one of which has two words, and three in another, already at this stage you get a rather impressive structure (in order not to overload the picture completely, I did not sign the functions): <br><img src="https://habrastorage.org/storage2/2cb/419/5ec/2cb4195ecf29122582d73627e5e55536.png"><br><br>  As you can see, there are a lot of cycles here, and they come from the fact that the same parameters Œ± and Œ≤ are used repeatedly to obtain different observations.  This situation, of course, occurs very often - in fact, this is the main formulation of tasks in machine learning: assuming that the distribution parameters are the same, evaluate them or predict subsequent observations.  Therefore, cycles in graphic models are not a bug, but a feature.  And we will now try to talk about how to implement this feature.  Immediately I warn you that our narrative is gradually entering more and more mathematically complex areas;  today I will be waving my hands more than proving something, and then we will go on to concrete examples.  For those who wish to study the material, I recommend to see <a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/">Bishop‚Äôs excellent book on Bayesian output</a> , <a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/">Hastie et al.</a>  <a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/">about machine learning</a> , <a href="https://www.coursera.org/course/pgm">course on graphic models on Coursera</a> and other sources - nowadays there is no shortage of sources. <br><br>  And we will slowly cope with cycles.  Today I will look at several methods that allow you to cope with cycles in some cases, and when they work, it is usually more convenient and computationally easier to use and use.  And another time we will talk about a more general method related to Gibbs sampling (more precisely, sampling based on Markov chains - Markov chain Monte Carlo). <br><br><h3>  Replace loop to the top </h3><br>  Sometimes a cycle can be painlessly replaced by one vertex, simply by increasing the number of variables of the corresponding function.  For example, you got a triangle somewhere in the factor graph: <br><img src="https://habrastorage.org/storage2/9f0/2fe/d38/9f02fed38c8ec990394e29b5691a16ae.png"><br><br>  As we already know, it means that in decomposition occurs <br><img src="https://habrastorage.org/getpro/habr/post_images/c34/955/e56/c34955e56b2ddbf6a3c55df954fa5d32.png"><br><br>  In most cases, such a triangle can be safely replaced by a new function. <br><img src="https://habrastorage.org/getpro/habr/post_images/e6b/d13/861/e6bd13861d63a419a2435152a1ba580f.png"><br>  and the cycle will disappear: <br><img src="https://habrastorage.org/storage2/518/b07/c98/518b07c98d5d329136b90511353732f1.png"><br><br>  The disadvantage of this method is clear - as the number of arguments increases, the calculation function, as a rule, grows exponentially (if they do not grow, then the task of marginalizing does not pose much difficulty, so there is no need to fence a garden with graphic models).  Therefore, you can only cope with small cycles.  And in practically important tasks, the cycles are often very large, passing through the entire diameter of the graph: look at least at the cycles in the LDA model, passing from Œ± to Œ≤ and back.  Therefore, this ‚Äútoy‚Äù method, in reality, as a rule, is not applicable - more precisely, the models are usually immediately built as if it had already been used in all possible cases. <br><br><h3>  Message Transfer Algorithm </h3><br>  The first practically important method that can be used if the message transfer algorithm is not applicable is, of course, a message transfer algorithm.  :) Of course, for cycles it, formally speaking, does not work - there is no place to start, no leaves.  Therefore, a modification of the message passing algorithm, known as loopy belief propagation, is used here: <br><ul><li>  send from each vertex variable to each vertex function a message that is identically equal to 1; </li><li>  continue to use the usual message passing algorithm. </li></ul><br>  Of course, such a mockery of the message passing algorithm is not completely painless.  First, it becomes approximate from the exact;  if earlier it was enough to drive the messages along each edge once in each direction, now the messages are sent immediately along all edges and gradually change;  we hope that over time these messages will stop changing (converge on some values), and then we take these values ‚Äã‚Äãas an answer. <br><br>  Secondly, even this hope is not always justified.  It is easy to build an example in which the message passing algorithm cannot converge anywhere ‚Äî for example, it can oscillate between two fixed states (let's leave it as an exercise for the reader, building such an example is not very difficult).  On the other hand, he may still converge, but converge to the wrong answer (here non-trivial examples are more technically complex, let me give you the word of a gentleman that they exist).  However, there are a number of works in which the conditions are given under which the loopy belief propagation converges where necessary;  these conditions are often met, and even when they are not met, the algorithm often works in practice as it should. <br><br>  I will note here for the future: loopy belief propagation looks very similar to the Gibbs sampling, which we will do next time;  However, in reality there is a fundamental difference between the methods.  The main difference lies in the fact that loopy belief propagation is a deterministic algorithm, and if you are not lucky with it, then nothing will be done.  And the sampling methods are fundamentally randomized, and even if one of the sampler launches leads to incorrect results, in most other launches the case is likely to improve.  But more about that later. <br><br><h3>  Variational approximations </h3><br>  The last method of dealing with overly complex models, which I wanted to talk about today, is variational approximations.  Most likely, if you have a physical or mathematical education, you understand better than me what a variational approximation is;  but I still try to explain on the fingers.  The essence of the method is as follows: we are trying to add new degrees of freedom to the original problem and present it as an optimization problem (for new, artificially introduced parameters). <br><br>  To understand what all this means in general, let us consider the classic simplest example ‚Äî let us present the logarithm function in variational form: <br><img src="https://habrastorage.org/getpro/habr/post_images/1e3/1ea/fd3/1e31eafd39b28a29e94926fee703aec7.png"><br>  (this equality is easy to check, just taking the derivative with respect to Œª - check!). <br><br>  Now this is a family of linear functions, each of which gives an upper estimate for the logarithm (in other words, the logarithm is the envelope of this family), and if we succeed in choosing Œª well, then we get good estimates in the region of interest.  It looks like this (picture from the article [ <a href="http://citeseerx.ist.psu.edu/viewdoc/summary%3Fdoi%3D10.1.1.61.4999">Jordan et al., 1999</a> ]): <br><img src="https://habrastorage.org/storage2/b69/75d/ddf/b6975dddfbc74055b8d3cf14c8bd78ed.png"><br><br>  It was very important that the logarithm is a concave function, otherwise it would not work.  But often you can achieve the properties of the bulge in a different way.  The second example is the logistic function: <br><img src="https://habrastorage.org/getpro/habr/post_images/f34/c10/2dc/f34c102dc9504f2471acbece2743a3a6.png"><br>  It is not convex or concave, but it is log-concave, i.e. <br><img src="https://habrastorage.org/getpro/habr/post_images/80f/cd2/274/80fcd22746fbd9b2881ea84b21d315c2.png"><br>  is a concave function of <i>x</i> .  Therefore, for <i>g</i> ( <i>x</i> ) one can find linear estimates: <br><img src="https://habrastorage.org/getpro/habr/post_images/b6c/653/eb9/b6c653eb905c3fb1b620e7831f759c31.png"><br>  Where <img src="https://habrastorage.org/getpro/habr/post_images/4bf/897/71d/4bf89771dad5e9826fd949573068a083.png">  i.e.  ordinary entropy.  Again, exercise - check it out (something a bit too much exercise today, but what to do).  Then you can take the exponent (it is monotonous, ie, the inequality will remain): <br><img src="https://habrastorage.org/getpro/habr/post_images/cb6/9a5/ef1/cb69a5ef10d9af4fe7a7f4cb0f0b5e0c.png"><br>  In particular, we obtain a family of upper bounds: <br><img src="https://habrastorage.org/getpro/habr/post_images/253/196/a8b/253196a8bbccc995712759f880c8b195.png"><br>  and the better we choose Œª, the better we estimate.  Graphically, it looks like this (image from [ <a href="http://citeseerx.ist.psu.edu/viewdoc/summary%3Fdoi%3D10.1.1.61.4999">Jordan et al., 1999</a> ]): <br><img src="https://habrastorage.org/storage2/580/371/904/580371904a94ef4e4fa3e56508573e5b.png"><br><br>  What does the graphic model?  But with what.  Let me remind you that we are dealing with the problem of marginalization: using this model and, possibly, the values ‚Äã‚Äãof some variables, find the marginal probability distribution <br><img src="https://habrastorage.org/getpro/habr/post_images/b25/b71/bb3/b25b71bb3b5ee15146a25824c02cbbf7.png"><br>  where <i>X</i> and <i>Y</i> are subsets of variables (usually <i>Y</i> is a small subset, often one variable of interest).  Let us not summarize the joint distribution in the forehead (in particular, when we draw a factor graph, it turns out many cycles).  If we can find variational estimates for the joint distribution, we get an estimate <br><img src="https://habrastorage.org/getpro/habr/post_images/631/76c/ac2/63176cac23564d13f490131cfe7e7fad.png"><br>  where Œª - variation parameters.  Here it is assumed that we, of course, were not fools and chose them specifically so that now the summation was easy to carry out.  Then the result is a function of a relatively small number of variables, <i>Y</i> and Œª, and the initial task of marginalization turns into the problem of minimizing this function with respect to Œª, which is often much easier to do. <br><br>  If we look at everything that happens from a slightly different angle, then the essence of the variational approximations can be represented as follows: <br><ul><li>  we have a complex distribution, which is not clear what to do; </li><li>  we find a class of simple distributions with which it is easy and pleasant to live; </li><li>  and then we try in this class to find a distribution that is most similar to our original complex distribution (optimizing by variational parameters). </li></ul><br><br>  There are two minuses here: first, the answer is approximate, because  even with a good solution of the optimization problem, variational estimates do not always relate so successfully to the desired function, as in the pictures above, sometimes there is a gap (that is, the class of simple distributions is noticeably separated from the desired complex distribution).  But it is not so bad, no one expects an exact solution in difficult cases.  The more important problem is that variation methods are always a ‚Äúmanual‚Äù solution; you cannot automate them: you need to find a successful form of variation variation, which is easily summed up and which can then be optimized.  Such forms were found for some classes of models, but in general for every serious modification the scientific search begins anew (although, of course, not from scratch). <br><br>  So, for example, in the developed scheme of the variation approximation for the LDA model, the graphical model from the one shown above (in the first section of the article) turns into this (picture from the article [ <a href="http://citeseerx.ist.psu.edu/viewdoc/summary%3Fdoi%3D10.1.1.8.2585">Blei, Jordan, Ng, 2003</a> ]): <br><img src="https://habrastorage.org/storage2/667/1d4/a01/6671d4a01ba8350bc8e93fc229eb2f7a.png"><br><br>  As you can see, there are no longer any cycles left, and the problem of marginalization has received an approximate solution, which is reduced to an optimization problem with respect to the variational parameters Œ≥ and œÜ. <br><br>  However, for LDA there is a fairly simple Gibbs sampling scheme;  the difference here is that Gibbs sampling is in this case a very simple and straightforward method, it is easy to understand and program (more precisely, it is easy to understand <i>how</i> it works, and that‚Äôs <i>why</i> it works is a slightly more subtle question), and the variational method is more complicated, but the algorithm is more efficient as a result.  As a result, as a rule, articles on various extensions of LDA are divided into two classes: if the authors manage to find a variational approximation, they give the scheme of such an approximation and the corresponding inference algorithm;  if not, they are limited by the Gibbs sampling scheme, which, as a rule, is more or less automatic for any extensions. <br><br>  Next time, we will continue our acquaintance with inference methods in complex probabilistic models and talk about sampling, which is currently the most versatile tool for approximate inference. </div><p>Source: <a href="https://habr.com/ru/post/188812/">https://habr.com/ru/post/188812/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../188798/index.html">Startup Happy Farm - Advice Wallet attracted an investment of $ 300 thousand</a></li>
<li><a href="../188800/index.html">Mobile application developers - the key to success for Russian business</a></li>
<li><a href="../188802/index.html">Postgres 9.3 highlights of material features: materialized views</a></li>
<li><a href="../188804/index.html">Creating an IT business from scratch</a></li>
<li><a href="../188808/index.html">MEGA released its own SDK</a></li>
<li><a href="../188814/index.html">ThL W100 - a small but powerful smartphone</a></li>
<li><a href="../188816/index.html">Qt 5.1 and correct deployment on Windows</a></li>
<li><a href="../188818/index.html">Miracle Prosthesis from Holland</a></li>
<li><a href="../188820/index.html">Jiayu G3S work on bugs</a></li>
<li><a href="../188822/index.html">ETERNUS DX functionality</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>