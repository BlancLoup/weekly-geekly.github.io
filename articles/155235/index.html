<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Neural network with SoftMax c # layer</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, in the last article I talked about the error back-propagation algorithm and gave an implementation that does not depend on the error function and ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Neural network with SoftMax c # layer</h1><div class="post__text post__text-html js-mediator-article">  Hi, <a href="http://habrahabr.ru/post/154369/">in the last article</a> I talked about the error back-propagation algorithm and gave an implementation that does not depend on the error function and on the neuron activation function.  Several simple examples of the substitution of these very parameters were shown: minimizing the square of the Euclidean distance and log-likelihood for the sigmoid function and the hyperbolic tangent.  This post will be a logical continuation of the past, in which I will consider a slightly non-standard example, namely <a href="http://en.wikipedia.org/wiki/Softmax_activation_function">the Softmax activation function</a> to minimize <a href="http://en.wikipedia.org/wiki/Cross_entropy">cross-entropy</a> .  This model is relevant for the classification problem, when it is necessary to obtain at the output of a neural network the probabilities of belonging to the input image to one of the non-intersecting classes.  Obviously, the total network output for all neurons of the output layer must be equal to one (as well as for the output images of the training set).  However, it is not enough just to normalize the outputs, but to make the network model a probability distribution, and train it specifically for this.  By the way, now on <a href="https://www.coursera.org/">coursera.org</a> there is a <a href="https://class.coursera.org/neuralnets-2012-001/">course on neural networks</a> , it was he who helped to delve into understanding softmax, otherwise I would continue to use third-party implementations. <br><br><a name="habracut"></a><br><br><h4>  Using </h4><br>  I recommend to get acquainted with the <a href="http://habrahabr.ru/post/154369/">previous post</a> , since all the notation, interfaces and the learning algorithm itself are used without change here. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  Softmax activation function </h4><br>  So, the first task that we face is to provide a way to model the network with a probability distribution.  To do this, create a network of direct distribution, such that: <br><ul><li>  the network contains a number of hidden layers, all neurons can have their own activation function; </li><li>  on the last layer is the number of neurons, which corresponds to the number of classes;  All these neurons will be called softmax layer or group. </li></ul><br><br>  The softmax group neurons will have the <a href="http://en.wikipedia.org/wiki/Softmax_activation_function">following activation function</a> (in this section I will omit the layer index, which means that it is the last and contains <b>n</b> neurons): <br><br><ul><li><img src="https://habrastorage.org/storage2/0b9/576/c57/0b9576c5749c825d5b6417348e8e8d03.gif">  , for <b>i-th</b> neuron </li></ul><br><br>  It can be seen from the formula that the output of each neuron depends on the adders of all other neurons of the softmax group, and the sum of the output values ‚Äã‚Äãof the whole group is equal to one.  The beauty of this function is that the partial derivative of the i-th neuron is equal in its adder: <br><ul><li><img src="https://habrastorage.org/storage2/682/2a9/e76/6822a9e76ef0dae8763af381f1b206e4.gif"><br></li></ul><br><br>  We implement this function using the <b>IFunction</b> interface from the <a href="http://habrahabr.ru/post/154369/">previous article</a> : <br><br><div class="spoiler">  <b class="spoiler_title">Implement softmax function</b> <div class="spoiler_text"> It is worth noting that the implementation of the <code>double Compute(double x)</code> method is generally not necessary, since the calculation of the output values ‚Äã‚Äãof the group will be cheaper to do in the implementation of the softmax layer.  But for completeness, and just in case, let it be -) <br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">internal</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">SoftMaxActivationFunction</span></span> : <span class="hljs-title"><span class="hljs-title">IFunction</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> ILayer _layer = <span class="hljs-literal"><span class="hljs-literal">null</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> _ownPosition = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">internal</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">SoftMaxActivationFunction</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">ILayer layer, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> ownPosition</span></span></span><span class="hljs-function">)</span></span> { _layer = layer; _ownPosition = ownPosition; } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">double</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Compute</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">double</span></span></span></span><span class="hljs-function"><span class="hljs-params"> x</span></span></span><span class="hljs-function">)</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> numerator = Math.Exp(x); <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> denominator = numerator; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; _layer.Neurons.Length; i++) { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (i == _ownPosition) { <span class="hljs-keyword"><span class="hljs-keyword">continue</span></span>; } denominator += Math.Exp(_layer.Neurons[i].LastNET); } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> numerator/denominator; } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">double</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">ComputeFirstDerivative</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">double</span></span></span></span><span class="hljs-function"><span class="hljs-params"> x</span></span></span><span class="hljs-function">)</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> y = Compute(x); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> y*(<span class="hljs-number"><span class="hljs-number">1</span></span> - y); } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">double</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">ComputeSecondDerivative</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">double</span></span></span></span><span class="hljs-function"><span class="hljs-params"> x</span></span></span><span class="hljs-function">)</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">throw</span></span> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> NotImplementedException(); } }</code> </pre><br></div></div><br><br><h4>  Error function </h4><br>  In each training example, we will get a network output that models the probability distribution we need, and to compare two probability distributions, a correct measure is needed.  As such a measure will be used <a href="http://en.wikipedia.org/wiki/Cross_entropy">cross entropy</a> : <br><br><ul><li><img src="https://habrastorage.org/storage2/ef6/b03/f47/ef6b03f47341e28a59aa458435f086ce.gif"></li><li>  <b>t</b> - required outputs for the current training example </li><li>  <b>y</b> - real neural network outputs </li></ul><br><br>  And the total network error is calculated as: <br><img src="https://habrastorage.org/storage2/351/e05/1b3/351e051b3a0ddc6b8628124dbdc2b565.gif"><br><br>  To realize the elegance of the entire model, you need to see how the gradient is calculated from one of the output dimensions or a neuron.  In the <a href="http://habrahabr.ru/post/154369/">previous post,</a> in the ‚Äúoutput layer‚Äù section, it is described that the task is reduced to the calculation of dC / dz_i, we will continue from this point on: <br><br><ul><li><img src="https://habrastorage.org/storage2/dd2/dba/383/dd2dba383058f7ebbf0302f345c5341b.gif">  because  the output of each neuron contains the adder of the current, we have to differentiate the entire amount.  Due to the fact that the cost function depends only on the outputs of the neurons, and the outputs only on adders, it can be decomposed into two partial derivatives.  Next, we consider individually each member of the sum (the main thing is to pay attention to the indices, in our case <b>j</b> runs through the neurons of the softmax group, and <b>i</b> is the current neuron) </li><li><img src="https://habrastorage.org/storage2/b35/a28/bbc/b35a28bbc23ea1111612e1639769bca2.gif"><hr></li><li><img src="https://habrastorage.org/storage2/5ff/ac2/aaf/5ffac2aaf01487ba2b269de86b1e424e.gif"><hr></li><li><img src="https://habrastorage.org/storage2/6c3/53c/4a4/6c353c4a42269f20476664c5d7dfa69b.gif"><hr></li><li><img src="https://habrastorage.org/storage2/ea5/ad2/672/ea5ad2672759836a05e407c2024a7eeb.gif"></li></ul><br><br>  The last transformation is obtained due to the fact that the sum of the values ‚Äã‚Äãof the output vector must be equal to one, according to the property of neurons of the softmax-group.  This is an important requirement for the training set, otherwise the gradient will not be calculated correctly! <br><br>  Let us turn to the implementation using the same representation as before: <br><br><div class="spoiler">  <b class="spoiler_title">Implementation of cross entropy</b> <div class="spoiler_text"><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">internal</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">CrossEntropy</span></span> : <span class="hljs-title"><span class="hljs-title">IMetrics</span></span>&lt;<span class="hljs-title"><span class="hljs-title">double</span></span>&gt; { <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">internal</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">CrossEntropy</span></span></span><span class="hljs-function">(</span><span class="hljs-params"></span><span class="hljs-function"><span class="hljs-params"></span>)</span></span> { } <span class="hljs-comment"><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> \sum_i v1_i * ln(v2_i) </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> public override double Calculate(double[] v1, double[] v2) { double d = 0; for (int i = 0; i </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt; v1.Length; i++) { d += v1[i]*Math.Log(v2[i]); } return -d; } public override double CalculatePartialDerivaitveByV2Index(double[] v1, double[] v2, int v2Index) { return v2[v2Index] - v1[v2Index]; } }</span></span></span></span></code> </pre><br></div></div><br><br><h4>  Softmax layer </h4><br><br>  Generally speaking, a specific layer can not be done, just in the constructor of an ordinary network of direct distribution, create the last layer, with the activation function given above, and transmit to it in the designer a link to the softmax layer, but then, when calculating the output of each neuron, the denominator of the function will be calculated each time activation, but if you implement the <code>double[] ComputeOutput(double[] inputVector)</code> neural network properly: <br><pre> <code class="cs hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">double</span></span></span><span class="hljs-function">[] </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">ComputeOutput</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">double</span></span></span></span><span class="hljs-function"><span class="hljs-params">[] inputVector</span></span></span><span class="hljs-function">)</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[] outputVector = inputVector; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; _layers.Length; i++) { outputVector = _layers[i].Compute(outputVector); } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> outputVector; }</code> </pre><br><br>  then, because the network does not call the neuron's Compute method directly, but delegates this function to the layer, it can be done so that the denominator of the activation function is calculated once. <br><br><div class="spoiler">  <b class="spoiler_title">Softmax layer</b> <div class="spoiler_text"><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">internal</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">SoftmaxFullConnectedLayer</span></span> : <span class="hljs-title"><span class="hljs-title">FullConnectedLayer</span></span> { <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">internal</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">SoftmaxFullConnectedLayer</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> inputDimension, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> size</span></span></span><span class="hljs-function">)</span></span> { _neurons = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> INeuron[size]; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; size; i++) { IFunction smFunction = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> SoftMaxActivationFunction(<span class="hljs-keyword"><span class="hljs-keyword">this</span></span>, i); _neurons[i] = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> InLayerFullConnectedNeuron(inputDimension, smFunction); } } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">override</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">double</span></span></span><span class="hljs-function">[] </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Compute</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">double</span></span></span></span><span class="hljs-function"><span class="hljs-params">[] inputVector</span></span></span><span class="hljs-function">)</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[] numerators = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[_neurons.Length]; <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> denominator = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; _neurons.Length; i++) { numerators[i] = Math.Exp(_neurons[i].NET(inputVector)); denominator += numerators[i]; } <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[] output = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[_neurons.Length]; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; _neurons.Length; i++) { output[i] = numerators[i]/denominator; _neurons[i].LastState = output[i]; } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> output; } }</code> </pre><br></div></div><br><br><h4>  Total </h4><br>  So, the missing parts are ready, and you can assemble the <a href="http://ru.wikipedia.org/wiki/%25D0%259A%25D0%25BE%25D0%25BD%25D1%2581%25D1%2582%25D1%2580%25D1%2583%25D0%25BA%25D1%2582%25D0%25BE%25D1%2580_(%25D0%25B8%25D0%25B3%25D1%2580%25D1%2583%25D1%2588%25D0%25BA%25D0%25B0)">designer</a> .  For example, I use the same implementation of a direct distribution network, just with a different <a href="http://ru.wikipedia.org/wiki/%25D0%259A%25D0%25BE%25D0%25BD%25D1%2581%25D1%2582%25D1%2580%25D1%2583%25D0%25BA%25D1%2582%25D0%25BE%25D1%2580_(%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B3%25D1%2580%25D0%25B0%25D0%25BC%25D0%25BC%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5)">constructor</a> . <br><br><div class="spoiler">  <b class="spoiler_title">Constructor Example</b> <div class="spoiler_text"><pre> <code class="cs hljs"><span class="hljs-comment"><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> Creates network with softmax layer at the outlut, and hidden layes with theirs own activation functions </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> internal FcMlFfNetwork(int inputDimension, int outputDimension, int[] hiddenLayerStructure, IFunction[] hiddenLayerFunctions, IWeightInitializer wi, ILearningStrategy</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;IMultilayerNeuralNetwork&gt;</span></span></span><span class="hljs-comment"> trainingAlgorithm) { _learningStrategy = trainingAlgorithm; _layers = new ILayer[hiddenLayerFunctions.Length + 1]; _layers[0] = new FullConnectedLayer(inputDimension, hiddenLayerStructure[0], hiddenLayerFunctions[0]); for (int i = 1; i </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt; hiddenLayerStructure.Length; i++) { _layers[i] = new FullConnectedLayer(_layers[i - 1].Neurons.Length, hiddenLayerStructure[i], hiddenLayerFunctions[i]); } //create softmax layer _layers[hiddenLayerStructure.Length] = new SoftmaxFullConnectedLayer(hiddenLayerStructure[hiddenLayerStructure.Length - 1], outputDimension); for (int i = 0; i &lt; _layers.Length; i++) { for (int j = 0; j &lt; _layers[i].Neurons.Length; j++) { _layers[i].Neurons[j].Bias = wi.GetWeight(); for (int k = 0; k &lt; _layers[i].Neurons[j].Weights.Length; k++) { _layers[i].Neurons[j].Weights[k] = wi.GetWeight(); } } } }</span></span></span></span></code> </pre><br></div></div></div><p>Source: <a href="https://habr.com/ru/post/155235/">https://habr.com/ru/post/155235/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../155221/index.html">AWS Global Start-Up Challenge</a></li>
<li><a href="../155223/index.html">Crowd funding without using third-party services on the example of Star Citizen</a></li>
<li><a href="../155225/index.html">AMQP. Sending messages to RabbitMQ directly from Nginx (embedded Perl)</a></li>
<li><a href="../155227/index.html">Meteor 0.5.0: Authentication, OAuth, SRP</a></li>
<li><a href="../155233/index.html">Lenovo ThinkPad X1 Carbon Ultrabook Video Review</a></li>
<li><a href="../155239/index.html">Google calls on new Android</a></li>
<li><a href="../155241/index.html">Development just for fun at the weekend or save habr for yourself</a></li>
<li><a href="../155245/index.html">Philosophy of selling videos</a></li>
<li><a href="../155247/index.html">Overview of the flagship smartphone Samsung GALAXY Note II</a></li>
<li><a href="../155249/index.html">ZTE V970 Grand Smartphone Review</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>