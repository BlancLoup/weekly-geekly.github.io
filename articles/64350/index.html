<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The organization of distributed disk storage with the possibility of unlimited expansion using the technology of LVM and ATAoE</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Task 
 When the disks were small and the Internet was large, the owners of private FTP servers faced the following problem: 
 On each hard disk, a dad...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The organization of distributed disk storage with the possibility of unlimited expansion using the technology of LVM and ATAoE</h1><div class="post__text post__text-html js-mediator-article"><h1>  Task </h1><br>  When the disks were small and the Internet was large, the owners of private FTP servers faced the following problem: <br>  On each hard disk, a daddy Video or Soft was created, and it turned out that adding a new hard disk, you had to make daddy Video2, Soft2, etc. on it. <br>  The task of changing the hard disk to a larger disk meant that the data needed to be transferred somewhere, it all happened nontrivially and with large downtime. <br>  The system we developed in 2005 allowed us to assemble a reliable and fast array of 3 terabytes, scalable, expandable, online, adding disks or entire servers with disks. <br>  The price of the entire solution was 110% of the cost of the disks themselves, i.e.  in essence, free, with a slight overhead. <br><br>  Here is an example diagram of our storage device: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9ff/83d/e5a/9ff83de5a1cf7e955ad699f8e8ab1e40.jpg"><br><a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1>  Implementation </h1><br>  The idea is this: there is a supervisor and there are nodes.  Supervisor is a public server that clients log in to, it has several gigabit bonding interfaces outside, and a few inside, to our nodes.  The supervisor takes arrays or individual disks exported with vblade via ATAoE, makes LVM over them and makes this section available via FTP.  The supervisor is also a diskless boot server for the nodes, and it also has the entire node file system with which they boot via NFS after the download.  Nodes are pure disks, boot by PXE, then our etherpopulate starts and all disks are exported. <br><br><h2>  1. Setup for remote node loading </h2><br><br>  device node <br> <code>ftp # ls /diskless/ <br> bzImage node pxelinux.0 pxelinux.cfg</code> <br> <br>  kernel, node directory - rootfs, configs for pxe <br> <code>ftp # ls /diskless/node/ <br> bin boot dev etc lib mnt proc root sbin sys tmp usr var <br> ftp # chroot /diskless/node/ <br> ftp / # which vblade <br> /usr/sbin/vblade <br> ftp / # vblade <br> usage: vblade [ -m mac[,mac...] ] shelf slot netif filename <br> ftp / #</code> <br> <br>  all this will be available on nodes <br> <code>ftp node # cat /diskless/pxelinux.cfg/default <br> DEFAULT /bzImage <br> APPEND ip=dhcp root=/dev/nfs nfsroot=172.18.0.193:/diskless/node idebus=66</code> <br>  config for pxe, nfsroot specified <br><br>  dhcpd config, do not forget to run. <br> <code>ftp etc # more dhcp/dhcpd.conf <br> option domain-name "domain.com"; <br> default-lease-time 600; <br> max-lease-time 7200; <br> ddns-update-style none; <br> <br> option space PXE; <br> option PXE.mtftp-ip code 1 = ip-address; <br> option PXE.mtftp-cport code 2 = unsigned integer 16; <br> option PXE.mtftp-sport code 3 = unsigned integer 16; <br> option PXE.mtftp-tmout code 4 = unsigned integer 8; <br> option PXE.mtftp-delay code 5 = unsigned integer 8; <br> option PXE.discovery-control code 6 = unsigned integer 8; <br> option PXE.discovery-mcast-addr code 7 = ip-address; <br> <br> subnet 172.16.0.0 netmask 255.255.0.0 { <br> } <br> <br> subnet 172.18.0.192 netmask 255.255.255.192 { <br> class "pxeclients" { <br> match if substring (option vendor-class-identifier, 0, 9) = "PXEClient"; <br> option vendor-class-identifier "PXEClient"; <br> vendor-option-space PXE; <br> option PXE.mtftp-ip 0.0.0.0; <br> filename "pxelinux.0"; <br> next-server 172.18.0.193; <br> } <br> <br> host node-1 { <br> hardware ethernet 00:13:d4:68:b2:7b; <br> fixed-address 172.18.0.194; <br> } <br> host node-2 { <br> hardware ethernet 00:11:2f:45:e9:fd; <br> fixed-address 172.18.0.195; <br> } <br> host node-3 { <br> hardware ethernet 00:07:E9:2A:A9:AC; <br> fixed-address 172.18.0.196; <br> } <br> }</code> <br> <br>  tftpd config <br> <code>ftp etc # more /etc/conf.d/in.tftpd <br> # /etc/init.d/in.tftpd <br> <br> # Path to server files from <br> INTFTPD_PATH="/diskless" <br> <br> INTFTPD_USER="nobody" <br> # For more options, see tftpd(8) <br> INTFTPD_OPTS="-u ${INTFTPD_USER} -l -vvvvvv -p -c -s ${INTFTPD_PATH}"</code> <br> <br>  is tftpd running? <br> <code><a href="http://procps.sf.net/faq.html"></a> ftp etc # ps -ax |grep tft <br> Warning: bad ps syntax, perhaps a bogus '-'? See procps.sf.net/faq.html <br> 5694 ? Ss 0:00 /usr/sbin/in.tftpd -l -u nobody -l -vvvvvv -p -c -s /diskless <br> 31418 pts/0 R+ 0:00 grep tft</code> <br> <br>  config for nfs, do not forget to run. <br> <code>ftp etc # more exports <br> /diskless/node 172.18.0.192/255.255.255.192(rw,sync,no_root_squash)</code> <br> <br>  Setup for remote boot is complete, all nodes are registered. <br><br><h2>  2. The software part to automate the assembly of arrays </h2><br><br>  the software that runs on the nodes makes md * raid arrays and exports them ataoe to the supervisor. <br> <code>ftp# chroot /diskless/node <br> ftp etc # more /usr/sbin/etherpopulate <br> #!/usr/bin/perl <br> <br> my $action = shift(); <br> <br> #system('insmod /lib/modules/vb-2.6.16-rc1.ko') <br> # if ( -f '/lib/modules/vb-2.6.16-rc1.ko'); <br> <br> # Get information on node_id's of ifaces <br> my @ifconfig = `ifconfig`; <br> my $int; <br> my %iface; <br> foreach my $line (@ifconfig) { <br> if ($line =~ /^(\S+)/) { <br> $int = $1; <br> } <br> if ($line =~ /inet addr:(\d+\.\d+\.\d+\.)(\d+)/ &amp;&amp; $1 ne '127.0.0.' &amp;&amp; $int) { <br> $iface{$int} = $2; <br> $int = ""; <br> } <br> } <br> <br> my $vblade_kernel = ( -d "/sys/vblade" )?1:0; <br> if ( $vblade_kernel ) { <br> print " Using kernelspace vblade\n" if ($action eq "start"); <br> } else { <br> print " Using userspace vblade\n" if ($action eq "start"); <br> } <br> <br> # Run vblade <br> foreach my $int (keys %iface) { <br> my $node_id = $iface{$int}; <br> open(DATA, "/etc/etherpopulate.conf"); <br> while () { <br> chomp; <br> s/#.*//; <br> s/^\s+//; <br> s/\s+$//; <br> next unless length; <br> <br> if ($_ =~ /^node-$node_id\s+(\S+)\s+(\S+)\s+(\S.*)/) { <br> my $cfg_action = $1; <br> my $command = $2; <br> my $parameters = $3; <br> <br> # Export disk over ATAoE <br> if ($action eq $cfg_action &amp;&amp; $command eq "ataoe" &amp;&amp; $parameters =~ /(\S+)\s+(\d+)/) { <br> my $disk_name = $1; <br> my $disk_id = $2; <br> if ($vblade_kernel) { <br> if ( $disk_name =~ /([a-z0-9]+)$/ ) { <br> my $disk_safe_name = $1; <br> system("echo 'add $disk_safe_name $disk_name' &gt; /sys/vblade/drives"); <br> system("echo 'add $int $node_id $disk_id' &gt; /sys/vblade/$disk_safe_name/ports"); <br> } <br> } else { <br> system("/sbin/start-stop-daemon --background --start --name 'vblade_$node_id_$disk_id' --exec /usr/sbin/vblade $node_id $disk_id eth0 $disk_name"); <br> } <br> print " Exporting disk: $disk_name [ $node_id $disk_id ] via $int\n"; <br> } <br> <br> # Execute specified command <br> if ($action eq $cfg_action &amp;&amp; $command eq "exec") { <br> system($parameters); <br> } <br> } <br> } <br> close(DATA); <br> } <br> <br></code> <br><br>  config for etherpopulate with the participation of three nodes.  two more add.  drives from each node are exported for other purposes (backup on raid1) <br> <code>ftp sbin # more /diskless/node/etc/etherpopulate.conf <br> # ---------------------- <br> # Node 194 160gb <br> node-194 start exec /sbin/mdadm -A /dev/md0 -f /dev/hd[ah] /dev/hdl <br> node-194 start ataoe /dev/md0 0 # Vblade FTP array <br> node-194 start ataoe /dev/hdk 1 # Vblade BACKUP disk <br> node-194 stop exec /usr/bin/killall vblade <br> node-194 stop exec /sbin/mdadm -S /dev/md0 <br> <br> # ---------------------- <br> # Node 195 200 gb <br> node-195 start exec /sbin/mdadm -A /dev/md0 /dev/hd[ab] /dev/hd[ef] /dev/hd[gh] /dev/sd[ac] <br> node-195 start ataoe /dev/md0 0 # Vblade FTP array <br> node-195 start ataoe /dev/sdd 1 # Vblade BACKUP disk <br> node-195 stop exec /usr/bin/killall vblade <br> node-195 stop exec /sbin/mdadm -S /dev/md0 <br> <br> # ---------------------- <br> # Node 196 200 gb <br> node-196 start exec /sbin/mdadm -A /dev/md0 /dev/hd[af] <br> node-196 start ataoe /dev/md0 0 # Vblade FTP array <br> node-196 stop exec /usr/bin/killall vblade <br> node-196 stop exec /sbin/mdadm -S /dev/md0</code> <br> <br><h2>  3. Final work </h2><br><br>  make the screws on the nodes work at maximum speed to the detriment of reliability <br> <code>hd*_args="-d1 -X69 -udma5 -c1 -W1 -A1 -m16 -a16 -u1"</code> <br> <br>  Make sure the kernel for the supervisor.  On the nodes themselves, export to ATAoE takes place in userland, using vblade. <br> <code>ftp good # grep -i OVER_ETH .config <br> CONFIG_ATA_OVER_ETH=y</code> <br> <br>  on the nodes themselves immediately after loading and launching etherpopulate in accordance with the config. <br> <code><a href="http://procps.sf.net/faq.html"></a> node-195 ~ # cat /proc/mdstat <br> Personalities : [linear] [raid0] [raid1] [raid5] [raid4] [raid6] [multipath] [faulty] <br> md0 : active raid5 hda[0] sdc[8] sdb[7] sda[6] hdh[5] hdg[4] hdf[3] hde[2] hdb[1] <br> 1562887168 blocks level 5, 64k chunk, algorithm 2 [9/9] [UUUUUUUUU] <br> <br> unused devices: node-195 ~ # ps -ax | grep vblade | grep md <br> Warning: bad ps syntax, perhaps a bogus '-'? See procps.sf.net/faq.html <br> 2182 ? Ss 2090:41 /usr/sbin/vblade 195 0 eth0 /dev/md0 <br> <br> node-195 ~ # mount <br> rootfs on / type rootfs (rw) <br> /dev/root on / type nfs (ro,v2,rsize=4096,wsize=4096,hard,nolock,proto=udp,addr=172.18.0.193) <br> proc on /proc type proc (rw) <br> sysfs on /sys type sysfs (rw) <br> udev on /dev type tmpfs (rw,nosuid) <br> devpts on /dev/pts type devpts (rw) <br> none on /var/lib/init.d type tmpfs (rw) <br> shm on /dev/shm type tmpfs (rw,nosuid,nodev,noexec) <br> <br></code> <br><br>  we collect lvm from disks at the supervisor, in the future we don‚Äôt need to do this, just by vgscan there will be a partition ready for mounting <br><br> <code>ftp / # ls -la /dev/etherd/* <br> cw--w---- 1 root disk 152, 3 Jun 7 2008 /dev/etherd/discover <br> brw-rw---- 1 root disk 152, 49920 Jun 7 2008 /dev/etherd/e194.0 <br> brw-rw---- 1 root disk 152, 49936 Jun 7 2008 /dev/etherd/e194.1 <br> brw-rw---- 1 root disk 152, 49920 Jun 7 2008 /dev/etherd/e195.0 <br> brw-rw---- 1 root disk 152, 49936 Jun 7 2008 /dev/etherd/e195.1 <br> brw-rw---- 1 root disk 152, 49920 Jun 7 2008 /dev/etherd/e196.0 <br> cr--r----- 1 root disk 152, 2 Jun 7 2008 /dev/etherd/err <br> cw--w---- 1 root disk 152, 4 Jun 7 2008 /dev/etherd/interfaces</code> <br> <br>  From the first two nodes, 1 array and 1 disk were exported, and only the array from the third node. <br>  Before you can use these devices on the LVM Supervisor, you need to do ‚Äúspecial‚Äù markup so that LVM adds some internal identifiers to the disk. <br><br> <code># pvcreate /dev/etherd/e194.0 <br> ... <br> ...</code> <br> <br>  Disks are ready to use.  We create Volume Group. <br> <code># vgcreate cluster /dev/etherd/e194.0 /dev/etherd/e195.0 /dev/etherd/e196.0</code> <br> <br>  Although the group becomes active immediately, in principle it can be included <br> <code># vgchange -ay cluster</code> <br> <br>  on and off <br> <code># vgchange -an cluster</code> <br> <br>  To add something to the volume group use <br> <code># vgextend cluster /dev/*...</code> <br> <br>  Create a Logical Volume hyperspace for all available space.  Each PE defaults to 4mb.  So <br> <code># vgdisplay cluster | grep "Total PE" <br> Total PE 1023445 <br> # lvcreate -l 1023445 cluster -n hyperspace</code> <br> <br>  See what happened you can vgdisplay, lvdisplay, pvdisplay. <br>  You can extend everything with vgextend, lvextend, resize_reiserfs. <br>  Read more here <a href="http://tldp.org/HOWTO/LVM-HOWTO/">http://tldp.org/HOWTO/LVM-HOWTO/</a> <br><br>  We end up with / dev / cluster / hyperspace and make it mkreiserfs and mount.  All is ready.  Setting the FTP server is omitted.  TA-dah! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f8e/2d1/924/f8e2d1924a79c37156b2ecd81407fd8f.jpg"><br><br><h3>  Reuse </h3><br>  On the supervisor itself, if it is restarted, it is sufficient to perform <br> <code>more runme.sh <br> #!/bin/sh <br> vgscan <br> vgchange -ay <br> mount /dev/cluster/hyperspace /mnt/ftp</code> <br>  to use a pre-created array. <br><br><h1>  disadvantages </h1><br><ul><li>  specifically in our case, the error was with the choice of the hard drives themselves.  For some reason, the choice fell on Maxtor and almost the entire batch of 30 disks in a year went bad; </li><li>  hot swapping was not used, since  it was still an IDE.  In the case of hotplug SATA, it would be necessary at the mdadm level on the nodes themselves to set up a notification about the failure of the drives; </li><li>  proftpd needs to be run only after lvm from ataoe devices is mounted to the file system of the supervisor.  if proftpd was launched earlier, then he did not understand what had happened at all; </li><li>  They experimented with nuclear and userspace vblade on nodes for a long time, but then it was the dawn of ataoe and everything worked as lucky.  but it worked; </li><li>  either reiserfs or xfs can be used as a file system - only they supported online resizing at that time if the disk under them increased; </li><li>  then patches just started appearing that allowed raid-5 to expand the md array online </li><li>  there was a limit on ataoe of 64 slots per "shelf".  Shelves could make 10 pieces, i.e., in principle, there were some restrictions, such as 640 GCD :) </li><li>  There are many nuances with performance, but they can all be solved to one degree or another.  in short - do not be afraid, when at first the speed is not very, there is no limit to perfection; </li></ul><br><br><h1>  findings </h1><br>  The solution is certainly interesting, and I want to make it already on terabyte screws, hotplug sata and with new fresh versions of software.  but who will go on such a feat is unknown.  Maybe you% username%? <br><br><h1>  Related Links </h1><br>  <a href="http://tldp.org/HOWTO/LVM-HOWTO/">http://tldp.org/HOWTO/LVM-HOWTO/</a> <br>  <a href="http://sourceforge.net/search/%3Fwords%3Dataoe%26type_of_search%3Dsoft%26pmode%3D0%26words%3Dvblade%26Search%3DSearch">http://sourceforge.net/search/?words=ataoe&amp;type_of_search=soft&amp;pmode=0&amp;words=vblade&amp;Search=Search</a> <br>  <a href="">http://www.gentoo.org/doc/en/diskless-howto.xml</a> <br><br>  <a href="http://www.futurecolors.ru/blog/2009/07/ataoe_raid_lvm_ftp_howto/">Original article</a> <br><br>  PS Special thx to <a href="https://habrahabr.ru/users/029ah/" class="user_link">029ah</a> for writing scripts. </div><p>Source: <a href="https://habr.com/ru/post/64350/">https://habr.com/ru/post/64350/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../64336/index.html">OperaMini and Javascript</a></li>
<li><a href="../64337/index.html">ZendX_JQuery + jqGrid</a></li>
<li><a href="../64345/index.html">PDA from GPS</a></li>
<li><a href="../64348/index.html">Calendar UNIQLO</a></li>
<li><a href="../64349/index.html">Talking context translator of sites</a></li>
<li><a href="../64353/index.html">nekobeans - cool icons for netbeans</a></li>
<li><a href="../64355/index.html">The new TwittARound client shows where the Twitterers are, right on top of the camera image.</a></li>
<li><a href="../64359/index.html">Notes on Python Metaprogramming</a></li>
<li><a href="../64361/index.html">Accounting for time spent</a></li>
<li><a href="../64362/index.html">Suse studio beta</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>