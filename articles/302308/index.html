<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How we made a system for separating information from the text in natural language for Bank Bank CenterCredit JSC (Kazakhstan)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Some time ago, we were approached by a representative of Bank CenterCredit JSC (Kazakhstan) with an interesting task. It was necessary to integrate in...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How we made a system for separating information from the text in natural language for Bank Bank CenterCredit JSC (Kazakhstan)</h1><div class="post__text post__text-html js-mediator-article">  Some time ago, we were approached by a representative of Bank CenterCredit JSC (Kazakhstan) with an interesting task.  It was necessary to integrate into the data processing pipeline, which is a natural language text, an additional processing tool.  We cannot disclose all the details of the project, as it is in the field of bank security and is being developed by its security service.  In covering the technological aspects of the task and how to implement them, the customer was not opposed to what we actually want to do in this article. <br><br>  In general, the task was to extract some entities from a large array of texts.  Not very different problem from the classical task of extracting named entities, on the one hand.  But the definitions of entities differed from the usual ones and the texts were quite specific, and the time for solving the problem was two weeks. <br><a name="habracut"></a><br>  <b>Input data</b> <br>  At that time, there were no marked and accessible packages of named entities in Russian at all.  And if they were, they would have turned out to be not quite those entities.  Or at all those entities.  There were similar solutions, but they turned out to be either bad (they found, for example, organizations such as the ‚ÄúBack Cover‚Äù in the sentence ‚Äúafter a year of operation the Back Cover broke‚Äù) or very good, but they do not highlight what is needed (there were requirements concerning exactly which entities need to be allocated with respect to the boundaries of these entities). <br><br>  <b>Beginning of work</b> <br>  The data had to mark themselves.  From the specific texts provided, a training set was created.  During the week, with the efforts of one and a half diggers, we managed to mark out a sample of 112,000 words containing about 9,000 references to the necessary entities.  After training several classifiers on a validation sample, we obtained the following: <br><table><tbody><tr><td>  <b>Method</b> </td><td>  <b>F1</b> </td></tr><tr><td>  CRF (basic feature set) </td><td>  67.5 </td></tr><tr><td>  Elman's Bidirectional Multilayer Network <br></td><td>  68.5 </td></tr><tr><td>  Bidirectional LSTM <br></td><td>  74.5 </td></tr></tbody></table><br>  For entities that are simple in content, this is not very good; on comparable tasks, specialized systems often give out F1 around 90-94 (according to published works).  But then on samples of more than a million word forms and subject to a careful selection of characters. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In the preliminary results, the LSTM model showed itself best of all, with a large margin.  But I didn‚Äôt really want to use it, because it is relatively slow, it is expensive to process large arrays of text in real time.  By the time of receipt of the marked sample, and preliminary results, a week remained before the deadline. <br><br>  <b>Day 1.</b>  <b>Regularization</b> <br>  The main problem of neural networks in small samples is retraining.  Classically, this can be fought by selecting the correct size of the net, or using special regularization methods. <br><br>  We tried sizing, max-norm regularization on different layers, with selection of values ‚Äã‚Äãof constants and dropout.  Got red eyes, a headache and a couple of percent winnings. <br><table><tbody><tr><td>  <b>Method</b> </td><td>  <b>F1</b> </td></tr><tr><td>  Reduce network size to optimal <br></td><td>  69.3 </td></tr><tr><td>  Max-norm <br></td><td>  71.1 </td></tr><tr><td>  Dropout <br></td><td>  69.0 </td></tr></tbody></table><br>  Dropout didn‚Äôt help us much, the network is learning slower, and the result is not particularly good.  The best was Max-norm and network resizing.  But the increase is small, to the desired values ‚Äã‚Äãas to the moon, and all that can be kind of done. <br><br>  <b>Day # 2.</b>  <b>Rectified Linear <s>Pain</s> Unit</b> <br>  Articles recommend using the RelU activation function.  Written to improve results.  RelU is a simple function if x&gt; 0 then x else 0. <br><br>  Nothing has improved.  Optimization with them does not converge at all or the result is terrible.  I spent the day trying to understand why.  Not understood. <br><br>  <b>Day # 3.</b>  <b>LSTM-like monsters</b> <br>  Is it possible to make it so that it is like LSTM, but on ordinary layers?  After some reflection, the imagination prompted the construction (Fig. 1).  Before the recurrent layer, one feed-forward layer has been added (it should of course control what information goes to the network), and on top there is a design for controlling the output: <br><br><img src="https://habrastorage.org/files/2fc/aed/fe9/2fcaedfe9bd94e4c90ceb9caf7803ba1.png"><br>  figure 1. The architecture of a special neural network to highlight terms from the text <br><br>  Oddly enough, with the right choice of parameters, this design gave an increase of F1 to 72.2, which is not bad for a completely invented architecture. <br><br>  <b>Day # 4.</b>  <b>RelU Returns</b> <br>  Out of stubbornness tried RelU on the ‚Äúmonster‚Äù.  It turned out that if RelU was installed only on a recurrent layer, then the optimization not only converges, but it turns out F1 73.8!  Where does this miracle come from? <br><br>  Let's see.  Why does LSTM work well?  This is usually explained by the fact that he can memorize information for a longer time, and thus ‚Äúsee‚Äù more of the context.  In principle, a regular RNN can also be trained to remember a long context if one uses the appropriate learning algorithm.  But, as applied to our problem of marking up a sequence with input vector words, a regular RNN first looks for dependencies in the current time interval and in the near context, and manages to retrain them before the training reaches the possibility of a meaningful analysis of this context.  In the usual RNN Ellman, we cannot increase the ‚Äúmemory size‚Äù without significantly increasing the network's ability to retrain. <br><br>  If we look at the picture of the new architecture, we will see that here we have smashed the module that stores information and the ‚Äúcrucial‚Äù module.  At the same time, the memory module itself is unable to build complex hypotheses, so it can be increased without the fear that it will make a significant contribution to retraining.  By this we were able to control the degree of relative importance of memory and information in the current window for a specific task. <br><br>  <b>Day # 5.</b>  <b>Diagonal elements</b> <br>  Following the idea described in [1], we excluded from the recurrent layer all recurrent connections except from other neurons, leaving only its own previous state to the input of each neuron.  In addition, we added made the top layer also recurrent (Fig. 2).  This gave us F1 74.8, which, on this task, was a better result than we were able to first get with the help of LSTM. <br><br>  <b>Day # 6.</b>  <b>Sample size</b> <br>  Since we continued to mark the data throughout the week, on this day we proceeded to use a double sample of double size, which allowed (after a new round of selection of hyperparameters) to get F1 83.7.  There is nothing better than a larger sample when it is easy to get it.  True to double the amount of marked data is usually not easy at all.  This is what we end up with: <br><table><tbody><tr><td>  <b>Method</b> </td><td>  <b>F1</b> </td></tr><tr><td>  CRF (basic feature set) </td><td>  76.1 </td></tr><tr><td>  Elman's Bidirectional Multilayer Network <br></td><td>  77.8 </td></tr><tr><td>  Bidirectional LSTM <br></td><td>  83.2 </td></tr><tr><td>  Our architecture <br></td><td>  83.7 </td></tr></tbody></table><br>  <b>Conclusions and comparison with analogues</b> <br>  It is impossible to adequately compare our system of recognition with similar implementations of the above-mentioned web-API, since the definitions of the entities and boundaries differ.  We did some very rough analysis, on a small test sample, trying to put all systems in equal conditions.  We had to manually analyze the result according to special rules, using the binary overlap metric (the definition of an entity counts if the system selects at least part of it, which removes the problem of boundary mismatch) and eliminates cases from the analysis when the entities did not need to be identified due to mismatched definitions.  This is what happened: <br><table><tbody><tr><td>  <b>Method</b> </td><td>  <b>F1</b> </td></tr><tr><td>  Our system </td><td>  76.1 </td></tr><tr><td>  Analog # 1 <br></td><td>  77.8 </td></tr><tr><td>  Analog # 2 <br></td><td>  83.2 </td></tr></tbody></table><br>  Analog # 2 has only a slight advantage in this metric, and Analog # 1 has shown itself even worse.  Both solutions will produce lower quality results if we test them on our task with the specifications specified by the customer. <br><br>  From all of the above, we made two conclusions: <br>  1. Even well-defined and solved problems of extracting named entities have sub-options that can make the use of ready-made systems impossible. <br>  2. The use of neural networks allows you to quickly create specialized solutions that are approximately in the same range of quality as the more complex developments. <br><br>  <b>Literature</b> <br>  1. T. Mikolov, A. Joulin, S. Chopra, M. Mathieu, and M. Ranzato.  Learning longer memory in <br>  recurrent neural networks </div><p>Source: <a href="https://habr.com/ru/post/302308/">https://habr.com/ru/post/302308/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../302298/index.html">I invite you to a virtual reality party VR-Today</a></li>
<li><a href="../302300/index.html">Storage Market Perspectives</a></li>
<li><a href="../302302/index.html">Angular Attack: my experience of participating in the hackathon</a></li>
<li><a href="../302304/index.html">Unreal Engine Development Digest</a></li>
<li><a href="../302306/index.html">Forecast of e-commerce industry for 2016</a></li>
<li><a href="../302310/index.html">Attackers use the Burger King theme to send spam to WhatsApp</a></li>
<li><a href="../302312/index.html">Tony Robbins seminar review ‚ÄúFree your inner strength‚Äù. Day 2: Turning dreams into reality (part 1)</a></li>
<li><a href="../302316/index.html">DotNext 2016 Piter: Full Stack .NET conference</a></li>
<li><a href="../302320/index.html">How analytics find another good analyst</a></li>
<li><a href="../302322/index.html">How to promote video content? Review of the best video marketing platforms</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>