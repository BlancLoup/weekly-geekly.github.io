<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Deep learning and Caffe on New Year's holidays</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Motivation 
 In this article you will learn how to apply deep learning in practice. The Caffe framework will be used on the SVHN dataset . 

 Deep Lea...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Deep learning and Caffe on New Year's holidays</h1><div class="post__text post__text-html js-mediator-article"><h2>  Motivation </h2><br>  In this article you will learn how to apply deep learning in practice.  The <a href="http://caffe.berkeleyvision.org/">Caffe</a> framework will be used on the SVHN <a href="http://ufldl.stanford.edu/housenumbers/">dataset</a> . <br><br>  Deep Learning.  This buzz word has long been ringing in the ears, but it could not be tried in practice.  Turned up a chance to fix it!  On New Year's holidays, <a href="https://inclass.kaggle.com/c/svhn-mipt2">a kaggle contest on house</a> number recognition was appointed as part of an image analysis course. <br><a name="habracut"></a><br>  A part of the well-known <a href="http://ufldl.stanford.edu/housenumbers/">SVHN</a> sample was <a href="http://ufldl.stanford.edu/housenumbers/">given</a> , consisting of 73257 images in the training and 26032 in the test (unlabeled) samples.  Only 10 classes for each digit.  The image is 32x32 in RGB color space.  As the <a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html">benchmark</a> shows, methods based on deep learning show accuracy higher than that of a person - 1.92% vs. 2% error! <br><br>  I had experience with SVM and Naive Bayes based machine learning algorithms.  It is boring to use already known methods, so I decided to use something from deep learning, namely a <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B2%25D1%2591%25D1%2580%25D1%2582%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C">convolutional neural network</a> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  Caffe selection </h2><br>  There are many different libraries and frameworks for working with deep neural networks.  My criteria were: <br><ol><li>  tutorials </li><li>  ease of learning, </li><li>  ease of deployment, </li><li>  active community. </li></ol><br>  Caffe suited them perfectly: <br><ol><li>  Good tutorials are on their <a href="http://caffe.berkeleyvision.org/">website</a> .  Separately, I recommend lectures from the <a href="http://courses.cs.tau.ac.il/Caffe_workshop/Bootcamp/">Caffe Summer Bootcamp</a> .  For a quick start, you can read about the <a href="https://sites.google.com/site/deeplearningcvpr2014/">foundations of neural networks</a> and then <a href="https://docs.google.com/presentation/d/1UeKXVgRvvxg9OUdh_UiC5G71UMscNPlvArsWER41PsU/edit">about Caffe</a> . <br></li><li>  To start working with Caffe, you don‚Äôt even need a programming language.  Caffe is configured using configuration files, and is launched from the command line. <br></li><li>  For deployment, there are <a href="https://github.com/robomakery/caffe-cookbook">chef-kukbook</a> and <a href="https://registry.hub.docker.com/u/tleyden5iwx/caffe-cpu-master/">docker-images</a> . <br></li><li>  On <a href="https://github.com/BVLC/caffe/">github</a> is under active development, and in the <a href="https://groups.google.com/forum/">Google group,</a> you can ask a question about using the framework. <br></li></ol><br>  In addition, Caffe is very fast, because  uses a GPU (although you can do without the CPU). <br><br><h2>  Installation </h2><br>  Initially, I installed Caffe on my laptop using <a href="https://registry.hub.docker.com/u/tleyden5iwx/caffe-cpu-master/">docker</a> and ran it in CPU mode.  Neural network training was very slow, but there was nothing to compare with and it seemed to be normal. <br><br>  Then I came across an <a href="http://habrahabr.ru/post/243757/">$ 25 Amazon coupon</a> and decided to try on AWS <a href="http://aws.amazon.com/ru/ec2/instance-types/">g2.2xlarge</a> with NVIDIA GPU and CUDA support.  There unfolded the Caffe with the help of <a href="https://github.com/robomakery/caffe-cookbook">Chef</a> .  As a result, it turned out 41 times faster - on the CPU 100 iterations took place in 290 seconds, on the GPU with CUDA in 7 seconds! <br><br><h2>  Neural network architecture </h2><br>  If it was necessary to form a good feature vector in machine learning algorithms in order to obtain an acceptable quality, then this is not necessary in convolutional neural networks.  The main thing is to come up with a good network architecture. <br><br>  We introduce the following notation: <br><ul><li>  input - the input layer, usually the image pixels, </li><li>  conv - convolution layer [ <a href="http://ufldl.stanford.edu/tutorial/supervised/FeatureExtractionUsingConvolution/">1</a> ], </li><li>  pool - subsample layer [ <a href="http://ufldl.stanford.edu/tutorial/supervised/Pooling/">2</a> ], </li><li>  fully-conn - fully connected layer [ <a href="http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/">3</a> ], </li><li>  output - output layer, gives the estimated class of the image. </li></ul><br>  For the problem of image classification, the following NA architecture is the main one: <br><pre><code class="markdown hljs">input -&gt; conv -&gt; pool -&gt; conv -&gt; pool -&gt; fully-conn -&gt; fully-conn -&gt; output</code> </pre>  The number (conv -&gt; pool) of layers may be different, but usually not less than 2x.  The number of fully-conn is not less than 1. <br><br>  In the context of this contest, several architectures were tried.  I received the greatest accuracy with the following: <br><pre> <code class="markdown hljs"> input -&gt; conv -&gt; pool -&gt; conv -&gt; pool -&gt; conv -&gt; pool -&gt; fully-conn -&gt; fully-conn -&gt; output</code> </pre><br><br><h2>  Caffe architecture implementation </h2><br>  Caffe is configured using Protobuf files.  The implementation architecture for the contest is <a href="https://gist.github.com/sld/6ecd597d455b62ef5d36">here</a> .  Consider the key points of the configuration of each layer. <br><br><h4>  Input layer (input) </h4><br><div class="spoiler">  <b class="spoiler_title">Input Layer Configuration</b> <div class="spoiler_text"><pre> <code class="markdown hljs">name: "WinnyNet-F" layers { name: "svhn-rgb" type: IMAGE<span class="hljs-emphasis"><span class="hljs-emphasis">_DATA top: "data" top: "label" image_</span></span>data<span class="hljs-emphasis"><span class="hljs-emphasis">_param { source: "/home/deploy/opt/SVHN/train-rgb-b.txt" batch_</span></span>size: 128 shuffle: true } transform<span class="hljs-emphasis"><span class="hljs-emphasis">_param { mean_</span></span>file: "/home/deploy/opt/SVHN/svhn/winny<span class="hljs-emphasis"><span class="hljs-emphasis">_net5/mean.binaryproto" } include: { phase: TRAIN } } layers { name: "svhn-rgb" type: IMAGE_</span></span>DATA top: "data" top: "label" image<span class="hljs-emphasis"><span class="hljs-emphasis">_data_</span></span>param { source: "/home/deploy/opt/SVHN/test-rgb-b.txt" batch<span class="hljs-emphasis"><span class="hljs-emphasis">_size: 120 } transform_</span></span>param { mean<span class="hljs-emphasis"><span class="hljs-emphasis">_file: "/home/deploy/opt/SVHN/svhn/winny_</span></span>net5/mean.binaryproto" } include: { phase: TEST } } ...</code> </pre><br></div></div><br>  The first 2 layers (for the training and test phases) are of type: IMAGE_DATA, i.e.  network at the entrance takes images.  The images are listed in a <a href="https://gist.github.com/sld/5cb766398471c63903f3">text file</a> , where 1 column is the path to the image, 2 column is the class.  The path to the text file is specified in the image_data_param attribute. <br><br>  In addition to images, you can feed data from <a href="http://ru.wikipedia.org/wiki/Hierarchical_Data_Format">HDF5</a> , LevelDB and lmbd to the input.  The last 2 options are especially relevant if the speed of work is critical.  Thus, Caffe can work with any data, not just images.  The easiest way to work with IMAGE_DATA, so he was chosen for the contest. <br><br>  Also, input layers can include the transform_param attribute.  It specifies the transformations to which the input data should be subjected.  Usually, before submitting images to a neural network, they are normalized or more cunning operations are performed, for example, <a href="http://www.frontiersincomputervision.com/slides/FCV_Learn_LeCun.pdf">Local Contrast Normalization</a> .  In this case, mean_file was specified ‚Äî subtracting the <a href="https://gist.github.com/sld/5cb766398471c63903f3">‚Äúaverage‚Äù image</a> from the input. <br><br>  Caffe uses <a href="http://en.wikipedia.org/wiki/Stochastic_gradient_descent">batch gradient descent</a> .  The input layer contains the batch_size parameter.  In one iteration, the batch_size of the sampling elements arrives at the input of the neural network. <br><br><h4>  Convolution and subsample layers (conv, pool) </h4><br><div class="spoiler">  <b class="spoiler_title">Configuration of layers of convolution and subsample</b> <div class="spoiler_text"><pre> <code class="markdown hljs"> ... layers { bottom: "data" top: "conv1/5x5<span class="hljs-emphasis"><span class="hljs-emphasis">_s1" name: "conv1/5x5_</span></span>s1" type: CONVOLUTION blobs<span class="hljs-emphasis"><span class="hljs-emphasis">_lr: 1 blobs_</span></span>lr: 2 convolution<span class="hljs-emphasis"><span class="hljs-emphasis">_param { num_</span></span>output: 64 kernel<span class="hljs-emphasis"><span class="hljs-emphasis">_size: 5 stride: 1 pad: 2 weight_</span></span>filler { type: "xavier" std: 0.0001 } } } layers { bottom: "conv1/5x5<span class="hljs-emphasis"><span class="hljs-emphasis">_s1" top: "conv1/5x5_</span></span>s1" name: "conv1/relu<span class="hljs-emphasis"><span class="hljs-emphasis">_5x5" type: RELU } layers { bottom: "conv1/5x5_</span></span>s1" top: "pool1/3x3<span class="hljs-emphasis"><span class="hljs-emphasis">_s2" name: "pool1/3x3_</span></span>s2" type: POOLING pooling<span class="hljs-emphasis"><span class="hljs-emphasis">_param { pool: MAX kernel_</span></span>size: 3 stride: 2 } } layers { bottom: "pool1/3x3<span class="hljs-emphasis"><span class="hljs-emphasis">_s2" top: "conv2/5x5_</span></span>s1" name: "conv2/5x5<span class="hljs-emphasis"><span class="hljs-emphasis">_s1" type: CONVOLUTION blobs_</span></span>lr: 1 blobs<span class="hljs-emphasis"><span class="hljs-emphasis">_lr: 2 convolution_</span></span>param { num<span class="hljs-emphasis"><span class="hljs-emphasis">_output: 64 kernel_</span></span>size: 5 stride: 1 pad: 2 weight<span class="hljs-emphasis"><span class="hljs-emphasis">_filler { type: "xavier" std: 0.01 } } } layers { bottom: "conv2/5x5_</span></span>s1" top: "conv2/5x5<span class="hljs-emphasis"><span class="hljs-emphasis">_s1" name: "conv2/relu_</span></span>5x5" type: RELU } layers { bottom: "conv2/5x5<span class="hljs-emphasis"><span class="hljs-emphasis">_s1" top: "pool2/3x3_</span></span>s2" name: "pool2/3x3<span class="hljs-emphasis"><span class="hljs-emphasis">_s2" type: POOLING pooling_</span></span>param { pool: MAX kernel<span class="hljs-emphasis"><span class="hljs-emphasis">_size: 3 stride: 2 } } layers { bottom: "pool2/3x3_</span></span>s2" top: "conv3/5x5<span class="hljs-emphasis"><span class="hljs-emphasis">_s1" name: "conv3/5x5_</span></span>s1" type: CONVOLUTION blobs<span class="hljs-emphasis"><span class="hljs-emphasis">_lr: 1 blobs_</span></span>lr: 2 convolution<span class="hljs-emphasis"><span class="hljs-emphasis">_param { num_</span></span>output: 128 kernel<span class="hljs-emphasis"><span class="hljs-emphasis">_size: 5 stride: 1 pad: 2 weight_</span></span>filler { type: "xavier" std: 0.01 } } } layers { bottom: "conv3/5x5<span class="hljs-emphasis"><span class="hljs-emphasis">_s1" top: "conv3/5x5_</span></span>s1" name: "conv3/relu<span class="hljs-emphasis"><span class="hljs-emphasis">_5x5" type: RELU } layers { bottom: "conv3/5x5_</span></span>s1" top: "pool3/3x3<span class="hljs-emphasis"><span class="hljs-emphasis">_s2" name: "pool3/3x3_</span></span>s2" type: POOLING pooling<span class="hljs-emphasis"><span class="hljs-emphasis">_param { pool: MAX kernel_</span></span>size: 3 stride: 2 } } ...</code> </pre><br></div></div><br>  3m is a convolution layer with type: CONVOLUTION.  Next comes the indication of the activation function c type: RELU.  The 4th layer is a subsample layer with type: POOL.  Then 2 times the conv, pool layers are repeated, but with different parameters. <br><br>  The selection of parameters for these layers is empirical. <br><br><h4>  Fully connected and output layers (fully-conn, output) </h4><br><div class="spoiler">  <b class="spoiler_title">Configuration of fully connected and output layers</b> <div class="spoiler_text"><pre> <code class="markdown hljs"> ... layers { bottom: "pool3/3x3<span class="hljs-emphasis"><span class="hljs-emphasis">_s2" top: "ip1/3072" name: "ip1/3072" type: INNER_</span></span>PRODUCT blobs<span class="hljs-emphasis"><span class="hljs-emphasis">_lr: 1 blobs_</span></span>lr: 2 inner<span class="hljs-emphasis"><span class="hljs-emphasis">_product_</span></span>param { num<span class="hljs-emphasis"><span class="hljs-emphasis">_output: 3072 weight_</span></span>filler { type: "gaussian" std: 0.001 } bias<span class="hljs-emphasis"><span class="hljs-emphasis">_filler { type: "constant" } } } layers { bottom: "ip1/3072" top: "ip1/3072" name: "ip1/relu_</span></span>5x5" type: RELU } layers { bottom: "ip1/3072" top: "ip2/2048" name: "ip2/2048" type: INNER<span class="hljs-emphasis"><span class="hljs-emphasis">_PRODUCT blobs_</span></span>lr: 1 blobs<span class="hljs-emphasis"><span class="hljs-emphasis">_lr: 2 inner_</span></span>product<span class="hljs-emphasis"><span class="hljs-emphasis">_param { num_</span></span>output: 2048 weight<span class="hljs-emphasis"><span class="hljs-emphasis">_filler { type: "xavier" std: 0.001 } bias_</span></span>filler { type: "constant" } } } layers { bottom: "ip2/2048" top: "ip2/2048" name: "ip2/relu<span class="hljs-emphasis"><span class="hljs-emphasis">_5x5" type: RELU } layers { bottom: "ip2/2048" top: "ip3/10" name: "ip3/10" type: INNER_</span></span>PRODUCT blobs<span class="hljs-emphasis"><span class="hljs-emphasis">_lr: 1 blobs_</span></span>lr: 2 inner<span class="hljs-emphasis"><span class="hljs-emphasis">_product_</span></span>param { num<span class="hljs-emphasis"><span class="hljs-emphasis">_output: 10 weight_</span></span>filler { type: "xavier" std: 0.1 } } } layers { name: "accuracy" type: ACCURACY bottom: "ip3/10" bottom: "label" top: "accuracy" include: { phase: TEST } } layers { name: "loss" type: SOFTMAX_LOSS bottom: "ip3/10" bottom: "label" top: "loss" }</code> </pre><br></div></div><br>  The full layer has type: INNER_PRODUCT.  The output layer is connected to the layer with the loss function (type: SOFTMAX_LOSS) and the accuracy layer (type: ACCURACY).  The accuracy layer only works in the test phase and shows the percentage of correctly classified images in the validation sample. <br><br>  It is important to specify the weight_filler attribute.  If it is large, the loss function (loss) may return NaN at initial iterations.  In this case, you need to reduce the std parameter for the weight_filler attribute. <br><br><h4>  Learning options </h4><br><div class="spoiler">  <b class="spoiler_title">Learning Settings Configuration</b> <div class="spoiler_text"><pre> <code class="markdown hljs"> net: "/home/deploy/opt/SVHN/svhn/winny-f/winny<span class="hljs-emphasis"><span class="hljs-emphasis">_f_</span></span>svhn.prototxt" test<span class="hljs-emphasis"><span class="hljs-emphasis">_iter: 1 test_</span></span>interval: 700 base<span class="hljs-emphasis"><span class="hljs-emphasis">_lr: 0.01 momentum: 0.9 weight_</span></span>decay: 0.004 lr<span class="hljs-emphasis"><span class="hljs-emphasis">_policy: "inv" gamma: 0.0001 power: 0.75 solver_</span></span>type: NESTEROV display: 100 max<span class="hljs-emphasis"><span class="hljs-emphasis">_iter: 77000 snapshot: 700 snapshot_</span></span>prefix: "/mnt/home/deploy/opt/SVHN/svhn/snapshots/winny<span class="hljs-emphasis"><span class="hljs-emphasis">_net/winny-F" solver_</span></span>mode: GPU</code> </pre><br></div></div><br>  To obtain a well-trained neural network, you need to set training parameters.  In Caffe, the learning parameters are set via the configuration protobuf file.  The configuration file for this contest is <a href="https://gist.github.com/sld/6ecd597d455b62ef5d36">here</a> .  <a href="http://caffe.berkeleyvision.org/tutorial/solver.html">There are a lot of</a> parameters, we will consider some of them in more detail: <br><ul><li>  net - the path to the configuration of the NA architecture </li><li>  test_interval - the number of iterations between which the NA is tested (phase: test), </li><li>  snapshot - the number of iterations between which the state of learning NA is preserved. <br>  In Caffe, you can pause and resume training. </li></ul><br><br><h2>  Training and Testing </h2><br>  To start training the NN, you need to run the caffe train command with the <a href="https://gist.github.com/sld/6ecd597d455b62ef5d36">configuration file</a> , where the training parameters are set: <br><pre> <code class="bash hljs">&gt; caffe train --solver=/home/deploy/winny<span class="hljs-_"><span class="hljs-_">-f</span></span>/winny_f_svhn_solver.prototxt</code> </pre><br><div class="spoiler">  <b class="spoiler_title">Short training log</b> <div class="spoiler_text"><pre> <code class="markdown hljs"> ....................... I0109 18:12:17.035543 12864 solver.cpp:160] Solving WinnyNet-F I0109 18:12:17.035578 12864 solver.cpp:247] Iteration 0, Testing net (#0) I0109 18:12:17.077910 12864 solver.cpp:298] Test net output #0: accuracy = 0.0666667 I0109 18:12:17.077997 12864 solver.cpp:298] Test net output #1: loss = 2.3027 (<span class="hljs-bullet"><span class="hljs-bullet">* 1 = 2.3027 loss) I0109 18:12:17.107712 12864 solver.cpp:191] Iteration 0, loss = 2.30359 I0109 18:12:17.107795 12864 solver.cpp:206] Train net output #0: loss = 2.30359 (*</span></span> 1 = 2.30359 loss) I0109 18:12:17.107817 12864 solver.cpp:516] Iteration 0, lr = 0.01 ....................... I0109 18:13:17.960325 12864 solver.cpp:247] Iteration 700, Testing net (#0) I0109 18:13:18.045385 12864 solver.cpp:298] Test net output #0: accuracy = 0.841667 I0109 18:13:18.045462 12864 solver.cpp:298] Test net output #1: loss = 0.675567 (<span class="hljs-bullet"><span class="hljs-bullet">* 1 = 0.675567 loss) I0109 18:13:18.072872 12864 solver.cpp:191] Iteration 700, loss = 0.383181 I0109 18:13:18.072949 12864 solver.cpp:206] Train net output #0: loss = 0.383181 (*</span></span> 1 = 0.383181 loss) ....................... I0109 20:08:50.567730 26450 solver.cpp:247] Iteration 77000, Testing net (#0) I0109 20:08:50.610496 26450 solver.cpp:298] Test net output #0: accuracy = 0.916667 I0109 20:08:50.610571 26450 solver.cpp:298] Test net output #1: loss = 0.734139 (<span class="hljs-bullet"><span class="hljs-bullet">* 1 = 0.734139 loss) I0109 20:08:50.640389 26450 solver.cpp:191] Iteration 77000, loss = 0.0050708 I0109 20:08:50.640470 26450 solver.cpp:206] Train net output #0: loss = 0.0050708 (*</span></span> 1 = 0.0050708 loss) I0109 20:08:50.640494 26450 solver.cpp:516] Iteration 77000, lr = 0.00197406 ....................... I0109 20:52:32.236827 30453 solver.cpp:247] Iteration 103600, Testing net (#0) I0109 20:52:32.263108 30453 solver.cpp:298] Test net output #0: accuracy = 0.883333 I0109 20:52:32.263183 30453 solver.cpp:298] Test net output #1: loss = 0.901031 (<span class="hljs-bullet"><span class="hljs-bullet">* 1 = 0.901031 loss) I0109 20:52:32.290550 30453 solver.cpp:191] Iteration 103600, loss = 0.00463345 I0109 20:52:32.290627 30453 solver.cpp:206] Train net output #0: loss = 0.00463345 (*</span></span> 1 = 0.00463345 loss) I0109 20:52:32.290644 30453 solver.cpp:516] Iteration 103600, lr = 0.00161609</code> </pre><br></div></div><br>  One epoch is (73257-120) / 128 ~ = 571 iteration.  Slightly more than 1 epoch, at 700 iterations, the network accuracy in the validation sample is 84%.  At 134 era, accuracy is already 91%.  At 181 era - 88%.  Perhaps, if you train the network more than ages, for example 1000, the accuracy will stabilize and be higher.  In this contest, training was stopped at 181 epochs. <br><br>  In Caffe, you can resume network training from a snapshot by adding the --snapshot option: <br><pre> <code class="bash hljs">&gt; caffe train --solver=/home/deploy/winny<span class="hljs-_"><span class="hljs-_">-f</span></span>/winny_f_svhn_solver.prototxt --snapshot=winny_net/winny-F_snapshot_77000.solverstate</code> </pre><br><br><h4>  Testing on untagged images </h4><br>  To test the NA, you must create the <a href="https://gist.github.com/sld/6ecd597d455b62ef5d36">deploy configuration of the network architecture</a> .  In it, unlike the previous configuration, there is no precision layer and the input layer is simplified. <br><br>  The test sample, consisting of 26032 images, goes without markup.  Therefore, in order to evaluate the accuracy of a test sample of a contest, you need to write <a href="https://gist.github.com/sld/5cb766398471c63903f3">some code</a> .  Caffe has <a href="http://caffe.berkeleyvision.org/tutorial/interfaces.html">interfaces for Python and Matlab</a> . <br><br>  To test networks from different eras, Caffe has snapshots.  The network of 134 eras showed accuracy (Private Score in kaggle) 88.7%, and the network of 181 epochs - 87.6%. <br><br><h2>  Ideas to improve accuracy </h2><br>  Judging by the <a href="http://www.cs.toronto.edu/~nitish/msc_thesis.pdf">master's thesis</a> , the accuracy of the implemented architecture can reach 96%. <br><br>  How can you try to increase the accuracy obtained by 88.7%? <br><br><ul><li>  Educate the network for more eras.  For example, in the tutorial on <a href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/">deep learning in the facial keypoints detection</a> network trained 1000 epochs. </li><li>  Standardize data so that the expectation is 0 and variance 1. To do this, you need to use HDF5 or LevelDb / lmdb to store data. </li><li>  Work with learning options.  For example, decrease learning_rate every 100 epochs. </li><li>  You can also try using dropout layers, but this <a href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/">will require training the network for even more epochs than 1000</a> . </li><li>  Dataset SVHN contains an additional 600,000 tagged images.  They are used in research, but in the context of the contest their use would be dishonest.  In this case, you can generate new data based on existing data. </li></ul><br><br><h2>  Conclusion </h2><br>  The implemented convolutional neural network showed an accuracy of 88.9%.  This is not the best result, but not bad for the first pancake.  There is potential to increase accuracy up to 96%. <br><br>  Thanks to the Caffe framework, immersion in deep learning does not cause great difficulties.  It is enough to create a couple of configuration files and start the learning process with one command.  Of course, basic knowledge of the theory of artificial neural networks is also needed.  I tried to give this (in the form of links to materials) and other information for a quick start in this article. </div><p>Source: <a href="https://habr.com/ru/post/249089/">https://habr.com/ru/post/249089/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../249077/index.html">To the attention of Masterhost / Mastermind customers!</a></li>
<li><a href="../249079/index.html">Microsoft News: R support, new Power BI for analytics and reports, animation and 3D graphics in the cloud</a></li>
<li><a href="../249083/index.html">How to protect Linux ‚Äì server from critical vulnerability Ghost. Update your OS!</a></li>
<li><a href="../249085/index.html">PVS-Studio and hostile habitat</a></li>
<li><a href="../249087/index.html">FutoIn AsyncSteps: the concept and implementation of asynchronous business logic</a></li>
<li><a href="../249091/index.html">An extension for the normal selection of text inside a link in browsers</a></li>
<li><a href="../249093/index.html">Open Data Day February 21 will be held around the world</a></li>
<li><a href="../249095/index.html">Data Center per billion or "Super Ring" for the "Wild West"</a></li>
<li><a href="../249097/index.html">New GHOST vulnerability threatens popular Linux distributions</a></li>
<li><a href="../249099/index.html">Warming data centers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>