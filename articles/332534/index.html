<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Annealing and freezing: two fresh ideas on how to accelerate the learning of deep networks.</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This post outlines two recently published ideas on how to speed up the learning process of deep neural networks while increasing prediction accuracy. ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Annealing and freezing: two fresh ideas on how to accelerate the learning of deep networks.</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/web/6d8/3d0/c76/6d83d0c76fb34e8ea590ca3a190f61c8.png"><br><br>  This post outlines two recently published ideas on how to speed up the learning process of deep neural networks while increasing prediction accuracy.  The proposed (by different authors) methods are orthogonal to each other, and can be used together and separately.  The methods proposed here are simple to understand and implement.  Actually, links to original publications: <br><br><ul><li>  <a href="https://arxiv.org/abs/1704.00109">Snapshot ensembles (April 2017)</a> </li><li>  <a href="https://arxiv.org/abs/1706.04983">FreezeOut (June 2017)</a> </li></ul><br><a name="habracut"></a><br><h2>  1. Ensemble of pictures: many models for the price of one </h2><br><h4>  Ordinary ensembles of models </h4><br>  Ensembles are groups of models used collectively for prediction.  The idea is simple: train several models with different hyperparameters, and average their predictions when testing.  This technique gives a noticeable increase in the accuracy of the prediction; most winners of machine learning competitions <s>do it</s> using ensembles. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  So what's the problem? </h4><br>  Training N models will require N times more time than training a single model.  <s>Instead of mining</s> , you have to spend GPU resources and wait long for results. <br><br><h4>  SGD Mechanics </h4><br>  Stochastic Gradient Descent (SGD) is a greedy algorithm.  It moves in the parameter space in the direction of the greatest gradient.  At the same time, there is one key parameter: learning speed.  If the learning rate is too high, SGD ignores the narrow valleys in the relief of the hyperplane of parameters (minima) and jumps over them like a tank through trenches.  On the other hand, if the learning rate is low, SGD falls into one of the local minima and cannot get out of it. <br><br>  However, it is possible to pull the SGD from the local minimum by <b>increasing</b> the learning rate. <br><br><h4>  Watching hands ... </h4><br>  The authors of the article use this controlled parameter SGD to roll into a local minimum and exit from there.  Different local minima can give the <b>same percentage of</b> errors when testing, but the specific errors for each local minimum will be <b>different</b> ! <br><br>  This picture explains the concept very clearly.  The left shows how a normal SGD works, trying to find a local minimum.  Right: SGD fails to the first local minimum, takes a snapshot of the trained model, then selects the SGD from the local minimum and looks for the next one.  So you get three local minima with the same percentage of errors, but different error characteristics. <br><br><img src="https://habrastorage.org/web/d17/434/113/d17434113522415c97e1321c140bc041.png"><br><br><h4>  What does an ensemble consist of? </h4><br>  The authors exploit the property of local minima to reflect different ‚Äúpoints of view‚Äù on model predictions.  Every time when SGD reaches a local minimum, they keep a snapshot of the model, of which they will form an ensemble during testing. <br><br><h4>  Cyclic cosine annealing </h4><br><br>  To automatically decide when to dive into a local minimum, and when to leave it, the authors use the learning speed annealing function: <br><br><img src="https://habrastorage.org/web/1b1/b9e/6d7/1b1b9e6d762544fc8eae04e662b619d6.png"><br><br>  The formula looks cumbersome, but is actually quite simple.  Authors use monotonically decreasing function.  Alpha - the new value of learning speed.  Alpha zero is the previous value.  T is the total number of iterations that you plan to use (the size of the batch is X the number of epochs).  M - the number of shots of the model you want to receive (ensemble size). <br><br><img src="https://habrastorage.org/web/197/36e/70f/19736e70fc2e45129eef7eca3ebe0905.png"><br><br>  Notice how quickly the loss function drops before saving each shot.  This is because the learning rate is <b>continuously decreasing</b> .  After saving the snapshot, the learning speed is restored (the authors use a level of 0.1).  This deduces the trajectory of the gradient descent from the local minimum, and the search for a new minimum begins. <br><br><h4>  Conclusion </h4><br>  The authors present test results on several datasets (Cifar 10, Cifar 100, SVHN, Tiny IMageNet) and several popular neural network architectures (ResNet-110, Wide-ResNet-32, DenseNet-40, DenseNet-100).  In all cases, an ensemble trained by the proposed method showed the lowest percentage of errors. <br><br>  Thus, a useful strategy has been proposed for obtaining an increase in accuracy without additional computational costs when training models.  On the effect of various parameters, such as T and M on performance, see the original article. <br><br><h2>  2. Freezing: acceleration of learning by successive freezing of layers. </h2><br>  The authors of the article demonstrated the acceleration of learning by freezing layers without loss of prediction accuracy. <br><br><h4>  What does freezing layers mean? </h4><br>  Freezing the layer prevents changes in the weights of the layer during training.  This technique is often used in transfer learning, when the base model trained in another dataset is frozen. <br><br><h4>  How does freezing affect model speed? </h4><br>  If you do not want to change the weight of the layer, the reverse passage through this layer can be completely excluded.  This seriously speeds up the calculation process.  For example, if half the layers in your model are frozen, it will take half as many calculations to train the model. <br><br>  On the other hand, you still need to train the model, so if you freeze the layers too early, the model will give inaccurate predictions. <br><br><h4>  What is the novelty? </h4><br>  The authors showed a way to freeze the layers one by one as early as possible, optimizing the learning time by eliminating back passes.  At the beginning, the model is fully trained as usual.  After several iterations, the first layer of the model is frozen, and the remaining layers continue to be trained.  After a few more iterations, the next layer is frozen, and so on. <br><br><h4>  (Again) annealing learning speed </h4><br>  The authors used annealing learning rate.  An important difference between their approach: the learning rate varies <b>from layer to layer</b> , and not for the entire model.  They used the following expression: <br><br><img src="https://habrastorage.org/web/0c1/045/a67/0c1045a67d3f42c38860ff02fdaa72ec.png"><br><br>  Here, alpha is the learning rate, t is the iteration number, i is the layer number in the model. <br><br>  Note that since the first layer of the model will be frozen first, its training will last the least amount of cycles.  To compensate for this, the authors scaled the initial learning rate for each layer: <br><br><img src="https://habrastorage.org/web/18a/0ff/b6a/18a0ffb6a99b449cb86ab89a429c8e79.png"><br><br>  As a result, the authors achieved a 20% acceleration of learning due to a 3% <b>decrease in accuracy</b> , or a 15% acceleration <b>without reducing the</b> prediction <b>accuracy</b> . <br><br>  However, the proposed method does not work well with models that do not use layer skips (such as VGG-16).  In such networks, neither acceleration nor an effect on the prediction accuracy was found. </div><p>Source: <a href="https://habr.com/ru/post/332534/">https://habr.com/ru/post/332534/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../332524/index.html">On the role of interfaces in the context of existentialism</a></li>
<li><a href="../332526/index.html">You are a big brother, or try yourself as an all-seeing eye.</a></li>
<li><a href="../332528/index.html">AWS EC2 Apache CloudStack-style public cloud using KVM hypervisor and NFS storage</a></li>
<li><a href="../332530/index.html">How to escape from reading tutorials on programming</a></li>
<li><a href="../332532/index.html">Day of the sysadmin: light on the quest, spin the eggplant</a></li>
<li><a href="../332536/index.html">Making money from rain or drought. The Weather Company Experience</a></li>
<li><a href="../332538/index.html">WI-FI in the subway: network architecture and underground stones</a></li>
<li><a href="../332540/index.html">5 fresh examples of parsing and improving the design in simple ways</a></li>
<li><a href="../332542/index.html">Meanwhile, Proxmox VE has been updated to version 5.0.</a></li>
<li><a href="../332544/index.html">Using timeout & strace utilities to monitor user inactivity to break a Shellinabox connection</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>