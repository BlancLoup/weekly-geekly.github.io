<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Analysis of audio analysis algorithms</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The development of Synesis is not limited to video analytics only. We are engaged in audio analysis. That's what we wanted to tell you today. From thi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Analysis of audio analysis algorithms</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/3a9/066/8cb/3a90668cb1ff41afaf9116cfd7599900.gif" align="left">  The development of Synesis is not limited to video analytics only.  We are engaged in audio analysis.  That's what we wanted to tell you today.  From this article you will learn about the most well-known audioanalytic systems, as well as algorithms and their specifics.  At the end of the material - traditionally - a list of sources and useful links, including audio libraries. <br><br>  Caution: the article may take a long time to load - many pictures. <br>  Author: Mikhail Antonenko. <br><a name="habracut"></a><br>  While video analysis has become a standard feature of many security cameras, the built-in audio analytics continues to be a fairly rare phenomenon, despite the presence of both the audio channel in the devices and the available computing power for processing audio data [1].  However, audio analytics have some advantages compared to video analytics: the cost of microphones and their maintenance is much cheaper than video cameras;  when the system is running in real time, the data stream of audio information is significantly less in volume than the data stream from video cameras, which imposes more loyal requirements on the data transmission channel capacity.  Audio analytics systems can be especially in demand for city surveillance, where you can automatically start streaming live video to a police console from the scene of an explosion and shooting.  Audio analytics technologies can also be used in studying video recordings and determining events.  With increasing awareness of the capabilities of these systems, the use of audio analytics will only expand.  Let's start with the basics: <br><br><h3>  Existing Audio Analytics Solutions </h3><br>  Perhaps the most famous system is ShotSpotter [2], the leader in the United States in detecting shots from a firearm in an urban setting.  Directional microphones were installed in the most dangerous areas (on poles, houses, and other tall buildings) that capture the sounds of the city background.  In the case of a positive shot identification, information from the GPS coordinates of a specific microphone is transmitted to the central computer, where additional sound analysis is performed to screen out possible false alarms, such as a flying helicopter or, for example, an exploding firecracker.  If the shot is confirmed, the patrol leaves on the spot.  The system installed in Washington in 2006, over the years, localized 39,000 shots from firearms, and the police were able to quickly respond in each case.  [3] 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/files/e2b/2a3/9f1/e2b2a39f1f7a46a192f33e80dca7c610.jpg"><br><br>  Among the service providers of audio analysis in the Russian Federation can be identified "SistemaSarov" [4].  The acoustic monitoring system allows you to automatically allocate acoustic artifacts in the audio stream, perform their preliminary classification (according to the type of alarm event), and in case the terminal device is equipped with a GPS / GLONASS signal receiver, determine the coordinates of the event and save the events to the archive.  [five] <br><br><img src="https://habrastorage.org/files/2db/04d/964/2db04d96449c458c9724654fd7373843.jpg"><br><br>  The project "AudioAnalytics" [6], based in the UK, provides several solutions for different use cases.  The architecture of the proposed solutions is as follows: the CoreLogger program, running on the end-user device, allows receiving and displaying / storing alarm events.  It works in conjunction with another part of the overall system - Sound Packs - which is nothing more than a set of various audio analytics modules.  The main features of these modules are the detection of the following audio events: <br><br><ul><li>  aggression (talking on high tones, cry); </li><li>  car alarm; </li><li>  breaking glass; </li><li>  search for keywords (‚Äúpolice‚Äù, ‚Äúhelp‚Äù, etc.); </li><li>  shots; </li><li>  cry / cry baby. </li></ul><br>  Additionally, a part of the system called Core Trainer is provided, which, based on the set of audio signals submitted to it, will select the most different (prominent) parts and form a new pattern for SoundPacks. <br><br><h3>  Methods and approaches to detecting and recognizing audio events of various types (shot, broken glass, scream) </h3><br>  As an example of the initial data, spectrograms of three different types of audio events are presented: <br>  1. A set of shots. <br><img src="https://habrastorage.org/files/34e/fc2/5ef/34efc25ef7f34c56817c774dc315bad1.jpg"><br><br>  2. Broken glass <br><img src="https://habrastorage.org/files/0d0/b3b/57d/0d0b3b57d0a64064b80371007c6269bb.jpg"><br><br>  3. Scream <br><img src="https://habrastorage.org/files/345/54c/07b/34554c07b4a04a959c0bdb544eb5f91b.jpg"><br><br>  The corresponding audio recordings are downloaded from the audio database [7].  Other sources of source audio data include databases [8] [9] [10] [11]. <br>  The task of recognizing various disturbing audio events is divided into 2 subtasks [12]: <br><br><ul><li>  detection (extraction) of sharp impulse signals from background noise in the audio data stream; </li><li>  classification (recognition) of the detected signal to one of the types of audio events. </li></ul><br><img src="https://habrastorage.org/files/002/f5f/064/002f5f064df34402911d8bd3d75a7051.jpg"><br><br><h3>  Detection of impulsive eye-catching audio events </h3><br>  Most methods are based on determining the power for a set of consecutive non-overlapping blocks of audio signal [13].  The power for the k-th block of a signal consisting of for N samples is defined as <br><br><img src="https://habrastorage.org/files/7ad/d34/78f/7add3478f0f347ed80af12985ca15ad5.jpg"><br><img src="https://habrastorage.org/files/98f/df4/80b/98fdf480b7644261af6ab66cb6a546bc.jpg"><br><br>  As an example, an audio signal with a shot that occurred at 4.6 s is given, as well as a set of power values ‚Äã‚Äãfor blocks of N = 4000 samples, which corresponds to the duration of each block of about 90 ms. <br>  Various methods differ in the method of automatic detection of a block corresponding to a sharp pulse sound: <br>  based on the standard deviation of the normalized power values ‚Äã‚Äãof the blocks; <br>  based on the application of a median filter for block power values; <br>  over dynamic threshold for block power values. <br>  Let's take a closer look at them: <br><br>  <i>Method based on standard deviation of normalized block power values</i> <br><br>  The key aspect of this method is the normalization of the considered set of values ‚Äã‚Äãof power blocks to the range [0, 1]: <br><br><img src="https://habrastorage.org/files/c8a/5c3/594/c8a5c3594d41471eb925c2a09f421256.jpg"><br><br>  Next, the standard deviation (variance) of the resulting set of values ‚Äã‚Äãis calculated: <br><br><img src="https://habrastorage.org/files/bb2/acf/6ec/bb2acf6ec59c48279e3a4fecc1b96cd9.jpg"><br><img src="https://habrastorage.org/files/103/559/237/1035592376bf43f59a5a89c487b491a3.jpg"><br><br>  In the case of background noise, the values ‚Äã‚Äãof the power blocks will be approximately evenly distributed in the range [0-1] (seen in the figure to the left).  Since when a new power value is received for an audio signal block, the values ‚Äã‚Äãare renormalized to the specified range, when a block with a significantly higher power level occurs, the standard deviation value will decrease significantly compared to the same value for a set of previous background power blocks.  By reducing the standard deviation below the threshold value, a block with a pulse signal can be automatically detected. <br><br>  The advantage of this method is its resistance to changes in the noise level, as well as the ability to detect a slowly varying signal by analyzing the average value of the normalized power blocks. <br><br>  <i>Method based on using median filter</i> <br><br>  The main stages of automatic detection of pulsed precipitated signals using a median filter are presented below.  An example of applying a median filter of order k to the set of power blocks is presented in the figure below. <br><br><img src="https://habrastorage.org/files/46d/e5f/e4e/46de5fe4ed73480fa9ad4e3a735775eb.jpg"><br><br>  To detect a block with a pulse event, a conditional median filter (conditional median filtering) is applied, which leaves the original signal value if the difference between the original sample and the median value is less than the threshold value, and the median value otherwise. <br><br><img src="https://habrastorage.org/files/218/192/f29/218192f29ca9428c8ab447ad3c7e8246.jpg"><br><img src="https://habrastorage.org/files/a0e/eb6/e26/a0eeb6e2648544c9a00ec6f0a708a1a0.jpg"><br><br>  By calculating the difference between the signal after applying the <i>conditional median filter</i> and the shifted source signal, you can automatically select the block with the impulse event that occurred. <br><br>  <i>Dynamic threshold method for block power values</i> <br><br>  In this method, it is proposed to detect a pulsed signal using the average value of the power set of the blocks and the standard deviation as a dynamic threshold.  Automatic detection occurs when the power of the next block exceeds the threshold value, defined as <br><br>  th = par * std + m <br><br>  Where par is a parameter that determines the sensitivity of the algorithm.  An example of using this method with par = 3 is shown in the following figure: <br><br><img src="https://habrastorage.org/files/847/80d/5b4/84780d5b4a67408b9afd3ea99a32a714.jpg"><br><br><h3>  Audio Event Recognition </h3><br>  <b>The general audio event recognition scheme includes the following steps:</b> <br><br><img src="https://habrastorage.org/files/342/ad0/a1e/342ad0a1e59047689085c866af5e275c.gif"><br><br>  <i>Overlap Buffering</i> <br><br>  At the first stage, the original audio signal is converted into an overlapping frame set: <br><br><img src="https://habrastorage.org/files/841/b3f/495/841b3f4956a9487d96d0e714ce8c05ea.jpg"><br><br>  <i>Preprocessing stage</i> <br><br>  The preprocessing stage includes, as a rule, pre-emphasis filtering and window weighting.  Pre-emphasis processing is carried out by applying the FIR filter H (z) = 1‚Äì a / Z. This is necessary for spectral smoothing of the signal [14].  In this case, the signal becomes less susceptible to various noises arising during processing. <br><br>  Window weighting should be applied due to the fact that the audio frame is limited in time, therefore, when going into the frequency domain, there will be a side-lobe spectrum filtering effect associated with the shape of the spectrum of the rectangular window function (it looks like sin (x) / x).  Therefore, in order to reduce the effect of this effect, weighting of the original signal of different types of windows is applied, with a shape other than rectangular.  The samples of the input sequence are multiplied by the corresponding window function, which entails zeroing of the signal values ‚Äã‚Äãat the edges of the sample.  The most commonly used weighted functions are the Hamming, Blackman, Flat, Kaisel-Bessel, Dolph-Chebyshev windows [15].  The spectral characteristics of some are given below: <br><br><img src="https://habrastorage.org/files/dfc/4e1/3e5/dfc4e13e53a94b9e95527be2fb3455a2.jpg"><br><br>  <i>Feature extraction</i> <br><br>  There are several approaches to extracting descriptors (features) from an audio signal.  All of them are set by the common goal to reduce signal redundancy and highlight the most relevant information, and, at the same time, discard irrelevant information.  Typically, features that describe an audio signal from different points of view are combined into one feature vector, on the basis of which the learning process takes place and then classified using the selected trained model.  Next will be presented the most popular features extracted from the audio signal. <br>  <b>1. Statistics in the time domain (Time-domain statistics)</b> <br><ul><li>  ZCR (Zero crossing rate) is the number of intersections of the time axis by the audio signal [16]. </li></ul><br><img src="https://habrastorage.org/files/5c0/1d9/37c/5c01d937c8b543ccacd3535a0465f684.png"><br><img src="https://habrastorage.org/files/443/f33/a98/443f33a983794963ae32199d32519f35.jpg"><br><br><ul><li>  Short-time energy [17] - average energy for an audio frame </li></ul><br><img src="https://habrastorage.org/files/d51/d43/16a/d51d4316adcd4f3e9c60cfe1183916c5.png"><br><br><ul><li>  Entropy of energy [18] </li></ul><br><br>  Dividing each frame into a set of sub-frames, the set of energies for each subframe is calculated.  Next, normalizing the energy of each of the subframes to the energy of the entire frame, we can consider the set of energies as a set of probabilities and calculate the information entropy using the formula: <br><img src="https://habrastorage.org/files/2b4/f2c/8bc/2b4f2c8bc5a948cc9ebe71899dc8ef03.png"><br><br>  <b>2. Frequency domain statistics (Frequency-domain statistic)</b> <br><br>  Spectral centroid [17] <br>  It is an interpretation of the "center of mass" of the spectrum.  Calculated as the sum of the frequencies weighted by the corresponding amplitudes of the spectrum divided by the sum of the amplitudes: <br><br><img src="https://habrastorage.org/files/ad9/4ba/b1c/ad94bab1c1ef4050a54caa4a4d0acd33.jpg"><br><br>  Where F [k] is the amplitude of the spectrum corresponding to the k-th value of the frequency in the DFT spectrum. <br><br>  Then, the obtained value is conveniently normalized to the maximum frequency (Fs / 2), as a result, the range of possible "center of mass" of the spectrum will lie in the range [0-1]. <br><ul><li>  <i>Spectrum spread</i> [17] (also called instant spectrum width / bandwidth). </li></ul><br>  Defined as the second central point: <br><br><img src="https://habrastorage.org/files/bbd/917/400/bbd91740045742d787c9df5f41364cf5.png"><br><br>  Where fk - values ‚Äã‚Äãof frequencies in DFT, F [fk] - values ‚Äã‚Äãof amplitudes, SC - value Spectral centroid <br><br><img src="https://habrastorage.org/files/e18/a1e/db9/e18a1edb9f604b549254dbbb5f260a19.jpg"><br><br><ul><li>  <i>Audio Spectrum Flatness (ASF)</i> [19] </li></ul><br>  Reflects the deviation of the power spectrum of the signal from the flat form.  From the point of view of human perception, it characterizes the degree of tonality of the sound signal. <br><br><img src="https://habrastorage.org/files/29a/ea0/f40/29aea0f401934766838578df3c703681.jpg"><br><br><ul><li>  <i>Energy-band spectrum [20]</i> </li></ul><br>  The frequency space is divided into N bands, after which the spectrum energy is calculated in each band.  The values ‚Äã‚Äãobtained are taken as indications.  In essence, the signs in this case represent the value of the energy of the spectrum in the "low resolution". <br><ul><li>  <i>Cepstral coefficients</i> </li></ul><br>  Obtained on the basis of the previous signs by transferring them to the cepstral space.  A cepstrum is nothing more than the ‚Äúspectrum spectrum logarithm‚Äù; instead of the Fourier transform, use the discrete cosine transform (DCT): <br><br><img src="https://habrastorage.org/files/a69/e80/8c9/a69e808c9f5049ab969c5e8e255dbb0f.jpg"><br><img src="https://habrastorage.org/files/b9e/a67/d6b/b9ea67d6b672498eb9451e0ba65eaaad.jpg"><br><br><ul><li>  <i>Spectral entropy</i> </li></ul><br>  The entropy of the spectrum for a given frame is calculated in a similar way with the entropy of energy in the time domain: the frequency region of the spectrum is divided into N frequency subregions, for each of which the part of the total spectrum energy attributable to this subdomain is calculated, and then the information entropy is calculated by analogy with the time domain. <br><br><ul><li>  <i>Spectral flux [21]</i> </li></ul><br>  Spectrum Flow - reflects how quickly the energy of the spectrum changes, is calculated based on the spectrum on the current and previous frames.  It is defined as the second norm (Euclidean distance) between two normalized spectra: <br><br>  windowFFT = windowFFT / sum (windowFFT); <br>  F = sum ((windowFFT - windowFFTPrev). ^ 2); <br><br><ul><li>  <i>Spectral rolloff ("collapse" of energy) [22]</i> </li></ul><br>  It is defined as the relative frequency within which a certain part of the total energy of the spectrum is concentrated (set as parameter c): <br><br>  countFFT -&gt; c * totalEnergy <br>  SR = countFFT / lengthFFT <br><br>  A schematic representation of the Spectral rolloff value at c = 0.95: <br><br><img src="https://habrastorage.org/files/d39/e5f/6ec/d39e5f6ec07f4f9fbc5db21c30b935e1.jpg"><br><br><ul><li>  Signs that reflect the harmony of the audio signal <i>(harmonic ration, fundamental frequency</i> ): </li></ul><br><img src="https://habrastorage.org/files/575/20b/697/57520b697f204389b45139e099392467.jpg"><br><br>  <b>3. MFCC - Mel-frequency cepstral coefficients.</b>  <b>(Habra source [23])</b> <br>  The general scheme for obtaining chalk-frequency cepstral coefficients is presented in the following diagram [24]. <br><br><img src="https://habrastorage.org/files/b08/73c/b59/b0873cb5985d43a985d6755f377523ac.jpg"><br><br>  As mentioned earlier, the audio signal is divided into frames, pre-emphasis filtering and window weighting are performed, a fast Fourier transform is performed, then the spectrum is passed through a set of triangular filters evenly spaced on the chalk scale.  This leads to a higher density of filters in the low-frequency range and lower density in the high-frequency range, which reflects the sensitivity of the perception of sound signals by the human ear.  Thus, the basic information is ‚Äúremoved‚Äù from the audio signal in the low-frequency region, which is the most relevant feature of sound signals (especially speech).  Next, the samples are transferred to the cepstral space using a discrete cosine transform (DCT). <br><br><img src="https://habrastorage.org/files/488/98b/d64/48898bd6488946bf8ed06196ffe5b372.jpg"><br><br>  <i>Mathematical description:</i> <br><br>  Apply Fourier transform to the signal. <br><br><img src="https://habrastorage.org/files/b21/63d/c9b/b2163dc9b94a43e7b2ed9cedcc9e504c.gif"><br><br>  We make a combination of filters using the window function <br><br><img src="https://habrastorage.org/files/d07/d98/395/d07d983957084da0a21c919afc14e782.gif"><br><br>  For which the frequency f [m] is obtained from the equality <br><br><img src="https://habrastorage.org/files/f49/13f/bd8/f4913fbd825c49039981b4e675a1496b.gif"><br><br>  B (b) - conversion of the value of the frequency in the chalk scale, respectively, <br><br><img src="https://habrastorage.org/files/126/c42/7da/126c427da38842469a29fe4a06e1cec8.gif"><br><br>  We calculate the energy for each window <br><br><img src="https://habrastorage.org/files/573/21d/a07/57321da07af5493ca53065a37e11b916.gif"><br><br>  Apply DCT <br><br><img src="https://habrastorage.org/files/552/058/00b/55205800b36e42e5a2973de46871764c.gif"><br><br>  <b>4. LPC (Linear prediction coefficients)</b> <br><br>  The characteristics are coefficients predicting a signal based on a linear combination of previous samples.  [25] Essentially, they are coefficients of an FIR filter of the corresponding order: <br><br><img src="https://habrastorage.org/files/73a/7a2/71e/73a7a271e3124687a55c96c89680e9b4.png"><br><br>  They are also used in speech coding (Linear Prediction Coding), as it has been shown that they well approximate the characteristics of speech signals. <br><br>  In various articles, the effect of certain components of the feature vector on the recognition quality of audio events is investigated. <br><br><h4>  Postprocessing signs </h4><br>  After extracting the necessary features of the signal for their further use, signs are normalized so that each component of the feature vector has an average value of 0 and a standard deviation of 1: <br><br><img src="https://habrastorage.org/files/563/5d9/631/5635d963156e4c4992f6865b2b21529f.jpg"><br><br>  Often used the technique of <i>mid-term analysis</i> [26], when the averaging of signs over a set of successive frames.  As a rule, 1-10 seconds is selected as the interval for averaging. <br><br><img src="https://habrastorage.org/files/6e7/966/8bf/6e79668bf4b84395b3d0fd7a8529fb0c.jpg"><br><br>  As a rule, the dimension of the feature vector is quite large, which significantly affects the performance of the learning process in the future.  That is why in this case, the well-known and theoretically studied methods for reducing the dimension of the feature vector are applied (LDA, PCA, etc.) [27]. <br><br>  One of the advantages of using dimension reduction methods is the ability to significantly increase the speed of the learning process by reducing the number of signs.  Moreover, by selecting features that have the best discriminatory abilities in a separate set, it is possible to remove unnecessary, irrelevant information, which will increase the accuracy of machine learning algorithms (such as SVM, etc.) [28] <br><br>  These methods were addressed in the facial recognition material.  The difference between the PCA and LDA methods is that the PCA method allows one to reduce the dimension of the feature vector by identifying independent components that cover the variance in all events as much as possible. <br><br><img src="https://habrastorage.org/files/eea/4af/f56/eea4aff56e2e41eab79902b5bdd371f4.jpg"><br><br>  While LDA highlights components that show the best discriminatory abilities among a set of classes. <br><br><img src="https://habrastorage.org/files/e59/50d/679/e5950d67902641739d6bc3dc3a6d23f0.jpg"><br><br>  In both the PCA and LDA, at the learning stage, the matrix W is calculated, which determines the linear transformation of the original feature vector to the new space of the reduced dimension Y = WTX <br><br><h4>  Choosing a classifier </h4><br>  Finally, the last step is the selection of a classifier (training model).  Some studies in the field of audio analysis in detecting audio signals of interest are devoted to comparing the recognition accuracy when using different models of classifiers for various types of audio events.  It is noted that the use of hierarchical classifiers significantly increases the recognition accuracy in comparison with the use of multi-class classifiers.  [29] [19] <br><br>  The simplest version of the recognition model is the <b>Bayesian classifier</b> , which is based on the calculation of the likelihood function for each of the classes, and at the stage of recognition the event belongs to the class, the posterior probability of which is maximum among all classes.  The following figure shows an example of classification for a feature vector of dimension 2. Ellipses are centered with respect to the mean values ‚Äã‚Äãcorresponding to each of the classes, the red borders are a set of feature values ‚Äã‚Äãwhere the classes are equivalent. <br><br><img src="https://habrastorage.org/files/08f/8fa/b00/08f8fab002a94449b3f2919c99c5706e.jpg"><br><br>  In the case of selecting features that show good discriminatory properties, a high recognition result can be achieved using Bayesian classifiers. <br><br>  In contrast to the classical Bayesian classifier, where the parameters of the Gaussian distribution were used for the approximation of each class, the ‚Äúmix‚Äù of several Gaussians [19] is used as a function of the probability distribution density in <b>GMM (Gaussian Mixture Model)</b> [19]; learning using the EM (Expectation Maximization) algorithm.  As can be seen from the figure representing the result of the classification of the three classes, the area of ‚Äã‚Äãspace corresponding to each class is a more complex shape than the ellipse, the recognition model has become more accurate, respectively, the recognition result will improve. <br><br><img src="https://habrastorage.org/files/0a8/33a/2d9/0a833a2d93174237821c8b078935a1ce.gif"><br><br>  Among the shortcomings of this model, it is possible to note the high sensitivity to variations in the training sample of data when choosing a large number of Gaussian distributions, which can lead to a retraining process. <br><br>  <b>SVM (Support Vector Machine)</b> classifier [30], which translates the original feature vectors into a space with a higher dimension and finds the maximum separation from recognizable classes, is another way to classify the attributes extracted from an audio signal.  Not only a linear function, but also a polynomial function, as well as RBF (radial-basis function) [31], can act as the core. <br><br><img src="https://habrastorage.org/files/1d6/a4b/840/1d6a4b84092e41f0a8fdfef79b23a629.png"><br><br>  The advantage of this classifier is that at the stage of learning the method finds a band of maximum width, which at the stage of recognition will allow a more accurate classification. Among the shortcomings can be identified sensitivity to data standardization and noise. <br><br>  The use of <b>HMM (Hidden Markov Model)</b> [32] and <b>neural networks</b> [27] as a classifier was considered in our previous review of face recognition algorithms (Habraistochnik [33]). <br><br>  A page about audio analytics on <a href="http://synesis.ru/security/audioanalitika">the company's website</a> . <br><br>  Bibliography <br><br>  [1] "http://www.secuteck.ru/articles2/videonabl/10-trendov-videonablyudeniya-2014po-versii-kompanii-ihs/," [On the Internet]. <br>  [2] "http://www.shotspotter.com/," [On the Internet]. <br>  [3] "http://habrahabr.ru/post/200850/," [On the Internet]. <br>  [4] "http://sarov-itc.ru/," [On the Internet]. <br>  [5] "http://sarov-itc.ru/docs/acoustic_monitoring_description.pdf," [On the Internet]. <br>  [6] ‚Äúhttp://www.audioanalytic.com/,‚Äù [On the Internet]. <br>  [7] "http://www.freesound.org/," [On the Internet]. <br>  [8] ‚Äúhttp://sounds.bl.uk/,‚Äù [On the Internet]. <br>  [9] "http://www.pdsounds.org/," [On the Internet]. <br>  [10] ‚Äúhttp://macaulaylibrary.org/,‚Äù [On the Internet]. <br>  [11] "http://www.audiomicro.com/," [On the Internet]. <br>  [12] A. Dufaux, "Automatic Sound Detection And Recognition For Noisy Environment.". <br>  [13] IL Freire, ‚ÄúGunshot detection in noisy environments‚Äù. <br>  [14] F. Capman, "Abnormal audio event detection." <br>  [15] Mikulovich, ‚ÄúDigital Signal Processing,‚Äù [On the Internet]. <br>  [16] L. Gerosa, ‚ÄúSCREAM AND GUNSHOT DETECTION IN NOISY ENVIRONMENTS‚Äù. <br>  [17] C. Clavel, ‚ÄúEVENTS DETECTION FOR AN AUDIO-BASED SURVEILLANCE SYSTEM‚Äù. <br>  [18] A. Pikrakis, "GUNSHOT DETECTION IN AUDIO STREAMS FROM MOVIES BY MEANS OF DYNAMIC PROGRAMMING AND BAYESIAN NETWORKS". <br>  [19] S. Ntalampiras, "ON ACOUSTIC SURVEILLANCE OF HAZARDOUS SITUATIONS". <br>  [20] MF McKinney, "AUTOMATIC SURVEILLANCE OF THE ACOUSTIC ACTIVITY IN OUR LIVING". <br>  [21] D. Conte, ‚ÄúAn ensemble of rejecting classifiers for anomaly detection of audio events‚Äù. <br>  [22] G. Valenzise, ‚Äã‚Äã‚ÄúScreaming and Gunshot Detection and Localization for Audio Surveillance Systems‚Äù. <br>  [23] ‚Äúhttp://habrahabr.ru/post/140828/,‚Äù [On the Internet]. <br>  [24] I. Paraskevas, "Feature Extraction for Audio Classification of Gunshots Using the Hartley Transform". <br>  [25] W. Choi, ‚ÄúSelective Background Adaptation Based Abnormal Acoustic Event Recognition for Audio Surveillance.‚Äù <br>  [26] T. Giannakopoulos, "Realtime depression estimation using mid-term audio features," [On the Internet]. <br>  [27] J. Port-elo, ‚ÄúNON-SPEECH AUDIO EVENT DETECTION‚Äù. <br>  [28] B. Uzkent, "NON-SPEECH ENVIRONMENTAL SOUND CLASSIFICATION USING SVMS WITH A NEW SET OF FEATURES". <br>  [29] PK Atrey, "AUDIO BASED EVENT DETECTION FOR MULTIMEDIA SURVEILLANCE". <br>  [30] A. Kumar, ‚ÄúAUDIO EVENT DETECTION FROM ACOUSTIC UNIT OCCURRENCE PATTERNS‚Äù. <br>  [31] T. Ahmed, "IMPROVING EFFICIENCY AND RELIABILITY OF GUNSHOT DETECTION SYSTEMS". <br>  [32] M. Pleva, ‚ÄúAutomatic detection of audio events indicating threats‚Äù. <br>  [33] "http://habrahabr.ru/company/synesis/blog/238129/," [On the Internet]. </div><p>Source: <a href="https://habr.com/ru/post/250935/">https://habr.com/ru/post/250935/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../250921/index.html">Selenium for Python. Chapter 2. First Steps</a></li>
<li><a href="../250927/index.html">Payment to a Yandex.Money account with a VISA / MasterCard card or how to block an arbitrary wallet</a></li>
<li><a href="../250929/index.html">Running two or more MySQL instances on the same Linux server</a></li>
<li><a href="../250931/index.html">Moving to HTTPS on Nginx: cheat sheet</a></li>
<li><a href="../250933/index.html">Supercomputer in NArFU: the development of the Arctic by numerical methods</a></li>
<li><a href="../250937/index.html">At the start, attention, DUMP! Yekaterinburg, March 20</a></li>
<li><a href="../250939/index.html">How to make a website and make money on it (for performers)</a></li>
<li><a href="../250941/index.html">Problem solving experience by creating an OLAP cube using C #</a></li>
<li><a href="../250943/index.html">Building an animated pie chart on jQuery</a></li>
<li><a href="../250945/index.html">ZeroNet - Distributed sites via Bittorrent and Bitcoin</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>