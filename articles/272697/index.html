<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Optimizing hyperparameters in Vowpal Wabbit with the new vw-hyperopt module</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! In this article we will discuss this not very pleasant aspect of machine learning, as the optimization of hyperparameters. Two weeks ago, th...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Optimizing hyperparameters in Vowpal Wabbit with the new vw-hyperopt module</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr!  In this article we will discuss this not very pleasant aspect of machine learning, as the optimization of hyperparameters.  Two weeks ago, the <b>vw-hyperopt.py</b> module, which can find good configurations of hyper parameters of Vowpal Wabbit models in large spaces, was poured into a very well-known and useful <a href="https://github.com/JohnLangford/vowpal_wabbit">Vowpal Wabbit</a> project.  The module was developed inside DCA (Data-Centric Alliance). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/3f7/cdd/c7e/3f7cddc7edad47d684c2588688d04d5b.jpg"></div><br>  To find good configurations, vw-hyperopt uses algorithms from the Python library <a href="https://github.com/hyperopt/hyperopt">Hyperopt</a> and can optimize hyperparameters adaptively using the Tree-Structured Parzen Estimators (TPE) method.  This allows you to find better optima than a simple grid search, with an equal number of iterations. <br><br>  This article will be interesting to everyone who deals with Vowpal Wabbit, and especially to those who are annoyed by the lack of methods for tuning numerous model handles in the source code, and either manually typing them or by optimizing on their own. <br><a name="habracut"></a><br><h2>  Hyperparameters </h2><br>  <i>What are hyperparameters?</i>  These are all ‚Äúdegrees of freedom‚Äù of the algorithm, which it does not directly optimize, but on which the result depends.  Sometimes the result depends quite a bit, and then, if it is not a kaggle, you can get by with default values ‚Äã‚Äãor select manually.  But sometimes an unsuccessful configuration can spoil everything: the algorithm is either greatly retrained, or, conversely, will not be able to use most of the information. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In the narrow sense, hyperparameters are often understood only as regularization and other ‚Äúobvious‚Äù settings of machine learning methods.  However, in a broad sense, hyperparameters are generally any manipulations with data that can affect the result: engineering of features, weighing observations, undersampling, etc. <br><br><h3>  Grid search </h3><br>  Of course, it would be nice to have an algorithm that, in addition to optimizing parameters, would also optimize hyperparameters.  Even better, if we could trust this algorithm more than intuition.  Some steps in this direction, of course, have long been made.  Naive methods are built into many machine learning libraries: grid search - a grid scan, or random search - sampling points from a fixed distribution (the most well-known instances are GridSearchCV and RandomizedGridSearchCV to sklearn).  The advantage of passing through the grid is that it is easy to code it yourself and easy to parallelize.  However, it has serious drawbacks: <br><br><ul><li>  He goes through many obviously unsuccessful points.  Suppose there is already a set of some configurations with results or some other information.  A person can understand which configurations will accurately provide a settling result, and will guess not to check these regions once again.  Grid search can't do that. <br><br></li><li>  If there are many hyperparameters, then the size of the ‚Äúcell‚Äù has to be made too large, and you can miss a good optimum.  Thus, if we include in the search space a lot of unnecessary hyperparameters that do not affect the result in any way, then grid search will work much worse with the same number of iterations.  However, for random search this is true to a lesser extent: <br></li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/files/7d1/2fe/6d1/7d12fe6d1e334bdfb273ae9bdebfd75e.jpg" height="285" width="575"></div><br><h3>  Bayesian methods </h3><br>  In order to reduce the number of iterations to find a good configuration, adaptive Bayesian methods are invented.  They choose the next point to check, taking into account the results on the already checked points.  The idea is to find a compromise at every step between (a) exploring regions near the most successful points among those found and (b) exploring regions with great uncertainty, where even more successful points can be found.  This is often called the explore-exploit dilemma or ‚Äúlearning vs earning‚Äù.  Thus, in situations where checking every new point is expensive (in machine learning, checking = training + validation), you can get closer to the global optimum in a much smaller number of steps. <br><br>  Similar algorithms in different variations are implemented in the <a href="https://github.com/Yelp/MOE">MOE</a> , <a href="https://github.com/JasperSnoek/spearmint">Spearmint</a> , <a href="http://www.cs.ubc.ca/labs/beta/Projects/SMAC/">SMAC</a> , <a href="http://rmcantin.bitbucket.org/html/">BayesOpt</a> and <a href="http://jaberg.github.io/hyperopt/">Hyperopt tools</a> .  We will dwell on the latter in more detail, since <code>vw-hyperopt</code> is a wrapper over Hyperopt, but first you need to write a little about Vowpal Wabbit. <br><br><h3>  Vowpal wabbit </h3><br>  Many of you have probably used this tool or at least heard about it.  In short, it is one of the fastest (if not the fastest) machine learning library in the world.  To train the model for our CTR-predictor (binary classification) on 30 million observations and tens of millions of features takes only a few gigabytes of RAM and 6 minutes on one core.  Vowpal Wabbit implements several online algorithms: <br><br><ul><li>  Stochastic gradient descent with different bells and whistles; </li><li>  FTRL-Proximal, which you can read about <a href="https://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf">here</a> ; </li><li>  Online similarity to SVM; </li><li>  Online boosting; </li><li>  Factoring machines. </li></ul><br>  In addition, feed-forward neural networks, batch optimization (BFGS) and LDA are implemented in it.  You can run Vowpal Wabbit in the background and take the data stream to the input, either by learning to them or simply by making predictions. <br><br>  FTRL and SGD can solve both regression and classification problems, this is regulated only by the loss function.  These algorithms are linear with respect to features, but non-linearity can easily be achieved with the help of polynomial features.  There is a very useful mechanism for early stopping in order to avoid retraining if too many epochs are indicated. <br><br>  Vowpal Wabbit is also famous for its feature hashing, which serves as an additional regularization if there are a lot of features.  Due to this, it is possible to study categorical features with billions of rare categories, fitting the model into the operational memory without sacrificing quality. <br><br>  Vowpal Wabbit requires a special <a href="https://github.com/JohnLangford/vowpal_wabbit/wiki/Input-format">input data format</a> , but it is easy to understand.  It is naturally sparse and takes up little space.  Only one observation (or several, for LDA) is loaded into RAM at any time.  Learning is easiest to run through the console. <br><br>  Those interested can read the <a href="https://github.com/JohnLangford/vowpal_wabbit/wiki/Tutorial">tutorial</a> and other examples and articles in their repositories, as well as the <a href="http://www.slideshare.net/jakehofman/technical-tricks-of-vowpal-wabbit">presentation</a> .  About the insides of Vowpal Wabbit can be read in detail in the <a href="http://hunch.net/~jl/projects/projects.html">publications of John Langford</a> and in <a href="http://hunch.net/">his blog</a> .  Habr√© also has a <a href="http://habrahabr.ru/company/mlclass/blog/248779/">suitable post</a> .  The list of arguments can be obtained through <code>vw --help</code> or read a <a href="https://github.com/JohnLangford/vowpal_wabbit/wiki/Command-line-arguments">detailed description</a> .  As can be seen from the description, there are dozens of arguments, and many of them can be considered as hyper-parameters that can be optimized. <br><br>  In Vowpal Wabbit, there is a <a href="https://github.com/JohnLangford/vowpal_wabbit/wiki/Using-vw-hypersearch">vw-hypersearch module</a> , which can pick up one hyperparameter <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%25B7%25D0%25BE%25D0%25BB%25D0%25BE%25D1%2582%25D0%25BE%25D0%25B3%25D0%25BE_%25D1%2581%25D0%25B5%25D1%2587%25D0%25B5%25D0%25BD%25D0%25B8%25D1%258F">using the golden section method</a> .  However, if there are several local minima, this method is likely to find a far from the best option.  In addition, it is often necessary to optimize many hyperparameters at once, and this is not in vw-hypersearch.  A couple of months ago I tried to write a multidimensional method of the golden section, but the number of steps that he needed for convergence exceeded any grid search, so this option was dropped.  It was decided to use Hyperopt. <br><br><h2>  Hyperopt </h2><br>  This library, written in python, implements the Tree-Structured Parzen Estimators (TPE) optimization algorithm.  Its advantage is that it can work with very ‚Äúawkward‚Äù spaces: when one hyperparameter is continuous, the other is categorical;  the third is discrete, but its neighboring values ‚Äã‚Äãare correlated with each other;  Finally, some combinations of parameter values ‚Äã‚Äãmay simply not make sense.  TPE takes as input a hierarchical search space with a priori probabilities, and at each step mixes them with a Gaussian distribution with a center at a new point.  Its author, James Bergster, claims that this algorithm solves the problem of explore-exploit quite well and works better both on grid search and expert busting, at least for deep learning tasks, where there are especially many hyperparameters.  Read more about it <a href="http://jmlr.csail.mit.edu/proceedings/papers/v28/bergstra13.pdf">here</a> and <a href="http://compneuro.uwaterloo.ca/files/publications/bergstra.2014.pdf">here</a> .  About the TPE algorithm itself can be read <a href="http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf">here</a> .  Perhaps in the future it will be possible to write a detailed post about him. <br><br>  Although Hyperopt was not built into the source code of known machine learning libraries, many use it.  For example, here is a <a href="https://districtdatalabs.silvrback.com/parameter-tuning-with-hyperopt">great tutorial on hyperopt + sklearn</a> .  Here is the <a href="https://github.com/bamine/Kaggle-stuff/blob/master/otto/hyperopt_xgboost.py">application of hyperopt + xgboost</a> .  All my input is a similar wrapper for Vowpal Wabbit, a more or less tolerable syntax for defining the search space and running it all from the command line.  Since Vowpal Wabbit did not yet have this functionality, Langford liked my module, and it was poured into it.  In fact, anyone can try Hyperopt for their favorite machine learning tool: this is easy to do, and everything you need is in <a href="https://github.com/hyperopt/hyperopt/wiki/FMin">this tutorial</a> . <br><br><h2>  vw-hyperopt </h2><br>  We now turn to using the <code>vw-hyperopt</code> .  First you need to install the latest version of Vowpal Wabbit from the github.  The module is located in the utl folder. <br><br>  <b>Attention!</b>  The latest changes (in particular, the new command syntax) so far (as of December 15) are not merged into the main repository.  In the coming days, I hope the problem will be solved, but for now you can use the latest version of the code <a href="https://github.com/kirillfish/vowpal_wabbit/blob/vw-hyperopt-2/utl/vw-hyperopt.py">from my branch</a> .  <b>EDIT:</b> On December 22, the changes are merged, now you can use the main repository. <br><br><h3>  Usage example: </h3><br><pre> <code class="bash hljs">./vw-hyperopt.py --train ./train_set.vw --holdout ./holdout_set.vw --max_evals 200 --outer_loss_function logistic --vw_space <span class="hljs-string"><span class="hljs-string">'--algorithms=ftrl,sgd --l2=1e-8..1e-1~LO --l1=1e-8..1e-1~LO -l=0.01..10~L --power_t=0.01..1 --ftrl_alpha=5e-5..8e-1~L --ftrl_beta=0.01..1 --passes=1..10~I --loss_function=logistic -q=SE+SZ+DR,SE~O --ignore=T~O'</span></span> --plot</code> </pre> <br>  The input module requires training and validation samples, as well as a priori distribution of hyper <code>--vw_space</code> (quoted inside <code>--vw_space</code> ).  You can specify integer, continuous, or categorical hyperparameters.  For all but categorical, you can set a uniform or log-uniform distribution.  The example search space is converted inside <code>vw-hyperopt</code> approximately into such an object for <code>Hyperopt</code> (if you have completed the <code>Hyperopt</code> tutorial, you will understand this): <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> hyperopt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> hp prior_search_space = hp.choice(<span class="hljs-string"><span class="hljs-string">'algorithm'</span></span>, [ {<span class="hljs-string"><span class="hljs-string">'type'</span></span>: <span class="hljs-string"><span class="hljs-string">'sgd'</span></span>, <span class="hljs-string"><span class="hljs-string">'--l1'</span></span>: hp.choice(<span class="hljs-string"><span class="hljs-string">'sgd_l1_outer'</span></span>, [<span class="hljs-string"><span class="hljs-string">'empty'</span></span>, hp.loguniform(<span class="hljs-string"><span class="hljs-string">'sgd_l1'</span></span>, log(<span class="hljs-number"><span class="hljs-number">1e-8</span></span>), log(<span class="hljs-number"><span class="hljs-number">1e-1</span></span>))]), <span class="hljs-string"><span class="hljs-string">'--l2'</span></span>: hp.choice(<span class="hljs-string"><span class="hljs-string">'sgd_l2_outer'</span></span>, [<span class="hljs-string"><span class="hljs-string">'empty'</span></span>, hp.loguniform(<span class="hljs-string"><span class="hljs-string">'sgd_l2'</span></span>, log(<span class="hljs-number"><span class="hljs-number">1e-8</span></span>), log(<span class="hljs-number"><span class="hljs-number">1e-1</span></span>))]), <span class="hljs-string"><span class="hljs-string">'-l'</span></span>: hp.loguniform(<span class="hljs-string"><span class="hljs-string">'sgd_l'</span></span>, log(<span class="hljs-number"><span class="hljs-number">0.01</span></span>), log(<span class="hljs-number"><span class="hljs-number">10</span></span>)), <span class="hljs-string"><span class="hljs-string">'--power_t'</span></span>: hp.uniform(<span class="hljs-string"><span class="hljs-string">'sgd_power_t'</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), <span class="hljs-string"><span class="hljs-string">'-q'</span></span>: hp.choice(<span class="hljs-string"><span class="hljs-string">'sgd_q_outer'</span></span>, [<span class="hljs-string"><span class="hljs-string">'emtpy'</span></span>, hp.choice(<span class="hljs-string"><span class="hljs-string">'sgd_q'</span></span>, [<span class="hljs-string"><span class="hljs-string">'-q SE -q SZ -q DR'</span></span>, <span class="hljs-string"><span class="hljs-string">'-q SE'</span></span>])]), <span class="hljs-string"><span class="hljs-string">'--loss_function'</span></span>: hp.choice(<span class="hljs-string"><span class="hljs-string">'sgd_loss'</span></span>, [<span class="hljs-string"><span class="hljs-string">'logistic'</span></span>]), <span class="hljs-string"><span class="hljs-string">'--passes'</span></span>: hp.quniform(<span class="hljs-string"><span class="hljs-string">'sgd_passes'</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), }, {<span class="hljs-string"><span class="hljs-string">'type'</span></span>: <span class="hljs-string"><span class="hljs-string">'ftrl'</span></span>, <span class="hljs-string"><span class="hljs-string">'--l1'</span></span>: hp.choice(<span class="hljs-string"><span class="hljs-string">'ftrl_l1_outer'</span></span>, [<span class="hljs-string"><span class="hljs-string">'emtpy'</span></span>, hp.loguniform(<span class="hljs-string"><span class="hljs-string">'ftrl_l1'</span></span>, log(<span class="hljs-number"><span class="hljs-number">1e-8</span></span>), log(<span class="hljs-number"><span class="hljs-number">1e-1</span></span>))]), <span class="hljs-string"><span class="hljs-string">'--l2'</span></span>: hp.choice(<span class="hljs-string"><span class="hljs-string">'ftrl_l2_outer'</span></span>, [<span class="hljs-string"><span class="hljs-string">'emtpy'</span></span>, hp.loguniform(<span class="hljs-string"><span class="hljs-string">'ftrl_l2'</span></span>, log(<span class="hljs-number"><span class="hljs-number">1e-8</span></span>), log(<span class="hljs-number"><span class="hljs-number">1e-1</span></span>))]), <span class="hljs-string"><span class="hljs-string">'-l'</span></span>: hp.loguniform(<span class="hljs-string"><span class="hljs-string">'ftrl_l'</span></span>, log(<span class="hljs-number"><span class="hljs-number">0.01</span></span>), log(<span class="hljs-number"><span class="hljs-number">10</span></span>)), <span class="hljs-string"><span class="hljs-string">'--power_t'</span></span>: hp.uniform(<span class="hljs-string"><span class="hljs-string">'ftrl_power_t'</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), <span class="hljs-string"><span class="hljs-string">'-q'</span></span>: hp.choice(<span class="hljs-string"><span class="hljs-string">'ftrl_q_outer'</span></span>, [<span class="hljs-string"><span class="hljs-string">'emtpy'</span></span>, hp.choice(<span class="hljs-string"><span class="hljs-string">'ftrl_q'</span></span>, [<span class="hljs-string"><span class="hljs-string">'-q SE -q SZ -q DR'</span></span>, <span class="hljs-string"><span class="hljs-string">'-q SE'</span></span>])]), <span class="hljs-string"><span class="hljs-string">'--loss_function'</span></span>: hp.choice(<span class="hljs-string"><span class="hljs-string">'ftrl_loss'</span></span>, [<span class="hljs-string"><span class="hljs-string">'logistic'</span></span>]), <span class="hljs-string"><span class="hljs-string">'--passes'</span></span>: hp.quniform(<span class="hljs-string"><span class="hljs-string">'ftrl_passes'</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), <span class="hljs-string"><span class="hljs-string">'--ftrl_alpha'</span></span>: hp.loguniform(<span class="hljs-string"><span class="hljs-string">'ftrl_alpha'</span></span>, <span class="hljs-number"><span class="hljs-number">5e-5</span></span>, <span class="hljs-number"><span class="hljs-number">8e-1</span></span>), <span class="hljs-string"><span class="hljs-string">'--ftrl_beta'</span></span>: hp.uniform(<span class="hljs-string"><span class="hljs-string">'ftrl_beta'</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, <span class="hljs-number"><span class="hljs-number">1.</span></span>) } ])</code> </pre> <br>  Optionally, you can change the loss function on the validation sample and the maximum number of iterations ( <code>--outer_loss_function</code> , the default is <code>logistic</code> , and <code>--max_evals</code> , the default is 100).  You can also save the results of each iteration and build graphs with <code>--plot</code> if <code>matplotlib</code> installed and, preferably, <code>seaborn</code> : <br><br><img src="https://habrastorage.org/files/e64/48b/0ba/e6448b0baf3b4e7393d6dc42ef99466e.png"><br><br><h2>  Documentation </h2><br>  Since it is not customary to lay out detailed documentation on Habrahabr, I will limit myself to referring to it.  You can read about all the semantics in the <a href="https://github.com/kirillfish/vowpal_wabbit/wiki/%25D0%2598%25D1%2581%25D0%25BF%25D0%25BE%25D0%25BB%25D1%258C%25D0%25B7%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5-vw-hyperopt">Russian-language wiki in my fork</a> or wait for the English version in the main Vowpal Wabbit repository. <br><br><h2>  Plans </h2><br>  In the future, it is planned to add to the module: <br><br><ol><li>  Support regression and multiclass classification tasks. <br><br></li><li>  Support for a ‚Äúwarm start‚Äù: give Hyperopt pre-estimated points, and start optimizing considering the results for them. <br><br></li><li>  The error estimation option at each step on another test sample (but without optimizing the hyperparameters on it).  This is necessary in order to better assess the generalizing ability ‚Äî have we not retrained? <br><br></li><li>  Support for binary parameters that do not take any values, such as <code>--lrqdropout, --normalized, --adaptive</code> , etc.  Now you can, in principle, write <code>--adaptive=\ ~O</code> , but this is not intuitive at all.  You can do something like <code>--adaptive=~B</code> or <code>--adaptive=~BO</code> . </li></ol><br>  I would be very happy if someone uses the module and it will help someone.  I will welcome any suggestions, ideas or bugs found.  You can write them here or create an issue <a href="https://github.com/JohnLangford/vowpal_wabbit/issues">on the githaba</a> . <br><br><h2>  Update 12/22/2015 </h2><br>  Pull request with the latest changes in the main repository Vowpal Wabbit, so that you can now use it, not a branch. </div><p>Source: <a href="https://habr.com/ru/post/272697/">https://habr.com/ru/post/272697/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../272679/index.html">Neural network in Python, part 2: gradient descent</a></li>
<li><a href="../272681/index.html">Compare Swift and Rust</a></li>
<li><a href="../272689/index.html">Own index types in Cach√© DBMS</a></li>
<li><a href="../272693/index.html">Microsoft fixed a dangerous vulnerability in Windows Server</a></li>
<li><a href="../272695/index.html">As I have been rewriting my cryptocurrency with PHP for Go for 8 months. Part 1</a></li>
<li><a href="../272701/index.html">How to graze cats. The history of building a system of control and accounting of working time for an IT company</a></li>
<li><a href="../272703/index.html">The WikiLeaks Revolution: The Digest of Mishaps</a></li>
<li><a href="../272705/index.html">Record and video processing on Android</a></li>
<li><a href="../272707/index.html">Bit magic: getting the next lexicographic combination</a></li>
<li><a href="../272709/index.html">Site navigation - burping a bygone era</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>