<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>UX research and evidence strength</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello to all! I bring to your attention the translation of the article Dr. Philip Hodgson ( @bpusability on Twitter). He has a BSc, MA and PhD in expe...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>UX research and evidence strength</h1><div class="post__text post__text-html js-mediator-article"><blockquote><p>  Hello to all!  I bring to your attention the translation of the <a href="https://www.userfocus.co.uk/articles/strength-of-evidence.html">article</a> Dr.  Philip Hodgson ( <a href="https://twitter.com/">@bpusability</a> on Twitter).  He has a BSc, MA and PhD in experimental psychology.  He has more than 20 years of experience as a researcher, consultant and trainer in usability, user experience, human factors and experimental psychology.  His work has influenced product and system design in the areas of consumption, telecommunications, manufacturing, packaging, public safety, web and medical areas for the markets of North America, Europe and Asia. </p></blockquote><br><p>  The concept of "strength of proof" plays an important role in all areas of research, but is rarely discussed in the context of user interaction research.  We will find out what this means in a UX study, and group the research methods by the power of the data they provide. </p><a name="habracut"></a><br><p>  Someone once said: "There are no questions in usability."  I think it was me.  I admit that it was not the best statement.  It does not stand next to words such as ‚ÄúNever on the margins of human conflicts‚Äù and ‚ÄúOne small step for a man‚Äù, but, nevertheless, it makes sense and leads to a useful rule of thumb for UX researchers. </p><br><p>  Let me explain. </p><br><p>  A few years ago, while working at a large corporation and preparing a usability test, the project manager called me and asked me to send him a list of questions about usability. </p><br><p>  ‚ÄúThere are no questions in usability,‚Äù I replied. </p><br><p>  ‚ÄúWhat do you mean?‚Äù He asked, ‚ÄúHow can there be no questions?  How do you plan to understand whether people like our new design? ". </p><br><p>  ‚ÄúAnd I'm not trying to understand whether he likes them or not,‚Äù I grumbled.  ‚ÄúI'm trying to figure out if they can use it.  I have a list of tasks, not questions. ‚Äù </p><br><h3>  Good and bad data </h3><br><p>  Requests to add explicit questions like ‚Äúwhat do you think?‚Äù In the UX study not only indicates that some stakeholders do not understand the purpose of usability testing, but also that they believe in the great value of each answer of the testing participant.  This shows that they are not aware of the concept of <strong>good</strong> and <strong>bad</strong> data and, as a result, they believe that everything the user has said is useful. </p><br><p>  But it is not. </p><br><p>  There are strong and weak data.  This is true for all areas of scientific research, be it the development of a new drug, the discovery of a new planet, the discovery of a crime, or the evaluation of a new software interface. </p><br><p>  UX research is the direct observation of what people do.  This is not a collection of their opinions.  This is because <strong>as data, opinions are useless</strong> .  For every 10 people who like your design, there are 10 others who will hate it, and another 10 who will not care at all.  <strong>Opinions are not proof.</strong> </p><br><p>  Behavior, on the contrary, is evidence.  That is why the detective will rather catch someone by the hand at the time of the commission of the crime than simply believe someone for the word.  Therefore, the advice is often repeated: <strong>‚ÄúPay attention to what people do, and not what they say</strong> . <strong>‚Äù</strong>  This advice has almost become a clich√© in UX, but with this you can start a discussion about something important, for example, about the <strong>strength of evidence.</strong>  It is a good idea that some data is supported by strong evidence, some relatively strong, and some weak.  <strong>Nobody wants weak evidence at the heart of their product development.</strong> </p><br><p> <strong><img src="https://habrastorage.org/webt/ll/vu/g8/llvug8psncobwfcnimteczjhl2i.jpeg" width="768" height="512"></strong> </p><br><h3>  Evidence in UX research </h3><br><p>  Evidence is what we use to back up our statements and arguments.  This is what gives us credibility when we make decisions on specific design parameters, product features, on when to complete the next iteration in design, on ‚Äúdo / not do‚Äù decisions and on launching a new product, service or site.  Proof is what we provide to our development team and what we lay on the table during disagreements and disputes.  We substantiate our arguments with evidence based on good data.  Data is part of the study.  "Data!  Data!  Data! ‚ÄùShouted Sherlock Holmes.  "I can't make bricks without clay." </p><br><p>  It may seem that UX studies are events conducted on the principle of ‚Äúfirst method‚Äù (‚Äúwe need a usability test‚Äù, ‚ÄúI want to conduct a contextual survey‚Äù, ‚Äúlet's fix card sorting‚Äù), but a UX researcher that <a href="http://www.userfocus.co.uk/articles/going-beyond-the-obvious.html">focuses on primary research questions</a> , operates on the principle of "first data": <strong>"What type of data should I collect in order to provide reliable and convincing evidence on this issue?"</strong> .  And then follows the method. </p><br><p><img src="https://habrastorage.org/webt/el/uh/em/eluhem8hrb79r8non-dy3hdubh8.jpeg" width="796" height="448"></p><br><h3>  What is strong evidence? </h3><br><p>  Strong evidence follows from data that is reliable and reliable. </p><br><p>  In usability testing, reliable data is things like an indicator of task completion and efficiency, but not aesthetic appeal or personal preference. </p><br><p>  Reliable data is data from a survey that was conducted again using the same method as last time, but with the participation of other respondents. </p><br><p>  Regardless of the method, research data must be reliable and reliable.  Otherwise, they are simply thrown away. </p><br><p>  In UX studies, strong data comes from the performance of tasks, from observations of the user (objective and <a href="http://www.userfocus.co.uk/articles/fixing-experimenter-bias.html">independent</a> ), and when the user is unbiasedly caught ‚Äúby the hand‚Äù.  The data becomes strong along with our level of confidence and assures us that the continuation of the study is unlikely to change our level of confidence in our findings. </p><br><p>  Below is a brief systematization of methods based on levels of evidence.  In essence, this is a systematization of the data types that methods provide.  It is assumed that each method was well thought out.  This is not an exhaustive list, but it does include a list of the main methods that UX researchers typically use in creating user-oriented design. </p><br><h3>  Examples of strong UX evidence </h3><br><p>  Strong UX-proofs inevitably include target users performing tasks, or involved in activity that is relevant to the concept being developed or the problem under investigation.  Here they are: </p><br><ul><li>  <a href="http://www.userfocus.co.uk/articles/the_beginners_guide_to_contextual_interviewing.html">Contextual research</a> (field ethnographic research or other options, when the behavior of users is recorded when they perform their work or achieve their goals) </li><li>  <a href="http://www.userfocus.co.uk/articles/2-kinds-of-usability-test.html">Forming and summing usability tests</a> in which users perform tasks in the product interface </li><li>  Web / search analytics or any type of automatically collected product usage data </li><li>  A / B or multivariate testing </li><li>  <a href="http://www.userfocus.co.uk/articles/how_to_experiment.html">Controlled experiments</a> </li><li>  Task analysis </li><li>  Secondary behavioral research based on meta-analyzes and peer-reviewed articles, as well as previous UX reports that fully describe the method used </li></ul><br><h3>  Examples of relatively strong UX evidence </h3><br><p>  In order to define this category, the data must be based on the results of research, which at a minimum include the performance of tasks - either by users or experts, or include independent reporting of actual behavior.  These methods are often the precursors of methods from the ‚Äústrong‚Äù category.  They fall into this category because the data they give is usually less constant and less accurate. </p><br><ul><li>  <a href="http://www.userfocus.co.uk/articles/heuristics.html">Heuristic estimates</a> </li><li>  <a href="http://www.userfocus.co.uk/articles/cogwalk.html">Cognitive step by step problem solving</a> </li><li>  Feedback usability experts who performed real tasks </li><li>  Interview or any kind of report on your own behavior of the <a href="https://www.userfocus.co.uk/articles/eliciting-user-goals-part1.html">Jobs to be done type</a> </li><li>  <a href="http://www.userfocus.co.uk/articles/user-journey-mapping-workshop.html">Building user routes</a> </li><li>  <a href="http://www.userfocus.co.uk/articles/60-ways-to-understand-user-needs.html">Diary studies</a> </li><li>  <a href="http://www.userfocus.co.uk/articles/cardgames.html">Card sorting</a> </li><li>  Eye tracking </li><li>  <a href="https://medium.com/user-research/stop-accosting-people-in-coffee-shops-fd95c1629b5a">Pop-up study "partisans"</a> in cafes, libraries, etc. </li></ul><br><h3>  Examples of weak UX evidence </h3><br><p>  Decisions based on weak or erroneous data can cost companies millions of dollars if these decisions translate into poor design, poor marketing, or incorrect statements about the product. </p><br><p>  An obvious question arises: <strong>why do research, the result of which is bad data?</strong> </p><br><p>  There is no need. </p><br><p>  Data from these methods are not needed in UX studies.  They are only slightly better than simple assumptions.  If you can choose to spend a project budget on such methods or on charity, select the latter. </p><br><ul><li>  Any type of pseudo-usability testing, for example, interviewing people about which design they like more, or tests that largely depend on interviews for primary data collection </li><li>  Undefined testing out loud, which allows users to think that they are experts, not respondents who perform tasks </li><li>  Evaluation of usability, even by experts.  This is the same as ‚Äúkicking tires.‚Äù </li><li>  <a href="https://medium.com/research-things/focus-groups-are-worthless-7d30891e58f1">Group focus</a> ( <a href="http://www.userfocus.co.uk/articles/focuspocus.html">don't make me start</a> ) </li><li>  <a href="http://www.greenbookblog.org/2015/12/01/why-surveys-cannot-be-trusted/">Polls</a> (you have the right to disagree, but only if you slept during the US elections in 2016) </li><li>  Intuition, appeal to the authorities or personal experience </li><li>  Opinion of friends, colleagues, family, your boss, managers and company executives </li></ul><br><h3>  How to evaluate the power of evidence in a study or report </h3><br><p>  Start asking these questions: </p><br><ul><li>  Why should I believe this statement? </li><li>  How good is the proof? </li><li>  Can I count on these findings? </li></ul><br><p>  There are no dirty tricks in these questions: anyone who presents the findings of the study should be able to answer them. </p><br><p>  During the study, ask yourself: </p><br><ul><li>  I observe people who work (perform tasks in the prototype) or listen to what they say (share opinions on design)? </li><li>  Interviewed people reflect on what they can do in the future, or rely on events that happened to them in the past? </li></ul><br><p>  Some time ago I created a <a href="http://www.userfocus.co.uk/resources/researchquestions.html">checklist for evaluating research methods</a> .  If you want to give the research a good shake, you will find a lot of interesting things there. </p><br><p>  I started this article by promising a rule of thumb.  Here it is.  Use it as a mantra when evaluating the power of user research: </p><blockquote>  <strong>Behavior - strong data.</strong>  <strong>Opinions - weak data.</strong> </blockquote></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/347994/">https://habr.com/ru/post/347994/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../347984/index.html">From prototype to production on Kickstarter: $ 100,000 is not enough</a></li>
<li><a href="../347986/index.html">Mobile App Auto-Test Tools Guide</a></li>
<li><a href="../347988/index.html">Open lesson on the topic "Areas of knowledge of marketing and TV + Search"</a></li>
<li><a href="../347990/index.html">In a section: the news aggregator on Android with backend. Distributed Message Processing Systems (Spark, Storm)</a></li>
<li><a href="../347992/index.html">False alarms. New technique of catching two birds with one stone</a></li>
<li><a href="../347996/index.html">Parsing sites or long-term construction of the Moscow region</a></li>
<li><a href="../347998/index.html">We integrate TeamCity with JIRA - without plug-ins and administrators</a></li>
<li><a href="../348000/index.html">Convolutional neural network, part 1: structure, topology, activation functions and training set</a></li>
<li><a href="../348002/index.html">News from the world of OpenStreetMap ‚Ññ392 (01.16.2018-22.01.2018)</a></li>
<li><a href="../348004/index.html">Swift Package Manager</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>