<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The book "Working with BigData in the clouds. Processing and storing data with examples from Microsoft Azure ¬ª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Before you - the first initially Russian-language book, in which the real examples deal with the secrets of big data processing (Big Data) in the clou...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The book "Working with BigData in the clouds. Processing and storing data with examples from Microsoft Azure ¬ª</h1><div class="post__text post__text-html js-mediator-article"> <a href="https://habr.com/company/piter/blog/428375/"><img src="https://habrastorage.org/webt/un/xm/vy/unxmvydwj2ybjfabsmi3vbdcnsg.jpeg" align="left" alt="image"></a>  Before you - the first initially Russian-language book, in which the real examples deal with the secrets of big data processing (Big Data) in the clouds. <br><br>  The focus is on Microsoft Azure and AWS solutions.  We consider all stages of work - obtaining data prepared for processing in the cloud, the use of cloud storage, cloud-based data analysis tools.  Particular attention is paid to the SAAS services, demonstrating the advantages of cloud technologies compared to solutions deployed on dedicated servers or in virtual machines. <br><br>  The book is designed for a wide audience and will serve as an excellent resource for the development of Azure, Docker and other essential technologies, without which a modern enterprise is unthinkable. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      We offer you to familiarize yourself with the passage "Direct download of streaming data" <br><a name="habracut"></a><br><h3>  10.1.  Common architecture </h3><br>  In the previous chapter, we looked at the situation where a multitude of client applications have to send a large number of messages that need to be dynamically processed, put into storage and then processed again in it.  At the same time, it is necessary to be able to change the logic of the data processing and storage flow, without resorting to changing the client code.  And finally, from the point of view of security considerations, clients should have the right to do only one thing - send messages or receive them, but in no way read the data or delete the bases, and they should not have direct rights to record this data. <br><br>  Similar tasks are very common in systems that work with IoT devices connected via an Internet connection, as well as in online log analysis systems.  In addition to the above requirements for our dedicated service, there are two more requirements related to the specifics of the ‚ÄúInternet of things‚Äù and to ensure reliable processing of messages.  First of all, the interaction protocol between the client and the service receiver must be very simple, so that it can be implemented on a device with limited computing capabilities and with very limited memory (for example, the Arduino platform, Intel Edison, STM32 Discovery and other ‚Äúnon-binding‚Äù, such as before RaspberryPi).  The next requirement is reliable message delivery, regardless of possible failures of processing services.  This is a stronger requirement than high reliability.  Indeed, to ensure the overall reliability of the entire system, it is necessary that the reliability of all its components be high enough and the addition of a new component does not lead to a noticeable increase in the number of failures.  In addition to the failure in the cloud infrastructure, there may be an error in the service created by the user.  And even then the message should be processed as soon as the user service is restored.  To do this, the message receiving service must reliably store the message until it has been processed or until its lifetime has expired (this is necessary to prevent memory overflow with a continuous message flow).  A service with these properties is called a message hub (Event Hub).  For IoT devices, there are specialized hubs (IoT Hub) that have a number of other properties that are very important for use in conjunction with ‚ÄúInternet of things‚Äù devices (for example, bidirectional communication from one point, built-in message routing, digital twins) of the device and others).  However, these services are still specialized, and we will not consider them in detail. <br><br>  Before turning to the concept of message concentration, we turn to the ideas underlying it. <br><br>  Suppose we have a source of messages (for example, requests coming from a client) and a service that must process them.  Processing a separate request takes time and requires computational resources (CPU, memory, IOPS).  Moreover, during the processing of a single request, other requests cannot be processed.  In order for client applications not to hang while waiting for the service to be released, it is necessary to separate them with the help of an additional service that will be responsible for intermediate storage of messages while they are waiting for processing while in the queue.  This separation is also necessary to increase the overall reliability of the system.  Indeed, the client sends a message to the system, but the processing service may ‚Äúfall‚Äù, but the message should not be lost, it should be saved in a service that has greater reliability than the processing service.  The simplest version of this service is called the queue (fig. 10.1). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ql/8v/vn/ql8vvnkz7cm8mkc0mrgtrdczzae.png" alt="image"></div><br>  The queue service works as follows: the client knows the URL of the queue and has access keys to it.  Using the SDK or the queue API, the client places in it a message containing in its composition a timestamp, an identifier, and a message body with a payload in JSON, XML or binary format. <br><br>  The service program code includes a loop that ‚Äúlistens‚Äù to the queue, retrieving the next message at each step, and if there is a message in the queue, then it is retrieved and processed.  If the service processes the message successfully, it is removed from the queue.  If an error occurs during processing, it is not deleted and can be processed again when the new version of the service, with the corrected code, is launched.  The queue is designed to synchronize one client (or a group of similar clients) and exactly one processing service (although the latter may be located on a server cluster or on a server farm).  Cloud queuing services include Azure Storage Queue, Azure Service Bus Queue and AWS SQS.  The services hosted on virtual machines include RabbitMQ, ZeroMQ, MSMQ, IBM MQ, and others. <br><br>  Different queue services guarantee different types of message delivery: <br><ul><li>  at least one-time delivery of the message; </li><li>  strictly single delivery; </li><li>  delivery of messages with preservation of the order; </li><li>  message delivery without order. </li></ul><br>  The queue provides reliable delivery of messages from one source to one processing service, that is, one-to-one interaction.  But what if you need to ensure the delivery of messages to several services?  In this case, you need to use a service called ‚Äútopic‚Äù (topic) (Fig. 10.2). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wo/mi/6i/womi6if7bwniohwxjvik1r68v_4.png" alt="image"></div><br>  An important element of such an architecture is ‚Äúsubscriptions‚Äù.  This is the path that the message is sent to in the section.  Messages are published in the topic by the client and transmitted to one of the subscriptions, from which they are retrieved by one of the services and processed by them.  Topics provide a one-to-many architecture for client and service interaction.  Examples of such services include the Azure Service Bus Topic and AWS SNS. <br><br>  And now suppose that there are a large number of disparate customers who have to send many messages to various services, that is, you need to build a ‚Äúmany to many‚Äù interaction system.  Of course, such an architecture can be built using several sections, but such a construction is not scalable and requires effort for administration and monitoring.  However, there are separate services - message concentrators (Fig. 10.3). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5t/na/sg/5tnasgyiet_8zcd0zmtvxj4koeg.png" alt="image"></div><br>  A hub accepts messages from many clients.  All clients can send messages to one common service endpoint or connect separately to different endpoints via special keys.  These keys allow you to flexibly manage clients: disconnect some, connect new ones, etc. There are also partitions inside the hub.  But in this case, they can be distributed among all clients in order to increase productivity (round robin - ‚Äúwith a cyclic addition‚Äù) or the client can publish messages in one of the sections.  On the other hand, processing services are grouped into groups of consumers (consumer group).  One or several services can be connected to one group.  Thus, a message hub is the most flexible service that can be configured as a queue, section or group of queues, or a set of sections.  In general, a message hub provides a many-to-many scheme between clients and services.  These hubs include Apache Kafka, Azure Event Hub and AWS Kinesis Stream. <br><br>  Before we look at the cloud PaaS services, let's pay attention to a very powerful and well-known service - Apache Kafka.  In cloud environments, it can be accessed as a distribution kit deployed in a virtual machine cluster directly or through the HDInsight service.  So, Apache Kafka is a service that provides the following features: <br><ul><li>  posting and subscribing to a message flow; </li><li>  reliable storage of messages; </li><li>  use of third-party services for streaming message processing. </li></ul><br>  Physically, Kafka runs in a cluster of one or more servers.  Kafka provides an API for interacting with external clients (Figure 10.4). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gz/50/ug/gz50ugazn-xwectzo--c8pulh4c.png" alt="image"></div><br>  Consider these APIs in order. <br><ul><li>  Manufacturer APIs allow client applications to publish message flows in one or more Kafka topics. </li><li>  Consumer APIs allow client applications to subscribe to one or more topics and process message flows delivered by topics to customers. </li><li>  Stream Processor APIs allow applications to interact with a Kafka cluster as a stream processing engine.  Sources for one processor can be one or several topics.  At the same time, processed messages are also placed in one or several topics. </li><li>  Connector APIs help connect external data sources (for example, DDB) as sources of messages (for example, interception of data change events in the database is possible) and as receivers. </li></ul><br>  In Kafka, the interaction between clients and the cluster occurs via TCP, which is facilitated by the existing SDKs for various programming languages, including .Net.  But the basic languages ‚Äã‚Äãof the SDK are Java and Scala. <br><br>  In a cluster, the storage of message flows (in Kafka terminology, also referred to as records) occurs logically in objects called topics (Fig. 10.5).  Each entry consists of a key, a value, and a timestamp.  In essence, a topic is a sequence of records (messages) that have been published by customers.  Kafka topics support from 0 to several subscribers.  Each topic is physically represented as a partitioned log.  Each section is an ordered sequence of records, to which new ones are constantly added to the input of Kafka. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qn/yi/wu/qnyiwuqtcxijhasj3iz7q8eh4jm.png" alt="image"></div><br>  Each entry in the section corresponds to a number in the sequence, also called an offset, which uniquely identifies the message in the sequence.  Unlike the queue, Kafka deletes the message not after processing the service, but after the lifetime of the messages.  This is a very important feature that provides the ability to read from one topic to different consumers.  Moreover, an offset is associated with each consumer (Fig. 10.6).  And each reading act only leads to an increase in the value for each client individually and is determined by the client. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mj/1b/mk/mj1bmkl7ooowixrdekhfg3-bix8.png" alt="image"></div><br>  In the normal case, this offset is increased by one after successfully reading one message from the topic.  But if necessary, the client can shift this offset and repeat the read operation. <br><br>  The use of the concept of sections has the following objectives. <br><br>  First, sections provide the ability to scale topics in the case when one topic does not fit within a single node.  At the same time, each section has one master (do not confuse it with the head node of the entire cluster) a node and zero or several successor nodes.  The head node is responsible for processing read-write operations, while followers are passive copies.  If the lead node fails, one of the follower nodes will automatically become the head.  Each cluster node is head for some sections and a follower for others.  Secondly, such replication increases reading performance due to the possibility of parallel read operations. <br><br>  The manufacturer can place the message in any topic of his choice either explicitly or in the round robin mode implicitly (that is, with a uniform filling).  Consumers are united in the so-called consumer groups, and each message published in the topic is delivered to one customer in each consumer group.  Clients in this case can be physically located on one or several servers / virtual machines.  More detailed message delivery is as follows.  For all customers belonging to the same consumer group, messages can be distributed among customers in order to optimize the load.  If customers belong to different consumer groups, then each message will be sent to each group.  The separation of messages from sections into different groups of consumers is shown in Fig.  10.7. <br><br>  Now briefly describe the basic parameters of delivery and storage of messages, guaranteed by Kafka. <br><ul><li>  Messages sent by the manufacturer to a specific topic will be added strictly in the order in which they were sent. </li><li>  The client sees the order of messages in the topic, which was received when saving messages.  As a result, the delivery of messages from the manufacturer to the consumer is made strictly in the order of their receipt. </li><li>  N-fold replication of the topic ensures the resistance of the topic to the failure of N - 1 nodes without loss of efficiency. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/rf/qp/ms/rfqpmsno04krpiqxhs1qv8agwhu.png" alt="image"></div><br>  So, the Apache Kafka service can be used in the following modes. <br><br><ul><li>  Service - message broker (queue) or publication service - message subscriptions (topic).  Indeed, at the core of Kafka is a group of topics, which can be transformed into a queue with one subscriber.  (It should be remembered: unlike conventional message broker services, built on the queuing principle, Kafka deletes messages only after its lifetime has passed, while Peek-Delete, ie, extraction and deletion, is implemented after successful processing. ) The principle of consumer groups summarizes these two concepts, and the ability to publish messages in all topics with round robin distribution makes Kafka a universal multimode message broker. </li><li>  Service streaming message analysis.  This is possible thanks to the Kafka API for stream processors, which allows you to build complex systems created on the principle of Event Driven, with services that filter messages or react to them, as well as services that aggregate messages. </li></ul><br>  All of these properties allow Kafka to be used as a key component of a streaming data platform that is highly capable of building complex message flow processing systems.  But at the same time, Kafka is quite complicated in terms of deploying and configuring a cluster of several nodes, which requires substantial administrative efforts.  But, on the other hand, since the ideas underlying Kafka are very well suited for building systems, streaming and processing messages, cloud providers provide PaaS services that implement these ideas and hide all the complexities of building and administering a Kafka cluster.  But since these services have a number of restrictions in terms of customization and expansion beyond the limits allocated for services, cloud providers provide special IaaS / PaaS services for the physical deployment of Kafka in a virtual machine cluster.  In this case, the user has almost complete freedom of configuration and expansion.  These services include Azure HDInsight.  He has already been mentioned above.  It was created to, on the one hand, provide the user with services from the Hadoop ecosystem on their own, without external wrappers, and on the other, relieve them of the difficulties that arise during the direct installation, administration and configuration of IaaS.  Docker-hosting stands a little apart.  Since this is an extremely important topic, we will look at it, but first we will get acquainted with PaaS-services implemented using the basic concepts of Kafka. <br><br><h3>  10.2.  Azure Event Hub </h3><br>  Consider the Azure Event Hub message hub service.  It is a service built on the PaaS model.  Different groups of clients can act as sources of messages for Azure Event Hub (Fig. 10.8).  First of all, this is a very large group of cloud services, whose outputs or triggers can be configured to send messages directly to the Event Hub.  These can be Stream Analytics Job, Event Grid, and a significant group of services that redirect events ‚Äî logs in the Event Hub (primarily built using the AppService: Api App, Web App, Mobile App and Function App). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/th/wr/-y/thwr-yanwimel2wqzorsolaojxo.png" alt="image"></div><br>  Messages delivered to the hub can be captured directly and placed in the Blob Storage or Data Lake Store. <br><br>  The next group of sources is external software clients or devices for which there is no Azure Event Hub SDK and which cannot be directly integrated with Azure services.  These clients primarily include IoT devices.  They can send messages to the Event Hub entry using HTTPS or AMQP protocols.  Considering how to connect these devices is beyond the scope of this book. <br><br>  Finally, software clients that generate messages and send them to the Event Hub, using the Azure Event Hub SDK.  This group includes Azure PowerShell and Azure CLI. <br>  Stream analytics job streaming analytics services or the Event Grid integration service can act as message receivers (consumers - ‚Äúconsumers‚Äù) from the Event Hub.  In addition, software clients can receive messages using the Azure Event Hub SDK.  Consumers connect to the Event Hub using the AMQP 1.0 protocol. <br><br>  Consider the basic concepts of Azure Event Hub, which are necessary for understanding how to use and configure it.  Any source (in the documentation also referred to as publisher) who sends a message to a hub must use HTTPS or AMQP 1.0.  The choice of a particular protocol is determined by the type of client, communication network and message transfer rate requirements.  AMQP needs to create a permanent connection between two bidirectional TCP sockets.  It is protected by using the TLS or SSL / TLS transport layer encryption protocol.  All this means, in particular, that for the initial connection establishment, the AMQP protocol takes more time than HTTPS, but with a constant message flow, the first one has much better performance.  With rare messages, HTTPS is preferred. <br><br>  In order to identify themselves, sources can use identification mechanisms based on the SAS (Shared Access Signature) tokens mechanism.  In the simplest case, all publishers can apply one common SAS token to all or use a flexible distribution policy for different SAS tokens for different publishers.  In addition to the separation of sources by different SAS tokens, the division by key of the section is used (more on this below). <br><br>  Each source can send a message of no more than 256 KB.  This means that you need to split a long message into several messages sent in sequence or switch to a binary format and use data compression. <br><br>  Now consider how the message is processed directly in the Event Hub.  As mentioned in the previous chapter, the concept of message hubs is based on a group of topics that have entry points and multiple exits to which consumer subscribers can connect.  In the case of EventHub, such topics are referred to as partitions.  Conceptually, EventHub sections are an ordered sequence of messages organized according to the ‚Äúfirst in, first out‚Äù (FIFO) queue principle (Figure 10.9). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/iq/2g/_p/iq2g_p_ldvolfuahy8admgxqhzo.png" alt="image"></div><br>  Each section is an independent sequence of messages within the entire Event Hub.  One Event Hub can include from 2 to 32 sections, and this value cannot be changed after creating the Event Hub.  It is very important to understand that the number of SIMULTANEOUS retrieved messages from a hub is equal to the number of sections. <br><br>  Messages in the section (and in fact in the queue) are stored until the consumer retrieves it (it is not deleted, but ceases to be available - see below), or until a certain storage time (retention period), which may tune up.  It should be clarified.  In any case, the message is physically deleted only after the storage time has expired.  To provide the ability to read messages from a section, the concept of offset is implemented in Azure Event Hub.  The offset in this case is the position in the section that shows the current position for reading, that is, in essence, the cursor with the number of the current readable position.  After successfully retrieving the message, the consumer shifts the cursor one position.  Azure Event Hub SDK allows you to set an arbitrary position from which you can read messages, but this is not recommended.  In this case, the client-consumer is responsible for the storage of the bias, and he himself must store and update this value. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fe/ab/bc/feabbcymdq52xiddkbp2vcajz-k.png" alt="image"></div><br>  Thus, the consumer can read the same message several times, but only if he will each time indicate the offset corresponding to the position of this message.  However, standard Azure Event Hub SDKs by default exclude this possibility because they provide a mechanism for safe storage and updating of the offset.  As a rule, Storage Account is used to store the offset.  Azure services, which are message listeners from Event Hub, take responsibility for storing messages. <br><br>  Each section within the Event Hub has its own partition key (partition key), which allows you to send messages from specific sources to a specific section.  The purpose of this action - the organization of message flows.  For example, a hub contains many sections and sources of different performance (the number of messages per unit of time) and you need to create a dedicated channel with the publication and reading of messages.  If the source does not explicitly specify a partition key, then they are distributed between partitions evenly (round robin). <br><br>  Now consider the issue of extracting messages from the hub.  All consumers logically coalesce into groups that are managed jointly and are called the consumer group (Fig. 10.11).  Groups allow independent reading from the same sections by different users.  In addition, each group gives each client application the opportunity to have its own view (the magnitude of the current offsets for each section) and, thus, independently read the data from the sections.  For each group of consumers, such views are completely different, which allows for the implementation of complex data processing scenarios.  The maximum number of consumer groups is 20, while in each group there can be no more than five actual consumers, and only one consumer from the group can read a message from the section at a time. <br><br>  The division of the hub into sections provides parallel reading of messages by many consumers.  In addition, the performance of the hub in terms of receiving a specified number per unit of time can be adjustable.  For this, there is a parameter called the throughput unit.  Each unit of bandwidth includes the following values: <br><ul><li>  for the input stream - 1 MB per second or 1000 messages per second (depending on which of these limits will be reached first); </li><li>  for the output stream - 2 MB per second. </li></ul><br>  The specific amount of bandwidth is configured for each hub separately.  If sources exceed the allowable bandwidth, an exception is thrown.  In this case, the receiver simply cannot receive messages faster than the specified value.  Bandwidth units also affect the cost of using the service.  Be careful!  You should be aware that there are bandwidth limits allocated to the namespace, and not to individual instances of the Event Hub service. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/i_/l0/jc/i_l0jcqwomkuargf-sygdrfufdc.png" alt="image"></div><br>  Message hubs are logically grouped into namespaces (Figure 10.12). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jf/de/h-/jfdeh-khouxn6olnkxcgw6-w1a4.png" alt="image"></div><br><br>  ¬ªMore information about the book can be found on <a href="https://www.piter.com/collection/new/product/rabota-s-bigdata-v-oblakah-obrabotka-i-hranenie-dannyh-s-primerami-iz-microsoft-azure">the publisher's website.</a> <br>  ¬ª <a href="https://storage.piter.com/upload/contents/978544610578/978544610578_X.pdf">Table of Contents</a> <br>  ¬ª <a href="https://storage.piter.com/upload/contents/978544610578/978544610578_p.pdf">Excerpt</a> <br><br>  For Habrozhiteley a 20% discount on the coupon - <b>BigData</b> </div><p>Source: <a href="https://habr.com/ru/post/428375/">https://habr.com/ru/post/428375/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../428365/index.html">Tale of how Google billing was changed, or how to avoid unnecessary costs</a></li>
<li><a href="../428367/index.html">We made the world's smallest sonar modem</a></li>
<li><a href="../428369/index.html">Getting out of the wheel of Samsara, extremism and some greens - analysis of tasks from the GridGain booklet at the Joker 2018 conference</a></li>
<li><a href="../428371/index.html">Old play IBM</a></li>
<li><a href="../428373/index.html">iPhone inconvenient to use</a></li>
<li><a href="../428377/index.html">Epigenetic Aging Biomarkers</a></li>
<li><a href="../428379/index.html">Real design process. Step-by-step story on how to create a business-oriented site</a></li>
<li><a href="../428381/index.html">$ 10 million investment and Wozniak's praise is a long way to create a computer designer for children</a></li>
<li><a href="../428383/index.html">Sony has published a full list of games for the PlayStation Classic</a></li>
<li><a href="../428385/index.html">More coffee, less caffeine: Intel 9th ‚Äã‚ÄãGen (Part 1)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>