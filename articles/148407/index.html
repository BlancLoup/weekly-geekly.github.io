<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>PyBrain work with neural networks in Python</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Within the framework of one project, I faced the need to work with neural networks, considered several options, I liked PyBrain the most . I hope its ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>PyBrain work with neural networks in Python</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/storage2/876/89c/c9b/87689cc9b163aa6ef8c555c797f4f398.png" align="left"><br>  Within the framework of one project, I faced the need to work with neural networks, considered several options, I liked <a href="http://pybrain.org/">PyBrain the most</a> .  I hope its description will be interesting for many to read. <br><br>  PyBrain is one of the best Python libraries to study and implement a large variety of algorithms associated with neural networks.  It is a good example of combining compact Python syntax with a good implementation of a large set of different algorithms from the field of machine intelligence. <br><br>  Created for: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  <b>Researchers</b> - provides a uniform environment for the implementation of various algorithms, eliminating the need to use dozens of different libraries.  Allows you to focus on the algorithm itself and not the features of its implementation. </li><li>  <b>Students</b> - using PyBrain it is convenient to implement homework, course project or calculations in the thesis.  The flexibility of the architecture allows you to conveniently implement a variety of complex methods, structures and topologies. </li><li>  <b>Lecturers</b> - learning Machine Learning techniques was one of the main goals when creating a library.  The authors will be happy if the results of their work will help in the preparation of competent students and specialists. </li><li>  <b>The developers</b> are an Open Source project, so new developers are always welcome. </li></ul><br><a name="habracut"></a><br><h4>  About the library </h4><br>  PyBrian is a modular library designed to implement various machine learning algorithms in Python.  Its main purpose is to provide the researcher with flexible, easy-to-use, but at the same time powerful tools for implementing tasks from the field of machine learning, testing and comparing the effectiveness of various algorithms. <br>  The name PyBrain is an abbreviation of English: Python-Based Reinforcement Learning, Artificial Intelligence and Neural Network Library. <br>  As stated on one site: <i>PyBrain - swiss army knife for neural networking</i> (PyBrain is a Swiss army knife in the area of ‚Äã‚Äãneural network computing). <br><br>  The library is built on a modular basis, which allows it to be used both for students to learn the basics and for researchers who need to implement more complex algorithms.  The general structure of the procedure for its use is shown in the following diagram: <br><img src="https://habrastorage.org/storage2/95d/411/f57/95d411f57052950520145c1c41b6b163.png"><br><br>  The library itself is an open source product and is free for use in any project with only one reservation. When used for scientific research, they are asked to add the following book to the list of cited information sources (which people do): <br><blockquote>  Tom Schaul, Justin Bayer, Daan Wierstra, Sun Yi, Martin Felder, Frank Sehnke, Thomas R√ºckstie√ü, J√ºrgen Schmidhuber.  PyBrain.  To appear in: Journal of Machine Learning Research, 2010. </blockquote><br><h4>  Main features </h4><br>  The main features of the library (for version 0.3) are: <br><ul><li>  Algorithms for <a href="http://ru.wikipedia.org/wiki/%25D0%259E%25D0%25B1%25D1%2583%25D1%2587%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D1%2581_%25D1%2583%25D1%2587%25D0%25B8%25D1%2582%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BC">learning with a teacher</a> (Supervised Learning). <br><ul><li>  Back Propagation <a href="http://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%25BE%25D0%25B1%25D1%2580%25D0%25B0%25D1%2582%25D0%25BD%25D0%25BE%25D0%25B3%25D0%25BE_%25D1%2580%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25BE%25D1%2581%25D1%2582%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B5%25D0%25BD%25D0%25B8%25D1%258F_%25D0%25BE%25D1%2588%25D0%25B8%25D0%25B1%25D0%25BA%25D0%25B8">Method</a> </li><li>  <a href="http://www.basegroup.ru/glossary/definitions/rprop/">R-Prop</a> (Resilient propagation) </li><li>  <a href="http://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%25BE%25D0%25BF%25D0%25BE%25D1%2580%25D0%25BD%25D1%258B%25D1%2585_%25D0%25B2%25D0%25B5%25D0%25BA%25D1%2582%25D0%25BE%25D1%2580%25D0%25BE%25D0%25B2">Support-Vector-Machines</a> (interface to third-party library LIBSVM) </li><li>  <a href="http://www.idsia.ch/~juergen/evolino.html">Evolino</a> </li></ul><br></li><li>  <a href="http://ru.wikipedia.org/wiki/%25CE%25E1%25F3%25F7%25E5%25ED%25E8%25E5_%25E1%25E5%25E7_%25F3%25F7%25E8%25F2%25E5%25EB%25FF">Education without a teacher</a> (Black-Box Optimization / Evolutionary <br>  Methods) <br><ul><li>  <a href="http://ru.wikipedia.org/wiki/K-means">K-Means Clustering</a> </li><li>  <a href="http://ru.wikipedia.org/wiki/%25CC%25E5%25F2%25EE%25E4_%25E3%25EB%25E0%25E2%25ED%25FB%25F5_%25EA%25EE%25EC%25EF%25EE%25ED%25E5%25ED%25F2">Method of principal components</a> / <a href="http://www.robots.ox.ac.uk/~cvrg/hilary2006/ppca.pdf">Probabilistic Principal Component Analysis</a> (PCA / pPCA) </li><li>  LSH for Hamming and Euclidean spaces </li><li>  <a href="http://www.scholarpedia.org/article/Deep_belief_networks">Deep Belief Networks</a> </li></ul><br></li><li>  Reinforcement Learning <br><ul><li>  <a href="http://www.cs.utexas.edu/~pstone/Papers/bib2html/b2hd-AAAI06-yaxin.html">Value-based</a> <br><ul><li>  <a href="http://ru.wikipedia.org/wiki/Q-%25D0%25BE%25D0%25B1%25D1%2583%25D1%2587%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">Q-Learning</a> (with / without calculating acceptable paths) </li><li>  <a href="http://en.wikipedia.org/wiki/SARSA">SARSA</a> </li><li>  <a href="http://damas.ift.ulaval.ca/_seminar/filesA07/articleCharles.pdf">Neural Fitted Q-iteration</a> </li></ul><br></li><li>  <a href="http://videolectures.net/mlss06au_aberdeen_pgrl/">Policy Gradients</a> <br><ul><li>  REINFORCE </li><li>  <a href="http://www6.in.tum.de/~ruecksti/seminar/rl-ss07/papers/nac-peters.pdf">Natural Actor-Critic</a> </li></ul><br></li><li>  Research strategies <br><ul><li>  <a href="http://en.wikipedia.org/wiki/Multi-armed_bandit">Epsilon-Greedy Exploration</a> (discrete) </li><li>  <a href="http://www.cs.cmu.edu/afs/cs/academic/class/15381-s07/www/slides/050107reinforcementLearning1.pdf">Boltzmann Exploration</a> (discrete) </li><li>  <a href="http://books.nips.cc/papers/files/nips23/NIPS2010_0606.pdf">Gaussian Exploration</a> (continuous) </li><li>  <a href="http://citeseerx.ist.psu.edu/viewdoc/summary%3Fdoi%3D10.1.1.139.5478">State-Dependent Exploration</a> (continuous) </li></ul></li></ul><br></li><li>  <a href="http://ru.wikipedia.org/wiki/%25D0%25A7%25D1%2591%25D1%2580%25D0%25BD%25D1%258B%25D0%25B9_%25D1%258F%25D1%2589%25D0%25B8%25D0%25BA">Black box</a> <a href="http://pybrain.org/docs/tutorial/optimization.html">optimization</a> ( <a href="http://www.tigen.org/kevin.kofler/bbowda/">Black-box Optimization</a> ) <br><ul><li>  <a href="http://en.wikipedia.org/wiki/Hill_climbing">Hill climbing</a> </li><li>  <a href="http://en.wikipedia.org/wiki/Particle_swarm_optimization">Particle Swarm</a> <a href="http://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D1%2580%25D0%25BE%25D1%258F_%25D1%2587%25D0%25B0%25D1%2581%25D1%2582%25D0%25B8%25D1%2586">Method</a> ( <a href="http://en.wikipedia.org/wiki/Particle_swarm_optimization">Particle Swarm Optimization</a> (PSO) </li><li>  <a href="http://en.wikipedia.org/wiki/Evolution_strategy">Evolution</a> <a href="http://en.wikipedia.org/wiki/Evolution_strategy">Strategies</a> (ES) </li><li>  <a href="http://en.wikipedia.org/wiki/CMA-ES">Covariance Matrix Adaptation ES</a> (CMA-ES) </li><li>  <a href="http://en.wikipedia.org/wiki/Natural_Evolution_Strategies">Natural Evolution Strategies</a> (NES) </li><li>  <a href="http://citeseerx.ist.psu.edu/viewdoc/summary%3Fdoi%3D10.1.1.164.2624">Fitness Expectation-Maximization (FEM)</a> </li><li>  <a href="http://hci.iwr.uni-heidelberg.de/Staff/bgoldlue/fvia_ws_2011/fvia_ws_2011_02_gradient_descent.pdf">Finite Difference Gradient Descent</a> </li><li>  <a href="http://kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/Neural-Networks-2010-Sehnke_%255B0%255D.pdf">Policy Gradients with Parameter Exploration (PGPE)</a> </li><li>  <a href="http://www.jhuapl.edu/spsa/">Simultaneous Perturbation Stochastic Approximation (SPSA)</a> </li><li>  <a href="http://ru.wikipedia.org/wiki/%25D0%2593%25D0%25B5%25D0%25BD%25D0%25B5%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B8%25D0%25B9_%25D0%25B0%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC">Genetic Algorithm</a> / <a href="http://en.wikipedia.org/wiki/Genetic_algorithm">Genetic Algorithms</a> (GA) </li><li>  <a href="http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume21/stanley04a-html/jairhtml.html">Competitive Co-Evolution</a> </li><li>  <a href="http://en.wikipedia.org/wiki/Memetic_algorithm">Memetic Search</a> (Inner / Inverse) </li><li>  <a href="http://en.wikipedia.org/wiki/Multi-objective_optimization">Multi-Objective Optimization</a> / <a href="http://en.wikipedia.org/wiki/Multi-objective_optimization">Multi-Objective Optimization NSGA-II</a> </li></ul></li></ul><br><h4>  Network </h4><br>  PyBrain operates with network structures that can be used to build virtually all the complex algorithms supported by the library.  An example is: <br><ul><li>  Direct distribution networks, including <a href="http://www.scholarpedia.org/article/Deep_belief_networks">Deep Belief Networks</a> and <a href="http://deeplearning.net/tutorial/rbm.html">Restricted Boltzmann Machines (RBM)</a> </li><li>  <a href="http://ru.wikipedia.org/wiki/%25D0%25E5%25EA%25F3%25F0%25F0%25E5%25ED%25F2%25ED%25E0%25FF_%25ED%25E5%25E9%25F0%25EE%25ED%25ED%25E0%25FF_%25F1%25E5%25F2%25FC">Recurrent neural networks</a> (Recurrent networks - RNN), including the architecture of <a href="http://en.wikipedia.org/wiki/Long_short_term_memory">Long Short-Term Memory</a> (LSTM) </li><li>  <a href="">Multi-Dimensional Recurrent Networks</a> (MDRNN) </li><li>  <a href="http://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B0%25D0%25BC%25D0%25BE%25D0%25BE%25D1%2580%25D0%25B3%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B7%25D1%2583%25D1%258E%25D1%2589%25D0%25B0%25D1%258F%25D1%2581%25D1%258F_%25D0%25BA%25D0%25B0%25D1%2580%25D1%2582%25D0%25B0_%25D0%259A%25D0%25BE%25D1%2585%25D0%25BE%25D0%25BD%25D0%25B5%25D0%25BD%25D0%25B0">Kohonen Networks</a> / <a href="http://en.wikipedia.org/wiki/Self-organizing_map">Self-Organizing Maps</a> </li><li>  <a href="http://minds.jacobs-university.de/sites/default/files/uploads/papers/2261_LukoseviciusJaeger09.pdf">Reservoirs</a> </li><li>  <a href="http://ru.wikipedia.org/wiki/%25D0%259D%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C_%25D0%259A%25D0%25BE%25D1%2581%25D0%25BA%25D0%25BE">Kosco neural network</a> / <a href="http://en.wikipedia.org/wiki/Bidirectional_associative_memory">Bidirectional networks</a> </li><li>  Creating topologies of own structure </li></ul><br><h4>  Instruments </h4><br>  Additionally, there are software tools that allow you to implement related tasks: <br><ul><li>  Building / Visualizing Graphs </li><li>  NetCDF support </li><li>  Write / Read XML </li></ul><br><h4>  Library installation </h4><br>  Before installing Pybrain, the creators <a href="http://pybrain.org/docs/quickstart/installation.html">recommend</a> installing the following libraries: <br>  Setuptools is a batch manager for Python that greatly simplifies the installation of new libraries.  To install it, it is recommended to download and execute (python ez_setup.py) <a href="http://peak.telecommunity.com/dist/ez_setup.py">this</a> script. <br>  After installation you will be able to use the command <pre><code class="bash hljs">easy_install</code> </pre>  to install new libraries. <br>  Immediately use them and install the two necessary packages: <br><pre> <code class="bash hljs">$ easy_install scipy $ easy_install matplotlib</code> </pre><br><h5>  Next, PyBrain itself is installed. </h5><br><ul><li>  Or use the repository with github <br><img src="https://habrastorage.org/storage2/cf7/d3d/50d/cf7d3d50d998933a730c98b8becec936.png" align="left"><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> git://github.com/pybrain/pybrain.git</code> </pre><br></li><li>  Either download the latest stable version <a href="">here</a> .  And set the standard way: <br><pre> <code class="bash hljs">$ python setup.py install</code> </pre><br></li></ul><br><br><h4>  Library basics </h4><br><h5>  Neural network creation </h5><br>  Creating a neural network with two inputs, three hidden layers and one output: <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pybrain.tools.shortcuts <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> buildNetwork &gt;&gt;&gt; net = buildNetwork(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre><br>  As a result, in the net object there is a created neural circuit initialized with random weights. <br><br><h5>  Activation function </h5><br>  The activation function is set as follows: <br><pre> <code class="python hljs">net.activate([<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>])</code> </pre><br>  The number of elements transmitted to the network must be equal to the number of inputs.  The method returns the answer in the singular form, if the current circuit has one output, and an array, in the case of a larger number of outputs. <br><br><h5>  Retrieving network information </h5><br>  In order to obtain information about the current network structure, each of its elements has a name.  This name can be given automatically or by other criteria when creating a network. <br>  For example, for the net network names are given automatically: <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net[<span class="hljs-string"><span class="hljs-string">'in'</span></span>] &lt;LinearLayer <span class="hljs-string"><span class="hljs-string">'in'</span></span>&gt; &gt;&gt;&gt; net[<span class="hljs-string"><span class="hljs-string">'hidden0'</span></span>] &lt;SigmoidLayer <span class="hljs-string"><span class="hljs-string">'hidden0'</span></span>&gt; &gt;&gt;&gt; net[<span class="hljs-string"><span class="hljs-string">'out'</span></span>] &lt;LinearLayer <span class="hljs-string"><span class="hljs-string">'out'</span></span>&gt;</code> </pre><br>  Hidden layers are named with the layer number added to the name. <br><br><h5>  Opportunities when creating a network </h5><br>  Of course, in most cases, the created neural network should have other characteristics than the default ones.  There are various possibilities for this.  For example, by default, the hidden layer is created using the <a href="http://ru.wikipedia.org/wiki/%25D1%25E8%25E3%25EC%25EE%25E8%25E4">sigmoid activation function</a> , and you can use the following constants to specify its other type: <br><ul><li>  Biasunit </li><li>  GaussianLayer </li><li>  Linearlayer </li><li>  Lstmlayer </li><li>  MDLSTMLayer </li><li>  SigmoidLayer </li><li>  Softmaxlayer </li><li>  StateDependentLayer </li><li>  Tanhlayer </li></ul><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pybrain.structure <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> TanhLayer &gt;&gt;&gt; net = buildNetwork(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, hiddenclass=&lt;b&gt;TanhLayer&lt;/b&gt;) &gt;&gt;&gt; net[<span class="hljs-string"><span class="hljs-string">'hidden0'</span></span>] &lt;TanhLayer <span class="hljs-string"><span class="hljs-string">'hidden0'</span></span>&gt;</code> </pre><br><br>  It is also possible to specify the type of the output layer: <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pybrain.structure <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SoftmaxLayer &gt;&gt;&gt; net = buildNetwork(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, hiddenclass=TanhLayer, outclass=SoftmaxLayer) &gt;&gt;&gt; net.activate((<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>)) array([ <span class="hljs-number"><span class="hljs-number">0.6656323</span></span>, <span class="hljs-number"><span class="hljs-number">0.3343677</span></span>])</code> </pre><br>  Additionally it is possible to use bias <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = buildNetwork(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, bias=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) &gt;&gt;&gt; net[<span class="hljs-string"><span class="hljs-string">'bias'</span></span>] &lt;BiasUnit <span class="hljs-string"><span class="hljs-string">'bias'</span></span>&gt;</code> </pre><br><br><h5>  Handling Data (Building a DataSet) </h5><br>  The created network should process the data that this section is dedicated to.  A typical data set is a set of input and output values.  PyBrain uses the pybrain.dataset module to work with them, and the SupervisedDataSet class is also used below. <br><br><h5>  Data setting </h5><br>  The SupervisedDataSet class is used for typical teacher training.  It supports output and output arrays.  Their sizes are set when creating a class instance: <br>  Record type: <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pybrain.datasets <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SupervisedDataSet &gt;&gt;&gt; ds = SupervisedDataSet(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre><br>  means that a data structure is created to store two-dimensional input data and one-dimensional output. <br><br><h5>  Adding Samples </h5><br>  The classic task in training a neural network is to learn the XOR function, then the data set used to create such a network is shown. <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>ds.addSample((<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>), (<span class="hljs-number"><span class="hljs-number">0</span></span>,)) &gt;&gt;&gt; ds.addSample((<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), (<span class="hljs-number"><span class="hljs-number">1</span></span>,)) &gt;&gt;&gt; ds.addSample((<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>), (<span class="hljs-number"><span class="hljs-number">1</span></span>,)) &gt;&gt;&gt; ds.addSample((<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), (<span class="hljs-number"><span class="hljs-number">0</span></span>,))</code> </pre><br><br><h5>  Examination of the sample structure </h5><br>  To obtain data arrays in their current set, it is possible to use standard Python functions for working with arrays. <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>len(ds)</code> </pre><br>  will print 4, since this is the number of elements. <br>  Iteration over a set can also be organized in the usual way for arrays: <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> inpt, target <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> ds: <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> inpt, target</code> </pre><br><pre> <code class="bash hljs">... [ 0. 0.] [ 0.] [ 0. 1.] [ 1.] [ 1. 0.] [ 1.] [ 1. 1.] [ 0.]</code> </pre><br>  Also, each set of fields can be directly accessed using its name: <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>ds[<span class="hljs-string"><span class="hljs-string">'input'</span></span>]</code> </pre><br><pre> <code class="bash hljs">array([[ 0., 0.], [ 0., 1.], [ 1., 0.], [ 1., 1.]])</code> </pre><br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>ds[<span class="hljs-string"><span class="hljs-string">'target'</span></span>]</code> </pre><br><pre> <code class="bash hljs">array([[ 0.], [ 1.], [ 1.], [ 0.]])</code> </pre><br><br>  You can also manually free the memory occupied by the sample by completely removing it: <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>ds.clear() &gt;&gt;&gt; ds[<span class="hljs-string"><span class="hljs-string">'input'</span></span>]</code> </pre><br><pre> <code class="bash hljs">array([], shape=(0, 2), dtype=float64)</code> </pre><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>ds[<span class="hljs-string"><span class="hljs-string">'target'</span></span>]</code> </pre><br><pre> <code class="bash hljs">array([], shape=(0, 1), dtype=float64)</code> </pre><br><br><h5>  Network training on samples </h5><br>  In PyBrain, the concept of trainers is used to train networks with a teacher.  The trainer receives a copy of the network and a copy of the sample set and then trains the network on the set received. <br>  The classic example is backpropagation.  To simplify the implementation of this approach in PyBrain, there is a class BackpropTrainer. <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pybrain.supervised.trainers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BackpropTrainer</code> </pre><br>  The training set of samples (ds) and the target network (net) have already been created in the examples above, now they will be merged. <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = buildNetwork(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, bias=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, hiddenclass=TanhLayer) &gt;&gt;&gt; trainer = BackpropTrainer(net, ds)</code> </pre><br>  The coach received a link to the network structure and can train it. <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>trainer.train()</code> </pre><br><pre> <code class="bash hljs">0.31516384514375834</code> </pre><br>  Calling the train () method performs one iteration (epoch) of training and returns the value of the quadratic error (double proportional to the error). <br>  If you do not need to organize a cycle for each epoch, then there is a method of training a network up to convergence: <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>trainer.trainUntilConvergence()</code> </pre><br>  This method will return an array of errors for each epoch. <br><br><h4>  More examples of the implementation of different networks </h4><br>  In the article <blockquote>  Tom Schaul, Martin Felder, et.al.  PyBrain, Journal of Machine Learning Research 11 (2010) 743-746. </blockquote>  An example of creating a network with loading data from a .mat file is given. <br><img src="https://habrastorage.org/storage2/faf/e04/ef7/fafe04ef7f719856a2c48a59dff17a6e.png" align="right"><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Load Data Set. ds = SequentialDataSet.loadFromFile('parity.mat') # Build a recurrent Network. net = buildNetwork(1, 2, 1, bias=True, hiddenclass=TanhLayer, outclass=TanhLayer, recurrent=True) recCon = FullConnection(net['out'], net['hidden0']) net.addRecurrentConnection(recCon) net.sortModules() # Create a trainer for backprop and train the net. trainer = BackpropTrainer(net, ds, learningrate=0.05) trainer.trainEpochs(1000)</span></span></code> </pre><br><br><h4>  A few links: </h4><br><ul><li>  <a href="http://stackoverflow.com/questions/6006187/how-to-save-and-recover-pybrain-traning">How to save and load a network in PyBrain</a> ? </li><li>  <a href="http://stackoverflow.com/questions/5692624/creating-custom-connectivity-in-pybrain-neural-networks/5716233">Creating a network of your own structure in PyBrain</a> . </li><li>  <a href="http://stackoverflow.com/questions/9137463/how-can-i-calculate-or-monitor-the-training-of-a-neural-network-in-pybrain/9344722">How can I follow the network learning process in PyBrain</a> ? </li><li>  <a href="http://stackoverflow.com/questions/8150772/pybrain-how-to-print-a-network-nodes-and-weights/8161274">How to output the resulting network (nodes and arcs) in PyBrain</a> ? </li><li>  <a href="http://stackoverflow.com/questions/8139822/how-to-load-training-data-in-pybrain/8143012">How to load a training set into PyBrain</a> ? </li><li>  <a href="http://biomunky.wordpress.com/2010/03/17/hmmm-pybrains/">A small How to start</a> , article 2010. </li><li>  <a href="http://simontechblog.blogspot.com/2010/08/pybrain-reinforcement-learning-tutorial_21.html">Another one</a> , also from 2010. </li></ul><br><br><h4>  Conclusion </h4><br>  In conclusion I want to say that this library makes a very good impression, it is convenient to work with it, the descriptions of the algorithms are compact, but do not lose clarity in the wilds of the code. <br><br>  PS If there are amendments on the names of some terms, then I am ready to listen, not sure about a pair of translations 100% accurate, perhaps there are already well-established names. </div><p>Source: <a href="https://habr.com/ru/post/148407/">https://habr.com/ru/post/148407/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../148401/index.html">Attempt to do without regular expressions for phone numbers of own region</a></li>
<li><a href="../148402/index.html">Gittip - Github crowdfunding</a></li>
<li><a href="../148403/index.html">SOAP Web service using Spring-WS</a></li>
<li><a href="../148404/index.html">A simple bookmarklet as a tool to clean webpage content from unwanted items.</a></li>
<li><a href="../148406/index.html">New standoff between WHATWG and W3C: in whose hands is the future of HTML5?</a></li>
<li><a href="../148409/index.html">OS X, Dual Stack and VK API problem</a></li>
<li><a href="../148410/index.html">How to create a simple tower defense game on Unity3D, part one</a></li>
<li><a href="../148411/index.html">System approach to requirements management</a></li>
<li><a href="../148413/index.html">CRIU - an ambitious new project to preserve and restore the state of processes</a></li>
<li><a href="../148415/index.html">IOS Application Testing Systems</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>