<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Apache Ignite + Apache Spark Data Frames: More Fun Together</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! My name is Nikolai Izhikov, I work for Sberbank Technology in the development team for Open Source solutions. Over 15 years of commercial de...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Apache Ignite + Apache Spark Data Frames: More Fun Together</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr!  My name is Nikolai Izhikov, I work for Sberbank Technology in the development team for Open Source solutions.  Over 15 years of commercial development in Java.  I am an Apache Ignite committer and an Apache Kafka contributor. <br><br>  Under the cat you will find a video and text version of my report on the Apache Ignite Meetup on how to use Apache Ignite with Apache Spark and what features we have implemented for this. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f42/5f3/df5/f425f3df59ff99d03d4a3e6aff3b2655.png"><br><a name="habracut"></a><br><h2>  What Apache Spark Can Do </h2><br>  What is Apache Spark?  This is a product that allows you to quickly perform distributed computing and analytical queries.  Basically, Apache Spark is written in Scala. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Apache Spark has a rich API for connecting to various storage systems or retrieving data.  One of the product features is a universal SQL-like query engine for data obtained from various sources.  If you have several sources of information, you want to combine them and get some results, Apache Spark is what you need. <br><br>  One of the key abstractions that Spark provides is Data Frame, DataSet.  In terms of a relational database, this is a table, a kind of source that provides data in a structured way.  The structure, type of each column, its name, etc. are known.  Data frames can be created from various sources.  Examples include json files, relational databases, various hadoop systems, and Apache Ignite. <br><br>  Spark supports joines in SQL queries.  You can combine data from various sources and get results, perform analytical queries.  In addition, there is an API for saving data.  When you have completed queries, conducted a study, Spark provides an opportunity to save the results to the receiver that supports this feature, and, accordingly, to solve the data processing problem. <br><br><h2>  What kind of integration capabilities of Apache Spark with Apache Ignite we have implemented </h2><br><ol><li>  Reading data from Apache Ignite SQL tables. </li><li>  Writing data to Apache Ignite SQL tables. </li><li>  IgniteCatalog inside IgniteSparkSession is the ability to use all existing Ignite SQL tables without hands-on registration. </li><li>  SQL Optimization - the ability to execute SQL statements inside Ignite. </li></ol><br>  Apache Spark can read data from Apache Ignite SQL tables and write them in the form of such a table.  Any DataFrame that is formed in Spark can be saved as an Apache Ignite SQL table. <br><br>  Apache Ignite allows you to use all existing SQL tables Ignite in Spark Session without registering "hands" - using IgniteCatalog inside an extension of the standard SparkSession - IgniteSparkSession. <br><br>  Here it is necessary to go deep into the Spark device.  In terms of a normal database, a directory is a place where meta-information is stored: which tables are available, which columns are in them, and so on.  When a request arrives, meta-information is pulled from the directory and the SQL engine does something with the tables, data.  By default, Spark all read tables (it doesn‚Äôt matter from the relational database, Ignite, Hadoop) have to be manually registered in the session.  As a result, you get the opportunity to make a SQL query on these tables.  Spark finds out about them. <br><br>  To work with the data that we loaded into Ignite, we need to register the tables.  But instead of registering each table with "hands", we implemented the ability to automatically access all Ignite tables. <br><br>  What is the feature here?  For some reason I do not understand, the directory in Spark is the internal API, i.e.  A third-party person cannot come and create their own directory implementation.  And since Spark left Hadoop, it only supports Hive.  And everything else you have to register with your hands.  Users often ask how to get around this and immediately make SQL queries.  I implemented a directory that allows you to browse and access Ignite tables without registering ~ and sms ~, and initially offered this patch to the Spark community, to which I received the answer: this patch is not interesting for some internal reasons.  And they also did not give the internal API forward. <br><br>  Now the Ignite-catalog is an interesting feature implemented using the internal Spark API.  To use this directory, we have our own session implementation, This is a regular SparkSession, within which you can make requests, process data.  The differences are that we have built in ExternalCatalog for working with Ignite tables, as well as IgniteOptimization, which will be described below. <br><br>  <b>SQL Optimization</b> - the ability to execute SQL statements inside Ignite.  By default, when performing join, grouping, calculating aggregates, other complex SQL queries, Spark reads data in row by row mode.  The only thing that a data source can do is to filter rows efficiently. <br><br>  If join or grouping is used, Spark pulls out all the data from the table to itself in memory for the worker, applying the specified filters, and only then groups them or performs other SQL operations.  In the case of Ignite, this is not optimal, because Ignite itself has a distributed architecture and has knowledge of the data that is stored in it.  Therefore, Ignite itself can effectively calculate the aggregates, carry out grouping.  In addition, there can be a lot of data, and to group them you will need to subtract everything, pick up all the data in Spark, which is quite expensive. <br><br>  Spark provides an API with which you can change the original SQL plan of the query, perform the optimization, and forward into the Ignite that part of the SQL query that can be executed there.  This will be effective in terms of speed, as well as memory consumption, because we will not use it to extract data that will be immediately grouped. <br><br><h2>  How things work </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/b28/1df/0ef/b281df0ef5f2ea2a08f73267ef7f5edb.png"><br><br>  We have an Ignite cluster - this is the bottom half of the picture.  Zookeeper is not, because there are only five nodes.  There are Spark workers, inside each worker a client Ignite node rises.  Through it, we can make a request and read the data, interact with the cluster.  Also, the client node rises inside the IgniteSparkSession to run the directory. <br><br><h2>  Ignite Data Frame </h2><br>  Moving to code: how to read data from a SQL table?  In the case of Spark, everything is quite simple and good: we say that we want to calculate some data, we specify the format - this is a definite constant.  Next we have several options - the path to the configuration file for the client node, which starts when reading data.  We indicate which table we want to read, and tell Spark to load.  We receive data and we can do with them what we want. <br><br><pre><code class="scala hljs">spark.read .format(<span class="hljs-type"><span class="hljs-type">FORMAT_IGNITE</span></span>) .option(<span class="hljs-type"><span class="hljs-type">OPTION_CONFIG_FILE</span></span>, <span class="hljs-type"><span class="hljs-type">TEST_CONFIG_FILE</span></span>) .option(<span class="hljs-type"><span class="hljs-type">OPTION_TABLE</span></span>, <span class="hljs-string"><span class="hljs-string">"person"</span></span>) .load()</code> </pre> <br>  Once we have generated the data ‚Äî not necessarily from Ignite, from any source ‚Äî we can just as easily save everything, specifying the format and the corresponding table.  Command Spark to record, specify the format.  In the config we prescribe which cluster to connect to.  We specify the table in which we want to save.  Additionally, we can prescribe service options - specify the primary key, which we create on this table.  If the data simply goes without creating a table, then this parameter is not needed.  At the end, click save and the data is written. <br><br><pre> <code class="scala hljs">tbl.write. format(<span class="hljs-type"><span class="hljs-type">FORMAT_IGNITE</span></span>). option(<span class="hljs-type"><span class="hljs-type">OPTION_CONFIG_FILE</span></span>, <span class="hljs-type"><span class="hljs-type">CFG_PATH</span></span>). option(<span class="hljs-type"><span class="hljs-type">OPTION_TABLE</span></span>, tableName). option(<span class="hljs-type"><span class="hljs-type">OPTION_CREATE_TABLE_PRIMARY_KEY_FIELDS</span></span>, pk). save</code> </pre><br>  Now let's see how this all works. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b35/41a/b86/b3541ab86eca15cd240765bf15907979.png"><br>  <a href="https://github.com/nizhikov/data-frames-examples/blob/master/src/main/scala/LoadDataExample.scala">LoadDataExample.scala</a> <br><br>  This obvious application will first demonstrate recording capabilities.  I chose for example data on football matches, downloaded statistics from a known resource.  It contains information on tournaments: leagues, matches, players, teams, player attributes, team attributes - data that describe football matches in European leagues (England, France, Spain, etc.). <br><br>  I want to upload them to Ignite.  We create a Spark session, specify the address of the wizard, and trigger the loading of these tables, passing parameters.  An example on Scala, and not on Java, because Scala is less verbose and so is better for example. <br><br>  We pass the name of the file, read it, indicate that it is multiline, this is a standard json file.  Then write to Ignite.  We do not describe the structure of our file anywhere - Spark itself determines what data we have and what their structure is.  If everything goes smoothly, a table is created that contains all the necessary fields of the desired data types.  This is how we can load everything inside Ignite. <br><br>  When the data is loaded, we can see it in Ignite and use it immediately.  As a simple example - a request that allows you to find out which team played the most matches.  We have two columns: hometeam and awayteam, hosts and guests.  We select, group, count, summarize and join with data by command - to enter the name of the command.  Ta-dam - and the data from the json-chikov we got into Ignite.  We see Paris Saint-Germain, Toulouse - we had a lot of data on French teams. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8a4/202/52b/8a420252be6fb8df3a9083d7411911a9.png"><br><br>  We summarize.  We have now loaded the data from the source, json-file, into Ignite, and quite quickly.  Perhaps, from the point of view of big data, this is not too large, but for a local computer it is decent.  The table schema is taken from the json file in its original form.  The table was created, the column names were copied from the source file, the primary key was created.  The ID is everywhere, and the primary key is the ID.  This data got into Ignite, we can use it. <br><br><h2>  IgniteSparkSession and IgniteCatalog </h2><br>  Let's see how it works. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/654/24a/4ee/65424a4eeda4a4c2c6cce7038e13d1a9.png"><br>  <a href="https://github.com/nizhikov/data-frames-examples/blob/master/src/main/scala/CatalogExample.scala">CatalogExample.scala</a> <br><br>  In a fairly simple way, you can access and query all your data.  In the past example, we ran a standard Spark session.  And Ignite had no specifics there - besides the fact that you have to attach a jar with the necessary data source - a completely standard operation via the public API.  But, if you want to access the Ignite-tables automatically, you can use our extension.  The difference is that instead of SparkSession, we write IgniteSparkSession. <br><br>  As soon as you create an IgniteSparkSession object, you can see in the directory all the tables that you just loaded into Ignite.  You can see their diagram and all the information.  Spark already knows about the tables that Ignite has, and you can easily get all the data. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/dec/f1b/a0c/decf1ba0c5db2e0d84e50a0e88b6c192.png"><br><br><h2>  IgniteOptimization </h2><br>  When you make complex queries in Ignite using a JOIN, Spark first pulls out the data and only then JOIN groups it.  To optimize the process, we made the IgniteOptimization feature - it optimizes the Spark query plan and allows you to forward into the Ignite those parts of the query that can be executed inside Ignite.  We show optimization on a specific query. <br><br><pre> <code class="sql hljs">SQL Query: <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span>   city_id,   <span class="hljs-keyword"><span class="hljs-keyword">count</span></span>(*) <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span>   person p <span class="hljs-keyword"><span class="hljs-keyword">GROUP</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> city_id <span class="hljs-keyword"><span class="hljs-keyword">HAVING</span></span> <span class="hljs-keyword"><span class="hljs-keyword">count</span></span>(*) &gt; <span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br>  Execute the request.  We have a person table - some employees, people.  Each employee knows the ID of the city in which he lives.  We want to know how many people live in each city.  Filter - in which city more than one person lives.  Here is the original plan Spark is building: <br><br><pre> <code class="scala hljs">== <span class="hljs-type"><span class="hljs-type">Analyzed</span></span> <span class="hljs-type"><span class="hljs-type">Logical</span></span> <span class="hljs-type"><span class="hljs-type">Plan</span></span> == city_id: bigint, count(<span class="hljs-number"><span class="hljs-number">1</span></span>): bigint <span class="hljs-type"><span class="hljs-type">Project</span></span> [city_id#<span class="hljs-number"><span class="hljs-number">19</span></span>L, count(<span class="hljs-number"><span class="hljs-number">1</span></span>)#<span class="hljs-number"><span class="hljs-number">52</span></span>L] +- <span class="hljs-type"><span class="hljs-type">Filter</span></span> (count(<span class="hljs-number"><span class="hljs-number">1</span></span>)#<span class="hljs-number"><span class="hljs-number">54</span></span>L &gt; cast(<span class="hljs-number"><span class="hljs-number">1</span></span> as bigint))  +- <span class="hljs-type"><span class="hljs-type">Aggregate</span></span> [city_id#<span class="hljs-number"><span class="hljs-number">19</span></span>L], [city_id#<span class="hljs-number"><span class="hljs-number">19</span></span>L, count(<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-type"><span class="hljs-type">AS</span></span> count(<span class="hljs-number"><span class="hljs-number">1</span></span>)#<span class="hljs-number"><span class="hljs-number">52</span></span>L, count(<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-type"><span class="hljs-type">AS</span></span> count(<span class="hljs-number"><span class="hljs-number">1</span></span>)#<span class="hljs-number"><span class="hljs-number">54</span></span>L] +- <span class="hljs-type"><span class="hljs-type">SubqueryAlias</span></span> p    +- <span class="hljs-type"><span class="hljs-type">SubqueryAlias</span></span> person       +- <span class="hljs-type"><span class="hljs-type">Relation</span></span>[<span class="hljs-type"><span class="hljs-type">NAME</span></span>#<span class="hljs-number"><span class="hljs-number">11</span></span>,<span class="hljs-type"><span class="hljs-type">BIRTH_DATE</span></span>#<span class="hljs-number"><span class="hljs-number">12</span></span>,<span class="hljs-type"><span class="hljs-type">IS_RESIDENT</span></span>#<span class="hljs-number"><span class="hljs-number">13</span></span>,<span class="hljs-type"><span class="hljs-type">SALARY</span></span>#<span class="hljs-number"><span class="hljs-number">14</span></span>,<span class="hljs-type"><span class="hljs-type">PENSION</span></span>#<span class="hljs-number"><span class="hljs-number">15</span></span>,<span class="hljs-type"><span class="hljs-type">ACCOUNT</span></span>#<span class="hljs-number"><span class="hljs-number">16</span></span>,<span class="hljs-type"><span class="hljs-type">AGE</span></span>#<span class="hljs-number"><span class="hljs-number">17</span></span>,<span class="hljs-type"><span class="hljs-type">ID</span></span>#<span class="hljs-number"><span class="hljs-number">18</span></span>L,<span class="hljs-type"><span class="hljs-type">CITY_ID</span></span>#<span class="hljs-number"><span class="hljs-number">19</span></span>L]         <span class="hljs-type"><span class="hljs-type">IgniteSQLRelation</span></span>[table=<span class="hljs-type"><span class="hljs-type">PERSON</span></span>]</code> </pre><br>  Relation is just an Ignite table.  There are no filters - we just download all data from the Person table from the cluster over the network.  Then Spark aggregates all this - in accordance with the query and returns the result of the query. <br><br>  It is easy to see that all this subtree with filter and aggregation can be executed inside Ignite.  This will be much more efficient than pulling all the data from a potentially large Spark table ‚Äî this is what our IgniteOptimization feature does.  After analyzing and optimizing the tree, we get the following plan: <br><br><pre> <code class="scala hljs">== <span class="hljs-type"><span class="hljs-type">Optimized</span></span> <span class="hljs-type"><span class="hljs-type">Logical</span></span> <span class="hljs-type"><span class="hljs-type">Plan</span></span> == <span class="hljs-type"><span class="hljs-type">Relation</span></span>[<span class="hljs-type"><span class="hljs-type">CITY_ID</span></span>#<span class="hljs-number"><span class="hljs-number">19</span></span>L,<span class="hljs-type"><span class="hljs-type">COUNT</span></span>(<span class="hljs-number"><span class="hljs-number">1</span></span>)#<span class="hljs-number"><span class="hljs-number">52</span></span>L]   <span class="hljs-type"><span class="hljs-type">IgniteSQLAccumulatorRelation</span></span>(     columns=[<span class="hljs-type"><span class="hljs-type">CITY_ID</span></span>, <span class="hljs-type"><span class="hljs-type">COUNT</span></span>(<span class="hljs-number"><span class="hljs-number">1</span></span>)], qry=<span class="hljs-type"><span class="hljs-type">SELECT</span></span> <span class="hljs-type"><span class="hljs-type">CITY_ID</span></span>, <span class="hljs-type"><span class="hljs-type">COUNT</span></span>(<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-type"><span class="hljs-type">FROM</span></span> <span class="hljs-type"><span class="hljs-type">PERSON</span></span> <span class="hljs-type"><span class="hljs-type">GROUP</span></span> <span class="hljs-type"><span class="hljs-type">BY</span></span> city_id <span class="hljs-type"><span class="hljs-type">HAVING</span></span> count(<span class="hljs-number"><span class="hljs-number">1</span></span>) &gt; <span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br>  As a result, we have only one relation, since we optimized the whole tree.  And inside you can already see that the request will go to Ignite, which is quite close to the original request. <br><br>  Suppose we are joining different data sources: for example, one DataFrame is from Ignite, the second is from json, the third is again from Ignite, and the fourth is from some relational database.  In this case, only the subtree will be optimized in the plan.  We optimize what we can, throw it into Ignite, and Spark will do the rest.  Due to this, we get a gain in speed. <br><br>  Another example with JOIN: <br><br><pre> <code class="sql hljs">SQL Query - <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> jt1.id <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> id1, jt1.val1, jt2.id <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> id2, jt2.val2 <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> jt1 <span class="hljs-keyword"><span class="hljs-keyword">JOIN</span></span> jt2 <span class="hljs-keyword"><span class="hljs-keyword">ON</span></span> jt1.val1 = jt2.val2</code> </pre><br>  We have two tables.  We click by value and select all of them - ID, values.  Spark offers a plan like this: <br><br><pre> <code class="scala hljs">== <span class="hljs-type"><span class="hljs-type">Analyzed</span></span> <span class="hljs-type"><span class="hljs-type">Logical</span></span> <span class="hljs-type"><span class="hljs-type">Plan</span></span> == id1: bigint, val1: string, id2: bigint, val2: string <span class="hljs-type"><span class="hljs-type">Project</span></span> [id#<span class="hljs-number"><span class="hljs-number">4</span></span>L <span class="hljs-type"><span class="hljs-type">AS</span></span> id1#<span class="hljs-number"><span class="hljs-number">84</span></span>L, val1#<span class="hljs-number"><span class="hljs-number">3</span></span>, id#<span class="hljs-number"><span class="hljs-number">6</span></span>L <span class="hljs-type"><span class="hljs-type">AS</span></span> id2#<span class="hljs-number"><span class="hljs-number">85</span></span>L, val2#<span class="hljs-number"><span class="hljs-number">5</span></span>] +- <span class="hljs-type"><span class="hljs-type">Join</span></span> <span class="hljs-type"><span class="hljs-type">Inner</span></span>, (val1#<span class="hljs-number"><span class="hljs-number">3</span></span> = val2#<span class="hljs-number"><span class="hljs-number">5</span></span>) :- <span class="hljs-type"><span class="hljs-type">SubqueryAlias</span></span> jt1 : +- <span class="hljs-type"><span class="hljs-type">Relation</span></span>[<span class="hljs-type"><span class="hljs-type">VAL1</span></span>#<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-type"><span class="hljs-type">ID</span></span>#<span class="hljs-number"><span class="hljs-number">4</span></span>L] <span class="hljs-type"><span class="hljs-type">IgniteSQLRelation</span></span>[table=<span class="hljs-type"><span class="hljs-type">JT1</span></span>] +- <span class="hljs-type"><span class="hljs-type">SubqueryAlias</span></span> jt2    +- <span class="hljs-type"><span class="hljs-type">Relation</span></span>[<span class="hljs-type"><span class="hljs-type">VAL2</span></span>#<span class="hljs-number"><span class="hljs-number">5</span></span>,<span class="hljs-type"><span class="hljs-type">ID</span></span>#<span class="hljs-number"><span class="hljs-number">6</span></span>L] <span class="hljs-type"><span class="hljs-type">IgniteSQLRelation</span></span>[table=<span class="hljs-type"><span class="hljs-type">JT2</span></span>]</code> </pre> <br>  We see that he will pull out all the data from one table, all the data from the second, join them inside of himself and produce results.  After processing and optimization, we get exactly the same query that goes to Ignite, where it is executed relatively quickly. <br><br><pre> <code class="scala hljs">== <span class="hljs-type"><span class="hljs-type">Optimized</span></span> <span class="hljs-type"><span class="hljs-type">Logical</span></span> <span class="hljs-type"><span class="hljs-type">Plan</span></span> == <span class="hljs-type"><span class="hljs-type">Relation</span></span>[<span class="hljs-type"><span class="hljs-type">ID</span></span>#<span class="hljs-number"><span class="hljs-number">84</span></span>L,<span class="hljs-type"><span class="hljs-type">VAL1</span></span>#<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-type"><span class="hljs-type">ID</span></span>#<span class="hljs-number"><span class="hljs-number">85</span></span>L,<span class="hljs-type"><span class="hljs-type">VAL2</span></span>#<span class="hljs-number"><span class="hljs-number">5</span></span>] <span class="hljs-type"><span class="hljs-type">IgniteSQLAccumulatorRelation</span></span>(columns=[<span class="hljs-type"><span class="hljs-type">ID</span></span>, <span class="hljs-type"><span class="hljs-type">VAL1</span></span>, <span class="hljs-type"><span class="hljs-type">ID</span></span>, <span class="hljs-type"><span class="hljs-type">VAL2</span></span>], qry= <span class="hljs-type"><span class="hljs-type">SELECT</span></span> <span class="hljs-type"><span class="hljs-type">JT1</span></span>.<span class="hljs-type"><span class="hljs-type">ID</span></span> <span class="hljs-type"><span class="hljs-type">AS</span></span> id1, <span class="hljs-type"><span class="hljs-type">JT1</span></span>.<span class="hljs-type"><span class="hljs-type">VAL1</span></span>, <span class="hljs-type"><span class="hljs-type">JT2</span></span>.<span class="hljs-type"><span class="hljs-type">ID</span></span> <span class="hljs-type"><span class="hljs-type">AS</span></span> id2, <span class="hljs-type"><span class="hljs-type">JT2</span></span>.<span class="hljs-type"><span class="hljs-type">VAL2</span></span> <span class="hljs-type"><span class="hljs-type">FROM</span></span> <span class="hljs-type"><span class="hljs-type">JT1</span></span> <span class="hljs-type"><span class="hljs-type">JOIN</span></span> <span class="hljs-type"><span class="hljs-type">JT2</span></span> <span class="hljs-type"><span class="hljs-type">ON</span></span> <span class="hljs-type"><span class="hljs-type">JT1</span></span>.val1 = <span class="hljs-type"><span class="hljs-type">JT2</span></span>.val2 <span class="hljs-type"><span class="hljs-type">WHERE</span></span> <span class="hljs-type"><span class="hljs-type">JT1</span></span>.val1 <span class="hljs-type"><span class="hljs-type">IS</span></span> <span class="hljs-type"><span class="hljs-type">NOT</span></span> <span class="hljs-type"><span class="hljs-type">NULL</span></span> <span class="hljs-type"><span class="hljs-type">AND</span></span> <span class="hljs-type"><span class="hljs-type">JT2</span></span>.val2 <span class="hljs-type"><span class="hljs-type">IS</span></span> <span class="hljs-type"><span class="hljs-type">NOT</span></span> <span class="hljs-type"><span class="hljs-type">NULL</span></span>)</code> </pre> <br>  I will show another example. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ba4/39a/493/ba439a493e76dd573966cad413c07650.png"><br>  <a href="https://github.com/nizhikov/data-frames-examples/blob/master/src/main/scala/OptimizationExample.scala">OptimizationExample.scala</a> <br><br>  We create the IgniteSpark session, which automatically includes all our optimization capabilities.  Here is the query: find the players with the highest rating and display their names.  In the player table - their attributes and data.  We join, filter garbage data and display the players with the highest rating.  Let's see what plan we have after optimization, and show the results of this query. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c7d/c51/9ab/c7dc519abdfa6b3b1d7a8396ef9725b3.png"><br><br>  We start.  We see familiar names: Messi, Buffon, Ronaldo, etc.  By the way, some for some reason are found in two ways - Messi and Ronaldo.  It may seem strange to football fans that unknown players are on the list.  These are goalkeepers, players with rather high characteristics - compared to other players.  Now we look at the plan of the request that was executed.  In Spark, almost nothing was done, that is, we sent the entire request again to Ignite. <br><br><h2>  Apache Ignite Development </h2><br>  Our project is an open source product, so we always welcome patches and feedback from developers.  Your help, feedback, patches are very welcome.  We are waiting for them.  90% of the Ignite-community is Russian-speaking.  For example, for me, until I started working on Apache Ignite, not the best knowledge of English was a deterrent.  In Russian, it is hardly worth writing a dev-list, but even if you write something wrong, they will answer and help you. <br><br>  What can be improved on this integration?  How can you help if you have such a desire?  The list below.  Asterisks denote complexity. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/de4/d43/ed0/de4d43ed01894ce6b02865ad9f6aef5d.png"><br>  To test the optimization you need to write tests with complex queries.  Above, I showed some obvious queries.  It is clear that if you write a lot of groupings and a lot of joins, then something can fall.  This is a very simple task - come and do it.  If we find any bugs on the results of the tests, they will need to be fixed.  There will be more difficult. <br><br>  Another clear and interesting task is the integration of Spark with a thin client.  He is initially able to specify some sets of IP addresses, and this is enough to join the Ignite cluster, which is convenient in the case of integration with an external system.  If you suddenly want to join the solution of this problem, I will personally help with it. <br><br>  If you want to join the Apache Ignite community, here are some helpful links: <br><br><ul><li>  <i>Start here - <a href="https/ignite.apache.org/community/resources.html">https://ignite.apache.org/community/resources.html</a></i> <br></li><li>  <i>The sources here are <a href="https://github.com/apache/ignite/">https://github.com/apache/ignite/</a></i> <br></li><li>  <i>Docks here - <a href="https://apacheignite.readme.io/docs">https://apacheignite.readme.io/docs</a></i> <br></li><li>  <i>The bugs here are <a href="https://issues.apache.org/jira/browse/IGNITE">https://issues.apache.org/jira/browse/IGNITE</a></i> <br></li><li>  <i>You can write here - dev@ignite.apache.org, user@ignite.apache.org</i> <br></li></ul><br>  We have a responsive dev-list, which will help you.  It is still far from ideal, but in comparison with other projects it is really alive. <br><br>  <i>If you know Java or C ++, you are looking for a job and want to develop Open Source (Apache Ignite, Apache Kafka, Tarantool, etc.) write here: join-open-source@sberbank.ru.</i> <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/CzbAweNKEVY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p>Source: <a href="https://habr.com/ru/post/427297/">https://habr.com/ru/post/427297/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../427285/index.html">School on the basics of digital circuitry: Novosibirsk - Ok, Krasnoyarsk - get ready</a></li>
<li><a href="../427289/index.html">Geological 3D-modeling, logging and technaton from Aramco Innovations</a></li>
<li><a href="../427291/index.html">Traffic minimization in ASP.NET Web Forms, clickable div and periodic server polling</a></li>
<li><a href="../427293/index.html">JavaScript Design Patterns</a></li>
<li><a href="../427295/index.html">JavaScript Currying</a></li>
<li><a href="../427299/index.html">And let's collect something else? The designer 3 in 1 "Lunar fleet"</a></li>
<li><a href="../427301/index.html">DB crash on GitHub</a></li>
<li><a href="../427303/index.html">Slow Windows, Part 2: Creating Processes</a></li>
<li><a href="../427305/index.html">Arkady Volozh refused to sell his stake in Yandex</a></li>
<li><a href="../427307/index.html">The practice of testing backend for Java + Rest-Assured</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>