<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to switch to microservices and not break production</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Today we will tell how a monolithic solution was transferred to microservices. Through our application around the clock passes from 20 to 120 thousand...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to switch to microservices and not break production</h1><div class="post__text post__text-html js-mediator-article">  Today we will tell how a monolithic solution was transferred to microservices.  Through our application around the clock passes from 20 to 120 thousand transactions per day.  Users work in 12 time zones.  At the same time, functionality was added a lot and often, which is quite difficult to do on a monolith.  That is why the system required steady operation in 24/7 mode, that is, HighLoad, High Availability and Fault Tolerance. <br><br>  We develop this product on the <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B8%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25B0%25D0%25BB%25D1%258C%25D0%25BD%25D0%25BE_%25D0%25B6%25D0%25B8%25D0%25B7%25D0%25BD%25D0%25B5%25D1%2581%25D0%25BF%25D0%25BE%25D1%2581%25D0%25BE%25D0%25B1%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B4%25D1%2583%25D0%25BA%25D1%2582">MVP</a> model.  The architecture changed in several stages following the requirements of the business.  Initially, it was not possible to do everything at once, because no one knew what the solution should look like.  We moved along the Agile model, iteratively adding and extending functionality. <br><br><img src="https://habrastorage.org/webt/go/ss/pl/gosspl6ntqz3_bcmlex9tqndxjg.jpeg"><br><a name="habracut"></a><br>  Initially, the architecture looked like this: we had MySql with a single war, Tomcat and Nginx for proxying user requests. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/webt/hj/qk/fm/hjqkfm7v_pta5urwbeeujw-pk6w.jpeg"><br><br>  Environments (&amp; Minimum CI / CD): <br><br><ul><li>  Dev - deploy to push to develop, </li><li>  QA - develop once a day, </li><li>  Prod - on the button with the master, </li><li>  Run integration tests manually, </li><li>  Everything works on Jenkins. </li></ul><br>  The development was based on user scripts.  Already at the start of the project, most of the scenarios fit into a certain workflow.  But still not everything, and this circumstance complicated our development and did not allow us to carry out a ‚Äúdeep design‚Äù. <br><br><img src="https://habrastorage.org/webt/m4/xr/wf/m4xrwfqfgqvbosct0hy6uiinldk.jpeg"><br><br>  In 2015, our application saw production.  Industrial operation has shown that we do not have enough flexibility in the operation of the application, its development and in sending changes to the prod-server.  We wanted to achieve High Availability (HA), Continuous Delivery (CD) and Continuous Integration (CI). <br><br>  Here are the problems that needed to be solved in order to come to HI, CD, CI: <br><br><ul><li>  idle time when rolling out new versions - deployed applications for too long </li><li>  the problem with changing product requirements and new user cases - it took too much time to test and verify even with small fixes, </li><li>  Problem with restoring sessions to Tomcat: session management for the booking system and third-party services; upon restarting the application, the session was not restored by Tomcat, </li><li>  problems with the release of resources: it was necessary to reboot Tomcat sooner or later, a memory leak occurred. </li></ul><br>  We began to solve all these problems one by one.  And the first thing that they took up is the changing requirements for the product. <br><br><h2>  First microservice </h2><br>  <b>Challenge:</b> Changing product requirements and new use cases. <br>  <b>Technological answer: The</b> first microservice appeared - they carried part of the business logic into a separate war file and put it in Tomcat. <br><br><img src="https://habrastorage.org/webt/be/xm/ow/bexmowv2tqy3pjt43bkmpz2voby.jpeg"><br><br>  Another task of the form came to us: by the end of the week, update the business logic in the service and we decided to put this part into a separate war-file and put it in the same Tomcat.  We used Spring Boot for speed configuration and development. <br><br>  We made a small business function that solved a problem with periodically changing user parameters.  In the event of a change in business logic, we would not have to restart the whole Tomcat, lose our users for half an hour and reload only a small part of it. <br><br>  After successfully making the logic on the same principle, we continued to make changes to the application.  And from the moment when tasks that radically changed something within the system came to us, we carried these parts separately.  Thus, we constantly accumulated new microservices. <br><blockquote>  The main approach by which we began to allocate microservices is the allocation of a business function or the entire business service. </blockquote>  So we quickly separated services integrated with third-party systems, such as 1C. <br><br><h2>  The first problem is typing. </h2><br>  <b>Call:</b> Microservices are already 15. The problem of typing. <br>  <b>Technical answer:</b> Spring Cloud Feign. <br><br>  The problems did not dare by themselves just because we started cutting our solutions into microservices.  Moreover, new problems began to arise: <br><br><ul><li>  the problem of typing and versioning in Dto between modules, </li><li>  How to fix not one war file in Tomcat, but a lot. </li></ul><br>  New problems have increased the restart time of all Tomcat with technical work.  So, we have complicated our work. <br><br>  The problem with typing, of course, did not arise by itself.  Most likely, during several releases we simply ignored it, because we found these errors during the testing or development stages and had time to do something.  But when several errors were discovered quite halfway in production and required an urgent correction, we introduced regulations or began to use tools that solve this problem.  We paid attention to the Spring Cloud Feign - this is the client library for http requests. <br><blockquote>  <a href="https://github.com/OpenFeign/feign">github.com/OpenFeign/feign</a> <br>  <a href="https://cloud.spring.io/spring-cloud-netflix/multi/multi_spring-cloud-feign.html">cloud.spring.io/spring-cloud-netflix/multi/multi_spring-cloud-feign.html</a> </blockquote>  We chose it because <br><br>  - little overhead for implementation in the project, <br>  - he generated the client himself, <br>  - it is possible to use one interface both on the server and on the client. <br><br>  He solved our typing problems with what we formed clients.  And for the controllers of our services, we used the same interfaces as for the formation of clients.  So the typing problems are gone. <br><br><h2>  Downtime.  First fight.  Performance </h2><br>  <b>Challenge of business:</b> 18 microservices, now idle times in system operation are inadmissible. <br>  <b>Technical answer:</b> changing architecture, increasing servers. <br><br>  We still have a problem with downtime and rolling out new versions, there is a problem with restoring a Tomcat session and freeing resources.  The number of microservices continued to grow. <br><br>  The process of deploying all microservices took about an hour.  Periodically, you had to restart the application due to the problem with the release of resources from tomcat.  There were no easy ways to do this more quickly. <br><br>  We began to think about how to change the architecture.  Together with the department of infrastructure solutions, we built a new solution based on what we already had. <br><br><img src="https://habrastorage.org/webt/0k/ot/tu/0kottu3meen40fgkxn3cfm8tje8.jpeg"><br><br>  The architecture has changed its look as follows: <br><br><ul><li>  horizontally divided our application into several data centers, </li><li>  added a filebeat to each server </li><li>  added a separate server for ELK, as the number of transactions and logs grew, </li><li>  several haproxy + Tomcat + Nginx + MySQL servers (this is how we ensured High Availability). </li></ul><br>  The technologies used were: <br><br><ul><li>  Haproxy is engaged in routing and balancing between servers, </li><li>  Nginx is responsible for the distribution of statics, tomcat was the application server, </li><li>  The peculiarity of the solution was that MySQL on each of the servers does not know about the existence of its other MySQL, </li><li>  Because of the latency problem between data centers, replication at the MySQL level was impossible.  Therefore, we decided to implement sharding at the level of microservices. </li></ul><br>  Accordingly, when a request came from the user to the services in Tomcat, they simply requested data from MySQL.  The data that required integrity was collected from all servers and glued together (all requests were through the API). <br><br>  Applying this approach, we lost a little in the consistency of the data, but we solved the current problem.  The user could work with our application in any situations. <br><br><ul><li>  Even if one of the servers fell, we still had 3-4, which, supported the performance of the entire system. </li><li>  We stored backups not on servers in the same data center in which they were made, but in neighboring ones.  This helped us with disaster recovery. </li><li>  Fault tolerance was also solved at the expense of several servers. </li></ul><br>  So major problems were solved.  Out easy to work users.  Now they did not feel when we rolled update. <br><br><h2>  Downtime.  Fight the second.  Full value </h2><br>  <b>Business challenge:</b> 23 microservices.  Data consistency issues. <br>  <b>Technical solution:</b> launch services separately from each other.  Improved monitoring.  Zuul and Eureka.  Simplified the development of individual services and their delivery. <br><br>  Problems continued to appear.  This is how our finite looked like: <br><br><ul><li>  We did not have the consistency of the data with rediploy, so some of the functionality (not the most important) faded into the background.  For example, when rolling in a new application, statistics worked defectively. </li><li>  We had to kick users from one server to another in order to restart the application.  It also took about 15-20 minutes.  On top of that, users had to log in when switching from server to server. </li><li>  We also restarted Tomcat more often due to the growing number of services.  And now I had to follow a large number of new microservices. </li><li>  Redploying time has grown in proportion to the number of services and servers. </li></ul><br>  Thinking, we decided that our problem would be solved by the launch of services separately from each other - if we launch services not in one Tomcat, but each in its own on one server. <br><br><img src="https://habrastorage.org/webt/t-/64/9w/t-649wqv_wbymazrk1tkbgs8e1i.jpeg"><br><br>  But there were other questions: how can services now communicate with each other, which ports should be open to the outside? <br><br>  We selected a number of ports and distributed them to our modules.  To avoid the need to keep all this information about the ports somewhere in the pom-file or the general configuration, we chose Zuul and Eureka to solve these problems. <br><blockquote>  <a href="https://github.com/Netflix/eureka">Eureka - service discovery</a> <br>  <a href="https://github.com/Netflix/zuul">Zuul - proxy</a> (to save contextual URLs that were in Tomcat) </blockquote>  Eureka also improved our performance in High Availability / Fault Tolerance, since communication between services is now possible.  We set up so that if the current data center does not have the necessary service, go to another. <br><br>  To improve monitoring, we added Spring Boot Admin from the existing stack to understand what service is running on it. <br><br>  We also began to translate our dedicated services to a stateless architecture in order to get rid of the deployment problems of several identical services on one server.  This gave us horizontal scaling within a single data center.  Inside one server, we ran different versions of the same application when upgrading, so that even on it there was no downtime. <br><br>  It turned out that we approached the Continuous Delivery / Continuous Integration by simplifying the development of individual services and their delivery.  Now there was no need to fear that the delivery of one service would cause a leak of resources and would have to restart the entire service. <br><br>  The idle time while rolling out new versions still remained, but not entirely.  When we alternately updated several jar on the server, this happened quickly.  And there was no problem on the server when updating a large number of modules.  But restarting all 25 microservices during the update took a lot of time.  Though faster than inside Tomcat, which does it consistently. <br><br>  The problem with the release of resources, we also decided that we started everything with a jar, and the system Out of memory killer dealt with leaks or problems. <br><br><h2>  Third fight, information management </h2><br>  <b>Business challenge:</b> 28 microservices.  A lot of information that needs to be managed. <br>  <b>Technical solution:</b> Hazelcast. <br><br>  We continued to implement our architecture and realized that our basic business transaction covers several servers at once.  It was inconvenient for us to send a request to a dozen systems.  Therefore, we decided to use Hazelcast for event-messaging and for system work with users.  Also for subsequent services used it as a layer between the service and the database. <br><br><img src="https://habrastorage.org/webt/h5/8m/kz/h58mkzx0ogomm7cs6atxeutwpdu.jpeg"><br><br>  We finally got rid of the problem with the consistency of our data.  Now we could save any data in all databases at the same time, without doing any unnecessary actions.  We told Hazelcast, in which databases it should store incoming information.  He did this on every server, which simplified our work and allowed us to get rid of sharding.  And thus we moved to replication at the application level. <br><br>  Also now we began to store the session in Hazelcast and used it for authorization.  This allowed users to be poured between servers imperceptibly for them. <br><br><h2>  From microservices to CI / CD </h2><br>  <b>Challenge business:</b> you need to speed up the release of updates in production. <br>  <b>Technical solution:</b> our application deployment pipeline, GitFlow for working with code. <br><br>  Together with the number of microservices, the internal infrastructure also developed.  We wanted to speed up the delivery of our services to production.  To do this, we implemented a new deployment pipeline for our application and moved to GitFlow to work with the code.  CI began to collect and run tests for each commit, run unit-tests, integration tests, add artifacts with the delivery of the application. <br><br><img src="https://habrastorage.org/webt/k9/f3/6n/k9f36nzpdqvfpowwbl1wb-hiqyi.jpeg"><br><br>  To do this quickly and dynamically, we deployed several GitLab-runners that started all these tasks according to the developers' push.  Thanks to the GitLab Flow approach, we have several servers: Develop, QA, Release-candidate and Production. <br><br>  Development occurs as follows.  The developer adds a new feature in a separate branch (feature branch).  After the developer has finished, he creates a request to merge his branch with the main development branch (Merge Request to Develop branch).  The request to merge is viewed by other developers and is accepted or not accepted, after which the comments are corrected.  After merging into the main branch, a special environment is developed, on which tests for elevation of the environment are performed. <br><br>  When all these stages are completed, the QA engineer takes the changes to his ‚ÄúQA‚Äù branch and performs tests on previously written feature cases and research tests. <br><br>  If the QA engineer approves the work done, then the changes go to the Release-Candidate branch and unfold on the environment that is accessible to external users.  In this environment, the customer accepts and verifies our technologies.  Then we translate it all into Production. <br><br>  If at some stage there are bugs, then it is in these branches that we solve these problems and are missing them in Develop.  We also made a small plugin so that Redmine could tell us what stage the feature is at. <br><br><img src="https://habrastorage.org/webt/fa/__/u9/fa__u9j7koqmzdfc9mjdxtbteby.jpeg"><br><br>  This helps testers look at what stage they need to connect to the task, and developers - to correct bugs, because they see at what stage the error occurred, they can go to a specific branch and play it there. <br><br><h2>  Further development </h2><br>  <b>Business Challenge:</b> Switch between servers without downtime. <br>  <b>Technical Solution:</b> Packaging in Kubernetes. <br><br><img src="https://habrastorage.org/webt/6m/qq/0n/6mqq0nhxkjz2noxidivvno7rwru.jpeg"><br><br>  Now at the end of the deployment, technical specialists report jer-ki to the PROD-server and restart them.  This is not very convenient.  We want to automate the work of the system further by implementing Kubernetes and linking it with the data center, updating them and rolling it all at once. <br><br>  To go to this model, we need to complete the following work. <br><br><ul><li>  Bring our current solutions to a stateless architecture so that the user can send requests to all servers without parsing.  Some of our services still support some session data.  This work concerns the replication of database data. </li><li>  We also need to cut the last small monolith, which contains several business processes.  This will lead us to the last major step - Continuous Delivery. </li></ul><br><h2>  PS What has changed with the transition to microservices </h2><br><ul><li>  We got rid of the problem of changing demands. </li><li>  We got rid of the problem of restoring sessions from Tomcat by transferring them to Hazelcast. </li><li>  When transferring users from one server to another, they do not have to re-login. </li><li>  Solved all the problems with the release of resources, shifting them onto the shoulders of the operating system. </li><li>  The problems of typing and versioning were solved thanks to Feign. </li><li>  We are confidently moving towards Continuous Delivery using Gitlab Pipelines. </li></ul></div><p>Source: <a href="https://habr.com/ru/post/352618/">https://habr.com/ru/post/352618/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../352608/index.html">What we should build Scrum: an interview with Agile-coach Vasily Savunov</a></li>
<li><a href="../352610/index.html">As I did Hot-Spot through Virtual Box</a></li>
<li><a href="../352612/index.html">Virtual server on VPS.house: performance review</a></li>
<li><a href="../352614/index.html">How to solve 90% of NLP tasks: a step-by-step guide to natural language processing</a></li>
<li><a href="../352616/index.html">Ansible is not so simple</a></li>
<li><a href="../352620/index.html">We open the history of the Bolshoi Theater. Part one</a></li>
<li><a href="../352622/index.html">Performance analysis of the drive Intel Optane SSD 750GB</a></li>
<li><a href="../352624/index.html">The Metrix has you ...</a></li>
<li><a href="../352626/index.html">Storage options for cryptographic keys</a></li>
<li><a href="../352628/index.html">Rumors about the cancellation of the Kotelnikov theorem are greatly exaggerated</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>