<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Word2vec in pictures</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="‚Äú There is a pattern in every thing that is part of the universe. It has symmetry, elegance and beauty - qualities that are captured above all by any ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Word2vec in pictures</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/85d/ad8/627/85dad8627ae6845b62f5bb965c291b19.png"></div><br><br><blockquote>  <font color="gray">‚Äú <b>There is a pattern in every thing that is part of the universe.</b></font>  <font color="gray"><b>It has symmetry, elegance and beauty</b> - qualities that are captured above all by any true artist who captures the world.</font>  <font color="gray">This pattern can be caught in the change of seasons, in the way the sand flows along the slope, in the tangled branches of a creosote shrub, in the pattern of its leaf.</font> <font color="gray"><br><br></font>  <font color="gray">We are trying to copy this pattern in our life and our society and therefore we love rhythm, song, dance, various pleasing and comforting forms.</font>  <font color="gray">However, one can also see the danger hidden in the search for absolute perfection, for it is obvious that the perfect pattern is unchanged.</font>  <font color="gray">And, approaching perfection, everything that exists goes to death "- <i>Dune</i> (1965)</font> </blockquote><br>  I believe that the concept of embeddings is one of the most remarkable ideas in machine learning.  If you have ever used Siri, Google Assistant, Alexa, Google Translate, or even a smartphone keyboard with the next word prediction, you have already worked with an attachment-based natural language processing model.  Over the past decades, there has been a significant development of this concept for neural models (the latest developments include contextualized word embeddings in advanced models, such as <a href="https://jalammar.github.io/illustrated-bert/">BERT</a> and GPT2). <br><a name="habracut"></a><br>  Word2vec is an effective investment creation method developed in 2013.  In addition to working with words, some of his concepts have proven effective in developing recommender mechanisms and giving meaning to data, even in commercial, non-linguistic tasks.  This technology has been applied in their recommendation engines by companies such as <a href="https://www.kdd.org/kdd2018/accepted-papers/view/real-time-personalization-using-embeddings-for-search-ranking-at-airbnb">Airbnb</a> , <a href="https://www.kdd.org/kdd2018/accepted-papers/view/billion-scale-commodity-embedding-for-e-commerce-recommendation-in-alibaba">Alibaba</a> , <a href="https://www.slideshare.net/AndySloane/machine-learning-spotify-madison-big-data-meetup">Spotify</a> and <a href="https://towardsdatascience.com/using-word2vec-for-music-recommendations-bb9649ac2484">Anghami</a> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In this article, we will look at the concept and mechanics of generating attachments using word2vec.  Let's start with an example to get acquainted with how to represent objects in a vector form.  Do you know how much a list of five numbers (vector) can tell about your personality? <br><br><h1>  Embedding personalities: what are you? </h1><br><blockquote>  <font color="gray">‚ÄúI give you the Desert chameleon;</font>  <font color="gray">its ability to blend in with sand will tell you everything you need to know about the roots of ecology and the foundations for preserving your personality. ‚Äù</font>  <font color="gray">- <i>Children of Dune</i></font> </blockquote><br>  On a scale from 0 to 100, do you have an introverted or extrovert personality type (where 0 is the most introverted type and 100 is the most extrovert)?  Have you ever passed a personality test: for example, MBTI, or even better, the <a href="https://en.wikipedia.org/wiki/Big_Five_personality_traits">‚Äúbig five‚Äù</a> ?  You are given a list of questions, and then evaluated on several axes, including introversion / extroversion. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/79f/11e/e22/79f11ee220ebf9d6f52f51a5b780b090.png"></div><br>  <i><font color="gray">Sample test results of the "big five".</font></i>  <i><font color="gray">He really says a lot about the individual and is able to predict <a href="http://psychology.okstate.edu/faculty/jgrice/psyc4333/FiveFactor_GPAPaper.pdf">academic</a> , <a href="">personal</a> and <a href="https://www.massgeneral.org/psychiatry/assets/published_papers/soldz-1999.pdf">professional success</a> .</font></i>  <i><font color="gray">For example, <a href="https://projects.fivethirtyeight.com/personality-quiz/">here</a> you can pass it</font></i> <br><br>  Suppose I scored 38 out of 100 for introversion / extraversion.  This can be represented as follows: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e56/729/35d/e5672935d7de17d41e78354d3742e6bc.png"></div><br><br>  Or on a scale from ‚àí1 to +1: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b39/e23/ce1/b39e23ce1c036b11763e3c45c3659a3e.png"></div><br><br>  How well do we recognize a person only by this assessment?  Not really.  People are complex creatures.  Therefore we will add one more dimension: one more characteristic from the test. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2aa/ab2/ebc/2aaab2ebc1ff30f1fd832e5cf5bf9cb1.png"></div><br>  <i><font color="gray">You can think of these two dimensions as a point on a graph or, even better, as a vector from the origin to this point.</font></i>  <i><font color="gray">There are great tools for working with vectors that will come in handy very soon.</font></i> <br><br>  I do not show what personality traits we put on the chart so that you don‚Äôt become attached to specific traits, but immediately understand the vector representation of the personality of the person as a whole. <br><br>  Now we can say that this vector partially reflects my personality.  This is a useful description if you compare different people.  Suppose I got hit by a red bus, and I need to replace me with a similar person.  Which of the two people on the next chart is more like me? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/de5/380/b84/de5380b84dc9fec4bb8b52ebe6519e15.png"></div><br><br>  When working with vectors, the similarity is usually calculated by <a href="https://en.wikipedia.org/wiki/Cosine_similarity">the Otiai coefficient</a> (geometric coefficient): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/640/e59/7dd/640e597dd741a28bcec986454633e31d.png"></div><br>  <i><font color="green">Man number 1 is</font> <font color="gray">more like me in character.</font></i>  <i><font color="gray">Vectors in one direction (length is also important) give a larger Otiai coefficient</font></i> <br><br>  Again, two measurements are not enough to assess people.  Decades of development of psychological science led to the creation of a test for five basic personality characteristics (with many additional ones).  So let's use all five dimensions: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/df5/3ae/d7b/df53aed7b1e439561a01e69b3f765487.png"></div><br><br>  The problem with five dimensions is that it is no longer possible to draw neat arrows in 2D.  This is a common problem in machine learning, where it is often necessary to work in multidimensional space.  It is good that the geometric factor works with any number of measurements: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/521/ab3/bf1/521ab3bf1374c5b37115441b7c2d27cc.png"></div><br>  <i><font color="gray">Geometric coefficient works for any number of measurements.</font></i>  <i><font color="gray">In five dimensions, the result is much more accurate.</font></i> <br><br>  At the end of this chapter I want to repeat two main ideas: <br><br><ol><li>  People (and other objects) can be represented as numerical vectors (which is great for cars!). <br></li><li>  We can easily calculate how similar the vectors are. </li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/195/73d/16d/19573d16de1150ac1874640c79e0b381.png"></div><br><br><h1>  Word embedding </h1><br><blockquote>  <font color="gray">"The gift of words is the gift of deception and illusion."</font>  <font color="gray">- <i>Children of Dune</i></font> </blockquote><br>  With this understanding, we turn to the vector representations of words obtained as a result of learning (they are also called embeddings) and look at their interesting properties. <br><br>  Here is an attachment for the word "king" (vector GloVe, trained on Wikipedia): <br><br> <code>[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]</code> <br> <br>  We see a list of 50 numbers, but it‚Äôs difficult to say something about them.  Let's visualize them to compare with other vectors.  Put the numbers in one row: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/516/c90/5ac/516c905ac831fe8688db73f0a63d325b.png"></div><br><br>  Let's color the cells by their values ‚Äã‚Äã(red for close to 2, white for close to 0, blue for close to ‚àí2): <br><br><div style="text-align:center;"> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/46f/7cb/1d5/46f7cb1d5adc32bd16368b2681ab26a4.png"></a> </div><br><br>  Now let's forget about the numbers, and only in colors we oppose the ‚Äúking‚Äù with other words: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1c8/6b2/909/1c86b290963e8a42b375cb6a71245185.png"></div><br><br>  See that the "man" and "woman" are much closer to each other than to the "king"?  It says something.  Vector representations capture quite a lot of information / meaning / associations of these words. <br><br>  Here is another list of examples (compare columns with similar colors): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d61/30b/d45/d6130bd4502710631a8c812923230f47.png"></div><br><br>  You may notice a few things: <br><br><ol><li>  Through all the words goes one red column.  That is, these words are similar in this particular dimension (and we do not know what is encoded in it). <br></li><li>  You can see that ‚Äúwoman‚Äù and ‚Äúgirl‚Äù are in many ways similar.  The same with "man" and "boy". <br></li><li>  "Boy" and "girl" are also similar in some dimensions, but different from "woman" and "man."  Perhaps this is coded vague idea of ‚Äã‚Äãyouth?  Probably. <br></li><li>  Everything, except the last word, is a representation of people.  I added an object (water) to show the differences between the categories.  For example, you can see how the blue column goes down and stops in front of the ‚Äúwater‚Äù vector. <br></li><li>  There are clear measurements, where the "king" and "queen" are similar to each other and differ from all others.  Maybe there is encoded vague concept of royal power? </li></ol><br><h1>  Analogies </h1><br><blockquote>  <font color="gray">‚ÄúWords endure whatever burden we desire.</font>  <font color="gray">All that is required for this is an agreement on tradition, according to which we build concepts. ‚Äù</font>  <font color="gray">- <i>God-emperor Dunes</i></font> </blockquote><br>  Famous examples that show the incredible properties of investments - the concept of analogies.  We can add and subtract word vectors to get interesting results.  The most famous example is the formula ‚Äúking is a man + woman‚Äù: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c23/71f/ead/c2371feadc58f2f2a1236c94b6b05eff.png"></div><br>  <i><font color="gray">Using the <a href="https://radimrehurek.com/gensim/">Gensim</a> library in python, we can add and subtract word vectors, and the library will find the words that are closest to the resulting vector.</font></i>  <i><font color="gray">The image shows a list of the most similar words, each with a coefficient of geometric similarity</font></i> <br><br>  We visualize this analogy, as before: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a19/84b/fea/a1984bfeab5a597c6fb6300f7d694901.png"></div><br>  <i><font color="gray">The resulting vector from the ‚Äúking ‚Äì man + woman‚Äù calculation is not exactly equal to the ‚Äúqueen‚Äù, but this is the closest result of 400,000 word attachments in the data set</font></i> <br><br>  Having considered the attachments of words, let's explore how learning takes place.  But before moving on to word2vec, you need to look at the conceptual ancestor of word embedding: the neural language model. <br><br><h1>  Language model </h1><br><blockquote>  <font color="gray">‚ÄúThe Prophet is not subject to the illusions of the past, present, or future.</font>  <font color="gray"><b>The fixedness of language forms determines such linear differences.</b></font>  <font color="gray">The prophets hold the key to the tongue lock.</font>  <font color="gray">For them, the physical image remains only a physical image and nothing more.</font> <font color="gray"><br><br></font>  <font color="gray">Their universe does not have the properties of a mechanical universe.</font>  <font color="gray">A linear sequence of events is assumed by the observer.</font>  <font color="gray">Cause and investigation?</font>  <font color="gray">We are talking about something completely different.</font>  <font color="gray">The Prophet expresses fateful words.</font>  <font color="gray">You see a glimpse of an event that should happen ‚Äúaccording to the logic of things‚Äù.</font>  <font color="gray">But the prophet instantly releases the energy of infinite miraculous power.</font>  <font color="gray">The universe is undergoing a spiritual shift. ‚Äù</font>  <font color="gray">- <i>God-emperor Dunes</i></font> </blockquote><br>  One example of NLP (natural language processing) is the function of predicting the next word on the keyboard of a smartphone.  Billions of people use it hundreds of times a day. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ca4/d48/a13/ca4d48a133d58fe3c4c11e0933ea218e.png"></div><br><br>  The prediction of the next word is a suitable task for <i>the language model</i> .  She can take a list of words (say two words) and try to predict the following. <br><br>  In the screenshot above, the model took these two green words ( <code>thou shalt</code> ) and returned a list of options (the highest probability for the word <code>not</code> ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5a7/0fc/492/5a70fc49208b501202ed188f24ad1f2c.png"></div><br><br>  We can present the model as a black box: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/164/72f/a83/16472fa83e5eadf58f4bb05b50075654.png"></div><br><br>  But in practice, the model gives more than one word.  It derives an estimate of the likelihood of virtually all known words (the ‚Äúdictionary‚Äù of the model varies from a few thousand to over a million words).  Then the keyboard application finds the words with the highest scores and shows them to the user. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7a7/eda/ad6/7a7edaad67dd51240d90426de0b198c2.png"></div><br>  <i><font color="gray">The neural model of the language gives the probability of all known words.</font></i>  <i><font color="gray">We indicate probability in percent, but in the resulting vector, 40% will be represented as 0.4</font></i> <br><br>  After training, the first neural models ( <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">Bengio 2003</a> ) calculated the forecast in three stages: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/27b/082/4f8/27b0824f81962e2863d6d4dcccabfdd2.png"></div><br><br>  The first step for us is the most relevant, because we are discussing investments.  As a result of the training, a matrix is ‚Äã‚Äãcreated with the attachments of all the words in our dictionary.  To get the result, we simply look for the input word attachments and run the prediction: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1d1/34b/5ac/1d134b5ac32406ea363944887ce5fc53.png"></div><br><br>  Now let's look at the learning process and find out how this attachment matrix is ‚Äã‚Äãcreated. <br><br><h1>  Learning a language model </h1><br><blockquote>  <font color="gray">‚ÄúThe process cannot be understood by stopping it.</font>  <font color="gray">Understanding should move with the process, merge with its flow and flow with it ‚Äù- <i>Dune</i></font> </blockquote><br>  Language models have a huge advantage over most other machine learning models: they can be taught on texts that we have in abundance.  Think of all the books, articles, Wikipedia materials and other forms of textual data that we have.  Compare with other machine learning models that need manual labor and specially collected data. <br><br><blockquote>  <b>‚ÄúYou must recognize the word from his company‚Äù - J. R. Furs</b> </blockquote><br>  Attachments for words are calculated according to the surrounding words, which most often appear alongside.  The mechanics are as follows: <br><br><ol><li>  We get a lot of text data (say, all Wikipedia articles) <br></li><li>  Install a window (for example, of three words) that slides across the text. <br></li><li>  A sliding window generates patterns to train our model. </li></ol><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a31/fc4/626/a31fc4626de165a21c2c91844b21e7ab.png"></div><br><br>  When this window slides through the text, we (actually) generate a data set, which we then use to train the model.  To understand, let's see how a sliding window handles this phrase: <br><br><blockquote>  <b>"Do not build a machine endowed with the likeness of the human mind" - <i>Dune</i></b> </blockquote><br>  When we start, the window is located on the first three words of the sentence: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/81c/c51/a04/81cc51a0478e1655c8f3f85641cf1e4e.png"></div><br><br>  The first two words are taken as signs, and the third word is taken as a label: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/097/981/086/0979810868ca398fdcad3066294055f5.png"></div><br>  <i><font color="gray">We generated the first sample in the data set, which can later be used to train a language model.</font></i> <br><br>  Then move the window to the next position and create the second sample: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/98c/0b3/f98/98c0b3f98ebf4790890fd2f66cf86ce9.png"></div><br><br>  And pretty soon we have a larger set of data: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4e7/3ce/50d/4e73ce50d863e1cbfde92a3b595dbaa3.png"></div><br><br>  In practice, models are usually trained directly in the process of moving a sliding window.  But logically, the ‚Äúdata set generation‚Äù phase is separated from the learning phase.  In addition to neural network approaches, the N-gram method was often used to teach language models (see the third chapter of the book <a href="http://web.stanford.edu/~jurafsky/slp3/">"Speech and Language Processing"</a> ).  To see the difference in the transition from N-grams to neural models in real products, <a href="https://blog.swiftkey.com/neural-networks-a-meaningful-leap-for-mobile-typing/">here‚Äôs a post in 2015 on Swiftkey's blog</a> , the developer of my favorite Android keyboard, which presents its neural language model and compares it with the previous N-gram model.  I like this example because it shows how the algorithmic properties of an attachment can be described in a marketing language. <br><br><h1>  We look both ways </h1><br><blockquote>  <font color="gray">‚ÄúThe paradox is a sign that we must try to consider what lies behind it.</font>  <font color="gray">If the paradox gives you concern, it means that you are striving for the absolute.</font>  <font color="gray">Relativists view the paradox simply as an interesting, perhaps funny, sometimes scary thought, but a thought very instructive. ‚Äù</font>  <font color="gray"><i>God the emperor Dunes</i></font> </blockquote><br>  Taking into account all the above, fill in the gap: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/680/613/871/680613871307e53415ab86fab022276a.png"></div><br><br>  As a context, there are five previous words (and the earlier mention of a ‚Äúbus‚Äù).  I am sure that most of you have guessed that there should be a ‚Äúbus‚Äù here.  But if I give you another word after the space, will it change your answer? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/45f/1fa/af0/45f1faaf0cdd4f57ac1699d87861934a.png"></div><br><br>  This completely changes the situation: now the missing word, most likely, is ‚Äúred‚Äù.  Obviously, there is informational value in words both before and after the space.  It turns out that accounting in both directions (left and right) allows you to calculate higher-quality investments.  Let's see how to adjust the training of the model in such a situation. <br><br><h1>  Skip-gram </h1><br><blockquote>  <font color="gray">"When an absolutely error-free choice is unknown, intelligence gets a chance to work with limited data in the arena, where mistakes are not only possible, but necessary."</font>  <font color="gray">- <i>Chapter Dunes</i></font> </blockquote><br>  In addition to the two words before the target, you can take into account two more words after it. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e2b/1f6/1a1/e2b1f61a179e7d6835b47c7149a47486.png"></div><br><br>  Then the data set for learning the model will look like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6ff/729/ed4/6ff729ed4ce86722dc9c3aa689614195.png"></div><br><br>  This is called the CBOW (Continuous Bag of Words) architecture and is described in <a href="https://arxiv.org/pdf/1301.3781.pdf">one of the word2vec</a> [pdf] <a href="https://arxiv.org/pdf/1301.3781.pdf">documents</a> .  There is another architecture, which also show excellent results, but it works a little differently: it tries to guess the adjacent words using the current word.  A sliding window looks like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc9/72d/baa/dc972dbaa78b592ba91b76e950ec56e0.png"></div><br>  <i><font color="gray">In the green slot is the input word, and each pink field represents a possible exit.</font></i> <br><br>  Pink rectangles have different shades, because this sliding window actually creates four separate patterns in our training data set: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/709/8ac/dde/7098acddea8266d1efd5663ed98e6303.png"></div><br><br>  This method is called <b>skip-gram</b> architecture.  You can visualize the sliding window as follows: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ee2/1d8/508/ee21d850835bde9e3f14250d267d88b1.png"></div><br><br>  The following four samples are added to the training data set: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a40/871/f1c/a40871f1c1c7b48723d3737c05fc6284.png"></div><br><br>  Then we move the window to the following position: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/14a/429/c7b/14a429c7b2ae6ba7383d6d39be9e3031.png"></div><br><br>  Which generates four more examples: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e9b/b3c/89a/e9bb3c89a00306b3fd18eb86d8f2160b.png"></div><br><br>  Soon we have much more samples: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6bb/749/096/6bb749096d3329712a7c00727b4d3cff.png"></div><br><br><h1>  Revision of the learning process </h1><br><blockquote>  <font color="gray">‚ÄúMuad'Dib quickly learned because first of all he was taught how to learn.</font>  <font color="gray">But the very first lesson was the assimilation of the belief that he can learn, and this is the basis of everything.</font>  <font color="gray">It‚Äôs amazing how many people don‚Äôt believe in what they can learn and learn, and how many more people feel that it‚Äôs very difficult to learn. ‚Äù</font>  <font color="gray">- <i>Dune</i></font> </blockquote><br>  Now that we have a set of skip-gram, we use it to teach a basic neural language model that predicts a neighboring word. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/944/fb7/70d/944fb770d3aff38f1befa40dfaa7402a.png"></div><br><br>  Let's start with the first sample in our data set.  We take a sign and send it to the untrained model with a request to predict the adjacent word. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/abb/5cb/9a3/abb5cb9a38d29f1a54176206637131dc.png"></div><br><br>  The model goes through three steps and displays the prediction vector (with probability for each word in the dictionary).  Since the model is not trained, at this stage its prediction is probably incorrect.  But it is nothing.  We know what word it predicts - this is the resultant cell in the row that we currently use to train the model: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8af/4fd/c3d/8af4fdc3d3cc86ec1c81fdb3d2715529.png"></div><br>  <i><font color="gray">‚ÄúTarget vector‚Äù is the one in which the target word has a probability of 1, and all other words have a probability of 0</font></i> <br><br>  How wrong was the model?  Subtract the forecast vector from the target and get the error vector: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e6d/b3e/395/e6db3e39593e9c8639d94ef4caccde58.png"></div><br><br>  This error vector can now be used to update the model, so the next time it is more likely to produce an exact result on the same input data. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7d3/6c0/476/7d36c047604b937c907a4ef38ceaaeb7.png"></div><br><br>  Here the first stage of training is completed.  We continue to do the same with the next sample in the dataset, and then with the next, until we look at all the samples.  This is the end of the first era of learning.  We repeat everything over and over again over several epochs, and as a result we get a trained model: from it we can extract the attachment matrix and use it in any applications. <br><br>  Although we learned a lot, but to fully understand how word2vec really learns, a couple of key ideas are missing. <br><br><h1>  Negative selection </h1><br><blockquote>  <font color="gray">‚ÄúTrying to understand Muad'Dib without understanding his mortal enemies, the Harkonnens, is the same thing as trying to understand the Truth without realizing what Lie is.</font>  <font color="gray">This is an attempt to know the Light without knowing the Darkness.</font>  <font color="gray">It's impossible".</font>  <font color="gray">- <i>Dune</i></font> </blockquote><br>  Let us recall three stages, as the neural model calculates the prediction: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8dd/fe1/4ca/8ddfe14ca387bd4d16c77eb9de8ce98f.png"></div><br><br>  The third step is very expensive from the computational point of view, especially if you do it for each sample in the data set (tens of millions of times).  It is necessary to somehow improve performance. <br><br>  One way is to divide the goal into two stages: <br><br><ol><li>  Create high quality word attachments (without predicting the next word). <br></li><li>  Use these high quality investments for language model training (for forecasting). </li></ol><br>  This article will focus on the first step.  To increase performance, you can move away from predicting the next word ... <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/77d/0a8/c17/77d0a8c17587248a0f790155809798fe.png"></div><br><br>  ... and switch to a model that takes the input and output words and calculates the probability of their proximity (from 0 to 1). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/220/0e3/e06/2200e3e063f3119099d1615e59538d2a.png"></div><br><br>  Such a simple transition replaces the neural network with a logistic regression model - thus, the calculations become much easier and faster. <br><br>  At the same time, the structure of our dataset needs to be refined: the label is now a new column with values ‚Äã‚Äãof 0 or 1. In our table there is a unit everywhere, because we added neighbors there. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc2/d1e/874/dc2d1e87438b2492dc9b6e4b1c72162e.png"></div><br><br>  This model is calculated at an incredible speed: millions of samples in minutes.  But you need to close one loophole.  If all our examples are positive (goal: 1), then a tricky model can be formed that always returns 1, demonstrating 100% accuracy, but it does not learn anything and generates garbage attachments. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1ba/aa2/2d0/1baaa22d0b0c06be5398f896fa7a4c4b.png"></div><br><br>  To solve this problem, you need to enter <i>negative patterns</i> into the data set ‚Äî words that are not exactly neighbors.  For them, the model is obliged to return 0. Now the model will have to work hard, but the calculations still go on at high speed. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f4c/194/0d8/f4c1940d80c5203620196907a1478431.png"></div><br>  <i><font color="gray">For each sample in the dataset add negative examples with label 0</font></i> <br><br>  But what to enter as output words?  Choose words arbitrarily: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/84e/b22/06f/84eb2206f26b053f1ea8ec4e1b76c5b6.png"></div><br><br>  This idea was born under the influence of the <a href="http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf">noise-comparison estimation</a> method [pdf].  We match the actual signal (positive examples of adjacent words) with noise (randomly selected words that are not neighbors).  This provides an excellent trade-off between performance and statistical efficiency. <br><br><h1>  Skip-gram with negative sampling (SGNS) </h1><br>  We looked at two central concepts of word2vec: together they are called ‚Äúskip-gram with negative sampling.‚Äù <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/873/720/fae/873720fae559ce7d6020be66ccb6c397.png"></div><br><br><h1>  Learning word2vec </h1><br><blockquote>  <font color="gray">‚ÄúThe machine cannot foresee every problem important to a living person.</font>  <font color="gray">There is a big difference between discrete space and a continuous continuum.</font>  <font color="gray">We live in one space, and machines exist in another. ‚Äù</font>  <font color="gray">- <i>God-emperor Dunes</i></font> </blockquote><br>  Having analyzed the basic ideas of skip-gram and negative sampling, we can move on to a closer look at the word2vec learning process. <br><br>  First, we pre-process the text on which we are training the model.  We define the size of the dictionary (we will call it <code>vocab_size</code> ), say, 10,000 attachments and the parameters of words in the dictionary. <br><br>  At the beginning of the training we create two matrices: <code>Embedding</code> and <code>Context</code> .  Attachments for each word in our dictionary are stored in these matrices (therefore, <code>vocab_size</code> is one of their parameters).  The second parameter is the dimension of the attachment (usually <code>embedding_size</code> set to 300, but earlier we considered an example with 50 dimensions). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1b7/8f8/018/1b78f8018d20fd36d0c5aef37d87a249.png"></div><br><br>  First, we initialize these matrices with random values.  Then we begin the learning process.  At each stage, we take one positive example and the associated negative ones.  Here is our first group: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2d2/261/806/2d22618069aa9e3f8a820cb431c6c014.png"></div><br><br>  We now have four words: the input word <code>not</code> and the output / context words <code>thou</code> (actual neighbor), <code>aaron</code> and <code>taco</code> (negative examples).  We start searching for their attachments in the <code>Embedding</code> matrices (for the input word) and <code>Context</code> (for context words), although both matrices contain attachments for all words from our dictionary. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/23e/ff0/691/23eff069128db956ce358ae758c0b8bb.png"></div><br><br>  Then we compute the scalar product of the input attachment with each of the context attachments.  In each case, a number is obtained that indicates the similarity of the input data and contextual attachments. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/615/319/e4a/615319e4accc7235c28fc8c76dca09f6.png"></div><br><br>  Now we need a way to turn these estimates into some kind of probabilities: they should all be positive numbers between 0 and 1. This is a great problem for <a href="https://jalammar.github.io/feedforward-neural-networks-visual-interactive/">sigmoid</a> logistic equations. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/98b/125/8b2/98b1258b21744917c993e617e0844ad8.png"></div><br><br>  The result of the sigmoid calculation can be considered as the issuance of the model for these samples.  As you can see, <code>taco</code> the highest score, while <code>aaron</code> still has the lowest score both before and after the sigmoid. <br><br>  When the untrained model made a prediction and having a real target label for comparison, let's calculate how many errors in the model prediction.  To do this, simply subtract the sigmoid estimate from the target labels. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d49/458/333/d49458333c28225c596014df3c6fcedb.png"></div><br>  <i><font color="gray"><code>error</code> = <code>target</code> - <code>sigmoid_scores</code></font></i> <br><br>  Here comes the phase of "learning" from the term "machine learning".  Now we can use this error estimate to adjust the <code>not</code> , <code>thou</code> , <code>aaron</code> and <code>taco</code> attachments so that the next calculation would result in a result closer to the target estimates. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a5d/e51/cd3/a5de51cd3a86a1ed0784a709cb979bdc.png"></div><br><br>  This completes one stage of training.  We have slightly improved the embedding of several words ( <code>not</code> , <code>thou</code> , <code>aaron</code> and <code>taco</code> ).  Now we go to the next stage (the next positive sample and the negative ones associated with it) and repeat the process. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/637/2ab/c3a/6372abc3a3f6b623d5b2cbab02953030.png"></div><br><br>  Attachments continue to improve as we cycle through the entire data set several times.  You can then stop the process, put off the <code>Context</code> matrix, and use the trained <code>Embeddings</code> matrix for the next task. <br><br><h1>  Window size and number of negative samples </h1><br>  In the process of learning word2vec, two key hyperparameters are the window size and the number of negative samples. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9fe/437/447/9fe4374479f547c1b324c7471cd61cbd.png"></div><br><br>  Different window sizes are suitable for different tasks.  <a href="https://youtu.be/tAxrlAVw-Tk%3Ft%3D648">It is noticed</a> that smaller windows (2‚àí15) generate <i>interchangeable</i> attachments with similar indices (note that antonyms are often interchangeable when looking at the surrounding words: for example, the words ‚Äúgood‚Äù and ‚Äúbad‚Äù are often mentioned in similar contexts).  Larger window sizes (15‚Äì50 or even more) generate <i>related</i> investments with similar indices.  In practice, you often have to provide <a href="https://youtu.be/ao52o9l6KGw%3Ft%3D287">annotations</a> for the sake of useful semantic similarity in your task.  In Gensim, the default window size is 5 (two words left and right, in addition to the input word itself). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4f4/b0a/45a/4f4b0a45a8552d6c19c7c9459302ac48.png"></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The number of negative samples is another factor in the learning process. </font><font style="vertical-align: inherit;">The original document recommends 5‚Äì20. </font><font style="vertical-align: inherit;">It also states that 2‚Äì5 samples seem sufficient when you have a large enough data set. </font><font style="vertical-align: inherit;">In Gensim, the default is 5 negative samples.</font></font><br><br><h1>  Conclusion </h1><br><blockquote> <font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúIf your behavior falls behind your measurements, then you are a living person, not an automaton‚Äù - </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">God-emperor of Dune</font></font></i></font> </blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I hope you now understand the attachments of words and the essence of the word2vec algorithm. </font><font style="vertical-align: inherit;">I also hope that now you will understand the articles that mention the concept of ‚Äúnegative sampling gram‚Äù (SGNS), as in the aforementioned recommendation systems.</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> References and additional literature </font></font></h1><br><ul><li> <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúDistributed representations of words and phrases and their composition‚Äù</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> [pdf]</font></font></li><li> <a href="https://arxiv.org/pdf/1301.3781.pdf">¬´      ¬ª</a> [pdf] </li><li> <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">¬´   ¬ª</a> [pdf] </li><li> <a href="https://web.stanford.edu/~jurafsky/slp3/">¬´   ¬ª</a>      ‚Äî    NLP. Word2vec    . </li><li> <a href="https://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies/dp/1627052984">¬´      ¬ª</a> by <a href="https://twitter.com/yoavgo"> </a> ‚Äî      . </li><li> <a href="http://mccormickml.com/"> </a>        Word2vec.        <a href="https://www.preview.nearist.ai/paid-ebook-and-tutorial">¬´ word2vec¬ª</a> </li><li>   ?   : <ul><li> <a href="https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py"> word2vec  Python</a>  Gensim </li><li>  <a href="">   C</a> ,    <a href="">       </a> </li></ul></li><li> <a href="http://sro.sussex.ac.uk/id/eprint/61062/1/Batchkarov,%2520Miroslav%2520Manov.pdf">    </a> </li><li> <a href="http://ruder.io/word-embeddings-1/index.html">¬´  ¬ª</a> , <a href="http://ruder.io/word-embeddings-softmax/"> 2</a> </li><li> <a href="https://www.amazon.com/Dune-Frank-Herbert/dp/0441172717/">¬´¬ª</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/446530/">https://habr.com/ru/post/446530/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../446512/index.html">.NET Core Workers as Windows Services</a></li>
<li><a href="../446514/index.html">Gmail is 15 years old</a></li>
<li><a href="../446516/index.html">Visualization of the revival time of Roshan</a></li>
<li><a href="../446520/index.html">How it all began: the story of flying drones</a></li>
<li><a href="../446522/index.html">Swift 5.1 - what's new?</a></li>
<li><a href="../446532/index.html">Upwork introduces a fee for the right to write to a potential customer.</a></li>
<li><a href="../446534/index.html">Visual Studio 2019 released</a></li>
<li><a href="../446536/index.html">Queues and JMeter: sharing with Publisher and Subscriber</a></li>
<li><a href="../446538/index.html">PhotoGuru went to the "dark side" and "wiser"</a></li>
<li><a href="../446546/index.html">Microsoft expands Azure IP Advantage with new IP benefits for Azure IoT innovators and startups</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>