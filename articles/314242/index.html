<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Deep Learning for Newbies: Recognize Handwritten Numbers</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Introducing the first article in the series, conceived to help quickly understand the technology of deep learning ; we will move from basic principles...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Deep Learning for Newbies: Recognize Handwritten Numbers</h1><div class="post__text post__text-html js-mediator-article"><p>  Introducing the first article in the series, conceived to help quickly understand the technology of <i>deep learning</i> ;  we will move from basic principles to non-trivial features in order to get decent performance on two data sets: MNIST (handwritten classification) and CIFAR-10 (classification of small images into ten classes: airplane, car, bird, cat, deer, dog, frog , horse, ship and truck). </p><br><div style="text-align:center;"><img src="https://habrastorage.org/files/5b0/df1/74d/5b0df174d0b5431488f3a8bd2d49320b.png"></div><br><a name="habracut"></a><br><div style="text-align:center;"><img src="https://habrastorage.org/files/c5f/8c5/0f5/c5f8c50f5daa4d5db824713e9f74f445.png"></div><br><p>  Intensive development of machine learning technologies has led to the emergence of several very convenient frameworks that allow us to quickly design and build prototypes of our models, as well as provide unlimited access to the data sets used to test learning algorithms (such as those mentioned above).  The development environment we will use here is called <b>Keras</b> ;  I found it most convenient and intuitive, but at the same time possessing expressive possibilities, sufficient to make changes to the model if necessary. </p><br><p>  At the end of this lesson, you will understand the principle of how a simple deep learning model, called a ‚Äúmultilayer perceptron‚Äù (MLP), will work, and also learn how to build it in Keras, getting a decent degree of accuracy on MNIST.  In the next lesson, we will analyze methods for solving more complex problems of image classification (such as CIFAR-10). </p><br><h2>  (Artificial) Neurons </h2><br><p>  Although the term ‚Äúdeep learning‚Äù can be understood in a wider sense, in most cases it is used in the field of <b>(artificial) neural networks</b> .  The idea of ‚Äã‚Äãthese constructions is borrowed from biology: neural networks mimic the process of processing of images perceived from the environment by the neurons of the brain and the participation of these neurons in decision making.  The principle of operation of a single artificial neuron is essentially very simple: it calculates a weighted sum of all elements of the <i>input vector</i> <img src="https://tex.s2cms.ru/svg/%5Cinline%5Cvec%7B_w_%7D" alt="\ inline \ vec {_x_}">  using <i>the weights vector</i> <img src="https://tex.s2cms.ru/svg/%5Cinline%5Cvec%7B_x_%7D" alt="\ inline \ vec {_w_}">  (as well as the additive <i>component of the displacement</i> <img src="https://tex.s2cms.ru/svg/%7B_w__0%7D" alt="{_w__0}">  ), and then <i>the activation function</i> <i>œÉ</i> can be applied to the result. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/files/885/b2d/770/885b2d77052342789555c7915837a0f8.jpeg"></div><br><p>  Among the most popular activation features: </p><br><ul><li>  The identity function (Identity): <i>œÉ</i> ( <i>z</i> ) = <i>z</i> ; </li><li>  Sigmoidal function, namely, logistic function (Logistic): <img src="https://tex.s2cms.ru/svg/%5Csigma(z)%3D%7B1%5Cover(1%20%2B%20exp(-z))" alt="\ sigma (z) = {1 \ over (1 + exp (-z))">  and hyperbolic tangent (Tanh): <img src="https://tex.s2cms.ru/svg/%5Csigma(z)%3Dtanh(z)" alt="\ sigma (z) = tanh (z)"></li><li>  Semilinear function (Rectified linear, ReLU) <img src="https://tex.s2cms.ru/svg/%5Csigma(z)%3Dmax(0%2C%20z)" alt="\ sigma (z) = max (0, z)"></li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/files/fcc/5d2/100/fcc5d2100b524a309b0ab6e895a0dfc9.png"></div><br><p>  Initially (from the 1950s), perceptron models were completely linear, that is, only identity served as an activation function.  But it soon became clear that the main tasks were often of a non-linear nature, which led to the appearance of other activation functions.  Sigmoidal functions (owing their name to the characteristic S-shaped graphics) well simulate the initial ‚Äúuncertainty‚Äù of a neuron regarding a binary solution when <i>z is</i> close to zero, combined with rapid saturation when <i>z is</i> shifted in any direction.  The two functions presented here are very similar, but the output values ‚Äã‚Äãof the hyperbolic tangent belong to the segment [-1, 1], and the range of values ‚Äã‚Äãof the logistic function is [0, 1] (thus, the logistic function is more convenient for representing the probabilities). </p><br><p>  In recent years, semilinear functions and their variations have become widespread in deep learning - they appeared as a simple way to make a model nonlinear (‚Äúif the value is negative, nullify it‚Äù), but in the end it turned out to be more successful than the historically more popular sigmoidal functions, moreover, they are more consistent with the way a biological neuron transmits an electrical impulse.  For this reason, in this lesson we will focus on semilinear functions (ReLU). </p><br><p>  Each neuron is uniquely determined by its weight vector. <img src="https://tex.s2cms.ru/svg/%5Cinline%5Cvec%7B_x_%7D" alt="\ inline \ vec {_w_}">  and the main goal of the <i>learning algorithm</i> is to assign a set of weights to a neuron on the basis of a <i>training sample of</i> known pairs of input and output data in order to minimize the prediction error.  A typical example of such an algorithm is the <i>gradient descent</i> method, which for a specific loss function <img src="https://tex.s2cms.ru/svg/E(%5Cvec%7Bw%7D)" alt="E (\ vec {w})">  changes the weight vector in the direction of the greatest decrease of this function: </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%5Cvec%7Bw%7D%5Cleftarrow%5Cvec%7Bw%7D-%5Ceta%20%7B%5Cpartial%20E(%5Cvec%7Bw%7D)%20%5Cover%20%5Cpartial%20%5Cvec%7Bw%7D%7D%20" alt="\ vec {w} \ leftarrow \ vec {w} - \ eta {\ partial E (\ vec {w}) \ over \ partial \ vec {w}}"></div><p></p><br><p>  where <i>Œ∑</i> is a positive parameter, called the <i>learning rate</i> . </p><br><p> The loss function reflects our understanding of how imprecise a neuron is in making decisions for the current value of the parameters.  The simplest choice of the loss function, which is good for most problems, is a <i>quadratic function</i> ;  for a given training sample <img src="https://tex.s2cms.ru/svg/(%5Cvec%7Bx%7D%2C%20y)" alt="(\ vec {x}, y)">  it is defined as the square of the difference between the target value <i>y</i> and the actual output value of the neuron for a given input <img src="https://tex.s2cms.ru/svg/%5Cvec%7Bx%7D" alt="\ vec {x}">  : </p><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/E(%5Cvec%7Bw%7D)%20%3D%20(y%20-%20%5Csigma(w_0%20%2B%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20w_ix_i))%5E2" alt="E (\ vec {w}) = (y - \ sigma (w_0 + \ sum_ {i = 1} ^ {n} w_ix_i)) ^ 2"></div><p></p><br><p>  The network has a large number of training courses that look at gradient descent algorithms in more depth.  In our case, the framework will take care of all the optimization for us, so I will not pay much attention to it in the future. </p><br><h2>  Introduction to Neural Networks (and Deep Learning) </h2><br><p>  Now that we have introduced the concept of a neuron, it becomes possible to connect the output of one neuron with the input of another, thus marking the beginning of a <b>neural network</b> .  In general, we will focus our attention on <i>direct propagation</i> neural networks in which neurons form layers in such a way that the neurons of one layer process the output of the previous layer.  In the most powerful of these architectures ( <i>multilayer perceptrons</i> , MLP), all the output of one layer is connected to all the neurons of the next layer, as in the diagram below. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/files/c4a/733/236/c4a733236d5e4ef7bc3486c03a94627c.png"></div><br><p>  To change the weights of the output neurons, the gradient descent method described above with a given loss function can be directly used; for other neurons, these losses must be propagated in the opposite direction (using the differentiation rule of a complex function), thus initiating the <i>backpropagation algorithm</i> .  Just as with the gradient descent method, I will not pay attention to the mathematical substantiation of the algorithm, since all the calculations are performed by our framework. </p><br><p>  According to the universal approximation theorem Tsybenko, a sufficiently wide multilayer perceptron with one hidden layer of sigmoidal neurons can approximate any continuous function of real variables on a given interval.  The proof of this theorem has no practical application and does not offer an effective training algorithm for such structures.  The answer gives deep learning: instead of <i>width,</i> increase <i>depth</i> ;  By definition, any neural network with more than one hidden layer is considered deep. </p><br><p>  Displacement also allows us to input <i>raw input to the</i> neural network: in the past, single-layer networks were fed with <i>key features</i> ( <i>features</i> ) that were extracted from the input data using special functions.  This meant that different classes of tasks, such as <i>computer vision</i> , <i>speech recognition,</i> or the <i>processing of natural languages</i> , required different approaches, which hampered scientific collaboration between these areas.  But when the network contains several hidden layers, it acquires the ability to learn to select the key features that best describe the input data, thus finding end-to-end learning (without the traditional programmable processing between input and output), as well as allowing you to use one the same network for a <i>wide range of tasks</i> , since it is no longer necessary to derive functions for obtaining key features.  I will give a graphic confirmation of the above in the second part of the lecture, when we consider convolutional neural networks. </p><br><h2>  Apply Deep MLP to MNIST </h2><br><p>  Now we will implement the simplest possible deep neural network - MLP with two hidden layers - and apply it to the problem of recognizing handwritten numbers from the MNIST data set. </p><br><p>  Only the following imports are required: </p><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.datasets <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mnist <span class="hljs-comment"><span class="hljs-comment"># subroutines for fetching the MNIST dataset from keras.models import Model # basic class for specifying and training a neural network from keras.layers import Input, Dense # the two types of neural network layer we will be using from keras.utils import np_utils # utilities for one-hot encoding of ground truth values</span></span></code> </pre> <br><p>  Then we define some parameters of our model.  These parameters are often referred to as <i>hyperparameters</i> , since they are expected to be refined even before the start of training.  In this guide, we take the pre-selected values, the process of clarifying them will pay more attention in subsequent lessons. </p><br><p>  In particular, we define: <br>  <strong>batch_size</strong> - the number of training samples processed simultaneously in one iteration of the gradient descent algorithm; <br>  <strong>num_epochs</strong> - the number of iterations of the learning algorithm over the entire training set; <br>  <strong>hidden_size</strong> is the number of neurons in each of the two hidden MLP layers. </p><br><pre> <code class="python hljs">batch_size = <span class="hljs-number"><span class="hljs-number">128</span></span> <span class="hljs-comment"><span class="hljs-comment"># in each iteration, we consider 128 training examples at once num_epochs = 20 # we iterate twenty times over the entire training set hidden_size = 512 # there will be 512 neurons in both hidden layers</span></span></code> </pre> <br><p>  It's time to download MNIST and do some preliminary processing.  With Keras, this is done very simply: it simply reads data from a remote server directly into the NumPy library arrays. </p><br><p>  To prepare the data, we first present the images as one-dimensional arrays (since we consider each pixel as a separate input feature), and then divide the intensity value of each pixel by 255 so that the new value falls into the [0, 1] segment.  This is a very simple way to normalize the data, we will discuss other ways in subsequent lessons. </p><br><p>  A good approach to the classification problem is a <i>probabilistic classification</i> , in which we have one output neuron for each class, which gives the probability that the input element belongs to this class.  This implies the need to convert the training output into direct encoding: for example, if the desired output class is 3 and there are five classes in total (and they are numbered from 0 to 4), then suitable direct encoding is [0, 0, 0, 1, 0] .  Again, Keras offers us all this functionality out of the box. </p><br><pre> <code class="python hljs">num_train = <span class="hljs-number"><span class="hljs-number">60000</span></span> <span class="hljs-comment"><span class="hljs-comment"># there are 60000 training examples in MNIST num_test = 10000 # there are 10000 test examples in MNIST height, width, depth = 28, 28, 1 # MNIST images are 28x28 and greyscale num_classes = 10 # there are 10 classes (1 per digit) (X_train, y_train), (X_test, y_test) = mnist.load_data() # fetch MNIST data X_train = X_train.reshape(num_train, height * width) # Flatten data to 1D X_test = X_test.reshape(num_test, height * width) # Flatten data to 1D X_train = X_train.astype('float32') X_test = X_test.astype('float32') X_train /= 255 # Normalise data to [0, 1] range X_test /= 255 # Normalise data to [0, 1] range Y_train = np_utils.to_categorical(y_train, num_classes) # One-hot encode the labels Y_test = np_utils.to_categorical(y_test, num_classes) # One-hot encode the labels</span></span></code> </pre> <br><p>  And now the time has come to determine our model!  To do this, we will use a stack of three Dense layers, which corresponds to a fully connected MLP, where all the outputs of one layer are connected to all the inputs of the next.  We will use ReLU for the neurons of the first two layers, and softmax for the last layer.  This activation function is designed to transform any vector with real values ‚Äã‚Äãinto a probability vector and is defined for the <i>jth</i> neuron as follows: </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%5Csigma(%5Cvec%7Bz%7D)_j%20%3D%20%7Bexp(z_j)%20%5Cover%20%5Csum_iexp(z_i)%7D" alt="\ sigma (\ vec {z}) _ j = {exp (z_j) \ over \ sum_iexp (z_i)}"></div><p></p><br><p>  Keras's remarkable feature that distinguishes it from other frameworks (for example, from TansorFlow) is the automatic calculation of layer sizes;  we only need to specify the dimension of the input layer, and Keras automatically initializes all other layers.  When all layers are defined, we just need to set the input and output data, as is done below. </p><br><pre> <code class="python hljs">inp = Input(shape=(height * width,)) <span class="hljs-comment"><span class="hljs-comment"># Our input is a 1D vector of size 784 hidden_1 = Dense(hidden_size, activation='relu')(inp) # First hidden ReLU layer hidden_2 = Dense(hidden_size, activation='relu')(hidden_1) # Second hidden ReLU layer out = Dense(num_classes, activation='softmax')(hidden_2) # Output softmax layer model = Model(input=inp, output=out) # To define a model, just specify its input and output layers</span></span></code> </pre> <br><p>  Now we just need to determine the loss function, the optimization algorithm and the metrics that we will collect. </p><br><p>  When dealing with a probabilistic classification, it‚Äôs best to use as a loss function the quadratic error not defined above, but the cross entropy.  For a certain output probability vector <img src="https://tex.s2cms.ru/svg/%5Cvec%7By%7D" alt="\ vec {y}">  compared to the actual vector <img src="https://tex.s2cms.ru/svg/%5Cvec%5Chat%7By%7D" alt="\ vec \ hat {y}">  loss (for the <i>k-</i> th class) will be defined as <br></p>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%5Cmathcal%7BL%7D(%5Cvec%7By%7D%2C%20%5Cvec%5Chat%7By%7D)%20%3D%20-%5Csum_%7Bi%3D1%7D%5Ek%5Chat%7By%7D_i%5Clog%7By_i%7D" alt="\ mathcal {L} (\ vec {y}, \ vec \ hat {y}) = - \ sum_ {i = 1} ^ k \ hat {y} _i \ log {y_i}"></div><p></p><br><p>  Losses will be less for probabilistic tasks (for example, with a logistic / softmax function for the output layer), mainly due to the fact that this function is designed to maximize the confidence of the model in the correct class definition, and it does not care about the probability distribution of the sample to other classes (while the quadratic error function tends to ensure that the probability of hitting the other classes is as close to zero as possible). </p><br><p>  The optimization algorithm used will resemble some form of the gradient descent algorithm, the only difference being in how the <i>learning rate</i> <i>Œ∑</i> is chosen.  An excellent overview of these approaches is presented <a href="http://sebastianruder.com/optimizing-gradient-descent/index.html">here</a> , and now we will use Adam's optimizer, which usually shows good performance. </p><br><p>  Since our classes are balanced (the number of handwritten numbers belonging to each class is the same), <i>accuracy</i> will be a suitable metric - the proportion of input data assigned to the correct class. </p><br><pre> <code class="python hljs">model.compile(loss=<span class="hljs-string"><span class="hljs-string">'categorical_crossentropy'</span></span>, <span class="hljs-comment"><span class="hljs-comment"># using the cross-entropy loss function optimizer='adam', # using the Adam optimiser metrics=['accuracy']) # reporting the accuracy</span></span></code> </pre> <br><p>  Finally, we run the learning algorithm.  It is good practice to set aside some subset of data to verify that our algorithm (still) correctly recognizes the data ‚Äî this data is also called the <i>validation set</i> ;  here we separate 10% of the data for this purpose. </p><br><p>  Another nice feature of Keras is the granularity: it displays detailed logging of all steps of the algorithm. </p><br><pre> <code class="python hljs">model.fit(X_train, Y_train, <span class="hljs-comment"><span class="hljs-comment"># Train the model using the training set... batch_size=batch_size, nb_epoch=num_epochs, verbose=1, validation_split=0.1) # ...holding out 10% of the data for validation model.evaluate(X_test, Y_test, verbose=1) # Evaluate the trained model on the test set!</span></span></code> </pre> <br><pre> <code class="python hljs">Train on <span class="hljs-number"><span class="hljs-number">54000</span></span> samples, validate on <span class="hljs-number"><span class="hljs-number">6000</span></span> samples Epoch <span class="hljs-number"><span class="hljs-number">1</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">9</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.2295</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9325</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.1093</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9680</span></span> Epoch <span class="hljs-number"><span class="hljs-number">2</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">9</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0819</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9746</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0922</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9708</span></span> Epoch <span class="hljs-number"><span class="hljs-number">3</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">11</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0523</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9835</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0788</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9772</span></span> Epoch <span class="hljs-number"><span class="hljs-number">4</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">12</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0371</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9885</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0680</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9808</span></span> Epoch <span class="hljs-number"><span class="hljs-number">5</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">12</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0274</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9909</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0772</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9787</span></span> Epoch <span class="hljs-number"><span class="hljs-number">6</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">12</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0218</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9931</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0718</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9808</span></span> Epoch <span class="hljs-number"><span class="hljs-number">7</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">12</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0204</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9933</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0891</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9778</span></span> Epoch <span class="hljs-number"><span class="hljs-number">8</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">13</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0189</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9936</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0829</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9795</span></span> Epoch <span class="hljs-number"><span class="hljs-number">9</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">14</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0137</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9950</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0835</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9797</span></span> Epoch <span class="hljs-number"><span class="hljs-number">10</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">13</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0108</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9969</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0836</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9820</span></span> Epoch <span class="hljs-number"><span class="hljs-number">11</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">13</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0123</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9960</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0866</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9798</span></span> Epoch <span class="hljs-number"><span class="hljs-number">12</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">13</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0162</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9951</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0780</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9838</span></span> Epoch <span class="hljs-number"><span class="hljs-number">13</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">12</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0093</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9968</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.1019</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9813</span></span> Epoch <span class="hljs-number"><span class="hljs-number">14</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">12</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0075</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9976</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0923</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9818</span></span> Epoch <span class="hljs-number"><span class="hljs-number">15</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">12</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0118</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9965</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.1176</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9772</span></span> Epoch <span class="hljs-number"><span class="hljs-number">16</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">12</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0119</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9961</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0838</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9803</span></span> Epoch <span class="hljs-number"><span class="hljs-number">17</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">12</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0073</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9976</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0808</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9837</span></span> Epoch <span class="hljs-number"><span class="hljs-number">18</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">13</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0082</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9974</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0926</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9822</span></span> Epoch <span class="hljs-number"><span class="hljs-number">19</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">12</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0070</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9979</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0808</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9835</span></span> Epoch <span class="hljs-number"><span class="hljs-number">20</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">11</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0039</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9987</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.1010</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9822</span></span> <span class="hljs-number"><span class="hljs-number">10000</span></span>/<span class="hljs-number"><span class="hljs-number">10000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">1</span></span>s [<span class="hljs-number"><span class="hljs-number">0.099321320021623111</span></span>, <span class="hljs-number"><span class="hljs-number">0.9819</span></span>]</code> </pre> <br><p>  As you can see, our model achieves an accuracy of approximately 98.2% on a test dataset, which is quite worthy for such a simple model, despite the fact that it is far surpassed by the cutting-edge approaches described <a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html">here</a> . </p><br><p>  I urge you to experiment with this model: try various hyperparameters, optimization algorithms, activation functions, add hidden layers, etc.  In the end, you should be able to achieve accuracy above 99%. </p><br><h2>  Conclusion </h2><br><p>  In this post, we covered the basic concepts of deep learning, successfully implemented a simple two-layer deep MLP using the Keras framework, applied it to the MNIST data set ‚Äî all in less than 30 lines of code. </p><br><p>  Next time, we will look at convolutional neural networks (CNN), which solve some of the problems that arise when applying MLP to large-scale images (such as CIFAR-10). </p></div><p>Source: <a href="https://habr.com/ru/post/314242/">https://habr.com/ru/post/314242/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../314230/index.html">Corporate Laboratories - Program Update</a></li>
<li><a href="../314232/index.html">My experience with getting PMP</a></li>
<li><a href="../314234/index.html">5 amazing ways to manage time enjoyed by successful people</a></li>
<li><a href="../314236/index.html">Kivy. From creation to production one step. Part 1</a></li>
<li><a href="../314240/index.html">Short and simple about difficult - routing in "8-800"</a></li>
<li><a href="../314248/index.html">PayPal will not come to Ukraine in the next 12 months - NBU</a></li>
<li><a href="../314250/index.html">What makes games funny? Comedy and humor in video games. Part two</a></li>
<li><a href="../314252/index.html">ASP.NET Core: Deploying a web app in Azure App Service using Visual Studio</a></li>
<li><a href="../314254/index.html">Phone number verification using Ruby on rails and Twilio</a></li>
<li><a href="../314256/index.html">Out of range: anomalous communication areas</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>