<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Create a cloud for software testing</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="While companies like Google and Microsoft are actively telling a simple user about the happiness that awaits them on cloud services, I want to share t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Create a cloud for software testing</h1><div class="post__text post__text-html js-mediator-article">  While companies like Google and Microsoft are actively telling a simple user about the happiness that awaits them on cloud services, I want to share the other side of the clouds - happiness for software developers and testers.  In the few years during which I have led the Parallels Plesk Panel testing team, a good collection of live hacking has been compiled for using the cloud for our purposes. <br><br>  I am sure that this experience will be useful to the overwhelming majority of companies and startups.  First, <b>you can create a test cloud yourself</b> .  This is important when your budget is limited.  Secondly, the test cloud in its most initial version can be deployed on 2-3 servers.  Thirdly, all efforts associated with the creation of a test cloud are more than compensated by the automation of the testing process.  This is especially critical if you regularly update and if the code for your project is rather lengthy. <br><br>  With a preamble like everything.  Curious invite under the cat. <br><a name="habracut"></a><br><h4>  Why do we need a cloud? </h4><br>  The Parallels Plesk panel costs about 50% of all servers used for hosting in the world.  These are millions of hardware and VPS, on which hundreds of millions of client sites revolve.  The price of bugs in the software is huge, there are no trifles.  In this regard, Plesk is a very complex product for QA engineers.  For it we need to run about 2000 p0 and p1 regression autotests per day (of which about 700 tests are dedicated to the user interface) in 60 configurations.  In 24 hours, more than 120,000 auto-test launches are received.  In addition, at least once a week, more work is underway - tests are necessarily initiated to check upgrade / backup / restore / migration from the seven supported versions of Splash on dozens of configurations.  Once a month, QA engineers conduct performance, density and load testing.  Well, it is quite hot on the eve of release.  Then the need to verify integration with dozens of products is added to all the above operations. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Obviously, doing it all by hand, preparing the environment, installing products, uploading frameworks and tests, running on separate servers, collecting and analyzing logs and results is not even difficult - it is impossible in principle.  That is why we decided to feed these tasks to the cloud. <br><br><h4>  What will it be? </h4><br>  The ‚ÄúWishes‚Äù of the SpA QA-team (and indeed of the Parallels development center in Novosibirsk, where a number of other products are being created for service providers) were formed in 2008 and were packed into four laconic points: <br><ul><li>  Reliability and fault tolerance.  As further practice showed, cloud uptime for QA turned out to be almost the same important criterion as uptime for public cloud services. </li><li>  High performance.  The criteria here is the ability to carry out the full amount of testing for the required time, the speed of creating a virtual machine (read VPS). </li><li>  Scalable.  The ability to allocate the necessary amount of resources at any time, based on the complexity of the task.  For example, to solve a simple task, we will single out one machine and get the result in an hour, and to solve a complex task, we select 20 machines to also get the result in an hour.  Plus, extensibility.  A new fully configured server prepares no more than one hour. </li><li>  Support change requirements.  Having the ability to quickly adapt the cloud to changing requirements (changing types of virtualization, the ability to prioritize tasks between projects, etc.). </li></ul><br>  A year later, the "cloud", which allows you to perform basic tasks (deploying the machine, running autotests), was ready.  2-3 people worked on its creation.  Virtually all cloud software is self-written, with the exception of a pair of out-of-box products.  These are Munin (monitoring resource availability) and Nagios (monitoring resource utilization).  But TestLink (storage of verbal descriptions of test cases, the formation of the results of manual and automatic execution of test cases) had to be seriously shoveled.  The core and reporting system have been modified.  For example, the latter gave us an acceleration of generating a report by two orders: it was 5 minutes, it became 5 seconds. <br><br>  A literate reader will ask why we didn‚Äôt use the Amazon cloud and didn‚Äôt rivet instances in it for our goals and objectives.  I will answer thesis: <br><ul><li>  We have no shortage of hardware.  It was accumulated for many years, it was logical to load it in full. </li><li>  Even now, external clouds do not fully meet our requirements, among which there are many specific configurations and types of virtualization. </li><li>  Expensive.  The daily volume of resources consumed in carrying out our tasks in terms of banknotes will be simply astronomical. </li></ul><br><h4>  Moment of truth: how has the cloud changed the lives of testers? </h4><br>  Before the advent of the cloud, we unfolded several configurations with handles, filled in fresh product builds, installed, filled in auto tests, ran them, periodically checked to ensure that they did not fall, collected performance results from all machines, analyzed.  It was long, difficult and very inefficient. <br><br>  How come?  The cloud saved the QA-team from routine tasks.  The tester has ceased to be ‚Äúa little admin‚Äù and is no longer engaged in self-rolling the OS, product builds and test plans for servers.  Now the robot is engaged in this, which can be commanded through the GUI or API interfaces. <br><br>  Here's what it looks like: <br><br><img src="https://habrastorage.org/storage2/748/ada/a1e/748adaa1e18729b72dd11b80a0a0a178.png"><br><br>  Select the product and version (1), the name of the test plan (2), build (3), the list of configurations (4), press the 'Run' button (5) and rushed.  Deploying dozens and hundreds of machines, installing a product on them, launching auto tests takes place in just a few clicks. <br><br>  In the Scheduled tasks tab, you can add tasks for regular automated testing using our task manager, which allows you to add tasks taking into account the load on the cloud. <br><br><img src="https://habrastorage.org/storage2/b56/92e/bfd/b5692ebfdc194a025b4065479b2e86f4.png"><br><br>  And in the tab Mass execution there is an opportunity to launch groups of test plans.  A couple of clicks - and you run half a million autotests.  You can get enough of their own greatness. <br><br><h4>  Under the hood of the test cloud </h4><br>  This tricky moving picture illustrates the work of our cloud and gives an idea of ‚Äã‚Äãits architecture: <br><br><img src="https://habrastorage.org/storage2/8a8/84e/62b/8a884e62b13f922fce8ee295903bb10b.gif"><br><br>  Let's see how the gears are spinning inside using the example of continuous integration scheme. <br><ol><li>  The initiator of the process start are the developers.  After they have built another feature, they commit to the version control system. </li><li>  After that, the build process starts from the source. </li><li>  As soon as a build is made for one of the configurations, a request is immediately sent to Test executor to launch Build Verification Test (BVT) - a test plan containing 10‚Äì20 autotests, checking the main functionality of the product and ensuring that there are no blocking problems. </li><li>  Test executor sends a request to the Deployment server to create the appropriate environment. </li><li>  Deployment server selects a pre-prepared image of the operating system on OS dump storage ... </li><li>  ... and expands it on the most suitable server or group of servers that are selected by the Load balancer according to certain rules (we will look at some of them later).  After this, the assembled build is installed on all the created machines. </li><li>  As soon as the Deployment server prepares all the necessary environments, management returns to the Test executor, which runs the BVT test plan on the prepared machines, distributing the autotests to them. </li><li>  Depending on the nature of autotests, servers with external services can be used: for example, Selenium, external mail or databases. </li><li>  If the test plan was executed with errors, then the developer who made the unsuccessful commit receives a notification that everything is gone and you need to quickly repair it.  If the test plan was executed without errors, then the builds of the built builds are shifted to a special Product builds storage server, from where builds become available for manual and full-scale automated testing. </li></ol><br>  There are a few more nodes on the diagram that I did not mention.  Inside the cloud is the Tests storage, on which test frameworks and autotests are stored, and the Test specification system, on which the TestLink is located.  Outside, there are the Tools management server, which provides interfaces for working with the cloud (for example, the launch form of test plans mentioned above), and the Infrastructure monitoring server that allows you to control the cloud, quickly detect problems in it, find bottlenecks in the cloud, etc. d. <br><br>  Of course, the cloud in this form did not appear immediately.  It evolved (and continues to change) in accordance with the objectives and our ideas about its effective use. <br><br><h4>  Lifehacks and improvements </h4><br>  One of the most significant life hacking of our test cloud is the reporting system.  Its advantage over conventional TestLink is visibility.  TestLink is only good when tests pass without errors.  Otherwise, TestLink is practically useless.  Take a look at the screenshot below. <br><br><img src="https://habrastorage.org/storage2/f22/d6d/97d/f22d6d97dca93b1c63ff887a28fad7f9.png"><br><br>  80 drops are 80 bugs in a product?  Are these 80 problems in the tests?  Is this one bug in the product that causes 80 tests to fail?  What is the condition of the product?  What exactly broke?  Where to see the logs?  Many questions and few answers. <br><br>  Now the results are reported to the LogTracker system - our internal development. <br>  The screen shows one of the pages of this system, containing information about the results of the execution of one of the testplans on a particular build. <br><br><img src="https://habrastorage.org/storage2/9c5/947/a5a/9c5947a5a7bfc6df52dba986da4f678d.png"><br><br>  In the upper left part, statistics on each of the configurations, the number of successfully and unsuccessfully passed autotests, the number of known errors are shown.  Clicking on the configuration, you can get information about each of the autotests, logs of different levels of detail, screenshots, linking to known bugs in the system of bug tracking and much more. <br>  At the top right, a list of autotests is displayed, sorted by the number of crashes in different configurations.  In conditions of limited resources, this allows engineers to focus on ‚Äúcorrect‚Äù problems (a more likely bug in a product or a problem in an outdated autotest caused by changes in the product). <br>  Information about known errors, the number of crashes and linking to entities in our system of bug tracking is displayed at the bottom left (there are also bugs in the product and bugs in the tests). <br>  The bottom right shows general statistics for this test plan and build, including information about unknown errors and errors cured by "quarantine".  We call quarantine the automatic restart of dropped tests in a completely fresh environment.  This allows us to weed out "false" crashes of autotests, caused by instability of the network or problems with the work of selenium-servers under heavy loads. <br><br>  Please note: the <b>entire array of data with which the QA-engineer will work is collected and generated automatically</b> . <br><br><h4>  Conclusion </h4><br>  If you get to the conclusion, you can be congratulated.  You are really interested in the topic of using cloud in QA and you may well want to build your own test cloud - at least on two or three servers.  Here are my recommendations to those who will do it for the first time: <br><ol><li>  Be clear on your cloud requirements.  Understand what tasks the cloud should help decide what criteria of performance and stability it should meet, etc. </li><li>  Decide on the necessary resources, based on the requirements for performance and stability: what should be the hardware and network device, what software, virtualization tools, and what human resources you need </li><li>  Develop a system of load distribution between servers and determine the criteria by which the choice of a suitable server will occur.  To be honest, I did not consciously include the chapter on load balancer in order not to completely scare readers by the size of the publication.  If it is interesting to read about the balancer - write in the comments, I will make a separate post </li><li>  Create a cloud monitoring system that allows you to quickly find problems and bottlenecks in the cloud </li><li>  Create a cloud management system (initially it may be just an API, and then you will need a GUI that allows you to use the cloud faster and require less technical knowledge) </li><li>  Choose a test management system and, if necessary, change it - as we did with TestLink. </li><li>  Decide on a test framework.  His choice will largely depend on the technology on which your product is based, and the programming languages ‚Äã‚Äãin which it is written. </li></ol><br>  After all these actions, you and yourself will understand exactly what direction to develop your personal cloud. </div><p>Source: <a href="https://habr.com/ru/post/143891/">https://habr.com/ru/post/143891/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../143885/index.html">Sensors are coming</a></li>
<li><a href="../143886/index.html">Automaton-style programs - translation difficulties</a></li>
<li><a href="../143887/index.html">Infinite scrolling, as a dubious interface improvement</a></li>
<li><a href="../143888/index.html">Recover dead pixels by freezing</a></li>
<li><a href="../143890/index.html">Scala Conference in St. Petersburg in 4 days</a></li>
<li><a href="../143892/index.html">Gamification diet</a></li>
<li><a href="../143893/index.html">Using a synthesizer as a computer keyboard</a></li>
<li><a href="../143894/index.html">What applications are you testing?</a></li>
<li><a href="../143895/index.html">The average identifier length in popular JavaScript libraries is 8.27 characters.</a></li>
<li><a href="../143896/index.html">We are waiting for everyone on the bright side of power!</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>