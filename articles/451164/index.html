<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Looking for a free parking space with Python</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I live in a good city. But, as in many others, finding a parking space always turns into a test. Empty places are quickly occupied, and even if you ha...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Looking for a free parking space with Python</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/vz/x5/od/vzx5odyqel0ow-z2qolfdo1htd4.gif" alt="image"><br><br>  I live in a good city.  But, as in many others, finding a parking space always turns into a test.  Empty places are quickly occupied, and even if you have your own, it will be difficult for friends to drive in because they will have no place to park. <br><br>  Therefore, I decided to send the camera to the window and use deep learning so that my computer would notify me when the space becomes available: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/webt/lx/md/gy/lxmdgyxkvnwtwc5nsqccy83mp34.gif" alt="image"><br><br>  It may sound hard, but actually writing a working prototype with deep learning is quick and easy.  All the necessary components are already there - you just need to know where to find them and how to put them together. <br><br>  So let's have a little fun and write the exact free parking notification system using Python and deep learning. <a name="habracut"></a><br><br><h3>  Decomposing the task </h3><br>  When we have a difficult task that we want to solve with the help of machine learning, the first step is to break it down into a sequence of simple tasks.  Then we can use different tools to solve each of them.  Combining a few simple solutions together, we get a system that is capable of something complex. <br><br>  Here is how I broke my task: <br><br><img src="https://habrastorage.org/webt/q7/gi/hi/q7gihifth7-k9mad7fhgbj4itcc.jpeg" alt="image"><br><br>  The video stream from a webcam is sent to the window at the entrance of the conveyor: <br><br><img src="https://habrastorage.org/webt/aa/wk/ig/aawkigsexhbk5s4slqmvksvofcm.gif" alt="image"><br><br>  Through the pipeline, we will transmit each frame of video, one at a time. <br><br>  The first step is to recognize all possible parking spaces on the frame.  Obviously, before we can search for unallocated places, we need to understand which parts of the image there is parking. <br><br>  Then on each frame you need to find all the cars.  This will allow us to track the movement of each machine from frame to frame. <br><br>  The third step is to determine which places are occupied by cars and which are not.  To do this, combine the results of the first two steps. <br><br>  Finally, the program should send an alert when the parking space becomes available.  This will be determined by changes in the location of the machines between video frames. <br><br>  Each of these stages can be completed in different ways using different technologies.  There is no only right or wrong way to make this pipeline, different approaches will have their advantages and disadvantages.  Let's deal with each step in more detail. <br><br><h3>  We recognize parking spaces </h3><br>  This is what our camera sees: <br><br><img src="https://habrastorage.org/webt/2u/zl/xt/2uzlxtgxbn6jvfkhfy0e523ow88.png" alt="image"><br><br>  We need to somehow scan this image and get a list of places to park: <br><br><img src="https://habrastorage.org/webt/m-/bq/xb/m-bqxb9ybcjc44blvsuzhnw6xyk.png" alt="image"><br><br>  The solution ‚Äúin the forehead‚Äù would be to simply hard-code the locations of all parking spaces manually instead of automatic recognition.  But in this case, if we move the camera or want to search for parking spaces on another street, we will have to do the whole procedure again.  It sounds so-so, so let's look for an automatic way to recognize parking spaces. <br><br>  Alternatively, you can search for parking meters on the image and assume that there is a parking space next to each of them: <br><br><img src="https://habrastorage.org/webt/qi/g8/cj/qig8cjwmp7dmduejcddjk6tnoiw.png" alt="image"><br><br>  However, this approach is not so smooth.  Firstly, not every parking space has a parking meter, and indeed, we are more interested in finding parking spaces for which there is no need to pay.  Secondly, the location of the parking meter tells us nothing about where the parking space is, but only allows us to make a guess. <br><br>  Another idea is to create an object recognition model that looks for parking space labels drawn on the road: <br><br><img src="https://habrastorage.org/webt/bo/vv/nu/bovvnu6rsl-zimlr1gtpp1a_egm.png" alt="image"><br><br>  But this approach is also so-so.  Firstly, in my city all such marks are very small and difficult to see at a distance, so it will be difficult to detect them using a computer.  Secondly, the street is full of all sorts of other lines and tags.  It will be difficult to separate the parking tags from the strip separators and pedestrian crossings. <br><br>  When you encounter a problem that at first glance seems difficult, take a few minutes to find another approach to solving a problem that will help circumvent some technical problems.  What generally is a parking space?  This is just a place for which a car is parked for a long time.  Perhaps we do not need to recognize parking spaces at all.  Why don't we just recognize cars that have been standing for a long time and not assume that they are standing in a parking space? <br><br>  In other words, parking spaces are located where cars are standing for a long time: <br><br><img src="https://habrastorage.org/webt/b8/tb/ua/b8tbuafyf4uci3jy61jnjlwanqa.png" alt="image"><br><br>  Thus, if we can recognize the cars and find out which of them do not move between frames, we will be able to guess where the parking spaces are.  Simply simple - go to the recognition of machines! <br><br><h3>  We recognize cars </h3><br>  Recognizing machines in a video frame is a classic object recognition task.  There are many machine learning-based approaches that we could use for recognition.  Here are some of them in order from the ‚Äúold school‚Äù to the ‚Äúnew school‚Äù: <br><br><ul><li>  You can train a HOG-based detector (Histogram of Oriented Gradients, directional gradient histograms) and walk them through the whole image to find all the machines.  This old approach, which does not use deep learning, works relatively quickly, but does not do very well with machines located differently. </li><li>  You can train a detector based on CNN (Convolutional Neural Network, a convolutional neural network) and walk it through the entire image until we find all the machines.  This approach works exactly, but not as efficiently, since we need to scan the image several times using CNN to find all the machines.  And although this way we can find machines located differently, we will need much more training data than for the HOG detector. </li><li>  You can use a deep learning approach like Mask R-CNN, Faster R-CNN or YOLO, which combines the accuracy of CNN and a set of technical tricks that greatly increase recognition speed.  Such models will work relatively quickly (on a GPU) if we have a lot of data to train the model. </li></ul><br>  In general, we need the simplest solution that will work as it should and will require the least amount of training data.  It is not necessary that this is the newest and fastest algorithm.  However, specifically in our case, the Mask R-CNN is a reasonable choice, despite the fact that it is quite new and fast. <br><br>  The Mask R-CNN architecture is designed in such a way that it recognizes objects in the entire image, effectively wasting resources, and does not use the sliding window approach.  In other words, it works pretty fast.  With modern GPU, we will be able to recognize objects in high-definition video at a speed of several frames per second.  For our project this should be enough. <br><br>  In addition, the Mask R-CNN gives a lot of information about each recognized object.  Most recognition algorithms return only the bounding box for each object.  However, the Mask R-CNN will not only give us the location of each object, but also its outline (mask): <br><br><img src="https://habrastorage.org/webt/n2/b0/hp/n2b0hpwgwpkn6ahfhqetvbhq1rg.png" alt="image"><br><br>  To learn the Mask R-CNN, we need a lot of images of objects that we want to recognize.  We could go outside, take a picture of the cars and label them in the photos, which would require several days of work.  Fortunately, cars are one of those objects that people often want to recognize, so there are already several publicly available datasets with images of cars. <br><br>  One of them is the popular DOCET <a href="http://cocodataset.org/">COCO</a> (short for Common Objects In Context), which has images annotated with masks of objects.  In this dataset there are more than 12,000 images with already marked machines.  Here is an example of an image from dataset: <br><br><img src="https://habrastorage.org/webt/dv/lz/7l/dvlz7ltgwmudog9-b2f6i7tlmhe.jpeg" alt="image"><br><br>  Such data is great for training models based on Mask R-CNN. <br><br>  But hold the horses, there is even better news!  We are not the first who wanted to train their model with the help of COCO dataset - many people have already done it before us and shared their results.  Therefore, instead of teaching our model, we can take ready-made, which can already recognize the machine.  For our project we will use the <a href="https://github.com/matterport/Mask_RCNN">open-source model from Matterport.</a> <br><br>  If we give an image from the camera to the input of this model, here‚Äôs what we‚Äôll get out of the box: <br><br><img src="https://habrastorage.org/webt/vy/kq/50/vykq50pcxhyt_vkmfzmxk_fgl5g.png" alt="image"><br><br>  The model recognized not only cars, but also objects such as traffic lights and people.  It's funny that she recognized the tree as a houseplant. <br><br>  For each recognized object, the R-CNN Mask model returns 4 things: <br><br><ul><li>  The type of object detected (integer).  The pre-trained COCO model is able to recognize 80 different common objects such as cars and trucks.  A full list of them can be found <a href="https://gist.github.com/ageitgey/b143ee809bf08e4927dd59bace44db0d">here.</a> </li><li>  The degree of confidence in the recognition results.  The higher the number, the more confident the model is that the object is recognized correctly. </li><li>  The bounding box for an object in the form of XY-coordinates of pixels in the image. </li><li>  A ‚Äúmask‚Äù that shows which pixels within the bounding box are part of an object.  Using the mask data you can find the outline of the object. </li></ul><br>  Below is the Python code for detecting the bounding box for machines using the pre-trained model Mask R-CNN and OpenCV: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mrcnn.config <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mrcnn.utils <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mrcnn.model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MaskRCNN <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pathlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Path <span class="hljs-comment"><span class="hljs-comment"># ,     Mask-RCNN. class MaskRCNNConfig(mrcnn.config.Config): NAME = "coco_pretrained_model_config" IMAGES_PER_GPU = 1 GPU_COUNT = 1 NUM_CLASSES = 1 + 80 #   COCO  80  + 1  . DETECTION_MIN_CONFIDENCE = 0.6 #    ,    . def get_car_boxes(boxes, class_ids): car_boxes = [] for i, box in enumerate(boxes): #     ,   . if class_ids[i] in [3, 8, 6]: car_boxes.append(box) return np.array(car_boxes) #   . ROOT_DIR = Path(".") #       . MODEL_DIR = ROOT_DIR / "logs" #       . COCO_MODEL_PATH = ROOT_DIR / "mask_rcnn_coco.h5" #   COCO  . if not COCO_MODEL_PATH.exists(): mrcnn.utils.download_trained_weights(COCO_MODEL_PATH) #     . IMAGE_DIR = ROOT_DIR / "images" #      ‚Äî   0,    ,   . VIDEO_SOURCE = "test_images/parking.mp4" #   Mask-RCNN   . model = MaskRCNN(mode="inference", model_dir=MODEL_DIR, config=MaskRCNNConfig()) #   . model.load_weights(COCO_MODEL_PATH, by_name=True) #   . parked_car_boxes = None #  ,     . video_capture = cv2.VideoCapture(VIDEO_SOURCE) #      . while video_capture.isOpened(): success, frame = video_capture.read() if not success: break #      BGR ( OpenCV)  RGB. rgb_image = frame[:, :, ::-1] #    Mask R-CNN   . results = model.detect([rgb_image], verbose=0) # Mask R-CNN ,       . #     ,     . r = results[0] #  r    : # - r['rois'] ‚Äî      ; # - r['class_ids'] ‚Äî  () ; # - r['scores'] ‚Äî  ; # - r['masks'] ‚Äî   (    ). #      . car_boxes = get_car_boxes(r['rois'], r['class_ids']) print("Cars found in frame of video:") #     . for box in car_boxes: print("Car:", box) y1, x1, y2, x2 = box #  . cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 1) #    . cv2.imshow('Video', frame) #  'q',  . if cv2.waitKey(1) &amp; 0xFF == ord('q'): break #    . video_capture.release() cv2.destroyAllWindows()</span></span></code> </pre> <br>  After running this script, an image with a frame around each detected machine will appear on the screen: <br><br><img src="https://habrastorage.org/webt/_p/il/0r/_pil0reoz3gj7dtqboav_rgerl8.jpeg" alt="image"><br><br>  Also, the coordinates of each machine will be displayed in the console: <br><br><pre> <code class="python hljs">Cars found <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> frame of video: Car: [<span class="hljs-number"><span class="hljs-number">492</span></span> <span class="hljs-number"><span class="hljs-number">871</span></span> <span class="hljs-number"><span class="hljs-number">551</span></span> <span class="hljs-number"><span class="hljs-number">961</span></span>] Car: [<span class="hljs-number"><span class="hljs-number">450</span></span> <span class="hljs-number"><span class="hljs-number">819</span></span> <span class="hljs-number"><span class="hljs-number">509</span></span> <span class="hljs-number"><span class="hljs-number">913</span></span>] Car: [<span class="hljs-number"><span class="hljs-number">411</span></span> <span class="hljs-number"><span class="hljs-number">774</span></span> <span class="hljs-number"><span class="hljs-number">470</span></span> <span class="hljs-number"><span class="hljs-number">856</span></span>]</code> </pre><br>  So we learned to recognize the cars in the image. <br><br><h3>  We recognize empty parking spaces </h3><br>  We know the pixel coordinates of each machine.  Looking through several consecutive frames, we can easily determine which of the cars did not move, and assume that there are parking spaces.  But how to understand that the car left the parking? <br><br>  The problem is that the frames of the machines partially overlap: <br><br><img src="https://habrastorage.org/webt/7t/vi/4q/7tvi4q1rgvkfkaljrsp8sjathr0.jpeg" alt="image"><br><br>  Therefore, if we imagine that each frame represents a parking space, it may turn out that it is partially occupied by a car, when in fact it is empty.  We need to find a way to measure the degree of intersection of two objects in order to search for only the ‚Äúmost empty‚Äù frames. <br><br>  We will use a measure called Intersection Over Union (the ratio of the intersection area to the sum of the areas) or IoU.  IoU can be found by counting the number of pixels where two objects intersect, and divided by the number of pixels occupied by these objects: <br><br><img src="https://habrastorage.org/webt/zs/c0/sz/zsc0szsct8xjwkx5eo-6ieynfuc.png" alt="image"><br><br>  So we can understand how strongly the bounding frame of the car intersects with the frame of the parking space.  This makes it easy to determine if parking is available.  If the IoU value is low, like 0.15, then the car takes up a small part of the parking space.  And if it is high, like 0.6, then this means that the car takes up most of the space and you cannot park there. <br><br>  Since IoU is used quite often in computer vision, in the respective libraries there is a high probability that this measure is implemented.  In our Mask R-CNN library, it is implemented as a function mrcnn.utils.compute_overlaps (). <br><br>  If we have a list of bounding frames for parking spaces, then we can add a check for the presence of cars in this framework by adding a whole line of different code: <br><br><pre> <code class="python hljs"> <span class="hljs-comment"><span class="hljs-comment">#      . car_boxes = get_car_boxes(r['rois'], r['class_ids']) # ,        . overlaps = mrcnn.utils.compute_overlaps(car_boxes, parking_areas) print(overlaps)</span></span></code> </pre><br>  The result should look something like this: <br><br><pre> <code class="python hljs">[ [<span class="hljs-number"><span class="hljs-number">1.</span></span> <span class="hljs-number"><span class="hljs-number">0.07040032</span></span> <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-number"><span class="hljs-number">0.</span></span>] [<span class="hljs-number"><span class="hljs-number">0.07040032</span></span> <span class="hljs-number"><span class="hljs-number">1.</span></span> <span class="hljs-number"><span class="hljs-number">0.07673165</span></span> <span class="hljs-number"><span class="hljs-number">0.</span></span>] [<span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-number"><span class="hljs-number">0.02332112</span></span> <span class="hljs-number"><span class="hljs-number">0.</span></span>] ]</code> </pre><br>  In this two-dimensional array, each row reflects one frame of the parking space.  And each column indicates how strongly each of the places intersects with one of the detected machines.  The result 1.0 means that the whole place is completely occupied by the car, and a low value like 0.02 means that the car got into place a little, but you can still park on it. <br><br>  To find unallocated places, you only need to check every line in this array.  If all numbers are close to zero, then most likely the place is free! <br><br>  However, keep in mind that object recognition does not always work perfectly with live video.  Although the model based on the Mask R-CNN is pretty accurate, from time to time it can skip a car or two in one frame of video.  Therefore, before claiming that the place is free, you need to make sure that it remains so for the next 5‚Äì10 frames of the video.  This way we will be able to avoid situations when the system mistakenly marks the place as empty due to a glitch on one video frame.  As soon as we make sure that the place remains free for several frames, you can send a message! <br><br><h3>  We send SMS </h3><br>  The last part of our conveyor is sending an SMS notification when a free parking space appears. <br><br>  Send a message from Python is very easy if you use Twilio.  Twilio is a popular API that allows you to send SMS from almost any programming language with just a few lines of code.  Of course, if you prefer another service, you can use it.  I have nothing to do with Twilio, it's just the first thing that comes to mind. <br><br>  To use Twilio, sign up for a <a href="https://www.twilio.com/try-twilio">trial account</a> , create a Twilio phone number and get account authentication data.  Then install the client library: <br><br><pre> <code class="python hljs">$ pip3 install twilio</code> </pre><br>  After that use the following code to send the message: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> twilio.rest <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Client <span class="hljs-comment"><span class="hljs-comment">#   Twilio. twilio_account_sid = ' Twilio SID' twilio_auth_token = '   Twilio' twilio_source_phone_number = '   Twilio' #    Twilio. client = Client(twilio_account_sid, twilio_auth_token) #  SMS. message = client.messages.create( body=" ", from_=twilio_source_phone_number, to=" ,   " )</span></span></code> </pre><br>  To add the ability to send messages to our script, just copy this code there.  However, you need to make sure that the message is not sent on each frame, where you can see the free space.  Therefore, we will have a flag that, in the established state, will not allow us to send messages for some time or until another place becomes free. <br><br><h3>  We put everything together </h3><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mrcnn.config <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mrcnn.utils <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mrcnn.model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MaskRCNN <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pathlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Path <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> twilio.rest <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Client <span class="hljs-comment"><span class="hljs-comment"># ,     Mask-RCNN. class MaskRCNNConfig(mrcnn.config.Config): NAME = "coco_pretrained_model_config" IMAGES_PER_GPU = 1 GPU_COUNT = 1 NUM_CLASSES = 1 + 80 #   COCO  80  + 1  . DETECTION_MIN_CONFIDENCE = 0.6 #    ,    . def get_car_boxes(boxes, class_ids): car_boxes = [] for i, box in enumerate(boxes): #     ,   . if class_ids[i] in [3, 8, 6]: car_boxes.append(box) return np.array(car_boxes) #  Twilio. twilio_account_sid = ' Twilio SID' twilio_auth_token = '   Twilio' twilio_phone_number = '   Twilio' destination_phone_number = ',   ' client = Client(twilio_account_sid, twilio_auth_token) #   . ROOT_DIR = Path(".") #       . MODEL_DIR = ROOT_DIR / "logs" #       . COCO_MODEL_PATH = ROOT_DIR / "mask_rcnn_coco.h5" #   COCO  . if not COCO_MODEL_PATH.exists(): mrcnn.utils.download_trained_weights(COCO_MODEL_PATH) #     . IMAGE_DIR = ROOT_DIR / "images" #      ‚Äî   0,   ,   . VIDEO_SOURCE = "test_images/parking.mp4" #   Mask-RCNN   . model = MaskRCNN(mode="inference", model_dir=MODEL_DIR, config=MaskRCNNConfig()) #   . model.load_weights(COCO_MODEL_PATH, by_name=True) #   . parked_car_boxes = None #  ,     . video_capture = cv2.VideoCapture(VIDEO_SOURCE) #         . free_space_frames = 0 #    SMS? sms_sent = False #      . while video_capture.isOpened(): success, frame = video_capture.read() if not success: break #      BGR  RGB. rgb_image = frame[:, :, ::-1] #    Mask R-CNN   . results = model.detect([rgb_image], verbose=0) # Mask R-CNN ,       . #     ,     . r = results[0] #  r    : # - r['rois'] ‚Äî      ; # - r['class_ids'] ‚Äî  () ; # - r['scores'] ‚Äî  ; # - r['masks'] ‚Äî   (    ). if parked_car_boxes is None: #     ‚Äî ,       . #            . parked_car_boxes = get_car_boxes(r['rois'], r['class_ids']) else: #   ,  . ,   . #     . car_boxes = get_car_boxes(r['rois'], r['class_ids']) # ,         . overlaps = mrcnn.utils.compute_overlaps(parked_car_boxes, car_boxes) # ,    ,      . free_space = False #        . for parking_area, overlap_areas in zip(parked_car_boxes, overlaps): #        #    (, ). max_IoU_overlap = np.max(overlap_areas) #         . y1, x1, y2, x2 = parking_area # ,   ,   IoU. if max_IoU_overlap &lt; 0.15: #  !     . cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 3) # ,        . free_space = True else: #     ‚Äî   . cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 1) #   IoU  . font = cv2.FONT_HERSHEY_DUPLEX cv2.putText(frame, f"{max_IoU_overlap:0.2}", (x1 + 6, y2 - 6), font, 0.3, (255, 255, 255)) #       ,   . #   ,  ,     #      . if free_space: free_space_frames += 1 else: #   ,  . free_space_frames = 0 #       ,  ,   . if free_space_frames &gt; 10: #   SPACE AVAILABLE!!  . font = cv2.FONT_HERSHEY_DUPLEX cv2.putText(frame, f"SPACE AVAILABLE!", (10, 150), font, 3.0, (0, 255, 0), 2, cv2.FILLED) #  ,     . if not sms_sent: print("SENDING SMS!!!") message = client.messages.create( body="Parking space open - go go go!", from_=twilio_phone_number, to=destination_phone_number ) sms_sent = True #    . cv2.imshow('Video', frame) #  'q',  . if cv2.waitKey(1) &amp; 0xFF == ord('q'): break #  'q',  . video_capture.release() cv2.destroyAllWindows()</span></span></code> </pre><br>  To run this code, you first need to install Python 3.6+, <a href="https://github.com/matterport/Mask_RCNN">Matterport Mask R-CNN</a> and <a href="https://pypi.org/project/opencv-python/">OpenCV</a> . <br><br>  I specifically wrote the code as easy as possible.  For example, if he sees on the first frame of the car, he concludes that they are all parked.  Try experimenting with it and see if you can improve its reliability. <br><br>  By simply changing the identifiers of the objects that the model is looking for, you can turn the code into something completely different.  For example, imagine that you work at a ski resort.  Having made a couple of changes, you can turn this script into a system that automatically recognizes snowboarders jumping off the ramp, and records videos with great jumps.  Or, if you work in the reserve, you can create a system that counts zebras.  You are limited only by your imagination. <br><br>  More similar articles can be read in the <a href="http://t.me/neurondata">Neuron</a> telegram channel (@neurondata) <br><br>  Link to alternative translation: <a href="https://tproger.ru/translations/parking-searching/">tproger.ru/translations/parking-searching/</a> <br><br>  All knowledge.  Experiment! </div><p>Source: <a href="https://habr.com/ru/post/451164/">https://habr.com/ru/post/451164/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../451152/index.html">Resistor in the gate circuit or how to do it right</a></li>
<li><a href="../451154/index.html">Local Autonomous Data Collection System (continued)</a></li>
<li><a href="../451158/index.html">Electrical diagrams. Types of schemes</a></li>
<li><a href="../451160/index.html">Apache Kafka and Stream Processing with Spark Streaming</a></li>
<li><a href="../451162/index.html">Correction of errors - physical constants in the present and new versions of the International System of Units (SI)</a></li>
<li><a href="../451166/index.html">What will the new storages for AI and MO systems offer?</a></li>
<li><a href="../451170/index.html">Jeff Bezos announced plans to conquer the moon</a></li>
<li><a href="../451172/index.html">Julia: functions and structure-as-functions</a></li>
<li><a href="../451174/index.html">Adaptation programs for the ZX Spectrum to TR-DOS with modern tools. Part 1</a></li>
<li><a href="../451176/index.html">News from the world of OpenStreetMap ‚Ññ458 (04/23/2019-29.04.2019)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>