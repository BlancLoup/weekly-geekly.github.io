<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Convolution network in python. Part 3. Application of the model</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This is the final part of articles on convolution networks. Before reading, I recommend that you familiarize yourself with the first and second parts,...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Convolution network in python. Part 3. Application of the model</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img width="95%" height="95%" src="https://habrastorage.org/webt/wh/qt/4v/whqt4vbr0q4blnemfpa0b28ka88.png"></div><br>  This is the final part of articles on convolution networks.  Before reading, I recommend that you familiarize yourself with the <a href="https://habrahabr.ru/company/ods/blog/344008/">first</a> and <a href="https://habrahabr.ru/company/ods/blog/344116/">second</a> parts, which examine the layers of the network and the principles of their work, as well as the formulas that are responsible for teaching the entire model.  Today we will look at the features and difficulties that can be encountered when testing a convolutional network written manually in python, apply the written network to the MNIST dataset and compare the results obtained with the tensorflow library. <br><a name="habracut"></a><br>  Now you can see the basic concept of the network, the structure of the layers and their sequence.  Below I presented it in the way I implemented it in the code - each layer as a separate function (you can remove or add new layers as an experiment, swap them or write your new layer): <br><br>  <b>Direct passage through the network</b> <br><br>  1) The first layer of the convolutional network <br>  2) Maxpooling Layer <br>  3) The second layer of the convolutional network <br>  4) Adding all feature maps to one vector (this is not quite the ‚Äúfull-fledged‚Äù layer, but nevertheless occupies an important place here) <br>  5) First layer of fc network <br>  6) Second layer fc network <br>  7) Calculation of the loss function 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>Reverse passing through the network and updating the parameters (we go through all layers in the reverse order)</b> <br><br>  8) Backprop through loss <br>  9) Second layer fc network <br>  10) First layer of fc network <br>  11) Expanding feature maps from a vector (also not a ‚Äúfull-fledged‚Äù layer) <br>  12) The second layer of the convolutional network <br>  13) Maxpooling Layer <br>  14) The first layer of the convolutional network <br><br>  Of course, one should not expect this model to work faster than an optimized library for machine learning.  But after all, the ultimate goal is not to write a quick implementation, but to understand how the library works, to learn how to build a neural network on the lowest level, and after previous articles with the considered principles of operation and formulas, all that remains is to write code.  Further an example of such code. <br><br>  <a href="https://github.com/skalinin/cnn_in_python/blob/master/model.py">model.py</a> - all major functions that make up the network are stored here.  Many of the functions we have already covered in past articles. <br><br>  <a href="https://github.com/skalinin/cnn_in_python/blob/master/np_mnist_train_test.py">np_mnist_train_test.py</a> - the network itself and the input parameters for it.  The network is built from the functions defined in model.py, and it looks literally in the same way as the 14 points that we identified at the beginning of the article: <br><br><div style="text-align:center;"><img width="85%" height="85%" src="https://habrastorage.org/webt/en/5m/xr/en5mxrkl6vwkzeb1yvmj_9yrdvw.png"></div><br>  (I hid the function arguments, in expanded form everything looks scary) <br>  But before going on to describe the results, I would like to dwell on another.  Applying the written model to the data, I saw that she was trained and everything passed without errors.  However, I wanted to be sure that the network is working correctly, that all internal calculations are correct.  And the most obvious solution was to compare this model with a similar architecture at tensorflow, using, for example, MNIST as a dataset. <br><br><div style="text-align:center;"><img width="85%" height="85%" src="https://habrastorage.org/getpro/habr/post_images/ec7/992/24e/ec799224e0014dc47358c54e0777c994.png"></div><br>  It was much easier to write a network to tensorflow, in fact, I just took <a href="https://www.tensorflow.org/get_started/mnist/pros">this</a> option from the tensorflow manual and slightly modified the parameters.  It turned out this model: <br><br>  <a href="https://github.com/skalinin/cnn_in_python/blob/master/tf_mnist_train_test.py">tf_mnist_train_test.py</a> <br><br>  To make sure that the results of both models are the same, you must first make sure that their starting weights are identical.  Here was the first difficulty - I could not fix the seed separately for numpy and tensorflow so that the initial randomly generated weights matrices coincide.  I came up with this trick: create weights in tesnrflow and feed them into both models.  But it seems to me that this issue could be solved and somehow easier.  But in the end, it looks like this: <br><br><div class="spoiler">  <b class="spoiler_title">code_demo_tf_reshape.py</b> <div class="spoiler_text">  <a href="https://github.com/skalinin/cnn_in_python/blob/master/code_demo/code_demo_tf_reshape.py">git link</a> <br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf tf_w = tf.truncated_normal([<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>], stddev=<span class="hljs-number"><span class="hljs-number">0.1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.Session() <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sess: np_w = sess.run(tf_w) print(<span class="hljs-string"><span class="hljs-string">'\n    tensorflow: \n \n'</span></span>, np_w) print(<span class="hljs-string"><span class="hljs-string">'\n \n        :'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(np_w)): print(<span class="hljs-string"><span class="hljs-string">'\n'</span></span>, np_w[i]) conv_w = [] np_w = np.reshape(np_w, (np_w.size,)) np_w = np.reshape(np_w, (<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>), order=<span class="hljs-string"><span class="hljs-string">'F'</span></span>) print(<span class="hljs-string"><span class="hljs-string">'\n \n      ,    tensorflow:'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">4</span></span>): conv_w.append(np_w[i].T) print(<span class="hljs-string"><span class="hljs-string">'\n'</span></span>, conv_w[<span class="hljs-number"><span class="hljs-number">-1</span></span>])</code> </pre> <br></div></div><div class="spoiler">  <b class="spoiler_title">Sample script output</b> <div class="spoiler_text"><div style="text-align:center;"><img width="60%" height="60%" src="https://habrastorage.org/webt/sn/gn/hh/sngnhh3d5y0xjbjp5zzhdbbxvza.jpeg"></div><br></div></div><br>  To extract the weights from the tensor in the order that tensorflow sees them, that is, in the manner in which they are used in further work inside the library, I used the code above.  As you can see, everything is not quite trivial: the tensorflow tensor had to go through a couple of Resapes and transposition so that it could be used in the numpy model.  Also for this reason, some other functions had to be rewritten.  For example, the function of combining all feature maps into one vector is when many feature maps form after the convolution layers and it is necessary to vectorize them for submission to the fc network.  After we have broken the tensor into matrices using the above method, we cannot combine the matrices into a vector anymore if we want it to look the same as inside a tensorflow.  And in order to achieve this, we must do the following: <br><br><div style="text-align:center;"><img width="40%" height="40%" src="https://habrastorage.org/webt/fv/ad/vd/fvadvdwbr3sfhtdnxzx2a_cjkxy.jpeg"></div><br>  Accordingly, when in the case of the reverse propagation of the error, we went through fully connected layers, we will have a vector of gradients, which we need to ‚Äústraighten‚Äù back into the feature maps.  This is done as in the picture above, only in the opposite direction.  Of course, if the goal is not to compare the results of calculations of numpy- and tenosorflow-models, these functions can be written in a simple way.  Here, in fact, I described what the matrix2vector (matrix-to-vector) and vector2matrix (vector-to-matrix) functions do in model.py. <br><br>  The following: feature maps are viewed as channels of one, so to speak, image, and not as image independent of each other.  Initially, I assumed that if we want four feature maps at the output, then there should be four cores, but it all turns out a little differently.  Since all feature maps of the previous layer are considered as one image with multiple channels, then to get only one map on the next layer, you need to create as many weight matrices as there are channels on the previous layer (respectively, for two maps, twice as many weight matrices).  Then get the appropriate number of ‚Äúintermediate‚Äù maps and put them into one - this will be the desired feature map.  Let's say, if there were two cards on the previous layer, and we want to get four on the next layer, then it will look something like this: <br><br><div style="text-align:center;"><img width="80%" height="80%" src="https://habrastorage.org/webt/a8/43/km/a843kmdwcrmjy5jdy2yjepercfe.jpeg"></div><br>  When adding, ‚Äúintermediate‚Äù maps of signs seem to be shuffled to produce a ‚Äúfinal‚Äù map, but if you look closely, it is clear that only those ‚Äúintermediate‚Äù maps of signs that originate from different channels of the original layer are added to the final maps.  Maybe thanks to the image below it will be better understood: <br><br><div style="text-align:center;"><img width="80%" height="80%" src="https://habrastorage.org/webt/ej/q5/5w/ejq55wpxe9ijxfc141srxj8lzfy.jpeg"></div><br>  Here we want to get two feature maps from one RGB image.  And I‚Äôd also note: during the reverse propagation of an error, those ‚Äúintermediate‚Äù cards are not involved at all: they don‚Äôt exist at all in the understanding of the program.  Each matrix of weights and a feature map from the previous layer are tied to the corresponding ‚Äúfinal‚Äù maps of the next layer. <br><br>  The next feature of tensorflow calculations was the fact that inside the tf.nn.conv2d function, cross-correlation actually occurs and this is even written in the <a href="https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/convolution">tensorflow manual</a> : <br><blockquote>  It is strictly speaking that they have been strictly speaking.  For details, see the properties of cross-correlation. </blockquote><br>  But this did not become a big problem, since in my implementation for numpy it is enough to change True to False.  Here is an example of code with which you can make sure that it is cross-correlation that is used: <br><br><div class="spoiler">  <b class="spoiler_title">code_demo_cross_correlation.py</b> <div class="spoiler_text">  <a href="https://github.com/skalinin/cnn_in_python/blob/master/code_demo/code_demo_cross_correlation.py">git link</a> <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np np.random.seed(<span class="hljs-number"><span class="hljs-number">0</span></span>) tf.set_random_seed(<span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-comment"><span class="hljs-comment"># from pudb import set_trace; set_trace() #   #        numpy tf_w1 = tf.truncated_normal([3, 3, 1, 1], stddev=0.1) with tf.Session() as sess: w1 = sess.run(tf_w1) x = tf.constant(0.1, shape=[4, 4]) input_image = tf.reshape(x, [-1, 4, 4, 1]) w_conv1 = tf.Variable(w1) h_conv1 = tf.nn.conv2d(input_image, w_conv1, strides=[1, 1, 1, 1], padding='SAME') with tf.Session() as sess: sess.run(tf.global_variables_initializer()) w_conv = sess.run(h_conv1) w_conv = np.reshape(w_conv, (w_conv.size,)) w_conv = np.reshape(w_conv, (1,4,4), order='F') #       ! print('\n   ""  tensorflow:') for i in range(1): print(w_conv[i].T) w1 = np.reshape(w1, (w1.size,)) w1 = np.reshape(w1, (1,3,3), order='F') for i in range(1): w_l = w1[i].T y_l_minus_1 = np.array([ [0.1,0.1,0.1,0.1], [0.1,0.1,0.1,0.1], [0.1,0.1,0.1,0.1], [0.1,0.1,0.1,0.1]]) other_parameters={ 'convolution':False, 'stride':1, 'center_w_l':(1,1), } def convolution_feed_x_l(y_l_minus_1, w_l, conv_params): indexes_a, indexes_b = create_indexes(size_axis=w_l.shape, center_w_l=conv_params['center_w_l']) stride = conv_params['stride'] #          x_l = np.zeros((1,1)) #          if conv_params['convolution']: g = 1 #   else: g = -1 #   #   i  j   y_l_minus_1  ,     x_l    for i in range(y_l_minus_1.shape[0]): for j in range(y_l_minus_1.shape[1]): demo = np.zeros([y_l_minus_1.shape[0], y_l_minus_1.shape[1]]) #     result = 0 element_exists = False for a in indexes_a: for b in indexes_b: # ,        if i*stride - g*a &gt;= 0 and j*stride - g*b &gt;= 0 \ and i*stride - g*a &lt; y_l_minus_1.shape[0] and j*stride - g*b &lt; y_l_minus_1.shape[1]: result += y_l_minus_1[i*stride - g*a][j*stride - g*b] * w_l[indexes_a.index(a)][indexes_b.index(b)] #    ""      w_l demo[i*stride - g*a][j*stride - g*b] = w_l[indexes_a.index(a)][indexes_b.index(b)] element_exists = True #       ,    i  j    if element_exists: if i &gt;= x_l.shape[0]: #  ,    x_l = np.vstack((x_l, np.zeros(x_l.shape[1]))) if j &gt;= x_l.shape[1]: #  ,    x_l = np.hstack((x_l, np.zeros((x_l.shape[0],1)))) x_l[i][j] = result #   demo     # print('i=' + str(i) + '; j=' + str(j) + '\n', demo) return x_l def create_axis_indexes(size_axis, center_w_l): coordinates = [] for i in range(-center_w_l, size_axis-center_w_l): coordinates.append(i) return coordinates def create_indexes(size_axis, center_w_l): #              coordinates_a = create_axis_indexes(size_axis=size_axis[0], center_w_l=center_w_l[0]) coordinates_b = create_axis_indexes(size_axis=size_axis[1], center_w_l=center_w_l[1]) return coordinates_a, coordinates_b print('\n   -:') print(convolution_feed_x_l(y_l_minus_1, w_l, other_parameters))</span></span></code> </pre><br></div></div><div class="spoiler">  <b class="spoiler_title">Sample script output</b> <div class="spoiler_text"><div style="text-align:center;"><img width="65%" height="65%" src="https://habrastorage.org/webt/zl/np/2i/zlnp2iabqna_lfrtgytft0tclju.jpeg"></div><br></div></div><br>  The same demo code contains another interesting detail of calculations in tensorflow.  Inside the library, during the convolution operation, the central element of the convolution kernel sometimes changes.  Usually it is in position (0,0), but for the 3 by 3 core it moves to the central position (1,1) (and maybe, in fact, the opposite is true and the ‚Äúusual‚Äù position is exactly in the center (1,1) and the core is relative to it and moves to the upper left corner ...).  If, for this kernel, we set the stride to two, then the central element in tensorflow will again move to the zero position.  The logic is, as it seems to me that with the dimension of the input matrix 4 by 4 pixels and a step of two pixels, we expect an output matrix of dimension 2 by 2. And this is exactly what happens, but only if the central element is in position (0.0 ), if the central element is in (1,1), then the dimension of the output matrix under such conditions will be three by three pixels.  This is due to the fact that the core with the central element in (1.1) manages to perform more calculations before it is ‚Äúhidden‚Äù outside the input matrix.  Here, look at the picture below: <br><br><div style="text-align:center;"><img width="75%" height="75%" src="https://habrastorage.org/webt/dy/y6/hf/dyy6hfmqgbtamiasuhv5g1b1wy4.png"></div><br>  And thus, if the code written from scratch does not take into account the possibility of choosing a central element, then one could never understand the reason for the difference of results from tensorflow. <br><br>  So, finally, we dismantled all the nuances that led to differences in the results, we took into account all this in the from scratch model and we can compare the losses of both networks: the first one implemented using numpy, and the second - the library at tensorflow. <br><br>  Next, a little about network architecture.  On the first convolutional layer, I used a 2x2 pixel core (and the central element in the zero position) with a step equal to two, this layer produces five feature maps of 14x14 pixels (that is, two times smaller than the original mnist images of 28x28 pixels).  These cards are fed to the second layer with the core already three by three pixels and one step (which, I recall, leads to the displacement of the central element, in fact, in the center of the core), and at the output of 20 feature maps of the same dimension.  Then a layer of max-cooling reduces the dimension of maps to 7x7 pixels.  Then the cards add up to a vector.  Next is the hidden layer of a fully connected network with two thousand neurons.  On this layer in the weights matrix, almost two million (7 x 7 x 20 x 2000 = 1 960 000) elements are obtained!  And then already 2000 neurons are connected with 10 output neurons responsible for the number of classes: that is, 10 digits that the network must learn to predict.  All these parameters are listed in a dictionary called model settings, which is located inside the np_mnist_train_test.py file: <br><br><div style="text-align:center;"><img width="85%" height="85%" src="https://habrastorage.org/webt/82/db/lh/82dblhhvohdmchtj3egna8f3urc.jpeg"></div><br>  So, let's try to run both models and compare the results: loss and accuracy, averaged over each five images: <br><br><div style="text-align:center;"><img width="75%" height="75%" src="https://habrastorage.org/webt/8l/bp/7f/8lbp7flttgvmecokyyybwh_usaa.png"></div><br>  As you can see, the losses in almost completely coincide, which means that all the calculations within the manually assembled model and the model written in tensorflow are at least very similar. <br><br>  After a day and one epoch of training (that is, 55 thousand iterations of the forward and backward passage of the network), we got such graphs for loss and accuracy: <br><br><div style="text-align:center;"><img width="100%" height="100%" src="https://habrastorage.org/webt/xt/tk/b6/xttkb6us-kb0b0_3ucoylzshnvo.jpeg"></div><br>  And on the test sample, accuracy is 89%: <br><br><div style="text-align:center;"><img width="90%" height="90%" src="https://habrastorage.org/webt/t-/jc/d1/t-jcd1zxojwb_aoz3su1zjvtovs.png"></div><br>  As you can see, the loss for the numpy-model team on the test and training samples is very similar to the results of the network by tensorflow.  The difference in accuracy on the test sample after one epoch of training is only one image per ten thousand!  But nevertheless the weights matrices themselves differ in thousandths (whereas there was no difference in the first stages of training) - the difference is most likely due to the different accuracy of calculations in numpy and tensorflow: float32 and float64, or other unaccounted nuances that make minor differences in model calculations. <br><br>  Accuracy 89% was achieved after just one era.  But after already four epochs on the same model, the results on the test sample would have been significantly higher - 96%.  But I tried it only at tensorflow, on the numpy-model further training took too much time, but in general it can be concluded that the chosen architecture is not as bad as it could be! <br><br><div style="text-align:center;"><img width="55%" height="55%" src="https://habrastorage.org/webt/4r/pc/ud/4rpcudonz-jvw7bjukbwdl4qmxc.png"></div><br>  To be precise, one epoch on the numpy-model took about 15 hours on an old laptop.  On the model assembled using tensorflow, the same era passes in just 7-8 minutes.  That is a difference of 120 times!  What does it take all the time in the manually assembled model and where is the bottleneck?  To understand this, just look at the timeline that each function takes in the numpy-model (total for forward and backward passes through the network): <br><br><div style="text-align:center;"><img width="80%" height="80%" src="https://habrastorage.org/webt/ph/z4/4u/phz44uvwt5fuebs4vvmum6siynq.jpeg"></div><br>  As you can see, almost all 15 hours several convolution cores are trained.  That is why in the model parameters I have so few feature maps at the output of convolutional layers.  The full-connected layers take almost no time (especially the second fc-layer with a smaller number of parameters), since inside there is an optimized multiplication of the numpy-matrices, whereas the convolutional functions were written on the basis of formulas and the literal ‚Äúmovement‚Äù of the kernel along the matrix.  The calculations of tensorflow are much more optimized and the convolutional layers are also presented in the form of matrix multiplication.  <a href="https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/">Here</a> you can read more. <br><br>  I tried to study the model with ‚Äúnon-standard‚Äù parameters - the operation of convolution instead of cross-correlation and the central elements in the position (1.0) for the first layer and (1.2) for the second convolution layer.  The results were worse, but with further training, it would probably be aligned with the usual parameters.  Of course, I could no longer compare the calculations with the tensorflow model, since it is not possible to change the specified central element to any other or cross-correlation to convolution for any layer of the convolutional network (in general, it is unlikely may come in handy someday).  Interestingly, accuracy at the very beginning began to grow sharply upwards, but after that it was already far behind the ‚Äúnormal‚Äù model. <br><br><div style="text-align:center;"><img width="110%" height="110%" src="https://habrastorage.org/webt/qm/2q/3m/qm2q3muk4t8838lmpo8ffcf8i2c.png"></div><br>  Of course, the numpy-model is not suitable for real calculations and tensorflow or other machine learning libraries should be used.  I can‚Äôt say that I‚Äôm absolutely sure that there are no errors in the python implementation, that all the subtleties and nuances of training that are definitely in tensorflow are taken into account.  But the main thing is the time of study.  The fact that python takes two hours, to tensorflow - just a minute. <br><br>  To improve the results obtained on MNIST, you need to use more convolutionary layers, generate more feature maps for the network to adequately extract features from images (all this, of course, will seriously affect the learning time if you use the model on numpy).  You should also use the batch in more than one image, the classical SGD optimization method should be replaced with, for example, Adam (which can be read <a href="https://habrahabr.ru/post/318970/">here</a> or <a href="http://ruder.io/optimizing-gradient-descent/">here</a> ).  The results we obtained in the article are, of course, not impressive (the leaderboard can be seen <a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html">here</a> ), but it is clear that the model, written literally according to the formulas, really learns, works.  If you want to reproduce the test results, or continue your training, we have cnn_weights_mnist.npy weights for the numpy model in the <a href="">git</a> repository. <br><br>  This is the end of the article on convolutional networks.  Well, if now you already have an idea about the internal structure and mathematics, which stands behind the networks and is hidden inside the library for machine learning.  I tried to tell everything as simple as possible and sort out all the formulas so that there were no questions left, and I hope that these articles save someone time.  Well, that's all, thanks for reading to the end! </div><p>Source: <a href="https://habr.com/ru/post/344888/">https://habr.com/ru/post/344888/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../344876/index.html">JUnit logging tests</a></li>
<li><a href="../344878/index.html">Making responsive HTML by adding one line to CSS</a></li>
<li><a href="../344882/index.html">Changes in 3CX audio conferencing</a></li>
<li><a href="../344884/index.html">‚ÄúTruth in wine‚Äù or try programming NanoCAD under Linux (MultiCAD.NET API)</a></li>
<li><a href="../344886/index.html">Dagger 2 for novice Android developers. Dagger 2. Part 2</a></li>
<li><a href="../344890/index.html">Function, script and event approximation</a></li>
<li><a href="../344892/index.html">Useful to the designer: free new items to optimize design processes. 2nd Edition</a></li>
<li><a href="../344894/index.html">Legal aspects of operations with cryptocurrencies for residents of the Russian Federation</a></li>
<li><a href="../344896/index.html">We comprehend C deeper using assembler</a></li>
<li><a href="../344898/index.html">Tips for creating a modern Android application. Yandex lecture</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>