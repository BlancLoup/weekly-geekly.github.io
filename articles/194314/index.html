<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>You understand Hadoop wrong</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="‚ÄúWe receive more than a million tweets per day, and our server simply does not have time to process them.‚Äù Therefore, we want to install the Hadoop cl...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>You understand Hadoop wrong</h1><div class="post__text post__text-html js-mediator-article"><p>  ‚ÄúWe receive more than a million tweets per day, and our server simply does not have time to process them.‚Äù  Therefore, we want to install the Hadoop cluster and distribute the processing. </p><br><br><p>  It was a matter of computationally heavy <a href="http://en.wikipedia.org/wiki/Sentiment_analysis">sentimental analysis</a> , so I could believe that one server really did not have enough CPU to cope with the large tweet stream. </p><br><br><p>  - What are you going to do with the already processed data? <br>  - Most likely, we will add them to MySQL, as they did before, or even delete them. <br>  ‚ÄúThen you definitely don't need Hadoop.‚Äù </p>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>  My former colleague was not the first to talk about distributed computing on Hadoop.  And each time I saw a complete misunderstanding of why this platform was invented and developed. </p><br><br><a name="habracut"></a><br><br><p>  Generally speaking, there are several typical situations in data processing, and for each of them there are different approaches and tools.  Consider the main ones. </p><br><br><h3>  Accurate Active Data Processing </h3><br><p>  The most frequent scenario with which we all have to face is the storage of ‚Äúactive‚Äù data - information about users, a list of goods, comments to articles, etc.  - everything that can change frequently.  In this case, we want to be able to process the data pointwise - retrieve the desired object by index, process it and load it back.  This is the functionality that most <strong>DBMSs</strong> provide (both relational and NoSQL). </p><br><br><p>  When scaling the main problem here arises with the maximum amount of data stored in the database.  However, even if the used DBMS does not support a distributed structure, the problem is easily solved by partitioning at the application level. </p><br><br><h3>  Real-time stream processing </h3><br><p>  Sometimes, however, the emphasis is not on storage, but on data processing.  This is exactly the situation that my former colleague engaged in sentimental analysis faced.  He needed to get real-time tweets, analyze them and display the result on a dynamically generated graph.  A million daily tweets were not a problem - after all, this is no more than 140 million characters or 280 MB using the UTF16 encoding.  The problem was analyzing these tweets in real time. </p><br><br><p>  Fortunately, most streaming algorithms (whether sentimental analysis of tweets, collection of cumulative statistics, or <a href="http://en.wikipedia.org/wiki/Online_machine_learning">online machine learning</a> ) use <em>small</em> , <em>independent</em> pieces of data for their work.  This makes it easy to parallelize processing by simply adding more compute nodes and placing a <strong>load balancer</strong> in front of them. <br>  In the simplest case, a message broker (such as <a href="http://www.rabbitmq.com/">RabbitMQ</a> or <a href="http://zeromq.org/">ZeroMQ</a> ) can act as a balancer; in more complex, you can use ready-made frameworks for stream processing, such as <a href="http://storm-project.net/">Storm</a> .  At the same time, the main code that directly performs data processing remains practically unchanged compared to the single-server version. </p><br><br><h3>  Batch processing of historical data </h3><br><p>  In addition to active and streaming, there is also another important type of data - historical, i.e.  those that were once generated and are unlikely to ever change.  This includes event logs, financial indicators, document indices for a certain day, and indeed any data tied to a certain point in the past.  Most often, such data is accumulated in large quantities and then used by analysts to solve business problems.  A distinctive feature here is that at the time of processing the necessary data has already been collected and decomposed on multiple servers (if the data fit on one server, then they are simply <a href="http://www.chrisstucchio.com/blog/2013/hadoop_hatred.html">not so large</a> ). </p><br><br><p>  Imagine that we have data on all purchases in a large supermarket chain over the past half year.  We want to analyze this data: calculate the average efficiency of each supermarket, the effect of the shares held, the correlation between the purchased goods and many other metrics.  How to organize work with data in such a way that the calculation of these parameters takes reasonable time? </p><br><br><p>  We can load all data into a distributed Oracle database and work with them in the same way as with active ones.  But in this case, the application server will consistently take data from the database and sequentially process each record, which is extremely inefficient. </p><br><br><p>  We can also set up a pipeline for stream processing, distributing the load between application servers.  But sooner or later we will rest on the communication channel between the processing nodes and data nodes. </p><br><br><p>  The only way to distribute the load and not overflow the communication channel is to <em>minimize the movement of data</em> between nodes.  And for this, it is necessary to produce maximum calculations <em>locally</em> on the machines where the processed data lie.  It is the <strong>principle of data locality that</strong> underlies the MapReduce paradigm and the entire Hadoop. </p><br><br><h3>  More about MapReduce </h3><br><p>  It is considered that each MapReduce task consists of two phases - the map phase and the reduce phase.  In fact, everything is somewhat more complicated: first, the data is split into splits, then the map function is performed on each of them, then the results are sorted, then combined, then sorted again, and finally the reduce function is passed.  However, it is map and reduce that describe the main idea of ‚Äã‚Äãthe paradigm. </p><br><br><p>  At the stage of applying the map function, all the work that can be done locally is performed.  For example, locally, you can count the number of words in a line or calculate the metric of one entry in a CSV file, or analyze a tweet, etc.  Map provides transparent parallelization and minimal load on the communication channel. </p><br><br><p>  But one local map is not enough for most tasks: to calculate the number of words in the whole text, you need to add up their number in each line, and to calculate the average value of the metric, you need to put together the results for each entry from the CSV file.  This is precisely the task of the reduce function.  At the same time, the amount of data transmitted over the network is significantly reduced (the combination function, which acts as a local reduce, also helps a lot). </p><br><br><p>  So why was my colleague wrong, intending to use Hadoop to sentimentally analyze tweets?  Indeed, as mentioned above, in the map phase, you can analyze each tweet separately, completely ignoring the reduce phase!  It's all about the infrastructure.  First, tweets will still have to be first transported to compute nodes, and this means losing the advantage of data locality.  Secondly, Hadoop is poorly suited for online processing: work is carried out with data packets, which means you have to first collect tweets, and only then start the MapReduce task.  Even if you set up the map task for a constant and endless reading of tweets from the source, Hadoop will kill the entire task as a failed task after some time.  Well, and thirdly, if you hear that Hadoop is fast, remember that performance is achieved by minimizing data movement, while MapReduce tasks themselves, especially on small amounts of data, can take quite a long time due to overhead (starting JVM, performing backup tasks, writing intermediate results to disk, etc.). </p><br><br><p>  So use the right tools for the right tasks, and you will be happy! </p></div><p>Source: <a href="https://habr.com/ru/post/194314/">https://habr.com/ru/post/194314/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../194304/index.html">Communication with the lists in the Worklite Docs Electronic Document Management document card at Sharepoint 2013</a></li>
<li><a href="../194306/index.html">The second in the history of commercial cargo spacecraft launched to the ISS</a></li>
<li><a href="../194308/index.html">YouTube will soon add the ability to view videos in offline mode (for mobile devices)</a></li>
<li><a href="../194310/index.html">The Most Difficult Game</a></li>
<li><a href="../194312/index.html">Google vs. Death = Calico</a></li>
<li><a href="../194316/index.html">USB condom. Malware via USB</a></li>
<li><a href="../194318/index.html">CyanogenMod installer will appear in Google Play, developers have received $ 7 million investment</a></li>
<li><a href="../194324/index.html">Determining the weights of the importance of users relative to each other based on their actions (Tarantool + Lua)</a></li>
<li><a href="../194326/index.html">New Brief: Samsung announced the start of mass production of new 4-gigabit DDR4 memory</a></li>
<li><a href="../194332/index.html">TeamLab PM for iOS as the start of a series of mobile applications for business</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>