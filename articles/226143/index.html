<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Speech Recognition for Dummies</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this article I want to consider the basics of such an interesting area of ‚Äã‚Äãsoftware development as Speech Recognition. Naturally, I am not an expe...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Speech Recognition for Dummies</h1><div class="post__text post__text-html js-mediator-article"> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/9a3/68c/5e1/9a368c5e1f0b39c388e1c92bbcd80ea3.jpg"></a> <br>  In this article I want to consider the basics of such an interesting area of ‚Äã‚Äãsoftware development as Speech Recognition.  Naturally, I am not an expert in this topic, so my story will be replete with inaccuracies, mistakes and disappointments.  However, the main purpose of my ‚Äúwork‚Äù, as the name implies, is not a professional analysis of the problem, but a description of the basic concepts, problems and their solutions.  In general, I ask all interested in welcome under the cut! <br><br><a name="habracut"></a><br><br><h2>  Prologue </h2>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      To begin with, our speech is a sequence of sounds.  Sound, in turn, is a superposition (superposition) of sound vibrations (waves) of various frequencies.  The wave, as we know from physics, is characterized by two attributes - amplitude and frequency. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/3c5/cdd/7b4/3c5cdd7b450ee5d9cfe70b466fdd85a4.png"></a> <br><br>  In order to save the audio signal on a digital medium, it must be divided into many gaps and take some "average" value on each of them. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/2ff/740/1d1/2ff7401d11100a403d3b933cbd33aeb0.png"></a> <br><br>  In this way, mechanical vibrations are transformed into a set of numbers suitable for processing on modern computers. <br><br>  It follows that the task of speech recognition is reduced to ‚Äúmatching‚Äù a set of numerical values ‚Äã‚Äã(digital signal) and words from a certain dictionary (Russian, for example). <br><br>  Let's see how, in fact, this very "comparison" can be implemented. <br><br><br><br><h2>  Input data </h2><br><br>  Suppose we have some file / stream with audio data.  First of all, we need to understand how it works and how to read it.  Let's look at the simplest option - WAV file. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/832/307/7bf/8323077bf18a761988fcb1cf40176e01.gif"></a> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/d67/e06/464/d67e0646432b366bf14788b2ff00c6f7.gif"></a> <br><br>  The format implies the presence of two blocks in the file.  The first block is a header with information about the audio stream: bitrate, frequency, number of channels, file length, etc.  The second block consists of ‚Äúraw‚Äù data - that same digital signal, a set of amplitude values. <br><br>  The logic of reading data in this case is quite simple.  We read the header, check some restrictions (no compression, for example), save the data to a dedicated array. <br><br>  <a href="">An example</a> . <br><br><br><br><h2>  Recognition </h2><br><br>  Purely theoretically, now we can compare (element by element) the sample we have with some other, the text of which we already know.  That is, try to ‚Äúrecognize‚Äù the speech ... But it is better not to do it :) <br><br>  Our approach should be sustainable (well, at least a little) to change the timbre of the voice (the person who utters the word), volume and speed of pronunciation.  By element-wise comparison of two audio signals, this, naturally, cannot be achieved. <br><br>  Therefore, we will go a little different way. <br><br><br><br><h2>  Frames </h2><br><br>  First of all, we divide our data by small time intervals - frames.  And the frames should not go strictly one after another, but ‚Äúoverlapped‚Äù.  Those.  the end of one frame should intersect with the beginning of another. <br><br>  Frames are a more suitable unit of data analysis than specific signal values, since it is much more convenient to analyze the waves at a certain interval than at specific points.  The location of the overlapping frames makes it possible to smooth the results of frame analysis, turning the frame idea into a ‚Äúwindow‚Äù moving along the original function (signal values). <br><br>  It has been experimentally established that the optimal frame length should correspond to a gap of 10ms, an ‚Äúoverlap‚Äù of 50%.  Taking into account the fact that the average word length (at least in my experiments) is 500ms - this step will give us approximately 500 / (10 * 0.5) = 100 frames per word. <br><br><br><br><h2>  Word splitting </h2><br><br>  The first task, which has to be solved in speech recognition, is the partitioning of this very speech into separate words.  For simplicity, suppose that in our case it contains some pauses (intervals of silence), which can be considered as ‚Äúdelimiters‚Äù of words. <br><br>  In this case, we need to find some value, the threshold - the values ‚Äã‚Äãabove which are a word, below - silence.  There may be several options: <br><ul><li>  set by constant (will work if the source signal is always generated under the same conditions, in the same way); </li><li>  cluster the signal values ‚Äã‚Äãby explicitly highlighting the set of values ‚Äã‚Äãcorresponding to silence (it will only work if silence occupies a significant part of the original signal); </li><li>  analyze entropy; </li></ul><br><br><br>  As you have already guessed, we are now talking about the last point :) Let's start with the fact that entropy is a measure of confusion, ‚Äúa measure of the uncertainty of any experience‚Äù (c).  In our case, entropy means how strongly our signal oscillates within a given frame. <br><br>  In order to calculate the entropy of a specific frame, perform the following actions: <br><ul><li>  suppose that our signal is normalized and all its values ‚Äã‚Äãlie in the range [-1; 1]; </li><li>  construct a histogram (distribution density) of the frame signal values: </li></ul><br>  calculate entropy as <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/17c/51f/c47/17c51fc47432ee86449a72d230dfcd8c.png"></a>  ; <br><br><br><br>  <a href="">An example</a> . <br><br>  And so, we got the value of entropy.  But this is just one more characteristic of the frame, and in order to separate the sound from the silence, we still need to compare it with something.  In some articles, it is recommended to take the entropy threshold equal to the average between its maximum and minimum values ‚Äã‚Äã(among all frames).  However, in my case, this approach did not give any good results. <br>  Fortunately, entropy (in contrast to the same mean square of values) is a relatively independent value.  That allowed me to choose the value of its threshold in the form of a constant (0.1). <br><br>  Nevertheless, the problems do not end there: (Entropy can sag in the middle of a word (on vowels), and can suddenly jump because of a small noise. In order to deal with the first problem, you have to introduce the concept of ‚Äúminimum distance between words‚Äù and ‚Äúglue‚Äù near recumbent framesets, separated due to sagging. The second problem is solved by using the ‚Äúminimum word length‚Äù and cutting off all candidates that did not pass the selection (and not used in the first paragraph). <br><br>  If, in principle, speech is not ‚Äúarticulate‚Äù, one can try to split the initial frame set into certain prepared subsequences, each of which will be subjected to a recognition procedure.  But that's another story :) <br><br><br><br><h2>  Mfcc </h2><br><br>  And so, we have a set of frames corresponding to a certain word.  We can take the path of least resistance and use the mean square of all its values ‚Äã‚Äã(Root Mean Square) as the numerical characteristic of a frame.  However, such a metric carries very little information suitable for further analysis. <br><br>  This is where the Mel-frequency cepstral coefficients come into play.  According to Wikipedia (which, as you know, it does not lie), the MFCC is a unique representation of the energy of the signal spectrum.  The advantages of its use are as follows: <br><ul><li>  The spectrum of the signal is used (that is, decomposition over the basis of orthogonal [co] sinusoidal functions), which allows one to take into account the wave ‚Äúnature‚Äù of the signal in further analysis; </li><li>  The spectrum is projected onto a special <a href="http://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D0%25BB_(%25D0%25B2%25D1%258B%25D1%2581%25D0%25BE%25D1%2582%25D0%25B0_%25D0%25B7%25D0%25B2%25D1%2583%25D0%25BA%25D0%25B0">mel-scale</a> , allowing you to select the most significant frequencies for human perception; </li><li>  The number of calculated coefficients can be limited to any value (for example, 12), which allows you to ‚Äúcompress‚Äù the frame and, as a result, the amount of information processed; </li></ul><br><br><br>  Let's look at the process of calculating the MFCC coefficients for a frame. <br><br>  Imagine our frame as a vector <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/f87/6cd/e9a/f876cde9a001d2517d741e530662ea08.png"></a>  where N is the frame size. <br><br><br><br><h3>  Fourier series expansion </h3><br><br>  First of all, we calculate the signal spectrum using the discrete Fourier transform (preferably with its ‚Äúfast‚Äù FFT implementation). <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/a84/ce0/202/a84ce0202f7e8b66a5ad4ed307d785fc.png"></a> <br><br>  It is also recommended to apply the Hamming window function to the obtained values ‚Äã‚Äãin order to ‚Äúsmooth out‚Äù the values ‚Äã‚Äãat the frame boundaries. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/b70/26b/834/b7026b834cc9863a0f68b8b14785e61f.png"></a> <br><br>  That is, the result will be a vector of the following form: <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/b40/e90/5cc/b40e905cc2e32c323d01c70e2834f565.png"></a> <br><br>  It is important to understand that after this transformation along the X axis we have the frequency (hz) of the signal, and along the Y axis - the magnitude (as a way to get away from the complex values): <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/2c8/f43/97b/2c8f4397b4aa3129cead0623fe32106f.png"></a> <br><br><br><br><h3>  Calculation of mel filters </h3><br><br>  Let's start with what mel.  Again, according to Wikipedia, mel is a ‚Äúpsychophysical unit of pitch‚Äù, based on the subjective perception of the average people.  It depends primarily on the frequency of the sound (as well as on the volume and tone).  In other words, this value shows how much a sound of a certain frequency is ‚Äúsignificant‚Äù for us. <br><br>  You can convert the frequency into chalk using the following formula (let's remember it as ‚Äúformula-1‚Äù): <br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/02a/afa/54c/02aafa54cc690f09f823d317e3075a3b.png"></a> <br><br>  The inverse transform looks like this (remember it as ‚Äúformula-2‚Äù): <br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/279/67e/785/27967e7854e222fbd8137eac5728e468.png"></a> <br><br>  Plot of mel / frequency: <br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/9b7/4c1/98d/9b74c198d72f9794b70b5bb88ebc024e.png"></a> <br><br>  But back to our problem.  Suppose we have a frame size of 256 elements.  We know (from the audio format data) that the frequency of the sound in this frame is 16000hz.  Suppose that human speech is in the range of [300;  8000] hz.  The number of the desired chalk coefficients put M = 10 (recommended value). <br><br>  In order to decompose the spectrum obtained above on the mel scale, we will need to create a ‚Äúcomb‚Äù of filters.  In fact, each mel-filter is a <a href="http://en.wikipedia.org/wiki/Window_function">triangular window function</a> that allows you to sum up the amount of energy at a certain frequency range and thereby get a mel-coefficient.  Knowing the number of chalk coefficients and the analyzed frequency range, we can build a set of such filters: <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/17e/62e/d85/17e62ed852c896be2f74cd5cb263092b.png"></a> <br><br>  Note that the higher the number of the chalk factor, the wider the filter base.  This is due to the fact that the division of the frequency range of interest to the ranges processed by the filters occurs on the chalk scale. <br><br>  But we were distracted again.  And so for our case, the range of frequencies of interest to us is equal to [300, 8000].  According to the formula-1 in on a chalk-scale, this range turns into [401.25;  2834.99]. <br><br>  Further, in order to build 10 triangular filters, we need 12 reference points: <br><blockquote>  m [i] = [401.25, 622.50, 843.75, 1065.00, 1286.25, 1507.50, 1728.74, 1949.99, 2171.24, 2392.49, 2613.74, 2834.99] <br></blockquote><br><br>  Notice that the points on the chalk scale are evenly spaced.  Let's translate the scale back to hertz using formula-2: <br><br><blockquote>  h [i] = [300, 517.33, 781.90, 1103.97, 1496.04, 1973.32, 2554.33, 3261.62, 4122.63, 5170.76, 6446.70, 8000] <br></blockquote><br><br>  As you can see now the scale began to gradually stretch, thereby aligning the dynamics of growth of ‚Äúsignificance‚Äù at low and high frequencies. <br><br>  Now we need to impose the resulting scale on the spectrum of our frame.  As we remember, along the X axis we have the frequency.  The length of the spectrum of 256 - elements, while it fits 16000hz.  Having solved a simple proportion, you can get the following formula: <br><br><blockquote>  f (i) = floor ((frameSize + 1) * h (i) / sampleRate) <br></blockquote><br><br>  which in our case is equivalent to <br><br><blockquote>  f (i) = 4, 8, 12, 17, 23, 31, 40, 52, 66, 82, 103, 128 <br></blockquote><br><br>  That's all!  Knowing the reference points on the X axis of our spectrum, it is easy to construct the filters we need using the following formula: <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/8b4/c4d/93d/8b4c4d93d47063f2c2d1fd8ad53188f1.png"></a> <br><br><br><br><h3>  Apply filters, log energy spectrum </h3><br><br>  The application of the filter consists in pairwise multiplying its values ‚Äã‚Äãwith the values ‚Äã‚Äãof the spectrum.  The result of this operation is the mel coefficient.  Since we have M filters, the coefficients will be the same. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/93c/2a2/074/93c2a20743ed58b7db1e4f7a3091266a.png"></a> <br><br>  However, we need to apply mel-filters not to the values ‚Äã‚Äãof the spectrum, but to its energy.  After that, the results obtained are prologized.  It is believed that this reduces the sensitivity of the coefficients to noise. <br><br><br><br><h3>  Cosine transform </h3><br><br>  The discrete cosine transform (DCT) is used to get those same ‚Äúcepstral‚Äù coefficients.  Its meaning is to ‚Äúcompress‚Äù the results obtained, increasing the significance of the first coefficients and decreasing the significance of the latter. <br><br>  In this case, DCTII is used without any multiplication by <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/a22/5f1/ba3/a225f1ba3f16cc20c22af05b0fb1006d.png"></a>  (scale factor). <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/a9d/61f/4e6/a9d61f4e6894cb655f9bcce6dca08345.png"></a> <br><br>  Now for each frame we have a set of M mfcc coefficients that can be used for further analysis. <br><br>  Examples of code for overlying methods can be found <a href="">here</a> . <br><br><br><br><h2>  Recognition algorithm </h2><br><br>  Here, dear reader, the main disappointment awaits you.  I have seen on the Internet a lot of highly intelligent (and not so) debates about which method of recognition is better.  Someone is in favor of Hidden Markov Models, some are for neural networks, whose thoughts are in principle impossible to understand :) <br><br>  In any case, a lot of preferences are given to the <a href="http://ru.wikibooks.org/wiki/%25D0%25A1%25D0%25BA%25D1%2580%25D1%258B%25D1%2582%25D1%258B%25D0%25B5_%25D0%25BC%25D0%25B0%25D1%2580%25D0%25BA%25D0%25BE%25D0%25B2%25D1%2581%25D0%25BA%25D0%25B8%25D0%25B5_%25D0%25BC%25D0%25BE%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B8">SMM</a> , and it is their implementation that I am going to add to my code ... in the future :) <br><br>  At the moment, I propose to stop at a much less efficient, but at times more simple way. <br><br>  And so, remember that our task is to recognize a word from a certain dictionary.  For simplicity, we will recognize the naming of the first ten numbers: ‚Äúone‚Äú, ‚Äútwo‚Äú, ‚Äúthree‚Äú, ‚Äúfour‚Äú, ‚Äúfive‚Äú, ‚Äúsix‚Äú, ‚Äúseven‚Äú, ‚Äúeight‚Äú, ‚Äúnine‚Äú, ‚Äúten‚Äú. <br><br>  Now we will take an iPhone / android in our hands and go through L colleagues asking them to dictate these words under the entry.  Next, we assign (in some local database or a simple file) to each word L the sets of mfcc coefficients of the corresponding records. <br><br>  We will call this correspondence ‚ÄúModel‚Äù, and the process itself - Machine Learning!  In fact, the simple addition of new models to the database has an extremely weak connection with machine learning ... But the term is very painful :) <br><br>  Now our task is to choose the most ‚Äúclose‚Äù model for a certain set of mfcc coefficients (recognizable word).  At first glance, the problem can be solved quite simply: <br><ul><li>  for each model, we find the average (Euclidean) distance between the identified mfcc vector and the model vectors; </li><li>  we choose as the correct model, the average distance to which will be the smallest; </li></ul><br><br><br>  However, the same word can be pronounced both by Andrey Malakhov and some of his Estonian colleagues.  In other words, the size of the mfcc vector for the same word may be different. <br><br>  Fortunately, the task of comparing sequences of different lengths has already been solved in the form of the Dynamic Time Warping algorithm.  This dynamic programming algorithm is beautifully painted both in the bourgeois <a href="http://en.wikipedia.org/wiki/Dynamic_time_warping">Wiki</a> and in the Orthodox <a href="http://habrahabr.ru/post/135087/">Habr√©</a> . <br><br>  The only change that should be made to it is the method of finding the distance.  We have to remember that the mfcc-vector of the model is actually a sequence of mfcc- ‚Äúsubvectors‚Äù of dimension M, obtained from frames.  So, the DTW algorithm should find the distance between the sequences of these same ‚Äúsubvectors‚Äù of dimension M. That is, the distances (Euclidean) between mfcc- ‚Äúsubvectors‚Äù of frames should be used as the values ‚Äã‚Äãof the distance matrix. <br><br>  <a href="">An example</a> . <br><br><br><br><h2>  Experiments </h2><br><br>  I did not have the opportunity to test the work of this approach on a large ‚Äútraining‚Äù sample.  The results of tests on a sample of 3 specimens for each word in non-synthetic conditions showed, to say the least, the best result - 65% of correct recognitions. <br><br>  Nevertheless, my task was to create a maximum simple speech recognition application.  So say ‚Äúproof of concept‚Äù :) <br><br><br><br><h2>  Implementation </h2><br><br>  The attentive reader noted that the article contains many links to the GitHub project.  It is worth noting here that this is my first C ++ project since university.  This is also my first attempt to calculate something more complicated than the arithmetic mean since the same university ... In other words, it comes with absolutely no warranty (c) :) <br><br><br></div><p>Source: <a href="https://habr.com/ru/post/226143/">https://habr.com/ru/post/226143/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../226125/index.html">DevConf 2014 - tomorrow - June 14! The final conference program has been published.</a></li>
<li><a href="../226127/index.html">How does it feel to live in a star cluster?</a></li>
<li><a href="../226129/index.html">Internal USB charging</a></li>
<li><a href="../226131/index.html">Ilon Musk will open all Tesla Motors patents</a></li>
<li><a href="../226141/index.html">Why is it time to support only Android 4.0 and higher?</a></li>
<li><a href="../226145/index.html">Three months of use of the Samsung ativ smart pc xe700t1c tablet</a></li>
<li><a href="../226149/index.html">Intel will create the world's first smart city (USA)</a></li>
<li><a href="../226151/index.html">Onyx Boox M92M Perseus or 16 shades of gray</a></li>
<li><a href="../226157/index.html">Adding a Youtube video to an email newsletter</a></li>
<li><a href="../226161/index.html">By Dart: "We need more web programming languages"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>