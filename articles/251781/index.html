<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>We master technical vision on the example of Bioloid STEM and HaViMo2.0</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Good afternoon, dear readers of Habr! With this article I open a series of publications on robotics. The main areas of focus of the articles will be t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>We master technical vision on the example of Bioloid STEM and HaViMo2.0</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/19c/06f/bb4/19c06fbb454349fdb3c248655e8542ab.png" alt="image"><br><br>  Good afternoon, dear readers of Habr!  With this article I open a series of publications on robotics.  The main areas of focus of the articles will be the description of practical implementations of various tasks - from the simplest programming of robots to the implementation of navigation and autonomous behavior of the robot in various conditions.  The main purpose of these articles is to show and teach how simple it is to solve a particular application task, or how to quickly adapt your robotic set to specific conditions.  I will try to use the kits available and common in the market so that many of you can use my solutions and refine them for your own purposes.  We hope that these articles will be useful both to students of various educational institutions and teachers of robotics. <br><a name="habracut"></a><br>  <b>Instead of the preface</b> <br><br>  The work of modern mobile robots is often associated with constant and active movement in a dynamic (changeable) environment.  Currently, due to the intensive robotization of the service sector, for example, the introduction of robokars in production, service robots for contact with people, there is a serious need to create such robots that could not only be able to move along predetermined routes and detect obstacles, but also to classify them in order to adapt flexibly to changing environments if necessary.  This task can and should be solved using technical vision.  In this paper, I propose to deal with the main points in the implementation of a technical vision using the example of using a machine from the <a href="http://robotgeeks.ru/collection/bioloid/product/robotis-bioloid-stem-standard">Bioloid STEM robotic set</a> and the HaViMo2.0 camera. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      At present, such a movement of robots is widespread, as is driving along a line.  This is widely used both at the factories in the AGV (Automatic Guided Vehicle) - robots move along pre-drawn lines, and when organizing robotic competitions - you can often meet tasks one way or another connected with orientation along lines.  The main sensors with which the robot receives data are infrared sensors, which determine the difference in the contrast of the line and the ambient background, and magnetic sensors, which are used in the case of magnetic lines.  The above examples of solutions require both thorough maintenance during operation and certain purity of the environment: for example, in a dirty warehouse, lines can get dirty and no longer recognized by IR sensors, and, for example, in metalworking magnetic lines can become clogged with iron chips.  In addition to all this, it is assumed that the robot moves along predetermined routes, on which there should be no obstacles in the form of people whom it can cripple or in the form of any other objects.  However, as a rule, there are always many moving objects in production that can potentially become an obstacle for the robot, so the robot should always not only receive information about its route and moving objects, but also have to analyze the environment so that, if necessary, be able to competently respond on the situation. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/d74/78f/ddd/d7478fddd19948bf9541cd05628b09ce.jpg" alt="image"></div><br>  <i>Figure 1. Warehouse robots moving along lines</i> <br><br>  When developing mobile robots, it is necessary to consider what tasks are set before the mobile robot in order to select the necessary sensor devices for their solution.  For example, to solve the problem of moving within the working area, the robot can be equipped with expensive laser scanning range finders and GPS devices to determine its own position, while to solve the problem of local navigation, the mobile robot can be equipped with simple ultrasonic or infrared sensors around the perimeter.  However, all the above means cannot give the robot a complete picture of what is happening around him, since the use of various distance sensors allows the robot to determine the distance to objects and their dimensions, but they do not allow to determine their other properties - shape, color, position in space, which leads to the impossibility to classify such objects by any criteria. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/6f1/2e7/c0b/6f12e7c0b9494aa3864f997154394e75.png" alt="image"></div><br><br>  To solve the above problem, vision systems come to the rescue, allowing the robot to obtain the most complete information about the state of the environment around it.  In essence, the technical vision system is the ‚Äúeyes‚Äù of the robot, capable of using the camera to digitize the surrounding area and provide information about the physical characteristics of objects located in it in the form of data about <br>  - sizes <br>  - location in space <br>  - appearance (color, surface condition, etc.) <br>  - labeling (recognition of logos, barcodes, etc.). <br><br>  The resulting data can be used to identify objects, measure their characteristics, as well as manage them. <br><br>  The vision system is based on a digital camera that captures the surrounding space, then the data is processed by the processor using a specific image analysis algorithm to isolate and classify the parameters of interest to us.  At this stage, the data are prepared and output in a form suitable for processing by the robot controller.  Then the data is transmitted directly to the robot controller, where we can use it to control the robot. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/2cb/9ac/d9a/2cb9acd9ab704263b573e67c5f7378f9.jpg" alt="image"></div><br>  <i>Figure 2. The use of a vision system to monitor road conditions</i> <br><br>  As I mentioned earlier, our solution will be based on the popular robotic designer produced by Korean firm Robotis, namely the designer Bioloid STEM. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/415/b3c/2b1/415b3c2b12ca4f0b88224aa662d58080.jpg" alt="image"></div><br>  <i>Figure 3. Constructor Bioloid STEM</i> <br><br>  This constructor contains components that allow you to assemble one of the 7 basic models of robots.  I allow myself not to dwell on the detailed description of this set, since there are enough reviews of varying degrees of completeness on the network, for example, the <a href="http://robotgeeks.ru/blogs/articles/obzor-konstruktora-bioloid-stem">Review of the designer Bioloid STEM</a> .  As a working model, we will use the standard set configuration - the Avoider machine. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/6a1/e86/767/6a1e867677ee4f658f86b158265705c1.png" alt="image"></div><br>  <i>Figure 4. Avoider is one of the 7 standard Bioloid STEM configurations</i> <br><br>  As the basis of the vision system, we will use the HaViMo2.0 image processing module, built on the basis of a color CMOS camera.  This module is specifically designed for use with low-power processors.  In addition to the camera, this module is equipped with a microcontroller that performs image processing, so the controller's resources for the image processing itself are not wasted.  Data output is carried out through the serial port. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/efc/0f1/bd6/efc0f1bd67a746bcbedc527816419c6b.jpg" alt="image"></div><br>  <i>Figure 5. HaViMo2.0 image processing module</i> <br><br><div class="spoiler">  <b class="spoiler_title">Characteristics of the module</b> <div class="spoiler_text">  ‚Ä¢ Built-in color CMOS camera: <br>  o Resolution: 160 * 120 pixels <br>  o Color depth: 12 bit YCrCb <br>  o Frame rate: 19 frames per second <br>  ‚Ä¢ Saving the image processing parameters in the EEPROM, there is no need to configure the camera every time after power on. <br>  ‚Ä¢ Automatic / manual exposure, gain and white balance <br>  ‚Ä¢ Adjustable Hue / Saturation <br>  ‚Ä¢ Image processing based on color <br>  ‚Ä¢ Built-in color reference table <br>  ‚Ä¢ Save calibration parameters to internal memory, no need to recalibrate after power on <br>  ‚Ä¢ Able to distinguish up to 256 objects <br>  ‚Ä¢ The composition includes tools for 3D viewing and editing. <br>  ‚Ä¢ Supports the ability to display calibration results in real time <br>  ‚Ä¢ Supports implementation of the Region-Growing algorithm in real time <br>  ‚Ä¢ Detects up to 15 contiguous areas in a frame <br>  ‚Ä¢ Defines color, number of pixels and area borders. <br>  ‚Ä¢ Defines color and pixel counts for each 5x5 cell. <br>  ‚Ä¢ Supports the output of the raw image in the calibration mode <br>  ‚Ä¢ Frame rate for interlaced output - 19 FPS <br>  ‚Ä¢ Full frame refresh rate - 0.5 FPS <br>  ‚Ä¢ Supported hardware: <br>  ÔÇß CM5 <br>  ÔÇß CM510 <br>  ÔÇß USB2 Dynamixel <br><br></div></div><br>  As you can see, HaViMo2.0 does not have official support for the CM-530 controller included in the Bioloid STEM kit.  Nevertheless, it is possible to connect CM-530 and HaViMo2.0, but first things first. <br>  First of all, we need to somehow install the image processing module on the typewriter.  And the first thing that comes to mind is to fix the camera as shown on the models below.  By the way, these models were obtained using the modeling environment from Bioloid - R + Design.  This environment is ‚Äúsharpened‚Äù especially for modeling robots from components offered by Bioloid.  What is both good and bad at the same time.  Since there is no image module in the R + Design parts collection, I have replaced it with a simulator with a similar-sized plate.  And framed: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/bc4/466/ec1/bc4466ec144f429395769f32e5926ccd.png" alt="image"></div><br>  <i>Figure 6. The location of the HaViMo2.0 image processing module on Avoider</i> <br><br>  Well, it should not look bad, but it is necessary to take into account another important point - the viewing angles of the camera module, because  the image that the module will process will depend on it.  Since the manufacturer HaViMo2.0 did not specify camera angles in the specification, I had to determine them myself.  I do not pretend to high accuracy of determination, but the results I got are the following: <br><br>  <u>Viewing angles HaViMo2.0:</u> <br>  horizontal - 44 ¬∞ (no, I will not round up to 45 ¬∞, since I already rounded up to 44 ¬∞) <br>  vertical - 30 ¬∞ <br><br>  I can say in advance - the results of the viewing angles correlated well with reality, so I will consider them as workers.  Now, having the values ‚Äã‚Äãof viewing angles, I will formulate the task of locating the module as follows: <br><br>  <i>At what angle from the vertical it is necessary to reject the module at the location shown above in order to minimize the field of view, but at the same time exclude the elements of the robot from entering the field of view of the camera?</i> <br><br>  I decided to exactly minimize the scope for the following reasons: <br>  - first, in practice, to check what comes of it <br>  - secondly, to facilitate the subsequent calibration of the module for yourself (the less various objects fall within the scope, the easier it is to calibrate the module) <br><br>  Let's draw our task on the plane (linear dimensions are indicated in mm, angular dimensions are in degrees): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/d57/92f/da9/d5792fda993547009c0a266efd786080.png" alt="image"></div><br>  <i>Figure 7. Determining the angle of deviation of the module from the vertical</i> <br><br>  We know the position of the camera (C), we know the size of the front of the robot (AB), and we also know the vertical viewing angle (marked with red lines).  Since there is nothing more complicated than the geometry of the 7th class, I allow myself not to lead the calculation (read - calculations) and say the answer immediately - there should be a 52 ¬∞ angle between the plane of the camera and the vertical (thanks to CAD for accuracy).  When calculating manually, I got a value of 50 ¬∞, so we will consider the results acceptable. <br>  In addition to the angle, we also obtained that the visible area of ‚Äã‚Äãthe camera horizontally is about 85 mm. <br><br>  The next step is to estimate how large the objects should be in order for them to be in the camera's field of view.  We do this for the width of the line that the robot will have to track and move along it.  Draw a drawing similar to the previous one for horizontal projection: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/31b/056/948/31b0569485ec4348b14023c944bf912d.png" alt="image"></div><br>  <i>Figure 8. Visible area.</i>  <i>Horizontal projection.</i> <br><br>  On this drawing, we again know the point of location of the camera (C), the size of the front of the robot (CO), the viewing angle horizontally, and we already know the length of the visible area, which we obtained in the previous step.  Here we are primarily interested in the length of the segment AB, which shows the limiting value of the width of the object that the camera can fix.  As a result, we have - if the robot goes along the line, then with this arrangement of the camera the line width should not exceed 31 mm.  If the line is wider, we will not be able to accurately track its behavior. <br><br>  Using the protractor and improvised means, I tried to fix the camera module as it was intended and this is what came of it: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/da2/bb4/350/da2bb4350e2c4a138a298d65275d3ab4.jpg" alt="image"></div><br>  <i>Figure 9. Harsh reality - a living photo of the resulting structure</i> <br><br>  It's time to start working directly with the HaViMo2.0 module.  Personally, I want to say from myself that this module is very capricious.  Therefore, if something did not work for you, then try again.  And better two. <br>  <u><b>Very important!</b></u>  The camera must be connected to the Dynamixel port located at the end of the controller between the battery connector and the Communication Jack. <br>  To work with the camera and the subsequent programming of the robot, we need the following set of software: <br>  - RoboPlus version 1.1.3.0 ( <a href="http://en.robotis.com/BlueAD/board.php%3Fbbs_id%3Ddownloads%26mode%3Dview%26bbs_no%3D1132559%26page%3D1%26key%3D%26keyword%3D%26sort%3D%26scate%3D">download from the manufacturer‚Äôs website</a> ) <br>  - HaViMoGUI version 1.5 ( <a href="http://sourceforge.net/projects/havimo/">download</a> ) <br>  - calibration firmware for CM-530 controller ( <a href="http://www.havimo.com/%3Fp%3D130">download</a> ) <br><br>  Let's start with the preparation of the controller.  Since the CM-530 controller bundled with the Bioloid STEM robotic kit is not officially supported by the image processing module, we need to use a custom firmware for the controller.  So we reflash the controller.  We connect the controller to the computer, turn it on with a toggle switch and perform the following actions: <br><br>  1. Open RoboPlus <br>  2. In the Expert tab, launch RoboPlus Terminal <br>  3. We will see an error message, do not be alarmed and click OK.  Note that No recent file is written in the lower left corner and Disconnect is written in the lower right corner: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/806/865/1ac/8068651ac7ac4a368bec0a8ae4b1457f.png" alt="image"></div><br>  <i>Figure 10. Preparing the controller.</i>  <i>Steps 1-3</i> <br><br>  4. Select the item Connect in the Setup menu.  A window will appear with a choice of port and its setting.  Select the COM port to which the controller is connected.  Specify the speed - 57600. Click Connect.  If the connection is successful, in the lower right corner Disconnect will change to the following: COM8-57600. <br>  5. Without disconnecting the controller from the computer, turn it off with the POWER switch.  Then we will make sure that we have the English keyboard layout set, hold down the Shift and 3 keys simultaneously (the three of them marked with #) and, without releasing the buttons, turn on the controller with a toggle switch.  If everything is done correctly, system information will appear in the terminal window.  Press Enter once. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/639/362/443/6393624439d54d5cbdf46f568a0bd25e.png" alt="image"></div><br>  <i>Figure 11. Preparing the controller.</i>  <i>Steps 4-5</i> <br><br>  6. Make sure that we have no wires hanging anywhere and the USB cable is securely connected to the computer and the controller.  Further, it is forbidden to disconnect the controller from the PC and remove power from it.  We will enter into the terminal the command l (English small L).  Press Enter. <br>  7. Select the menu item Transmit file.  Select the downloaded custom firmware CM-530.hex and click the Open button. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/78f/850/021/78f850021d664ddbb5de8c0b43437a9c.png" alt="image"></div><br>  <i>Figure 12. Preparing the controller.</i>  <i>Steps 6-7</i> <br><br>  8. After the download is complete, enter the GO command and press Enter.  The terminal can be closed, the controller is disconnected from the computer. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/3ba/6f6/5d8/3ba6f65d8b2f4dd3b83643dac03afd48.png" alt="image"></div><br>  <i>Figure 13. Preparing the controller.</i>  <i>Step 8</i> <br><br>  Our CM-530 controller is ready to work with the HaViMo2.0 module.  Externally, it will not look very good - only one red LED will be lit, and it will not give any other signs of life.  Do not be scared.  This is normal.  Now let's start calibrating our image processing module.  Open HaViMoGUI, and the launch is carried out on behalf of the administrator.  We will see the following window: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/2aa/f9c/e12/2aaf9ce12e894d9d8d11755a001c2bed.png" alt="image"></div><br>  <i>Figure 14. HaViMoGUI front panel</i> <br><br>  In the Connection Type item, select the CM-510 controller, in the Connection Port - the COM port number to which the controller is connected.  Click Connect.  In case of a successful connection, we will see Port Opened in the lower left corner of the window.  Now connect directly to the camera.  To do this, click <b>Check Camera</b> .  If the connection to the camera was successful, then in the lower left corner we will see the inscription Check Camera: HaViMo2 Found.  If such a label does not appear, check that the camera is connected to the controller correctly and repeat the steps starting from the launch of HaViMoGUI. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/43f/793/454/43f793454ad24c8d8906f78d168cf83a.png" alt="image"></div><br>  <i>Figure 15. Camera connected successfully</i> <br><br>  Once connected to the camera, go to the <b>Camera Settings</b> tab and activate <b>Auto Update Settings</b> , which will allow the camera to automatically select the brightness / contrast (and other settings) of the image, depending on the brightness of the object. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/61a/ac5/3bd/61aac53bdcf3486c97b626c3b218a2c0.png" alt="image"></div><br>  <i>Figure 16. Setting auto-settings</i> <br><br>  Then go back to the <b>Color Look Up Table</b> tab and click <b>Sample Image</b> to capture the image from the camera.  As a result, we obtain an image of our object at the top of the window in the <b>Original Image</b> section.  In the <b>Color Look Up Table</b> tab (right), the shades recognized by the camera appeared in the form of a 3D gradient.  The camera recognizes an object by its color.  At the same time, you can memorize 7 different objects (+ background).  In order to memorize a color, in the <b>Color Look Up Table</b> tab, select <b>Color 1</b> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/bf8/365/61e/bf836561eb4b4865b107d86c851beaa2.png" alt="image"></div><br>  <i>Figure 17. Capturing an image from a camera</i> <br><br>  You can select a color hue using a 3D gradient (click on the appropriate box) or by clicking on a specific <b>Original Image</b> point.  In our case, we choose the white line, on which our robot will go.  By clicking on <b>Region Growing</b> , we will see the recognition area that the module recognized.  The simplest settings are made.  In order to remember them, press the button <b>Flash LUT</b> .  If the status bar says LUT Written Successfully, then the settings have been saved successfully.  You can also save the settings to a separate file by clicking on the <b>Save LUT to File</b> button, to then read them from the file without first selecting the desired colors.  The <b>Read LUT</b> button allows you to read the current camera settings. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/110/f71/d01/110f71d010674e19bcc24ec3efee27d1.png" alt="image"></div><br>  <i>Figure 18. Saving module settings</i> <br><br>  Setting up the image recognition module is solved.  You can return the controller to the world of living and working controllers.  To do this, launch RoboPlus, open the RoboPlus Manager and select the <b>Controller Firmware Manager</b> icon in the top line.  Click <b>Next</b> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/daf/8fb/c0a/daf8fbc0ade843edaa8d75f32a6777b4.png" alt="image"></div><br>  <i>Figure 19. Restoring controller firmware</i> <br><br>  Then we execute everything that the utility asks for us: in the window that appears, click on <b>Find</b> , then turn off and turn on the controller with the toggle switch.  The controller should be detected by the utility, and then click <b>Next</b> twice. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/26c/e97/e2f/26ce97e2fcf049f58589af235138c45f.png" alt="image"></div><br>  <i>Figure 20. Controller successfully detected</i> <br><br>  We are waiting for the installation of the firmware to finish, click <b>Next</b> , then <b>Finish</b> .  After that, with a probability of 99%, we see an error that we don‚Äôt pay attention to - just close the window.  This error occurs due to incorrect identification of the image recognition module connected to the controller.  The controller is back in the living world - you can start programming it to control our robot. <br><br>  We will develop the firmware in a standard environment from Robotis - RoboPlus Task.  Programming in the RoboPlus Task is carried out using a specialized language similar to the C programming language. For the convenience of the user, RoboPlus implements the basic dialing capabilities in the form of graphic blocks, such as timers, data processing units from sensors, data transfer units between devices, etc.  I will not dwell on the description of the programming process, as the environment is quite intuitive and does not cause difficulties.  However, if you have any questions, I recommend to look at the development examples in the "Articles" and "Lessons" on our <a href="http://robotgeeks.ru/blogs/lessons">website</a> .  But before we start programming, let's look at how data is exchanged between the controller and the HaViMo2.0 module. <br><br>  The connection between the module and the controller is implemented using the same protocol, which is used to exchange data between the controller and the Dynamixel AX-12 servos.  The module itself is built around the DSP (Digital signal processor), which is configured through access to its registers, each of which can be read or configured using a calibration interface.  A detailed description of the DSP settings is given in the manual modulo HaViMo2.0.  There is also a complete list of registers.  For us to work with the image processing module, it is enough to understand how data is output from the module. <br><br>  The following example shows the output of the image processed by the module.  Up to 15 areas with the address 0x10 to 0xFF can be read using the READ DATA (0x02) command.  Each area consists of 16 bytes, which consist of the following parts: <br>  - Region Index - Region Index: contains the value 0 if the region is not selected <br>  - Region COLOR - Region Color <br>  - Number of Pixels - Number of pixels: the number of detected pixels within the region. <br>  - Sum of X Coordinates - Sum of Coordinates X: The result of adding the X coordinates of all detected points.  Can be divided by the number of pixels to calculate the average X. <br>  - Sum of Y Coordinates - The sum of Y coordinates: the result of adding the Y coordinates of all detected points.  Can be divided by the number of pixels to calculate the average Y. <br>  - Max X: Bounding side line to the right. <br>  - Min X: Limiting side line to the right. <br>  - Max Y: Limit line from below. <br>  - Min Y: Limit line on top. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/6e4/a24/cb4/6e4a24cb4c17498abffff25931780dc5.png" alt="image"></div><br>  <i>Figure 21. An example of image output from the HaViMo2.0 module</i> <br><br>  This example is enough to start programming the robot.  In the RoboPlus Task environment, we write a function that will be responsible for determining the boundaries of the recognized area.  Since this environment does not allow to display the code in a digestible text form, you will have to post screenshots.  Let's call our function Get_Bounding_Box.  The algorithm of the function is as follows: <br>  - we sequentially poll the regions (Index) from 1 to 15 in a cycle, at each iteration of which we change the value of the address (Addr) in accordance with the rule specified above. <br>  - if we find the recognized area (its index is not equal to 0), then we check it for compliance with the specified color (Color) <br>  - if its color matches the specified one, we fix the size of the area (Size) and the values ‚Äã‚Äãof the bounding lines of our area (Maxx, Minx, Maxy, Miny) <br><div style="text-align:center;"><img src="https://habrastorage.org/files/426/a31/540/426a315409d64e8ab888dda7d379dd8a.png" alt="image"></div><br>  <i>Figure 22. Function code for determining the boundaries of the recognized area</i> <br><br>  Having obtained the values ‚Äã‚Äãof the size of the area and the coordinates of the bounding lines, you can proceed to programming the movement of the robot.  The algorithm of the robot movement is quite simple, knowing the coordinates of the restrictive lines in X, we find the average X. When moving, if the average X is displaced, the robot will turn in the right direction, thereby self-positioning itself on the line along which it is traveling.  Graphically, the algorithm for determining the movement of the robot is shown in the following figure: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/823/c35/00f/823c3500f14849f785ff6c032afcc0e1.png" alt="image"></div><br>  <i>Figure 23. Algorithm for determining the movement of the robot</i> <br><br>  And its text-block implementation: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/078/cf1/bdc/078cf1bdcdc94d33bfc14ad2b11ca94d.png" alt="image"></div><br>  <i>Figure 24. Code of the main function of the program</i> <br><br>  So, to do this, when we turn on the controller, we first of all initialize our variables - the movement speeds, the forward movement time, the time during which the robot will perform the turn, and the time for pause.  Further, in an infinite loop, we initialize the connection to the camera (specify the port number to which it is connected - CamID) and the color of the area, which we will detect.  As we remember, during calibration we chose Color 1, so now we indicate it as interesting to us.  After that, we turn to the camera and pause for 150 ms, because after the first call the camera does not respond for at least 128 ms.  I put the function to initialize the camera connection in the main program loop, since the camera is very capricious and it may want to ‚Äúfall off‚Äù.  Initializing it at each iteration of the loop, we ‚Äúping‚Äù it, as it were.  Then we call the function described above to determine the coordinates of the bounding lines of the recognized region and, based on the obtained values, we find the mean (Cx).  And then we compare the obtained value for compliance with the conditions - if the obtained value is in a certain range, it means the robot goes straight, if it has shifted to the right - the robot turns it to the right, if it is to the left - accordingly to the left.  The functions responsible for the movement are presented below: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/9ff/a8e/78f/9ffa8e78f14949db9a4f7fbb6673a0de.png" alt="image"></div><br>  <i>Figure 25. Examples of functions that are responsible for moving forward and for turns to the left - to the right.</i> <br><br>          ‚Äì   ,      ,   .        2  : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/8ce/c21/bcc/8cec21bcc96c4714a39f32828aa1a399.png" alt="image"></div><br> <i> 26.     </i> <br><br>  Everything.       .       ,  .    : <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/DG0FlhsrRjA%3Ffeature%3Doembed&amp;xid=17259,15700019,15700186,15700191,15700253&amp;usg=ALkJrhiv7KHDW6-aa9KJ6BSmSf7K7raFSA" frameborder="0" allowfullscreen=""></iframe><br><br> ,            , , ,           .   ,         .     HaViMo2.0  : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/fd6/349/6a9/fd63496a9bc0495db55b761ab7344345.png" alt="image"></div><br> <i> 27.    </i> <br><br>   ,     ,       : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/33b/d37/b1f/33bd37b1fcf44d65b339164d65a78749.png" alt="image"></div><br> <i> 28.     </i> <br><br>  ,   ,      ,       : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/557/9af/0bc/5579af0bc59d465085a5e3b023a73fac.png" alt="image"></div><br> <i> 29.      .</i> <br><br>        : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/8ec/840/59e/8ec84059ef924a4f859f18d38e78cde5.png" alt="image"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 30. Interpretation of the recognition area in relation to our task.</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Well, this is all great, but the camera does not allow us to transmit video in real time so that you can visually see what the camera sees and how it recognizes it. But I really wanted to track at least schematically what the camera recognizes, and how the robot behaves. For this, I used the LabView environment, with which I organized data collection from the CM-530 controller and its processing (For more information on how to collect data from the CM-530 controller, see my article </font></font><a href="http://habrahabr.ru/post/223395/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúLabView in robotics - creating a SCADA system to control the robot‚Äù</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">). </font><font style="vertical-align: inherit;">Not that the utility turned out beautiful, but, in my opinion, quite useful for analyzing the movement of the robot. </font><font style="vertical-align: inherit;">Since I did not have at hand any wireless adapter - neither ZigBee, nor Bluetooth, I had to drive data about the USB wire, which somewhat hampered my actions. </font><font style="vertical-align: inherit;">The interface is as follows:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/cc2/584/66c/cc258466cfc44b2daee07afa5d22d667.jpg" alt="image"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 31. The interface of the application for analyzing the movement of the robot</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> And marking the recognition area, with explanations, in relation to our task:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/2e6/a43/c9d/2e6a43c9dc7c4c38ae1bf912328c4a31.jpg" alt="image"></div><br> <i> 32.   ,    </i> <br><br>     : <br> ‚Äî   :    .     ,   ‚Äì  .   ‚Äì  ,         (      29.    ‚Äì   .            ‚Äì      ,   ,    ,      ‚Äì     . <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- the lower left window shows the position of the robot relative to the track in real time. Red lines, again, indicate the boundaries of our route, green dotted - middle. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- the lower right window shows the real movement of the robot - shooting the movement of the robot with a mobile phone camera. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- in the upper right, there are elements of the connection settings and indicators for displaying data from the robot, as well as a window for displaying an error if it occurs. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But it‚Äôs not interesting to look at the static picture, so I suggest you watch the video, where the whole process of the robot movement is clearly visible:</font></font><br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/dvmBI-r2oGY%3Ffeature%3Doembed&amp;xid=17259,15700019,15700186,15700191,15700253&amp;usg=ALkJrhjLKcCxOC-EUErNcUYkOEuW-uNvXQ" frameborder="0" allowfullscreen=""></iframe><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Now it is possible to analyze the movement of the robot in some detail. You can see the following things: </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- the scope is reduced when the robot turns corners </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- shadows (for example, from the wire) can interfere with recognition, again reducing the recognition area, so I highlighted for myself and want to offer you some recommendations that need to be considered when programming such a robot : </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- ‚Äúless is better, but more‚Äù - the lower the speed of movement and the less distance the robot travels per unit of time, the better, because the less likely the robot is to skip (‚Äúnot see‚Äù) the turn, or leave the route badly; </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- follows from the first recommendation - the sharper the angle of rotation, the greater the likelihood that the robot will miss it;</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- our main enemy is floor covering. At this point, think for yourself what to do - I personally, for driving along linoleum, attached a sheet of paper to the lower part of the robot, since the resistance and creaking were terrible; </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- Murphy's law - if a robot travels along a light line against a dark background and there is an object in the neighborhood that can be called light - know: for a robot, it will be the brightest, and he will gladly run to it; </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- a consequence of the Murphy law - no matter how you reduce the camera's field of view, the robot will always find such a bright object; </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- the robot can be afraid of its own shadow, so always think about how the light will fall, and train the recognition module to work in different lighting conditions;</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- Be prepared for the fact that the batteries are discharged at the most inopportune moment, and outwardly it will look as if you have an error in the control program, the controller firmware, the controller itself and DNA. So always control the battery charge - for such a robot the normal voltage is 8-9V. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Well, in conclusion I would like to say that there is no limit to perfection - so everything is in your hands. Go ahead for adventures in the field of robotics!</font></font><br><br>  <b>Conclusion</b> <br>             <a href="http://robotgeeks.ru/collection/bioloid/product/robotis-bioloid-stem-standard"> Bioloid STEM</a>      ‚Äì HaViMo2.0.              .   ,   ,     ‚Äì   ,    ,         ‚Äì     . ,       .  See you again! </div><p>Source: <a href="https://habr.com/ru/post/251781/">https://habr.com/ru/post/251781/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../251755/index.html">Visualize it</a></li>
<li><a href="../251757/index.html">Work on the modernization of the explorer in ReactOS completed</a></li>
<li><a href="../251759/index.html">Some interesting and useful things for web developer # 40</a></li>
<li><a href="../251765/index.html">Filling text templates with model-based data. Implementing on .NET using dynamic functions in bytecode (IL)</a></li>
<li><a href="../251777/index.html">Feedback of the PVS-Studio team on the C ++ Russia conference, 2015</a></li>
<li><a href="../251783/index.html">Introduction to developing a slideshow in JavaScript</a></li>
<li><a href="../251793/index.html">Programming Philosophy 5 - Hummingbirds and Reactos</a></li>
<li><a href="../251795/index.html">Visual Studio Extensibility. Part One: MSBuild</a></li>
<li><a href="../251797/index.html">Static hosting: life after death narod.ru</a></li>
<li><a href="../251801/index.html">SummaryJS Release 3</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>