<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Skyforge rendering technology</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! My name is Sergey Makeev, and I am the Technical Director in the Skyforge project in the Allods Team, the game studio Mail.Ru Group. I would li...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Skyforge rendering technology</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/0aa/050/406/0aa050406432462d8bad41a05e225f44.jpg"><br><br>  Hello!  My name is Sergey Makeev, and I am the Technical Director in the Skyforge project in the Allods Team, the game studio Mail.Ru Group.  I would like to tell you about rendering technologies that we use to create graphics in Skyforge.  I'll tell you a little about the tasks that we faced when developing Skyforge from the point of view of a programmer.  We have our own engine.  Developing your technology is expensive and difficult, but the fact is that at the time of launching the game (three years ago) there was no technology that could satisfy all our needs.  And we had to create the engine from scratch. <br><a name="habracut"></a><br>  The main art-style game is a mixture of fantasy with Sci-Fi.  To realize the ideas of the art director and artists, we needed to create a very strong, powerful system of materials.  The player can see the manifestations of magic, technology and natural phenomena, and the system of materials is needed to plausibly draw all this on the screen.  Another ‚Äúpillar‚Äù of our graphic style is that we create a stylized reality.  That is, objects are recognizable and look realistic, but this is not photorealism from life.  A good example, in my opinion, is the film Avatar.  Reality, but reality is artistically embellished, reality and at the same time a fairy tale.  The next pillar of the graphic style - lighting and materials - look as natural as possible.  And "naturally" from the point of view of a programmer - this means "physically." <br><br>  An important point for a programmer: we need huge open spaces.  In the game there are both small locations-instansy, and huge areas in which there are many players.  In this video you can see what we have achieved at the moment.  Everything that is shown in the video is rendered using the game in the maximum settings. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/po61RUhOa9M%3Ffeature%3Doembed&amp;xid=17259,15700021,15700043,15700186,15700190,15700253&amp;usg=ALkJrhi1tcqBVZRHezsIZBxUCu5KSk_dHA" frameborder="0" allowfullscreen=""></iframe><br><br><img src="https://habrastorage.org/files/6fe/279/ddc/6fe279ddc7c74b20ad265ae660a942af.jpg"><br><br><img src="https://habrastorage.org/files/837/efd/a57/837efda573d14ef8bae9a47e0abcc664.jpg"><br><br><img src="https://habrastorage.org/files/ff7/832/1b5/ff78321b586244c89fe309056220b9f4.jpg"><br><br><img src="https://habrastorage.org/files/682/1aa/2a7/6821aa2a715b4790967d47af7f2a18dd.jpg"><br><br><img src="https://habrastorage.org/files/50d/ddb/1d7/50dddb1d74d343d2a06084cc8075f9c5.jpg"><br><br><img src="https://habrastorage.org/files/5b7/4d4/c1a/5b74d4c1adff4669a9fca1e94640a05d.jpg"><br><br>  Next, I will talk about how we have achieved such a picture. <br><br><h1>  Physically based shading (shading based on the laws of physics) </h1><br>  Why do we need shading based on physics? <br><ul><li>  This gives us a more realistic and detailed picture.  3D models made by different artists look holistic in different types of lighting.  The picture is less falling apart, and lighting artists do not need to mix all the lighting options with all the 3D models. </li><li>  Materials can be customized separately from light.  That is, material artists and light artists can work in parallel.  Correctly tuned physical materials in any light look as expected, do not suddenly become black and overexposed, as it often happened before, with non-physical materials. </li><li>  The parameters of the material has become less, and they all have a physical meaning.  It is easier for artists to navigate these parameters.  Of course, first they have to relearn, but then the work becomes much more predictable. </li><li>  Compliance with the law of conservation of energy in the system of materials means that it is easier for lighting designers to work.  You can not, setting up the light, "scatter" the image into pieces. </li><li>  By the way, physical correctness does not commit us to photorealism.  For example, all the latest cartoons created by Pixar and Disney Studios are made using a physical render.  But at the same time there is no photorealism, but there is quite recognizable stylization. </li></ul><br><h1>  Process physics </h1><br>  Before programming anything, you must first understand the physics of the process.  What happens when light reflects off the surface?  In real life, unlike computer graphics, the surfaces are not smooth, but in fact consist of many small irregularities.  These irregularities are so small that their eyes are not visible.  However, they are large enough to affect light reflected from the surface.  Here in the picture I called it a micro-surface. <br><br><img src="https://habrastorage.org/files/5df/38f/8e2/5df38f8e2a414670abe9ecdd8e42040d.jpg"><br><br>  Here is an example from real life.  In the picture is a sheet of A4 paper under an electron microscope.  It can be seen that a sheet of paper actually consists of many intertwined wood fibers, but the eye does not distinguish them. <br><br><img src="https://habrastorage.org/files/6de/217/cbe/6de217cbef004d77a18e6a52ed13967e.jpg"><br><br>  For scale, I derived wavelengths for a different spectrum of illumination in nanometers and selected a square with a side of 100,000 nanometers on the scan of the paper.  Obviously, although we do not see these fibers, they affect the reflection of light. <br><br><img src="https://habrastorage.org/files/c69/408/f76/c69408f7683b4b529dcb496ecdf6cd2b.jpg"><br><br>  Even offline filmmakers can calculate lighting with such a level of detail.  This is a huge amount of computation. <br><br>  So, surface microgeometry.  Part of the light penetrates inside and is re-emitted after random reflections inside the material or is absorbed ‚Äî it is converted into heat.  Part of the incident light is reflected from the surface.  There is a difference in how different materials reflect light.  Two groups that behave differently are dielectrics and conductors (metals).  Light does not penetrate inside the metal, and almost all of it is reflected from the surface.  From dielectrics, light is mostly re-emitted, and a small amount of light is reflected ‚Äî about 5%. <br><br><img src="https://habrastorage.org/files/cd2/fdb/9fe/cd2fdb9fed254a158682db1eeafc5cd1.jpg"><br><br>  The theory is good, but we create a computer game and operate not with rays of light, but pixels on the screen.  In addition, each pixel is a large number of photons of light.  And all this light can be absorbed, reemitted or reflected.  This is what we have to calculate when designing shaders. <br><br><h1>  BRDF </h1><br>  With the physics of the process in general, we decided to move on to the mathematical model.  The main function that will allow us to determine what percentage of light was reflected, and which was reemitted or absorbed, is called the <b>BRDF</b> (Bidirectional Reflection Distribution Function) or in Russian <b>DFOS</b> (dual-beam reflectance function).  The purpose of this function is to calculate the amount of energy emitted towards the observer for a given incoming radiation.  In theory, this is a multidimensional function, which can depend on a large number of parameters 3D, 4D, 6D. <br><br>  In practice, we will consider the function of the two parameters <b>F (l, v)</b> , where <b>l</b> is the direction from the surface point to the light source, and <b>v</b> is the direction of gaze. <br><br><h1>  BRDF for diffused light (Diffuse) </h1><br>  For the model of reradiated light, we make several assumptions: <br><ul><li>  we can neglect the point of entry and exit of the beam, because  this is a very small value in our case; </li><li>  we assume that all re-radiated rays are uniformly distributed inside the hemisphere. </li></ul><br>  The behavior of the photon inside the material is very complex, and for the current development of computer graphics this is a normal approximation, moreover, it adequately corresponds to real physical measurements. <br><br><img src="https://habrastorage.org/files/8c6/ef7/2f1/8c6ef72f1598466e835ab8b9da25a9f8.jpg"><br><br>  We obtain the following function for the calculation of reradiated (scattered) light. <br><br><img src="https://habrastorage.org/files/704/ffa/3fa/704ffa3faeb8475cbd5a7169a97ad3b4.jpg"><br><br>  <b>l</b> is the direction to the light source, <b>v</b> is the direction of gaze, it is not used in any way in this simplified model, since  all energy is reemitted uniformly over the hemisphere. <br><br>  <b>albedo (rgb)</b> - determines how much energy the surface absorbs, and how much re-emits.  So, for example, a surface with absolutely black albedo absorbs all energy (converts it into heat).  In fact, this is known to all <b>dot (n, l)</b> graphical programmers, with the exception of the division by <b>PI</b> .  The division by <b>PI is</b> necessary to comply with the law of energy conservation.  Since  the light is scattered over the hemisphere, then with n equal to <b>l</b> , we will reflect the incident light without changing the intensity in all directions along the hemisphere, which is physically impossible.  But usually the intensity of the light source transmitted to the shader already takes into account the division by <b>PI</b> , therefore only the <b>dot (n, l)</b> remains in the shader. <br><br>  We know that the dot product of vectors (dot) is the cosine between these vectors.  The question arises: how does the angle of incidence of light affect the amount of reradiated light?  The answer is simple: the projection area depends on the angle of incidence of the beam on the surface and is equal to the cosine of the angle of incidence.  Accordingly, the sharper the angle of incidence, the less energy gets to the surface. <br><br><img src="https://habrastorage.org/files/c93/fff/42a/c93fff42a81e42d5abc6e3398d004f31.jpg"><br>  <i>Light falls at a "blunt" angle.</i> <br><br><img src="https://habrastorage.org/files/b9e/96f/10b/b9e96f10b5aa4b469697cc82cb1c4472.jpg"><br>  <i>Light falls at an acute angle, the projection area has become larger</i> <br><br><h1>  BRDF for reflected light (Specular) </h1><br>  Let us turn to the model of reflected light.  Everyone knows that the angle of incidence is equal to the angle of reflection, but in the above picture there are many reflected rays.  This is due to the irregularities of the surface.  Due to these irregularities, each individual photon is reflected slightly in different directions.  If the surface is sufficiently smooth, then most of the rays are reflected in one direction and we see a clear reflection of objects, such as in a mirror. <br><br>  If the surface is more rough, then a much larger number of rays are reflected in different directions, and then we see a very dull reflection.  For a highly irregular surface, the reflected light can be evenly distributed inside the hemisphere and look like a diffused light to an external observer. <br><br><h1>  Microfacet theory </h1><br>  Microfacet Theory was developed to simulate the effect of reflection of light from the surface, taking into account the surface microgeometry.  This is a mathematical model that represents the surface as a set of microfaces oriented in different directions. At the same time, each of the microfaces is an ideal mirror and reflects light according to the same simple law: the angle of incidence equals the angle of reflection. <br><br>  In order to calculate the illumination at a point, we need to calculate the sum of the contribution of the reflected light of each of the micrograins.  Those.  we need an integral.  There are a lot of microfaces at a point, and we cannot simply integrate numerically.  We will find the solution of the integral analytically (approximately).  Here is what the function for calculating reflected light looks like in general. <br><br><img src="https://habrastorage.org/files/55f/5a1/f11/55f5a1f11bb84e1599715aafeca48b5b.jpg"><br><br>  This is a function of Cook-Torrens reflected light. <br>  <b>l</b> - the direction of light <br>  <b>v</b> - the direction of view of the observer <br>  <b>n</b> - normal to the surface <br>  <b>h</b> - vector between vectors <b>l</b> and <b>v</b> (half vector) <br><br>  <b>D</b> (h) - the distribution function of micro faces <br>  <b>F</b> (v, h) - Fresnel function <br>  <b>G</b> (l, v, h) - the function of shading micro faces <br><br>  All parameters of this function are quite simple and have a physical meaning.  But what is the physical meaning of the half-vector?  Half-vector is needed to filter out those micro-lands that contribute to the reflection of light for the observer.  If the micrograin normal is half-vector, then this microface contributes to the illumination in the direction of view <b>V.</b> <br><br><img src="https://habrastorage.org/files/734/cd0/7b6/734cd07b685a4656be2c8ed8ba5c8a04.jpg"><br><br>  Let us consider in more detail the members of our BRDF. <br><br><h1>  The distribution function of micro faces D (h) </h1><br>  As a distribution function of the light reflected from the microfaces, we use the degree of cosine, with rationing to observe the law of conservation of energy.  First, we take the surface roughness coefficient, which lies in the range of 0..1, and calculate the degree of alpha from it, which lies in the range of 0.25 - 65536. Next, we take the scalar product of the <b>N</b> and <b>H</b> vectors and raise them to the degree of alpha.  And so that the resulting result does not violate the energy conservation law, we apply the <b>NDF</b> normalization constant. <br><br>  Without normalization, more energy will be reflected from the surface than it has come.  Thus we set the volume in which the light is reflected and the energy is distributed in this volume.  And this volume depends on how smooth or rough the surface is.  Now consider the next BRDF member. <br><br><h1>  Fresnel function F (v, h) </h1><br>  The intensity of light reflection depends on the angle of incidence.  This behavior is described by Fresnel formulas.  Fresnel formulas determine the amplitude and intensity of the reflected and refracted electromagnetic wave as it passes through the boundary of two media.  This effect is very noticeable on water, if you look at water from an acute angle, water reflects most of the light and we see a reflection.  If we look at the water from top to bottom, we practically do not see the reflections, but we see what is at the bottom. <br><br><img src="https://habrastorage.org/files/f76/c9c/c45/f76c9cc459df4b3bbf0830f6a00902ed.jpg"><br><br>  So, for example, a graph of reflected light depending on the angle of incidence for various materials.  Tabular data I took from the site <a href="http://refractiveindex.info/">http://refractiveindex.info/</a> <br><br><img src="https://habrastorage.org/files/148/2d7/c69/1482d7c69c8f4005aae5ca2c27770f50.jpg"><br><br>  The graph shows that plastic scatters almost all the light, but does not reflect it, until the angle of incidence is 60-70 degrees.  After that, the amount of reflected light increases dramatically.  For most dielectrics, the graph will be similar. <br><br>  Much more interesting is the situation with metals.  Metals reflect a lot of light at any angle, but the amount of reflected light is different for different wavelengths.  Dotted graphs show the reflection of light for copper.  The wavelengths correspond to the red, green and blue colors.  As you can see, red copper reflects much more, which is why metals paint reflected light in the color of their surface.  And dielectrics reflect light without coloring it. <br><br>  As a function to calculate the Fresnel, we use the Schlick approximation, since  The original Fresnel equations are too heavy for real-time calculations. <br><br><img src="https://habrastorage.org/files/85f/4f0/981/85f4f0981a8b407db5146ad7d839df3a.jpg"><br><br>  As can be seen, the <b>H</b> and <b>V</b> vectors, which determine the angle of incidence, and <b>F0</b> , with which the type of material is practically specified, participate in the Schlick function.  The coefficient <b>F0</b> can be calculated by knowing the <b>Index of refraction (IOR) of the</b> material we are modeling.  In fact, it can be found in reference books on the Internet.  Since  we know that <b>IOR of</b> air 1, then, knowing the tabular <b>IOR of the</b> material, we calculate <b>F0</b> by the formula <br><br><img src="https://habrastorage.org/files/99c/4e4/496/99c4e44968f14b1ab5a112330156a9f6.jpg"><br><br>  The physical meaning of <b>F0 is the</b> following: the percentage of light reflected at a right angle.  So: the fresnel determines how much light will be absorbed, and how much is reflected depending on the angle of incidence. <br><br>  <b>F0 is</b> used in the Schlick approximation and determines how much light is reflected at a right angle to the surface.  The usual value of <b>F0</b> for dielectrics is <b>2% - 5%</b> , i.e.  dielectrics reflect little and dissipate a lot. <br><br>  Metals reflect almost all the light, while for different wavelengths this amount is different.  Reflections on metals painted in the color of the surface.  Now consider the next BRDF member. <br><br><h1>  The function of shading micro faces G (l, v, h) </h1><br>  In fact, not every microface, the normal of which corresponds to half-vector, contributes to the lighting.  The ray reflected by the microface can not reach the observer. <br><br><img src="https://habrastorage.org/files/a8f/73f/c2f/a8f73fc2f728488fa981f92b7f8d229b.jpg"><br><br>  The microgeometry of the surface can interfere with the reflected beam.  It can be seen that part of the reflected rays will not reach the observer.  Thus, not all micro-edges will participate in the reflection of light.  We use the simplest function of visibility.  In fact, we believe that all micro-edges reflect light.  This is valid with our reflected light distribution function. <br><br>  If we imagine that the microsurface is given by a height map (convex), then, if you look at the surface at a right angle and illuminate the surface, also at a right angle, but there are no such microfaces that do not participate in the lighting, because  the surface is convex.  At the same time, our function tends to zero at large angles, which also agrees with the representation of microgeometry in the form of a height map.  Since  the sharper the angle, the less microfaces reflect light. <br><br>  This is our final function for calculating the reflected light: <br><br><img src="https://habrastorage.org/files/7f7/ac4/ef1/7f7ac4ef104d4ca2b47a1380d4d97182.jpg"><br><br>  This corresponds to the normalized Blin-Phong model with a microsurface representation in the form of a height map.  Here are some sample examples of how material parameters, roughness and IOR affect the appearance of the material. <br><br><img src="https://habrastorage.org/files/638/8d9/85c/6388d985cca04e99b757c307c8ca8d55.jpg"><br><br><h1>  Energy saving </h1><br>  I have repeatedly mentioned the conservation of energy.  Saving energy means a simple thing: the sum of the reflected and scattered light must be less than or equal to one.  It follows an important property for artists: the brightness and shape / area of ‚Äã‚Äãthe glare of the reflected light are related.  Such a connection was not previously in the non-physical renderer, since  there it is possible to violate the law of conservation of energy.  For example - a series of images.  The light source in all the pictures is the same; I will only change the surface roughness. <br><br><img src="https://habrastorage.org/files/35d/50d/611/35d50d61115a494982ba1d238b67cb99.jpg"><br><br><img src="https://habrastorage.org/files/1a2/f28/2ad/1a2f282ad03743c7834764603fcaa32d.jpg"><br><br><img src="https://habrastorage.org/files/13f/3f4/174/13f3f41748584f1ab37406fba0350ee8.jpg"><br><br><img src="https://habrastorage.org/files/ea4/969/48a/ea496948a40149179c897598ebe2cc42.jpg"><br><br><img src="https://habrastorage.org/files/3aa/c40/457/3aac404570ad45b8ab32d2dc97c6a4f2.jpg"><br><br>  It can be seen that the smaller the glare area is distributed, the brighter it is. <br><br><h1>  Light source intensity </h1><br>  Our model is based on physics, and it is important for us to use honest attenuation of the energy of the light source.  We know that the intensity of light is inversely proportional to the square of the distance to the light source.  This behavior can be described by the following formula: <br><br><img src="https://habrastorage.org/files/391/777/670/3917776708ce48d9b1b992e450bcb685.jpg"><br><br>  This feature is well suited to describe how energy fades, but has several drawbacks: <br><ul><li>  we want to take into account the attenuation of energy not from a point in space, but from a volume, because in real life there are no sources of light that do not have their own size; </li><li>  The radiation intensity described by this function tends to zero, but never reaches it.  We want the intensity to become zero at a certain distance.  This is a simple optimization of computing resources, we do not need to calculate what does not affect the final result. </li></ul><br>  For the new light attenuation function, we will need two new parameters: <br>  <b>R <sub>inner</sub></b> is the size of the light source. <br>  <b>R <sub>outer</sub></b> is the distance at which the contribution of the source to the lighting is no longer important. <br><br>  Our decay function: <br><br><img src="https://habrastorage.org/files/0ef/872/d21/0ef872d21aa64d1db5b1ed7a0aad00eb.jpg"><br><br>  It has the following properties: <br><ul><li>  Constant inside <b>R <sub>inner</sub></b> . </li><li>  At a distance of <b>R <sub>outer</sub></b> is equal to <b>0</b> . </li><li>  Corresponds to the square law of attenuation. </li><li>  Cheap enough to calculate in the shader. </li></ul><br><pre><code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">//      float GetAttenuation(float distance, float lightInnerR, float invLightOuterR) { float d = max(distance, lightInnerR); return saturate(1.0 - pow(d * invLightOuterR, 4.0)) / (d * d + 1.0); }</span></span></code> </pre> <br>  Here is a comparison of two graphs.  Quadratic attenuation and ours.  It is seen that most of our function coincides with the quadratic one. <br><br><img src="https://habrastorage.org/files/3f6/212/77b/3f621277bcaa47e0994c32ba5423306e.jpg"><br><br><h1>  Material model </h1><br>  With physics and mathematics processes sorted out.  Now we define what the artists influence.  What kind of parameters do they customize?  Our artists set the parameters of materials through textures.  Here are the textures they create: <br><table><tbody><tr><th></th><th>  for dielectrics </th><th>  for metals </th></tr><tr><th>  Base color </th><td>  sets the value of albedo </td><td>  sets the vector part of F0 </td></tr><tr><th>  Normal </th><td>  normal to the surface (macro level) </td><td>  normal to the surface (macro level) </td></tr><tr><th>  Roughness (gloss) </th><td>  surface roughness (micro level) </td><td>  surface roughness (micro level) </td></tr><tr><th>  Fresnel f0 </th><td>  type of material (IOR) for dielectrics is almost always constant </td><td>  for metals scalar part F0 </td></tr><tr><th>  Metal </th><td>  always 0 </td><td>  always 1 </td></tr></tbody></table><br>  Here is an example of the surface properties of such a mechanical pegasus: <br><br><img src="https://habrastorage.org/files/337/05a/025/33705a02564f439d9c2b2471e52a31aa.jpg"><br><br><img src="https://habrastorage.org/files/218/66f/d3b/21866fd3bc614a2e80c12c9332169a22.jpg"><br>  <i>albedo</i> <br><br><img src="https://habrastorage.org/files/9e0/b61/516/9e0b6151657840a2af69e71dfc5a16db.jpg"><br>  <i>normal</i> <br><br><img src="https://habrastorage.org/files/b55/520/3a1/b555203a1ade4eefb5d0c06c36213641.jpg"><br>  <i>gloss</i> <br><br><img src="https://habrastorage.org/files/f17/6ab/e35/f176abe358134917b9d4d88053aaeeae.jpg"><br>  <i>FO (IOR)</i> <br><br><img src="https://habrastorage.org/files/518/fa1/027/518fa1027fd44dd7ad7677097723ff72.jpg"><br>  <i>metal</i> <br><br>  To simplify the work, our artists have compiled a fairly large library of materials.  Here are some examples. <br><br><img src="https://habrastorage.org/files/479/35a/e61/47935ae611794bb98ec26301d1dd2c86.jpg"><br><br><img src="https://habrastorage.org/files/e09/635/51e/e0963551e5ed44f8be3287ea74397d67.jpg"><br><br><img src="https://habrastorage.org/files/d6d/3eb/b8a/d6d3ebb8a5714d5b9f84a69e2deb4bd0.jpg"><br><br><img src="https://habrastorage.org/files/430/d4b/c32/430d4bc32b3f43779eeddb093a39056c.jpg"><br><br><img src="https://habrastorage.org/files/ca1/9db/176/ca19db1764414db1b45c9d095b71717c.jpg"><br><br><img src="https://habrastorage.org/files/cd5/c75/766/cd5c7576614c461eaa7450675d985e3d.jpg"><br><br><h1>  Deferred shading </h1><br>  When developing Skyforge we use a deferred shading model.  This is a currently widely used method.  The method is called deferred, because  during the main rendering pass, only the buffer containing the parameters necessary for calculating the final shading occurs.  Such a parameter buffer is called a G-Buffer, short for geometry buffer. <br><br>  I will briefly describe the pros and cons of deferred shading: <br><br>  <b>Pros:</b> <br><ul><li>  Shaders geometry and lighting are separated. </li><li>  Easy to make a large number of light sources. </li><li>  Absence of combinatorial explosion in shaders. </li></ul><br>  <b>Minuses:</b> <br><ul><li>  Bandwidth.  You need a large memory bandwidth, because  The surface parameter buffer is thick enough. </li><li>  Light sources with shadows are still expensive.  Light sources without shadows have rather limited application. </li><li>  The complexity of embedding different BRDF.  It's hard to make a different lighting model for different surfaces.  For example, BRDF for hair or anisotropic metal. </li><li>  Transparency.  Practically not supported, transparency needs to be drawn after the main picture is drawn and lit. </li></ul><br>  The main disadvantage of technology - need a large memory bandwidth.  We try to pack the surface parameters necessary for lighting as much as possible.  As a result, we have come to a format that fits into 128 bits per pixel - 96, if we ignore the depth information. <br><br>  How we store surface properties (128 bits per pixel). <br><br>  <b>Skyforge g-buffer</b> <br><img src="https://habrastorage.org/files/f34/422/ef5/f34422ef554f49a3979f85c4cd9ac35c.jpg"><br><br><h1>  Tips &amp; Tricks </h1><br><br><h1>  Position reconstruction </h1><br>  When using Deferred shading, we often encounter the need to reconstruct the position of a pixel in various spaces.  For example, world space, view space, shadow space, etc.  In our GBuffer, we store only the pixel depth using the hardware depth buffer.  We need to be able to solve the problem: how to quickly obtain the position of a pixel in space, having only the hardware depth, which also has a hyperbolic distribution, and not a linear one.  Our algorithm does this conversion in two steps. <br><br><h1>  Linear depth conversion </h1><br>  After we fill up the Gbuffer, we will convert the depth buffer with a hyperbolic distribution to a linear one.  To do this, we use a full-screen step, ‚Äústraightening‚Äù the depth.  Conversion takes place using this shader: <br><br><pre> <code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">//          float ConvertHyperbolicDepthToLinear(float hyperbolicDepth) { return ((zNear / (zNear-zFar)) * zFar) / (hyperbolicDepth - (zFar / (zFar-zNear))); }</span></span></code> </pre><br>  We keep the linear depth in the R32F format and then at all stages of the render we use only the linear depth.  The second stage is the reconstruction of the position using linear depth. <br><br><h1>  Position reconstruction using linear depth </h1><br>  To quickly reconstruct a position, we use the following property of similar triangles.  <i>The ratio of perimeters and lengths (either bisectors, or medians, or heights, or median perpendiculars) is equal to the similarity coefficient, i.e.</i>  <i>in such triangles, the corresponding lines (heights, medians, bisectors, etc.) are proportional.</i>  Consider two triangles: a triangle (P1, P2, P3) and a triangle (P1, P4, P5). <br><br><img src="https://habrastorage.org/files/fd6/0f0/9e3/fd60f09e3a2848eabe7db427e2d4e496.jpg"><br><br>  The triangle (P1, P2, P3) is similar to the triangle (P1, P4, P5). <br><br><img src="https://habrastorage.org/files/c32/00f/ef0/c3200fef00c34de4853cda484ea48baa.jpg"><br><br><img src="https://habrastorage.org/files/4f2/7c9/d02/4f27c9d0243e45b4b56d720cc3be4d36.jpg"><br><br>  Thus, we, knowing the distance (P1-P4) (our linear depth) and the hypotenuse (P1, P3), using the similarity of triangles, can calculate the distance of the pixel to the camera (P1, P5).  And knowing the distance to the camera, the camera position and the direction of gaze, we can easily calculate the position in the camera space.  The camera itself, in turn, can be set in any space: world space, view space, shadow space, etc., which gives us a reconstructed position in any space we need. <br><br>  So, once again, the algorithm is in steps: <br><ol><li>  Convert hyperbolic depth to linear. </li><li>  In the vertex shader, calculate the triangle (P1, P2, P3). </li><li>  We transfer the segment (P1, P3) to the pixel shader, via the interpolator. </li><li>  We get the interpolated vector RayDir (P1, P3). </li><li>  Read the linear depth at this point. </li><li>  Position = CameraPosition + RayDir * LinearDepth. </li></ol><br>  The algorithm is very fast: one interpolator, one ALU MAD instruction and one depth reading.  You can reconstruct the position in any convenient homogeneous space.  HLSL code for reconstruction at the end of the article. <br><br><h1>  Reversed Depth Buffer </h1><br>  When developing Skyforge, we were faced with the task: to be able to draw locations with a very large visibility range, about 40 km.  Here are some pictures illustrating the drawing distance. <br><br><img src="https://habrastorage.org/files/448/007/b01/448007b01cf24700a6b48af2284b7133.jpg"><br><br><img src="https://habrastorage.org/files/4d0/7d9/f5e/4d07d9f5eb26415095248b072a22a087.jpg"><br><br><img src="https://habrastorage.org/files/5d1/28f/574/5d128f574ca74c9ea57d1fdef95c5678.jpg"><br><br><img src="https://habrastorage.org/files/ece/0a1/575/ece0a1575f4d4ea2b8ef18fc1b02108d.jpg"><br><br>  In order to avoid Z-fighting with large Far Plane values, we use the technique of reversed depth buffer.  The meaning of this technique is very simple: when calculating the projection matrix, you need to swap Far Plane and Near Plane and invert the depth comparison function by more or equal (D3DCMP_GREATEREQUAL).  This trick works only if you swap FarPlane and NearPlane in the projection matrix.  The trick does not work if you change the viewport parameters or expand the depth in the shader. <br><br>  Now I will explain why this is happening, and where we win in the accuracy of calculations.  To understand where accuracy is lost, let's figure out how the projection matrix works. <br><br><img src="https://habrastorage.org/files/a1b/bc8/9e2/a1bbc89e2e50402683ce7892e957d42c.jpg"><br><br>  So, the standard projection matrix.  We are interested in that part of the matrix, which is highlighted in gray.  Z and W position components.  How is the depth calculated? <br><br><pre> <code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">//      float4 postProjectivePosition = mul( float4(pos, 1.0), mtxProjection ); //   float depth = postProjectivePosition.z / postProjectivePosition.w;</span></span></code> </pre><br>  After multiplying the position on the projection matrix, we obtain the position in the post-projective space.  After the perspective division by W, we get the position in the clip space, this space is given by the unit cube.  Thus, the following transformation is obtained. <br><br><img src="https://habrastorage.org/files/d18/a02/0f8/d18a020f87854686b80a21e4967d2003.jpg"><br><br>  For example, consider Znear and Zfar, the distance between which is very large, about 50 km. <br><br>  Znear = 0.5 <br>  Zfar = 50000.0 <br><br>  We obtain the following two projection matrices: <br><br><img src="https://habrastorage.org/files/170/6ac/fb6/1706acfb63d94fb6ad8e3069c24a10b1.jpg"><br>  <i>Standard Projection Matrix</i> <br><br><img src="https://habrastorage.org/files/949/c36/440/949c36440b8b426b89da9cd0d11307cb.jpg"><br>  <i>Extended Projection Matrix</i> <br><br>  The depth after multiplying by the standard projection matrix will be equal to the following: <br><br><img src="https://habrastorage.org/files/758/f7a/fe7/758f7afe7c1a46958869f92f9684d6cf.jpg"><br><br>  Accordingly, after multiplying by the unfolded projection matrix: <br><br><img src="https://habrastorage.org/files/ccf/0eb/a27/ccf0eba27ce548af9cb94bad6be69d86.jpg"><br><br>  As we can see, in the case of the standard projection matrix, when calculating the depth, the addition of numbers of very different order will occur - tens of thousands and 0.5.  To add numbers of different order, the FPU must first bring their exponents to a single value (greater exponential), then add and normalize the addition obtained by the mantis.  In fact, at large z values, this simply adds white noise to the low bits of the mantis.  In the case of using the unfolded projection matrix, this behavior occurs only near the camera, where, due to the hyperbolic distribution of depth, there is already excessive accuracy.  Here is an example of what happens when the value of z = 20 km: <br><br><img src="https://habrastorage.org/files/b14/2ef/642/b142ef642e5f4c50827216bfd351dd90.jpg"><br><br>  And for the unfolded projection matrix: <br><br><img src="https://habrastorage.org/files/c2c/3bb/d02/c2c3bbd0237147ec93b62eeaaa9e907d.jpg"><br><br>  Total: <br><ul><li>  The standard 24-bit D24 depth buffer easily covers a range of 50 km without Z-fighting. </li><li>  Reverse depth is suitable for any engine, I would recommend to use it in all projects. </li><li>  It is best to build support from the beginning of development, because  There are many places that may have to be redone: extracting the frustum planes from the projection matrix, bias from the shadows, etc. </li><li>  If a float depth buffer is available on the target platform, then it is best to use it.  This will further increase the accuracy, because  values ‚Äã‚Äãwill be stored with greater accuracy. </li></ul><br>  That's all, thank you for your attention! <br><br>  <b>Literature</b> <br><br>  <a href="http://renderwonk.com/publications/s2010-shading-course/hoffman/s2010_physically_based_shading_hoffman_b.pdf">Naty Hoffman.</a>  <a href="http://renderwonk.com/publications/s2010-shading-course/hoffman/s2010_physically_based_shading_hoffman_b.pdf">Crafting Physically Motivated Shading Models for Game Development</a> <br><br>  <a href="http://renderwonk.com/publications/s2010-shading-course/gotanda/slide_practical_implementation_at_triace.pdf">Yoshiharu Gotanda.</a>  <a href="http://renderwonk.com/publications/s2010-shading-course/gotanda/slide_practical_implementation_at_triace.pdf">Practical implementation at tri-Ace</a> <br><br>  <a href="http://www.humus.name/Articles/Persson_CreatingVastGameWorlds.pdf">Emil Persson.</a>  <a href="http://www.humus.name/Articles/Persson_CreatingVastGameWorlds.pdf">Creating vast game worlds</a> <br><br>  <a href="http://www.crytek.com/download/S2011_SecretsCryENGINE3Tech.ppt">Nickolay Kasyan, Nicolas Schulz, Tiago Sousa.</a>  <a href="http://www.crytek.com/download/S2011_SecretsCryENGINE3Tech.ppt">Secrets of CryENGINE 3 Graphics Technology</a> <br><br>  <a href="http://blog.selfshadow.com/publications/s2014-shading-course/heitz/s2014_pbs_masking_shadowing_slides.pdf">Eric Heitz.</a>  <a href="http://blog.selfshadow.com/publications/s2014-shading-course/heitz/s2014_pbs_masking_shadowing_slides.pdf">Understanding the Masking-Shadowing Function</a> <br><br>  <a href="http://blog.selfshadow.com/publications/s2013-shading-course/karis/s2013_pbs_epic_slides.pptx">Brian Karis.</a>  <a href="http://blog.selfshadow.com/publications/s2013-shading-course/karis/s2013_pbs_epic_slides.pptx">Real Shading in Unreal Engine 4</a> <br><br>  <b>HLSL reconstruction code (vertex shader)</b> <br><br><pre> <code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">//    float tanHalfVerticalFov; // invProjection.11; float tanHalfHorizontalFov; // invProjection.00; //      float3 camBasisUp; float3 camBasisSide; float3 camBasisFront; // postProjectiveSpacePosition  homogeneous projection space float3 CreateRay(float4 postProjectiveSpacePosition) { float3 leftRight = camBasisSide * -postProjectiveSpacePosition.x * tanHalfHorizontalFov; float3 upDown = camBasisUp * postProjectiveSpacePosition.y * tanHalfVerticalFov; float3 forward = camBasisFront; return (forward + leftRight + upDown); } void VertexShader(float4 inPos, out float4 outPos : POSITION, out float3 rayDir : TEXCOORD0) { outPos = inPos; rayDir = CreateRay(inPos); }</span></span></code> </pre><br>  <b>HLSL reconstruction code (pixel shader)</b> <br><br><pre> <code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">//      float3 camPosition; float4 PixelShader(float3 rayDir : TEXCOORD0) : COLOR0 { ... float linearDepth = tex2D(linearDepthSampler, uv).r; float3 position = camPosition + rayDir * linearDepth; ... } //          float ConvertHyperbolicDepthToLinear(float hyperbolicDepth) { return ((zNear / (zNear-zFar)) * zFar) / (hyperbolicDepth - (zFar / (zFar-zNear))); }</span></span></code> </pre><br>  <b>Slides original report</b> <br><br>  <a href="http://www.slideshare.net/makeevsergey/skyforge-rendering-techkri14finalv21">www.slideshare.net/makeevsergey/skyforge-rendering-techkri14finalv21</a> </div><p>Source: <a href="https://habr.com/ru/post/248873/">https://habr.com/ru/post/248873/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../248857/index.html">Go main advantage</a></li>
<li><a href="../248863/index.html">Clustering: tell me what you buy and I will tell you who you are</a></li>
<li><a href="../248867/index.html">Interview: CISSP, CISA, SecurityPlus and other words that only IT security experts can understand</a></li>
<li><a href="../248869/index.html">The digest of interesting materials from the world of web development and IT for the last week ‚Ññ144 (January 19 - 25, 2015)</a></li>
<li><a href="../248871/index.html">Runscript - utility to run python scripts</a></li>
<li><a href="../248875/index.html">Meteor. And now loading photos</a></li>
<li><a href="../248879/index.html">Boost.DI: dependency injection in C ++</a></li>
<li><a href="../248881/index.html">Overview Friendly interactive shell (fish) and why it is better than bash</a></li>
<li><a href="../248887/index.html">Hacking a bitcoin exchange on Rails</a></li>
<li><a href="../248889/index.html">‚ÄúOpen Financial Data: Possibilities for Using It‚Äù</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>