<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Neurogrid plays dota</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! In fact, the neural grid does not play the usual Dota 2, but in RussianAICup 2016 CodeWizards. RussianAICup is an annual open competition in ar...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Neurogrid plays dota</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/6eb/37b/d1c/6eb37bd1c8ea4820a1cd0683923b9ea1.png"><br><br>  Hello!  In fact, the neural grid does not play the usual Dota 2, but in RussianAICup 2016 CodeWizards.  RussianAICup is an annual open competition in artificial intelligence programming.  To participate in this competition is quite interesting.  This year the theme was a game similar to DotA.  Since I have been training with reinforcements for some time, I wanted to try to apply it in RussianAICup.  The main goal was to teach the neural network to play this game, although the occupation of the prize place would be nice, of course.  As a result, the neural network keeps around 700 places.  Which, I think, is not bad, due to the limitations of the competition.  This article will focus on training with reinforcements and the <a href="https://arxiv.org/abs/1509.02971">DDPG</a> and <a href="">DQN algorithms</a> , rather than the competition itself. <br><a name="habracut"></a><br>  In this article I will not describe the basic information about neural networks, now there is a lot of information about them on the Internet, and I don‚Äôt want to inflate the publication.  I would also like to focus on reinforcement learning using neural networks, that is, on how to train an artificial neural network to demonstrate the desired behavior by trial and error.  I will describe the two algorithms in a simplified way, they have one base, and the fields of application are slightly different. <br><br><h3>  Markov state </h3><br>  At the input, the algorithms take the current state of the agent S. Here it is important that the state have the Markov property, that is, if possible, include all the information necessary to predict the next system states.  For example, we have a stone thrown at an angle to the horizon.  If we take the coordinates of a stone as a state, it will not allow us to predict its future behavior, but if we add a velocity vector of a stone to the coordinates, then this state will allow us to predict the entire further trajectory of the stone, that is, it will be Markov.  Based on the state, the algorithm provides information on action A, which must be done to ensure optimal behavior, that is, such behavior that leads to the maximization of the amount of rewards.  The reward R is a real number that we use to train the algorithm.  The reward is given at the moment when the agent, being in the state S, performs the action A and changes to the state S '.  For learning, these algorithms use information about what they did in the past and what was the result.  These elements of experience consist of 4 components (S, A, R, S ').  These fours we add to the array of experience and then use for training.  In the articles, it is named replay buffer. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  DQN </h3><br>  Or Deep Q-Networks is the same algorithm that was taught <a href="https://www.youtube.com/watch%3Fv%3DW2CAghUiofY">to play Atari games</a> .  This algorithm is designed for cases when the number of control commands is strictly limited and in the case of Atari it is equal to the number of joystick buttons (18).  Thus, the output of the algorithm is a vector of assessments of all possible actions Q. The vector index indicates the specific action, and the value indicates how good this action is.  That is, we need to find an action with the maximum rating.  In this algorithm, one neural network is used, to the input of which we feed S, and the action index is defined as argmax (Q).  We teach the neural network using the Bellman equation, which ensures the convergence of the neural network output to values ‚Äã‚Äãthat implement the behavior that maximizes the sum of future rewards.  For this case it looks like: <br><img src="https://habrastorage.org/files/691/6ec/6bd/6916ec6bd779497eba9854fdb19c7c2c.png"><br><img src="https://habrastorage.org/files/868/c99/eb5/868c99eb507445dd89f4302e27cacfca.png"><br>  If this decrypts the following: <br><br><ol><li>  Take a pack (minibatch) of N experience elements (S, A, R, S '), for example 128 elements <br></li><li>  For each item count <br><br>  y = R + Œ≥ * max <sub>for all A '</sub> (Q <sup>past</sup> (S', A ')) <br><br>  Œ≥ is the discount factor, a number in the interval [0, 1], which determines how important future awards are for us.  Typical values ‚Äã‚Äãare usually selected in the range of 0.9 - 0.99. <br><br>  Q <sup>past</sup> is a copy of our neural network, but a certain number of iterations of learning backwards, for example 100 or 1000. We simply update the weights of this neural network periodically, do not teach. <br><br>  Q <sup>past</sup> = (1 - Œ±) * Q <sup>past</sup> + Œ± * Q <br><br>  Œ± - update rate Q <sup>past</sup> , a typical value of 0.01 <br><br>  This trick provides the best convergence of the algorithm. <br><br></li><li>  Next, we consider the loss function for our minibatch. <br><br>  L = Œ£ <sub>by minibatch elements</sub> (y - Q (S, A)) <sup>2</sup> / N <br><br>  We use the mean square error <br></li><li>  Apply favorite optimization algorithm using L <br></li><li>  Repeat while the neural network does not learn enough <br></li></ol><br>  If you look closely at the Bellman equation, you can see that it recursively evaluates the current action by a weighted sum of rewards from all subsequent actions possible after the current action has been completed. <br><br><h3>  DDPG </h3><br>  Or Deep Deterministic Policy Gradient is an algorithm created for the case when control commands are values ‚Äã‚Äãfrom continuous spaces (continuous action spaces).  For example, when you drive a car, you turn the steering wheel at a certain angle or press the gas pedal to a certain depth.  Theoretically, there are theoretically infinitely many possible values ‚Äã‚Äãof the steering wheel rotation and the positions of the gas pedal, so we want the algorithm to give us these very specific values ‚Äã‚Äãthat we need to apply in the current situation.  Therefore, in this approach, two neural networks are used: Œº (S) is the actor that returns us the vector A, whose components are the values ‚Äã‚Äãof the corresponding control signals.  For example, this could be a vector of three elements (steering angle, accelerator position, brake pedal position) for the autopilot of a car.  The second neural network Q (S, A) is a critic, which returns q - an estimate of the action A in the state S. The critic is used to train the actor and is trained using the Bellman equation: <br><br><img src="https://habrastorage.org/files/423/890/39d/42389039d37d436f97690c34742795fa.png"><br><img src="https://habrastorage.org/files/a0f/997/458/a0f997458c02477fbf0726da9b67257e.png"><br><br>  The critic is trained through the gradient from the evaluation of the critic ‚àáQ (S, Œº (S)) by the actor weights <br><br><img src="https://habrastorage.org/files/0a7/bb9/dcb/0a7bb9dcb88d441a833b0e13d4f9699d.png"><br><br>  If this decrypts: <br><br><ol><li>  We take minibatch from N elements of experience (S, A, R, S ') <br><br></li><li>  y = R + Œ≥ * Q <sup>past</sup> (S ', Œº <sup>past</sup> (S')) <br><br>  Q <sup>past</sup> , Œº <sup>past</sup> is the same trick as in the case of DQN, only now it is a copy of two networks <br><br></li><li>  We consider the loss function <br><br>  L = Œ£ <sub>by mini-batches</sub> (y - Q (S, A)) <sup>2</sup> <br><br></li><li>  We teach criticism using L <br></li><li>  We take the gradient in the weights of the actor ‚àáQ (S, Œº (S)) by our favorite optimization algorithm and modify the actor with it <br></li><li>  Repeat until we reach the desired <br></li></ol><br><h3>  RussianAICup </h3><br><img src="https://habrastorage.org/files/06d/472/63b/06d47263bd1f440e8c609287d3a89591.png"><br>  Now I will talk about the agent I coached for the competition.  Detailed rules can read the <a href="http://russianaicup.ru/s/1482846016574/assets/documentation/codewizards2016-docs-ru.pdf%3Frnd">link</a> , if interested.  You can also watch the video and description on the <a href="http://russianaicup.ru/">main page</a> .  In the competition there are 2 types of games, the first is when your strategy controls one of ten wizards.  And second, when your strategy controls a team of 5 wizards, the enemy controls the opposite team of 5 wizards.  The information that is available for the strategy includes all objects that are in the field of visibility of all friendly wizards, the rest of the space is covered with the fog of war: <br><br><ul><li>  wizards <br></li><li>  minions <br></li><li>  buildings (towers and bases) <br></li><li>  the trees <br></li><li>  bonuses <br></li><li>  shells <br></li></ul><br>  A unit may have the following information about itself: <br><br><ul><li>  coordinates <br></li><li>  number of hit points <br></li><li>  amount of mana <br></li><li>  time to next strike <br></li><li>  other parameters, there are many, in the rules there is a full description <br></li></ul><br>  There is a local simulator (it is on KPDV), in which you can drive your strategy against yourself or 2 built-in opponents, rather simple strategies.  To begin, I decided to try to solve the problem in the forehead.  I used DDPG.  I collected all the available parameters about the world, they turned out 1184 and filed for input.  The output I had was a vector of 13 numbers responsible for the speed of movement directly and sideways, rotation, different types of attack, as well as attack parameters, such as the range of the projectile, in general, all the parameters that exist.  With a reward for the beginning, I also decided not to be wise, the game gives out the number of XP, so I gave a positive reward for increasing XP and a negative one for decreasing hit points.  Having experimented for some time, I realized that in this form it is very difficult to get from the neural network sense, since the dependencies that my model had to reveal were, according to my estimates, quite complex.  Therefore, I decided to simplify the task for the neural network. <br><br>  For a start, I rebuilt the state so that it represented not just a collection of all the incoming information, but was submitted as a circular review from a controlled wizard.  The whole circular review was divided into 64 segments, each segment could get 4 closest objects in this segment.  There were 3 such circular reviews for friendly, neutral and enemy units, as well as to indicate where to move.  The result was even more parameters - 3144. I also began to give a reward for moving in the right direction, that is, along the path to the enemy base.  Since the circular view includes information on the relative position of objects, I also tried to use the convolutional network.  But nothing worked.  Wizards just spun in one place without the slightest glimpse of intelligence.  And I left for some time trying to read information about training neural networks and get inspiration. <br><br>  After some time, I returned to the experiments, having already inspired, reduced the learning rate from 1e-3 to 1e-5, greatly reduced the number of input parameters to 714, while abandoning some data in order to speed up the information processing.  I also decided to use the neural network only for movement and rotation, and left the shooting behind a manually-written simple strategy: if we can shoot, we turn in the direction of the weakest enemy and shoot.  Thus, I removed from the neural network a task that is rather difficult for learning, but easy for prescribing: aiming and shooting.  And he left a rather difficult formalizable task of advance and retreat.  As a result, glimpses of intelligence on a conventional fully connected network became noticeable.  After training it for some time (a couple of days on the GeForce GTX 1080 for the experiment), having achieved a good management quality (the network beat the built-in opponent), I decided to load the network into the sandbox. <br><br>  I unloaded the weights of the network from Tensorflow and zhardkodil them in the form of h files.  The result was an archive of several tens of megabytes and the sandbox refused to accept it, saying that the maximum file to download is 1 MB.  I had to cut down the neural network much more strongly and spend a lot of time (about a week or a few weeks, I don‚Äôt remember exactly) on its training, selection of hyperparameters and fit into the 1 MB archive size.  And when I finally tried to download again, it showed that the limit for the unzipped archive is 2 MB, and I had 4 MB.  Feyspalm.  Overcoming stormy feelings, I began to squeeze the neural network even more.  As a result, it has shrunk to the following architecture: <br><br><pre><code class="hljs">394 x 192 x 128 x 128 x 128 x 128 x 6</code> </pre> <br>  394 is the input layer, in all layers except the last and the last but one, relu is used, in the last but one tanh as an activation function, in the last one there is no activation function.  In this form, the control was not so good, but a glimpse of intelligence was noticeable and I decided to load this option exactly a few hours before the start of the final.  With minor improvements, he still plays <a href="http://russianaicup.ru/profile/Parilo2">russianaicup.ru/profile/Parilo2</a> in the sandbox around 700 places in the ranking.  In general, I am pleased with the result at the moment.  Below is a video with an example of a 1x1 match, my strategy controls the whole team of 5 wizards. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/vM_2Ned9gmY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h3>  Suggestions for improving the competition </h3><br>  The competition is quite interesting to participate.  There are buns in the form of a local simulator.  I would like to suggest changing the format somewhat.  That is, to run on the server only the server part and the rating table, and the clients to connect via the Internet to the server and participate in duels.  In this way, you can take the load off the server, as well as remove resource restrictions for strategies.  In general, since it is made in many online games like Starcraft 2 or the same Dota. <br><br><h3>  Code </h3><br>  Agent code is available on <a href="https://github.com/parilo/russian-ai-cup-2016-agent">github</a> .  Used with C ++ for both management and training.  The training is implemented as a separate WizardTrainer program, which depends on TensorFlow, the strategy connects over the network to the trainer, throws the state of the environment there and receives from there the actions that need to be performed.  She also sends all the experience she receives there.  Thus, several agents can be connected to one neural network and it will manage all at the same time.  It's not so easy to collect it, if you need to ask me how. <br><br><h3>  Resources </h3><br>  For more information about training with reinforcement, these, as well as other algorithms, you can see in David Silver's wonderful <a href="https://www.youtube.com/watch%3Fv%3D2pWv7GOvuf0">course</a> from Google DeepMind.  Slides are available <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">here</a> . <br>  Original articles <a href="https://arxiv.org/abs/1509.02971">DDPG</a> and <a href="">DQN</a> .  I took the formulas from them. <br><br>  Thank you for reading to the end.  Good luck on the path of knowledge! <br><br>  PS I apologize for the size of the formulas, I didn‚Äôt want them to be small, I overdid it with a print screen. </div><p>Source: <a href="https://habr.com/ru/post/319518/">https://habr.com/ru/post/319518/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../319506/index.html">Linkedin recently redesigned and I intend to discuss it.</a></li>
<li><a href="../319510/index.html">Ref attributes and DOM in React</a></li>
<li><a href="../319512/index.html">Skype for Business plugin release and new build 3CX Client for Windows</a></li>
<li><a href="../319514/index.html">Translation of Nuxt.JS documentation</a></li>
<li><a href="../319516/index.html">My home data center</a></li>
<li><a href="../319520/index.html">Uncontrolled components in React</a></li>
<li><a href="../319524/index.html">Sound generation for Unity projects using Chuck and OSC</a></li>
<li><a href="../319526/index.html">Horizontal scaling. What, why, when and how?</a></li>
<li><a href="../319528/index.html">A person. Commander Norton</a></li>
<li><a href="../319530/index.html">Shadow on the fence, or 25 trees for Adam Jensen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>