<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Hello, TensorFlow. Google Machine Learning Library</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The TensorFlow project is bigger than you might think. The fact that this is a library for deep learning, and its connection with Google helped the Te...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Hello, TensorFlow. Google Machine Learning Library</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/files/ab1/31c/495/ab131c495c314f998eb37292a6f74847.jpg" alt="tensorflow"></p><br><p>  The <a href="https://www.tensorflow.org/">TensorFlow</a> project is <a href="https://www.tensorflow.org/">bigger</a> than you might think.  The fact that this is a library for deep learning, and its connection with Google helped the TensorFlow project to attract a lot of attention.  But if you forget about the hype, some of its unique details deserve a deeper study: </p><br><ul><li>  The main library is suitable for a wide family of machine learning technicians, and not just for deep learning. </li><li>  Linear algebra and other entrails are clearly visible from the outside. </li><li>  In addition to the basic machine learning functionality, TensorFlow also includes its own logging system, its own interactive logging visualizer, and even a powerful data delivery architecture. </li><li>  The execution model of TensorFlow is different from the scikit-learn of the Python language and most of the tools in R. </li></ul><br><p>  All this is cool, but TensorFlow can be quite difficult to understand, especially for someone who is just introduced to machine learning. </p><br><p>  How does TensorFlow work?  Let's try to figure out, see and understand how each part works.  We will examine the data movement <a href="https://en.wikipedia.org/wiki/Graph_(abstract_data_type)">graph</a> , which defines the calculations that your data will go through, understand how to train models with a <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a> using TensorFlow, and how <a href="https://www.tensorflow.org/versions/r0.8/how_tos/summaries_and_tensorboard/">TensorBoard</a> visualizes work with TensorFlow.  Our examples will not help solve real-world industrial-level machine learning problems, but they will help you understand the components that underlie everything TensorFlow has created, including what you write in the future! <a name="habracut"></a></p><br><h1>  Names and execution in Python and TensorFlow </h1><br><p>  The way TensorFlow manages computations is not much different from how Python normally does.  In both cases, it is important to remember that, paraphrasing <a href="https://twitter.com/hadleywickham/status/732288980549390336">Hadley Wickham</a> , the object has no name (see image 1).  To understand the similarities and differences between the principles of Python and TensorFlow, let's take a look at how they refer to objects and handle the calculation. </p><br><p><img src="https://habrastorage.org/files/383/661/8d0/3836618d0db448b49a31d3a25e267c9f.jpg"></p><br><p>  <em>Image 1. The names "have" objects, but not vice versa.</em>  <em>Illustration by Hadley Wickham, used with permission.</em> </p><br><p> Variable names in Python are not what they represent.  They simply point to objects.  So, when you write in Python <code>foo = []</code> and <code>bar = foo</code> , this does not mean that <code>foo</code> is equal to <code>bar</code> ;  <code>foo</code> <em>is</em> <code>bar</code> , in the sense that they both point to the same list object. </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>foo = [] &gt;&gt;&gt; bar = foo &gt;&gt;&gt; foo == bar <span class="hljs-comment"><span class="hljs-comment">## True &gt;&gt;&gt; foo is bar ## True</span></span></code> </pre> <br><p>  You can also make sure that <code>id(foo)</code> and <code>id(bar)</code> same.  This identity, especially with <a href="https://codehabitude.com/2013/12/24/python-objects-mutable-vs-immutable/">variable</a> data structures like lists, can lead to serious bugs, if you misunderstand it. </p><br><p>  Inside Python, it manages all your objects and keeps track of the names of the variables and which object each name refers to.  The TensorFlow graph represents another layer of this type of control.  As we will see later, names in Python will refer to objects that are connected to more detailed and more clearly controlled operations on the TensorFlow graph. </p><br><p>  When you enter a Python expression, for example, in the interactive interpreter REPL (Read Evaluate Print Loop), everything you type will almost always be calculated right away.  Python is eager to do what you order.  So if I tell him to do <code>foo.append(bar)</code> , he will immediately add, even if I never use <code>foo</code> . </p><br><p>  A more lazy alternative is just to remember what I said to <code>foo.append(bar)</code> , and if at some point in the future I‚Äôll compute <code>foo</code> , then Python will add.  This is closer to how TensorFlow behaves: in it the definition of a relationship has nothing to do with the calculation of the result. </p><br><p>  TensorFlow separates the definition of computation from its execution even more strongly, since they occur generally in different places: the graph defines operations, but operations occur only within sessions.  Graphs and sessions are created independently of each other.  A graph is something like a drawing, and a session is something like a construction site. </p><br><p>  Returning to our simple Python example, let me remind you that <code>foo</code> and <code>bar</code> point to the same list.  Adding <code>bar</code> to <code>foo</code> , we inserted the list inside.  You can imagine this as a graph with a single node that points to itself.  Nested lists are one of the ways to represent the structure of a graph, similar to the computational graph TensorFlow. </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>foo.append(bar) &gt;&gt;&gt; foo <span class="hljs-comment"><span class="hljs-comment">## [[...]]</span></span></code> </pre> <br><p>  Real TensorFlow graphs will be more interesting! </p><br><h1>  Simplest graph TensorFlow </h1><br><p>  To immerse yourself in the topic, let's create a simplest TensorFlow graph from scratch.  Fortunately, TensorFlow is easier to <a href="https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html">install</a> than some other frameworks.  The example here will work with Python 2.7 or 3.3+, and we are using TensorFlow 0.8. </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf</code> </pre> <br><p>  By this time, TensorFlow has already started managing a bunch of fortunes for us.  For example, an explicit default column already exists.  <a href="https://github.com/tensorflow/tensorflow/blob/v0.8.0/tensorflow/python/framework/ops.py">Inside the</a> default graph is in <code>_default_graph_stack</code> , but we do not have access there directly.  We use <code>tf.get_default_graph()</code> . </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>graph = tf.get_default_graph()</code> </pre> <br><p>  The nodes of the TensorFlow graph are called operations (‚Äúoperations‚Äù or ‚Äúops‚Äù).  A set of operations can be seen using <code>graph.get_operations()</code> . </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>graph.get_operations() <span class="hljs-comment"><span class="hljs-comment">## []</span></span></code> </pre> <br><p>  Now in the graph is empty.  We will need to add there everything that the TensorFlow library will need to calculate.  Let's start by adding a simple constant with a value of one. </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>input_value = tf.constant(<span class="hljs-number"><span class="hljs-number">1.0</span></span>)</code> </pre> <br><p>  Now this constant exists as a node, an operation in the graph.  The Python name of the variable <code>input_value</code> indirectly refers to this operation, but it can also be found in the default column. </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>operations = graph.get_operations() &gt;&gt;&gt; operations <span class="hljs-comment"><span class="hljs-comment">## [&lt;tensorflow.python.framework.ops.Operation at 0x1185005d0&gt;] &gt;&gt;&gt; operations[0].node_def ## name: "Const" ## op: "Const" ## attr { ## key: "dtype" ## value { ## type: DT_FLOAT ## } ## } ## attr { ## key: "value" ## value { ## tensor { ## dtype: DT_FLOAT ## tensor_shape { ## } ## float_val: 1.0 ## } ## } ## }</span></span></code> </pre> <br><p>  TensorFlow uses the protocol buffers format inside.  ( <a href="https://developers.google.com/protocol-buffers/">Protocol buffers</a> are something like Google-level <a href="http://www.json.org/">JSON</a> ).  Displaying the <code>node_def</code> the constant operation above shows that TensorFlow stores in the protocol buffer view for the number one. </p><br><p>  People who are not familiar with TensorFlow sometimes wonder what the essence of creating "TensorFlow-versions" of existing things is.  Why not just use a regular Python variable instead of further defining a TensorFlow object?  <a href="https://www.tensorflow.org/versions/r0.8/tutorials/mnist/pros/index.html">One of the TensorFlow tutorials</a> has an explanation: </p><br><blockquote>  To make effective numerical computations in Python, libraries like NumPy are usually used, which perform such expensive operations as matrix multiplication outside of Python using highly efficient code implemented in another language.  Unfortunately, there is an additional load when switching back to Python after each operation.  This load is especially noticeable when you need to perform calculations on a GPU or in distributed mode, where data transfer is an expensive operation. <br><br>  TensorFlow also does complex computations outside of Python, but it goes even further to avoid additional workload.  Instead of running a single expensive operation independently of Python, TensorFlow allows us to describe a graph of interacting operations that work completely outside of Python.  A similar approach is used in Theano and Torch. </blockquote><br><p>  TensorFlow can do a lot of cool stuff, but it can only work with what has been explicitly transferred to it.  This is true even for one constant. </p><br><p>  If you look at our <code>input_value</code> , you can see it as a 32-bit zero-dimensional tensor: just one number. </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>input_value <span class="hljs-comment"><span class="hljs-comment">## &lt;tf.Tensor 'Const:0' shape=() dtype=float32&gt;</span></span></code> </pre> <br><p>  Note that the <em>value is</em> <em>not specified</em> .  To calculate <code>input_value</code> and get a numerical value, you need to create a "session" in which you can calculate the operations of the graph, and then explicitly calculate or "run" <code>input_value</code> .  (Session uses default graph). </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>sess = tf.Session() &gt;&gt;&gt; sess.run(input_value) <span class="hljs-comment"><span class="hljs-comment">## 1.0</span></span></code> </pre> <br><p>  It may seem strange to "run" a constant.  But this is not much different from the usual Python expression evaluation.  TensorFlow simply manages its own data space - a computational graph, and it has its own methods for computing. </p><br><h1>  The simplest neuron TensorFlow </h1><br><p>  Now that we have a session with a simple graph, let's build a neuron with one parameter or weight.  Often, even simple neurons also include the bias term and non-identity activation function, but we can do without them. </p><br><p>  The weight of the neuron will not be constant.  We expect it to change when learning, based on the truth of the input and output used for training.  Weight will be <a href="https://www.tensorflow.org/versions/r0.8/how_tos/variables/">variable</a> TensorFlow.  We give it an initial value of 0.8. </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>weight = tf.Variable(<span class="hljs-number"><span class="hljs-number">0.8</span></span>)</code> </pre> <br><p>  You might think that adding a variable will add an operation to the graph, but in fact this line alone will add four operations.  You can find out their names: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> op <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> graph.get_operations(): print(op.name) <span class="hljs-comment"><span class="hljs-comment">## Const ## Variable/initial_value ## Variable ## Variable/Assign ## Variable/read</span></span></code> </pre> <br><p>  You don‚Äôt want to analyze each operation ‚Äúfor the bones‚Äù for too long, let's better create at least one similar to the present calculation: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>output_value = weight * input_value</code> </pre> <br><p>  Now there are six operations in the graph, and the last one is multiplication. </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>op = graph.get_operations()[<span class="hljs-number"><span class="hljs-number">-1</span></span>] &gt;&gt;&gt; op.name <span class="hljs-comment"><span class="hljs-comment">## 'mul' &gt;&gt;&gt; for op_input in op.inputs: print(op_input) ## Tensor("Variable/read:0", shape=(), dtype=float32) ## Tensor("Const:0", shape=(), dtype=float32)</span></span></code> </pre> <br><p>  Here you can see how the multiplication operation monitors the source of input data: they come from other operations in the graph.  It is quite difficult for a person to keep track of all the connections in order to understand the structure of the entire graph.  <a href="https://www.tensorflow.org/versions/r0.8/how_tos/graph_viz/">The visualization of the TensorBoard graph is</a> designed specifically for this. </p><br><p>  How to determine the result of multiplication?  It is necessary to "start" the operation <code>output_value</code> .  But this operation depends on the variable <code>weight</code> .  We indicated that the initial <code>weight</code> value should be 0.8, but the value has not yet been set in the current session.  The <code>tf.initialize_all_variables()</code> function generates an operation that initializes all variables (in our case only one), and then we can start this operation. </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>init = tf.initialize_all_variables() &gt;&gt;&gt; sess.run(init)</code> </pre> <br><p>  The <code>tf.initialize_all_variables()</code> includes initializers for all variables that are <em>currently in the graph</em> , so if you add new variables, you will need to run <code>tf.initialize_all_variables()</code> again;  simple <code>init</code> will not include new variables. </p><br><p>  Now we are ready to start the operation <code>output_value</code> . </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>sess.run(output_value) <span class="hljs-comment"><span class="hljs-comment">## 0.80000001</span></span></code> </pre> <br><p>  This is 0.8 * 1.0 with 32-bit floats, and 32-bit floats <a href="https://en.wikipedia.org/wiki/Floating_point">barely understand the</a> number 0.8.  A value of 0.80000001 is the closest thing they could do. </p><br><h1>  We look at the graph in TensorBoard </h1><br><p>  Our graph is still quite simple, but it would already be nice to see its presentation in the form of a diagram.  Use TensorBoard to generate such a chart.  TensorBoard reads the name field that is stored in each operation (this is not at all the same as Python variable names).  You can use these TensorFlow names and switch to more familiar Python variable names.  Using <code>tf.mul</code> equivalent to simply multiplying with <code>*</code> in the example above, but here you can set a name for the operation. </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>x = tf.constant(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, name=<span class="hljs-string"><span class="hljs-string">'input'</span></span>) &gt;&gt;&gt; w = tf.Variable(<span class="hljs-number"><span class="hljs-number">0.8</span></span>, name=<span class="hljs-string"><span class="hljs-string">'weight'</span></span>) &gt;&gt;&gt; y = tf.mul(w, x, name=<span class="hljs-string"><span class="hljs-string">'output'</span></span>)</code> </pre> <br><p>  TensorBoard looks into the output directory created from TensorFlow sessions.  We can write to this output using <code>SummaryWriter</code> , and if we do nothing except one graph, only one graph will be recorded. </p><br><p>  The first argument to create a <code>SummaryWriter</code> is the name of the output directory that will be created if necessary. </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>summary_writer = tf.train.SummaryWriter(<span class="hljs-string"><span class="hljs-string">'log_simple_graph'</span></span>, sess.graph)</code> </pre> <br><p>  Now you can run TensorBoard on the command line. </p><br><pre> <code class="bash hljs">$ tensorboard --logdir=log_simple_graph</code> </pre> <br><p>  TensorBoard runs as a local web application on port 6006. (‚Äú6006‚Äù is ‚Äúgoog‚Äù upside down).  If you go to the browser on <code>localhost:6006/#graphs</code> , then you can see the graph of the graph created in TensorFlow.  It looks something like image 2. </p><br><p><img src="https://habrastorage.org/files/514/ec8/8c6/514ec88c62014f77bd71ea96cd4e5144.jpg"></p><br><p>  <em>Image 2. TensorBoard visualization of the simplest TensorFlow neuron.</em> </p><br><h1>  We learn neuron </h1><br><p>  We created a neuron, but how will it learn?  We set the input value to 1.0.  Suppose the correct final value is zero.  That is, we have a very simple data set for learning with one example with one characteristic: the value is one and the mark is zero.  We want to teach a neuron to convert one to zero. </p><br><p>  Now the system takes a unit and returns 0.8, which is not the correct behavior.  We need a way to determine how wrong the system is.  We call this measure of erroneousness ‚Äúloss‚Äù (‚Äúloss‚Äù) and set the goal of the system to minimize the loss.  If the loss can be a negative number, then minimization does not make sense, so let's define the loss as the square of the difference between the current input value and the desired output value. </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>y_ = tf.constant(<span class="hljs-number"><span class="hljs-number">0.0</span></span>) &gt;&gt;&gt; loss = (y - y_)**<span class="hljs-number"><span class="hljs-number">2</span></span></code> </pre> <br><p>  Up to this point, nothing in the graph is learning.  For training, we need an optimizer.  We use the gradient descent function to be able to update the weight based on the value of the derivative loss.  The optimizer needs to set the training level to control dimensional updates, we will set 0.025. </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>optim = tf.train.GradientDescentOptimizer(learning_rate=<span class="hljs-number"><span class="hljs-number">0.025</span></span>)</code> </pre> <br><p>  The optimizer is unusually smart.  It can automatically detect and use the desired gradient at the level of the entire network, making a step-by-step backward movement for learning. </p><br><p>  Take a look at what the gradient looks like for our simple example. </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>grads_and_vars = optim.compute_gradients(loss) &gt;&gt;&gt; sess.run(tf.initialize_all_variables()) &gt;&gt;&gt; sess.run(grads_and_vars[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>]) <span class="hljs-comment"><span class="hljs-comment">## 1.6</span></span></code> </pre> <br><p>  Why the gradient value is 1.6?  The loss value is squared, and the derivative is an error multiplied by two.  Now the system returns 0.8 instead of 0, so the error is 0.8, and the error multiplied by two is 1.6.  Works! </p><br><p>  In more complex systems, it will be especially useful that TensorFlow automatically calculates and applies these gradients for us. </p><br><p>  Let's apply a gradient to end back propagation. </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>sess.run(optim.apply_gradients(grads_and_vars)) &gt;&gt;&gt; sess.run(w) <span class="hljs-comment"><span class="hljs-comment">## 0.75999999 # about 0.76</span></span></code> </pre> <br><p>  The weight decreased by 0.04 because the optimizer took away the gradient multiplied by the learning level, 1.6 * 0.025, moving the weight in the right direction. </p><br><p>  Instead of leading the optimizer by the handle in this way, you can do an operation that calculates and applies the gradient: <code>train_step</code> . </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>train_step = tf.train.GradientDescentOptimizer(<span class="hljs-number"><span class="hljs-number">0.025</span></span>).minimize(loss) &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">100</span></span>): &gt;&gt;&gt; sess.run(train_step) &gt;&gt;&gt; &gt;&gt;&gt; sess.run(y) <span class="hljs-comment"><span class="hljs-comment">## 0.0044996012</span></span></code> </pre> <br><p>  After launching the training step several times, the weight and final value became very close to zero.  Neuron learned! </p><br><h1>  Diagnostics training in TensorBoard </h1><br><p>  We may be wondering what happens during training.  For example, we want to follow up on what the system predicts at every step of the training.  You can display the value on the screen at each step of the cycle. </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">100</span></span>): &gt;&gt;&gt; print(<span class="hljs-string"><span class="hljs-string">'before step {}, y is {}'</span></span>.format(i, sess.run(y))) &gt;&gt;&gt; sess.run(train_step) &gt;&gt;&gt; <span class="hljs-comment"><span class="hljs-comment">## before step 0, y is 0.800000011921 ## before step 1, y is 0.759999990463 ## ... ## before step 98, y is 0.00524811353534 ## before step 99, y is 0.00498570781201</span></span></code> </pre> <br><p>  It will work, but there are some problems.  Difficult to perceive the list of numbers.  The schedule would be better.  Even with one output value too much.  And we probably want to follow a few values.  It would be nice to write more systematically. </p><br><p>  Fortunately, the same system that was used to visualize the graph includes the mechanism we need. </p><br><p>  Add to the computation graph an operation that briefly describes its state.  In our case, the operation reports the current value of <code>y</code> , the current output of the neuron. </p><br><pre> <code class="hljs ruby"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;</span></span>&gt; summary_y = tf.scalar_summary(<span class="hljs-string"><span class="hljs-string">'output'</span></span>, y)</code> </pre> <br><p>  Running this operation returns a string in the protocol buffer format that can be written to the logs directory using the <code>SummaryWriter</code> . </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>summary_writer = tf.train.SummaryWriter(<span class="hljs-string"><span class="hljs-string">'log_simple_stats'</span></span>) &gt;&gt;&gt; sess.run(tf.initialize_all_variables()) &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">100</span></span>): &gt;&gt;&gt; summary_str = sess.run(summary_y) &gt;&gt;&gt; summary_writer.add_summary(summary_str, i) &gt;&gt;&gt; sess.run(train_step) &gt;&gt;&gt;</code> </pre> <br><p>  Now, after running <code>tensorboard --logdir=log_simple_stats</code> , an interactive graph is displayed on the <code>localhost:6006/#events</code> page (Figure 3). </p><br><p><img src="https://habrastorage.org/files/a20/edd/528/a20edd5289494ca49706024dad878ff6.jpg"></p><br><p>  <em>Image 3. TensorBoard visualization of the output value of the neuron and the number of training iteration.</em> </p><br><h1>  Moving on </h1><br><p>  Here is the final version of the code.  There is not much of it, and each part shows the useful (and understandable) functionality of TensorFlow. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf x = tf.constant(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, name=<span class="hljs-string"><span class="hljs-string">'input'</span></span>) w = tf.Variable(<span class="hljs-number"><span class="hljs-number">0.8</span></span>, name=<span class="hljs-string"><span class="hljs-string">'weight'</span></span>) y = tf.mul(w, x, name=<span class="hljs-string"><span class="hljs-string">'output'</span></span>) y_ = tf.constant(<span class="hljs-number"><span class="hljs-number">0.0</span></span>, name=<span class="hljs-string"><span class="hljs-string">'correct_value'</span></span>) loss = tf.pow(y - y_, <span class="hljs-number"><span class="hljs-number">2</span></span>, name=<span class="hljs-string"><span class="hljs-string">'loss'</span></span>) train_step = tf.train.GradientDescentOptimizer(<span class="hljs-number"><span class="hljs-number">0.025</span></span>).minimize(loss) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> value <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> [x, w, y, y_, loss]: tf.scalar_summary(value.op.name, value) summaries = tf.merge_all_summaries() sess = tf.Session() summary_writer = tf.train.SummaryWriter(<span class="hljs-string"><span class="hljs-string">'log_simple_stats'</span></span>, sess.graph) sess.run(tf.initialize_all_variables()) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">100</span></span>): summary_writer.add_summary(sess.run(summaries), i) sess.run(train_step)</code> </pre> <br><p>  This example is even simpler than the examples from <a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning by</a> Michael Nielsen, which served as inspiration.  Personally, the study of such details helps me to understand and build more complex systems that use simple building blocks as a basis. </p><br><p>  If you want to continue experiments with TensorFlow, then I advise you to try to make more interesting neurons, for example, with a different <a href="https://en.wikipedia.org/wiki/Activation_function">activation function</a> .  You can make training with more interesting data.  You can add more neurons.  You can add more layers.  You can dive into more complex <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/models">ready-made models</a> , or spend more time studying your own <a href="https://www.tensorflow.org/versions/r0.8/tutorials/">manuals</a> and <a href="https://www.tensorflow.org/versions/r0.8/how_tos/">TensorFlow</a> guides.  Successes! </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/305578/">https://habr.com/ru/post/305578/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../305568/index.html">The work of the technical support department of the local positioning system</a></li>
<li><a href="../305570/index.html">Tour of the world fintech hubs</a></li>
<li><a href="../305572/index.html">Digest of recent advances in cryptography. First release</a></li>
<li><a href="../305574/index.html">Torrent Monitoring and Auto Jump</a></li>
<li><a href="../305576/index.html">Treatment of all js-files on the server or the definition of the encryption method in the day off</a></li>
<li><a href="../305580/index.html">Global range of FINTECH accelerators</a></li>
<li><a href="../305582/index.html">9 Russian companies that make interesting content</a></li>
<li><a href="../305584/index.html">UltraVDS has launched a loyalty program for its VDS / VPS servers</a></li>
<li><a href="../305586/index.html">Everything under control: we protect corporate conversations</a></li>
<li><a href="../305590/index.html">Hidden e-commerce hazards and valid SSL certificates</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>