<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Sudden sofa of a leopard coloring</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="If you are interested in artificial intelligence and other recognition, you probably already saw this picture: 


 And if you have not seen it, then t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Sudden sofa of a leopard coloring</h1><div class="post__text post__text-html js-mediator-article">  If you are interested in artificial intelligence and other recognition, you probably already saw this picture: <br><br><img src="https://habrastorage.org/files/262/417/fc6/262417fc6b5d4ec3b56f8be2d6108c9b.png"><br>  <i>And if you have not seen it, then these are the results of Hinton and Krizhevsky according to the ImageNet-2010 classification by a deep convolutional network</i> <br><br>  Let's take a look at its right corner, where the algorithm identified the leopard with reasonable confidence, placing the jaguar and the cheetah in the second and third place by a large margin. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      This is generally quite a curious result, if you think about it.  Because ... let's say <i>you</i> know how to distinguish one big spotted cat from another big spotted cat?  I, for example, no.  Surely there are some zoological, rather subtle differences, such as general slimness / massiveness and body proportions, but we are still talking about a computer algorithm that still make some kind of mistakes <a href="">that are</a> stupid enough from a human point of view.  How does he do it, damn it?  Maybe there is something related to the context and background (leopard is more likely to be found on a tree or in the bushes, and a cheetah in the savannah)?  In general, when I first thought about this particular result, it seemed to me that it is very cool and powerful, intelligent machines are just around the corner waiting for us, long live deep learning and all that. <br><br>  So, in fact, everything is completely wrong. <br><a name="habracut"></a><br><h2>  One small zoological fact </h2><br>  So let's look at our cats a little closer.  This is the jaguar: <br><br><img src="https://habrastorage.org/files/90e/7d5/3ad/90e7d53ad8f8432bb488eca63dbe299c.jpg"><br><br>  The biggest cat on the continents of both Americas, with the only one that periodically kills the victim by piercing the scalp with the fangs and biting into the brain.  This is not a zoological fact that is important for our topic, but nonetheless.  Among the characteristic signs are eye color, large jaw and in general they are the most massive of the whole trio.  As you can see, quite subtle details. <br><br>  Here it is - a leopard: <br><br><img src="https://habrastorage.org/files/6cc/de7/1c1/6ccde71c1f4f4a5fb3541766dabf3895.jpg"><br><br>  He lives in Africa, second only to a lion on this continent.  They have light (yellow) eyes, and significantly smaller paws than the jaguar. <br><br>  And finally, the cheetah: <br><br><img src="https://habrastorage.org/files/384/0d9/eab/3840d9eab81d406fa2f6f6de812b8d6f.jpg"><br><br>  Perceptibly smaller in size than his two fellows.  It has a long, slender body, and at last at least something that can serve as a visually noticeable sign - a distinctive pattern on the face, similar to a dark path of tears from one eye to the other. <br><br>  And here he is the same zoological fact: it turns out that the spots on the skin of these cats are not at all randomly located.  They are assembled in several pieces in small groups, which are called "sockets".  Moreover, the leopard has conditionally small sockets, the jaguar has much more (and with small black dots inside), and the cheetah does not have them at all - just a scattering of lonely standing spots. <br><br><img src="https://habrastorage.org/files/23c/082/d97/23c082d9720b41239902219b9e2a3de3.jpg"><br>  <i>Here it is seen better.</i>  <i>Spontaneous education sponsor is <a href="http://imgur.com/gallery/md8HT">Imgur</a> , and I apologize in case I misrepresented something from the materiel.</i> <br><br><h2>  Bees are beginning to suspect </h2><br>  Somewhere at that moment a terrible guess starts to sneak into the brain - what if this difference in the texture of the spots <i>is the</i> main criterion by which the algorithm distinguishes three possible recognition classes from each other?  That is, in fact, the convolutional network does not pay attention to the shape of the depicted object, the number of paws, the thickness of the jaw, the peculiarities of the posture and all these subtle differences that, as we have assumed, it can understand - and simply compares the pictures as two pieces of texture? <br><br>  This assumption needs to be verified.  Let's take to check a simple, unsophisticated image, without any noise, distortion and other factors that complicate the life of recognition.  I am sure that at first sight any person can easily recognize this picture. <br><br><img src="https://habrastorage.org/files/91a/21d/689/91a21d689f9c463cb06c5523e3e3d45e.jpg"><br><br>  For the test, we will use <a href="http://caffe.berkeleyvision.org/">Caffe</a> and the tutorial on recognition on pre-trained models, which lies right on their website.  Here we are using not the same model, which is mentioned at the beginning of the post, but similar (CaffeNet), and in general for our purposes all convolution networks will show approximately the same result. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt caffe_root = <span class="hljs-string"><span class="hljs-string">'../'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys sys.path.insert(<span class="hljs-number"><span class="hljs-number">0</span></span>, caffe_root + <span class="hljs-string"><span class="hljs-string">'python'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> caffe MODEL_FILE = <span class="hljs-string"><span class="hljs-string">'../models/bvlc_reference_caffenet/deploy.prototxt'</span></span> PRETRAINED = <span class="hljs-string"><span class="hljs-string">'../models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'</span></span> IMAGE_FILE = <span class="hljs-string"><span class="hljs-string">'../sofa.jpg'</span></span> caffe.set_mode_cpu() net = caffe.Classifier(MODEL_FILE, PRETRAINED, mean=np.load(caffe_root + <span class="hljs-string"><span class="hljs-string">'python/caffe/imagenet/ilsvrc_2012_mean.npy'</span></span>).mean(<span class="hljs-number"><span class="hljs-number">1</span></span>).mean(<span class="hljs-number"><span class="hljs-number">1</span></span>), channel_swap=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>), raw_scale=<span class="hljs-number"><span class="hljs-number">255</span></span>, image_dims=(<span class="hljs-number"><span class="hljs-number">500</span></span>, <span class="hljs-number"><span class="hljs-number">500</span></span>)) input_image = caffe.io.load_image(IMAGE_FILE) prediction = net.predict([input_image]) plt.plot(prediction[<span class="hljs-number"><span class="hljs-number">0</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">'predicted class:'</span></span>, prediction[<span class="hljs-number"><span class="hljs-number">0</span></span>].argmax() plt.show()</code> </pre> <br>  What happened: <br><br><img src="https://habrastorage.org/files/91f/6c7/54f/91f6c754f4294cef9267fbdc699b917a.png"><br><br><pre> <code class="bash hljs">&gt;&gt; predicted class: 290</code> </pre><br><img src="https://habrastorage.org/files/e03/2d6/fed/e032d6fedb144ea78cc0caa06152c623.png"><br>  <i>Oops.</i> <br><br>  But hey, maybe this is a problem of a single model?  Let's do more checks: <br><br>  <b><a href="http://www.clarifai.com/">Clarifai</a></b> <br><br><img src="https://habrastorage.org/files/525/eed/757/525eed757c1a4d46a81549d48ce135af.png"><br><br>  <b><a href="https://www.imageidentify.com/">Steven Wolfram's recent mega-service</a></b> <br><br><img src="https://habrastorage.org/files/1de/40f/a87/1de40fa87ef54855968c7c23fa9f709e.png"><br><br>  Here the picture is turned to the side - this means that the service coped with the correctly oriented sofa, but it broke when turning 90 degrees.  In a sense, it is even worse than if he could not cope at all - such a simple transformation should not radically change the result.  It seems that our guess about the texture is close to the truth. <br><br>  Surprisingly, there are not so many open web recognition services.  I made a couple of checks ( <a href="https://www.projectoxford.ai/demo/visions">Microsoft</a> , <a href="https://images.google.com/">Google</a> ) - some of them behave better without slipping a jaguar, but no one could win the sofa.  A good result in a world where headlines in the spirit of "{Somebody} 's Deep Learning Project Outperforms Humans In Image Recognition" are already flashing. <br><br><h2>  Why it happens? </h2><br>  Here is one of the guesses.  Let's imagine ourselves in the place of an ordinary supervised classifier, without going into details of the architecture.  We receive many, many pictures at the entrance, each of which is marked with the corresponding class, and further we adjust our parameters so that for each picture the output data correspond to this very class.  Thus, we hope to extract some internal structure, distinctive features from the image, to formulate an analytically inexpressible recognition rule - so that later new, unfamiliar images that have these characteristics fall into the desired class.  In the process of learning, we are guided by the magnitude of our prediction error, and here the size and proportions of the sample are important - if we have 99 images of class A and one image of class B, then at the very stupid behavior of the classifier (‚Äúalways say A‚Äù) the error will be equal to 1 %, despite the fact that we especially have not learned anything. <br><br>  So, from this point of view, there is no problem here.  The algorithm behaves as it should behave.  The sudden leopard sofa is an anomaly, a rarity in the ImageNet sample, and in our everyday experience too.  On the other hand, the distinctive pattern of dark spots on a light background is a wonderful distinguishing feature of large spotted cats.  Where else in the end will you see such a pattern?  Moreover, taking into account that the classifier is charged with devilishly difficult tasks in the spirit of distinguishing different types of cats - the use of the pattern becomes <i>even</i> greater.  If we had enough of the number of leopard sofas in the sample, he would have to invent new criteria for the difference (I wonder what the result would be), but if there are none, then the error in such a rare case is completely natural. <br><br>  Or not? <br><br><h2>  What we humans do </h2><br>  Remember the first school classes when you were learning to write numbers. <br><br>  Each of the students was then brought in a weighty book called ‚ÄúMNIST database‚Äù, where sixty thousand numbers were written out on hundreds of pages, all with different handwritings and styles, bold and slightly noticeable italics.  Especially stubborn reached the even more huge application ‚ÄúPermutation MNIST‚Äù, where these same figures were rotated at different angles, stretched up and down and to the sides, and shifted right and left - without this it was impossible to learn how to determine the figure by looking at it at an angle.  Then, when the long and tedious training ended, everyone was given a small (comparatively) list of 10,000 digits, which had to be correctly identified in the general class testing.  But after mathematics came the next lesson, where you had to learn a much more voluminous alphabet ... <br><br>  Hm  Say, it was all wrong? <br><br>  It is curious, but it seems that you and I do not really need a sample in the learning process.  At least, if it is needed, then not for the purpose of why it is used now when training classifiers ‚Äî to develop resistance to spatial permutations and distortions.  We perceive the same handwritten numbers as abstract Platonic concepts - a vertical stick, two circles one above the other.  If, for example, we get an image where there is none of these concepts, we will reject it as ‚Äúnot a digit‚Äù, but the supervised classifier will never do that.  Our computer algorithms do not look for concepts - they rake up a pile of data, stuffing them into heaps, and in the end every picture should end up in a handful with which it has a little more in common than with the others. <br><br>  Have you seen leopards in your life?  Maybe decently, but certainly less than the "glasses", "computers" and "persons" (other classes from ImageNet).  Have you ever met sofas of this color?  For example, I almost never think.  And yet, none of those who read this text thought for a moment before correctly classifying the above picture. <br><br><h2>  Convolution networks exacerbate the situation. </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/5d8/f45/222/5d8f4522294d0514ae31b2a471b287d5.gif"><br><br>  Convolutional networks constantly show consistently high results in competitions based on ImageNet, and as of 2014, they seem to be second only to ensembles from convolutional networks.  You can read about them in more detail in many places - for now we will limit ourselves only to the fact that these are networks that form small ‚Äúfilters‚Äù (or kernels) in the learning process by which they go over the image, activating in those places where there is an element corresponding to a particular filter.  They are very fond of using them when recognizing images - firstly, because there are often local signs that can appear in different places of the picture, and secondly, because it is significantly computationally cheaper than cramming huge (1024x768 = ~ 800,000 parameters) picture in a regular network. <br><br>  Let's imagine our leopards again.  These are quite complex physical bodies that can take a bunch of different positions in space, and also photographed from different angles.  Each picture of a leopard is likely to contain a unique, almost non-repeating outline - somewhere you can see a mustache, paws and a tail, and somewhere only a slurred back.  In such situations, the convolutional network is simply our savior, because instead of trying to come up with one rule for this whole set of forms, we simply say "take this set of small distinctive signs, run them through the picture and summarize the number of matches."  Naturally, the texture of leopard spots becomes a good sign - it exists in many places, and practically does not change when the object's position changes.  That is why models like CaffeNet perfectly respond to spatial variations of objects in the pictures - and that is why, ahem, the sofa happens to them. <br><br>  This is actually a very unpleasant property.  Refusing to fully analyze the shape of an object in the picture, we begin to perceive it simply as a set of signs, each of which can be anywhere without any connection with the others.  Using convolutional nets, we immediately refuse to distinguish between a cat sitting on the floor and an inverted cat sitting on the ceiling.  This is good and great when recognizing individual pictures somewhere on the Internet, but if such computer vision coordinates your behavior in real life, it will not be great at all. <br><br>  If this argument does not sound very convincing to you, take a look at <a href="http://www.cs.toronto.edu/~fritz/absps/transauto6.pdf">Hinton‚Äôs article</a> , which has been around for several years, and where the phrase ‚Äúconvolutional networks are doomed‚Äù sounds distinctly - for this very reason.  The main part of the article is devoted to the development of an alternative concept - ‚Äúcapsule theory‚Äù (on which he is working <a href="http://www.kdnuggets.com/2014/12/geoffrey-hinton-talks-deep-learning-google-everything.html">right now</a> ), and is also very worthwhile to read. <br><br><h2>  Total </h2><br>  Think about it for a while (and not very seriously) - we are doing everything wrong. <br><br>  Stuffing huge datasets with pictures, annual competitions, even deeper networks, even more GPUs.  We improved the recognition of MNIST digits from an error of 0.87 to 0.23 ( <a href="http://en.wikipedia.org/wiki/MNIST_database">proof</a> ) - in three years (no one still knows exactly what kind of mistake a person can make).  In ImageNet Challenges, the competitors' account goes to tenths of a percent - it seems, here is a little bit more, and we will definitely win, and we will get real computer vision.  And yet at the same time - no.  All that we are doing is trying to scatter a bunch of pictures into categories as accurately as possible, without understanding this (I realize that the word ‚Äúunderstand‚Äù is dangerous to use here, but) what is depicted on them.  Something must be different.  Our algorithms should be able to determine the spatial position of the object, and still rotated on the side of the sofa still recognize as a sofa, but with a slight warning on the topic "owner, you take out the furniture."  They should be able to learn quickly, with just a few examples - like you and me - and not require multi-class sampling while extracting the necessary attributes from the objects themselves. <br><br>  In general, we clearly have something to do. </div><p>Source: <a href="https://habr.com/ru/post/259191/">https://habr.com/ru/post/259191/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../259181/index.html">Reuters: US plans to target Stuxnet to North Korea</a></li>
<li><a href="../259183/index.html">16 reasons why players leave your game</a></li>
<li><a href="../259185/index.html">Game server on Scala + Akka: Case study</a></li>
<li><a href="../259187/index.html">Shadow DOM (Shady DOM)</a></li>
<li><a href="../259189/index.html">The implementation of one of the variants of the mobile version of the site</a></li>
<li><a href="../259193/index.html">Qucs project news: preparing for release 0.0.19</a></li>
<li><a href="../259195/index.html">The digest of interesting materials for the mobile # 105 developer (on May 25-31)</a></li>
<li><a href="../259197/index.html">What do we know about MODX 3 at the moment?</a></li>
<li><a href="../259199/index.html">Introduction to KDF on the example of solving a cryptographic rebus</a></li>
<li><a href="../259201/index.html">MailChimp UX team: Improvements and iterations [the final part of the book]</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>