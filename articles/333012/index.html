<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Testing in Openshift: Internal cluster structure</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This is a continuation of a series of three articles ( first article , third article ) about automated testing of software products in Openshift Origi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Testing in Openshift: Internal cluster structure</h1><div class="post__text post__text-html js-mediator-article"><p>  This is a continuation of a series of three articles ( <a href="https://habrahabr.ru/post/332994/">first article</a> , <a href="https://habrahabr.ru/post/333014/">third article</a> ) about automated testing of software products in Openshift Origin.  This article will describe the main objects of Openshift, as well as the principles of the cluster.  I deliberately do not attempt to describe all possible objects and their features, since this is a very time-consuming task, which is beyond the scope of this article. </p><br><h4 id="klaster">  Cluster: </h4><br><p><img src="https://habrastorage.org/web/3b2/409/67c/3b240967c2294457bdf8bc95579f85a7.png"></p><a name="habracut"></a><br><p>  In general, the operation of the Openshift Origin cluster is not much different from other solutions.  Incoming tasks are distributed across work nodes based on their workload, this distribution is assumed by the scheduler. </p><br><p>  Container launch requires a Docker image that can be loaded from an internal or external register.  Containers are directly launched in various <a href="https://docs.openshift.org/latest/admin_guide/manage_scc.html">security contexts</a> ( <a href="https://docs.openshift.org/latest/admin_guide/manage_scc.html">security</a> policies that restrict container access to various resources). </p><br><p>  By default, containers from different projects can communicate with each other using the <a href="https://en.wikipedia.org/wiki/Overlay_network">overlay network</a> (one large subnet is allocated, which is divided into smaller subnets for all working nodes of the cluster).  When a container is launched on a working node, it is assigned an IP address from the subnet that was assigned to this node.  The overlay network itself is based on the Open vSwitch, which uses <a href="https://en.wikipedia.org/wiki/Virtual_Extensible_LAN">VXLAN</a> for communication between the working nodes. </p><br><p>  On each working node, a dedicated instance of <a href="http://www.thekelleys.org.uk/dnsmasq/doc.html">Dnsmasq is</a> launched, which redirects all DNS container requests to SkyDNS to the internal service subnet. </p><br><p>  If the container has crashed its work or simply cannot be initialized, then the task of deploying it is transferred to another work node. </p><br><p>  It is worth noting that: </p><br><ol><li><p>  SELinux is not a strict condition for cluster operation.  Turning it off ( <a href="https://www.youtube.com/watch%3Fv%3DgiFKMsIH4b0">not recommended for security reasons</a> ) will introduce some kind of speed increase (as well as turning off monitoring, by the way) when working with containers.  If SELinux interferes with the operation of the application in the container, it is possible to add SELinux exceptions directly to the work node of the cluster. </p><br></li><li><p>  By default, <a href="https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)">LVM is</a> used as the Docker Engine repository.  This is not the fastest solution, but you can use any other type of storage ( <a href="https://docs.docker.com/engine/userguide/storagedriver/btrfs-driver/">BTRFS</a> , for example). </p><br></li><li><p>  It should be borne in mind that the name of the service (see Service) - is the DNS name, which entails restrictions on the length and valid characters. </p><br></li><li><p>  To reduce the time and hardware costs when building Docker images, you can use the so-called "layered" approach ( <a href="https://docs.docker.com/engine/userguide/eng-image/multistage-build/">multi-stage in Docker</a> ).  This approach uses basic and intermediate images that complement each other.  We have the basic image "centos: 7" (fully updated), we have the intermediate image "centos: 7-tools" (installed tools), we have the final image "centos: 7-app" (contains "centos: 7" and "centos: 7 -tools ").  That is, you can create build tasks that are based on other images (see BuildConfig). </p><br></li><li><p>  A fairly flexible solution is the approach when there is one project that only builds Docker images and then links the image data to other projects (see ImageStream).  This allows you to not produce unnecessary entities in each project and will lead to some unification. </p><br></li><li><p>  Most objects in the cluster can be assigned arbitrary labels with which you can perform bulk operations on these objects (deleting certain containers in a project, for example). </p><br></li><li><p>  If the application requires some kernel functionality of the Linux kernel, then it is required to load this module on all the work nodes where this application needs to be launched. </p><br></li><li><p>  It should immediately worry about the removal of old images and forgotten environments.  If the first is solved with the help of the garbage collector / oadm prune, then the second requires the study and familiarization of all participants with the rules of joint work in Openshift Origin. </p><br></li><li><p>  Any cluster is limited by resources, so it is very desirable to organize monitoring at least at the level of work nodes (monitoring at the application level in the container is possible).  This can be done both with the help of a ready-made solution <a href="https://github.com/openshift/origin-metrics">Openshift Metrics</a> , and with the help of third-party solutions ( <a href="https://www.sysdig.org/">Sysdig</a> , for example).  In the presence of cluster load metrics (as a whole or according to design), it is possible to organize flexible dispatching of incoming tasks. </p><br></li><li><p>  I especially want to note the fact that work nodes can be dynamically initialized, which means that you can expand your Openshift Origin cluster on existing IaaS facilities.  That is, during pre-release testing, you can significantly expand your capacities and reduce testing time. <br><br></p><br></li></ol><br><h4 id="obekty">  Objects: </h4><br><p>  <strong>Project</strong> - The object is a <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/">Kubernetes namespace</a> .  The upper level of abstraction, which contains other objects.  Objects created in the project do not overlap with objects in other projects.  Quotas, privileges, labels of cluster nodes, etc. can be set up for a project.  There is no nested hierarchy and inheritance between projects; a "flat" project structure is available.  There are several system projects (kube-system, openshift, openshift-infra) that are intended for the normal functioning of the cluster. </p><br><p>  Creating a new project: </p><br><pre><code class="plaintext hljs">oc adm new-project project1 --node-selector='node_type=minion'</code> </pre> <br><p>  Editing project settings: </p><br><pre> <code class="plaintext hljs">oc edit namespace project1</code> </pre> <br><pre> <code class="plaintext hljs"># Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 kind: Namespace metadata: annotations: openshift.io/description: "" openshift.io/display-name: "" openshift.io/node-selector: node_type=minion ...</code> </pre> <br><br><p>  <strong>Pod</strong> is an object that has become one of the decisive factors, as it allows you to run arbitrary commands inside the container using special hooks (and not only).  Pod is the main unit of work in the cluster.  Any client running in the cluster - Pod.  At its core, a group of one or more containers, which work in the same <a href="https://lwn.net/Articles/531114/">namespaces (network, ipc, uts, cgroup)</a> for these containers, use shared data storage and secrets.  The containers that make up the Pods are always running on one node of the cluster, and not distributed in equal proportions across all nodes (if Pod consists of 10 containers, all 10 will work on one node). </p><br><p>  Pod: </p><br><pre> <code class="plaintext hljs">apiVersion: "v1" kind: "Pod" metadata: name: "nginx-redis" spec: containers: - name: "nginx" image: "nginx:latest" - name: "redis" image: "redis:latest"</code> </pre> <br><p>  Pod status: </p><br><pre> <code class="plaintext hljs">NAME READY STATUS RESTARTS AGE nginx-redis 2/2 Running 0 7s</code> </pre> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>  <strong>Secret</strong> - can be a string or a file, designed for forwarding sensitive (stored in clear form in etcd ( <a href="https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/">support for encryption in Kubernetes 1.7</a> )) information in the Pod.  One Secret can contain many values. </p><br><p>  Create Secret: </p><br><pre> <code class="plaintext hljs">oc secrets new gitconfig .gitconfig=/home/user/.gitconfig</code> </pre> <br><p>  Using Secret in BuildConfig: </p><br><pre> <code class="plaintext hljs">apiVersion: "v1" kind: "BuildConfig" metadata: name: "nginx-bc" spec: source: type: "Git" git: uri: "https://github.com/username/nginx.git" sourceSecret: name: "gitconfig" strategy: type: "Docker" dockerStrategy: dockerfilePath: docker/nginx-custom noCache: true output: to: kind: "ImageStreamTag" name: "nginx-custom:latest"</code> </pre> <br><br><p>  <strong>ServiceAccount</strong> is a special type of object that is designed to interact with cluster resources.  At its core is a system user. </p><br><p>  By default, the new project contains three ServiceAccount: </p><br><ul><li>  builder is responsible for building Docker images and uploading them to the register (see BuildConfig). </li><li>  deployer - deployment tasks are launched from this account (see DeploymentConfig). </li><li>  default - all other Pods (which are not related to deployment tasks) are launched from this particular account. </li></ul><br><p>  Listed service accounts: </p><br><ol><li>  Contains automatically created secrets that are used to access cluster resources. </li><li>  Possess roles that allow them to perform certain actions in a cluster. </li></ol><br><p>  ServiceAccount: </p><br><pre> <code class="plaintext hljs">apiVersion: "v1" kind: "ServiceAccount" metadata: name: "jenkins"</code> </pre> <br><p>  ServiceAccount properties: </p><br><pre> <code class="plaintext hljs">Name: jenkins Namespace: project1 Labels: &lt;none&gt; Image pull secrets: jenkins-dockercfg-pvgsr Mountable secrets: jenkins-dockercfg-pvgsr jenkins-token-p8bwz Tokens: jenkins-token-p8bwz jenkins-token-zsn9p</code> </pre> <br><p>  Adding admin rights to the ServiceAccount project: </p><br><pre> <code class="plaintext hljs">oc policy add-role-to-user admin system:serviceaccount:project1:jenkins</code> </pre> <br><br><p>  <strong>DeploymentConfig</strong> is an object that operates with the same Pod, but at the same time introduces a number of additional mechanisms for managing the life cycle of running applications, namely: </p><br><ol><li>  Adds a deployment strategy, i.e.  allows you to determine how the application will be updated: when a new version is released, rolled back to a working version in case of failure. </li><li>  Allows you to set the triggers that will cause the configuration to be redeployed. </li><li>  Allows you to specify the number of instances / replicas of the application. </li></ol><br><p>  DeploymentConfig: </p><br><pre> <code class="plaintext hljs">apiVersion: "v1" kind: "DeploymentConfig" metadata: name: "nginx-dc" spec: template: metadata: labels: name: "nginx-dc" spec: containers: - name: "nginx" image: "nginx:latest" replicas: 3 selector: name: "nginx-dc"</code> </pre> <br><p>  DeploymentConfig Status: </p><br><pre> <code class="plaintext hljs">NAME READY STATUS RESTARTS AGE nginx-dc-1-1wl8m 1/1 Running 0 7s nginx-dc-1-k3mss 1/1 Running 0 7s nginx-dc-1-t8qf3 1/1 Running 0 7s</code> </pre> <br><br><p>  <strong>ImageStream</strong> is essentially a ‚Äúlink‚Äù container (ImageStreamTag) that points to a Docker image or other ImageStream. </p><br><p>  ImageStream: </p><br><pre> <code class="plaintext hljs">apiVersion: "v1" kind: "ImageStream" metadata: name: "third-party"</code> </pre> <br><p>  Creating a tag / link to the Docker image between projects: </p><br><pre> <code class="plaintext hljs">oc tag project2/app:v1 project1/third-party:app</code> </pre> <br><p>  Creating a tag / link to the Docker image, which is located on the Docker Hub: </p><br><pre> <code class="plaintext hljs">oc tag --source=docker nginx:latest project1/third-party:nginx</code> </pre> <br><br><p>  <strong>BuildConfig</strong> - an object is a script of how a Docker image will be assembled and where it will be placed.  Building a new image can be based on other images, the "from:" section is responsible for this. </p><br><p>  Sources of assembly (the place where the source data for the assembly): </p><br><ul><li>  Binary </li><li>  Dockerfile </li><li>  Git </li><li>  Image </li><li>  Input secrets </li><li>  External artifcats </li></ul><br><p>  Build strategies (how the data source should be interpreted): </p><br><ul><li>  Custom </li><li>  Docker </li><li>  Pipeline </li><li>  S2i </li></ul><br><p>  Purpose of the assembly (where the assembled image will be unloaded): </p><br><ul><li>  DockerImage </li><li>  ImageStreamTag </li></ul><br><p>  BuildConfig: </p><br><pre> <code class="plaintext hljs">apiVersion: "v1" kind: "BuildConfig" metadata: name: "nginx-bc" spec: source: type: "Git" git: uri: "https://github.com/username/nginx.git" strategy: type: "Docker" dockerStrategy: from: kind: "ImageStreamTag" name: "nginx:latest" dockerfilePath: docker/nginx-custom noCache: true output: to: kind: "ImageStreamTag" name: "nginx-custom:latest"</code> </pre> <br><p>  What operations this BuildConfig will perform: </p><br><ol><li>  Based on ImageStream "nginx: latest" </li><li>  Clones the Git repository, finds the docker / nginx-custom file in this repository, loads the Dockerfile instructions from this file, and executes these instructions in a basic way. </li><li>  The resulting image will put "nginx-custom: latest" in ImageStream </li></ol><br><br><p>  <strong>Service</strong> is an object that has become one of the decisive factors when choosing a system for launching environments, as it allows the flexibility to customize communications between environments (which is very important in testing).  In cases using other systems, preparatory manipulations were required: select IP address ranges, register DNS names, perform port forwarding, etc.  etc.  Service can be declared before the actual deployment of the application. </p><br><p>  What happens during the publication of the service in the project: </p><br><ol><li>  For the service is allocated an IP address from a special service subnet. </li><li>  Register the DNS name of the service.  All Pods in a project that were run before / after publishing a service will be able to resolve this DNS name. </li><li>  All Pods in the project that will be launched after the service is published will receive a list of environment variables that contain the IP address and ports of the published service. </li></ol><br><p>  Service: </p><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: Service metadata: name: "nginx-svc" spec: selector: name: "nginx-pod" ports: - port: 80 targetPort: 80 name: "http" - port: 443 targetPort: 443 name: "https"</code> </pre> <br><p>  DNS name resolution: </p><br><pre> <code class="plaintext hljs">root@nginx-pod:/# ping nginx-svc PING nginx-svc.myproject.svc.cluster.local (172.30.217.250) 56(84) bytes of data.</code> </pre> <br><p>  Environment variables: </p><br><pre> <code class="plaintext hljs">root@nginx-pod:/# env | grep -i nginx NGINX_SVC_PORT_443_TCP_ADDR=172.30.217.250 HOSTNAME=nginx-pod NGINX_VERSION=1.13.1-1~stretch NGINX_SVC_PORT_80_TCP_PORT=80 NGINX_SVC_PORT_80_TCP_ADDR=172.30.217.250 NGINX_SVC_SERVICE_PORT=80 NGINX_SVC_PORT_80_TCP_PROTO=tcp NGINX_SVC_PORT_443_TCP=tcp://172.30.217.250:443 NGINX_SVC_SERVICE_HOST=172.30.217.250 NGINX_SVC_PORT_443_TCP_PROTO=tcp NGINX_SVC_SERVICE_PORT_HTTPS=443 NGINX_SVC_PORT_443_TCP_PORT=443 NGINX_SVC_PORT=tcp://172.30.217.250:80 NGINX_SVC_SERVICE_PORT_HTTP=80 NGINX_SVC_PORT_80_TCP=tcp://172.30.217.250:80</code> </pre> <br><p>  <strong>Conclusion:</strong> </p><br><p>  All cluster objects can be described using YAML, this, in turn, makes it possible to fully automate any processes that occur in Openshift Origin.  The whole difficulty in working with a cluster is in the knowledge of the principles of operation and mechanisms of interaction of objects.  Routine operations like initialization of new work nodes are covered by Ansible scripts.  The availability of the API opens up the opportunity to work with the cluster directly bypassing intermediaries. </p></div><p>Source: <a href="https://habr.com/ru/post/333012/">https://habr.com/ru/post/333012/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../332998/index.html">ES8: major innovations</a></li>
<li><a href="../333000/index.html">IBM has created a new generation of carbon nanotube transistors</a></li>
<li><a href="../333002/index.html">Creating Shaders</a></li>
<li><a href="../333006/index.html">Noty.js V3 is a smart javascript library for creating notifications. And also a ready plugin for vuejs</a></li>
<li><a href="../333008/index.html">Doctor Web: gosuslugi.ru portal has been compromised and may begin to infect visitors or steal information</a></li>
<li><a href="../333014/index.html">Testing in Openshift: Automated Testing</a></li>
<li><a href="../333016/index.html">Delhi - cut, or as we did mobile 2GIS Online</a></li>
<li><a href="../333018/index.html">What is ERP system</a></li>
<li><a href="../333020/index.html">Creating a blog engine with Phoenix and Elixir / Part 10. Channel testing</a></li>
<li><a href="../333024/index.html">Top Achievement Code</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>