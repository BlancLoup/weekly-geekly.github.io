<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>"Calendar of the tester" for April. Metrics in QA service</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The April article from the ‚ÄúCalendar of the Tester‚Äù series is devoted to metrics. Kirill Ratkin, Kontur.Externa tester, will tell you how to increase ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>"Calendar of the tester" for April. Metrics in QA service</h1><div class="post__text post__text-html js-mediator-article"><blockquote>  The April article from the <a href="http://t.me/QAcalendar">‚ÄúCalendar of the Tester‚Äù series is</a> devoted to metrics.  Kirill Ratkin, Kontur.Externa tester, will tell you how to increase the effectiveness of testing with their help and not go to extremes. </blockquote><br><img src="https://habrastorage.org/webt/bf/nu/mu/bfnumuk-196h9xogc_fohnuhivq.png"><br><br><p>  How often do you have to evaluate something?  Probably every day.  Good or bad weather today, whether the cat behaves tolerably, whether you like this shirt.  At work, you assess your goals and results: this is done well, and here you can do better.  Such assessments are often based on subjective feeling.  But these estimates can not improve the efficiency of processes, and need more detail.  Then metrics come to the rescue. </p><br><p>  How can you characterize your workflows and practices?  They are good?  Bad?  How much?  Why do you think so? </p><br><p>  I can‚Äôt hold back and quote Lord Kelvin‚Äôs words: </p><br><blockquote>  ‚ÄúIf you can measure what you are talking about and express it in numbers, then you know something about this subject.  But if you cannot express it quantitatively, your knowledge is extremely limited and unsatisfactory. ‚Äù </blockquote><p>  No process can be considered mature until it becomes transparent and manageable. </p><br><p>  I have seen two extremes: </p><br><ol><li>  People think that everything is good / bad and without radar.  "Well, this is already clear" (c). </li><li>  Each step is hung with numbers, but most of them are dead weight and are not used at all. <a name="habracut"></a></li></ol><br><p>  The truth, as always, is in the middle.  To get closer to it, you need to understand the fundamental thing: metrics are not a tribute to fashion and not a panacea for bad processes.  Metrics are just a tool, and the results of the work of this tool are not an end in itself, but only some basis for further steps. </p><br><p>  <strong>Metrics are needed for:</strong> </p><br><ul><li>  <strong>Enhance objectivity.</strong>  For example, nothing will be able to characterize the stability of your autotests more precisely than the dry numbers of the percentage of passing. </li><li>  <strong>Visualization</strong> </li><li>  <strong>Estimates of the dynamics of change.</strong>  If you cannot do everything you have planned, then choose more effective tasks from the list.  To make an informed decision, you need to understand what practices have a higher efficiency. </li></ul><br><p>  Properly gash metrics is not easy.  First, you need to clearly describe the causal relationships, identify all the factors influencing the process, and give them your weight.  Secondly, to technically implement the collection itself. </p><br><h1 id="metriki-avtotestov">  Autotest metrics </h1><br><p>  If you are going to enter metrics in autotests, you need to understand why it is for you. </p><br><p>  <strong>The specifics of our autotests:</strong> </p><br><ul><li>  Presence of guvy user scenarios. </li><li>  Integration checks with test stands of third-party services. </li><li>  Cross-browser compatibility. </li><li>  Configuring CI and test virtualok. </li><li>  Providing our product AT to other service teams. </li></ul><br><p>  We spent a lot of energy on maintaining stability and speed, and even appointed a duty officer for autotests.  When we no longer understand how good or bad everything is, how much resources we spend on it, we began to envelop our system with metrics. </p><br><img src="https://habrastorage.org/webt/re/64/oq/re64oqdj-q79ixb1vfxwkeyci4c.png"><br><p>  <em>Metrics attendant for autotests</em> </p><br><p>  We began to pay attention not only to the stability of the test and run time, but also to: </p><br><ul><li>  <strong>red run time</strong> <br>  Shows how long the test falls.  Runs that fall for a long time keep the queue on virtuals.  Sometimes WebDriver throws exceptions, and such tests could hang for hours.  Such situations are solved a level higher for all tests, but you can get rid of incorrect timeouts and expectations only by touching a specific test.  We are required to deal with runs that have this metric for more than 20 minutes. </li><li>  <strong>test time effectiveness</strong> <br>  How much the agent spends on the real run, and not on instability.  This metric is about noisy tests.  We wanted to understand how much time is spent on green and red runs, and calculate the ratio.  Tests with low efficiency are first in line for refactoring. </li><li>  <strong>relative time lost</strong> <br>  This metric shows how many minutes are lost for each run of this test due to instability.  In our team, it is considered the second most important after stability. </li></ul><br><p>  Plus, we took into account a number of specific features for us: </p><br><ul><li>  <strong>The unique key for us was not the name of the test, but a couple of test - the browser.</strong> <br>  The same UI test behaves differently in fast and slow browsers, different JS interpreters, etc.  Therefore, the tester rules not Test1, but Test1 in Browser1. </li><li>  <strong>Some agents with seemingly identical environments (OS, browser, etc.) differed from each other in the stability of passing tests.</strong> <br>  There are difficulties with the generation of absolutely identical environments.  Plus, virtuals are not closed from developers or testers who want to "podobezhit something."  Therefore, while these problems have not been solved, it is necessary to make samples for running tests in the context of each agent.  If some test passes on a specific agent is noticeably worse, then this triggers the person on duty to communicate more closely with this virtual machine. </li></ul><br><img src="https://habrastorage.org/webt/xg/qo/lr/xgqolrz63p0crwf6mewvgadohg8.png"><br><p>  <em>Search for corrupted virtual machines</em> </p><br><p>  Most teams can collect metrics using CI.  We didn‚Äôt have enough of it - we‚Äôve too tricky to set up our categories and launchers.  I had to write a small wrapper, which in TearDown sent to the database the build meta with all the indicators. </p><br><p>  <strong>Our goals:</strong> </p><br><ol><li>  Assessment of the situation </li><li>  Information for targeted response </li></ol><br><p>  Metrics made it possible to understand by what indicators we subside.  Next we set weights, benchmarks and work out the order of achieving the goal. </p><br><p>  <strong>Once a fortnight at the bat with the attendant we:</strong> </p><br><ul><li>  choose which characteristic will be influenced in the first place; </li><li>  discuss the causes of instability; </li><li>  we prioritize tasks; </li><li>  pretending to work front; </li><li>  plan targets. </li></ul><br><p>  These metrics are collected automatically every morning after nightly runs, consisting of a set of repeated test runs. </p><br><h1 id="ocenka-pokrytiya-avtotestami">  Autotest Coverage Evaluation </h1><br><p>  Another big task is to assess coverage.  No other option for calculating this indicator suited me completely.  What coverage do you think?  Share in the comments. </p><br><p>  <strong>Someone considers coverage on hand-written cases.</strong>  There are a number of drawbacks.  First, it is the human factor.  It is good if the task is taken by a competent analyst who will rely on representative statistics.  But in this case, you can skip something, do not take into account.  Secondly, as the system under test grows, this approach becomes too expensive to maintain. </p><br><p>  <strong>Someone considers code coverage.</strong>  But it does not take into account with which parameters we call this or that method.  Indeed, by going to a method that divides A into B with some ‚Äúsafe‚Äù arguments, we cannot say that the method is covered and tested.  And if the division by zero?  And if overflow?  The criticality of the uncovered code is also not taken into account, and therefore the motivation to increase this coverage is unclear. </p><br><p>  <strong>An ideal option for us is to consider covering popular user yuzkeys.</strong>  And we will consider the state of the system and atomic transitions between them.  The main difference from the first version of the calculation is the absence of the human factor.  Cases will be generated by a script.  Those.  switching to a specific page or some action on it sends a request to our metrics service.  This is a feature of our system under test, so statistics are collected automatically. </p><br><p>  Next, the principle is simple: </p><br><ul><li>  we calculate the top N of the popular states of our test system from combat sites, </li><li>  consider pairwise transitions between these states, </li><li>  sort them by the number of transitions, </li><li>  go from top to bottom and check if we have such a transition to AT. </li></ul><br><p>  This algorithm is restarted once a month to update the top.  For all the missing tests, tasks are added to the <a href="https://habrahabr.ru/company/skbkontur/blog/349300/">backlog</a> and cleared out in a separate thread. </p><br><h1 id="metriki-reliznogo-cikla">  Release Cycle Metrics </h1><br><p>  Another place of application of metrics served as our release cycle.  I do not know about you, but in our team the testers resource is limited.  Therefore, updates that are ready for testing are not taken immediately, but with a delay.  At some point, this time lag began to cause discomfort, and we decided to assess the situation. </p><br><p>  We began to consider how long the task is at each stage of the release cycle. </p><br><img src="https://habrastorage.org/webt/g3/nm/qu/g3nmquq0beceatv0ucelnxi6v9e.png"><br><p>  <em>Collection of release cycle metrics based on YouTrack</em> </p><br><p>  From all this we managed: </p><br><ul><li>  calculate how many testers should work on the release of releases, and how many can switch to other tasks, such as AT refactoring, finishing infrastructure, tools, etc., </li><li>  understand the optimal ratio of roles and, as a result, predict the number of vacancies, </li><li>  determine the points of application of force for such an intra-team goal as a technical release in an hour. </li></ul><br><p>  Now we have the ratio of writing developers to testers producing them in the region of 3 (¬± 0.1), 2.5 is considered to be comfortable.  Given the ongoing automation of the release cycle, we want to reach a ratio of 3.5 and 3. </p><br><p>  Our implementation of the collection of metrics was based on the editor of the rules of the internal team bugtracker.  This is YouTrack Workflow.  In your team, you can use the same capabilities of your trackers, or write bikes with web hooks. </p><br><p>  We brought private fields for counters for task and every hour we increment the value in them.  We took into account that the meter should tick only during working hours and only on weekdays - this is our working schedule.  Everything is cunning!  =) </p><br><h1 id="metriki-radi-metrik">  Metrics for the sake of metrics </h1><br><p>  Typical antipatterns are ‚Äúmetrics for the sake of metrics.‚Äù  As I noted above, metrics are only a tool that helps quantify how well you are achieving goals. </p><br><p>  Suppose you never understood such a metric as the number of tests.  What for?  What does the indicator say to you that in some team there are 12345 tests?  Do they test well?  Is not a fact.  Their auto tests can be trusted?  Hardly guys even know what they are checking.  Does it show autotest performance?  Perhaps there are minuses from support more than any profit - what kind of efficiency are we talking about?  In fact, this metric implies covering and trusting your test system.  But, damn, this is not the same thing.  If you want to talk about coverage, then they are operated. </p><br><p>  Always remember that collecting metrics takes you some time and effort.  Appreciate it and approach this practice wisely! </p><br><p>  It is very interesting to us, and which metrics you use, and which, on the contrary, you refused.  Write in the comments! </p><br><p>  List of calendar articles: <br>  <a href="https://habr.com/company/skbkontur/blog/433132/">Try a different approach</a> <br>  <a href="https://habr.com/company/skbkontur/blog/431460/">Reasonable pair testing</a> <br>  <a href="https://habr.com/company/skbkontur/blog/428089/">Feedback: as it happens</a> <br>  <a href="https://habr.com/company/skbkontur/blog/424349/">Optimize tests</a> <br>  <a href="https://habr.com/company/skbkontur/blog/421101/">Read the book</a> <br>  <a href="https://habr.com/company/skbkontur/blog/417287/">Analytics testing</a> <br>  <a href="https://habr.com/company/skbkontur/blog/414451/">The tester must catch the bug, read Kaner and organize a move.</a> <br>  <a href="https://habr.com/company/skbkontur/blog/358234/">Load service</a> <br>  <a href="https://habr.com/company/skbkontur/blog/353928/">Metrics in QA service</a> <br>  <a href="https://habr.com/company/skbkontur/blog/351806/">Test security</a> <br>  <a href="https://habr.com/company/skbkontur/blog/349274/">Know your customer</a> <br>  <a href="https://habr.com/company/skbkontur/blog/349300/">Disassemble backlog</a> </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/353928/">https://habr.com/ru/post/353928/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../353916/index.html">Features of games that had to be abandoned - developer analysis</a></li>
<li><a href="../353918/index.html">Concept: logo for Yekaterinburg from "Log machines"</a></li>
<li><a href="../353920/index.html">Fight for a smooth display in the video surveillance system: find jerks and neutralize</a></li>
<li><a href="../353924/index.html">9 Tips to Improve React Application Code Quality</a></li>
<li><a href="../353926/index.html">The most terrible mistakes that make DS. Meeting at Avito's office on April 24</a></li>
<li><a href="../353930/index.html">How to move from BuddyBuild to GitLab CI in 4 hours</a></li>
<li><a href="../353932/index.html">Information Retrieval Course at the Winter Pushchino School: we teach high school students to create search engines</a></li>
<li><a href="../353936/index.html">Google Analytics: create remarketing lists from old users without limits</a></li>
<li><a href="../353938/index.html">Services for the selection of keywords on the App Store: a comparative characteristic</a></li>
<li><a href="../353940/index.html">The C ++ Standardization Committee rips the shackles off</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>