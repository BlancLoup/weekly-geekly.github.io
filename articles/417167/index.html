<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Launch LDA in the real world. Detailed guide</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Foreword 


 On the Internet there are many tutorials explaining how LDA works (Latent Dirichlet Allocation - Dirichlet's latent placement) and how to...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Launch LDA in the real world. Detailed guide</h1><div class="post__text post__text-html js-mediator-article"><h2 id="predislovie">  Foreword </h2><br><p>  On the Internet there are many tutorials explaining how LDA works (Latent Dirichlet Allocation - Dirichlet's latent placement) and how to put it into practice.  Examples of LDA training are often shown on "exemplary" datasets, for example, "20 newsgroups dataset", which is in sklearn. </p><br><p>  The peculiarity of learning on the example of "exemplary" datasets is that the data there is always in order and conveniently stacked in one place.  When teaching production models, on the data obtained straight from real sources, everything is usually the opposite: </p><br><ul><li>  A lot of emissions. </li><li>  Incorrect markup (if any). </li><li>  Very strong class imbalances and 'ugly' distributions of any dataset parameters. </li><li>  For texts, it is: grammatical errors, a huge number of rare and unique words, multilingualism. </li><li>  Inconvenient way of data storing (different or rare formats, the need for parsing) </li></ul><br><p>  Historically, I try to learn from examples that are as close as possible to the realities of production-reality because it is in this way that you can more fully experience the problem areas of a particular type of task.  So it was with LDA and in this article I want to share my experience - how to run LDA from scratch, on completely raw data.  Some of the article will be devoted to obtaining these same data, in order for the example to acquire the appearance of a full-fledged 'engineering case'. </p><a name="habracut"></a><br><h2 id="topic-modeling-i-lda">  Topic modeling and LDA. </h2><br><p>  To begin with, consider what LDA generally does and what tasks it is used for. <br>  Most often, LDA is used for Topic Modeling tasks.  By such tasks are meant the tasks of clustering or classifying texts - in such a way that each class or cluster contains texts with similar topics. </p><br><p>  In order to apply LDA to a text dataset (hereinafter referred to as the text corpus), it is necessary to convert the corpus into the term-document matrix. </p><br><p>  A term document matrix is ‚Äã‚Äãa matrix that has a size. <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.936ex" height="2.057ex" viewBox="0 -780.1 4708.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-4E" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-74" x="1138" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-69" x="1500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-6D" x="1845" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-65" x="2724" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-73" x="3190" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-57" x="3660" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi><mtext>&nbsp;</mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-1"> N \ times W </script>  where <br>  N is the number of documents in the body, and W is the size of the body dictionary, i.e.  the number of words (unique) that are found in our body.  The i-th row, j-th column of the matrix contains a number ‚Äî how many times the j-word in the i-th text occurs. </p><br><p>  LDA builds, for a given Therm-document matrix and T a predetermined number of topics, two distributions: </p><br><ol><li>  Distribution of topics in the texts. (In practice, given by the size matrix <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>T</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.137ex" height="2.057ex" viewBox="0 -780.1 4364.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-4E" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-74" x="1138" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-69" x="1500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-6D" x="1845" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-65" x="2724" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-73" x="3190" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-54" x="3660" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi><mtext>&nbsp;</mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>T</mi></math></span></span><script type="math/tex" id="MathJax-Element-2"> N \ times T </script>  ) </li><li>  Word distribution by topic. (Size matrix <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>T</mi><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.509ex" height="2.057ex" viewBox="0 -780.1 4524.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-54" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-74" x="954" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-69" x="1316" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-6D" x="1661" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-65" x="2540" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-73" x="3006" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-57" x="3476" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi><mtext>&nbsp;</mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-3"> T \ times W </script>  ) </li></ol><br><p>  The values ‚Äã‚Äãof the cells of these matrices are according to the probability that the topic is contained in this document (or the proportion of the topic in the document, if we consider the document as a mixture of different topics) for the 'Distribution of topics across texts' matrix. </p><br><p>  For the matrix ‚ÄúDistribution of words by topic‚Äù values ‚Äã‚Äã- this is, respectively, the probability to find the word j in the text with topic i, qualitatively, you can consider these numbers as coefficients characterizing how characteristic this word is for this topic. </p><br><p>  It should be said that the word theme is not the 'everyday' definition of the word.  LDA identifies T topics, but what the topics are and whether they correspond to any well-known text topics, such as: ‚ÄúSport‚Äù, ‚ÄúScience‚Äù, ‚ÄúPolitics‚Äù - is unknown.  In this case, it is more appropriate to talk about the topic as a kind of abstract entity, which is specified by a row in the matrix of the distribution of words by themes and with a certain probability corresponds to the given text, if you like, you can imagine it as a family of characteristic sets of words that occur together with the corresponding probabilities (from the table) in a certain set of texts. </p><br><p>  If it is interesting for you to study in more detail and in the formulas how LDA studies and works, then here are some materials (which were used by the author): </p><br><ul><li>  <a href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">Original article</a> </li><li>  <a href="http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/">In English, with illustrative examples</a> </li><li>  <a href="https://habr.com/company/surfingbird/blog/228249/">Detailed in Russian</a> </li><li>  <a href="https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730">About implementation in Python</a> </li></ul><br><h2 id="dobyvaem-dikie-dannye">  We get wild data </h2><br><p>  For our "laboratory work", we need a custom dataset with its own shortcomings and features.  You can get it in different places: download reviews from Kinopoisk, Wikipedia articles, news from some news portal, we will take a bit more extreme option - posts from VKontakte communities. </p><br><p>  We will do this like this: </p><br><ol><li>  We select some user VK. </li><li>  Get a list of all his friends. </li><li>  For each friend we take all of his community. </li><li>  For each community of each friend, we extort the first n (n = 100) posts of the community and combine them into one text-content of the community. </li></ol><br><h4 id="instrumenty-i-stati">  Tools and Articles </h4><br><p>  To download posts, we will use the vk module to work with the VK API, for Python.  One of the most intricate moments when writing an application using the VKontakte API is authorization, fortunately, the code that does this work is already written and is publicly available, except for vk I used a small authorization module, vkauth. </p><br><p>  Links to used modules and articles for studying the VK API: </p><br><ul><li>  <a href="https://github.com/good-move/VKAuth">vkauth</a> </li><li>  <a href="https://habr.com/post/306022/">vkauth tutorial</a> </li><li>  <a href="https://habr.com/post/319178/">vk tutorial</a> </li><li>  <a href="https://habr.com/sandbox/84639/">vk tutorial ‚Ññ2</a> </li><li>  Official VK API documentation </li></ul><br><h4 id="pishem-kod">  Write the code </h4><br><p>  And so, using vkauth, log in: </p><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#authorization of app using modules imported. app_id = '6203169' perms = ['photos','friends','groups'] API_ver = '5.68' Auth = VKAuth(perms, app_id, API_ver) Auth.auth() token = Auth.get_token() user_id = Auth.get_user_id() #starting session session = vk.Session(access_token=token) api = vk.API(session)</span></span></code> </pre> <br><p>  In the process, a small module was written containing all the functions necessary for uploading content in the appropriate format, below they are translated, let's go over them: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_friends_ids</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(api, user_id)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' For a given API object and user_id returns a list of all his friends ids. '''</span></span> friends = api.friends.get(user_id=user_id, v = <span class="hljs-string"><span class="hljs-string">'5.68'</span></span>) friends_ids = friends[<span class="hljs-string"><span class="hljs-string">'items'</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> friends_ids <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_user_groups</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(api, user_id, moder=True, only_open=True)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' For a given API user_id returns list of all groups he subscribed to. Flag model to get only those groups where user is a moderator or an admin) Flag only_open to get only public(open) groups. '''</span></span> kwargs = {<span class="hljs-string"><span class="hljs-string">'user_id'</span></span> : user_id, <span class="hljs-string"><span class="hljs-string">'v'</span></span> : <span class="hljs-string"><span class="hljs-string">'5.68'</span></span> } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> moder == <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: kwargs[<span class="hljs-string"><span class="hljs-string">'filter'</span></span>] = <span class="hljs-string"><span class="hljs-string">'moder'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> only_open == <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: kwargs[<span class="hljs-string"><span class="hljs-string">'extended'</span></span>] = <span class="hljs-number"><span class="hljs-number">1</span></span> kwargs[<span class="hljs-string"><span class="hljs-string">'fields'</span></span>] = [<span class="hljs-string"><span class="hljs-string">'is_closed'</span></span>] groups = api.groups.get(**kwargs) groups_refined = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> group <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> groups[<span class="hljs-string"><span class="hljs-string">'items'</span></span>]: cond_check = (only_open <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> group[<span class="hljs-string"><span class="hljs-string">'is_closed'</span></span>] == <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> only_open <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> cond_check: refined = {} refined[<span class="hljs-string"><span class="hljs-string">'id'</span></span>] = group[<span class="hljs-string"><span class="hljs-string">'id'</span></span>] * (<span class="hljs-number"><span class="hljs-number">-1</span></span>) refined[<span class="hljs-string"><span class="hljs-string">'name'</span></span>] = group[<span class="hljs-string"><span class="hljs-string">'name'</span></span>] groups_refined.append(refined) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> groups_refined <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_n_posts_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(api, group_id, n_posts=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">50</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' For a given api and group_id returns first n_posts concatenated as one text. '''</span></span> wall_contents = api.wall.get(owner_id = group_id, count=n_posts, v = <span class="hljs-string"><span class="hljs-string">'5.68'</span></span>) wall_contents = wall_contents[<span class="hljs-string"><span class="hljs-string">'items'</span></span>] text = <span class="hljs-string"><span class="hljs-string">''</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> post <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> wall_contents: text += post[<span class="hljs-string"><span class="hljs-string">'text'</span></span>] + <span class="hljs-string"><span class="hljs-string">' '</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text</code> </pre> <br><p>  The final pipeline looks like this: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#id of user whose friends you gonna get, like: https://vk.com/id111111111 user_id = 111111111 friends_ids = vt.get_friends_ids(api, user_id) #collecting all groups groups = [] for i,friend in tqdm(enumerate(friends_ids)): if i % 3 == 0: sleep(1) friend_groups = vt.get_user_groups(api, friend, moder=False) groups += friend_groups #converting groups to dataFrame groups_df = pd.DataFrame(groups) groups_df.drop_duplicates(inplace=True) #reading content(content == first 100 posts) for i,group in tqdm(groups_df.iterrows()): name = group['name'] group_id = group['id'] #Different kinds of fails occures during scrapping #For examples there are names of groups with slashes #Like: 'The Kaaats / Indie-rock' try: content = vt.get_n_posts_text(api, group_id, n_posts=100) dst_path = join(data_path, name + '.txt') with open(dst_path, 'w+t') as f: f.write(content) except Exception as e: print('Error occured on group:', name) print(e) continue #need it because of requests limitaion in VK API. if i % 3 == 0: sleep(1)</span></span></code> </pre> <br><h4 id="fails">  Fails </h4><br><p>  In general, the procedure of pumping data by itself does not constitute anything difficult; it is necessary to pay attention only to two points: </p><br><ol><li>  Sometimes, due to the privacy of some communities, you will receive access errors, sometimes other errors are solved by installing try, except in the right place. </li><li>  <a href="https://vk.com/dev/api_requests">VC has a limit</a> on the number of requests per second. </li></ol><br><p>  When making a large number of requests, for example in a cycle, we will also catch errors.  This problem can be solved in several ways: </p><br><ol><li>  Stupidly and directly: Stick sleep (some) every 3 requests.  It is done in one line and it slows down the download greatly, in situations where the data volumes are not large, and there is no time for more sophisticated methods. It is quite acceptable. (Implemented in this article) </li><li>  Understand the Long Poll requests <a href="https://vk.com/dev/using_longpoll">https://vk.com/dev/using_longpoll</a> </li></ol><br><p>  In this paper, a simple and slow method was chosen, in the future, I may write with a micro-article about ways to bypass or relax the restrictions on the number of requests per second. </p><br><h4 id="itog">  Total </h4><br><p>  With a barely "some" user with ~ 150 friends, 4679 texts were obtained - each characterizes a certain VK community.  Texts vary greatly in size and are written in many languages ‚Äã‚Äã- some of them are not suitable for our purposes, but we'll talk about this a little further. </p><br><h3 id="osnovnaya-chast">  Main part </h3><br><p><img src="https://habrastorage.org/webt/bj/to/hm/bjtohmsrvsxlcbs78u0thlawxky.png" alt="image"></p><br><p>  Let's go through all the blocks of our pipeline - first, by mandatory (Ideal), then by the rest - they are of the greatest interest. </p><br><h4 id="countvectorizer">  CountVectorizer </h4><br><p>  Before teaching LDA, we need to present our documents in the form of a Therm-Doc-Matrix.  This usually includes operations such as: </p><br><ul><li>  Removal of pathways / numbers / unnecessary lexemes. </li><li>  Tokenization (word list view) </li><li>  Counting words, compiling term document matrix. </li></ul><br><p>  All of these actions in sklearn are conveniently implemented within a single program entity, sklearn.feature_extraction.text.CountVectorizer. </p><br><p>  <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">Documentation link</a> </p><br><p>  All you need to do is: </p><br><pre> <code class="python hljs">count_vect = CountVectorizer(input=<span class="hljs-string"><span class="hljs-string">'filename'</span></span>, stop_words=stopwords, vocabulary=voc) dataset = count_vect.fit_transform(train_names)</code> </pre> <br><h4 id="lda">  Lda </h4><br><p>  Similarly, with CountVectorizer, LDA, is perfectly implemented in Sklearn and other frameworks, so there is not much point in giving directly to their implementations a lot of space, in our purely practical article. </p><br><p>  <a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html">Documentation link</a> </p><br><p>  All you need to run LDA is: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#training LDA lda = LDA(n_components = 60, max_iter=30, n_jobs=6, learning_method='batch', verbose=1) lda.fit(dataset)</span></span></code> </pre> <br><h4 id="preprocessing">  Preprocessing </h4><br><p>  If we just take our texts right after we have downloaded them and convert them into a Therm-Doc Matrix using the CountVectorizer, with the default default tokenizer, we will get a matrix of size 4679x769801 (on the data I use). </p><br><p>  The size of our dictionary will be 769801. Even if we assume that most of the words are informative, we still hardly get a good LDA, we are waiting for something like 'Curse of dimensions', not to mention the fact that for almost any computer, we will just fill in all the RAM.  In fact, most of these words are completely informative.  A huge part of them is: </p><br><ul><li>  Smilies, symbols, numbers. </li><li>  Unique or very rare words (for example, Polish words from a group with Polish memes, words written with errors or in 'Olban'). </li><li>  Very frequent parts of speech (for example, prepositions and pronouns). </li></ul><br><p>  In addition, many groups in VC specialize exclusively in images - there are almost no text posts - the texts corresponding to them are degenerate, in the Therm-Document Matrix they will give us almost completely zero lines. </p><br><p>  And so, let's sort it all out! <br>  We tokenize all texts, remove punctuation and numbers from them, look at the histogram of the distribution of texts by the number of words: <br><img src="https://habrastorage.org/webt/v4/qh/w0/v4qhw0mrgpizranmnptbz5lnivk.png" alt="image"></p><br><p>  Remove all texts smaller than 100 words (there are 525) </p><br><p>  Now the dictionary: <br>  The deletion of all lexemes (words) consisting of non-letters, as part of our task, is quite acceptable.  CountVectorizer does it itself, even if it doesn't, then I think there is no need to give examples here (they are in the full version of the code for the article). </p><br><p>  One of the most common procedures for reducing the size of the dictionary is to remove the so-called stopwords (stopwords) - words that do not carry a semantic load or / and do not have thematic coloring (in our case, the same topic is Modeling).  Such words in our case are, for example: </p><br><ul><li>  Pronouns and prepositions. </li><li>  Articles - the, a. </li><li>  Common words: ‚Äúbe‚Äù, ‚Äúgood‚Äù, ‚Äúprobably‚Äù, etc. </li></ul><br><p>  In the nltk module there are formed lists of stopwords in Russian and in English, but they are rather weak.  On the Internet, you can find more lists of stopwords for any language and add them to those in nltk.  So we will do.  Take an additional stopword from here: </p><br><ul><li>  <a href="">https://github.com/stopwords-iso/stopwords-ru/blob/master/stopwords-ru.json</a> </li><li>  <a href="https://gist.github.com/menzenski/7047705">https://gist.github.com/menzenski/7047705</a> </li></ul><br><p>  In practice, when solving specific tasks, the lists of stopwords are gradually adjusted and supplemented as the models are taught, since for each specific dataset and tasks there are specific words that are not meaningful.  We will also pick up custom stopwords after learning our first generation LDA. </p><br><p>  By itself, the procedure for removing stopwords is built into the CountVectorizer - we just need their list. </p><br><p>  Is it enough what we did? </p><br><p><img src="https://habrastorage.org/webt/ja/xd/6l/jaxd6lnbbnd_dmmmk6nog9a2ntm.png" alt="image"></p><br><p>  Most of the words that are in our dictionary are still not very informative for learning LDA and are not in the list of stopwords.  Therefore, let's apply another filtering method to our data. </p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>i</mi><mi>d</mi><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>,</mo><mi>D</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>o</mi><mi>g</mi><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>D</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mspace linebreak=&quot;newline&quot; /><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mtext>&amp;#xA0;</mtext><mi>i</mi><mi>n</mi><mi>D</mi><mo>:</mo><mi>t</mi><mtext>&amp;#xA0;</mtext><mi>i</mi><mi>n</mi><mi>d</mi><mspace linebreak=&quot;newline&quot; /></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></mrow></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="37.218ex" height="8.202ex" viewBox="0 -832 16024.3 3531.4" role="img" focusable="false" style="vertical-align: -6.27ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-69" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-64" x="345" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-66" x="869" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMAIN-28" x="1419" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-74" x="1809" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMAIN-2C" x="2170" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-44" x="2615" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMAIN-29" x="3444" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMAIN-3D" x="4111" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-6C" x="5417" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-6F" x="5716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-67" x="6201" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-66" x="6932" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-72" x="7482" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-61" x="7934" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-63" x="8463" y="0"></use><g transform="translate(8897,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMAIN-7C" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-44" x="278" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMAIN-7C" x="1107" y="0"></use></g><g transform="translate(10282,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMAIN-7C" x="0" y="0"></use><g transform="translate(0,-1432)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-64" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-69" x="773" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-6E" x="1119" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-44" x="1719" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMAIN-3A" x="2825" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-74" x="3382" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-69" x="3993" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-6E" x="4339" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMATHI-64" x="4939" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/417167/&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhght8JwU0LKsnOvS53aD-263Ezy8g#MJMAIN-7C" x="5463" y="0"></use></g></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>i</mi><mi>d</mi><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>&nbsp;</mtext><mi>l</mi><mi>o</mi><mi>g</mi><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>D</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mrow><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mspace linebreak="newline"></mspace><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mtext>&nbsp;</mtext><mi>i</mi><mi>n</mi><mi>D</mi><mo>:</mo><mi>t</mi><mtext>&nbsp;</mtext><mi>i</mi><mi>n</mi><mi>d</mi><mspace linebreak="newline"></mspace></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-4"> idf (t, D) = \ log \ frac {| D |} {| \\ {d \ in D: t \ in d \\} |} </script></p><br><p>  where <br>  t - word from the dictionary. <br>  D - body (many texts) <br>  d - one of the corpus texts. <br>  We calculate the IDF of all our words, and cut off the words with the biggest idf (very rare) and with the smallest (common words). </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#'training' (tf-)idf vectorizer. tf_idf = TfidfVectorizer(input='filename', stop_words=stopwords, smooth_idf=False ) tf_idf.fit(train_names) #getting idfs idfs = tf_idf.idf_ #sorting out too rare and too common words lower_thresh = 3. upper_thresh = 6. not_often = idfs &gt; lower_thresh not_rare = idfs &lt; upper_thresh mask = not_often * not_rare good_words = np.array(tf_idf.get_feature_names())[mask] #deleting punctuation as well. cleaned = [] for word in good_words: word = re.sub("^(\d+\w*$|_+)", "", word) if len(word) == 0: continue cleaned.append(word)</span></span></code> </pre> <br><p>  Obtained after the above procedures is already quite suitable for learning LDA, but we will still make stemming - in our dataset, the same words are often found, but in different cases.  For <a href="https://github.com/nlpub/pymystem3">stemming used pymystem3</a> . </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Stemming m = Mystem() stemmed = set() voc_len = len(cleaned) for i in tqdm(range(voc_len)): word = cleaned.pop() stemmed_word = m.lemmatize(word)[0] stemmed.add(stemmed_word) stemmed = list(stemmed) print('After stemming: %d'%(len(stemmed)))</span></span></code> </pre> <br><p>  After applying the above described filterings, the size of the dictionary decreased from 769801 to <br>  13611 and already with such data, you can get an LDA model of acceptable quality. </p><br><h3 id="testirovanie-primenenie-i-tyuning-lda">  Testing, application and tuning LDA </h3><br><p>  Now, when we have datasets, preprocessing and models that we have trained in dataset, it would be good to check the adequacy of our models, as well as build some applications for them. </p><br><p>  As an application, to begin with, consider the task of generating keywords for a given text.  To do this in a fairly simple version, you can as follows: </p><br><ol><li>  We get from LDA distribution of topics for this text. </li><li>  Choose n (for example n = 2) the most pronounced topics. </li><li>  For each of the topics, choose m (for example, m = 3) the most characteristic words. </li><li>  We have a set of n * m words describing this text. </li></ol><br><p>  Let's write a simple interface class that will implement this method of generating keywords: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Let\`s do simple interface class class TopicModeler(object): ''' Inteface object for CountVectorizer + LDA simple usage. ''' def __init__(self, count_vect, lda): ''' Args: count_vect - CountVectorizer object from sklearn. lda - LDA object from sklearn. ''' self.lda = lda self.count_vect = count_vect self.count_vect.input = 'content' def __call__(self, text): ''' Gives topics distribution for a given text Args: text - raw text via python string. returns: numpy array - topics distribution for a given text. ''' vectorized = self.count_vect.transform([text]) lda_topics = self.lda.transform(vectorized) return lda_topics def get_keywords(self, text, n_topics=3, n_keywords=5): ''' For a given text gives n top keywords for each of m top texts topics. Args: text - raw text via python string. n_topics - int how many top topics to use. n_keywords - how many top words of each topic to return. returns: list - of m*n keywords for a given text. ''' lda_topics = self(text) lda_topics = np.squeeze(lda_topics, axis=0) n_topics_indices = lda_topics.argsort()[-n_topics:][::-1] top_topics_words_dists = [] for i in n_topics_indices: top_topics_words_dists.append(self.lda.components_[i]) shape=(n_keywords*n_topics, self.lda.components_.shape[1]) keywords = np.zeros(shape=shape) for i,topic in enumerate(top_topics_words_dists): n_keywords_indices = topic.argsort()[-n_keywords:][::-1] for k,j in enumerate(n_keywords_indices): keywords[i * n_keywords + k, j] = 1 keywords = self.count_vect.inverse_transform(keywords) keywords = [keyword[0] for keyword in keywords] return keywords</span></span></code> </pre> <br><p>  Apply our method to several texts and see what happens: <br>  Community: Paints of the World Travel Agency <br>  <strong>Keywords:</strong> ['photo', 'social', 'travel', 'community', 'travel', 'euro', 'accommodation', 'price', 'poland', 'departure'] <br>  <strong>Community:</strong> Food Gifs <br>  <strong>Keywords:</strong> ['butter', 'st', 'salt', 'pc', 'dough', 'cooking', 'onion', 'pepper', 'sugar', 'gr'] </p><br><p>  The results above are not 'cherry pick' and look quite adequate.  In fact, these are results from an already tuned model.  The first LDAs that were trained as part of this article showed significantly worse results; among the keywords one could often see, for example: </p><br><ol><li>  Components of web addresses: www, http, ru, com ... </li><li>  Common words. </li><li>  units of measure: cm, meter, km ... </li></ol><br><p>  The tuning (tuning) of the model was made as follows: </p><br><ol><li>  For each topic, choose n (n = 5) the most characteristic words. </li><li>  We consider them idf, by the training building. </li><li>  We enter in keywords 5-10% of the most widespread. </li></ol><br><p>  Such a ‚Äúpurge‚Äù should be carried out carefully, previewing the same 10% of words.  Rather, it is necessary to choose candidates for deletion, and then manually select the words to be removed from them. </p><br><p>  Somewhere on the 2-3 generation of models, with a similar method of selecting stopwords, for the top 5% of the widespread top words of distributions we get: <br>  ['any', 'completely', 'correctly', 'easy', 'next', 'internet', 'small', 'way', 'difficult', 'mood', 'so much', 'set', ' option ',' name ',' speech ',' program ',' competition ',' music ',' goal ',' film ',' price ',' game ',' system ',' play ',' company ' , 'nicely'] </p><br><h3 id="esche-prilozheniya">  More applications </h3><br><p>  The first thing that comes to mind specifically for me is to use the distribution of themes in the text as 'embeddings' of texts, in this interpretation one can apply visualization or clustering algorithms to them, and search for the final 'effective' thematic clusters in this way. </p><br><p>  Let's do this: </p><br><pre> <code class="python hljs">term_doc_matrix = count_vect.transform(names) embeddings = lda.transform(term_doc_matrix) kmeans = KMeans(n_clusters=<span class="hljs-number"><span class="hljs-number">30</span></span>) clust_labels = kmeans.fit_predict(embeddings) clust_centers = kmeans.cluster_centers_ embeddings_to_tsne = np.concatenate((embeddings,clust_centers), axis=<span class="hljs-number"><span class="hljs-number">0</span></span>) tSNE = TSNE(n_components=<span class="hljs-number"><span class="hljs-number">2</span></span>, perplexity=<span class="hljs-number"><span class="hljs-number">15</span></span>) tsne_embeddings = tSNE.fit_transform(embeddings_to_tsne) tsne_embeddings, centroids_embeddings = np.split(tsne_embeddings, [len(clust_labels)], axis=<span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre> <br><p>  At the output, we get the following image: <br><img src="https://habrastorage.org/webt/7j/2r/qv/7j2rqvpp2-blr9vfq44qjs75upc.png" alt="image"></p><br><p>  Crosses are centers of gravity (cenroids) of clusters. </p><br><p>  In the image of tSNE embeddings, it can be seen that the clusters selected with the help of KMeans form rather connected and most often spatially separable sets. </p><br><p>  Everything else, up to you. </p><br><p>  Link to all code: <a href="https://gitlab.com/Mozes/VK_LDA">https://gitlab.com/Mozes/VK_LDA</a> </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/417167/">https://habr.com/ru/post/417167/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../417155/index.html">Linux kernel 4.18: what is preparing the upcoming release</a></li>
<li><a href="../417157/index.html">Singularity is approaching: AI starts to control robots</a></li>
<li><a href="../417161/index.html">Burger King: secret surveillance, lies, bank card theft. Continuation</a></li>
<li><a href="../417163/index.html">Tight commits</a></li>
<li><a href="../417165/index.html">What threatens burger king</a></li>
<li><a href="../417171/index.html">Study: Women's managed hedge funds perform better</a></li>
<li><a href="../417173/index.html">"Old New Vinyl": 20 materials on the history and production of players and records</a></li>
<li><a href="../417175/index.html">Restoration of the Acme road semaphore in the first half of the 20th century</a></li>
<li><a href="../417177/index.html">Local web server under Linux, with automatic raising of hosts and switching versions of PHP</a></li>
<li><a href="../417179/index.html">Setting up a home environment for development (docker + gitlab + DNS)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>