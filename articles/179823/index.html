<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Ceph FS distributed file system in 15 minutes</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="It will take us only a few minutes to lift the distributed Ceph FS file system. 

 Quick reference 
 Ceph is an open source development of resilient, ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Ceph FS distributed file system in 15 minutes</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/b4b/5a1/9e0/b4b5a19e0015102d8361cfd0bb6420ce.png" alt="image"><br><br>  It will take us only a few minutes to lift the distributed Ceph FS file system. <br><a name="habracut"></a><br><h5>  Quick reference </h5><br>  <a href="http://ceph.com/">Ceph</a> is an open source development of resilient, highly scalable petabyte storage.  The basis is the integration of disk spaces of several dozen servers into object storage, which allows for the implementation of flexible multiple pseudo-random data redundancy.  Ceph developers complement this object storage with three more projects: <br><br><ul><li>  RADOS Gateway - S3- and Swift-compatible RESTful interface </li><li>  RBD - block device with support for thin growth and snapshots </li><li>  Ceph FS - Distributed POSIX Compatible File System </li></ul><br><h5>  Example Description </h5><br>  In my small example, I use only 3 servers as storage.  Each server has 3 SATA disks available to me: <code>/dev/sda</code> as system and <code>/dev/sdb</code> and <code>/dev/sdc</code> for the Ceph FS file system.  The OS in this example will be Ubuntu 12.04 LTS.  Another server will mount the file system, that is, in fact act as a client.  We use the default level of redundancy, that is, two replicas of one block. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      At the time of this writing, the developers offer two methods for creating simple configurations - the old one, using <code>mkcephfs</code> or the new <code>ceph-deploy</code> .  For newer versions, starting with the 0.6x branch (cuttlefish), it is already recommended to use <code>ceph-deploy</code> .  But in this example, I use the earlier, stable release of the 0.56.x branch (bobtail), using <code>mkcephfs</code> . <br><br>  I'll warn you right away - Ceph FS is still in prepraction status at the moment, but by community activity this project is called one of the hottest among software defined storage. <br><br><h4>  Let's get started </h4><br><h5>  Step 0. Install the OS </h5><br>  Perform the minimum installation.  Additionally, you must install <code>ntpdate</code> and your favorite editor, for example <code>vim</code> . <br><br><pre> <code class="bash hljs">aptitude update &amp;&amp; aptitude install ntpdate vim</code> </pre><br><h5>  Step 1. Install Ceph packages </h5><br>  We install Ceph packages for each node of the cluster and client. <br><br><pre> <code class="bash hljs">wget -q -O- <span class="hljs-string"><span class="hljs-string">'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc'</span></span> | sudo apt-key add - <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> deb http://ceph.com/debian-bobtail/ $(lsb_release -sc) main | tee /etc/apt/sources.list.d/ceph.list aptitude update &amp;&amp; aptitude install ceph</code> </pre><br><h5>  Step 2. Create a configuration file </h5><br>  On each node and client we create a single configuration file <code>/etc/ceph/ceph.conf</code> <br><br><pre> <code class="bash hljs">[global] auth cluster required = cephx auth service required = cephx auth client required = cephx [osd] osd journal size = 2000 osd mkfs <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> = xfs osd mkfs options xfs = -f -i size=2048 osd mount options xfs = rw,noatime,inode64 [mon.a] host = node01 mon addr = 192.168.2.31:6789 [mon.b] host = node02 mon addr = 192.168.2.32:6789 [mon.c] host = node03 mon addr = 192.168.2.33:6789 [osd.0] host = node01 devs = /dev/sdb [osd.1] host = node01 devs = /dev/sdc [osd.2] host = node02 devs = /dev/sdb [osd.3] host = node02 devs = /dev/sdc [osd.4] host = node03 devs = /dev/sdb [osd.5] host = node03 devs = /dev/sdc [mds.a] host = node01</code> </pre><br>  Making the file readable for everyone <br><br><pre> <code class="bash hljs">chmod 644 /etc/ceph/ceph.conf</code> </pre><br><h5>  Step 3. Make a password-free entry between nodes. </h5><br>  We set the root password, we generate ssh keys without specifying the passphrase <br><br><pre> <code class="bash hljs">passwd root ssh-keygen</code> </pre><br>  Create ssh aliases in <code>/root/.ssh/config</code> according to the name of the node in your case <br><br><pre> <code class="bash hljs">Host node01 Hostname node01.ceph.labspace.studiogrizzly.com User root Host node02 Hostname node02.ceph.labspace.studiogrizzly.com User root Host node03 Hostname node03.ceph.labspace.studiogrizzly.com User root</code> </pre><br>  Add public keys to neighboring nodes of the cluster. <br><br><pre> <code class="bash hljs">ssh-copy-id root@node02 ssh-copy-id root@node03</code> </pre><br><h5>  Step 4. Expand the cluster </h5><br>  To begin, we will prepare the necessary disks for work <br><br><pre> <code class="bash hljs">mkfs -t xfs fs-options -f -i size=2048 /dev/sdb mkfs -t xfs fs-options -f -i size=2048 /dev/sdc</code> </pre><br>  Next, prepare working directories and mount disks according to our design. <br><br>  So for node01 we will execute <br><br><pre> <code class="bash hljs">mkdir -p /var/lib/ceph/osd/ceph-0 mkdir -p /var/lib/ceph/osd/ceph-1 mount /dev/sdb /var/lib/ceph/osd/ceph-0 -o noatime,inode64 mount /dev/sdc /var/lib/ceph/osd/ceph-1 -o noatime,inode64</code> </pre><br>  for node02 <br><br><pre> <code class="bash hljs">mkdir -p /var/lib/ceph/osd/ceph-2 mkdir -p /var/lib/ceph/osd/ceph-3 mount /dev/sdb /var/lib/ceph/osd/ceph-2 -o noatime,inode64 mount /dev/sdc /var/lib/ceph/osd/ceph-3 -o noatime,inode64</code> </pre><br>  and for node03 <br><br><pre> <code class="bash hljs">mkdir -p /var/lib/ceph/osd/ceph-4 mkdir -p /var/lib/ceph/osd/ceph-5 mount /dev/sdb /var/lib/ceph/osd/ceph-4 -o noatime,inode64 mount /dev/sdc /var/lib/ceph/osd/ceph-5 -o noatime,inode64</code> </pre><br>  And finally, on node01 we run the Ceph storage creation script. <br><br><pre> <code class="bash hljs">mkcephfs -a -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.keyring</code> </pre><br>  and then copy the key <code>ceph.keyring</code> to the other nodes of the cluster <br><br><pre> <code class="bash hljs">scp /etc/ceph/ceph.keyring node02:/etc/ceph/ceph.keyring scp /etc/ceph/ceph.keyring node03:/etc/ceph/ceph.keyring</code> </pre><br>  and on the client node, in my case - <code>192.168.2.39</code> <br><br><pre> <code class="bash hljs">scp /etc/ceph/ceph.keyring 192.168.2.39:/etc/ceph/ceph.keyring</code> </pre><br>  Keys set access to read <br><br><pre> <code class="bash hljs">chmod 644 /etc/ceph/ceph.keyring</code> </pre><br><h5>  Step 5. Launch and Status </h5><br>  Thanks to the passwordless entry between the nodes, we start the entire cluster from any node <br><br><pre> <code class="bash hljs">service ceph -a start</code> </pre><br>  We also check the cluster status <br><br><pre> <code class="bash hljs">ceph -s</code> </pre><br>  The most expected status during normal operation is <code>HEALTH_OK</code> <br><br>  On the client side, we create a directory in the required location, for example <code>/mnt/cephfs</code> , parse the key for the <code>ceph</code> kernel <code>ceph</code> and mount the file system <br><br><pre> <code class="bash hljs">mkdir /mnt/cephfs ceph-authtool --name client.admin /etc/ceph/ceph.keyring --<span class="hljs-built_in"><span class="hljs-built_in">print</span></span>-key | tee /etc/ceph/admin.secret mount -t ceph node01:6789,node02:6789,node03:6789:/ /mnt/cephfs -o name=admin,secretfile=/etc/ceph/admin.secret,noatime</code> </pre><br><h5>  Afterword </h5><br>  This is how we get the distributed Ceph FS file system in just 15 minutes.  Issues of performance, safety and maintenance require <a href="http://ceph.com/docs/master/rados/operations/placement-groups/">more detailed immersion</a> and this is material on a separate article, or even more than one. <br><br><h4>  PS </h4><br>  Zababahali bundle <b>OpenNebula + Ceph</b> using only <b>Ceph</b> object storage, without the Ceph FS file system.  <a href="http://habrahabr.ru/post/196118/">Read more in the hub I am promoting</a> . <br><img src="https://habrastorage.org/getpro/habr/post_images/79c/f21/22e/79cf2122e3e14df200055c8cb14e01f8.png"></div><p>Source: <a href="https://habr.com/ru/post/179823/">https://habr.com/ru/post/179823/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../179813/index.html">Jasmine IM and Jimm Multi removed from Google Play by Mail.ru Group request</a></li>
<li><a href="../179815/index.html">IntelliJ IDEA based on new Android Studio</a></li>
<li><a href="../179817/index.html">Remove dust from 1000 photos using Gimp and Script-Fu</a></li>
<li><a href="../179819/index.html">Database Mail: Mailing Lists Directly From Microsoft SQL Server</a></li>
<li><a href="../179821/index.html">CentOS 6.4 + ReiserFS</a></li>
<li><a href="../179825/index.html">Installing Citrix XenServer on Software RAID</a></li>
<li><a href="../179827/index.html">Exploring SAP R / 3 in Unusual and Difficult Conditions</a></li>
<li><a href="../179829/index.html">Calculating the Nth sign of Pi without calculating the previous ones</a></li>
<li><a href="../179831/index.html">Multi-hosting django applications using nginx + uwsgi + virtualenv</a></li>
<li><a href="../179833/index.html">Fighting spam with standard mailer tools (using Exim for example)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>