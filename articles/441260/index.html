<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How Neural Networks Helped Graphics</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In 1943, American neuropsychologists McCulloch and Pitts developed a computer model of a neural network, and in 1958 the first working single-layer ne...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How Neural Networks Helped Graphics</h1><div class="post__text post__text-html js-mediator-article">  In 1943, American neuropsychologists McCulloch and Pitts developed a computer model of a neural network, and in 1958 the <strong>first working</strong> single-layer network recognized some letters.  Now, neural networks are not used for anything: for forecasting the exchange rate, diagnosing diseases, autopilots and plotting graphics in computer games.  Just about the last and talk. <br><br>  <strong>Evgeny Tumanov</strong> is a Deep Learning engineer at <strong>NVIDIA</strong> .  Following his speech at the HighLoad ++ conference, we prepared a story about using Machine Learning and Deep Learning in graphics.  Machine learning does not end with NLP, Computer Vision, recommendation systems, and search tasks.  Even if you are not very familiar with this area, you can apply the lessons from an article in your field or industry. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/eeCYmJQAyKA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  The story will consist of consists of three parts.  We will review the tasks in graphics that are solved using machine learning, derive the main idea, and describe the case of applying this idea to a specific task, and specifically in the <b>rendering of clouds</b> . <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  Supervised DL / ML in graphics, or teaching with a teacher in graphics </h2><br>  Let us examine two groups of tasks.  To begin with, we briefly denote them. <br><br>  <strong>Real-World or render engine</strong> : <br><br><ul><li>  Creating believable animations: locomotion, facial animation. </li><li>  Post-processing rendered images: supersampling, anti-aliasing. </li><li>  Slowmotion: frame interpolation. </li><li>  Generation of materials. </li></ul><br>  The second group of tasks is now conditionally called the " <strong>Heavy algorithm</strong> ".  We include such tasks as rendering complex objects, for example, clouds, and <strong>physical simulations</strong> : water, smoke. <br><br>  Our goal is to understand the fundamental difference between the two groups.  Consider the problem in more detail. <br><br><h3>  Creating believable animations: locomotion, facial animation <br></h3><br>  In the past few years, many <a href="https://arxiv.org/abs/1808.07371">articles have appeared</a> , where researchers are proposing new ways to generate beautiful animation.  Using the work of artists is expensive, and replacing them with an algorithm would be very beneficial for everyone.  Years ago at NVIDIA, we worked on a project in which we dealt with facial animation of characters in games: the synchronization of the face of the hero with the audio track of speech.  We tried to ‚Äúrevive‚Äù the face so that every point on it moved, and above all the lips, because this is the most difficult moment in the animation.  Manually the artist to do it is expensive and long.  What are the options to solve this problem and make a <strong>dataset</strong> for it? <br><br>  The first option is to <strong>determine the vowel sounds: the mouth opens to the vowels, it closes to consonants</strong> .  This is a simple algorithm, but too simple.  In games, we want more quality.  The second option is to <strong>plant people to read different texts and write down their faces, and then match the letters they pronounce with facial expressions.</strong>  This is a good idea, and we did so in a joint <a href="https://news.developer.nvidia.com/generating-expressive-3d-facial-animations-from-audio/">project</a> with Remedy Entertainment.  The only difference is that in the game we show not a video, but a 3D model from points.  To collect a dataset, you need to understand how specific points on the face move.  We took actors, asked to read texts with different intonations, filmed very good cameras from different angles, and then restored the 3D model of faces on each frame, and predicted the position of points on the face from the sound. <br><br><h3>  Post-processing rendered images: supersampling, anti-aliasing <br></h3><br>  Consider a case from a particular game: we have an engine that generates images in different resolutions.  We want to render the image in the resolution of 1000 √ó 500 pixels, and the player will show 2000 √ó 1000 - so it will be nicer.  How to build a dataset for this task? <br><br>  First, render the image in high resolution, then lower the quality, and then try to train the system to translate the image from low resolution to large. <br><br><h3>  Slowmotion: frame interpolation <br></h3><br>  We have a video, and we want to add frames in the middle of the network - to interpol frames.  The idea is obvious - to shoot a real video with a large number of frames, remove intermediate ones and try to predict what was removed by the network. <br><br><h3>  Generation of materials <br></h3><br>  We will not dwell on the generation of materials.  Its essence is that we remove, for example, a piece of wood from several angles of illumination, and interpolate the view from different angles. <br><br>  We reviewed the first group of tasks.  The second is fundamentally different.  About the rendering of complex objects, for example, clouds, we will talk later, and now let's deal with physical simulations. <br><br><h3>  Physical simulations of water and smoke <br></h3><br>  Imagine a pool in which moving solid objects are located.  We want to predict the movement of fluid particles.  There are particles in the pool at time <strong>t</strong> , and at time <strong>t + Œît</strong> we want to get their position.  For each particle, let's call the neural network and get an answer where it will be in the next frame. <br><br>  To solve the problem, <a href="https://ru.wikipedia.org/wiki/%25D0%25A3%25D1%2580%25D0%25B0%25D0%25B2%25D0%25BD%25D0%25B5%25D0%25BD%25D0%25B8%25D1%258F_%25D0%259D%25D0%25B0%25D0%25B2%25D1%258C%25D0%25B5_%25E2%2580%2594_%25D0%25A1%25D1%2582%25D0%25BE%25D0%25BA%25D1%2581%25D0%25B0">we</a> use <a href="https://ru.wikipedia.org/wiki/%25D0%25A3%25D1%2580%25D0%25B0%25D0%25B2%25D0%25BD%25D0%25B5%25D0%25BD%25D0%25B8%25D1%258F_%25D0%259D%25D0%25B0%25D0%25B2%25D1%258C%25D0%25B5_%25E2%2580%2594_%25D0%25A1%25D1%2582%25D0%25BE%25D0%25BA%25D1%2581%25D0%25B0">the</a> <strong><a href="https://ru.wikipedia.org/wiki/%25D0%25A3%25D1%2580%25D0%25B0%25D0%25B2%25D0%25BD%25D0%25B5%25D0%25BD%25D0%25B8%25D1%258F_%25D0%259D%25D0%25B0%25D0%25B2%25D1%258C%25D0%25B5_%25E2%2580%2594_%25D0%25A1%25D1%2582%25D0%25BE%25D0%25BA%25D1%2581%25D0%25B0">Navier-Stokes equation</a></strong> , which describes the motion of a fluid.  For a plausible, physically correct simulation of water, we will have to solve an equation or an approximation to it.  This can be done in a computational way, which has been invented a lot over the past 50 years: the SPH, FLIP algorithm or Position Based Fluid. <br><br><h3>  Difference of the first group of tasks from the second <br></h3><br>  In the first group, the teacher for the algorithm is something over: a record from real life, as in the case of individuals, or something from the engine, for example, rendering pictures.  In the second group of problems, we use the method of computational mathematics.  From this thematic division, an idea grows. <br><br><h2>  main idea <br></h2><br>  We have a computationally complex task that is long, hard and hard to solve by the classical computing university method.  To solve it and speed up, perhaps even losing a little in quality, we need: <br><br><blockquote><ul><li>  find the time-consuming place in the task where the code has been running the longest; </li><li>  see what this line produces; </li><li>  try to predict the result of the line using a neural network or any other machine learning algorithm. </li></ul></blockquote><br>  This is a general methodology and the main idea is a recipe for how to find an application for machine learning.  What should you do to make this idea useful?  There is no exact answer - use creativity, look at your work and find it.  I am engaged in graphics, and not so well acquainted with other areas, but I can imagine that in an academic environment - in physics, chemistry, robotics - you can definitely find a use.  If you solve a complex physical equation in your production, you may also be able to find an application for this idea.  For clarity, the ideas consider a specific case. <br><br><h2>  Cloud Rendering Task <br></h2><br>  We were involved in this project at NVIDIA six months ago: the task was to draw a physically correct cloud, which is represented as the density of liquid droplets in space. <br><br><blockquote>  A cloud is a physically complex object, a suspension of liquid droplets that cannot be modeled as a solid object. </blockquote><br>  It will not be possible to impose a texture on the cloud and render it, because water droplets are difficult geometrically located in 3d space and are complex in themselves: they practically do not absorb color, but reflect it, and anisotropically - in all directions differently. <br><br>  If you look at a drop of water that the sun shines on, and the vectors from the eye and the sun are parallel to the drop, you will see a large peak of light intensity.  This explains the physical phenomenon that everyone saw: in sunny weather, one of the cloud boundaries is very bright, almost white.  We look at the border of the cloud, and the view vector and the vector from this border to the sun are almost parallel. <br><img src="https://habrastorage.org/webt/yt/hz/ij/ythzijgn-xfjhl3xri9mefn4vrg.png"><br><br>  A cloud is a physically complex object and its rendering by the classical algorithm is very time consuming.  We will talk about the classical algorithm a bit later.  Depending on the parameters, the process can take hours or even days.  Imagine that you are an artist and draw a film with special effects.  You have a difficult scene with different lighting, with which you want to play.  We drew one cloud topology - I do not like it, and you want to redraw it and immediately get an answer.  It is important to get a response from one parameter change as quickly as possible.  This is problem.  So let's try to speed up this process. <br><br><h3>  Classic solution <br></h3><br>  To solve a problem, you need to solve this complex equation. <br><img src="https://habrastorage.org/webt/vs/fg/hq/vsfghqwn4s0rkmnsnescb1h62ma.png"><br><br>  The equation is harsh, but let's understand its physical meaning.  Consider a beam pierced from a camera, piercing through a cloud.  How does the light enter the camera in this direction?  First, the light can reach the point of ray exit from the cloud, and further propagate along this ray inside the cloud. <br><br>  For the second method of "propagation of light along a direction," the integral term of the equation corresponds.  Its physical meaning is as follows. <br><br>  Consider a segment within the cloud on the beam - from the entry point to the exit point.  The integration is carried out just over this segment, and for each point on it we consider the so-called <strong>Indirect light energy L (x, œâ)</strong> - the meaning of the integral I <sub>1</sub> - indirect illumination at the point.  It appears due to the fact that the drops in different ways re-reflect sunlight.  Accordingly, a huge number of mediated rays from the surrounding droplets come to the point.  I <sub>1</sub> is the integral over the sphere that surrounds a point on the ray.  In the classical algorithm it is considered using the <strong><a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%259C%25D0%25BE%25D0%25BD%25D1%2582%25D0%25B5-%25D0%259A%25D0%25B0%25D1%2580%25D0%25BB%25D0%25BE">Monte Carlo</a></strong> method. <br><br>  Classic algorithm. <br><br><ul><li>  Render the image from the pixels, and release the beam that goes from the center of the camera to the pixel and then on. </li><li>  We intersect the ray with the cloud, find the point of entry and exit. </li><li>  We consider the last term of the equation: cross, connect with the sun. </li><li>  Start <strong><a href="https://ru.wikipedia.org/wiki/%25D0%2592%25D1%258B%25D0%25B1%25D0%25BE%25D1%2580%25D0%25BA%25D0%25B0_%25D0%25BF%25D0%25BE_%25D0%25B7%25D0%25BD%25D0%25B0%25D1%2587%25D0%25B8%25D0%25BC%25D0%25BE%25D1%2581%25D1%2582%25D0%25B8">importance sampling</a></strong> </li></ul><br>  How to calculate the Monte-Carlo assessment I <sub>1</sub> we will not analyze, because it is difficult and not so important.  Suffice it to say that this is the longest and most difficult part in the whole algorithm. <br><br><h3>  We connect neural networks <br></h3><br>  From the main idea and description of the classical algorithm follows the recipe of how to apply neural networks to this problem.  The hardest thing is to calculate the Monte Carlo estimate.  It gives a number that means indirect illumination at a point, and this is exactly what we want to predict. <br><img src="https://habrastorage.org/webt/nz/on/st/nzonstvyqkk3uaiylmqjbs6fny0.png"><br><br>  We decided on the exit, now we will make out with the entrance - from which information it will be clear what is the magnitude of the indirect light at the point.  It is a light that reflects off the multitude of water droplets that surround a point.  The amount of light is strongly influenced by the topology of the density around the point, the direction to the source and the direction to the camera. <br><img src="https://habrastorage.org/webt/ks/x6/hc/ksx6hc2boi4sgdp3_j7det6firo.png"><br><br>  To construct the entrance to the neural network, we describe the local density.  This can be done in different ways, but we focused on the article <a href="http://drz.disneyresearch.com/~jnovak/publications/DeepScattering/DeepScattering.pdf">Deep Scattering: Rendering the Atmospheric Clouds with Radiation Predicting Neural Networks, Kallwcit et al.</a>  <a href="http://drz.disneyresearch.com/~jnovak/publications/DeepScattering/DeepScattering.pdf">2017</a> and many ideas are drawn from there. <br><br>  In short, the method of local representation of density around a point looks like this. <br><br><ul><li>  <strong>Fix a sufficiently small constant</strong> .  Let it be the free path in the cloud. <br></li><li>  <strong>Draw around a point on our segment a volumetric rectangular grid of a fixed size</strong> , say, 5 * 5 * 9.  In the center of this cube will be our point.  The grid spacing is a small fixed constant.  In the grid nodes we measure the density of the cloud. </li><li>  <strong>Let us increase the constant 2 times</strong> , draw a larger grid, and do the same - measure the density at the grid nodes. </li><li>  <strong>Repeat the previous step several times</strong> .  We did this 10 times, and after the procedure we got 10 grids - 10 tensors, each of which stores the density of the cloud, and each of the tensors covers a larger and larger neighborhood around the point. </li></ul><br>  This approach gives us the most detailed description of a small area - the closer to a point, the more detailed the description.  We decided on the output and input of the network, it remains to understand how to train it. <br><br><h3>  We train <br></h3><br>  Generate 100 different clouds with different topologies.  We will simply render them using the classical algorithm, write down what the algorithm gets in the very line where it integrates using the Monte Carlo method, and write down the properties that correspond to the point.  So we get a dataset, where you can learn. <br><img src="https://habrastorage.org/webt/yc/im/rh/ycimrhfbcbhfhx_rjwsqoc-xt1q.png"><br><br><h3>  What to train, or network architecture <br></h3><br>  The network architecture for this task is not the most crucial point, and if you don‚Äôt understand anything - don‚Äôt worry - this is not the most important thing I wanted to convey.  We used the following architecture: for each point there are 10 tensors, each of which is counted on an increasingly large-scale grid.  Each of these tensors falls into the corresponding block. <br><br><ul><li>  First, in the first ordinary <strong>fully connected layer</strong> . </li><li>  After exiting from the first fully connected layer, to the second fully connected layer, which has no activation. </li></ul><br>  Fully connected layer without activation is just a multiplication by a matrix.  We add the output from the previous <strong>residual-block to the</strong> multiplication result on the matrix, and only then apply activation. <br><img src="https://habrastorage.org/webt/he/fb/pn/hefbpncqogvya11gpsmzjwxtawi.png"><br><br>  We take a point, count the values ‚Äã‚Äãon each of the grids, put the obtained tensors into the corresponding residual block ‚Äî and we can carry out the <strong>inference of the neural network</strong> ‚Äî the production mode of the network.  We did this and made sure that we get pictures of clouds. <br><br><h3>  results <br></h3><br>  The first observation is that we got what we wanted: the neural network call, compared with the Monte Carlo evaluation, works faster, which is already good. <br><br>  But there is another observation on the results of training - it is the convergence in the number of samples.  What is this about? <br><img src="https://habrastorage.org/webt/mb/wd/eg/mbwdegnr_cpk6irbcsvrmyhe3n4.png"><br><br>  When rendering the image, let's cut it into small tiles ‚Äî small squares of pixels, say, 16 * 16.  Consider one image tile without loss of generality.  When we render this tile, for each pixel of the camera, we emit many rays corresponding to one pixel, and add a little bit of noise to the rays, so that they are slightly different.  These rays are called <strong>anti-aliasing</strong> and invented to reduce the noise in the final image. <br><br><ul><li>  We produce several anti-aliasing rays for each pixel. </li><li>  On the inside of the beam from the camera, in the cloud, on the segment, we calculate <em>n</em> samples of points where we want to carry out Monte-Carlo evaluation, or call a network for them. </li></ul><br>  There are also samples that correspond to the connection with light sources.  They appear when we connect a point with a light source, for example, with the sun.  This is easy to do, because the sun is the rays falling on the earth parallel to each other.  For example, the sky, as a source of light, is much more complicated, because it is represented as an infinitely distant sphere, which has a function of color in direction.  If the vector looks straight up in the sky, then the color is blue.  The lower - the brighter.  At the bottom of the sphere is usually a neutral color, imitating the earth: green, brown. <br><br>  When we connect a point with the sky in order to understand how much light comes into it, we always release several rays in order to get an answer that converges to the truth.  Rays release more than one to get a better grade.  Therefore, the entire <strong><a href="http://infocity.kiev.ua/graf/content/graf021.phtml">pipeline rendering</a></strong> requires so many samples. <br><br>  When we trained the neural network, we noticed that it learns a much more average solution.  If you fix the number of samples, it is clear that the classical algorithm converges to the left row of the picture column, and the network learns to the right.  This does not mean that the original method is bad - we just converge faster.  When we increase the number of samples, the original method will be getting closer and closer to what we get. <br><br>  Our main result that we wanted to get is an increase in rendering speed.  For a specific cloud in a specific resolution with the parameters of the samples, we see that the pictures obtained by the network and the classical method are almost identical, but we get the right picture 800 times faster. <br><img src="https://habrastorage.org/webt/qp/ly/xn/qplyxntotijyhtzhgeqzsygahmc.png"><br><br><h2>  Implementation <br></h2><br>  There is an Open Source program for 3D modeling - <strong><a href="https://www.blender.org/">Blender</a></strong> , which implements the classic algorithm.  We did not write the algorithm ourselves, but used this program: we conducted training in Blender, writing down everything we needed for the algorithm.  Production was also done in the program: they trained the network in <strong>TensorFlow</strong> , transferred it to C ++ using TensorRT, and already TensorRT-network was integrated into Blender, because its code is open. <br><br>  Since we did everything for Blender, our solution has all the features of the program: we can render any scenes and a lot of clouds.  Clouds in our solution are defined by creating a cube, inside which we determine the density function in a specific way for 3D programs.  We optimized this process - we cache the density.  If a user wants to draw the same cloud in a heap of different scene setups: under different lighting, with different objects on the scene, then he does not need to constantly recalculate the density of the cloud.  What happened, you can look at the <a href="https://yadi.sk/i/-vmdv2eL_r83DQ">video</a> . <br><br>  Finally, I will repeat once again the main idea that I wanted to convey: <em>if in your work you long and diligently consider something as a specific computational algorithm, and this does not suit you, find the hardest place in the code, replace it with a neural network, and Perhaps this will help you.</em> <br><br><blockquote>  Neural networks and artificial intelligence is one of the new topics that will be discussed at <a href="https://www.highload.ru/spb/2019">Saint HighLoad ++ 2019</a> in April.  We have already received several <a href="https://www.highload.ru/spb/2019/abstracts">applications</a> on this topic, and if you have great experience, not necessarily on neural networks, <a href="https://conf.ontico.ru/users/login.html%3Furl%3D/lectures/propose/%253Fconference%253Dhl2019-spb">submit an application for a report</a> before <strong>March 1</strong> .  We will be glad to see you among our speakers. <br><br>  To be aware of how the program is formed and what reports are accepted, subscribe to the <a href="http://eepurl.com/VYVaf">newsletter</a> .  In it we only publish thematic collections of reports, digests on articles and new videos. <br></blockquote></div><p>Source: <a href="https://habr.com/ru/post/441260/">https://habr.com/ru/post/441260/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../441248/index.html">Russia ranked 9th in the global SSL rating, ahead of China, Denmark and Switzerland</a></li>
<li><a href="../441250/index.html">Quick start: Go + Apache Kafka + Redis</a></li>
<li><a href="../441252/index.html">"Article about blowjob": scientists have processed 109 hours of oral sex to develop an AI that sucks dick</a></li>
<li><a href="../441254/index.html">Seminar ‚ÄúWhy we contacted Kubernetes and what we get from this,‚Äù February 28, Moscow</a></li>
<li><a href="../441258/index.html">Fully functional dynamic tracing in Linux using eBPF and bpftrace</a></li>
<li><a href="../441262/index.html">Simple and long tasks filter out candidates better than short and complex ones.</a></li>
<li><a href="../441264/index.html">Kibana User Guide. Visualization. Part 2</a></li>
<li><a href="../441266/index.html">How the tiOPF framework for delphi / lazarus works. Template "Visitor"</a></li>
<li><a href="../441268/index.html">Ceedling + Eclipse or unit tests for microcontrollers</a></li>
<li><a href="../441270/index.html">First look at FoundationDB open by Apple</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>