<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Let's talk for Hadoop</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Introduction 
 As a person with a not very stable psyche, one glance at a picture like this is enough for me to start a panic attack. But I decided th...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Let's talk for Hadoop</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/2d5/e32/63f/2d5e3263fae242a09be23a50e1fb063f.png" alt="image"><br><br><h5>  <b>Introduction</b> </h5><br>  As a person with a not very stable psyche, one glance at a picture like this is enough for me to start a panic attack.  But I decided that I would only suffer myself.  The purpose of the article is to make Hadoop look not so scary. <br><br><h6>  What will happen in this article: </h6><br><ul><li>  Let us examine what the framework consists of and why it is needed; </li><li>  analyze the issue of painless cluster deployment; </li><li>  look at a specific example; </li><li>  let's touch on the new features of Hadoop 2 (Namenode Federation, Map / Reduce v2). </li></ul>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h6>  What will not be in this article: </h6><br><ul><li>  in general, an overview article, therefore, without complications; </li><li>  let's not go into the finer points of the ecosystem; </li><li>  let's not dig deep into the jungle API; </li><li>  We will not consider all about devops-tasks. </li></ul><br><a name="habracut"></a><br><h5>  <b>What is Hadoop and why is it needed?</b> </h5><br>  Hadoop is not that complicated; the kernel consists of the HDFS file system and the MapReduce framework for processing data from this file system. <br><br>  If you look at the question ‚Äúwhy do we need Hadoop?‚Äù From the point of view of use in a large enterprise, then there are quite a lot of answers, and they vary from ‚Äústrongly for‚Äù to ‚Äúvery against‚Äù.  I recommend viewing the <a href="http://www.thoughtworks.com/insights/blog/hadoop-or-not-hadoop">ThoughtWorks</a> article. <br><br>  If you look at the same question from a technical point of view - for which tasks it makes sense for us to use Hadoop - this is also not so simple.  The manuals first understand two basic examples: word count and log analysis.  Well, what if I don‚Äôt have word count or log analysis? <br><br>  It would be nice to determine the answer as something simple.  For example, SQL - you need to use if you have a lot of structured data and you really want to talk with the data.  Ask as many questions as possible in advance of an unknown nature and format. <br><br>  The long answer is to look at a number of existing solutions and implicitly assemble, under the subcortex, the conditions for which Hadoop is needed.  You can poke around on blogs, I can still advise you to read the Mahmoud Parsian <a href="http://shop.oreilly.com/product/0636920033950.do">book Data Algorithms: Recipes for Scaling up with Hadoop and Spark</a> . <br><br>  I'll try to answer shorter.  Hadoop should be used if: <br><br><ul><li>  The calculations must be composable, in other words, you must be able to run the calculations on a subset of the data, and then merge the results. </li><li>  You plan to process a large amount of unstructured data ‚Äî more than you can fit on one machine (&gt; several terabytes of data).  The advantage here is the ability to use the commodity hardware for the cluster in the case of Hadoop. </li></ul><br><br>  Hadoop should not be used: <br><ul><li>  For uncombined tasks - for example, for recurrent problems. </li><li>  If the entire amount of data fits on one machine.  Significantly save time and resources. </li><li>  Hadoop as a whole is a batch processing system and is not suitable for real-time analysis (here the <a href="http://hortonworks.com/blog/apache-storm-real-time-processing-hadoop/">Storm</a> system comes to the rescue). </li></ul><br><br><h5>  <b>HDFS architecture and typical Hadoop cluster</b> </h5><br>  HDFS is similar to other traditional file systems: files are stored as blocks, there is a mapping between blocks and file names, a tree structure is supported, a rights-based access model is supported, etc. <br><br>  Differences HDFS: <br><ul><li>  Designed to store a large number of huge (&gt; 10GB) files.  One Corollary - large block size compared to other file systems (&gt; 64MB) </li><li>  Optimized to support streaming data access (high-streaming read), respectively, the performance of random data reading operations begins to limp. </li><li>  Focuses on the use of a large number of inexpensive servers.  In particular, servers use the JBOB structure (Just a bunch of disk) instead of RAID - mirroring and replication are performed at the cluster level, and not at the individual machine level. </li><li>  Many of the traditional problems of distributed systems are embedded in the design - by default, the entire failure of individual nodes is absolutely normal and natural operation, and not something out of the ordinary. </li></ul><br><br>  Hadoop cluster consists of three types of nodes: NameNode, Secondary NameNode, Datanode. <br><br>  <b>Namenode</b> is the brain of the system.  As a rule, one node per cluster (more in the case of the Namenode Federation, but we leave this case overboard).  It stores all the metadata of the system - directly mapping between files and blocks.  If node 1, then it is also Single Point of Failure.  This problem is solved in the second version of Hadoop using the <a href="http://www.slideshare.net/EdurekaIN/hadoop-20-architecture-hdfs-federation-namenode-high-availability">Namenode Federation</a> . <br><br>  <b>Secondary NameNode</b> - 1 node per cluster.  It is customary to say that ‚ÄúSecondary NameNode‚Äù is one of the most unfortunate names in the entire history of programs.  Indeed, the Secondary NameNode is not a replica of the NameNode.  The state of the file system is stored directly in the fsimage file and in the edits log file containing the latest file system changes (similar to the transaction log in the RDBMS world).  The job of the Secondary NameNode is in the periodic merge fsimage and edits - the Secondary NameNode maintains the size of the edits within reasonable limits.  The Secondary NameNode is necessary for a quick manual recovery of the NameNode in case the NameNode fails. <br><br>  In a real cluster, NameNode and Secondary NameNode are separate servers demanding memory and hard disk.  And the declared ‚Äúcommodity hardware‚Äù is already a DataNode case. <br><br>  <b>DataNode</b> - There are a lot of such nodes in a cluster.  They store blocks of files directly.  Noda regularly sends NameNode its status (shows that it is still alive) and hourly reports, information about all blocks stored on this node.  This is necessary to maintain the desired level of replication. <br><br>  Let's see how data is recorded in HDFS: <br><img src="https://habrastorage.org/files/e3c/8d0/cea/e3c8d0cea2104fd1814bae154a3e53bc.png" alt="image"><br><br>  1. The client cuts the file into chains of block size. <br>  2. The client connects to the NameNode and requests a write operation, sending the number of blocks and the required replication level <br>  3. NameNode responds with a chain of DataNode. <br>  4. The client connects to the first node from the chain (if it didn‚Äôt work out from the first, from the second, and so on.  The client records the first block on the first node, the first node on the second, and so on. <br>  5. Upon completion of the recording in the reverse order (4 -&gt; 3, 3 -&gt; 2, 2 -&gt; 1, 1 -&gt; to the client) messages about the successful recording are sent. <br>  6. As soon as the client receives confirmation that the block has been successfully written, it notifies the NameNode of the block's record, then receives the DataNode chain for the second block, etc. <br><br>  The client continues to write blocks if it is able to write successfully a block to at least one node, i.e., replication will work according to the well-known ‚Äúeventual‚Äù principle, then NameNode will compensate and still achieve the desired level of replication. <br>  Completing the review of HDFS and the cluster, let's pay attention to another great feature of Hadoop, rack awareness.  The cluster can be configured so that NameNode has an idea of ‚Äã‚Äãwhich nodes on which rackes are located, thereby providing the best <a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.1-latest/bk_system-admin-guide/content/admin_configure_rack_awareness.html">protection against failures</a> . <br><br><h6>  <b>MapReduce</b> </h6><br>  The job unit of job is a set of map (parallel data processing) and reduce (union of conclusions from a map) of tasks.  Map tasks are performed by mappers, reduce - by reducer.  Job consists of at least one mapper, reducer'y optional.  <a href="http://wiki.apache.org/hadoop/HowManyMapsAndReduces">Here the</a> problem of splitting the task into maps and reducees is discussed.  If the words ‚Äúmap‚Äù and ‚Äúreduce‚Äù are completely incomprehensible to you, you can see a <a href="http://www.joelonsoftware.com/items/2006/08/01.html">classic article</a> on this topic. <br><br><h6>  <b>MapReduce Model</b> </h6><br><img src="https://habrastorage.org/files/f1e/bbf/01c/f1ebbf01c5b54effb8ebb6762fff3c36.png" alt="image"><br><br><ul><li>  Data input / output occurs in the form of pairs (key, value) </li><li>  Two map functions are used: (K1, V1) -&gt; ((K2, V2), (K3, V3), ...) - displaying a key-value pair to a certain set of intermediate key and value pairs, and also reduce: (K1 , (V2, V3, V4, VN)) -&gt; (K1, V1), representing a set of values ‚Äã‚Äãthat has a common key to a smaller set of values. </li><li>  Shuffle and sort is needed to sort the input to the reducer by key, in other words, it makes no sense to send the value (K1, V1) and (K1, V2) to two different reducer.  They must be processed together. </li></ul><br><br>  Let's look at the architecture of MapReduce 1. First, let's expand the concept of the hadoop cluster by adding two new elements to the cluster - JobTracker and TaskTracker.  JobTracker directly requests from clients and manages map / reduce tasks on TaskTrackers.  JobTracker and NameNode are spread to different machines, while DataNode and TaskTracker are on the same machine. <br><br>  The interaction between the client and the cluster is as follows: <br><br><img src="https://habrastorage.org/files/e59/5b8/b54/e595b8b5411e4f3aad9a8c52849bccea.png" alt="image"><br><br>  1. Client sends job to JobTracker.  Job is a jar file. <br>  2. JobTracker is looking for TaskTrackers based on data locality, i.e.  preferring those that already store data from HDFS.  JobTracker assigns map and reduce tasks to tasktrackers <br>  3. TaskTrackers send job performance reports to JobTracker. <br><br>  Failure of the task - the expected behavior, failed task automatically restarted on other machines. <br>  In Map / Reduce 2 (Apache YARN), JobTracker / TaskTracker terminology is no longer used.  JobTracker is divided into <b>ResourceManager</b> - resource management and <b>Application Master</b> - application management (one of which is MapReduce itself).  MapReduce v2 uses new API <br><br><h6>  <b>Setting up the environment</b> </h6><br>  There are several different Hadoop distributions on the market: Cloudera, HortonWorks, MapR - in order of popularity.  However, we will not focus on the choice of a specific distribution.  A detailed analysis of distributions can be found <a href="http://www.networkworld.com/article/2369327/software/comparing-the-top-hadoop-distributions.html">here</a> . <br><br>  There are two ways to try Hadoop painlessly and with minimal effort. <br><br>  1. <a href="http://www.cloudera.com/content/cloudera/en/solutions/partner/Amazon-Web-Services.html">Amazon Cluster</a> - a full cluster, but this option will cost money. <br><br>  2. Download the virtual machine ( <a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/DemoVMs/Cloudera-QuickStart-VM/cloudera_quickstart_vm.html">manual number 1</a> or <a href="">manual number 2</a> ).  In this case, the downside is that all servers in the cluster are spinning on the same machine. <br><br>  We turn to painful ways.  Hadoop first version in Windows will require the installation of Cygwin.  Plus there will be excellent integration with development environments (IntellijIDEA and Eclipse).  More in this wonderful <a href="http://v-lad.org/Tutorials/Hadoop/03%2520-%2520Prerequistes.html">manual</a> . <br><br>  Starting with version two, Hadoop also supports Windows server editions.  However, I would not advise trying to use Hadoop and Windows not only in production, but generally outside the developer‚Äôs computer, although there are <a href="http://hortonworks.com/blog/install-hadoop-windows-hortonworks-data-platform-2-0/">special distributions</a> for this.  Windows 7 and 8 are not currently supported by vendors, but people who love a challenge can try to <a href="https://wiki.apache.org/hadoop/Hadoop2OnWindows">do it by hand</a> . <br><br>  I also note that for Spring fans there is a <a href="http://projects.spring.io/spring-hadoop/">Spring for Apache Hadoop</a> framework. <br><br>  We will go simple and install Hadoop on a virtual machine.  First, let's download <a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/DemoVMs/Cloudera-QuickStart-VM/cloudera_quickstart_vm.html">the</a> CDH-5.1 <a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/DemoVMs/Cloudera-QuickStart-VM/cloudera_quickstart_vm.html">distribution</a> for the virtual machine (VMWare or VirtualBox).  The size of the distribution is about 3.5 GB.  Download, unpack, upload to VM and that's it.  We have everything.  It's time to write your favorite WordCount! <br><br><h6>  <b>Specific example</b> </h6><br>  We need sample data.  I suggest to download <a href="https://wiki.skullsecurity.org/Passwords">any dictionary</a> for bruteforce passwords.  My file will be called john.txt. <br>  Now open Eclipse, and we already have a newly created training project.  The project already contains all the necessary libraries for development.  Let's throw out all the code carefully put out by the guys from Clouder and copy the following: <br><br><pre><code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">package</span></span> com.hadoop.wordcount; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> java.io.IOException; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> java.util.*; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.fs.Path; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.conf.*; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.io.*; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.mapred.*; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.util.*; <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">WordCount</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Map</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">MapReduceBase</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">implements</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Mapper</span></span></span><span class="hljs-class">&lt;</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">LongWritable</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Text</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Text</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">IntWritable</span></span></span><span class="hljs-class">&gt; </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">final</span></span> <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> IntWritable one = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> IntWritable(<span class="hljs-number"><span class="hljs-number">1</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> Text word = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Text(); <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">map</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(LongWritable key, Text value, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter)</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">throws</span></span></span><span class="hljs-function"> IOException </span></span>{ String line = value.toString(); StringTokenizer tokenizer = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> StringTokenizer(line); <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> (tokenizer.hasMoreTokens()) { word.set(tokenizer.nextToken()); output.collect(word, one); } } } <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Reduce</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">MapReduceBase</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">implements</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Reducer</span></span></span><span class="hljs-class">&lt;</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Text</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">IntWritable</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Text</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">IntWritable</span></span></span><span class="hljs-class">&gt; </span></span>{ <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">reduce</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(Text key, Iterator&lt;IntWritable&gt; values, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter)</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">throws</span></span></span><span class="hljs-function"> IOException </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> sum = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> (values.hasNext()) { sum += values.next().get(); } output.collect(key, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> IntWritable(sum)); } } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">static</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">main</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(String[] args)</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">throws</span></span></span><span class="hljs-function"> Exception </span></span>{ JobConf conf = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> JobConf(WordCount.class); conf.setJobName(<span class="hljs-string"><span class="hljs-string">"wordcount"</span></span>); conf.setOutputKeyClass(Text.class); conf.setOutputValueClass(IntWritable.class); conf.setMapperClass(Map.class); conf.setCombinerClass(Reduce.class); conf.setReducerClass(Reduce.class); conf.setInputFormat(TextInputFormat.class); conf.setOutputFormat(TextOutputFormat.class); FileInputFormat.setInputPaths(conf, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Path(args[<span class="hljs-number"><span class="hljs-number">0</span></span>])); FileOutputFormat.setOutputPath(conf, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Path(args[<span class="hljs-number"><span class="hljs-number">1</span></span>])); JobClient.runJob(conf); } }</code> </pre> <br>  We get something like this: <br><br><img src="https://habrastorage.org/files/fd8/f7b/952/fd8f7b9529a14f0ebd3311b2d0d26b80.png" alt="image"><br><br>  At the root of the training project, add the mail john.txt via the menu File -&gt; New File.  Result: <br><br><img src="https://habrastorage.org/files/b68/905/997/b68905997ab34b298072a7b9fa780f17.png" alt="image"><br><br>  Click Run -&gt; Edit Configurations and enter as input Program.txt and output as Program Arguments <br><br><img src="https://habrastorage.org/files/c08/cae/d5f/c08caed5fcdc4f3d8308855e1a105c37.png" alt="image"><br><br>  Click Apply, and then Run.  Work successfully completed: <br><br><img src="https://habrastorage.org/files/13d/671/aeb/13d671aebb56436e95feec0eb1049ed1.png" alt="image"><br><br>  And where are the results?  To do this, update the project in Eclipse (using the F5 button): <br><br><img src="https://habrastorage.org/files/e87/941/9ce/e879419ce3674607bfee7952cedade1f.png" alt="image"><br><br>  In the output folder you can see two files: _SUCCESS, which says that the work was completed successfully, and part-00000 directly with the results. <br>  This code, of course, can be debugged, and so on. We end the conversation with an overview of the unit tests.  Actually, for the time being, only the MRUnit framework (https://mrunit.apache.org/) has been written for writing unit tests in Hadoop, it is late for Hadoop: versions up to 2.3.0 are now supported, although the latest stable version of Hadoop is 2.5.0 <br><br><h5>  <b>Ecosystem Blitz Overview: Hive, Pig, Oozie, Sqoop, Flume</b> </h5><br>  In a nutshell and everything. <br><br>  <b>Hive &amp; Pig</b> .  In most cases, writing a Map / Reduce job in pure Java is too time consuming and overworking, which makes sense, as a rule, only to pull out all possible performance.  Hive and Pig are two tools for this case.  Hive love in Facebook, Pig love Yahoo.  Hive has a SQL-like syntax ( <a href="http://hortonworks.com/blog/hive-cheat-sheet-for-sql-users/">similarities and differences with SQL-92</a> ).  Many people have moved to the Big Data camp with experience in business analysis, as well as DBA - for them Hive is often the tool of choice.  Pig focuses on ETL. <br><br>  <b>Oozie</b> is a workflow engine for jobs.  Allows you to build jobs on different platforms: Java, Hive, Pig, etc. <br><br>  Finally, frameworks that provide direct data entry into the system.  Very short.  <b>Sqoop</b> - integration with structured data (RDBMS), <b>Flume</b> - with unstructured. <br><br><h5>  <b>Review of literature and video courses</b> </h5><br>  Literature on Hadoop is not so much.  As for the second version, I came across only one book that would concentrate on it - <a href="http://www.amazon.com/Hadoop-Essentials-End---End-Approach/dp/1495496120/ref%3Dsr_1_1%3Fs%3Dbooks%26ie%3DUTF8%26qid%3D1408657210%26sr%3D1-1%26keywords%3DHadoop%2B2%2BEssentials">Hadoop 2 Essentials: An End-to-End Approach</a> .  Unfortunately, the book does not get in electronic format, and read it did not work. <br><br>  I do not consider the literature on individual components of the ecosystem - Hive, Pig, Sqoop - because it is somewhat outdated, and most importantly, such books are unlikely to be read from cover to cover, rather, they will be used as a reference guide.  And then you can always do documentation. <br><br>  <a href="http://www.amazon.com/Hadoop-Definitive-Guide-Tom-White/dp/1449311520/ref%3Dsr_1_1%3Fs%3Dbooks%26ie%3DUTF8%26qid%3D1408657185%26sr%3D1-1%26keywords%3Dhadoop%2Bthe%2Bdefinitive%2Bguide">Hadoop: The Definitive Guide</a> is a book in the Amazon Top and has many positive reviews.  The material is outdated: 2012 and describes Hadoop 1. Plus, there are many positive reviews and a fairly wide coverage of the entire ecosystem. <br><br>  <a href="http://www.amazon.com/Professional-Hadoop-Solutions-Boris-Lublinsky/dp/1118611934/ref%3Dsr_1_1%3Fs%3Dbooks%26ie%3DUTF8%26qid%3D1408657171%26sr%3D1-1%26keywords%3Dprofessional%2Bhadoop%2Bsolutions">Lublinskiy B. Professional Hadoop Solution</a> is a book from which a lot of material is taken for this article.  Somewhat complicated, but a lot of real practical examples ‚Äî attention is paid to the specific nuances of building solutions.  Much nicer than just reading the description of the features of the product. <br><br>  <a href="http://www.amazon.com/Hadoop-Operations-Eric-Sammer/dp/1449327052/ref%3Dsr_1_1%3Fs%3Dbooks%26ie%3DUTF8%26qid%3D1408657139%26sr%3D1-1%26keywords%3Dhadoop%2Boperations">Sammer E. Hadoop Operations</a> - about half of the book is devoted to the description of the configuration of Hadoop.  Given that the book is 2012, will become obsolete very soon.  It is intended primarily for devOps, of course.  But I am of the opinion that it is impossible to understand and feel the system if it is only developed and not operated.  The book seemed useful to me due to the fact that the standard problems of backup, monitoring and benchmarking of the cluster were discussed. <br><br>  <a href="http://shop.oreilly.com/product/0636920033950.do">Parsian M. "Data Algorithms: Recipes for Scaling up with Hadoop and Spark"</a> - the main focus is on the design of Map-Reduce-applications.  A strong bias in the scientific side.  Useful for a comprehensive and deep understanding and application of MapReduce. <br><br>  <a href="http://www.amazon.com/Hadoop-Real-World-Solutions-Cookbook-ebook/dp/B00AIVQE3I/ref%3Dsr_1_2%3Fie%3DUTF8%26qid%3D1408656995%26sr%3D8-2%26keywords%3DHadoop%2Bcookbook">Owens J. Hadoop Real World Solutions Cookbook</a> - like many other books published by Packt with the word ‚ÄúCookbook‚Äù in the title, is a technical documentation that has been cut into questions and answers.  This is also not so simple.  Try it yourself.  Worth reading for a broad overview, well, and use as a reference. <br><br>  It is worth paying attention to two video courses from O'Reilly. <br><br>  <a href="http://shop.oreilly.com/product/110000753.do">Learning Hadoop</a> - 8 hours.  Seemed too superficial.  But for me some added value added.  materials, because I want to play with Hadoop, but we need some live data.  And here it is - a great source of data. <br><br>  <a href="http://shop.oreilly.com/product/110000685.do">Building Hadoop Clusters</a> - 2.5 hours.  As is clear from the title, here is the emphasis on building clusters on Amazon.  I really liked the course - short and clear. <br>  I hope that my humble contribution will help those who are just starting to learn Hadoop. </div><p>Source: <a href="https://habr.com/ru/post/234993/">https://habr.com/ru/post/234993/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../234979/index.html">Auto-generation of powershell scripts</a></li>
<li><a href="../234981/index.html">Zephyr-7 solar-powered unmanned aircraft stayed in the air for 11 days in winter conditions</a></li>
<li><a href="../234983/index.html">E-commerce platform. Part 2. What is Demandware and what is it eaten with?</a></li>
<li><a href="../234985/index.html">Rospotrebnadzor is going to regulate the physical parameters of electronics</a></li>
<li><a href="../234987/index.html">A little creative - calendar with antipatterns</a></li>
<li><a href="../234997/index.html">Getting RSS feeds from twitter after updating API 1.1</a></li>
<li><a href="../234999/index.html">New media consoles on Rockchip RK3288 from the company Tronsmart already on sale</a></li>
<li><a href="../235001/index.html">Google accepted Ice Bucket Challenge from Rambler & Co</a></li>
<li><a href="../235003/index.html">Rendering 3D illustrations for IKEA</a></li>
<li><a href="../235007/index.html">Skype bot for deployment without Skype API</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>