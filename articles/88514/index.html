<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Trigram-based text generator (python)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This article describes how to generate pseudo-text based on the trigram model. The resulting text is hardly possible to use anywhere, nevertheless, th...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Trigram-based text generator (python)</h1><div class="post__text post__text-html js-mediator-article">  This article describes how to generate pseudo-text based on the trigram model.  The resulting text is hardly possible to use anywhere, nevertheless, this is a good illustration of the use of statistical methods for processing natural language.  An example of the generator can be found <a href="http://linguis.ru/trigram">here</a> . <br><br><h3>  Dry theory </h3><br>  And so, our task is to generate the text.  This means we need to take the words and line them up in a certain order.  How to determine this order?  We can go as follows: to build phrases that are most likely for the Russian language.  But what does the probability of a language phrase mean?  From the point of view of common sense is nonsense.  Nevertheless, this probability can be formally defined as the probability of the occurrence of a sequence of words in a certain corpus (set of texts). <a name="habracut"></a>  For example, the probability of the phrase <i>‚Äúhappiness is pleasure without repentance‚Äù</i> can be calculated as the product of the probabilities of each of the words in this phrase: <br><br><blockquote>  P = P (happiness) P (is | happiness) P (pleasure | happiness is) P (without | happiness is pleasure) P (repentance | happiness is pleasure without) <br></blockquote>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Calculate the probability of <i>P (happiness) is</i> a simple matter: you only need to count how many times this word has occurred in the text and divide this value into the total number of words.  But calculating the probability of <i>P (repentance | happiness is pleasure without) is</i> no longer so simple.  Fortunately, we can simplify this task.  Let us assume that the probability of a word in a text depends only on the previous word.  Then our formula for calculating the phrase will take the following form: <br><br><blockquote>  P = P (happiness) P (is | happiness) P (pleasure | is) P (without | pleasure) P (repentance | without) <br></blockquote><br><br>  Already easier.  Calculate the conditional probability P (is | happiness) is easy.  To do this, consider the number of pairs 'happiness is' and divide by the number in the text of the word 'happiness': <br><br><blockquote>  P (is | happiness) = C (there is happiness) / C (happiness) <br></blockquote><br><br>  As a result, if we count all the pairs of words in a text, we can calculate the probability of an arbitrary phrase.  And if we can calculate the likelihood of a phrase, we can choose the most ‚Äúplausible" combinations of words in this text for automatic generation.  By the way, these pairs of words are called bigrams, and the set of calculated probabilities is called the bigram model. <br><br>  I want to note that for our algorithm we use not bigrams, but trigrams.  The difference is that the conditional probability of a word is determined not by one, but by the two previous words.  That is, instead of P (pleasure | happiness) we will calculate P (pleasure | happiness exists).  The calculation formula is similar to the formula for bigrams: <br><br><blockquote>  P (pleasure | happiness exists) = C (happiness is pleasure) / C (happiness exists) <br></blockquote><br><br>  So, the generation of the text can be carried out as follows.  We will form each offer separately by performing the following steps: <br><br>  * choose the word most likely to start the sentence; <br>  * we select the most probable word continuation depending on the two previous words; <br>  * repeat the previous step until we meet the end of sentence symbol. <br><br><h3>  Practice </h3><br>  To begin with, we need to prepare the body on which we will train our model.  I, for example, took a sample from Leo Tolstoy on the site lib.ru, and formed one text file (you can download it <a href="http://linguis.ru/static/tolstoy.txt">here</a> ). <br><br>  From this text we select the sequence of words we need. <br><br><blockquote><code><font color="#008000"><strong>import</strong></font> <font color="#0000FF"><strong>re</strong></font> <br> <br> r_alphabet <font color="#666666">=</font> re <font color="#666666">.</font> compile( <font color="#BA2121">u'[--0-9-]+|[.,:;?!]+'</font> ) <br> <br> <font color="#008000"><strong>def</strong></font> <font color="#0000FF">gen_lines</font> (corpus): <br> data <font color="#666666">=</font> <font color="#008000">open</font> (corpus) <br> <font color="#008000"><strong>for</strong></font> line <font color="#AA22FF"><strong>in</strong></font> data: <br> <font color="#008000"><strong>yield</strong></font> line <font color="#666666">.</font> decode( <font color="#BA2121">'utf-8'</font> ) <font color="#666666">.</font> lower() <br> <br> <font color="#008000"><strong>def</strong></font> <font color="#0000FF">gen_tokens</font> (lines): <br> <font color="#008000"><strong>for</strong></font> line <font color="#AA22FF"><strong>in</strong></font> lines: <br> <font color="#008000"><strong>for</strong></font> token <font color="#AA22FF"><strong>in</strong></font> r_alphabet <font color="#666666">.</font> findall(line): <br> <font color="#008000"><strong>yield</strong></font> token <br> <br> lines <font color="#666666">=</font> gen_lines( <font color="#BA2121">'tolstoy.txt'</font> ) <br> tokens <font color="#666666">=</font> gen_tokens(lines) <br></code> </blockquote><br><br>  The resulting tokens generator will produce a ‚Äúcleaned‚Äù sequence of words and punctuation.  However, the simple sequence is not interesting to us.  We are interested in triples of tokens (here a token is understood as a word or a punctuation mark, that is, some atomic elements of the text).  To do this, we will add another generator, at the output of which we will have three successive tokens. <br><br><blockquote> <code><font color="#008000"><strong>import</strong></font> <font color="#0000FF"><strong>re</strong></font> <br> <br> r_alphabet <font color="#666666">=</font> re <font color="#666666">.</font> compile( <font color="#BA2121">u'[--0-9-]+|[.,:;?!]+'</font> ) <br> <br> <font color="#008000"><strong>def</strong></font> <font color="#0000FF">gen_lines</font> (corpus): <br> data <font color="#666666">=</font> <font color="#008000">open</font> (corpus) <br> <font color="#008000"><strong>for</strong></font> line <font color="#AA22FF"><strong>in</strong></font> data: <br> <font color="#008000"><strong>yield</strong></font> line <font color="#666666">.</font> decode( <font color="#BA2121">'utf-8'</font> ) <font color="#666666">.</font> lower() <br> <br> <font color="#008000"><strong>def</strong></font> <font color="#0000FF">gen_tokens</font> (lines): <br> <font color="#008000"><strong>for</strong></font> line <font color="#AA22FF"><strong>in</strong></font> lines: <br> <font color="#008000"><strong>for</strong></font> token <font color="#AA22FF"><strong>in</strong></font> r_alphabet <font color="#666666">.</font> findall(line): <br> <font color="#008000"><strong>yield</strong></font> token <br> <br> <font color="#008000"><strong>def</strong></font> <font color="#0000FF">gen_trigrams</font> (tokens): <br> t0, t1 <font color="#666666">=</font> <font color="#BA2121">'$'</font> , <font color="#BA2121">'$'</font> <br> <font color="#008000"><strong>for</strong></font> t2 <font color="#AA22FF"><strong>in</strong></font> tokens: <br> <font color="#008000"><strong>yield</strong></font> t0, t1, t2 <br> <font color="#008000"><strong>if</strong></font> t2 <font color="#AA22FF"><strong>in</strong></font> <font color="#BA2121">'.!?'</font> : <br> <font color="#008000"><strong>yield</strong></font> t1, t2, <font color="#BA2121">'$'</font> <br> <font color="#008000"><strong>yield</strong></font> t2, <font color="#BA2121">'$'</font> , <font color="#BA2121">'$'</font> <br> t0, t1 <font color="#666666">=</font> <font color="#BA2121">'$'</font> , <font color="#BA2121">'$'</font> <br> <font color="#008000"><strong>else</strong></font> : <br> t0, t1 <font color="#666666">=</font> t1, t2 <br> <br> lines <font color="#666666">=</font> gen_lines( <font color="#BA2121">'tolstoy.txt'</font> ) <br> tokens <font color="#666666">=</font> gen_tokens(lines) <br> trigrams <font color="#666666">=</font> gen_trigrams(tokens) <br></code> </blockquote><br><br>  The gen_trigrams method requires clarification.  The '$' characters are used to mark the beginning of a sentence.  Further, it makes it easier to select the first word of the generated phrase.  In general, the method works as follows: it returns three tokens in succession, shifting by one token at each iteration: <br><br><blockquote>  At the entrance: <br>  'Happiness is pleasure without repentance' <br><br>  At the exit: <br>  iteration tokens <br>  0: $ $ happiness <br>  1: $ happiness is <br>  2: happiness is pleasure <br>  3: have fun without <br>  ... <br></blockquote><br><br>  Next, we calculate the trigram model: <br><br><blockquote> <code><font color="#008000"><strong>import</strong></font> <font color="#0000FF"><strong>re</strong></font> <br> <font color="#008000"><strong>from</strong></font> <font color="#0000FF"><strong>collections</strong></font> <font color="#008000"><strong>import</strong></font> defaultdict <br> <br> r_alphabet <font color="#666666">=</font> re <font color="#666666">.</font> compile( <font color="#BA2121">u'[--0-9-]+|[.,:;?!]+'</font> ) <br> <br> <font color="#666666">...</font> <br> <br> <font color="#008000"><strong>def</strong></font> <font color="#0000FF">train</font> (corpus): <br> lines <font color="#666666">=</font> gen_lines(corpus) <br> tokens <font color="#666666">=</font> gen_tokens(lines) <br> trigrams <font color="#666666">=</font> gen_trigrams(tokens) <br> <br> bi, tri <font color="#666666">=</font> defaultdict( <font color="#008000"><strong>lambda</strong></font> : <font color="#666666">0.0</font> ), defaultdict( <font color="#008000"><strong>lambda</strong></font> : <font color="#666666">0.0</font> ) <br> <br> <font color="#008000"><strong>for</strong></font> t0, t1, t2 <font color="#AA22FF"><strong>in</strong></font> trigrams: <br> bi[t0, t1] <font color="#666666">+=</font> <font color="#666666">1</font> <br> tri[t0, t1, t2] <font color="#666666">+=</font> <font color="#666666">1</font> <br> <br> model <font color="#666666">=</font> {} <br> <font color="#008000"><strong>for</strong></font> (t0, t1, t2), freq <font color="#AA22FF"><strong>in</strong></font> tri <font color="#666666">.</font> iteritems(): <br> <font color="#008000"><strong>if</strong></font> (t0, t1) <font color="#AA22FF"><strong>in</strong></font> model: <br> model[t0, t1] <font color="#666666">.</font> append((t2, freq <font color="#666666">/</font> bi[t0, t1])) <br> <font color="#008000"><strong>else</strong></font> : <br> model[t0, t1] <font color="#666666">=</font> [(t2, freq <font color="#666666">/</font> bi[t0, t1])] <br> <font color="#008000"><strong>return</strong></font> model <br> <br> model <font color="#666666">=</font> train( <font color="#BA2121">'tolstoy.txt'</font> ) <br></code> </blockquote><br><br>  In the first part of this method, we define generators.  Next, we calculate the bigrams and trigrams (in fact, we count the number of identical pairs and triples of words in the text).  Next, we calculate the probability of a word depending on the two previous ones, putting the given word and its probability in the dictionary.  I must say that this is not the most optimal method, since there is a significant memory consumption.  But for small cases this is quite enough. <br><br>  Now we are ready to generate the text.  The following function returns a sentence. <br><br><blockquote> <code><font color="#666666">...</font> <br> <br> model <font color="#666666">=</font> train( <font color="#BA2121">'tolstoy.txt'</font> ) <br> <br> <font color="#008000"><strong>def</strong></font> <font color="#0000FF">generate_sentence</font> (model): <br> phrase <font color="#666666">=</font> <font color="#BA2121">''</font> <br> t0, t1 <font color="#666666">=</font> <font color="#BA2121">'$'</font> , <font color="#BA2121">'$'</font> <br> <font color="#008000"><strong>while</strong></font> <font color="#666666">1</font> : <br> t0, t1 <font color="#666666">=</font> t1, unirand(model[t0, t1]) <br> <font color="#008000"><strong>if</strong></font> t1 <font color="#666666">==</font> <font color="#BA2121">'$'</font> : <font color="#008000"><strong>break</strong></font> <br> <font color="#008000"><strong>if</strong></font> t1 <font color="#AA22FF"><strong>in</strong></font> ( <font color="#BA2121">'.!?,;:'</font> ) <font color="#AA22FF"><strong>or</strong></font> t0 <font color="#666666">==</font> <font color="#BA2121">'$'</font> : <br> phrase <font color="#666666">+=</font> t1 <br> <font color="#008000"><strong>else</strong></font> : <br> phrase <font color="#666666">+=</font> <font color="#BA2121">' '</font> <font color="#666666">+</font> t1 <br> <font color="#008000"><strong>return</strong></font> phrase <font color="#666666">.</font> capitalize() <br></code> </blockquote><br><br>  The essence of the method is that we consistently select the most likely words or punctuation marks until we meet the sign of the beginning of the next phrase (the $ symbol).  The first word is selected as the most likely to start a sentence from the set model ['$', '$']. <br><br>  Here it is necessary to note an important point.  The model dictionary for each pair of words contains a list of pairs (word, probability).  We need to choose only one word from this set.  The option "in the forehead" - choose the word with the maximum probability.  But then all the generated phrases would be similar to each other.  A more suitable way is to choose words with a certain chaos, which would depend on the probability of the word (we do not want our phrases to consist of rare combinations).  This makes the unirand method, which returns a random word with a probability equal to the probability of the given word, depending on the two previous ones. <br><br>  Total, the full code of our generator is as follows: <br><br><blockquote> <code><font color="#408080"><em>#!/usr/bin/env python</em></font> <br> <font color="#408080"><em># -*- coding: utf-8 -*-</em></font> <br> <br> <font color="#008000"><strong>import</strong></font> <font color="#0000FF"><strong>re</strong></font> <br> <font color="#008000"><strong>from</strong></font> <font color="#0000FF"><strong>random</strong></font> <font color="#008000"><strong>import</strong></font> uniform <br> <font color="#008000"><strong>from</strong></font> <font color="#0000FF"><strong>collections</strong></font> <font color="#008000"><strong>import</strong></font> defaultdict <br> <br> r_alphabet <font color="#666666">=</font> re <font color="#666666">.</font> compile( <font color="#BA2121">u'[--0-9-]+|[.,:;?!]+'</font> ) <br> <br> <font color="#008000"><strong>def</strong></font> <font color="#0000FF">gen_lines</font> (corpus): <br> data <font color="#666666">=</font> <font color="#008000">open</font> (corpus) <br> <font color="#008000"><strong>for</strong></font> line <font color="#AA22FF"><strong>in</strong></font> data: <br> <font color="#008000"><strong>yield</strong></font> line <font color="#666666">.</font> decode( <font color="#BA2121">'utf-8'</font> ) <font color="#666666">.</font> lower() <br> <br> <font color="#008000"><strong>def</strong></font> <font color="#0000FF">gen_tokens</font> (lines): <br> <font color="#008000"><strong>for</strong></font> line <font color="#AA22FF"><strong>in</strong></font> lines: <br> <font color="#008000"><strong>for</strong></font> token <font color="#AA22FF"><strong>in</strong></font> r_alphabet <font color="#666666">.</font> findall(line): <br> <font color="#008000"><strong>yield</strong></font> token <br> <br> <font color="#008000"><strong>def</strong></font> <font color="#0000FF">gen_trigrams</font> (tokens): <br> t0, t1 <font color="#666666">=</font> <font color="#BA2121">'$'</font> , <font color="#BA2121">'$'</font> <br> <font color="#008000"><strong>for</strong></font> t2 <font color="#AA22FF"><strong>in</strong></font> tokens: <br> <font color="#008000"><strong>yield</strong></font> t0, t1, t2 <br> <font color="#008000"><strong>if</strong></font> t2 <font color="#AA22FF"><strong>in</strong></font> <font color="#BA2121">'.!?'</font> : <br> <font color="#008000"><strong>yield</strong></font> t1, t2, <font color="#BA2121">'$'</font> <br> <font color="#008000"><strong>yield</strong></font> t2, <font color="#BA2121">'$'</font> , <font color="#BA2121">'$'</font> <br> t0, t1 <font color="#666666">=</font> <font color="#BA2121">'$'</font> , <font color="#BA2121">'$'</font> <br> <font color="#008000"><strong>else</strong></font> : <br> t0, t1 <font color="#666666">=</font> t1, t2 <br> <br> <font color="#008000"><strong>def</strong></font> <font color="#0000FF">train</font> (corpus): <br> lines <font color="#666666">=</font> gen_lines(corpus) <br> tokens <font color="#666666">=</font> gen_tokens(lines) <br> trigrams <font color="#666666">=</font> gen_trigrams(tokens) <br> <br> bi, tri <font color="#666666">=</font> defaultdict( <font color="#008000"><strong>lambda</strong></font> : <font color="#666666">0.0</font> ), defaultdict( <font color="#008000"><strong>lambda</strong></font> : <font color="#666666">0.0</font> ) <br> <br> <font color="#008000"><strong>for</strong></font> t0, t1, t2 <font color="#AA22FF"><strong>in</strong></font> trigrams: <br> bi[t0, t1] <font color="#666666">+=</font> <font color="#666666">1</font> <br> tri[t0, t1, t2] <font color="#666666">+=</font> <font color="#666666">1</font> <br> <br> model <font color="#666666">=</font> {} <br> <font color="#008000"><strong>for</strong></font> (t0, t1, t2), freq <font color="#AA22FF"><strong>in</strong></font> tri <font color="#666666">.</font> iteritems(): <br> <font color="#008000"><strong>if</strong></font> (t0, t1) <font color="#AA22FF"><strong>in</strong></font> model: <br> model[t0, t1] <font color="#666666">.</font> append((t2, freq <font color="#666666">/</font> bi[t0, t1])) <br> <font color="#008000"><strong>else</strong></font> : <br> model[t0, t1] <font color="#666666">=</font> [(t2, freq <font color="#666666">/</font> bi[t0, t1])] <br> <font color="#008000"><strong>return</strong></font> model <br> <br> <font color="#008000"><strong>def</strong></font> <font color="#0000FF">generate_sentence</font> (model): <br> phrase <font color="#666666">=</font> <font color="#BA2121">''</font> <br> t0, t1 <font color="#666666">=</font> <font color="#BA2121">'$'</font> , <font color="#BA2121">'$'</font> <br> <font color="#008000"><strong>while</strong></font> <font color="#666666">1</font> : <br> t0, t1 <font color="#666666">=</font> t1, unirand(model[t0, t1]) <br> <font color="#008000"><strong>if</strong></font> t1 <font color="#666666">==</font> <font color="#BA2121">'$'</font> : <font color="#008000"><strong>break</strong></font> <br> <font color="#008000"><strong>if</strong></font> t1 <font color="#AA22FF"><strong>in</strong></font> ( <font color="#BA2121">'.!?,;:'</font> ) <font color="#AA22FF"><strong>or</strong></font> t0 <font color="#666666">==</font> <font color="#BA2121">'$'</font> : <br> phrase <font color="#666666">+=</font> t1 <br> <font color="#008000"><strong>else</strong></font> : <br> phrase <font color="#666666">+=</font> <font color="#BA2121">' '</font> <font color="#666666">+</font> t1 <br> <font color="#008000"><strong>return</strong></font> phrase <font color="#666666">.</font> capitalize() <br> <br> <font color="#008000"><strong>def</strong></font> <font color="#0000FF">unirand</font> (seq): <br> sum_, freq_ <font color="#666666">=</font> <font color="#666666">0</font> , <font color="#666666">0</font> <br> <font color="#008000"><strong>for</strong></font> item, freq <font color="#AA22FF"><strong>in</strong></font> seq: <br> sum_ <font color="#666666">+=</font> freq <br> rnd <font color="#666666">=</font> uniform( <font color="#666666">0</font> , sum_) <br> <font color="#008000"><strong>for</strong></font> token, freq <font color="#AA22FF"><strong>in</strong></font> seq: <br> freq_ <font color="#666666">+=</font> freq <br> <font color="#008000"><strong>if</strong></font> rnd <font color="#666666">&lt;</font> freq_: <br> <font color="#008000"><strong>return</strong></font> token <br> <br> <font color="#008000"><strong>if</strong></font> __name__ <font color="#666666">==</font> <font color="#BA2121">'__main__'</font> : <br> model <font color="#666666">=</font> train( <font color="#BA2121">'tolstoy.txt'</font> ) <br> <font color="#008000"><strong>for</strong></font> i <font color="#AA22FF"><strong>in</strong></font> <font color="#008000">range</font> ( <font color="#666666">10</font> ): <br> <font color="#008000"><strong>print</strong></font> generate_sentence(model), <br></code> </blockquote><br><br>  Great, you got to the end.  I envy your patience :). <br><br><h3>  Why trigrams </h3><br>  Trigram model is chosen for simplicity and clarity.  Bigrams would give poor results, while 4 grams would require significantly more resources.  In any case, it is quite simple to expand this algorithm to handle the general case of N-grams.  However, it is worth considering that the more N, the more your text is similar to the original body. <br><br></div><p>Source: <a href="https://habr.com/ru/post/88514/">https://habr.com/ru/post/88514/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../88507/index.html">Competition and new release of Cerebro</a></li>
<li><a href="../88509/index.html">Meet: MiniBot</a></li>
<li><a href="../88511/index.html">Ruby on Rails and Yandex. Photos API: show the latest photos on the home page</a></li>
<li><a href="../88512/index.html">About small OS</a></li>
<li><a href="../88513/index.html">Java EE 6. What's new in Servlet API 3.0</a></li>
<li><a href="../88520/index.html">It's time to fade</a></li>
<li><a href="../88524/index.html">Maths. Symmetry of "pseudo-simple twins"</a></li>
<li><a href="../88525/index.html">MTS canceled tariffs with unlimited Internet</a></li>
<li><a href="../88528/index.html">Is there informatization of schools or all the same?</a></li>
<li><a href="../88529/index.html">Combat railway missile complex "Molodets"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>