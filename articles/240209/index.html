<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Latent semantic analysis: implementation</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="As mentioned in the previous article , latent-semantic analysis (LSA / LSA) allows you to identify latent relationships of the phenomena or objects un...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Latent semantic analysis: implementation</h1><div class="post__text post__text-html js-mediator-article">  As mentioned in the <a href="http://geektimes.ru/post/230075/">previous article</a> , latent-semantic analysis (LSA / LSA) allows you to identify latent relationships of the phenomena or objects under study, which is an important criterion when modeling the processes of understanding and thinking. <br><br>  Now I will write a little about the implementation of the LSA. <br><a name="habracut"></a><br><h5>  A bit of history </h5><br>  LSA was patented in 1988 by a group of American research engineers S. Deerwester at al [US Patent 4,839,853]. <br>  In the field of information retrieval, this approach is called latent semantic indexing (LSI). <br><br>  For the first time, the LSA was used to automatically index texts, identify the semantic structure of the text, and obtain pseudo-documents.  Then this method was quite successfully used to represent knowledge bases and build cognitive models.  In the USA, this method was patented to test the knowledge of schoolchildren and students, as well as to check the quality of teaching methods. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h5>  Job Description LSA </h5><br>  As the initial information, LSA uses a matrix of terms-on-documents (terms - words, phrases or n-grams; documents - texts, classified either by some criterion, or arbitrarily separated - it depends on the problem to be solved), describing the data set used to train the system.  The elements of this matrix contain, as a rule, weights that take into account the frequency of use of each term in each document or probability measures (PLSA is a probabilistic latent semantic analysis) based on independent multimodal distribution. <br><br>  The most common variant of LSA is based on the use of a real-valued decomposition of a matrix with singular values ‚Äã‚Äãor an SVD-decomposition (SVD - Singular Value Decomposition).  With it, any matrix can be decomposed into a set of orthogonal matrices, the linear combination of which is a fairly accurate approximation to the original matrix. <br><br>  According to the singular decomposition theorem, in the simplest case, a matrix can be decomposed into the product of three matrices: <br><img src="https://habrastorage.org/getpro/habr/post_images/832/785/a72/832785a7218452a860a8e7bbd04a0520.png" alt="image"><br>  where the matrices U and V are orthogonal, and S is the diagonal matrix, the values ‚Äã‚Äãon the diagonal of which are called the singular values ‚Äã‚Äãof the matrix A. <br>  T symbol in the matrix notation <img src="https://habrastorage.org/getpro/habr/post_images/a8d/5af/eab/a8d5afeab38da5f2419948ee5de3e0ea.png" height="40" width="40" alt="image">  means matrix transposition. <br><br>  The peculiarity of such an expansion is that if we leave only k of the largest singular values ‚Äã‚Äãin the matrix S, then a linear combination of the resulting matrices <img src="https://habrastorage.org/getpro/habr/post_images/deb/f05/03e/debf0503e183cb137db9db2b8db87590.png" alt="image">  is the best approximation of the original matrix A to a matrix ƒÇ of rank k: <br><img src="https://habrastorage.org/getpro/habr/post_images/012/853/fbe/012853fbea3443b2e2e216bfbcab16db.png" alt="image"><br>  <b>The basic idea of ‚Äã‚Äãlatent semantic analysis is as follows:</b> <b><br></b>  <b>after matrix multiplication, the resulting matrix ƒÇ, containing only k first linearly independent components of the original matrix A, reflects the structure of dependencies (in this case, associative) that are latently present in the original matrix.</b>  <b>The structure of dependencies is determined by the weight functions of the terms for each document.</b> <br><br>  The choice of k depends on the task and is chosen empirically.  It depends on the number of source documents. <br>  If there are not many documents, for example, a hundred, then k can be taken 5-10% of the total number of diagonal values;  if there are hundreds of thousands of documents, then they take 0.1-2%.  It should be remembered that if the selected value of k is too large, then the method loses its power and approaches the characteristics of standard vector methods.  Too small a value of k does not allow catching differences between similar terms or documents: there will be only one main component that ‚Äúpulls the blanket over itself‚Äù, i.e.  all weak and even unrelated terms. <br><img src="https://habrastorage.org/getpro/habr/post_images/1d4/b69/82c/1d4b6982c4e1ce139a89c12cbcce55ff.jpg" alt="image"><br>  Figure 1. SVD decomposition of matrix A of dimension (TXD) into a matrix of terms U of dimension (TX k), matrix of documents V of dimension (k XD) and diagonal matrix S of dimension (k X k), where k is the number of singular values ‚Äã‚Äãof the diagonal matrix S . <br><br>  The volume of the case for building a model should be large - preferably about three to five million word usage.  But the method also works on collections of a smaller size, although a bit worse. <br><br>  Arbitrary splitting of text into documents usually produces from a thousand to several tens of thousands of parts of approximately the same volume.  Thus, the term-to-documents matrix is ‚Äã‚Äãrectangular and can be highly fragmented.  For example, with a volume of 5 million word forms, a matrix of 30-50 thousand documents per 200-300 thousand, and sometimes more, terms is obtained.  In fact, low-frequency terms can be omitted, because  this will significantly reduce the dimension of the matrix (say, if you do not use 5% of the total volume of low-frequency terms, then the dimension will be reduced by half), which will lead to a decrease in computational resources and time. <br><br>  The choice of reducing the singular values ‚Äã‚Äãof the diagonal matrix (of dimension k) for the inverse multiplication of matrices, as I wrote, is arbitrary.  With the above dimension of the matrix, several hundred (100-300) main components are left.  In this case, as practice shows, the dependence of the number of components and the accuracy change nonlinearly: for example, if you start increasing their number, then the accuracy will fall, but at a certain value, say, 10,000 - will increase again to the optimal case. <br><br>  Below is an example of the occurrence and change of the main factors with a decrease in the number of singular elements of the diagonal matrix. <br>  Accuracy can be assessed on the marked material, for example, by adjusting the result in advance according to the h of the answers to the questions. <br>  For more information about this method, see, for example, review articles [see  bibliography]. <br>  Those who want to try out LSA can <a href="http://lingurus.net/soft.html">download binaries</a> for building models and for testing and using them. <br>  The program for SVD decomposition of the matrix in the construction of models used <a href="http://alglib.sources.ru/">open source</a> (Copyright Sergey Bochkanov (ALGLIB project).), So the programs are distributed without restrictions. <br><br><h5>  Application </h5><br><ul><li>  comparison of two terms between themselves; </li><li>  comparison of two documents among themselves; </li><li>  comparison of the term and the document. </li></ul><br>  Also, this method is sometimes used to find the ‚Äúnearest neighbor‚Äù - the closest in terms of terms associated with the original.  This property is used to search for related terms. <br>  It should be clarified that proximity by meaning is a context-dependent value, therefore not every close term will correspond to an association (it can be a synonym, and an antonym, and just a word or phrase that often occurs together with the term being searched for). <br><br><h5>  Advantages and disadvantages of LSA </h5><br>  The advantage of the method can be considered its remarkable ability to identify dependencies between words when conventional statistical methods are powerless.  LSA can also be applied both with training (with preliminary thematic classification of documents), and without training (arbitrary splitting of an arbitrary text), which depends on the problem being solved. <br><br>  I wrote about the main drawbacks in the previous article.  To these, one can add a significant decrease in the computation speed with an increase in the amount of input data (in particular, during the SVD transformation). <br>  As shown in [Deerwester et al.], The calculation speed corresponds to the order <img src="https://habrastorage.org/getpro/habr/post_images/ca8/5eb/95c/ca85eb95cfc0c816effd82bc55cb3e08.png">  where <img src="https://habrastorage.org/getpro/habr/post_images/58b/469/5c1/58b4695c1bfd113b4b874ce6c4910804.png">  - the sum of the number of documents and terms, k is the dimension of the space of factors. <br><br><h5>  A small demo example of calculating the matrix of proximity of full-text documents with a different number of singular elements of the diagonal matrix in the SVD-transformation </h5><br><img src="https://habrastorage.org/getpro/habr/post_images/867/870/e2a/867870e2a7d75594a4c885bc0b567775.gif" title="LSA"><br><br>  The figures show the emergence and change of the main factors with a decrease in the number of singular elements of the diagonal matrix from 100% to ~ 12%.  Three-dimensional drawings are a symmetric matrix obtained by calculating the scalar product of the vectors of each reference document with each tested.  The reference set of vectors is a text pre-marked on 30 documents;  tested - with a decrease in the number of singular values ‚Äã‚Äãof the diagonal matrix obtained by SVD-analysis.  The number of documents (reference and tested) is deposited on the X and Y axes, and the volume of the lexicon is on the Z axis. <br>  The figures clearly show that when the number of singular diagonal elements decreases by 20-30%, the factors are not yet clearly revealed, but this results in correlations of similar documents (small peaks outside the diagonal), which initially increase slightly, and then, with a decrease in the number of singular values ‚Äã‚Äã(up to 70-80%) - disappear.  With automatic clustering, such peaks are noise, so it is desirable to minimize them.  If the goal is to obtain associative links within the documents, then an optimal ratio should be found for preserving the main lexicon and mixing the associative one. <br><br><h5>  LITERATURE </h5><br><br><ul><li>  Thomas Landauer, Peter W. Foltz, &amp; Darrell Laham (1998).  "Introduction to Latent Semantic Analysis" (PDF).  Discourse Processes 25: 259‚Äì284.  US Patent 4,839,853 (Eng.) </li><li>  Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, Richard Harshman (1990).  Indexing by Latent Semantic Analysis (PDF).  Journal of the American Society for Information Science 41 (6): 391‚Äì407. </li><li>  Thomas Landauer, T. Dumais Solution: A Case for Acceptance, Induction, and Representation of Knowledge 211‚Äì240 (1997).  Retrieved July 2, 2007. </li><li>  B. Lemaire, G. Denhi√®re Cognitive Models based on Latent Semantic Analysis (2003). </li><li>  Nekrestyanov I.S.  Thematic-oriented methods of information retrieval / Thesis for the degree of Ph.D.  SPSU, 2000. </li><li>  Solovyov A.N.  Modeling of processes of understanding of speech using latent-semantic analysis / thesis for the degree of K. f-m.n.  SPSU, 2008. </li><li>  Golub J., Van Lone C. Matrix calculations.  M .: Mir, 1999. </li></ul><br><br><h5>  Links </h5><br><br><ul><li>  <a href="http://webcom.upmf-grenoble.fr/LPNC/resources/benoit_lemaire/lsa.html">webcom.upmf-grenoble.fr/LPNC/resources/benoit_lemaire/lsa.html</a> - Readings in Latent Semantic Analysis for Cognitive Science and Education.  - Collection of articles and references about LSA. </li><li>  <a href="http://lsa.colorado.edu/">lsa.colorado.edu</a> is a site dedicated to LSA modeling. </li><li>  <a href="http://www.cs.utk.edu/~lsi/">www.cs.utk.edu/%7Elsi</a> - Latent Semantic Indexing (latent-semantic indexing). </li><li>  <a href="http://archive.today/1AnaN">archive.today/1AnaN</a> - Telcordia Latent Semantic Indexing (LSI).  Demo Machine - Demonstration site for latent semantic indexing. </li><li>  <a href="http://cran.at.r-project.org/web/packages/lsa/index.html">cran.at.r-project.org/web/packages/lsa/index.html</a> - Open Source LSA Package </li></ul></div><p>Source: <a href="https://habr.com/ru/post/240209/">https://habr.com/ru/post/240209/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../240197/index.html">How to speed up the container: we tune OpenVZ</a></li>
<li><a href="../240199/index.html">Opera Developer Bookmark Sync</a></li>
<li><a href="../240201/index.html">New Wi-Fi for the Internet of Things (Part 1)</a></li>
<li><a href="../240203/index.html">Substitution Benefit Criterion and Dynamic Profiling</a></li>
<li><a href="../240207/index.html">Print outsourcing: how to save on printing with active paper workflow</a></li>
<li><a href="../240213/index.html">How do we cluster gifts in OK</a></li>
<li><a href="../240217/index.html">Lapis: Lua site in Nginx configs</a></li>
<li><a href="../240219/index.html">Expressive JavaScript: An Introduction</a></li>
<li><a href="../240221/index.html">Development of quadrocopter angular stabilization</a></li>
<li><a href="../240223/index.html">Expressive JavaScript: Values, Types and Operators</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>