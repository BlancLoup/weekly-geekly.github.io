<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Neural Networks and Deep Learning, Chapter 4: Visual Proof that Neural Networks Can Calculate Any Function</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this chapter, I give a simple and mostly visual explanation of the universality theorem. To follow the material in this chapter, you do not have to...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Neural Networks and Deep Learning, Chapter 4: Visual Proof that Neural Networks Can Calculate Any Function</h1><div class="post__text post__text-html js-mediator-article">  In this chapter, I give a simple and mostly visual explanation of the universality theorem.  To follow the material in this chapter, you do not have to read the previous ones.  It is structured as an independent essay.  If you have the most basic understanding of NS, you should be able to understand the explanations. <br><br><div class="spoiler">  <b class="spoiler_title">Content</b> <div class="spoiler_text"><ul><li>  <a href="https://habr.com/ru/post/456738/">Chapter 1: using neural networks to recognize handwritten numbers</a> </li><li>  <a href="https://habr.com/ru/post/457980/">Chapter 2: how the backpropagation algorithm works</a> </li><li>  Chapter 3: <ul><li>  <a href="https://habr.com/ru/post/458724/">Part 1: improving the method of training neural networks</a> <br></li><li>  <a href="https://habr.com/ru/post/459816/">Part 2: Why does regularization help reduce retraining?</a> <br></li><li>  <a href="https://habr.com/ru/post/460711/">Part 3: how to choose neural network hyperparameters?</a> <br></li></ul></li><li>  <a href="https://habr.com/ru/post/461659/">Chapter 4: visual proof that neural networks are capable of computing any function</a> </li></ul></div></div><br>  One of the most amazing facts about neural networks is that they can calculate any function at all.  That is, let's say that someone gives you some kind of complex and winding function f (x): <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><a name="habracut"></a><br>  And regardless of this function, there is guaranteed such a neural network that for any input x the value f (x) (or some approximation close to it) will be the output of this network, that is: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/webt/eb/ud/zc/ebudzc72xyiytdvkf4ulx-u7onq.png"><br><br>  This works even if it is a function of many variables f = f (x <sub>1</sub> , ..., x <sub>m</sub> ), and with many values.  For example, here is a network computing a function with m = 3 inputs and n = 2 outputs: <br><br><img src="https://habrastorage.org/webt/1g/01/i7/1g01i7vpnwo-mlm1r2brhd9al9m.png"><br><br>  This result suggests that neural networks have a certain universality.  No matter what function we want to calculate, we know that there is a neural network that can do this. <br><br>  Moreover, the universality theorem holds even if we restrict the network to a single layer between incoming and outgoing neurons - the so-called  in one hidden layer.  So even networks with a very simple architecture can be extremely powerful. <br><br>  The universality theorem is well known to people using neural networks.  But although this is so, an understanding of this fact is not so widespread.  And most of the explanations for this are too technically complex.  For example, <a href="http://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf">one of the first papers</a> proving this result used the <a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B5%25D0%25BE%25D1%2580%25D0%25B5%25D0%25BC%25D0%25B0_%25D0%25A5%25D0%25B0%25D0%25BD%25D0%25B0_%25E2%2580%2594_%25D0%2591%25D0%25B0%25D0%25BD%25D0%25B0%25D1%2585%25D0%25B0">Hahn - Banach theorem</a> , the <a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B5%25D0%25BE%25D1%2580%25D0%25B5%25D0%25BC%25D0%25B0_%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D1%2581%25D1%2582%25D0%25B0%25D0%25B2%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B9_%25D0%25A0%25D0%25B8%25D1%2581%25D0%25B0">Riesz representation</a> <a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B5%25D0%25BE%25D1%2580%25D0%25B5%25D0%25BC%25D0%25B0_%25D0%25A5%25D0%25B0%25D0%25BD%25D0%25B0_%25E2%2580%2594_%25D0%2591%25D0%25B0%25D0%25BD%25D0%25B0%25D1%2585%25D0%25B0">theorem</a> , and some Fourier analysis.  If you are a mathematician, it‚Äôs easy for you to understand this evidence, but for most people it‚Äôs not so easy.  It‚Äôs a pity, because the basic reasons for universality are simple and beautiful. <br><br>  In this chapter, I give a simple and mostly visual explanation of the universality theorem.  We will go step by step through the ideas underlying it.  You will understand why neural networks can really calculate any function.  You will understand some of the limitations of this result.  And you will understand how the result is associated with deep NS. <br><br>  To follow the material in this chapter, you do not have to read the previous ones.  It is structured as an independent essay.  If you have the most basic understanding of NS, you should be able to understand the explanations.  But I will sometimes provide links to previous material to help fill knowledge gaps. <br><br>  Universality theorems are often found in computer science, so sometimes we even forget how amazing they are.  But it‚Äôs worth reminding yourself: the ability to calculate any arbitrary function is truly amazing.  Almost any process that you can imagine can be reduced to calculating a function.  Consider the task of finding the name of a musical composition based on a brief passage.  This can be considered a function calculation.  Or consider the task of translating a Chinese text into English.  And this can be considered a function calculation (in fact, many functions, since there are many acceptable options for translating a single text).  Or consider the task of generating a description of the plot of the film and the quality of the acting based on the mp4 file.  This, too, can be considered as the calculation of a certain function (the remark made regarding the text translation options is also correct here).  Universality means that, in principle, NSs can perform all these tasks, and many others. <br><br>  Of course, only from the fact that we know that there are NSs capable of, say, translating from Chinese to English, it does not follow that we have good techniques for creating or even recognizing such a network.  This restriction also applies to traditional universality theorems for models such as Boolean schemes.  But, as we have already seen in this book, the NS has powerful algorithms for learning functions.  The combination of learning algorithms and versatility is an attractive mix.  So far in the book, we have focused on training algorithms.  In this chapter, we will focus on universality and what it means. <br><br><h2>  Two tricks </h2><br>  Before explaining why the universality theorem is true, I want to mention two tricks contained in the informal statement ‚Äúa neural network can calculate any function‚Äù. <br><br>  Firstly, this does not mean that the network can be used to accurately calculate any function.  We can only get as good an approximation as we need.  By increasing the number of hidden neurons, we improve the approximation.  For example, I previously illustrated a network computing a certain function f (x) using three hidden neurons.  For most functions, using three neurons, only a low-quality approximation can be obtained.  By increasing the number of hidden neurons (say, up to five), we can usually get an improved approximation: <br><br><img src="https://habrastorage.org/webt/x2/nt/zw/x2ntzw4ykxb450nexszfsd-qz08.png"><br><br>  And to improve the situation, increasing the number of hidden neurons and further. <br><br>  To clarify this statement, let's say we were given a function f (x), which we want to calculate with some necessary accuracy Œµ&gt; 0.  There is a guarantee that when using a sufficient number of hidden neurons, we can always find an NS whose output g (x) satisfies the equation | g (x) ‚àíf (x) | &lt;Œµ for any x.  In other words, the approximation will be achieved with the desired accuracy for any possible input value. <br><br>  The second catch is that functions that can be approximated by the described method belong to a continuous class.  If the function is interrupted, that is, it makes sudden sharp jumps, then in the general case it will be impossible to approximate with the help of NS.  And this is not surprising, since our NSs calculate continuous functions of input data.  However, even if the function that we really need to calculate is discontinuous, the approximation is often quite continuous.  If so, then we can use NS.  In practice, this limitation is usually not important. <br><br>  As a result, a more accurate statement of the universality theorem will be that NS with one hidden layer can be used to approximate any continuous function with any desired accuracy.  In this chapter, we prove a slightly less rigorous version of this theorem, using two hidden layers instead of one.  In tasks, I will briefly describe how this explanation can, with minor modifications, be adapted to a proof that uses only one hidden layer. <br><br><h2>  Versatility with one input and one output value </h2><br>  To understand why the universality theorem is true, we begin by understanding how to create an NS approximating function with only one input and one output value: <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><br>  It turns out that this is the essence of the task of universality.  Once we understand this special case, it will be quite easy to extend it to functions with many input and output values. <br><br>  To create an understanding of how to construct a network for counting f, we start with a network containing a single hidden layer with two hidden neurons, and with an output layer containing one output neuron: <br><br><img src="https://habrastorage.org/webt/b7/cz/ql/b7czqllzyyxbpzeq7gs3h6a2338.png"><br><br>  To imagine how the network components work, we focus on the upper hidden neuron.  In the diagram in the <a href="http://neuralnetworksanddeeplearning.com/chap4.html">original article,</a> you can interactively change the weight with the mouse by clicking on ‚Äúw‚Äù and immediately see how the function calculated by the upper hidden neuron changes: <br><br><img src="https://habrastorage.org/webt/mh/r0/dz/mhr0dzpmf_zop4a3bwi2of04qtu.png"><br><br>  As we learned earlier in the book, a hidden neuron counts œÉ (wx + b), where œÉ (z) ‚â° 1 / (1 + e <sup>‚àíz</sup> ) is a <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B8%25D0%25B3%25D0%25BC%25D0%25BE%25D0%25B8%25D0%25B4%25D0%25B0">sigmoid</a> .  So far, we have used this algebraic form quite often.  However, to prove universality it would be better if we completely ignore this algebra, and instead manipulate and observe the shape on the graph.  This will not only help you better feel what is happening, but also give us a proof of universality applicable to other activation functions besides sigmoid. <br><br>  Strictly speaking, the visual approach I have chosen is traditionally not considered evidence.  But I believe that the visual approach provides more insight into the truth of the final result than traditional proof.  And, of course, such an understanding is the real purpose of the proof.  In the evidence I propose, gaps will occasionally come across;  I will give reasonable, but not always rigorous visual evidence.  If this bothers you, then consider it your task to fill these gaps.  However, do not lose sight of the main goal: to understand why the universality theorem is true. <br><br>  To begin with this proof, click on the offset b in the original diagram and drag to the right to enlarge it.  You will see that with increasing displacement, the graph moves to the left, but does not change shape. <br><br>  Then drag it to the left to reduce the offset.  You will see that the graph is moving to the right without changing shape. <br><br>  Reduce weight to 2-3.  You will see that as the weight decreases, the curve straightens.  To prevent the curve from running off the graph, you may need to correct the offset. <br><br>  Finally, increase the weight to values ‚Äã‚Äãgreater than 100. The curve will become steeper, and eventually approach the step.  Try adjusting the offset so that its angle is in the region of the point x = 0.3.  The video below shows what should happen: <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Your browser does not support HTML5 video. <source src="http://neuralnetworksanddeeplearning.com/movies/create_step_function.mp4" type="video/mp4"></video></div></div></div><br><br>  We can greatly simplify our analysis by increasing the weight so that the output is really a good approximation of the step function.  Below I built the output of the upper hidden neuron for the weight w = 999.  This is a static image: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/94c/24e/8a8/94c24e8a8a262c06c102b97bef033e99.jpg"><br><br>  Using step functions is a bit easier than with typical sigmoid.  The reason is that contributions from all hidden neurons are added up in the output layer.  The sum of a bunch of step functions is easy to analyze, but it‚Äôs more difficult to talk about what happens when a bunch of curves are added in the form of a sigmoid.  Therefore, it will be much easier to assume that our hidden neurons produce stepwise functions.  More precisely, we do this by fixing the weight w at some very large value, and then assigning the position of the step through the offset.  Of course, working with an output as a step function is an approximation, but it is very good, and so far we will treat the function as a true step function.  Later, I will return to a discussion of the effect of deviations from this approximation. <br><br>  What value of x is the step?  In other words, how does the position of the step depend on weight and displacement? <br><br>  To answer the question, try changing the weight and offset in the interactive chart.  Can you understand how the position of a step depends on w and b?  By practicing a little, you can convince yourself that its position is proportional to b and inversely proportional to w. <br><br>  In fact, the step is at s = ‚àíb / w, as will be seen if we adjust the weight and displacement to the following values: <br><br><img src="https://habrastorage.org/webt/ee/d9/zo/eed9zodaxp8ot33ip8y8j-sdcqg.png"><br><br>  Our lives will be greatly simplified if we describe hidden neurons with a single parameter, s, that is, by the position of the step, s = ‚àíb / w.  In the following interactive diagram, you can simply change s: <br><br><img src="https://habrastorage.org/webt/uy/g6/9h/uyg69hokiufnmt7zwwmysjof5uc.png"><br><br>  As noted above, we specially assigned a weight w at the input to a very large value - large enough so that the step function becomes a good approximation.  And we can easily turn the parameterized neuron in this way back to its usual form by choosing the bias b = ‚àíws. <br><br>  So far, we have concentrated on the output of only the superior hidden neuron.  Let's look at the behavior of the entire network.  Suppose that hidden neurons calculate the step functions defined by the parameters of the steps s <sub>1</sub> (upper neuron) and s <sub>2</sub> (lower neuron).  Their respective output weights are w <sub>1</sub> and w <sub>2</sub> .  Here is our network: <br><br><img src="https://habrastorage.org/webt/6u/ot/ns/6uotnsmlecwfh8iaqz1eb5p5tjo.png"><br><br>  On the right is a graph of the weighted output w <sub>1</sub> a <sub>1</sub> + w <sub>2</sub> a <sub>2 of the</sub> hidden layer.  Here a <sub>1</sub> and a <sub>2</sub> are the outputs of the upper and lower hidden neurons, respectively.  They are denoted by ‚Äúa‚Äù, as they are often called neuronal activations. <br><br>  By the way, we note that the output of the entire network is œÉ (w <sub>1</sub> a <sub>1</sub> + w <sub>2</sub> a <sub>2</sub> + b), where b is the bias of the output neuron.  This, obviously, is not the same as the weighted output of the hidden layer whose graph we are building.  But for now, we will concentrate on the balanced output of the hidden layer, and only later think about how it relates to the output of the entire network. <br><br>  Try to increase and decrease the step s <sub>1 of the</sub> upper hidden neuron on the interactive diagram <a href="http://neuralnetworksanddeeplearning.com/chap4.html">in the original article</a> .  See how this changes the weighted output of the hidden layer.  It is especially useful to understand what happens when s <sub>1</sub> exceeds s <sub>2</sub> .  You will see that the graph in these cases changes shape, as we move from a situation in which the upper hidden neuron is activated first to a situation in which the lower hidden neuron is activated first. <br><br>  Similarly, try manipulating the s <sub>2</sub> step <sub>of</sub> the lower hidden neuron and see how this changes the overall output of the hidden neurons. <br><br>  Try to reduce and increase output weights.  Notice how this scales the contribution from the corresponding hidden neurons.  What happens if one of the weights equals 0? <br><br>  Finally, try setting w <sub>1</sub> to 0.8 and w <sub>2</sub> to -0.8.  The result is a ‚Äúprotrusion‚Äù function, with a start at s <sub>1</sub> , an end at s <sub>2</sub> , and a height of 0.8.  For example, a weighted output might look like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/84d/9be/e75/84d9bee755d8a57bbdb3aed20d07da78.jpg"><br><br>  Of course, the protrusion can be scaled to any height.  Let's use one parameter, h, denoting height.  Also, for simplicity, I will get rid of the notation "s <sub>1</sub> = ..." and "w <sub>1</sub> = ...". <br><br><img src="https://habrastorage.org/webt/io/by/7l/ioby7lcd1whqowsw0ak9j1td16q.png"><br><br>  Try increasing and decreasing the h value to see how the height of the protrusion changes.  Try to make h negative.  Try changing the points of the steps to observe how this changes the shape of the protrusion. <br><br>  You will see that we use our neurons not just as graphic primitives, but also as units more familiar to programmers - something like an if-then-else instruction in programming: <br><br>  if input&gt; = start of step: <br>  add 1 to weighted output <br>  else: <br>  add 0 to weighted output <br><br>  For the most part I will stick to the graphic notation.  However, sometimes it will be useful for you to switch to the if-then-else view and reflect on what is happening in these terms. <br><br>  We can use our protrusion trick by gluing two parts of hidden neurons together on the same network: <br><br><img src="https://habrastorage.org/webt/4w/4p/pz/4w4ppzryydmyz3f3dglgzcwisfm.png"><br><br>  Here I dropped the weights by simply writing down the h values ‚Äã‚Äãfor each pair of hidden neurons.  Try playing with both h values ‚Äã‚Äãand see how it changes the graph.  Move the tabs, changing the points of the steps. <br><br>  In a more general case, this idea can be used to obtain any desired number of peaks of any height.  In particular, we can divide the interval [0,1] into a large number of (N) subintervals, and use N pairs of hidden neurons to obtain peaks of any desired height.  Let's see how this works for N = 5.  This is already quite a lot of neurons, so I'm a little narrower representation.  Sorry for the complicated diagram - I could hide the complexity behind additional abstractions, but it seems to me that it is worth a little torment with the complexity in order to better feel how the neural networks work. <br><br><img src="https://habrastorage.org/webt/do/2t/x-/do2tx-fp-h-w83rnboapp8w-o98.png"><br><br>  You see, we have five pairs of hidden neurons.  The points of the steps of the corresponding pairs are located at 0.1 / 5, then 1 / 5.2 / 5, and so on, up to 4 / 5.5 / 5.  These values ‚Äã‚Äãare fixed - we get five protrusions of equal width on the graph. <br><br>  Each pair of neurons has a value h associated with it.  Remember that neuron output links have weights h and ‚Äìh.  In the original article in the diagram, you can click on the h values ‚Äã‚Äãand move them left-right.  With a change in height, the schedule also changes.  By changing the output weights, we construct the final function! <br><br>  On the diagram, you can still click on the graph, and drag the height of the steps up or down.  When you change its height, you see how the height of the corresponding h changes.  The output weights + h and ‚Äìh change accordingly.  In other words, we directly manipulate a function whose graph is shown on the right and see these changes in the values ‚Äã‚Äãof h on the left.  You can also hold down the mouse button on one of the protrusions, and then drag the mouse left or right, and the protrusions will adjust to the current height. <br><br>  It is time to get the job done. <br><br>  Recall the function that I drew at the very beginning of the chapter: <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><br>  Then I did not mention this, but in fact it looks like this: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mn>0.2</mn><mo>+</mo><mn>0.4</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>0.3</mn><mi>x</mi><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>i</mi><mi>n</mi><mo stretchy=&quot;false&quot;>(</mo><mn>15</mn><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><mn>0.05</mn><mtext>&amp;#xA0;</mtext><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy=&quot;false&quot;>(</mo><mn>50</mn><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>113</mn></mrow></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="58.655ex" height="3.021ex" viewBox="0 -987.6 25254.3 1300.8" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMATHI-66" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-28" x="550" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMATHI-78" x="940" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-29" x="1512" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-3D" x="2179" y="0"></use><g transform="translate(3236,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-2E" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-32" x="779" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-2B" x="4737" y="0"></use><g transform="translate(5738,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-2E" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-34" x="779" y="0"></use></g><g transform="translate(7018,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-32" x="809" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-2B" x="8266" y="0"></use><g transform="translate(9267,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-2E" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-33" x="779" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMATHI-78" x="10546" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMATHI-73" x="11369" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMATHI-69" x="11838" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMATHI-6E" x="12184" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-28" x="12784" y="0"></use><g transform="translate(13174,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-35" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMATHI-78" x="14175" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-29" x="14747" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-2B" x="15359" y="0"></use><g transform="translate(16360,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-2E" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-30" x="779" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-35" x="1279" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMATHI-63" x="18390" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMATHI-6F" x="18823" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMATHI-73" x="19309" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-28" x="19778" y="0"></use><g transform="translate(20168,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-35"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-30" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMATHI-78" x="21169" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-29" x="21741" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMATHI-74" x="22381" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMATHI-61" x="22742" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMATHI-67" x="23272" y="0"></use><g transform="translate(23752,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-31" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/461659/&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265&amp;usg=ALkJrhiqWu9m-U363aqZ4fRVGAh2_qaAGw#MJMAIN-33" x="1001" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">f</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(</font></font></mo><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">)</font></font></mo><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">=</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0.2</font></font></mn><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0.4</font></font></mn><msup><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></mi><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></mn></msup><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0.3</font></font></mn><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></mi><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">n</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">15</font></font></mn><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">)</font></font></mo><mo><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0.05</font></font></mn><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">c</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(</font></font></mo><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">50</font></font></mn><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></mi><mo stretchy="false"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">)</font></font></mo><mtext>&nbsp;</mtext><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">t</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a</font></font></mi><mi><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">g</font></font></mi><mrow class="MJX-TeXAtom-ORD"><mn><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">113</font></font></mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> f (x) = 0.2 + 0.4 x ^ 2 + 0.3x \ sin (15 x) + 0.05 \ cos (50 x) \ tag {113} </script></p><br><br>  It is constructed for x values ‚Äã‚Äãfrom 0 to 1, and values ‚Äã‚Äãalong the y axis vary from 0 to 1. <br><br>  Obviously, this function is nontrivial.  And you have to figure out how to calculate it using neural networks. <br><br>  In our neural networks above, we analyzed a weighted combination ‚àë <sub>j</sub> w <sub>j</sub> a <sub>j of the</sub> output of hidden neurons.  We know how to get significant control over this value.  But, as I noted earlier, this value is not equal to the network output.  The network output is œÉ (‚àë <sub>j</sub> w <sub>j</sub> a <sub>j</sub> + b), where b is the bias of the output neuron.  Can we gain control directly over the network output? <br><br>  The solution is to develop a neural network in which the weighted output of the hidden layer is given by the equation œÉ <sup>‚àí1</sup> ‚ãÖf (x), where œÉ <sup>‚àí1</sup> is the inverse function of œÉ.  That is, we want the weighted output of the hidden layer to be like this: <br><br><img src="https://habrastorage.org/webt/sk/bu/bw/skbubwnwkrrpukeblqe9a1qo8cw.png"><br><br>  If this succeeds, then the output of the entire network will be a good approximation of f (x) (I set the offset of the output neuron to 0). <br><br>  Then your task is to develop an NS approximating the objective function shown above.  To better understand what is happening, I recommend that you solve this problem twice.  For the first time in the <a href="http://neuralnetworksanddeeplearning.com/chap4.html">original article,</a> click on the graph, and directly adjust the heights of the different protrusions.  It will be quite easy for you to get a good approximation to the objective function.  The degree of approximation is estimated by the average deviation, the difference between the objective function and the function that the network calculates.  Your task is to bring the average deviation to a minimum value.  The task is considered completed when the average deviation does not exceed 0.40. <br><br><img src="https://habrastorage.org/webt/jb/qh/j8/jbqhj8kul1dtc6o-nyrm0qxhh_0.png"><br><br>  Once successful, press the Reset button, which randomly changes the tabs.  The second time, do not touch the graph, but change the h values ‚Äã‚Äãon the left side of the diagram, trying to bring the average deviation to a value of 0.40 or less. <br><br>  And so, you have found all the elements necessary for the network to approximately calculate the function f (x)!  The approximation turned out to be rough, but we can easily improve the result by simply increasing the number of pairs of hidden neurons, which will increase the number of protrusions. <br><br>  In particular, it is easy to turn all the data found back into the standard view with parameterization used for NS.  Let me quickly remind you how this works. <br><br>  In the first layer, all weights have a large constant value, for example, w = 1000. <br><br>  The displacements of hidden neurons are calculated through b = ‚àíws.  So, for example, for the second hidden neuron, s = 0.2 turns into b = ‚àí1000 √ó 0.2 = ‚àí200. <br><br>  The last layer of the scale is determined by the values ‚Äã‚Äãof h.  So, for example, the value you choose for the first h, h = -0.2, means that the output weights of the two upper hidden neurons are -0.2 and 0.2, respectively.  And so on, for the entire output weight layer. <br><br>  Finally, the offset of the output neuron is 0. <br><br>  And that‚Äôs all: we got a complete description of the NS, which calculates the initial objective function well.  And we understand how to improve the quality of approximation by improving the number of hidden neurons. <br><br>  In addition, in our original objective function f (x) = 0.2 + 0.4x <sup>2</sup> + 0.3sin (15x) + 0.05cos (50x) there is nothing special.  A similar procedure could be used for any continuous function on the intervals from [0,1] to [0,1].  In fact, we use our single-layer NS to build a lookup table for the function.  And we can take this idea as a basis to get a generalized proof of universality. <br><br><h2>  Function of many parameters </h2><br>  We extend our results to the case of a set of input variables.  It sounds complicated, but all the ideas we need can already be understood for the case with only two incoming variables.  Therefore, we consider the case with two incoming variables. <br><br>  Let's start by looking at what will happen when a neuron has two inputs: <br><br><img src="https://habrastorage.org/webt/k5/cm/a9/k5cma9i-bgfwxnp2ao9h1wiwfz0.png"><br><br>  We have inputs x and y, with corresponding weights w <sub>1</sub> and w <sub>2</sub> and offset b of the neuron.  Set the weight of w <sub>2</sub> to 0 and play with the first one, w <sub>1</sub> , and offset b to see how they affect the output of the neuron: <br><br><img src="https://habrastorage.org/webt/bl/71/6p/bl716pdfanpkwighwk2dc6m20ey.png"><br><br>  As you can see, with w <sub>2</sub> = 0, the input y does not affect the output of the neuron.  Everything happens as if x is the only input. <br><br>  Given this, what do you think will happen when we increase the weight of w <sub>1</sub> to w <sub>1</sub> = 100 and w <sub>2</sub> leave 0?  If this is not immediately clear to you, think a little about this issue.  Then watch the following video, which shows what will happen: <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Your browser does not support HTML5 video. <source src="http://neuralnetworksanddeeplearning.com/movies/step_3d.mp4" type="video/mp4"></video></div></div></div><br><br>  As before, with an increase in the input weight, the output approaches the shape of the step.  The difference is that our step function is now located in three dimensions.  As before, we can move the location of the steps by changing the offset.  The angle will be at the point s <sub>x</sub> ‚â° ‚àí b / w1. <br><br>  Let's redo the diagram so that the parameter is the location of the step: <br><br><img src="https://habrastorage.org/webt/aw/qs/59/awqs59ahvnac-1i9piafzg2jbpi.png"><br><br>  We assume that the input weight of x is of great importance - I used w <sub>1</sub> = 1000 - and the weight of w <sub>2</sub> = 0.  The number on the neuron is the position of the step, and the x above it reminds us that we move the step along the x axis.  Naturally, it is quite possible to obtain a step function along the y axis, making the incoming weight for y large (say, w <sub>2</sub> = 1000), and the weight for x equal to 0, w <sub>1</sub> = 0: <br><br><img src="https://habrastorage.org/webt/xg/zw/y0/xgzwy0jgsj5q1gl3oqgygzqv0nq.png"><br><br>  The number on the neuron, again, indicates the position of the step, and the y above it reminds us that we move the step along the y axis.  I could directly designate the weights for x and y, but I didn‚Äôt, because that would litter the chart.  But keep in mind that the y marker indicates that the weight for y is large and for x is 0. <br><br>  We can use the step functions we have just designed to calculate the three-dimensional protrusion function.  To do this, we take two neurons, each of which will calculate a step function along the x axis.  Then we combine these step functions with weights h and ‚Äìh, where h is the desired height of the protrusion.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">All this can be seen in the following diagram: </font></font><br><br><img src="https://habrastorage.org/webt/5s/qn/wk/5sqnwkfmm7_uzs3jchov90ylyz8.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Try changing the value of h. </font><font style="vertical-align: inherit;">See how it relates to network weights. </font><font style="vertical-align: inherit;">And how she changes the height of the protrusion function on the right. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Also try to change the point of the step, the value of which is set to 0.30 in the upper hidden neuron. </font><font style="vertical-align: inherit;">See how it changes the shape of the protrusion. </font><font style="vertical-align: inherit;">What happens if we move it beyond the 0.70 point associated with the lower hidden neuron? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We learned how to build the protrusion function along the x axis. </font><font style="vertical-align: inherit;">Naturally, we can easily make the protrusion function along the y axis, using two step functions along the y axis. </font><font style="vertical-align: inherit;">Recall that we can do this by making large weights at input y and setting weight 0 at input x. </font><font style="vertical-align: inherit;">And so, what happens:</font></font><br><br><img src="https://habrastorage.org/webt/ic/uu/fq/icuufqisf9gjv8zccnkg0f0bevc.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It looks almost identical to the previous network! The only visible change is small y markers on hidden neurons. They remind us that they produce step functions for y, and not for x, so at the input y the weight is very large and at the input x it is zero, and not vice versa. As before, I decided not to show it directly, so as not to clutter up the picture. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's see what happens if we add two protrusion functions, one along the x axis, the other along the y axis, both of height h: </font></font><br><br><img src="https://habrastorage.org/webt/7f/u7/fc/7fu7fcn8xnl5r4zffk3tpunuedg.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To simplify the connection diagram with zero weight, I omitted. So far, I have left small x and y markers on hidden neurons to recall in which directions the protrusion functions are computed. Later we will refuse them, since they are implied by the incoming variable.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Try changing the parameter h. </font><font style="vertical-align: inherit;">As you can see, because of this, the output weights change, as well as the weights of both protrusion functions, x and y. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What we created is a bit like a ‚Äútower function‚Äù: </font></font><br><br><img src="https://habrastorage.org/webt/ad/le/ww/adlewwyzmc3zhrk-fm9a9yvx6zo.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If we can create such tower functions, we can use them to approximate arbitrary functions by simply adding towers of different heights in different places: </font></font><br><br><img src="https://habrastorage.org/webt/u1/lv/xv/u1lvxvdmfi4xxsqgpjabiwofr2k.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Of course, we have not yet reached the creation of an arbitrary tower function. </font><font style="vertical-align: inherit;">So far we have constructed something like a central tower of height 2h with a plateau of height h surrounding it. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But we can make a tower function. </font><font style="vertical-align: inherit;">Recall that we previously showed how neurons can be used to implement the if-then-else statement:</font></font><br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span>  &gt;= :  <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:  <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre> <br>  It was a one-input neuron.  And we need to apply a similar idea to the combined output of hidden neurons: <br><br><pre> <code class="python hljs"> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>     &gt;= :  <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:  <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre> <br>  If we choose the right threshold - for example, 3h / 2, squeezed between the height of the plateau and the height of the central tower - we can crush the plateau to zero, and leave only one tower. <br><br>  Imagine how to do this?  Try experimenting with the following network.  Now we are plotting the output of the entire network, and not just the weighted output of the hidden layer.  This means that we add the offset term to the weighted output from the hidden layer, and apply the sigmoid.  Can you find the values ‚Äã‚Äãfor h and b for which you get a tower?  If you get stuck at this point, here are two tips: (1) for the outgoing neuron to show the correct behavior in the if-then-else style, we need the incoming weights (all h or ‚Äìh) to be large;  (2) the value of b determines the scale of the if-then-else threshold. <br><br><img src="https://habrastorage.org/webt/ys/k-/1u/ysk-1uvu-jo68ikk5rqu274u7wc.png"><br><br>  With default parameters, the output is similar to a flattened version of the previous diagram, with a tower and plateau.  To get the desired behavior, you need to increase the value of h.  This will give us threshold if-then-else behavior.  Secondly, in order to set the threshold correctly, one must choose b ‚âà ‚àí3h / 2. <br><br>  Here's what it looks like for h = 10: <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Your browser does not support HTML5 video. <source src="http://neuralnetworksanddeeplearning.com/movies/tower_construction.mp4" type="video/mp4"></video></div></div></div><br>  Even for relatively modest values ‚Äã‚Äãof h, we get a nice tower function.  And, of course, we can get an arbitrarily beautiful result by increasing h further and keeping the displacement at the level b = ‚àí3h / 2. <br><br>  Let's try to glue two networks together to count two different tower functions.  To make the respective roles of the two subnets clear, I put them in separate rectangles: each of them calculates the tower function using the technique described above.  The graph on the right shows the weighted output of the second hidden layer, that is, the weighted combination of tower functions. <br><br><img src="https://habrastorage.org/webt/8-/cl/ke/8-clkebo6vphf1-0_jgoaf3exts.png"><br><br>  In particular, it can be seen that by changing the weights in the last layer, you can change the height of the output towers. <br><br>  The same idea allows you to calculate as many towers as you like.  We can make them arbitrarily thin and tall.  As a result, we guarantee that the weighted output of the second hidden layer approximates any desired function of two variables: <br><br><img src="https://habrastorage.org/webt/ig/0u/5z/ig0u5zbzifftdfq4ww4y9a4r7du.png"><br><br>  In particular, by making the weighted output of the second hidden layer approximate œÉ <sup>‚àí1</sup> ‚ãÖf well, we guarantee that the output of our network will be a good approximation of the desired function f. <br><br>  What about the functions of many variables? <br><br>  Let's try to take three variables, x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> .  Can the following network be used to calculate the tower function in four dimensions? <br><br><img src="https://habrastorage.org/webt/wg/ki/kn/wgkiknicnzeoaept0d-cemw0sd0.png"><br><br>  Here x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> denote the network input.  s <sub>1</sub> , t <sub>1,</sub> and so on - step points for neurons - that is, all the weights in the first layer are large, and the offsets are assigned so that the points of the steps are s <sub>1</sub> , t <sub>1</sub> , s <sub>2</sub> , ... The weights in the second layer alternate, + h, ‚àíh, where h is some very large number.  The output offset is ‚àí5h / 2. <br><br>  The network computes a function equal to 1 under three conditions: x <sub>1</sub> is between s <sub>1</sub> and t <sub>1</sub> ;  x <sub>2</sub> is between s <sub>2</sub> and t <sub>2</sub> ;  x <sub>3</sub> is between s <sub>3</sub> and t <sub>3</sub> .  The network is 0 in all other places.  This is such a tower, in which 1 is a small portion of the entrance space, and 0 is everything else. <br><br>  Gluing a lot of such networks, we can get as many towers as we like, and approximate an arbitrary function of three variables.  The same idea works in m dimensions.  Only the output offset (‚àím + 1/2) h is changed to properly squeeze the desired values ‚Äã‚Äãand remove the plateau. <br><br>  Well, now we know how to use NS to approximate the real function of many variables.  What about vector functions f (x <sub>1</sub> , ..., x <sub>m</sub> ) ‚àà R <sup>n</sup> ?  Of course, such a function can be considered simply as n separate real functions f1 (x <sub>1</sub> , ..., x <sub>m</sub> ), f2 (x <sub>1</sub> , ..., x <sub>m</sub> ), and so on.  And then we just glue all the networks together.  So it's easy to figure it out. <br><br><h3>  Task </h3><br><ul><li>  We saw how to use neural networks with two hidden layers to approximate an arbitrary function.  Can you prove that this is possible with one hidden layer?  Hint - try working with only two output variables, and show that: (a) it is possible to get the functions of the steps not only along the x or y axes, but also in an arbitrary direction;  (b) adding up many constructions from step (a), it is possible to approximate the function of a round rather than a rectangular tower;  ¬© using round towers, it is possible to approximate an arbitrary function.  Step ¬© will be easier to do using the material presented in this chapter a little below. </li></ul><br><h2>  Going beyond sigmoid neurons </h2><br>  We have proven that a network of sigmoid neurons can calculate any function.  Recall that in a sigmoid neuron, the inputs x <sub>1</sub> , x <sub>2</sub> , ... turn at the output into œÉ (‚àë <sub>j</sub> w <sub>j</sub> x <sub>j j</sub> + b), where w <sub>j</sub> are the weights, b is the bias, and œÉ is the sigmoid. <br><br><img src="https://habrastorage.org/webt/0h/ut/93/0hut93wneejtjvxvxiwnfwpmo40.png"><br><br>  What if we look at another type of neuron using a different activation function, s (z): <br><br><img src="https://habrastorage.org/webt/ua/0-/it/ua0-itpxz-uwkpnptxfsvszqabg.png"><br><br>  That is, we will assume that if a neuron has x <sub>1</sub> , x <sub>2</sub> , ... weights w <sub>1</sub> , w <sub>2</sub> , ... and bias b, then s (‚àë <sub>j</sub> w <sub>j</sub> x <sub>j</sub> + b) will be output. <br><br>  We can use this activation function to get stepped, just like in the case of the sigmoid.  Try (in the <a href="http://neuralnetworksanddeeplearning.com/chap4.html">original article</a> ) on the diagram to lift up the weight to, say, w = 100: <br><br><img src="https://habrastorage.org/webt/vz/-v/mu/vz-vmulc79w1g7xxom_btmnfyow.png"><br><br><img src="https://habrastorage.org/webt/nf/aq/iz/nfaqiznhl2klhebfc33iiy6htrg.png"><br><br>  As in the case of the sigmoid, because of this, the activation function is compressed, and as a result turns into a very good approximation of the step function.  Try changing the offset, and you will see that we can change the location of the step to any.  Therefore, we can use all the same tricks as before to calculate any desired function. <br><br>  What properties should s (z) have in order for this to work?  We need to assume that s (z) is well defined as z ‚Üí ‚àí‚àû and z ‚Üí ‚àû.  These limits are two values ‚Äã‚Äãaccepted by our step function.  We also need to assume that these limits are different.  If they didn‚Äôt differ, the steps would not work; there would simply be a flat schedule!  But if the activation function s (z) satisfies these properties, the neurons based on it are universally suitable for calculations. <br><br><h3>  Tasks </h3><br><ul><li>  Earlier in the book, we met a different type of neuron ‚Äî a straightened linear neuron, or a rectified linear unit, [ReLU].  Explain why such neurons do not satisfy the conditions necessary for universality.  Find evidence of versatility showing that ReLUs are universally suitable for computing. </li><li>  Suppose we consider linear neurons with an activation function s (z) = z.  Explain why linear neurons do not satisfy the conditions of universality.  Show that such neurons cannot be used for universal computing. </li></ul><br><h2>  Fix step function </h2><br>  For the time being, we assumed that our neurons produce accurate step functions.  This is a good approximation, but only an approximation.  In fact, there is a narrow gap of failure, shown in the following graph, where the functions do not behave at all like a step function: <br><br><img src="https://habrastorage.org/webt/mr/0t/ng/mr0tng4l1giob-gsuhyo_oh_vk0.png"><br><br>  In this period of failure, my explanation of universality does not work. <br><br>  Failure is not so scary.  By setting sufficiently large input weights, we can make these gaps arbitrarily small.  We can make them much smaller than on the chart, invisible to the eye.  So maybe we don‚Äôt have to worry about this problem. <br><br>  Nevertheless, I would like to have some way to solve it. <br><br>  It turns out that it‚Äôs easy to solve.  Let‚Äôs look at this solution for calculating NS functions with only one input and output.  The same ideas will work to solve the problem with a large number of inputs and outputs. <br><br>  In particular, let's say we want our network to compute some function f.  As before, we try to do this by designing the network so that the weighted output of the hidden layer of neurons is œÉ <sup>‚àí1</sup> ‚ãÖf (x): <br><br><img src="https://habrastorage.org/webt/sk/bu/bw/skbubwnwkrrpukeblqe9a1qo8cw.png"><br><br>  If we do this using the technique described above, we will make the hidden neurons give out a sequence of functions of the protrusions: <br><br><img src="https://habrastorage.org/webt/71/uc/x_/71ucx_26mzx0_isj6dlob9wdacq.png"><br><br>  Of course, I exaggerated the size of the intervals of failure, so that they were easier to see.  It should be clear that if we add up all these functions of the protrusions, we get a fairly good approximation of œÉ <sup>‚àí1</sup> ‚ãÖf (x) everywhere except for the intervals of failure. <br><br>  But, suppose that instead of using the approximation just described, we use a set of hidden neurons to calculate the approximation of half of our original objective function, i.e., œÉ <sup>‚àí1</sup> ‚ãÖf (x) / 2.  Of course, it will look just like a scaled version of the latest graph: <br><br><img src="https://habrastorage.org/webt/8-/0b/fv/8-0bfvrf5njiwum-w6d8edu4dro.png"><br><br>  And suppose we make one more set of hidden neurons calculate the approximation to œÉ <sup>‚àí1</sup> ‚ãÖf (x) / 2, however, at its base the protrusions will be shifted by half their width: <br><br><img src="https://habrastorage.org/webt/dj/cz/7n/djcz7nxhhm98yhiq94uluglvm-m.png"><br><br>  Now we have two different approximations for œÉ ‚àí 1‚ãÖf (x) / 2.  If we add up these two approximations, we obtain a general approximation to œÉ ‚àí 1‚ãÖf (x).  This general approximation will still have inaccuracies in small intervals.  But the problem will be less than before - because the points falling into the intervals of the failure of the first approximation will not fall into the intervals of the failure of the second approximation.  Therefore, the approximation in these intervals will be approximately 2 times better. <br><br>  We can improve the situation by adding a large number, M, of overlapping approximations of the function œÉ ‚àí 1‚ãÖf (x) / M.  If all of their failure intervals are narrow enough, any current will be in only one of them.  If you use a sufficiently large number of overlapping approximations of M, the result will be an excellent general approximation. <br><br><h2>  Conclusion </h2><br>  The explanation of universality discussed here definitely cannot be called a practical description of how to count functions using neural networks!  In this sense, it is more like proof of the versatility of NAND logic gates and more.  Therefore, I basically tried to make this design clear and easy to follow without optimizing its details.  However, trying to optimize this design can be an interesting and instructive exercise for you. <br><br>  Although the result obtained cannot be directly used to create NS, it is important because it removes the question of the computability of any particular function using NS.  The answer to such a question will always be positive.  Therefore, it is correct to ask if any function is computable, but what is the correct way to calculate it. <br><br>  Our universal design uses only two hidden layers to calculate an arbitrary function.  As we discussed, it is possible to get the same result with a single hidden layer.  Given this, you may wonder why we need deep networks, that is, networks with a large number of hidden layers.  Can't we just replace these networks with shallow ones that have one hidden layer? <br><br>  Although, in principle, it is possible, there are good practical reasons for using deep neural networks.  As described in Chapter 1, deep NSs have a hierarchical structure that allows them to adapt well to study hierarchical knowledge, which are useful for solving real problems.  More specifically, when solving problems such as pattern recognition, it is useful to use a system that understands not only individual pixels, but also increasingly complex concepts: from borders to simple geometric shapes, and beyond, to complex scenes involving several objects.  In later chapters we will see evidence in favor of the fact that deep NSs will be better able to cope with the study of such hierarchies of knowledge than shallow ones.  To summarize: universality tells us that NS can calculate any function;  empirical evidence suggests that deep NSs are better adapted to the study of functions useful for solving many real-world problems. </div><p>Source: <a href="https://habr.com/ru/post/461659/">https://habr.com/ru/post/461659/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../461649/index.html">How will I save the world</a></li>
<li><a href="../46165/index.html">‚ÄúAttention Profile‚Äù and ‚ÄúFavorite Authors‚Äù (Part 2)</a></li>
<li><a href="../461653/index.html">Software Defined Radio - how does it work? Part 10</a></li>
<li><a href="../461655/index.html">The digest of fresh materials from the world of the front-end for the last week No. 373 (July 22 - 28, 2019)</a></li>
<li><a href="../461657/index.html">Buying Red Hat: Will It Help The Blue Giant Fight For Hybrid Cloud Leadership</a></li>
<li><a href="../461661/index.html">Component Based Development Guide</a></li>
<li><a href="../461663/index.html">The story of how Linux brought in Windows</a></li>
<li><a href="../461665/index.html">Zen2. The evolution of the AM4 platform on the example of Ryzen 7 3700x</a></li>
<li><a href="../461669/index.html">PHP Digest No. 161 (July 15 - 29, 2019)</a></li>
<li><a href="../461673/index.html">8 tips for novice programmers or a retrospective of my career</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>