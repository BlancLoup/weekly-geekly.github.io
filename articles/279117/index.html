<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Linear models: simple regression</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Last time we examined in detail the diversity of linear models . Now let's move from theory to practice and build the simplest, but still useful model...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Linear models: simple regression</h1><div class="post__text post__text-html js-mediator-article">  Last time we examined in detail the <a href="https://habrahabr.ru/post/278513/">diversity of linear models</a> .  Now let's move from theory to practice and build the simplest, but still useful model that you can easily adapt to your tasks.  The model will be illustrated with code in R and Python, and in three flavors at once: scikit-learn, statsmodels and Patsy. <a name="habracut"></a><br><br><h4>  <b>Simple linear regression</b> </h4><br>  Let us be given the initial data in the form of a table with columns <i>x1</i> , <i>x2</i> , <i>x3</i> and <i>y</i> .  And we will build a linear model of dependence of <i>y</i> on the factors <i>x</i> , that is, the model will have the following form: <br>  <i>y = b <sub>0</sub> + b <sub>1</sub> x <sub>1</sub> + b <sub>2</sub> x <sub>2</sub> + b <sub>3</sub> x <sub>3</sub> +</i> <br>  where <i>x</i> , <i>y</i> is the initial data, <i>b</i> is the model parameters, <i>‚Ñá</i> is a random variable. <br>  Since we already have <i>x</i> and <i>y</i> , our task is to calculate the parameters <i>b</i> . <br>  Please note that we have introduced the parameter <i>b <sub>0</sub></i> , which is also called <i>intercept</i> , since our model line will not necessarily pass through the origin.  If the source data is centered, this parameter is not required. <br>  To construct a model, we also need to introduce a parametric assumption regarding <i>‚Ñá</i> ‚Äî without this, we cannot choose a solution method.  What horrible method we have no right to use, because we risk getting any erroneous result, while we need a ‚Äúreasonably good‚Äù estimation of model parameters.  So for simplicity and convenience, we assume that the errors are distributed normally, so we can use the ordinary least squares method. <br><div class="spoiler">  <b class="spoiler_title">R code</b> <div class="spoiler_text"><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#    df &lt;- read.csv("http://roman-kh.github.io/files/linear-models/simple1.csv") #    #  glm    intercept,         g &lt;- glm("y ~ x1 + x2 + x3", data=df) #    coef(g)</span></span></code> </pre> <br></div></div><br><div class="spoiler">  <b class="spoiler_title">Python code: common part</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> statsmodels.api <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> patsy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sklearn.linear_model <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> lm <span class="hljs-comment"><span class="hljs-comment">#     df = pd.DataFrame.from_csv("http://roman-kh.github.io/files/linear-models/simple1.csv") # x -      (x1, x2, x3) x = df.iloc[:,:-1] # y -       y = df.iloc[:,-1]</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Python code: scikit-learn</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    skm = lm.LinearRegression() #       skm.fit(x, y) #      print skm.intercept_, skm.coef_</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Python code: statsmodels</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#      intercept' x_ = sm.add_constant(x) #        (Ordinary Least Squares) smm = sm.OLS(y, x_) #    res = smm.fit() #      print res.params</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Python code: statsmodels with formulas</b> <div class="spoiler_text">  Moving from R to Python, many start with statsmodels, because it has the usual R'ov formulas: <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#      smm = sm.OLS.from_formula("y ~ x1 + x2 + x3", data=df) #    res = smm.fit() #      print res.params</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Python code: Patsy + numpy</b> <div class="spoiler_text">  Thanks to the Patsy library, you can easily use R-like formulas in any of your programs: <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#           pt_y, pt_x = pt.dmatrices("y ~ x1 + x2 + x3", df) #   numpy'     # ,     LinearRegression  scikit-learn       #     scipy.sparse.linalg.lsqr res = np.linalg.lstsq(pt_x, pt_y) #    b = res[0].ravel() print b</span></span></code> </pre><br></div></div><br>  Please note that all methods for calculating coefficients give the same result.  And this is absolutely no coincidence.  Due to the normality of errors, the optimized functional (ONC, OLS) turns out to be convex, and therefore has a single minimum, and the desired set of model coefficients will also be unique. <br>  However, even a slight change in the source data, despite the preservation of the form of distributions, will lead to a change in the coefficients.  Simply put, although the real life does not change, and the nature of the data also remains unchanged, as a result of the invariable method of calculation you will receive different models, and in fact the model should be the only one.  We will talk more about this in the regularization article. <br><br><h4>  <b>We study the data</b> </h4><br>  So, we have already built a model ... However, most likely we made a mistake, perhaps even more than one.  Since we did not study the data before tackling the model, we received a model of what was not clear.  Therefore, let's still look at the data and see what is there. <br>  As you remember, there were 4 columns in the source table: <i>x1</i> , <i>x2</i> , <i>x3,</i> and <i>y</i> .  And we built a regression dependence of <i>y</i> on all <i>x</i> .  Since we cannot immediately cover the entire 4-dimensional hypercube with one glance, we will look at the individual <i>xy</i> plots. <br><img src="https://habrastorage.org/files/315/0b0/cd4/3150b0cd4bb0442b80450abc7d621376.png"><br>  Immediately catches the eye <i>x3</i> - a binary sign - and with them it is worth being closer.  In particular, it makes sense to look at the joint distribution of the remaining <i>x</i> together with <i>x3</i> . <br><img src="https://habrastorage.org/files/1d8/5c0/395/1d85c0395f054184ac0ecc97be4c8afb.png"><br>  Now it's time to display the graph corresponding to the constructed model.  And here it is worth noting that our linear model is not a line at all, but a hyperplane, so on a two-dimensional graph we will be able to display only a single slice of this model hyperplane.  Simply put, to show the model's graph in <i>x1</i> - <i>y</i> coordinates, you need to fix the values ‚Äã‚Äãof <i>x2</i> and <i>x3</i> , and by changing them, we get an infinite number of graphs - and they will all be graphs of the model (or rather, projection onto the <i>x1</i> - <i>y plane</i> ). <br>  Since <i>x3</i> is a binary feature, you can show a separate line for each attribute value, and fix <i>x1</i> and <i>x2</i> at the arithmetic average level, that is, draw the graphs y = f (x1 | x2 = E (x2)) and y = f (x2 | x1 = E (x1)), where E is the arithmetic average over all values ‚Äã‚Äãof x1 and x2, respectively. <br><img src="https://habrastorage.org/files/b03/56e/466/b0356e4665774ecd8412146f487a7b7f.png"><br>  And it immediately catches the eye that the graph <i>y ~ x2</i> looks somehow wrong.  Remembering that the constructed model should predict the expectation <i>y</i> , here we see that the model line does not match the expectation at all: for example, at the beginning of the graph, blue points of real values ‚Äã‚Äãare below the blue line, and at the end - above, and the red line has on the contrary, although both lines should pass approximately in the middle of a cloud of points. <br>  Looking closer, you can guess that the blue and red lines should even be non-parallel.  How to do this in a linear model?  Obviously, by building a linear model <i>with y = f (x1, x2, x3)</i> we can get an infinite number of lines of the form <i>y = f (x2 | x1, x3)</i> , that is, fixing two of the three variables.  So, in particular, the red line <i>y = f (x2 | x1 = E (x), x3 = 0)</i> and the blue <i>y = f (x2 | x1 = E (x), x3 = 1) are</i> obtained on the right graph.  However, all such lines will be parallel. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  <b>Non-parallel linear model</b> </h4><br>  To make the model non-parallel, let‚Äôs make it a bit more complicated by adding just one term: <br>  <i>y = b <sub>0</sub> + b <sub>1</sub> x <sub>1</sub> + b <sub>2</sub> x <sub>2</sub> + b <sub>3</sub> x <sub>3</sub> + <b>b <sub>4</sub> x <sub>2</sub> x <sub>3</sub></b> +</i> <br>  Where it leads?  Previously, any line on the graph <i>y ~ x <sub>2</sub></i> had the same slope, given by the coefficient <i>b <sub>2</sub></i> .  Now, depending on the value of <i>x <sub>3, the</sub></i> line will have a slope <i>b <sub>2</sub></i> (for <i>x <sub>3</sub> = 0</i> ) or <i>b <sub>2</sub> + b <sub>4</sub></i> (for <i>x <sub>3</sub> = 1</i> ). <br><div class="spoiler">  <b class="spoiler_title">R code</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#    df &lt;- read.csv("http://roman-kh.github.io/files/linear-models/simple1.csv") #    #  glm    intercept,         g &lt;- glm("y ~ x1 + x2 + x3 + x2*x3", data=df) #    coef(g)</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Python code: common part</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> statsmodels.api <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> patsy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sklearn.linear_model <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> lm <span class="hljs-comment"><span class="hljs-comment">#     df = pd.DataFrame.from_csv("http://roman-kh.github.io/files/linear-models/simple1.csv") # x -      (x1, x2, x3) x = df.iloc[:,:-1] # y -       y = df.iloc[:,-1]</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Python code: scikit-learn</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    x["x4"] = x["x2"] * x["x3"] #    skm = lm.LinearRegression() #       skm.fit(x, y) #      print skm.intercept_, skm.coef_</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Python code: statsmodels</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    x["x4"] = x["x2"] * x["x3"] #      intercept' x_ = sm.add_constant(x) #        (Ordinary Least Squares) smm = sm.OLS(y, x_) #    res = smm.fit() #      print res.params</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Python code: statsmodels with formulas</b> <div class="spoiler_text">  Moving from R to Python, many start with statsmodels, because it has the usual R'ov formulas: <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#      smm = sm.OLS.from_formula("y ~ x1 + x2 + x3 + x2*x3", data=df) #    res = smm.fit() #      print res.params</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Python code: Patsy + numpy</b> <div class="spoiler_text">  Thanks to the Patsy library, you can easily use R-like formulas in any of your programs: <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#           pt_y, pt_x = pt.dmatrices("y ~ x1 + x2 + x3 + x2*x3", df) #   numpy'     # ,     LinearRegression  scikit-learn       #     scipy.sparse.linalg.lsqr res = np.linalg.lstsq(pt_x, pt_y) #    b = res[0].ravel() print b</span></span></code> </pre><br></div></div><br>  And now take a look at the updated schedule. <br><img src="https://habrastorage.org/files/18d/192/365/18d1923656b7485685faf6a114793b02.png"><br>  Much better! <br><br>  This technique - the multiplication of variables - is extremely useful for binary and categorical factors and allows you to actually build several models at once within the same model that reflect the characteristics of different groups of objects studied (men and women, ordinary employees and managers, lovers of classical, rock or pop music ).  Particularly interesting models can be obtained when there are several binary and categorical factors in the source data. <br><br>  At the request of those who wish, I also created a small <a href="https://github.com/roman-kh/roman-kh.github.com/blob/master/files/linear-models/slm.ipynb">ipython notebook</a> . <br><br>  To summarize: we have built a simple, but still very adequate model, which, judging by the graphs, reflects the real data quite well.  However, all these ‚Äúvery‚Äù and ‚Äúnot bad‚Äù are better presented in measurable terms.  It also remains unclear how much the built model is resistant to small changes in the initial data or to the structure of these data (in particular, to the interdependence between factors).  We will definitely return to these questions. </div><p>Source: <a href="https://habr.com/ru/post/279117/">https://habr.com/ru/post/279117/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../279105/index.html">Containers: The quest for the "magic framework" and why it became Kubernetes</a></li>
<li><a href="../279107/index.html">1.3 SFML and Linux</a></li>
<li><a href="../279109/index.html">KTV. Rake on the way to marshaling</a></li>
<li><a href="../279111/index.html">C ++ exception handling under the hood or how exceptions work in C ++</a></li>
<li><a href="../279113/index.html">Observation of internationalized domain names and the letter K</a></li>
<li><a href="../279123/index.html">Superscalar stack processor: we continue to cross the grass and the hedgehog</a></li>
<li><a href="../279125/index.html">Dagger 2. Part One. Basics, creating a dependency graph, Scopes</a></li>
<li><a href="../279127/index.html">Nutanix Big Update: NutanixOS 4.6</a></li>
<li><a href="../279129/index.html">On the issue of errors</a></li>
<li><a href="../279133/index.html">The digest of interesting materials for the mobile developer # 144 (on March 9-13)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>