<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Seals vs neural network 2. Or run SqueezeNet v.1.1 on Raspberry Zero in realtime (almost)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! 

 After writing it was not quite serious and not particularly useful in the practical way of the first part, I was slightly bogged down with c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Seals vs neural network 2. Or run SqueezeNet v.1.1 on Raspberry Zero in realtime (almost)</h1><div class="post__text post__text-html js-mediator-article">  Hello! <br><br>  After writing it was not quite serious and not particularly useful in the practical way of the <a href="https://habr.com/post/428021/">first part,</a> I was slightly bogged down with conscience.  And I decided to finish the job.  That is, choose the same implementation of the neural network to run on Rasperry Pi Zero W in real time (of course, as far as possible on such hardware).  Run it on the data from real life and highlight the results obtained on Habr√©. <br><br>  Caution!  There is a workable code under the cut and a little more seal than in the first part.  The picture shows the ko and ko respectively. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <div style="text-align:center;"><img src="https://habrastorage.org/webt/_n/hp/-p/_nhp-pj5btxq5w0yvmwdnd-kguc.jpeg" alt="image"></div><a name="habracut"></a><br><h2>  Which network to choose? </h2><br>  Let me remind you that due to the weakness of the malinki iron, the choice of implementations of the neural network is small.  Namely: <br><br>  1. SqueezeNet. <br>  2. YOLOv3 Tiny. <br>  3. MobileNet. <br>  4. ShuffleNet. <br><br>  How correct was the choice in favor of SqueezeNet in the <a href="https://habr.com/post/428021/">first part</a> ? .. To run each of the above mentioned neural networks on its hardware is a rather long event.  Therefore, tormented by vague doubts, I decided to google, if someone had asked myself a similar question.  It turned out that he asked and investigated it in some detail.  Those interested can refer to the <a href="https://deepdetect.com/overview/performance/">source</a> .  I will confine myself to the only picture of it: <br><br><img src="https://habrastorage.org/webt/gh/qq/_r/ghqq_rgrl9wjl13hsf7mtrbsyj4.png" alt="image"><br><br>  It follows from the picture that the processing time of one image for different models trained on ImageNet is the least time for SqueezeNet v.1.1.  We take this as a guide to action.  The comparison was not included YOLOv3, but, as I recall, YOLO is more expensive than MobileNet.  Those.  In speed, it should also give in to SqueezeNet. <br><br><h2>  Implementing the selected network </h2><br>  Weights and the SqueezeNet topology, trained on the ImageNet dataset (the Caffe framework), can be found on <a href="https://github.com/DeepScale/SqueezeNet">GitHub</a> .  Just in case I downloaded both versions so that they could be compared later.  Why ImageNet?  This set of all available has the maximum number of classes (1000 pieces), so the results of the neural network promise to be quite interesting. <br><br>  At this time, let's see how Raspberry Zero copes with the recognition of frames from the camera.  Here it is, our humble worker of today's post: <br><br><img src="https://habrastorage.org/webt/uu/vw/4a/uuvw4axgowtwqs1l-7q6wq94ej4.jpeg" alt="image"><br><br>  I took the source code from Adrian Rosebrock‚Äôs blog, mentioned in the <a href="https://habr.com/post/428021/">first part</a> , namely <a href="https://www.pyimagesearch.com/2017/10/16/raspberry-pi-deep-learning-object-detection-with-opencv/">from here</a> .  But I had to significantly plow it up: <br><br>  1. Replace the model in use with MobileNetSSD with SqueezeNet. <br>  2. Execution of item 1 led to an increase in the number of classes to 1000. But, at the same time, the function of selecting objects with multi-colored frames (SSD functionality) had, alas, to be removed. <br>  3. Remove the argument reception via the command line (for some reason, this puts me at the input of parameters). <br>  4. Remove the VideoStream method, and with it the imutils library, beloved by Adrian.  The original method was used to get the video stream from the camera.  But with the camera connected to Raspberry Zero, he stupidly did not work, giving out something like ‚ÄúIllegal instruction‚Äù. <br>  5. Add the frame rate (FPS) to the recognized picture, rewrite the FPS calculation. <br>  6. Make saving frames to write this post. <br><br>  On Malinka with Rapbian Stretch OS, Python 3.5.3 and installed via pip3 install OpenCV 3.4.1, the following happened and started: <br><br><div class="spoiler">  <b class="spoiler_title">Code here</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> picamera <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> picamera.array <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PiRGBArray <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sleep <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> datetime <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> dt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-comment"><span class="hljs-comment">#    prototxt = 'models/squeezenet_v1.1.prototxt' model = 'models/squeezenet_v1.1.caffemodel' labels = 'models/synset_words.txt' #    rows = open(labels).read().strip().split("\n") classes = [r[r.find(" ") + 1:].split(",")[0] for r in rows] #    print("[INFO] loading model...") net = cv2.dnn.readNetFromCaffe(prototxt, model) print("[INFO] starting video stream...") #   camera = picamera.PiCamera() camera.resolution = (640, 480) camera.framerate = 25 #   camera.start_preview() sleep(1) camera.stop_preview() #     raw rawCapture = PiRGBArray(camera) #   FPS t0 = time.time() #     for frame in camera.capture_continuous(rawCapture, format="bgr", use_video_port=True): #    blob frame = rawCapture.array blob = cv2.dnn.blobFromImage(frame, 1, (224, 224), (104, 117, 124)) #    blob,     net.setInput(blob) preds = net.forward() preds = preds.reshape((1, len(classes))) idxs = int(np.argsort(preds[0])[::-1][:1]) #  FPS FPS = 1/(time.time() - t0) t0 = time.time() #    ,   FPS,    text = "Label: {}, p = {:.2f}%, fps = {:.2f}".format(classes[idxs], preds[0][idxs] * 100, FPS) cv2.putText(frame, text, (5, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2) print(text) cv2.imshow("Frame", frame) #     Raspberry fname = 'pic_' + dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S') + '.jpg' cv2.imwrite(fname, frame) #    SD  key = cv2.waitKey(1) &amp; 0xFF #    `q`    if key == ord("q"): break #   raw       rawCapture.truncate(0) print("[INFO] video stream is terminated") #    cv2.destroyAllWindows() camera.close()</span></span></code> </pre> <br></div></div><br><h2>  results </h2><br>  The code displays on the screen of the monitor connected to the Raspberry, the next recognized frame in this form.  At the top of the frame, only the most likely class is displayed. <br><br><img src="https://habrastorage.org/webt/io/65/7s/io657sevsvfzlgnq803buqvv-mo.jpeg" alt="image"><br><br>  So, a computer mouse was defined as a mouse with a very high probability.  At the same time, images are updated with a frequency of 0.34 FPS (i.e., approximately once every three seconds).  It is a bit annoying to hold the camera and wait for the next frame to be processed, but you can live.  By the way, if you remove the save frame on the SD card, the processing speed will increase to 0.37 ... 0.38 FPS.  Surely, there are other ways to disperse.  We will live and see, in any case, leave this question for the next posts. <br><br>  Separately, I apologize for the white balance.  The fact is that the IR camera with the backlight turned on was connected to Rapberry, so most of the frames look rather strange.  But the more valuable every hit neural network.  Obviously, the white balance on the training set was more correct.  In addition, I decided to insert just raw frames so that the reader could see them in the same way as he sees a neural network. <br><br>  To begin, let's compare the work of SqueezeNet versions 1.0 (on the left frame) and 1.1 (on the right): <br><br><img src="https://habrastorage.org/webt/zc/rs/ao/zcrsaocvvwb7nmdhi3zg6v-fuwe.jpeg" alt="image"><br><br>  It is seen that version 1.1 runs at two and a quarter times faster than 1.0 (0.34 FPS vs. 0.15).  The gain in speed is palpable.  Findings on the accuracy of recognition in this example are not worth doing, since the accuracy strongly depends on the position of the camera relative to the object, lighting, glare, shadows, etc. <br><br>  Due to the significant speed advantage of v1.1 over v.1.0, only SqueezeNet v.1.1 was used in the future.  To evaluate the work of the model, I directed the camera at various items that <s>came to hand</s> and received the following shots at the output: <br><br><img src="https://habrastorage.org/webt/lo/nq/6v/lonq6vrvcdqtyld8eowva01yn6a.jpeg" alt="image"><br><br>  The keyboard is defined worse than the mouse.  Perhaps in the training set, most of the keyboards were white. <br><br><img src="https://habrastorage.org/webt/ds/2j/et/ds2jete8bvrzzvq1wtlvhvxad_a.jpeg" alt="image"><br><br>  Cell phone is determined pretty well, if you turn on the screen.  Cellular with the screen off the neural network for the cell does not count. <br><br><img src="https://habrastorage.org/webt/ke/zy/hs/kezyhs6o4dutrhqyjys0b7axyf0.jpeg" alt="image"><br><br>  An empty cup is quite well defined as a coffee cup.  So far so good. <br><br><img src="https://habrastorage.org/webt/co/xk/fi/coxkfidvlt1vgcec0rgvcop4yq4.jpeg" alt="image"><br><br>  With scissors, the situation is worse, they are stubbornly determined by the network as a hairpin.  However, the hit if not in the apple, then at least in the apple tree) <br><br><h2>  Complicate the task </h2><br>  Let's try to put the neural network <s>pig</s> something tricky.  I just got a homemade children's toy.  I believe that most readers recognize her as a toy cat.  Interestingly, what will our rudimentary artificial intelligence find it? <br><br><img src="https://habrastorage.org/webt/ye/vr/1b/yevr1bf2unalpdxfuta9txi8g3y.jpeg" alt="image"><br><br>  On the frame on the left, the IR illumination erased all the strips from the fabric.  As a result, the toy was defined as an oxygen mask with a fairly decent probability.  Why not?  The shape of the toy really resembles an oxygen mask. <br><br>  On the frame on the right, I covered the IR illuminator with my fingers, so the stripes appeared on the toy, and the white balance became more believable.  Actually, this is the only frame in this post that looks more or less normal.  But the neural network such an abundance of details on the image is confusing.  She identified the toy as a sweatshirt (sweatshirt).  I must say that this, too, does not look like a "finger to the sky."  Hitting if not in the "apple tree", then at least in the apple orchard). <br><br>  Well, we smoothly approached the culmination of our action.  The undisputed winner of the fight, thoroughly consecrated in the <a href="https://habr.com/post/428021/">first post,</a> enters the ring.  And it easily takes out the brain of our neural network from the very first frames. <br><br><img src="https://habrastorage.org/webt/8l/po/hz/8lpohzb3ijzpmvljyczip7w8uzq.jpeg" alt="image"><br><br>  It is curious that the cat practically does not change the position, but each time is determined differently.  And in this perspective, it is most similar to a skunk.  In second place is the similarity with the hamster.  Let's try to change the angle. <br><br><img src="https://habrastorage.org/webt/u_/hj/jc/u_hjjc5yj3fdnnm4msadfl-pzys.jpeg" alt="image"><br><br>  Yeah, if you take a picture of the cat from above, it is determined correctly, but you only have to change the position of the cat's body in the frame a little, for a neural network it becomes a dog - Siberian Husky and Malamute (Eskimo sled dog), respectively. <br><br><img src="https://habrastorage.org/webt/cd/up/oy/cdupoyl2mai8a6ctxorqtdw12mi.jpeg" alt="image"><br><br>  And this selection is beautiful because a cat of a different breed is defined on each separate frame.  And the breed does not repeat) <br><br><img src="https://habrastorage.org/webt/nx/nj/c4/nxnjc45ke4ieyvyreptl2i0nyzw.jpeg" alt="image"><br><br>  By the way, there are poses in which the neural network becomes obvious that this is still a cat, not a dog.  That is, SqueezeNet v.1.1 still managed to show itself even on such a complex object for analysis.  Given the success of the neural network in the recognition of objects at the beginning of the test and the recognition of the cat as a cat at the end this time we declare a solid combat draw) <br><br>  Well, that's all.  I suggest everyone to try the proposed code on his raspberry and any animated and inanimate objects that came into view.  I will be especially grateful to those who measure FPS on Rapberry Pi B +.  I promise to include the results in this post with reference to the person who sent the data.  I believe that it should turn out significantly more than 1 FPS! <br><br>  I hope that the information from this post will be useful to someone for entertainment or educational purposes, and someone, maybe even, will push on new ideas. <br><br>  All successful work week!  And to new meetings) <br><br><img src="https://habrastorage.org/webt/a9/-_/gx/a9-_gxtb2qzdzz5nbtbbet66wlg.jpeg" alt="image"><br><br>  UPD1: On the Raspberry Pi 3B +, the above script runs at a frequency of 2 with a small FPS. <br><br>  UPD2: On RPi 3B + with Movidius NCS, the script runs at 6 FPS. </div><p>Source: <a href="https://habr.com/ru/post/429400/">https://habr.com/ru/post/429400/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../429388/index.html">How to make a redesign of the site and not to make problems: 4 important steps</a></li>
<li><a href="../429390/index.html">MIT course "Computer Systems Security". Lecture 16: "Attacks through the side channel", part 1</a></li>
<li><a href="../429392/index.html">MIT course "Computer Systems Security". Lecture 16: "Attacks through the side channel", part 2</a></li>
<li><a href="../429394/index.html">MIT course "Computer Systems Security". Lecture 16: "Attacks through the side channel", part 3</a></li>
<li><a href="../429396/index.html">How to test an application when interacting with an API using SoapUI</a></li>
<li><a href="../429402/index.html">ML.NET 0.7 (Machine Learning .NET)</a></li>
<li><a href="../429404/index.html">8 s ¬Ω ways to prioritize functionality</a></li>
<li><a href="../429406/index.html">"Monsters in games or how to create fear"</a></li>
<li><a href="../429410/index.html">Port 22 SSH port or not</a></li>
<li><a href="../429414/index.html">Future VR Video - Google's VR180</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>