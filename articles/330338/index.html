<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Energy Limit: New Data Center Cooling Technologies</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ken Brill from the data center certification center Uptime Institute once said that the process of increasing the heat generated by installing more an...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Energy Limit: New Data Center Cooling Technologies</h1><div class="post__text post__text-html js-mediator-article">  Ken Brill from the data center certification center Uptime Institute once <a href="https://www.usenix.org/legacy/event/lisa07/tech/brill_talk.pdf">said</a> that the process of increasing the heat generated by installing more and more transistors on the chip will reach the limit where the economic feasibility of cooling the data center will be lost without the introduction of new technologies. <br><br>  And the industry is gradually moving in this direction, engaging in the prioritization of energy efficiency in the overall process of servicing data centers.  For this reason, classic server rooms with traditional cooling systems are becoming an increasingly rare choice for companies, which is explained by economic considerations: <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp%3Farnumber%3D7279063">the</a> IEEE <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp%3Farnumber%3D7279063">report</a> of the first quarter of 2016 describes the distribution of energy consumption between the various components of data centers - cooling systems account for 50% of the energy consumed . <br><br>  In this regard, companies began to look for new methods and solutions for cooling the data center, which we want to talk about in this material. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     <a href="https://habrahabr.ru/company/it-grad/blog/330338/"><img src="https://habrastorage.org/web/c38/37c/c50/c3837cc500db44fc9a23439bfb49d40c.jpg"></a> <a name="habracut"></a>  <i>/ photo by <a href="https://www.flickr.com/photos/robfahey/338833525/">Rob Fahey</a> <a href="https://creativecommons.org/licenses/by-sa/2.0/">CC</a></i> <br><br>  Mark Mills‚Äôs work ‚ÄúClouds start with coal‚Äù <a href="https://www.tech-pundit.com/wp-content/uploads/2013/07/Cloud_Begins_With_Coal.pdf%3Fc761ac">makes the</a> following conclusion: based on the average estimate, the information and communication technology (ICT) ecosystem consumes 1,500 TWh of electricity annually, which is 10% of its global production. <br><br>  According <a href="https://people.eecs.berkeley.edu/~istoica/classes/cs294/09/CERN_Whitepaper_r04.pdf">to</a> Intel, since 2002, the cost of electricity increases annually by about 5.5%.  Organizations spend about $ 0.50 on electricity and cooling from each dollar, aimed at servicing the hardware. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/65f/f26/35e/65ff2635e4964804b44297d70fcd3397.jpg" width="500"></div>  <i>Image: <a href="https://journal.uptimeinstitute.com/a-look-at-data-center-cooling-technologies/">Uptime Institute</a></i> <br><br>  To minimize the likelihood of an increase in the temperature of data centers, enterprises often use corridor mechanisms at the racks of data centers with air ducts for hot and cold flows.  In this case, the systems of organization, management and containment of air flow are used, which capture hot ‚Äúspent‚Äù air, pass it through air-conditioning devices and serve cooled directly to the air intakes of the server equipment. <br><br>  Some systems use fluids to absorb and transfer heat from the hardware.  Fluids generally perform this function more efficiently than air.  Systems such as cooling by fluid contact with a radiator and immersion cooling (indirect and direct) are common, in which individual components are immersed in a non-conductive fluid. <br><br>  Although water cooling in its traditional form is a more economical method, today it still remains secondary, facing a number of obstacles to mass adoption.  The following factors slow down the spread: <br><br><ul><li>  Physical limitations.  Unlike racks, access to the capacity for diving servers opens only from above, which complicates infrastructure scaling. </li><li>  Security.  Any maintenance requires contact with a special fluid. </li><li>  The cost of deploying the system.  Designing a new data center based on water cooling is less laborious than upgrading the existing infrastructure. </li><li>  Resource shortage  The maintenance of a large data center requires such an amount of water that the feasibility of cooling servers in dry areas is questioned from both an economic and an environmental point of view. </li></ul><br><h2>  Alternative solutions for cooling data centers </h2><br>  The rise in energy costs and certain drawbacks of classical cooling methods prompted the data center industry to look for innovative solutions, including those associated with "natural" cooling. <br><br><h4>  Data Center Migration </h4><br>  There are a number of regions on the planet whose climate naturally allows saving up to 100% of the energy expended on servicing cooling systems.  Such climatic zones, for example, are the northern part of Europe, Russia and several zones in the north of the USA. <br><br>  In confirmation of this, in 2013, Facebook <a href="http://bigpicture.ru/%3Fp%3D404950">built</a> its first data center outside the United States in the Swedish city of Lule√• with an average annual temperature of 1.3 ¬∞ C, and Google <a href="http://www.datacenterknowledge.com/archives/2013/11/04/google-data-center-investment-in-finland-tops-1-billion-usd/">invested a</a> billion dollars in building a large data center in Finland. <br><br>  The cold climate reduces operating costs due to the low temperatures of air and water that are launched into data centers.  According <a href="https://www.theregister.co.uk/2016/05/12/power_in_a_cold_climate/">to</a> Andrew Donohue of research company 451 Research, natural air cooling allows builders to abandon mechanical chillers, reducing the capital cost of the facility to 40%. <br><br><h4>  Natural forms of cooling </h4><br>  As a logical continuation of the previous solution, natural cooling should be considered.  There are three forms of its implementation: air, adiabatic and water. <br><br>  The air form of natural cooling differs from classic air conditioning in that hot air next to the servers is sent to the environment (fully or partially), and it is replaced by cooled air from the outside. <br><br>  Back in 2008, Intel <a href="http://www.intel.com/content/dam/doc/technology-brief/data-center-efficiency-xeon-reducing-data-center-cost-with-air-economizer-brief.pdf">conducted a</a> ten-month test to evaluate the effectiveness of using only external air to cool the data center.  As a result, it was not possible to fix the increase in the server failure rate due to fluctuations in temperature and humidity.  The range of temperature changes in the machine room was 30 degrees, while using only standard household air filter, which removes only large particles from the incoming air. <br><br>  As a result, the humidity in the data center ranged from 4 to 90%, and the servers were covered with a thin layer of dust.  Despite this, the failure rate in the test area was 4.46%, which did not differ much from the 3.83% achieved in the main Intel data center over the same period. <br><br>  The second form is adiabatic cooling, which is often called evaporative.  To cool the data center, a low-power fan delivers air to the irrigated surfaces, evaporating some of the water.  The resulting air is much colder than that passed by the fan. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/d49/363/e86/d49363e866a5417fb70363285d756e60.jpg" width="500"></div>  <i>Image: <a href="https://submer.com/data-center-liquid-immersion-cooling-with-adiabatic-cooling-towers/">Submer</a></i> <br><br>  Another <a href="http://www.datacenterknowledge.com/archives/2010/09/24/montana-deploys-kyoto-cooling-wheel/">solution</a> for adiabatic cooling is the so-called heat wheel.  This wheel <a href="http://telecombloger.ru/6793">is</a> part of the Kyoto cooling system used in Switzerland.  The 9 MW hybrid plant uses the cooled water of a closed adiabatic dry fan connected to a water-cooling device with compressors. <br><br>  The wheel is a large, slowly rotating aluminum disc, which has come to the data center industry from industrial air conditioning.  Instead of introducing outside air directly into the server air, it mixes outside air with exhaust air, forming heat exchange.  The wheel makes from 3 to 10 revolutions per minute and requires a minimum of energy to turn. <br><br>  <a href="http://searchdatacenter.techtarget.com/tip/Free-data-center-cooling-Methods-and-challenges">It is argued</a> that this type of cooling requires only 8‚Äì25% of the power consumed during mechanical cooling.  According to an independent <a href="http://telecombloger.ru/6793">report</a> , a wheel-based system installed in Montana, USA, reduces the cost of cooling costs to 5 cents per dollar spent on energy. <br><br>  As for the water form of natural cooling, in its real performance it illustrates the principle of operation of the Green Mountain data center, which is located in one of the fjords in Norway.  The reservoir provides uninterrupted water supply at a temperature of 8 ¬∞ C, which is optimal for data center cooling systems. <br><br><div class="spoiler">  <b class="spoiler_title">Interesting to read: About data centers</b> <div class="spoiler_text"> <a href="http://iaas-blog.it-grad.ru/%25D0%25BE%25D0%25B1%25D0%25BB%25D0%25B0%25D1%2587%25D0%25BD%25D1%258B%25D0%25B9-%25D1%2586%25D0%25BE%25D0%25B4-%25D0%25B4%25D0%25BB%25D1%258F-iaas-%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25B9%25D0%25B4%25D0%25B5%25D1%2580%25D0%25B0-%25D0%25BA%25D0%25B0%25D0%25BA-%25D0%25BE%25D0%25B1%25D0%25BB%25D0%25B0%25D0%25BA%25D0%25BE-%25D0%25B8%25D1%2582-%25D0%25B3%25D1%2580%25D0%25B0%25D0%25B4-%25D1%2580%25D0%25B0%25D0%25B7%25D0%25BC%25D0%25B5%25D1%2589%25D0%25B0%25D0%25B5%25D1%2582%25D1%2581%25D1%258F-%25D0%25B2-%25D0%25B4%25D0%25B0%25D1%2582%25D0%25B0-%25D1%2586%25D0%25B5%25D0%25BD%25D1%2582%25D1%2580%25D0%25B5-dataspace"><img src="https://habrastorage.org/web/c6f/932/7c2/c6f9327c21214b619b756b786b7f5dd3.png" width="500"></a> <br>  <a href="http://iaas-blog.it-grad.ru/%25D0%25BE%25D0%25B1%25D0%25BB%25D0%25B0%25D1%2587%25D0%25BD%25D1%258B%25D0%25B9-%25D1%2586%25D0%25BE%25D0%25B4-%25D0%25B4%25D0%25BB%25D1%258F-iaas-%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25B9%25D0%25B4%25D0%25B5%25D1%2580%25D0%25B0-%25D0%25BA%25D0%25B0%25D0%25BA-%25D0%25BE%25D0%25B1%25D0%25BB%25D0%25B0%25D0%25BA%25D0%25BE-%25D0%25B8%25D1%2582-%25D0%25B3%25D1%2580%25D0%25B0%25D0%25B4-%25D1%2580%25D0%25B0%25D0%25B7%25D0%25BC%25D0%25B5%25D1%2589%25D0%25B0%25D0%25B5%25D1%2582%25D1%2581%25D1%258F-%25D0%25B2-%25D0%25B4%25D0%25B0%25D1%2582%25D0%25B0-%25D1%2586%25D0%25B5%25D0%25BD%25D1%2582%25D1%2580%25D0%25B5-dataspace">Cloud data center for IaaS-provider: how the cloud "IT-GRAD" is located in the DataSpace data center</a> <br><br> <a href="http://iaas-blog.it-grad.ru/%25D0%25BE%25D0%25B1%25D0%25BB%25D0%25B0%25D1%2587%25D0%25BD%25D1%258B%25D0%25B9-%25D1%2586%25D0%25BE%25D0%25B4-%25D0%25BA%25D0%25B0%25D0%25BA-%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B5%25D1%2580%25D0%25B8%25D1%2582%25D1%258C-%25D0%25BD%25D0%25B0%25D0%25B4%25D0%25B5%25D0%25B6%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D1%258C-%25D0%25B4%25D0%25B0%25D1%2582%25D0%25B0-%25D1%2586%25D0%25B5%25D0%25BD%25D1%2582%25D1%2580%25D0%25B0-%25D0%25BF%25D1%2580%25D0%25B8-%25D0%25B2%25D1%258B%25D0%25B1%25D0%25BE%25D1%2580%25D0%25B5-iaas-%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25B9%25D0%25B4%25D0%25B5%25D1%2580%25D0%25B0"><img src="https://habrastorage.org/web/bad/19d/7ea/bad19d7ea9974525ba584a5f18ab3d3d.png" width="500"></a> <br>  <a href="http://iaas-blog.it-grad.ru/%25D0%25BE%25D0%25B1%25D0%25BB%25D0%25B0%25D1%2587%25D0%25BD%25D1%258B%25D0%25B9-%25D1%2586%25D0%25BE%25D0%25B4-%25D0%25BA%25D0%25B0%25D0%25BA-%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B5%25D1%2580%25D0%25B8%25D1%2582%25D1%258C-%25D0%25BD%25D0%25B0%25D0%25B4%25D0%25B5%25D0%25B6%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D1%258C-%25D0%25B4%25D0%25B0%25D1%2582%25D0%25B0-%25D1%2586%25D0%25B5%25D0%25BD%25D1%2582%25D1%2580%25D0%25B0-%25D0%25BF%25D1%2580%25D0%25B8-%25D0%25B2%25D1%258B%25D0%25B1%25D0%25BE%25D1%2580%25D0%25B5-iaas-%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25B9%25D0%25B4%25D0%25B5%25D1%2580%25D0%25B0">Cloud data center: how to check the reliability of the data center when choosing an IaaS provider</a> <br></div></div><br>  The Microsoft Natick project, which implements the idea of ‚Äã‚Äãan underwater data center, is aimed at a deeper exploitation of water resources.  <a href="http://spectrum.ieee.org/computing/hardware/want-an-energyefficient-data-center-build-it-underwater">It is assumed</a> that at the bottom of the ocean at a depth of 50 to 200 meters will be placed capsules containing several thousand servers.  The interior of the unit consists of standard racks with heat exchangers that transfer heat from air to water. <br><br>  Then the liquid enters the heat exchangers outside the capsules, which, in turn, redirect heat to the ocean.  During the experiment, the prototype was immersed for 105 days off the coast of California.  Water temperature ranged from 14 to 18 ¬∞ C.  The cost of cooling was significantly lower in comparison with mechanical methods of cooling. <br><br>  A similar concept is being developed by Google, but scientists from the IT giant are proposing to place data centers on barges.  According <a href="http://www.datacenterdynamics.com/content-tracks/power-cooling/floating-data-center-is-launched/95209.fullarticle">to</a> Nautilus Data Technologies, which is developing a similar technology, this approach saves up to 30% of energy due to natural cooling. <br><br>  Another embodiment of the concept of operating ‚Äúnatural coolers‚Äù is underground data centers, as well as data centers based in caves of natural origin.  In comparison with the idea of ‚Äã‚Äãdiving servers under water, underground objects have an advantage in terms of access to equipment.  As examples of the implementation of a cave in the city of Tampere, Finland, which rents Aiber Networks. <br><br>  Also worth noting is the Iron Mountain data center located in a former mine in Pennsylvania at a depth of 67 meters.  <a href="https://www.datacenters.com/news/infrastructure/218-new-underground-data-center-trend-takes-hold">It is argued</a> that the constant temperature of the object is maintained at 11 ¬∞ C in a natural way - this provides "one of the lowest energy efficiency indicators [as of 2014]". <br><br><h4>  Alternative liquid based cooling systems </h4><br>  Based on the principle of immersion in a liquid, the 2bm Iceotope system covers the electronics with non-conductive coolant, the heat from which is transmitted to the radiator.  Water passes through the rack and can be reused for other purposes, such as central heating.  According <a href="http://www.datacenterdynamics.com/content-tracks/power-cooling/2bm-adopts-iceotopes-liquid-cooling-system/97867.fullarticle">to</a> 2bm, this system allows you to save up to 40% of the total cost of maintaining the data center. <br><br>  In 2014, Data Center Knowledge also <a href="http://www.datacenterknowledge.com/archives/2014/09/04/nsa-exploring-use-mineral-oil-cool-servers/">submitted a</a> report on the optimization of the cooling process of the data center of the US National Security Agency through the immersion of servers in petroleum oils.  This approach also eliminated cooling with the help of fans. <br><br><h4>  Artificial Intelligence Cooling </h4><br>  In 2014, Google acquired the company DeepMind, specializing in the development of artificial intelligence systems.  A series of tests of the corporation AI was associated with the assessment of the energy efficiency of its own data centers.  According to the study, Google was <a href="https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/">able to</a> achieve a reduction in energy consumption in terms of cooling by 40%. <br><br>  For this, data collected by thousands of sensors and thermal sensors in data centers were used to train neural networks.  The networks learned to predict the temperature and pressure in the data center in the next hour and took the necessary actions to ensure the specified temperature thresholds.  Optimization results are presented in the following graph. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/3cf/cf6/329/3cfcf6329677472ab86d76c5734e4b67.png" width="500"></div>  <i>Source: <a href="https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/">DeepMind</a></i> <br><br><h4>  Directly modulated laser on a silicon substrate </h4><br>  The method is in the early stages of development and <a href="http://www.gazettabyte.com/home/2016/11/28/dimension-tackles-silicon-photonics-laser-shortfall.html">is implemented</a> as part of a project known as DIMENSION.  A group of scientists from Cardiff and Sheffield universities created a laser on a silicon substrate, capable of operating 100 thousand hours at temperatures up to 120 ¬∞ C. <br><br>  According to scientists, this technology will allow to combine two areas: electronics and photonics.  Lasers with direct modulation on a silicon substrate provide a tremendous speed of data transmission in electronic systems and, although not directly related to the cooling of the data center, can reduce the cost of electricity. <br><br><h2>  Prospects for innovative cooling systems </h2><br>  The focus on optimizing energy-intensive data center systems has led to the emergence of various concepts that may become a reality in the future.  One such example is the project of a 65-story data center in Iceland, which, according <a href="http://www.dailymail.co.uk/sciencetech/article-3558362/Forget-data-centers-future-data-SKYSCRAPERS-Radical-cylindrical-design-65-floors-servers.html">to</a> architects, is a ‚Äúgiant data tower‚Äù, reaching 50 meters in height and containing hundreds of thousands of servers serviced by ‚Äúclean‚Äù energy.  Given the proximity of the selected location to the Arctic Circle, the building will be provided with the means for natural cooling. <br><br>  However, a more promising direction in the deployment of innovative systems is the combination of free cooling solutions with the re-use of the energy generated by the operation of data centers.  For example, in December last year, the British telecommunications company aql officially <a href="http://www.datacenterdynamics.com/content-tracks/colo-cloud/new-data-center-in-leeds-will-provide-district-heat/97897.fullarticle">opened a</a> new data center that sends ‚Äúwaste‚Äù heat to the district heating system serving local residential and commercial buildings. <br><br>  Alfonso Capozzoli (Alfonso Capozzoli) <a href="http://www.sciencedirect.com/science/article/pii/S1876610215028337">argues</a> that the collection and reuse of waste heat produced by IT equipment is the next step in an energy efficiency strategy.  The implementation of appropriate measures, among other things, can significantly affect the reduction of CO2 emissions.  In addition to central heating, absorption cooling, <a href="https://ru.wikipedia.org/wiki/%25D0%25A6%25D0%25B8%25D0%25BA%25D0%25BB_%25D0%25A0%25D0%25B5%25D0%25BD%25D0%25BA%25D0%25B8%25D0%25BD%25D0%25B0">Rankine cycle</a> and water desalination are also considered as possible uses of waste heat. <br><br>  Thus, the data center industry is currently focused on the use of natural cooling sources, combined with intelligent systems that optimize energy efficiency.  Concepts that are currently at the prototype stage can radically change the approach to the cooling methods used in the future. </div><p>Source: <a href="https://habr.com/ru/post/330338/">https://habr.com/ru/post/330338/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../330322/index.html">Creating a third-party VR game</a></li>
<li><a href="../330324/index.html">Yii 2.0.12</a></li>
<li><a href="../330326/index.html">Five Docker Utilities You Should Know</a></li>
<li><a href="../330328/index.html">Android mitap at Badoo office on June 17</a></li>
<li><a href="../330332/index.html">15 best recipes for Smart Home with ioBroker</a></li>
<li><a href="../330340/index.html">In 2017, a fivefold increase in DDoS attacks was recorded.</a></li>
<li><a href="../330342/index.html">Evolutionary strategies as a scalable alternative to reinforcement learning</a></li>
<li><a href="../330346/index.html">Monitoring Linksys SPA8000 via Zabbix</a></li>
<li><a href="../330348/index.html">Ask a question to Mail.Ru Group cloud service developers</a></li>
<li><a href="../330350/index.html">Five steps to save the Linux server that crashed</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>