<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Proxmox cluster storage. Part Three Nuances</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! 

 The third part of the article is a kind of application to the previous two, in which I talked about working with the Proxmox cluster. In thi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Proxmox cluster storage. Part Three Nuances</h1><div class="post__text post__text-html js-mediator-article">  Hello! <img src="https://habrastorage.org/storage2/f4a/de2/6b8/f4ade26b81fa4635f6da117aca696aae.jpeg" align="right"><br><br>  The third part of the article is a kind of application to the previous two, in which I talked about working with the Proxmox cluster.  In this part, I will describe the problems we encountered in working with Proxmox, and how to solve them. <br><br><h4>  Authorized iSCSI connection </h4><br>  If you need to specify credit lines when connecting to <i>iSCSI,</i> it is better to do this bypassing <i>Proxmox</i> .  Why? 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  Firstly, because it is not possible to create an authorized <i>iSCSI</i> <i>connection</i> via the <i>Proxmox</i> <i>web</i> interface. </li><li>  Secondly, even if you decide to create <i>an</i> unauthorized connection in <i>Proxmox</i> in order to specify the authorization information manually, you will have to butt with the system for the ability to change the target configuration files, because if the connection to the <i>iSCSI</i> host <i>fails, Proxmox</i> overwrites the <i>target</i> information and tries to connect again. </li></ul><a name="habracut"></a><br>  <a href="http://www.howtoforge.com/iscsi_on_linux">Easier to connect manually</a> : <br><br><pre><code class="bash hljs">root@srv01-vmx:~<span class="hljs-comment"><span class="hljs-comment"># iscsiadm -m discovery -t st -p 10.11.12.13 root@srv01-vmx:~# iscsiadm -m node --targetname "iqn.2012-10.local.alawar.ala-nas-01:pve-cluster-01" --portal "10.11.12.13:3260" --op=update --name node.session.auth.authmethod --value=CHAP root@srv01-vmx:~# iscsiadm -m node --targetname "iqn.2012-10.local.alawar.ala-nas-01:pve-cluster-01" --portal "10.11.12.13:3260" --op=update --name node.session.auth.username --value=Admin root@srv01-vmx:~# iscsiadm -m node --targetname "iqn.2012-10.local.alawar.ala-nas-01:pve-cluster-01" --portal "10.11.12.13:3260" --op=update --name node.session.auth.password --value=Lu4Ii2Ai root@srv01-vmx:~# iscsiadm -m node --targetname "iqn.2012-10.local.alawar.ala-nas-01:pve-cluster-01" --portal "10.11.12.13:3260" --login</span></span></code> </pre> <br>  These commands must be executed for all portals that provide the target we need on all the nodes of the cluster.  Or, you can execute these commands on one node, and distribute configuration files for this connection to the others.  The files are located in the " <i>/ etc / iscsi / nodes</i> " and " <i>/ etc / iscsi / send_targets</i> " <i>directories</i> . <br><br><h4>  Mounting on a new GFS2-FS node </h4><br>  In order to mount a <i>GFS2</i> file system on a new node, it (the <i>file system</i> ) needs to add another journal.  This is done as follows: on any cluster node on which the required <i>FS</i> is mounted, the command is executed: <br><br><pre> <code class="bash hljs">root@pve01:~<span class="hljs-comment"><span class="hljs-comment"># gfs2_jadd -j 1 /mnt/cluster/storage01</span></span></code> </pre><br>  The " <i>-j</i> " parameter specifies the number of logs to add to <i>FS</i> . <br><br>  This command may fail with the error: <br><br><pre> <code class="hljs sql"><span class="hljs-keyword"><span class="hljs-keyword">create</span></span>: Disk <span class="hljs-keyword"><span class="hljs-keyword">quota</span></span> exceeded</code> </pre><br>  Causes of the error: <br><br>  Inside <i>GFS2</i> -tom is actually not one file system, but two.  Another file system is designed for service purposes.  If desired, it can be mounted by adding the option " <i>-o meta</i> ".  Changes within this <i>FS</i> can potentially lead to data file system corruption.  When adding a log to <i>FS</i> , the <i>meta-</i> file system is mounted in the " <i>/ tmp / TEMP_RANDOM_DIR</i> " <i>directory</i> , after which a log file is created in it.  For reasons unknown to us so far, the kernel sometimes believes that in the mounted <i>meta-FS,</i> the quota for creating objects is exceeded, which is why this error occurs.  You can get out of the situation by remounting <i>GFS2</i> with data (of <i>course, to do this you need to stop all virtuals located on this FS</i> ), and once again execute the command to add a log.  You also need to unmount the <i>meta-FS</i> , the rest of the last unsuccessful attempt to add the log: <br><br><pre> <code class="bash hljs">cat /proc/mounts | grep /tmp/ | grep -i gfs2 | awk <span class="hljs-string"><span class="hljs-string">'{print $2}'</span></span> | xargs umount</code> </pre><br><h4>  Mounting the data source inside the container </h4><br>  Container virtualization technology is good because the host has almost unlimited opportunities to communicate with the virtual machine. <br><br>  When the <i>vzctl</i> container starts, it tries to execute the following set of scripts ( <i>if available</i> ): <br><br><ul><li>  /etc/pve/openvz/vps.premount </li><li>  /etc/pve/openvz/CTID.premount </li><li>  /etc/pve/openvz/vps.mount </li><li>  /etc/pve/openvz/CTID.mount </li><li>  /etc/pve/openvz/CTID.start </li></ul><br>  When stopped, the following scripts are executed: <br><br><ul><li>  /etc/pve/openvz/CTID.stop </li><li>  /etc/pve/openvz/CTID.umount </li><li>  /etc/pve/openvz/vps.umount </li><li>  /etc/pve/openvz/CTID.postumount </li><li>  /etc/pve/openvz/vps.postumount </li></ul><br>  where " <i>CTID</i> " is the container number.  The " <i>vps. *</i> " Scripts are executed during operations with any container.  The scripts " <i>* .start</i> " and " <i>* .stop</i> " are executed in the context of the container, all the others are in the context of the host.  Thus, we can script the process of starting / stopping the container, adding data mounting to it.  Here are some examples: <br><br><h5>  Mounting the data directory inside the container </h5><br>  If the container works with a large amount of data, we try not to keep this data inside the container, but mount it from the host.  In this approach, there are two positive points: <br><br><ol><li>  The container is small, quickly backed up with <i>Proxmox</i> .  We have the ability at any time to quickly restore / clone the functionality of the container. </li><li>  Container data can be centrally backed up by an adult backup system with all the facilities provided by it (multi-level backups, rotation, statistics, and so on). </li></ol><br>  Contents of the file " <i>CTID.mount</i> ": <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash . /etc/vz/vz.conf #        OpenVZ.  ,     ${VE_ROOT} -     . . ${VE_CONFFILE} #       DIR_SRC=/storage/src_dir #   ,      DIR_DST=/data #   ,     $DIR_SRC mkdir -p ${VE_ROOT}/${DIR_DST} #      mount -n -t simfs ${DIR_SRC} ${VE_ROOT}/{$DIR_DST} -o /data #    </span></span></code> </pre><br><h5>  Mounting the file system inside the container </h5><br>  On the host there is a volume that needs to be given to the container.  Contents of the file " <i>CTID.mount</i> ": <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash . /etc/vz/vz.conf . ${VE_CONFFILE} UUID_SRC=3d1d8ec1-afa6-455f-8a27-5465c454e212 # UUID ,      DIR_DST=/data mkdir -p ${VE_ROOT}/${DIR_DST} mount -n -U ${UUID_SRC} ${VE_ROOT}/{$DIR_DST}</span></span></code> </pre><br><h5>  Mounting the file system located in the file inside the container </h5><br>  Why this may be needed?  If any tricky product ( <i>for example, <a href="http://docs.splunk.com/Documentation/Splunk/latest/Installation/Systemrequirements">Splunk</a></i> ) does not want to work with <i>simfs</i> in any <i>way</i> , or we are not satisfied with the speed of <i>GFS2</i> operation under certain conditions.  For example, we have some kind of cache on a bunch of small files.  <i>GFS2</i> does not work very quickly with large volumes of small files.  Then you can create a file system on the host, other than <i>GFS2</i> ( <i>ext3</i> ), and connect it to the container. <br><br>  Mount the <i>loop</i> device from file to container: <br><br>  First create the file: <br><br><pre> <code class="bash hljs">root@srv01:/storage<span class="hljs-comment"><span class="hljs-comment"># truncate -s 10G CTID_ext3.fs</span></span></code> </pre><br>  Format the <i>FS</i> in the file: <br><br><pre> <code class="bash hljs">root@srv01:/storage<span class="hljs-comment"><span class="hljs-comment"># mkfs.ext3 CTID_ext3.fs mke2fs 1.42 (29-Nov-2011) CTID_ext3.fs is not a block special device. Proceed anyway? (y,n) y ...</span></span></code> </pre><br>  Contents of the file " <i>CTID.mount</i> ": <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash . /etc/vz/vz.conf . ${VE_CONFFILE} CFILE_SRC=/storage/CTID_ext3.fs #   ,      DIR_DST=/data mkdir -p ${VE_ROOT}/${DIR_DST} mount -n ${CFILE_SRC} -t ext3 ${VE_ROOT}/{$DIR_DST} -o loop</span></span></code> </pre><br><h5>  Unmounting external data in a container when stopped </h5><br>  When the container is stopped, the system automatically tries to unmount all file systems connected to it.  But in a particularly exotic configuration, it does not work for her.  So just in case an example of a simple script " <i>CTID.umount</i> ": <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash . /etc/vz/vz.conf . ${VE_CONFFILE} DIR=/data if mountpoint -q "${VE_ROOT}${DIR}" ; then umount ${VE_ROOT}${DIR} fi</span></span></code> </pre><br><h4>  Working in a cluster with a non-cluster file system </h4><br>  If for some reason there is no desire to use cluster <i>FS</i> (it <i>does not suit the stability of work, it does not suit the speed, etc.</i> ), but you want to work with a single repository, then this option is possible.  For this we need: <br><br><ul><li>  A separate logical volume in <i>CLVM</i> for each node of the cluster </li><li>  The main storage for the normal operation of containers </li><li>  Empty backup storage for urgent mounting of a foreign node's volume in case of its crash / shutdown </li></ul><br>  Procedure: <br><br>  Each cluster node allocates its logical volume in the <i>CLVM</i> , format it. <br><br>  We create the main storage.  Create a directory that has the same name on all nodes of the cluster ( <i>for example, "/ storage"</i> ).  We mount our logical volume in it.  We create a repository of the type ‚Äú <i>Directory</i> ‚Äù in the admin <i>panel of Proxmox</i> , call it, for example, ‚Äú <i>STORAGE</i> ‚Äù, say that it is not shared. <br><br>  Create backup storage.  Create a directory with the same name on all nodes of the cluster ( <i>for example, "/ storage2"</i> ).  We create a repository of the type " <i>Directory</i> " in the admin <i>panel of Proxmox</i> , call it, for example, " <i>STORAGE2</i> ", we say that it is not shared.  In the event of a drop / shutdown of one of the nodes, we will mount its volume in the " <i>/ storage2</i> " <i>directory</i> on that node of the cluster, which will take over the load of the deceased. <br><br>  What we have in the end: <br><br><ul><li>  Migration ( <i>including online</i> ) of containers between nodes ( <i>if no data is mounted to the container on the side</i> ).  The container is transferred from the node to the node by copying, respectively, the migration time depends on the amount of data in the container.  The more data, the longer the container will be transferred between nodes.  Do not forget about the increasing disk load at the same time. </li><li>  ( <i>Under-</i> ) fault tolerance.  When a node is dropped, its data can be mounted on a neighboring node, and theoretically you can start working with them. </li></ul><br>  Why " <i>nedo</i> ", and why " <i>theoretically</i> ": <br><br>  Virtual machines live in the storage " <i>STORAGE</i> ", which is located in the " <i>/ storage</i> " directory.  The disk from the dead node will be mounted in the " <i>/ storage2</i> " <i>directory</i> , where <i>Proxmox</i> will see the containers, but cannot launch them from there.  In order to raise the virtual machines located in this storage, you need to do three things: <br><br><ol><li>  Report to firefighting containers that their new home is not the " <i>/ storage</i> " directory, but the " <i>/ storage2</i> " <i>directory</i> .  To do this, in each file " <i>* .conf</i> " in the directory " <i>/ etc / pve / nodes / name_death_name_name / openvz</i> " change the contents of the variable <i>VE_PRIVATE</i> from " <i>/ storage / private / CTID</i> " to " <i>/ storage2 / private / CTID</i> ". </li><li>  Tell the cluster that the virtualians over that non-alive node are now located on this live one.  To do this, it is enough to move all the files from the " <i>/ etc / pve / nodes / dead_number_networks / openvz</i> " directory to the " <i>/ etc / pve / nodes / live_number_notes / openvz</i> " <i>directory</i> .  Perhaps, for this there is some kind of correct <i>API-</i> instruction, but we did not bother with this :) </li><li>  Reset the quota for each fire container ( <i>just in case</i> ): <br><br><pre> <code class="hljs objectivec">vzquota drop <span class="hljs-built_in"><span class="hljs-built_in">CTID</span></span></code> </pre><br></li></ol><br>  Everything.  You can run containers. <br><br>  If the containers from the dead node take up some space, or we have incredibly nimble disks, or we can afford to wait, then we can avoid the first and third steps by simply transferring the containers we need from " <i>/ storage2 / private</i> " to " <i>/ storage / private</i> ". <br><br><h4>  If the cluster has collapsed </h4><br>  A cluster is a capricious creature, and there are cases when it gets into a pose.  For example, after a massive network problem, or due to a massive power failure.  The pose is as follows: when accessing the cluster storage, the current session is blocked, polling the fence-domain status displays alarm messages, such as " <i>wait state messages</i> ", and connection errors are added to the <i>dmesg</i> . <br><br>  If no attempts to revive the cluster lead to success, then the simplest thing is to disable automatic entry into the <i>fence-</i> domain on all nodes of the cluster ( <i>file "/ etc / default / redhat-cluster-pve"</i> ), and then restart all the nodes.  We must be prepared for the fact that the nodes will not be able to reboot on their own.  When all nodes will be rebooted, we manually connect to the <i>fence</i> domain, start <i>CLVM</i> , and so on.  The previous articles have written how to do this. <br><br><h4>  On this, perhaps, everything. </h4><br>  In the next part I will talk about how we automate the work in the cluster. <br><br>  Thanks for attention! <br><br><ul><li>  <a href="http://habrahabr.ru/company/alawar/blog/163297/">Proxmox cluster storage.</a>  <a href="http://habrahabr.ru/company/alawar/blog/163297/">Part one.</a>  <a href="http://habrahabr.ru/company/alawar/blog/163297/">Fencing</a> </li><li>  <a href="http://habrahabr.ru/company/alawar/blog/166919/">Proxmox cluster storage.</a>  <a href="http://habrahabr.ru/company/alawar/blog/166919/">Part two.</a>  <a href="http://habrahabr.ru/company/alawar/blog/166919/">Launch</a> </li><li>  Proxmox cluster storage.  Part Three  Nuances </li></ul></div><p>Source: <a href="https://habr.com/ru/post/178429/">https://habr.com/ru/post/178429/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../178415/index.html">The company AMLogic is preparing to release a new single-chip platform - AML8726-M8</a></li>
<li><a href="../178417/index.html">How do startups get into accelerator</a></li>
<li><a href="../178423/index.html">"Terminating" sms-spam</a></li>
<li><a href="../178425/index.html">How we made a smart office lighting system: compare two floors</a></li>
<li><a href="../178427/index.html">uptodate.js is a library for auto updating time elements</a></li>
<li><a href="../178435/index.html">Cutting into two equal parts, part two</a></li>
<li><a href="../178437/index.html">How we took questions to the President</a></li>
<li><a href="../178439/index.html">Hacker competition at Positive Hack Days III</a></li>
<li><a href="../178443/index.html">Application Excellence Labs are back with a new name and in a new format on DevCon 2013</a></li>
<li><a href="../178445/index.html">Overview of the high-capacity 6400mAh battery for Galaxy Note 2 on PocketNow</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>