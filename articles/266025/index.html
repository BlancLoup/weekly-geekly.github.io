<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Working with text data in scikit-learn (translation of documentation) - part 2</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This article is a translation of a chapter on textual data training from the official scikit-learn documentation . You can read the beginning of the a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Working with text data in scikit-learn (translation of documentation) - part 2</h1><div class="post__text post__text-html js-mediator-article">  <i>This article is a translation of a chapter on textual data training from the official <a href="http://scikit-learn.org/0.15/tutorial/text_analytics/working_with_text_data.html">scikit-learn</a> documentation <a href="http://scikit-learn.org/0.15/tutorial/text_analytics/working_with_text_data.html">.</a></i>  <i>You can read the beginning of the article in <a href="http://habrahabr.ru/post/264339/">part 1</a> .</i> <br><br><h3>  Training classifier </h3><br>  Now that we have identified the signs, we can train the classifier to predict the category of the text.  Let's start with the <a href="http://scikit-learn.org/stable/modules/naive_bayes.html">Naive Bayes classifier</a> , which will be an excellent starting point for our task.  scikit-learn includes several variants of this classifier.  The most suitable for counting words is its poly nominal version: <br><br><pre><code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.naive_bayes <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MultinomialNB &gt;&gt;&gt; clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)</code> </pre> <br><a name="habracut"></a><br>  In order for us to try to predict the results on a new document, we need to extract features (features) using almost the same sequence as before.  The difference is that the transform method is used instead of the fit_transform from transformers, since they have already been applied to our training set: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>docs_new = [<span class="hljs-string"><span class="hljs-string">'God is love'</span></span>, <span class="hljs-string"><span class="hljs-string">'OpenGL on the GPU is fast'</span></span>] &gt;&gt;&gt; X_new_counts = count_vect.transform(docs_new) &gt;&gt;&gt; X_new_tfidf = tfidf_transformer.transform(X_new_counts) &gt;&gt;&gt; predicted = clf.predict(X_new_tfidf) &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> doc, category <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(docs_new, predicted): ... print(<span class="hljs-string"><span class="hljs-string">'%r =&gt; %s'</span></span> % (doc, twenty_train.target_names[category])) ... <span class="hljs-string"><span class="hljs-string">'God is love'</span></span> =&gt; soc.religion.christian <span class="hljs-string"><span class="hljs-string">'OpenGL on the GPU is fast'</span></span> =&gt; comp.graphics</code> </pre> <br><br><h3>  Creating pipelining </h3><br>  To make it easier to work with the vectorizer =&gt;&gt; transformer =&gt; classifier chain, there is a Pipeline class in scikit-learn, which functions as a composite (pipeline) classifier: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.pipeline <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Pipeline &gt;&gt;&gt; text_clf = Pipeline([(<span class="hljs-string"><span class="hljs-string">'vect'</span></span>, CountVectorizer()), ... (<span class="hljs-string"><span class="hljs-string">'tfidf'</span></span>, TfidfTransformer()), ... (<span class="hljs-string"><span class="hljs-string">'clf'</span></span>, MultinomialNB()), ... ])</code> </pre> <br>  The name vect, tfidf and clf (classifier) ‚Äã‚Äãis chosen arbitrarily by us.  We will look at using them below in the chapter on grid search.  Now we will train the model with just 1 command: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>text_clf = text_clf.fit(twenty_train.data, twenty_train.target)</code> </pre> <br><br><h3>  Performance evaluation when working on a test sample </h3><br>  Evaluation of the accuracy of the forecast model is quite simple: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np &gt;&gt;&gt; twenty_test = fetch_20newsgroups(subset=<span class="hljs-string"><span class="hljs-string">'test'</span></span>, ... categories=categories, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">42</span></span>) &gt;&gt;&gt; docs_test = twenty_test.data &gt;&gt;&gt; predicted = text_clf.predict(docs_test) &gt;&gt;&gt; np.mean(predicted == twenty_test.target) <span class="hljs-number"><span class="hljs-number">0.834</span></span>...</code> </pre> <br>  For example, we got 83% accuracy.  Let's see if we can improve this result with the help of the linear <a href="http://scikit-learn.org/stable/modules/svm.html">support vector machine (support vector machine (SVM))</a> .  This method is usually considered the best text classification algorithm (although it is slightly slower than naive Bayes).  We can change the learning model simply by connecting another classification object to our pipeline: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.linear_model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SGDClassifier &gt;&gt;&gt; text_clf = Pipeline([(<span class="hljs-string"><span class="hljs-string">'vect'</span></span>, CountVectorizer()), ... (<span class="hljs-string"><span class="hljs-string">'tfidf'</span></span>, TfidfTransformer()), ... (<span class="hljs-string"><span class="hljs-string">'clf'</span></span>, SGDClassifier(loss=<span class="hljs-string"><span class="hljs-string">'hinge'</span></span>, penalty=<span class="hljs-string"><span class="hljs-string">'l2'</span></span>, ... alpha=<span class="hljs-number"><span class="hljs-number">1e-3</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">5</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">42</span></span>)), ... ]) &gt;&gt;&gt; _ = text_clf.fit(twenty_train.data, twenty_train.target) &gt;&gt;&gt; predicted = text_clf.predict(docs_test) &gt;&gt;&gt; np.mean(predicted == twenty_test.target) <span class="hljs-number"><span class="hljs-number">0.912</span></span>...</code> </pre> <br>  scikit-learn also provides utilities for more detailed analysis of the results: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> metrics &gt;&gt;&gt; print(metrics.classification_report(twenty_test.target, predicted, ... target_names=twenty_test.target_names)) ... precision recall f1-score support alt.atheism <span class="hljs-number"><span class="hljs-number">0.95</span></span> <span class="hljs-number"><span class="hljs-number">0.81</span></span> <span class="hljs-number"><span class="hljs-number">0.87</span></span> <span class="hljs-number"><span class="hljs-number">319</span></span> comp.graphics <span class="hljs-number"><span class="hljs-number">0.88</span></span> <span class="hljs-number"><span class="hljs-number">0.97</span></span> <span class="hljs-number"><span class="hljs-number">0.92</span></span> <span class="hljs-number"><span class="hljs-number">389</span></span> sci.med <span class="hljs-number"><span class="hljs-number">0.94</span></span> <span class="hljs-number"><span class="hljs-number">0.90</span></span> <span class="hljs-number"><span class="hljs-number">0.92</span></span> <span class="hljs-number"><span class="hljs-number">396</span></span> soc.religion.christian <span class="hljs-number"><span class="hljs-number">0.90</span></span> <span class="hljs-number"><span class="hljs-number">0.95</span></span> <span class="hljs-number"><span class="hljs-number">0.93</span></span> <span class="hljs-number"><span class="hljs-number">398</span></span> avg / total <span class="hljs-number"><span class="hljs-number">0.92</span></span> <span class="hljs-number"><span class="hljs-number">0.91</span></span> <span class="hljs-number"><span class="hljs-number">0.91</span></span> <span class="hljs-number"><span class="hljs-number">1502</span></span> &gt;&gt;&gt; metrics.confusion_matrix(twenty_test.target, predicted) array([[<span class="hljs-number"><span class="hljs-number">258</span></span>, <span class="hljs-number"><span class="hljs-number">11</span></span>, <span class="hljs-number"><span class="hljs-number">15</span></span>, <span class="hljs-number"><span class="hljs-number">35</span></span>], [ <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">379</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>], [ <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">33</span></span>, <span class="hljs-number"><span class="hljs-number">355</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>], [ <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">379</span></span>]])</code> </pre> <br>  As expected, the inaccuracy matrix shows that the model from the newsgroups' sample of atheism and Christianity often confuses the model with each other than with the texts about computer graphics. <br><br><h3>  Setting options for using grid search </h3><br>  We have already calculated some parameters, such as use_idf in the TfidfTransformer function.  Classifiers typically also have many parameters, for example, MultinomialNB includes a smoothing factor alpha, and the SGDClassifier has a penalty parameter alpha (penalty function method), a custom loss, and penalty terms in the objective function (see the documentation section or use the Python hint function for more information). <br>  Instead of searching for parameters of various components in the circuit, you can run a search (using the exhaustive search method) for the best parameters in the grid of possible values.  We tried all classifiers in words or bigrams, with or without idf, with penalty parameters of 0.01 and 0.001 for the SVM (support vector method): <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.grid_search <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GridSearchCV &gt;&gt;&gt; parameters = {<span class="hljs-string"><span class="hljs-string">'vect__ngram_range'</span></span>: [(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), (<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>)], ... <span class="hljs-string"><span class="hljs-string">'tfidf__use_idf'</span></span>: (<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>), ... <span class="hljs-string"><span class="hljs-string">'clf__alpha'</span></span>: (<span class="hljs-number"><span class="hljs-number">1e-2</span></span>, <span class="hljs-number"><span class="hljs-number">1e-3</span></span>), ... }</code> </pre> <br>  Obviously, a similar search using the exhaustive search method can be resource intensive.  If we have a lot of processor cores at our disposal, we can run a grid search to try all the parameter combinations in parallel with the n_jobs parameter.  If we set this parameter to -1, the grid search will determine how many cores are installed and use them all: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>gs_clf = GridSearchCV(text_clf, parameters, n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>)</code> </pre> <br>  A grid search instance behaves like a regular scikit-learn model.  Let's run the search on a small part of the training sample to increase the processing speed: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>gs_clf = gs_clf.fit(twenty_train.data[:<span class="hljs-number"><span class="hljs-number">400</span></span>], twenty_train.target[:<span class="hljs-number"><span class="hljs-number">400</span></span>])</code> </pre> <br>  As a result of applying the fit method on the GridSearchCV object, we get a classifier that can be used to execute the predict function: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>twenty_train.target_names[gs_clf.predict([<span class="hljs-string"><span class="hljs-string">'God is love'</span></span>])] <span class="hljs-string"><span class="hljs-string">'soc.religion.christian'</span></span></code> </pre> <br>  but on the other hand, it is a very large and bulky object.  We can still get the optimal parameters by examining the attribute of the grid_scores_ object, which is a list of pairs of parameters \ measure.  To get the attributes of measures, we can: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>best_parameters, score, _ = max(gs_clf.grid_scores_, key=<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: x[<span class="hljs-number"><span class="hljs-number">1</span></span>]) &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> param_name <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sorted(parameters.keys()): ... print(<span class="hljs-string"><span class="hljs-string">"%s: %r"</span></span> % (param_name, best_parameters[param_name])) ... clf__alpha: <span class="hljs-number"><span class="hljs-number">0.001</span></span> tfidf__use_idf: <span class="hljs-keyword"><span class="hljs-keyword">True</span></span> vect__ngram_range: (<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) &gt;&gt;&gt; score <span class="hljs-number"><span class="hljs-number">0.900</span></span>...</code> </pre> <br><h3>  Exercises </h3><br>  To complete the exercises, copy the contents of the 'skeletons' folder into a new folder called 'workspace': <br><br><pre> <code class="python hljs">% cp -r skeletons workspace</code> </pre> <br>  You can edit the contents of the workspace folder without fear of losing the original instructions for the exercises. <br>  Then open the ipython shell and run the incomplete script for the exercise: <br><br><pre> <code class="python hljs">[<span class="hljs-number"><span class="hljs-number">1</span></span>] %run workspace/exercise_XX_script.py arg1 arg2 arg3</code> </pre> <br>  If an exception was thrown, use% debug to start the emergency ipdb session. <br>  Clear the implementation and repeat until you solve the problem. <br>  <b>In each exercise, the skeleton files contain all the necessary import instructions, code templates for loading data, and examples for the code to estimate model prediction accuracy.</b> <br><br><h3>  Exercise 1: Language Definition </h3><br><ul><li>  Write a pipeline - a textual classifier using a special preprocessing and CharNGramAnalyzer.  As an educational sample, use articles from Wikipedia. </li><li>  Evaluate performance on any test sample that does not coincide with the training one. </li></ul><br>  ipython command line: <br><pre> <code class="python hljs">%run workspace/exercise_01_language_train_model.py data/languages/paragraphs/</code> </pre> <br><br><h3>  Exercise 2: Preference analysis based on movie reviews </h3><br><ul><li>  Write a conveyor - a text classifier to categorize movie reviews as positive and negative. </li><li>  Select the appropriate parameter set using grid search. </li><li>  Rate performance on the test sample. </li></ul><br>  ipython command line: <br><pre> <code class="python hljs">%run workspace/exercise_02_sentiment.py data/movie_reviews/txt_sentoken/</code> </pre> <br><br><h3>  Exercises 3: Utility - text classifier on the command line (console application) </h3><br>  Using the results of previous exercises and the cPickle module of the standard library, write a command line utility that determines the text language in stdin (keyboard input) and determines the polarity (positive or negative) if the text is written in English. <br><br><h3>  What's next? </h3><br>  In this section, some tips are given to help you learn more about scikit-learn after going through the exercises: <br><ul><li>  Try to use analyzer and token normalization in <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">CountVectorizer</a> </li><li>  If you have no markup, try using <a href="http://scikit-learn.org/stable/auto_examples/text/document_clustering.html">clustering</a> to solve your problem. </li><li>  If your document is tagged with many tags, i.e.  categories, look at the <a href="http://scikit-learn.org/stable/modules/multiclass.html">Multiclass and multilabel</a> section </li><li>  Try using <a href="http://scikit-learn.org/stable/modules/decomposition.html">Truncated SVD</a> for <a href="http://en.wikipedia.org/wiki/Latent_semantic_analysis">latent semantic analysis</a> . </li><li>  Familiarize yourself with using <a href="http://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html">Out-of-core Classification</a> for learning data that does not fit in RAM. </li><li>  Get to <a href="http://scikit-learn.org/stable/modules/feature_extraction.html">know Hashing Vectorizer</a> , which requires less memory compared to <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">CountVectorizer</a> . </li></ul></div><p>Source: <a href="https://habr.com/ru/post/266025/">https://habr.com/ru/post/266025/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../266015/index.html">IBM Watson cognitive system: principles of working with natural language</a></li>
<li><a href="../266017/index.html">Centrifuge + Go = Centrifugo - harder, better, faster, stronger</a></li>
<li><a href="../266019/index.html">The first book of the young programmer. Learning to write programs on Scratch</a></li>
<li><a href="../266021/index.html">Publish DITA to PDF using the DITA Open Toolkit</a></li>
<li><a href="../266023/index.html">Error compensation for floating-point operations</a></li>
<li><a href="../266027/index.html">#NoHacked: eliminating the effects of hacking with loading URLs containing meaningless text</a></li>
<li><a href="../266031/index.html">1C: Summer School 2015 - how to organize a "smart" vacation for young programmers - part 3. Protection and distribution of elephants</a></li>
<li><a href="../266033/index.html">Reliable maintenance of MS SQL Server databases for employees</a></li>
<li><a href="../266035/index.html">The course "Basics of effective work with Wolfram technologies". Session 1: Wolfram Mathematica and Wolfram Cloud Review</a></li>
<li><a href="../266037/index.html">Ruby Go shared library connections</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>