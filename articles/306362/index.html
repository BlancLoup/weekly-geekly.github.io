<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Working with the framework for iterative processing of graphs Giraph using RBM as an example</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The indifferent by xetobyte 

 Hello. In the previous article, we described how to create your applications under the Giraph framework (an add-on to t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Working with the framework for iterative processing of graphs Giraph using RBM as an example</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/bae/3a1/2c7/bae3a12c79d04b2580a72143b2f43a1c.jpg"><br>  <a href="http://xetobyte.deviantart.com/art/The-Indifferent-447733952">The</a> indifferent by xetobyte <br><br>  Hello.  In the <a href="https://habrahabr.ru/company/mailru/blog/305924/">previous article,</a> we described how to create your applications under the <a href="http://giraph.apache.org/">Giraph</a> framework (an add-on to the Hadoop data processing system), and promised to consider in detail how to work with the Giraph, using the example of the Restricted Boltzmann Machine learning algorithm.  So, at some point the group of segmentation of the audience of the department of advertising technologies Mail.Ru Group was faced with the need to choose a tool for quick analysis of graphs, and for a number of reasons (read below) our attention was attracted by the Apache Giraph system. <br><a name="habracut"></a><br>  To understand how convenient this system is / performance / stable, etc., two participants of the Mail.Ru Technosphere laboratory, <a href="https://sphere.mail.ru/alumni/2/386/">Alexander Shcherbakov</a> and <a href="https://sphere.mail.ru/alumni/2/344/">Pavel Kovalenko,</a> were given the task of researching work with Giraph using the example of the Restricted Boltzmann Machine learning algorithm and trying to speed it up.  On this task, the guys were able to better understand the subtleties of setting up Giraph and get practical experience working with open source projects. <br><br>  Below, we will analyze in detail the implementation of the learning algorithm of the Boltzmann Machine (Restricted Boltzmann Machine, RBM) using the Giraph framework, which, in turn, is a superstructure over Hadoop.  The main aspects of working with Giraph will be discussed, as well as some of the ‚Äúchips‚Äù and methods of tuning the performance of this framework.  You can read about the Giraph and how to create your first project on it in the previous article. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1>  Article layout </h1><br>  1. <a href="https://habrahabr.ru/company/mailru/blog/306362/">RBM and Giraph</a> <br>  1.1.  <a href="https://habrahabr.ru/company/mailru/blog/306362/">Brief description of RBM</a> <br>  1.2.  <a href="https://habrahabr.ru/company/mailru/blog/306362/">Giraph</a> <br>  1.3.  <a href="https://habrahabr.ru/company/mailru/blog/306362/">RBM and Giraph</a> <br>  2. <a href="https://habrahabr.ru/company/mailru/blog/306362/">Controlling the execution of compute-methods, worker context</a> <br>  2.1.  <a href="https://habrahabr.ru/company/mailru/blog/306362/">Options to run</a> <br>  2.2.  <a href="https://habrahabr.ru/company/mailru/blog/306362/">Overview of the wizard</a> <br>  2.3.  <a href="https://habrahabr.ru/company/mailru/blog/306362/">Using WorkerContext and parameters stored in it in neurons</a> <br>  3. <a href="https://habrahabr.ru/company/mailru/blog/306362/">Overview of Initialization Classes</a> <br>  4. <a href="https://habrahabr.ru/company/mailru/blog/306362/">Description of the general class for neurons.</a> <br>  5. <a href="https://habrahabr.ru/company/mailru/blog/306362/">Classes of neurons</a> <br>  5.1.  <a href="https://habrahabr.ru/company/mailru/blog/306362/">VisibleNeuronZero</a> <br>  5.2.  <a href="https://habrahabr.ru/company/mailru/blog/306362/">HiddenNeuronFirst</a> <br>  5.3.  <a href="https://habrahabr.ru/company/mailru/blog/306362/">VisibleNeuronFirst</a> <br>  5.4.  <a href="https://habrahabr.ru/company/mailru/blog/306362/">NeuronDefault</a> <br>  5.5.  <a href="https://habrahabr.ru/company/mailru/blog/306362/">VisibleNeuronLast</a> <br>  5.6.  <a href="https://habrahabr.ru/company/mailru/blog/306362/">HiddenNeuronLast</a> <br>  5.7.  <a href="https://habrahabr.ru/company/mailru/blog/306362/">VisibleNeuronOneStep</a> <br>  6. <a href="https://habrahabr.ru/company/mailru/blog/306362/">Overview of auxiliary classes</a> <br>  6.1.  <a href="https://habrahabr.ru/company/mailru/blog/306362/">JSONWritable</a> <br>  6.2.  <a href="https://habrahabr.ru/company/mailru/blog/306362/">RBMInputFormat</a> <br>  7. <a href="https://habrahabr.ru/company/mailru/blog/306362/">Optimization</a> <br>  7.1.  <a href="https://habrahabr.ru/company/mailru/blog/306362/">Rib storage</a> <br>  7.2.  <a href="https://habrahabr.ru/company/mailru/blog/306362/">Using vector operations</a> <br>  8. <a href="https://habrahabr.ru/company/mailru/blog/306362/">Future work</a> <br>  9. <a href="https://habrahabr.ru/company/mailru/blog/306362/">Conclusion</a> <br>  10. <a href="https://habrahabr.ru/company/mailru/blog/306362/">Literature</a> <br><br><a name="1"></a><h1>  1. RBM and Giraph </h1><br><a name="1-1"></a><h3>  1.1.  Brief description of RBM </h3><br>  In this article, we will limit ourselves only to the facts that are necessary to understand what is happening. You can read more about RBM <a href="https://habrahabr.ru/post/159909/">here</a> or <a href="https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf">here</a> <br><br>  The Restricted Boltzmann Machine (RBM) is a neural network consisting of two layers: the visible and the hidden.  Each visible neuron <i>i is</i> connected to each hidden <i>j</i> , and the initially connecting edge has a random weight <i>w <sub>ij</sub></i> .  Also, each neuron has an ‚Äúoffset‚Äù parameter (the visible layer is <i>a <sub>i</sub></i> , the hidden layer is <i>b <sub>i</sub></i> ).  The number of neurons on the visible layer is denoted by <i>N</i> , on the hidden - <i>M.</i> <br><br><img src="https://habrastorage.org/files/5e3/450/6ac/5e34506ac11e4db2920370bd548e7e78.png"><br><br>  We describe how this network works. <br><br><ol><li>  Let us take some vector <i>v <sup>(0)</sup></i> from the training sample as the initial activation values ‚Äã‚Äãof the neurons of the visible layer. </li><li>  On the hidden layer, the vector <i>h <sup>(1) is considered</sup></i> as a linear combination of the values ‚Äã‚Äãof the vector <i>v <sup>(0)</sup></i> with weights <i>w</i> with the subsequent use of the sigmoidal activation function: <img src="https://habrastorage.org/files/2a0/e7c/17c/2a0e7c17c3e64ca0a4d370ab1c6468b4.png">  where <img src="https://habrastorage.org/files/8d5/2ac/edc/8d52acedccdf4e1e87a5bd8ed0befeb0.png">  - sigmoidal activation function.  Further, <i>h <sub>j</sub></i> = 1 with probability <img src="https://habrastorage.org/files/685/7bd/f1a/6857bdf1afe34c28ae71bf8b7f3be38b.png">  and 0 otherwise, in other words, we will sample. </li><li>  Then, using the vector <i>h <sup>(1),</sup></i> you can find the new value of the vector <i>v <sup>(1)</sup></i> by the similar formulas: <img src="https://habrastorage.org/files/c99/8b5/184/c998b518403648caa5c42c0c26d7272f.png">  . </li></ol><br><br>  Steps 2‚Äì3 will be repeated <i>k</i> times.  As a result, we obtain the vectors <i>v <sup>(k)</sup></i> and <i>h <sup>(k)</sup></i> . <br><br>  A restricted Boltzmann machine can be considered as a method of nonlinear transformation of a feature space.  We describe the initial vector <i>v by</i> the vector <i>h</i> according to the formulas presented above.  According to the vector <i>h,</i> we can restore the original vector <i>v</i> with some accuracy.  Thus, the vector <i>v <sup>(k)</sup></i> is the result of applying <i>k</i> times the transform and restore operations to the vector <i>v <sup>(0)</sup></i> .  It is logical to require that the output vector <i>v <sup>(k)</sup></i> be as close as possible to <i>v <sup>(0)</sup></i> .  This will mean that the vector <i>x</i> can be represented by the vector y with minimal loss of accuracy. <br><br>  We will minimize the deviation of <i>v <sup>(k)</sup></i> from <i>v <sup>(0)</sup></i> by the method of <a href="http://neuralnetworksanddeeplearning.com/chap2.html">back propagation of error</a> .  We divide the entire training sample into batches ‚Äî subsamples of fixed size <i>T.</i>  For each batch we will find the activation values, gradients and update the values ‚Äã‚Äãof weights and offsets.  Let the current batch consist of objects z <sub>i</sub> , <i>i</i> = 1 ... <i>T.</i>  For each of them we find <i>v <sup>(j)</sup> (z <sub>i</sub> )</i> and <i>h <sup>(j)</sup> (z <sub>i</sub> )</i> , <i>j</i> = 1 ... <i>k</i> .  Then we get [1, 2] the following formulas for changing network parameters: <br><br><img src="https://habrastorage.org/files/d2a/bd9/c64/d2abd9c6406447cbbacf0b94aca83c26.png"><br><br>  where <i>Œ∑</i> is the learning rate. <br><br>  One run of the algorithm for all batch will be called one epoch of training.  To get the best result, it makes sense to conduct several ( <i>K</i> ) epochs of training so that each batch will be used <i>K</i> times to calculate gradients and update network parameters.  In essence, we obtained an algorithm for stochastic gradient descent with respect to variables <i>w</i> , <i>a,</i> and <i>b</i> to minimize the mean for objects in the sample of the deviation of v <sup>(k)</sup> from v <sup>(0)</sup> . <br><br><a name="1-2"></a><h3>  1.2.  Giraph </h3><br>  Giraph is an iterative processing system for large graphs that runs on top of the Hadoop distributed data processing system.  We describe the main points in the work of this system.  Giraph works with graphs.  A graph is a set of vertices and a set of edges that connect these vertices.  Counts are oriented and undirected.  Technically, Giraph supports only directed graphs;  to make an undirected graph, you need to create two edges between vertex A and B: from A to B and from B to A, with the same values. <br><br>  Each vertex and each edge have their own values, and the values ‚Äã‚Äãof the vertices and edges can be of different types (selected depending on the tasks).  Each vertex contains a class that implements the <code>Computation</code> interface and which describes the <code>compute</code> method.  Calculations in Giraph are super superstep.  At each super-step for the vertices, the compute method is called.  Between supersteps vertices are delivered messages from other vertices, sent during the previous superstep.  Messages have their own type, which does not have to match the types of vertices or edges. <br><br>  To coordinate the calculations, there is a special mechanism - MasterCompute (master).  By default, <code>DefaultMaster</code> is used in calculations - a class inherited from the abstract <code>MasterCompute</code> class.  By itself, it is rather useless, since it does nothing (in many tasks nothing is required of it).  Fortunately, there is an opportunity to make your own master, it must be inherited from <code>MasterCompute</code> , and the <code>compute</code> method must be implemented in it, which is performed before each super-step.  The wizard allows you to select a class for the vertices, aggregate some data from the vertices and complete the calculations. <br><br>  Each vertex can be in two states: active and inactive.  If the <code>voteToHalt()</code> method is called inside the <code>compute</code> , then at the end of the super step, the vertex ‚Äúfalls asleep‚Äù - it becomes inactive at the next super step.  Between supersteps vertices are delivered messages sent by other vertices in the course of superstep.  All vertices that did not fall asleep at the previous super-step or received messages before the next are active and participate in the new super-step, performing their compute method.  If all vertices are inactive, then the calculations end.  At the end of the super step, the current state of the graph is saved to disk. <br><br>  The graph is processed on several workers (workers), the number of which does not change at run time.  Each worker consistently processes several parts of the graph (Partition).  The graph is divided into parts, which may change during processing.  All this is done automatically and does not require user intervention. <br><br><img src="https://habrastorage.org/files/75e/4e5/fbe/75e4e5fbebda41b3b40cae99ed3347a7.png"><br><br>  Giraph uses the computational model vertex-oriented, in contrast to the same GraphX.  This can be convenient for a number of tasks, since GraphX ‚Äã‚Äãworks with a graph as a simple set of parallel data, without paying attention to its structure.  It is also worth noting that GraphX ‚Äã‚Äãworks with <a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">immutable graphs</a> and requires Spark. <br><br><a name="1-3"></a><h3>  1.3.  RBM and Giraph </h3><br>  And now let's correlate the received ideas about Giraph and RBM.  There are vertices (neurons) connected by ribs.  They are divided into two parts, which work in turn: the visible and hidden layer.  One layer sends messages (activation values) to the other, making it active while falling asleep.  Layers from time to time change the actions that they need to produce (this is work for the master).  As we can see, RBM is nothing more than a bipartite undirected graph, and fits perfectly with the architecture of Giraph. <br><br><a name="2"></a><h1>  2. Controlling the execution of compute-methods, worker context </h1><br>  Let us understand how the work of our implementation of RBM is coordinated.  First, we use our master, in which at each iteration we select the necessary class for the vertices.  Secondly, a <code>WorkerContext</code> is implemented - a class in which the current state of the network is calculated and from which the vertices can get the context of the running application.  Thirdly, these are the parameters passed at startup.  The master and context receive the values ‚Äã‚Äãpassed in the parameters. <br><br><a name="2-1"></a><h3>  2.1.  Options to run </h3><br>  To begin with, we note the parameters that can be set in our implementation: <br><br><ul><li>  <code>maxEpochNumber</code> - the number of training repetitions on all data; </li><li>  <code>maxBatchNumber</code> - the number of batches used for training; </li><li>  <code>maxStepInSample</code> - the number of runs of the batch from the visible layer to the hidden and back; </li><li>  <code>learningRate</code> - the pace of learning; </li><li>  <code>visibleLayerSize</code> - the number of neurons on the visible layer; </li><li>  <code>hiddenLayerSize</code> - the number of neurons on the hidden layer; </li><li>  <code>inputPath</code> - the path to the input files; </li><li>  <code>useSampling</code> - if true, performs sampling on a hidden layer. </li></ul><br><a name="2-2"></a><h3>  2.2.  Overview of the wizard </h3><br>  All parameters specified in clause 2.1 are used by the master to control the learning process.  Let us consider in more detail how this happens. <br><br><img src="https://habrastorage.org/files/ebc/e79/643/ebce79643f8246829ab5d3e7154ecf18.png"><br><br>  First, a visible layer is created.  It is configured by the <code>VisibleInitialize</code> class.  Then the same thing happens for the hidden layer with the class <code>HiddenInitialize</code> .  Once the network is created, you can start learning.  It consists of several eras.  The epoch, in turn, is a consistent learning on this set of batches.  Thus, in order to understand how the whole learning process takes place, it is necessary to understand learning in one batch. <br><br><img src="https://habrastorage.org/files/3d3/fee/5bf/3d3fee5bfb444adeb02f565e700e785a.png"><br><br>  In general, the training takes place as follows (scheme 1): <br><br><ol><li>  Reading data in a visible layer. </li><li>  Preservation of a positive gradient value by a hidden layer. </li><li>  Maintaining a positive gradient value in a visible layer. </li><li>  Calculating the activation value of the hidden layer. </li><li>  Calculate the activation value of the visible layer. </li><li>  Calculating the activation value of the hidden layer. </li><li>  Repeat steps 5 and 6 more ( <code>maxStepInSample</code> - 3) times. </li><li>  Update weights of outgoing edges of the visible layer. </li><li>  Update scales on a hidden layer. </li></ol><br>  It is worth noting that the activation values ‚Äã‚Äãare considered at all steps except the 1st and 9th.  Total will be executed <code>(maxStepInSample + 1) * 2</code> steps. <br><br><img src="https://habrastorage.org/files/79b/fe6/379/79bfe6379a3b479c82bae7ddd0b93032.png"><br><br>  In the case when we send data to the hidden layer only once ( <code>maxStepInSample</code> = 1), the scheme changes somewhat (scheme 2): <br><br><ol><li>  Reading data in a visible layer. </li><li>  Preservation of a positive gradient value by a hidden layer. </li><li>  Update weights of outgoing edges of the visible layer. </li><li>  Update scales on a hidden layer. </li></ol><br>  Activation values ‚Äã‚Äãin this variant are recalculated only on the 2nd and 3rd steps.  Thus, we have seven classes for neurons (graph vertices) used in the learning process: <br><br><table><tbody><tr><td rowspan="2">  Class name </td><td rowspan="2">  Purpose </td><td colspan="2">  Relevant Steps </td></tr><tr><td>  From scheme 1 </td><td>  From scheme 2 </td></tr><tr><td> <code>VisibleNeuronZero</code> </td> <td>  The zero stage of learning with reading data also preserves the positive part of the gradient for offsets </td><td>  one </td><td>  one </td></tr><tr><td> <code>VisibleNeuronFirst</code> </td> <td>  Preserves the positive part of the gradient in the visible layer </td><td>  3 </td><td>  - </td></tr><tr><td> <code>VisibleNeuronLast</code> </td> <td>  The last step of the training on the visible layer, considers the negative part of the gradient and updates the weights of the edges emerging from the visible layer. </td><td>  eight </td><td>  - </td></tr><tr><td> <code>VisibleNeuronOneStep</code> </td> <td>  The combination of classes <code>VisibleNeuronFirst</code> and <code>VisibleNeuronLast</code> , is used in the case when <code>maxStepInSample</code> = 1 </td><td>  - </td><td>  3 </td></tr><tr><td> <code>HiddenNeuronFirst</code> </td> <td>  Saves a positive gradient value for the hidden layer (only for offsets <code>b</code> , weights gradients <code>w</code> are calculated on the visible layer) </td><td>  2 </td><td>  2 </td></tr><tr><td> <code>HiddenNeuronLast</code> </td> <td>  Updates the weights of ribs emerging from the hidden layer. </td><td>  9 </td><td>  four </td></tr><tr><td> <code>NeuronDefault</code> </td> <td>  Called both on the visible layer and on the hidden layer and simply recalculates the activation values. </td><td>  4, 5, 6 </td><td>  - </td></tr></tbody></table><br><a name="2-3"></a><h3>  2.3.  Using WorkerContext and parameters stored in it in neurons </h3><br>  In order for the neurons to use the transmitted parameters, they made their own WorkerContext, which, firstly, transmits information about the size of the layers during initialization, secondly, gives the neurons information about the pace of learning, whether it is necessary to do sampling and data paths.  In this class, regardless of the master, it is calculated which class is currently used for neurons. <br><br><a name="3"></a><h1>  3. Overview of Initialization Classes </h1><br>  Once again, but in more detail we will stop on how the graph is initialized.  The application starts with a single dummy top.  It performs <code>compute</code> from <code>InitialNode</code> .  The only thing that happens in this class is sending out empty messages to vertices with id from 1 to <code>visibleLayerSize</code> (vertices with these identifiers are immediately created and used in the next step).  Immediately after this, the vertex goes to sleep (a voteToHalt call) and is never used again. <br><br>  Before the next step, MasterCompute sets <code>VisibleInitialize</code> as the compute class, so the vertices created as a result of sending the message will execute it.  First, the vertex generates edges going out of it with random weights to the vertices with the id from <code>‚ÄìhiddenLayerSize</code> to ‚Äì1.  Then these weights are sent to the corresponding vertices of the hidden layer (recall that this causes the creation of these vertices).  And at the end of each vertex a zero offset is added.  After that, the visible layer "falls asleep", since its creation is complete. <br><br>  At the last stage of initialization, the compute of <code>HiddenInitialize</code> is executed in the neurons of the hidden layer.  Each vertex is iterated by the messages that came to it and creates the corresponding edges.  Thus, each connection between neurons is defined by two edges with the same weights.  Next, create a zero offset.  Then the hidden layer ‚Äúwakes up‚Äù visible with empty messages, and ‚Äúfalls asleep‚Äù itself. <br><br><img src="https://habrastorage.org/files/8b8/7fb/3f4/8b87fb3f4faa45cfaf76e2e091abfb2c.gif"><br><br><a name="4"></a><h1>  4. Description of the general class for neurons. </h1><br>  Classes of neurons in many respects perform very similar actions that can be meaningfully combined into a set of various functions.  The difference lies in the fact that in some classes certain functions are called / not called.  Those functions that are called in all classes of neurons were allocated to a separate abstract class <code>NeuronCompute</code> .  All classes of neurons are inherited from it, and <code>compute</code> functions are implemented in them, in which both <code>NeuronCompute</code> methods and methods specific to a particular class can be called.  And here, actually, and the list of the functions realized in it: <br><br><ul><li>  <code>sigma</code> - returns the value of the sigma function from the passed parameter; </li><li>  <code>ComputeActivateValues</code> - returns the activation values ‚Äã‚Äãfor the batch according to the activation values ‚Äã‚Äãreceived in the messages and the vertex offset; </li><li>  <code>SendActivateValues</code> - sends activation values ‚Äã‚Äãto outgoing edges; </li><li>  <code>ComputePositivePartOfBiasGradient</code> - returns the positive part of the gradient for offsets; </li><li>  <code>UpdateBiases</code> - updates the offset; </li><li>  <code>AddBias</code> - adds zero offset to the vertex; </li><li>  <code>SendNewEdges</code> - sends new edge values ‚Äã‚Äãto the hidden layer. </li></ul><br><a name="5"></a><h1>  5. Classes of neurons </h1><br>  We describe the classes that implement neurons.  Depending on the number of superstep selected one or another class. <br><br><a name="5-1"></a><h3>  5.1.  VisibleNeuronZero </h3><br>  This class is the first in the era.  It contains two proprietary methods: <code>ReadBatch</code> and <code>compute</code> .  It is easy to guess that in the <code>ReadBatch</code> method the next batch is loaded.  The number of the batch and the path are taken from the WorkerContext. <br><br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">/** *    */</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[] activateValues; <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> positiveBias; activateValues = ReadBatch(vertex); <span class="hljs-comment"><span class="hljs-comment">//   .         WorkerContext' positiveBias = ComputePositivePartOfBiasGradient(activateValues); //       SendActivateValues(vertex, activateValues); //       /** *     */ JSONWritable value = vertex.getValue(); value.addDouble("Positive bias", positiveBias); value.addArray("Value", activateValues); vertex.setValue(value); vertex.voteToHalt(); // </span></span></code> </pre><br><a name="5-2"></a><h3>  5.2.  HiddenNeuronFirst </h3><br>  This is the first class running on a hidden layer.  It consists entirely of methods of the <code>NeuronCompute</code> class.  Take a look at his <code>compute</code> : <br><br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">/** *    */</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> bias = vertex.getValue().getDouble(<span class="hljs-string"><span class="hljs-string">"Bias"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[] activateValues; <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> positiveBias; activateValues = ComputeActivateValues(vertex, messages, bias); <span class="hljs-comment"><span class="hljs-comment">//    positiveBias = ComputePositivePartOfBiasGradient(activateValues); //       SendActivateValues(vertex, activateValues); //     /** *     */ JSONWritable value = vertex.getValue(); value.addDouble("Positive bias", positiveBias); value.addArray("Value", activateValues); vertex.setValue(value); vertex.voteToHalt(); // </span></span></code> </pre><br><a name="5-3"></a><h3>  5.3.  VisibleNeuronFirst </h3><br>  Now that we have activation values ‚Äã‚Äãfor the visible and hidden layer, we can calculate the positive part of the gradient for the edges. <br><br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">/** *    */</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> bias = vertex.getValue().getDouble(<span class="hljs-string"><span class="hljs-string">"Bias"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[] activateValues = vertex.getValue().getArray(<span class="hljs-string"><span class="hljs-string">"Value"</span></span>); HashMap&lt;Long, Double&gt; positiveGradient; ArrayList valueAndGrad = ComputeActivateValuesAndPositivePartOfGradient(vertex, messages, activateValues, bias); <span class="hljs-comment"><span class="hljs-comment">//         activateValues = (double[])valueAndGrad.get(0); positiveGradient = (HashMap&lt;Long, Double&gt;)valueAndGrad.get(1); SendActivateValues(vertex, activateValues); //    /** *     */ JSONWritable value = vertex.getValue(); value.addArray("Value", activateValues); value.addHashMap("Positive gradient", positiveGradient); vertex.setValue(value); vertex.voteToHalt(); // </span></span></code> </pre><br><a name="5-4"></a><h3>  5.4.  NeuronDefault </h3><br>  This class is common to both layers.  Here is his compute: <br><br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">/** *    */</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> bias = vertex.getValue().getDouble(<span class="hljs-string"><span class="hljs-string">"Bias"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[] activateValues; activateValues = ComputeActivateValues(vertex, messages, bias); <span class="hljs-comment"><span class="hljs-comment">//    SendActivateValues(vertex, activateValues); //    /** *     */ JSONWritable value = vertex.getValue(); value.addArray("Value", activateValues); vertex.setValue(value); vertex.voteToHalt(); // </span></span></code> </pre><br><a name="5-5"></a><h3>  5.5.  VisibleNeuronLast </h3><br>  This is the last stage, performed on the visible layer, and it is in it that the weights are updated.  It would all end there (as in successive implementations), but you also need to update the edges, directed in the opposite direction. <br><br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">/** *    */</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> bias = vertex.getValue().getDouble(<span class="hljs-string"><span class="hljs-string">"Bias"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[] activateValues = vertex.getValue().getArray(<span class="hljs-string"><span class="hljs-string">"Value"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> positiveBias = vertex.getValue().getDouble(<span class="hljs-string"><span class="hljs-string">"Positive bias"</span></span>); activateValues = ComputeGradientAndUpdateEdges(vertex, messages, activateValues.length, bias); <span class="hljs-comment"><span class="hljs-comment">//     .    bias = UpdateBiases(activateValues, bias, positiveBias); //   SendNewEdges(vertex); //       /** *     */ JSONWritable value = vertex.getValue(); value.addArray("Value", activateValues); value.addDouble("Bias", bias); vertex.setValue(value); vertex.voteToHalt(); // </span></span></code> </pre><br><a name="5-6"></a><h3>  5.6.  HiddenNeuronLast </h3><br>  At this step, we take the new edge weights from the messages and set them on the existing edges. <br><br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">/** *    */</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> bias = vertex.getValue().getDouble(<span class="hljs-string"><span class="hljs-string">"Bias"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[] activateValues = vertex.getValue().getArray(<span class="hljs-string"><span class="hljs-string">"Value"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> positiveBias = vertex.getValue().getDouble(<span class="hljs-string"><span class="hljs-string">"Positive bias"</span></span>); UpdateEdgesByMessages(vertex, messages); <span class="hljs-comment"><span class="hljs-comment">//        bias = UpdateBiases(activateValues, bias, positiveBias); //   /** *     */ JSONWritable value = vertex.getValue(); value.addDouble("Bias", bias); vertex.setValue(value); vertex.voteToHalt(); // </span></span></code> </pre><br><a name="5-7"></a><h3>  5.7.  VisibleNeuronOneStep </h3><br>  This class is used in the case when one pass of activation values ‚Äã‚Äãto the hidden layer and back is performed.  It combines the functionality of the classes VisibleNeuronFirst and VisibleNeuronLast. <br><br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">/** *    */</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> bias = vertex.getValue().getDouble(<span class="hljs-string"><span class="hljs-string">"Bias"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[] activateValues = vertex.getValue().getArray(<span class="hljs-string"><span class="hljs-string">"Value"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> positiveBias = vertex.getValue().getDouble(<span class="hljs-string"><span class="hljs-string">"Positive bias"</span></span>); activateValues = ComputeGradientAndUpdateEdges(vertex, messages, activateValues, bias); <span class="hljs-comment"><span class="hljs-comment">//             bias = UpdateBiases(activateValues, bias, positiveBias); //   SendNewEdges(vertex); //        /** *     */ JSONWritable value = vertex.getValue(); value.addArray("Value", activateValues); value.addDouble("Bias", bias); vertex.setValue(value); vertex.voteToHalt(); // </span></span></code> </pre><br><a name="6"></a><h1>  6. Description of auxiliary classes </h1><br><a name="6-1"></a><h3>  6.1.  JSONWritable </h3><br>  <code>JSONWritable</code> - the class that we use as a class for the value at the vertices and for the messages to be sent.  In essence, this is a <a href="http://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/io/Writable.html">Writable wrapper</a> for a JSON object.  This data storage format is convenient because the type and number of values ‚Äã‚Äãthat need to be stored vary depending on the current iteration.  <a href="https://github.com/google/gson">Google GSON is</a> used as a library for working with JSON.  For recording and sending, the JSON string is wrapped in an object of the <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/Text.html">Text</a> class. <br><br>  For the Writable class, the main methods are <code>readFieds</code> ‚Äî read an object of this class from the input stream and <code>write</code> ‚Äî write the object to the output stream.  Below is their implementation for the <code>JSONWritable</code> class. <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">JSONWritable</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">implements</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Writable</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> JsonElement json; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">write</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(DataOutput out)</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">throws</span></span></span><span class="hljs-function"> IOException </span></span>{ Text text = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Text(<span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.json.toString()); text.write(out); } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">readFields</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(DataInput in)</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">throws</span></span></span><span class="hljs-function"> IOException </span></span>{ Text text = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Text(); text.readFields(in); JsonParser parser = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> JsonParser(); <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.json = parser.parse(text.toString()); } <span class="hljs-comment"><span class="hljs-comment">/*   */</span></span> }</code> </pre><br>  Since it was necessary to store a lot of different data in the JSON object, it is convenient to present it as a dictionary with string keys (Bias, PositiveGradient, etc.).  For this task, it was necessary to write some complex objects in JSON, including at the vertices it is necessary to store the positive part of the gradient for the edges.  It is represented as a HashMap, the keys of which are the id vertices of the destination edges (of the long type), and the values ‚Äã‚Äãare the positive part of the gradient of the corresponding edge (of the double type).  To do this, <code>JSONWritable</code> has written special methods for adding and extracting a HashMap by key. <br><br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> HashMap </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getHashMap</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(String key)</span></span></span><span class="hljs-function"> </span></span>{ HashMap a = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> HashMap(); Iterator it = <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.json.getAsJsonObject().getAsJsonObject(key).entrySet().iterator(); <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> (it.hasNext()) { Map.Entry me = (Map.Entry) it.next(); a.put(Long.valueOf(me.getKey().toString()), ((JsonElement) me.getValue()).getAsDouble()); } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> a; } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">addHashMap</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(String key, HashMap a)</span></span></span><span class="hljs-function"> </span></span>{ Iterator it = a.entrySet().iterator(); JsonObject obj = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> JsonObject(); <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> (it.hasNext()) { Map.Entry me = (Map.Entry) it.next(); obj.addProperty(me.getKey().toString(), (<span class="hljs-keyword"><span class="hljs-keyword">double</span></span>) me.getValue()); } <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.json.getAsJsonObject().add(key, obj); }</code> </pre><br><a name="6-2"></a><h3>  6.2.  RBMInputFormat </h3><br>  This class is used to read the graph written to the file.  It is a modification of the standard <a href="http://giraph.apache.org/apidocs/org/apache/giraph/io/formats/JsonLongDoubleFloatDoubleVertexInputFormat.html">JsonLongDoubleFloatDoubleVertexInputFormat</a> to work with other data types.  Input data should be written in the following form: each line of the input file is a JSON list corresponding to one vertex.  Each list is written in the format <code>[Id_, __, _]</code> , where <code>Id</code> is an integer, <code></code> is a JSON-object, the <code> </code> is a JSON-list of the form <code>[[Id__, __], ‚Ä¶ ]</code> . <br><br><a name="7"></a><h1>  7. Optimization </h1><br>  Many of Giraph's work parameters, such as the method of storing edges and messages, the method of splitting vertices between workers, the level of debug output and the settings of counters, can be configured at startup from the command line.  A complete list of startup options can be found at <a href="http://giraph.apache.org/options.html">http://giraph.apache.org/options.html</a> .  Setting these options for a specific task allows you to achieve a noticeable increase in performance. <br><br><a name="7-1"></a><h3>  7.1.  Rib storage </h3><br>  In Giraph, the edges are stored by default as a byte array (ByteArray) and are de-serialized every time they are accessed.  This helps save memory, but increases the running time.  To speed up the program, we tried to store the edges in the form of a list (ArrayList) and in the form of a dictionary, the keys in which are the final vertices of the edges (HashMap).  The list allows you to quickly iterate over the edges, while the dictionary allows you to quickly find an edge along its top of the destination. <br>      ‚Äî      Giraph,    ,     <code>-ca giraph.outEdgesClass=org.apache.giraph.edge.ArrayListEdges</code> (  ). <br><br>       :  800  , 150 , 50 , 100   ,      ( <code>maxStepInSample=2</code> ), , 6    .               .   ,    ,     ,  ,    ( 2‚Äî4),     20%. <br><br><img src="https://habrastorage.org/files/28e/217/6a1/28e2176a164f4a06bb5f31a0564c5a51.png"><br><br><a name="7-2"></a><h3>  7.2.    </h3><br>             .       .      ,       .         . ,   ,     <a href="https://github.com/haifengl/smile">smile</a> ,         . <br><br>        ,     .         . <br><br><img src="https://habrastorage.org/files/518/c46/d5f/518c46d5f2f64931880f9d8f48cf5c75.png"><br><br><a name="8"></a><h1> 8.   </h1><br>          ‚Äî     .        ,     ,    <a href="http://hive.apache.org/">Hive</a> ‚Äî     HDFS   SQL- . <br><br><a name="9"></a><h1> 9. Future work </h1><br>   , Giraph ‚Äî   ,        ,      RBM.      ,        /            .   ,        Giraph.              ,        . <br><br><a name="10"></a><h1> 10.  </h1><br><ol><li> <a href="https://habrahabr.ru/post/159909/">    RBM</a> </li><li> <a href="https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf">  </a> </li><li> <a href="http://giraph.apache.org/"> Giraph'</a> </li><li> <a href="https://github.com/haifengl/smile">Smile</a> </li><li> Claudio Martella, Roman Shaposhnik, Dionysios Logothetis. Practical Graph Analytics with Apache Giraph.  2015 </li><li> <a href="https://github.com/AlexandrShcherbakov/RBM"> RBM  Giraph</a> </li></ol></div><p>Source: <a href="https://habr.com/ru/post/306362/">https://habr.com/ru/post/306362/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../306350/index.html">Swift Features</a></li>
<li><a href="../306352/index.html">Summer Practice: Top 10 Microsoft Virtual Academy Courses</a></li>
<li><a href="../306354/index.html">Linus Torvalds introduced the release of the Linux kernel 4.7</a></li>
<li><a href="../306356/index.html">2 + 2 * 2 = ::::::</a></li>
<li><a href="../306358/index.html">TerraServer useless service or missed opportunity?</a></li>
<li><a href="../306364/index.html">View and perspective in level design. Part two</a></li>
<li><a href="../306368/index.html">Monetizing Pokemon Go or Midas Touch</a></li>
<li><a href="../306370/index.html">Everything is under control: we protect corporate conversations. Part 2: Secure Telephone</a></li>
<li><a href="../306372/index.html">Basics of game design: 20 board games. Part Five: Munchkin, Contract Bridge, Arkham's Terror</a></li>
<li><a href="../306374/index.html">Game Education in Russia</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>