<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Probabilistic programming</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Introduction 
 This publication is the first part of a brief introduction to illustrations in probabilistic programming , which is one of the modern a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Probabilistic programming</h1><div class="post__text post__text-html js-mediator-article"><h3>  Introduction </h3><br>  This publication is the first part of a brief introduction to illustrations in <i>probabilistic programming</i> , which is one of the modern applied areas of machine learning and artificial intelligence.  While writing this publication, I was glad to find that recently there was already an <a href="http://habrahabr.ru/post/242993/">article</a> on Habrahabr <a href="http://habrahabr.ru/post/242993/">about probabilistic programming with consideration of applied examples from the field of the theory of knowledge</a> , although, unfortunately, there are still few materials on the Russian-speaking Internet on this topic. <br><br>  I, the author, Yura Perov, have been doing probabilistic programming for the past two years as part of my main educational and scientific activities.  I had a productive acquaintance with probabilistic programming when, as a student at the Institute of Mathematics and Fundamental Informatics of the Siberian Federal University, I undertook an internship at the Laboratory of Computer Science and Artificial Intelligence at the Massachusetts Institute of Technology under the guidance of Professor Joshua Tenenbaum and Dr. Vikas Mansinghi, and then continued at Fac. technical sciences of Oxford University, where at the moment I am a master's student under the guidance of  Prof. Frank Wood <br><br>  I like to define probabilistic programming as a <i>compact</i> , <i>compositional</i> way of representing <i>generating probabilistic models</i> and conducting <i>statistical inference</i> in them, taking into account data using generalized algorithms.  Although probabilistic programming does not bring much new fundamental to the theory of machine learning, this approach attracts with its simplicity: ‚Äúprobabilistic generating models to the masses!‚Äù 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  "Normal" programming </h3><br>  To get acquainted with probabilistic programming, let's first talk about ‚Äúordinary‚Äù programming.  In ‚Äúordinary‚Äù programming, the basis is an algorithm, usually deterministic, which allows us to obtain output from the input data according to well-established rules. <a name="habracut"></a><br><br><img src="https://habrastorage.org/files/f56/817/1a2/f568171a287448c181c414ef617c6c46.png"><br><br>  For example, if we have boy Vasya, and we know where he is, where he throws the ball and what are the external conditions (for example, wind power), we will know what window he unfortunately will break in the school building.  To do this, it is enough to simulate the simple laws of school physics, which can be easily written as an algorithm. <br><br><img src="https://habrastorage.org/files/e2c/8a7/161/e2c8a716153945ff8d24caba87553706.png"><br><br><h3>  And now probabilistic programming </h3><br>  However, often we only know the result, the outcome, and we are interested in finding out what unknown values ‚Äã‚Äãled to this particular result?  To answer this question using the theory of mathematical modeling, a probabilistic model is created, some of the parameters of which are not precisely defined. <br><br>  For example, in the case of the boy Vasya, knowing what window he broke, and having a priori knowledge of which window he and his friends usually play football, and knowing the weather forecast for that day, we want to know the a posteriori distribution of the boy‚Äôs location Wasi: where did he throw the ball from? <br><br><img src="https://habrastorage.org/files/39f/da6/b67/39fda6b674d44dc5b777a2c81f3c04fc.png"><br><br>  So, knowing the output, we are interested in finding out the most likely values ‚Äã‚Äãof hidden, unknown parameters. <br><br>  In the framework of machine learning are considered including <i>generating</i> probabilistic models.  In the framework of generating probabilistic models, the model is described as an algorithm, but instead of exact single-valued values ‚Äã‚Äãof hidden parameters and some input parameters, we use probability distributions on them. <br><br>  There are more than 15 probabilistic programming languages, a list with a brief description of each of them can be found <a href="http://probabilistic-programming.org/wiki/Home">here</a> .  This publication provides example code in the probabilistic <a href="http://probcomp.csail.mit.edu/venture/">Venture</a> / <a href="http://www.robots.ox.ac.uk/~fwood/anglican/">Anglican</a> languages, which have a very similar syntax and which originate from the probabilistic language <a href="http://en.wikipedia.org/wiki/Church_%2528programming_language%2529">Church</a> .  Church, in turn, is based on the "ordinary" programming language Lisp and Scheme.  An interested reader is highly recommended to familiarize themselves with the book <a href="https://www.google.com/search%3Fq%3DRussian_SICP%26oq%3DRussian_SICP%26aqs%3Dchrome..69i57%26sourceid%3Dchrome%26es_sm%3D122%26ie%3DUTF-8">‚ÄúStructure and Interpretation of Computer Programs‚Äù</a> , which is one of the best ways to begin acquaintance with the language of ‚Äúordinary‚Äù programming Scheme. <br><br><h3>  Bayesian linear regression example </h3><br>  Consider setting a simple probabilistic Bayesian linear regression model in the probabilistic programming language Venture / Anglican as a probabilistic program: <br><br><pre><code class="lisp hljs"><span class="hljs-number"><span class="hljs-number">01</span></span>: [ASSUME t1 (<span class="hljs-name"><span class="hljs-name">normal</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>)] <span class="hljs-number"><span class="hljs-number">02</span></span>: [ASSUME t2 (<span class="hljs-name"><span class="hljs-name">normal</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>)] <span class="hljs-number"><span class="hljs-number">03</span></span>: [ASSUME noise <span class="hljs-number"><span class="hljs-number">0.01</span></span>] <span class="hljs-number"><span class="hljs-number">04</span></span>: [ASSUME noisy_x (<span class="hljs-name"><span class="hljs-name">lambda</span></span> (<span class="hljs-name"><span class="hljs-name">time</span></span>) (<span class="hljs-name"><span class="hljs-name">normal</span></span> (<span class="hljs-name"><span class="hljs-name">+</span></span> t1 (<span class="hljs-name"><span class="hljs-name">*</span></span> t2 time)) noise))] <span class="hljs-number"><span class="hljs-number">05</span></span>: [OBSERVE (<span class="hljs-name"><span class="hljs-name">noisy_x</span></span> <span class="hljs-number"><span class="hljs-number">1.0</span></span>) <span class="hljs-number"><span class="hljs-number">10.3</span></span>] <span class="hljs-number"><span class="hljs-number">06</span></span>: [OBSERVE (<span class="hljs-name"><span class="hljs-name">noisy_x</span></span> <span class="hljs-number"><span class="hljs-number">2.0</span></span>) <span class="hljs-number"><span class="hljs-number">11.1</span></span>] <span class="hljs-number"><span class="hljs-number">07</span></span>: [OBSERVE (<span class="hljs-name"><span class="hljs-name">noisy_x</span></span> <span class="hljs-number"><span class="hljs-number">3.0</span></span>) <span class="hljs-number"><span class="hljs-number">11.9</span></span>] <span class="hljs-number"><span class="hljs-number">08</span></span>: [PREDICT t1] <span class="hljs-number"><span class="hljs-number">09</span></span>: [PREDICT t2] <span class="hljs-number"><span class="hljs-number">10</span></span>: [PREDICT (<span class="hljs-name"><span class="hljs-name">noisy_x</span></span> <span class="hljs-number"><span class="hljs-number">4.0</span></span>)]</code> </pre> <br>  The hidden sought-for parameters are the values ‚Äã‚Äãof the coefficients <b>t1</b> and <b>t2 of the</b> linear function <b>x = t1 + t2 * time</b> .  We have a priori assumptions about these coefficients, namely, we assume that they are distributed according to the Normal distribution law <b>Normal (0, 1)</b> with an average of 0 and a standard deviation of 1. Thus, we determined in the first two lines of the probabilistic program the prior probability for hidden variables, <b>P (T)</b> .  The <b>statement [ASSUME name expression]</b> can be viewed as a definition of a random variable with the name <b>name</b> , which takes the value of a calculated expression (program code) <b>expression</b> , which contains uncertainty. <br><br>  Probabilistic programming languages ‚Äã‚Äã(meaning specifically Church, Venture, Anglican), like Lisp / Scheme, are functional programming languages, and use Polish notation when writing expressions for computation.  This means that in the expression of the function call, the operator is first located, and only then the arguments: <b>(+ 1 2)</b> , and the function call is surrounded by parentheses.  In other programming languages, such as C ++ or Python, this will be equivalent to code <b>1 + 2</b> . <br><br>  In probabilistic programming languages, the expression of a function call can be divided into three different types: <br><ul><li>  Call of deterministic procedures <b>(primitive-procedure arg1 ... argN)</b> , which, with the same arguments, always return the same value.  Such procedures, for example, include arithmetic operations. </li><li>  Call probabilistic (stochastic) procedures <b>(stochastic-procedure arg1 ... argN)</b> , which at each call generate an element from a corresponding distribution at random.  Such a call defines a new <i>random variable</i> .  For example, a call to a probabilistic procedure <b>(normal 1 10)</b> determines a random variable distributed according to the Normal distribution law <b>Normal (1, sqrt (10))</b> , and the result of performing each time will be some real number. </li><li>  Calling compound procedures <b>(compound-procedure arg1 ... argN)</b> , where <b>compound-procedure</b> is a user-entered procedure using the special expression <b>lambda</b> : <b>(lambda (arg1 ... argN) body)</b> , where <b>body</b> is the procedure body consisting of expressions.  In the general case, a composite procedure is a stochastic (non-deterministic) composite procedure, since its body may contain probabilistic procedure calls. </li></ul><br>  Let's return to the source code in the Venture / Anglican programming language.  After the first two lines, we want to set the conditional probability <b>P (X | T)</b> , that is, the conditional probability of the observed variables <b>x1</b> , <b>x2</b> , <b>x3</b> for given values ‚Äã‚Äãof the hidden variables <b>t1</b> , <b>t2</b> and the <b>time</b> parameter. <br><br>  Before entering directly the observations themselves using the expression <b>[OBSERVE ...],</b> we define a general law for the observed variables <b>xi</b> within our model, namely, we assume that the data of the observed random variables for a given <b>t1</b> , <b>t2</b> and a given noise level noise Normal distribution <b>Normal (t1 + t2 * time, sqrt (noise))</b> with average <b>t1 + t2 * time</b> and standard deviation <b>noise</b> .  This conditional probability is defined in lines 3 and 4 of this probability program.  <b>noisy_x is</b> defined as a function that takes a <b>time</b> parameter and returns a random value, the expression <b>(normal (+ t1 (* t2 time)) noise)</b> determined by calculation and determined by the values ‚Äã‚Äãof the random variables <b>t1</b> and <b>t2</b> and the variable <b>noise</b> .  Note that the expression <b>(normal (+ t1 (* t2 time)) noise)</b> contains uncertainty, therefore, each time we calculate it, we will generally receive a different value. <br><br>  On lines 5-7, we directly enter the known values <b>x1 = 10.3</b> , <b>x2 = 11.1</b> , <b>x3 = 11.9</b> .  An instruction in the form <b>[OBSERVE expression value]</b> captures the observation that the random variable, which takes a value according to the execution of the <b>expression</b> , has assumed the value <b>value</b> . <br><br>  We will repeat at this stage all that we have done.  On lines 1‚Äì4, using the instructions of the form <b>[ASSUME ...],</b> we specified the probabilistic model directly: <b>P (T)</b> and <b>P (X | T)</b> .  On lines 5-7, we directly specified the values ‚Äã‚Äãof the observed random variables <b>X</b> known to us using instructions of the form <b>[OBSERVE ...]</b> . <br><br>  On lines 8-9, we ask the probabilistic programming system for the a posteriori distribution <b>P (T | X) of</b> hidden random variables <b>t1</b> and <b>t2</b> .  As already mentioned, with a large amount of data and rather complex models, it is impossible to obtain an accurate analytical representation, therefore instructions of the <b>[PREDICT ...] type</b> generate a sample of values ‚Äã‚Äãof random variables from the a posteriori distribution <b>P (T | X)</b> or its approximation.  An instruction of the form <b>[PREDICT expression]</b> generally generates one sample element from the values ‚Äã‚Äãof a random variable that takes a value according to the execution of the <b>expression</b> .  If instructions like <b>[OBSERVE ...]</b> are located in front of instructions like <b>[PREDICT ...]</b> , then the sample will be from the a posteriori distribution (or more precisely, of course, from the a posteriori distribution approximation) due to the previously listed observations. <br><br>  Note that at the end we can also predict the value of the function <b>x (time)</b> at another point, for example, with <b>time = 4.0</b> .  In this case, the prediction is the generation of a sample from the a posteriori distribution of a new random variable for the values ‚Äã‚Äãof hidden random variables <b>t1</b> , <b>t2</b> and the <b>time = 4.0</b> parameter. <br><br>  To generate a sample from the posterior distribution <b>P (T | X)</b> in the Venture programming language, the Metropolis-Hastings algorithm, which belongs to the Monte-Carlo methods according to the Markov circuit, is used as the main one.  The <i>generalized</i> conclusion in this case is that the algorithm can be applied to any probabilistic programs written in this probabilistic programming language. <br><br>  In the video attached below, you can look at the ongoing statistical inference in this model. <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/Xe3YSKCZfPs%3Ffeature%3Doembed&amp;xid=17259,15700002,15700022,15700186,15700190,15700253&amp;usg=ALkJrhg_IU-OrOxQfl3EsWQ0nKPjYlGiJQ" frameborder="0" allowfullscreen=""></iframe><br>  (The video shows an example based on the probabilistic programming language Venture.) <br><br>  At the very beginning we have no data, so we see an a priori distribution of straight lines.  By adding point by point (thus, data elements), we see the elements of the sample from the a posteriori distribution. <br><br>  This concludes the first part of this entry into probabilistic programming. <br><br><h3>  Materials </h3><br>  Below I will provide recommended links for those who want to learn more about probabilistic programming right now: <br><ol><li>  <a href="http://www.robots.ox.ac.uk/~fwood/anglican/">The site of the probable programming language Anglican, which is a fellow Venture and a descendant of Church</a> </li><li>  <a href="http://www.robots.ox.ac.uk/~fwood/anglican/teaching/mlss2014/index.html">Training course on the probabilistic programming language Anglican</a> . </li><li>  A probabilistic programming course read at one of the machine learning schools: <a href="https://www.youtube.com/watch%3Fv%3D6Lqt07enBGs">part 1</a> , <a href="https://www.youtube.com/watch%3Fv%3DDY5yuBNEuQs">part 2</a> and <a href="https://www.youtube.com/watch%3Fv%3Dk2Qj0e7H9aI">part 3</a> . </li><li>  <a href="http://dippl.org/">Course "Design and implementation of probabilistic programming languages."</a> </li><li>  <a href="https://probmods.org/">The course "Probabilistic generating models of knowledge (one of the applications of probabilistic programming)"</a> . </li></ol><br>  This publication is based on <a href="http://arxiv.org/abs/1601.07224">my undergraduate scientific work</a> , which was defended this summer at the Institute of Mathematics and Fundamental Informatics of the Siberian Federal University.  An interested reader will find in it a more detailed and formal introduction to probabilistic programming.  At the end there is also a complete bibliography, on the basis of which this publication was also written. </div><p>Source: <a href="https://habr.com/ru/post/244625/">https://habr.com/ru/post/244625/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../244611/index.html">How PayPal Mafia Affects Silicon Valley Success</a></li>
<li><a href="../244615/index.html">ABAP: Sampling reference values ‚Äã‚Äãby their keys from database tables</a></li>
<li><a href="../244619/index.html">Joker 2014 Java conference through the eyes of a speaker from Siberia</a></li>
<li><a href="../244621/index.html">Using the AdDuplex Service for Universal Applications</a></li>
<li><a href="../244623/index.html">non-FlexPod DC: Direct-Attached Storage, One-stop support</a></li>
<li><a href="../244627/index.html">Ethernet bandwidth measurement</a></li>
<li><a href="../244631/index.html">Horizontal scroll becomes fashionable. We will understand this</a></li>
<li><a href="../244633/index.html">Joy Inc. Dream job and business processes based on joy</a></li>
<li><a href="../244635/index.html">New free courses virtual academy Microsoft Virtual Academy, December 2014</a></li>
<li><a href="../244637/index.html">What is remembered by Digitale 6</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>