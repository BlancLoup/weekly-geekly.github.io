<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>pymorphy2</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Back in 2009, there was already an article in the Habr√©, ‚Äú Whether Kuzyavye Butyavki .. ‚Äù about pymorphy is a morphological analyzer for the Russian l...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>pymorphy2</h1><div class="post__text post__text-html js-mediator-article">  Back in 2009, there was already an article in the <a href="http://habrahabr.ru/post/49421/">Habr√©,</a> ‚Äú <a href="http://habrahabr.ru/post/49421/">Whether Kuzyavye Butyavki ..</a> ‚Äù about pymorphy is a morphological analyzer for the Russian language in Python (a thing that can bend words, communicate information about a part of speech, case, etc.) <br><br>  In 2012, I began to slowly do pymorphy2 ( <a href="https://github.com/kmike/pymorphy2">github</a> , <a href="https://bitbucket.org/kmike/pymorphy2">bitbucket</a> ) - I think it's time to introduce this library here: pymorphy2 can work hundreds of times faster than pymorphy (without using C / C ++ extensions) and still require less memory;  there are better dictionaries, better parsing quality, better support for the letter e, easier installation and a more ‚Äúhonest‚Äù API.  From the negative - not all features of pymorphy are now implemented in pymorphy2. <br><br>  This article is about how pymorphy2 was created (sometimes with rather boring technical details), and how much nonsense I did;  if you just want to try everything, you can read the <a href="http://pymorphy2.readthedocs.org/">documentation</a> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <a name="habracut"></a><br><br>  What were the problems with pymorphy: <br><br><ul><li>  sufficiently slow speed of work (several hundred words / sec at the default setting); </li><li>  dependence on dictionaries with aot.ru, which is not clear how to replenish / correct; </li><li>  some problems with the API - for example, according to the API for the supporter, it might seem that the library itself removes the ambiguity of parsing, although this is not the case; </li><li>  the letter  was supported by the ‚Äúeverywhere replace all  with e‚Äù method; </li><li>  rather complicated installation - you need to download dictionaries, some different formats, etc.  (even in Japanese, someone made a <a href="http://www.youtube.com/watch%3Fv%3Dv0kdk4B03K8">series of video lessons</a> ); </li><li>  Python 3.x was supported only in the version with bitbucket, which was never released; </li><li>  it is impossible to incline the infinitive: the verb could be put in the form of an infinitive, but it is no longer possible; </li><li>  hyphenated words are not always inclined; </li><li>  well, etc. </li></ul><br><br>  Before I did pymorphy, I never wrote in Python, I didn‚Äôt do any word processing in natural language (pymorphy was a way to make out in both) it was quite understandable that much could be done better. <br><br>  But a lot of things in pymorphy1 were nevertheless done correctly.  For example, documentation in Russian and installation without special dependencies (most importantly - without the need for a compiler);  more or less normal development process with tests;  The quality of the analysis and prediction itself was quite high.  Integration with django was also a good ‚Äúmarketing‚Äù move - with it, however, there were some conceptual problems (it‚Äôs impossible to properly deflect the word directly in the template without resolving the ambiguity of the parsing - and this was not provided for in the API). <br><br>  Despite all its shortcomings, the library (unexpectedly) turned out to be quite popular - for example, Nuggulil that pymorphy was slightly used in developing the Speech-to-Text system for the Russian language in the framework of the French <a href="http://ru.wikipedia.org/wiki/Quaero">Quaero</a> project, and is recommended as a teaching material in some universities.  I don‚Äôt think that there‚Äôs some great merit here; rather, I just ended up in the right place at the right time. <br><br>  I didn‚Äôt want to break backward compatibility for a long time and tried to develop what I had (although <a href="https://habr.com/users/buriy/" class="user_link">buriy</a> , for example, even did fork, which broke backward compatibility, but it worked faster).  The decisive impetus for writing pymorphy2 was the <a href="http://opencorpora.org/">OpenCorpora</a> project - the guys from there, among other things (and there are a lot of ‚Äúeverything else‚Äù), took the dictionary from aot.ru, completely reworked its structure and started updating and other improvements. <br><br>  So, the idea was to use a dictionary from OpenCorpora. <br><br>  In pymorphy, dictionaries from aot.ru were used, almost not processed in any way (they were converted to the sqlite format, but in fact the structure remained the same).  The ‚Äúbases‚Äù of words, separately ‚Äúprefixes‚Äù, ‚Äúendings‚Äù separately were stored separately, and the rules for the formation of words from these parts separately.  This method was good because it was possible to implement a morphological analyzer in a couple of "weekends" without much effort and research. <br><br>  But all of this ‚Äî access to the data through a multitude of wrappers and (especially) the collection of words from parts with the help of a code ‚Äî adversely affected the speed of work;  I somehow didn‚Äôt succeed in drastically improving speed with this approach, the solutions were complex and didn‚Äôt really help (note: the option <a href="https://github.com/Melkogotto/MAnalyzer">‚Äúrewrite everything as it is, but in C ++‚Äù it</a> works quickly, but it requires more memory than in pymorphy2 as a result) . <br><br>  In short, in pymorphy2 I wanted to try some other approach, since the dictionaries are new, and still, much of the vocabulary associated with them is rewritten.  At the same time, I wanted pymorphy2 to be a library library, and not just a wrapper to a pile of C / C ++ code - I wanted the parsing logic to remain on python.  There were several options for what to do. <br><br><h4>  Automatic machines </h4><br>  The first option that came to mind was to reformulate the problem in terms of finite automata.  <a href="https://habr.com/users/lightcaster/" class="user_link">lightcaster</a> described the approach well here: <a href="http://habrahabr.ru/post/109736/">habrahabr.ru/post/109736</a> .  The approach is beautiful, that is what it is. <br><br>  What confused me here: <br><br>  a) in the article I used the OpenFST C ++ library (like the most popular way to implement state machines), but forcing users to install it manually is not an option; <br>  b) even using the C ++ library, the results, judging by the article, were quite modest (2 thousand words / s against 100+ thousand words / s for mystem or lemmatizer);  It is clear that this figure could, most likely, be significantly improved (and the <a href="https://habr.com/users/lightcaster/" class="user_link">lightcaster</a> writes that he didn‚Äôt optimize anything) - but still; <br>  c) this is one of those approaches, which (in my opinion) raises the threshold of entry - I think this is rather a minus. <br><br>  The result was that I would need: to figure out how to optimize the code and why it happens so slowly even with the C ++ library;  write an easier to install wrapper for OpenFST (or use another FST implementation - for example, make one's own) + make a small part of the implementation of OpenFST (or just an FST implementation) in Python (so that pymorphy could be used without a compiler), well, to formulate all the algorithms in terms of finite automata. <br><br>  A few years ago, <a href="https://habr.com/users/lightcaster/" class="user_link">lightcaster</a> sent me another sketch of the implementation of pymorphy on finite automata (pythonium, without C extensions, I didn‚Äôt understand anything in it then :) - the sketch eventually worked at a speed of about 2 thousand words / sec and required about 300MB of memory.  All this, on the one hand, not bad, but on the other - not very inspiring after all;  it was clear that if this method was solved "in the forehead", it would not work very quickly, and that it was necessary to write much better than a person who understands the topic.  In short, it seemed that this is a lot of work and an unguaranteed result (+ there will be other architectural considerations, closer to the end of the article will be).  Perhaps it seemed wrong, who knows - I have not tried, only excuses. <br><br>  This way (to consider the task solely as an automaton problem) I did not go.  Although - how to say, it all depends on the point of view :) <br><br><h4>  Let's copy mystem </h4><br>  The second option was to implement approximately what is described in the <a href="http://download.yandex.ru/company/iseg-las-vegas.pdf">publication</a> on <a href="http://company.yandex.ru/technologies/mystem/">mystem</a> .  I do not know if mystem uses the same algorithms now, but the method described in the article is quite reasonable.  Many good things are written about mystem, so at first I tried to implement in pymorphy2 something similar to what is described in the publication. <br><br><div class="spoiler">  <b class="spoiler_title">The essence of the method (as I understood it)</b> <div class="spoiler_text"> a) We have a list of all the words of the Russian language, and for each word there is a ‚Äúdeclination model‚Äù (‚Äúparadigm‚Äù) - some kind of template by which you can build other forms of this word.  Well, for example, for many nouns you can add the letter ‚Äúand‚Äù at the end to get the plural of the nominative case, and ‚Äúam‚Äù - the instrumental.  Most of the implementations of Russian morphology are built on this principle. <br><br>  b) A prefix tree (trie) is built for all inverted ‚Äúendings‚Äù of words.  The ‚Äúending‚Äù is taken about the same as that of aot.ru - the part of the word on the right, which changes with declination.  Well, for example, the word ‚Äúhamsters‚Äù may have ‚Äúam‚Äù - add ‚Äúima‚Äù to the trie. <br><br>  c) a prefix tree (trie) is built for all inverted ‚Äúbases‚Äù of a word.  For example, for the word "hamsters" is "kyamoh."  In addition, at the end of the "basis" are assigned (through a separator) the possible indices of the models of declination (paradigm) for this basis.  For example, if a ‚Äúhamster‚Äù can be declined according to the paradigms A and B, and the separator is chosen $, then in the trie we add the values ‚Äã‚Äã‚Äúkyamoh $ A‚Äù and ‚Äúkyamoh $ B‚Äù.  In the article on mystem, not one tree was used for the foundations, but many (my opinion as a measure of optimization) - but this does not affect the algorithm much, we will assume that the tree is one. <br><br>  d) analysis of the word itself: we follow the word from the end, collect all possible ‚Äúendings‚Äù from the first trie (in the prefix tree, this operation is performed in O (1) of the number of words in trie and in O (N) of the word length).  Suppose for ‚Äúhamsters‚Äù we could find that there are words that have an inflectional ending "" (empty), "and", "ami" or "kami."  These "endings" correspond to the "basics": hamsters, hamsters, hamster and hamster. <br><br>  d) take the shortest found basis and look for it in the second trie (go to the $ delimiter).  If found, then besides the very basis we immediately get all possible indexes of the word declension patterns (they are behind the separator).  For each of the models of declension, it is now possible to check whether there is a form with the ending we need (found in step (d)) - if there is, then we have made out the word. <br><br>  f) if you find nothing, then there is no dictionary word with the shortest base - you can go to a longer basis;  you can also check similar fundamentals (I did not understand what was being done from paper, but this is not the point - for example, you can try the next, longer basis; after all possible fundamentals have been tested, you can start looking for similar words - it works) . <br><br>  The paper also describes various heuristics that can reduce the size of the dictionary and remove unlikely predictions. <br><br>  I could misinterpret or misunderstand;  if you need to do something, then it is better to read the original article, of course. <br><br></div></div><br><br><h5>  Trie </h5><br>  The method uses prefix trees (Trie) - to implement the method, you need a Trie implementation for Python.  I didn‚Äôt want to use a python implementation (I was worried about speed and gluttony from memory), but I was not surprised to find ready python wrappers for some good C / C ++ implementation of Trie. <br><br>  It‚Äôs rare to make sense to write your own C / C ++ implementation of data structures: there are a lot of things already in C / C ++, and many of the libraries implement state-of-art algorithms.  Why?  That person came up with a clever data structure, wrote an article about this in a scientific journal.  So that the results could be repeated, the author often lays out the implementation, and more often - in C or C ++.  If he does not post it, then somebody else reads the paper and writes the implementation - also in C / C ++ (ok, sometimes this is written in Java). <br><br>  And still, pymorphy2 is a hobby project, and it would not have been possible to spend a lot of time on it;  it was better to try to spend time efficiently and reinvent smaller bikes.  In short, I decided to take a ready-made implementation of Trie and make a wrapper for it on Cython (a separate package that is not related to pymorphy2). <br><br>  There are 2 big advantages in this approach: <br><br>  1) The data structure can be used for other purposes;  even if the approach did not justify itself (spoiler: it did, it didn‚Äôt justify itself), the efforts would not have been wasted; <br>  2) the complexity associated with data structures is ‚Äúdragged away‚Äù from the morphological analyzer itself;  the analyzer communicates with Trie through a simple API, while he himself is engaged only in the actual analysis of words.  To understand the algorithm of work, it is not necessary to understand in detail how the prefix tree is arranged.  In addition, replacing the implementation of basic data structures (for example, python) is a simple task;  we have a clearly defined border (= interface) between the morphs.  analyzer and storage for words. <br><br>  At first I liked the <a href="http://linux.thai.net/~thep/datrie/datrie.html">libdatrie</a> library, I <a href="http://linux.thai.net/~thep/datrie/datrie.html">wrote</a> about the wrapper for it here: <a href="http://habrahabr.ru/post/147963/">habrahabr.ru/post/147963</a> .  Some things in the library had to be fixed (it usually came out like this - I wrote the working implementation in C, the library author threw everything out and wrote more idiomatic C code for the same thing - and did it right, because the C code was really better for him :);  in the end it turned out quite a usable wrapper. <br><br>  With the datrie and the ‚Äústeal everything from the mystem!‚Äù Approach, it turned out 20-60 thousand words / sec (without a predictor and with a minimum of tying; I do not remember such a spread, I guess the weather), and it all took about 30MB of memory. <br><br>  With speed, the plug was in an unexpected (for me) place: the comparison of permissible analyzes for the ‚Äúendings‚Äù of a word and permissible analyzes for the ‚Äúbeginnings‚Äù of words (this is part of the algorithm) was the most inhibited.  In my implementation, it came down to finding the intersection of 2 sorted lists of numbers.  The obvious way (‚Äúto go through both lists in parallel‚Äù) turned out to be much slower in this task than the ‚Äúgo short list‚Äù approach and in the long look for a narrowing binary search ‚Äù.  But this second option remained a bottleneck, even rewritten in Cython - what to do with it, I did not know.  Probably, it was possible to write this code better, but it did not work right away. <br><br>  In addition, 30Mb is, on the one hand, good, but on the other - aot.ru is smaller (they write that 9Mb, then 19Mb - we will assume that 9, my hands did not reach me).  Memory consumption is important because  pymorphy is also used for firing sparrows (the declination of words on the site), and this requires loading it into each web process.  Well, or if you do not load (due to the memory being pitiful), then you should not make a fuss with a separate service (on some zeromq, or with http communication) - which is not desirable either. <br><br>  Implementing a datrie is not a ‚Äúnaive‚Äù trie on pointers; a datrie already requires 2 times less memory than a regular prefix tree (so 30MB is still good). <br><br>  Plus, somehow everything was complicated, in terms of algorithms;  it was clear that if you continue to finish (predictor, heuristics), then everything will be much slower and even more difficult.  Disorder. <br><br><h5>  marisa-trie </h5><br>  But I decided not to give up and not to abandon the ‚Äúa la mystem‚Äù approach, but try to replace the datrie with something else (which was rather silly, but had good consequences).  The choice fell on the C ++ library <a href="https://code.google.com/p/marisa-trie/">marisa-trie</a> , which was written by the guru of data structures Susumu Yata.  For her, I also made a <a href="https://github.com/kmike/marisa-trie">python wrapper</a> with about the same interface as datrie. <br><br>  When I started testing marisa-trie, I first had my eyes on my forehead.  See: <br><br><ul><li>  If you take and load all 3 million Russian words into the Python dictionary, it will take about 600MB of RAM (in the list - about 300MB); </li><li>  if you load the same data into a datrie, then it will take 70MB of RAM - which is already pretty cool; </li><li>  and if you load the same data into a MARISA-Trie, then it will take another 10 times less - 7 MB of memory. </li></ul><br><br>  How is it going?  The fact is that MARISA-Trie is in fact not a Trie at all :) What I mean is that I didn‚Äôt understand so normally;  the folder with the "meat" in the source is called <a href="http://en.wikipedia.org/wiki/Grimoire">grimoire</a> ;  The description of the algorithm is exclusively in Japanese. <br><br>  Apparently, MARISA-Trie is the ‚Äúsuccinct‚Äù implementation of Patricia-Trie ( <a href="http://en.wikipedia.org/wiki/Radix_tree">wiki</a> ), in which not only textual information can be compared to each node, but also the next level of MARISA-Trie.  ‚ÄúSuccinct‚Äù means that the tree structure is encoded with a bitmap, which allows you to spend very little memory on it.  Well, there is an interesting feature: MARISA Trie also appears immediately in the role of ‚Äúperfect hash‚Äù.  Each key is assigned a unique number from the range of 0 to len (trie) -1, and each number from 0 to len (trie) -1 corresponds to a single key;  You can either get a key by number or get a number by key. <br><br>  By speed - datrie faster than marisa-trie, every 10. <br><br>  Let's go back to pymorphy.  After a simple replacement of the datrie by marisa-trie, it turned out that the whole farm takes up 5MB of memory, and it works at a speed of 2-5 thousand words / sec (without a predictor). <br><br>  On the one hand - 5MB is cool (although so far without a predictor).  But on the other - 2‚Äì5 thousand words / sec - slowly after 20‚Äì60 thousand words / sec, + I didn‚Äôt really like to get tied up on a marisa-trie, because  this would lead to a compiler requirement for installing pymorphy2.  I fully understood how datrie works, and writing a python-compatible implementation would not be a problem, but marisa-trie ... In the case of marisa-trie, it would be possible to port, but it would require more effort, and it would most likely work this question would probably be ‚Äúpostponed‚Äù, and pymorphy2 would require a compiler to be installed. <br><br>  By itself, the attempt was a dead end, but revealed one interesting thing: 5MB (‚Äúalmost mystem‚Äù algorithm + marisa-trie) and 7MB (‚Äúwe put all the words in marisa-trie‚Äù) - the numbers are very comparable.  These numbers finally opened my eyes to the fact that you should not discard approaches, in which all words will be loaded into memory immediately and completely (without splitting into beginnings and ends). <br><br>  Intuitively: if you keep all words in memory ‚Äúas is‚Äù, then you will need to do fewer calculations, words will not need to be assembled from pieces - everything should work faster and the code should be easier. <br><br>  At this point, the idea of ‚Äã‚Äã‚Äúcopying the mystem‚Äù I stopped;  unfinished options with datrie and marisa-trie are available in early pymorphy2 commits. <br><br>  I have a suspicion that mystem uses (well, according to the publication, I cannot know what is being used there now) several trie with parts of words, not because they make it somehow easier to describe algorithms for the predictor, but simply for In order not to keep all words in memory in full.  According to my tests, with the usual trie it would take (words + data about their paradigms) 300-400 megabytes, with a 200 megabyte with datrie, but with MARISA-Trie everything could hold a megabyte of 20. Use a few "regular" trie, as in mystem paper, it‚Äôs a good trick, but I think this way (I can be wrong, I don‚Äôt finish the ‚ÄúI want my mystem‚Äù approach), it works slower in principle than ‚Äúwe keep all words as they are‚Äù. <br><br><h4>  Back to the Future </h4><br>  In itself, MARISA-Trie did not fit to keep all the words in memory: it was not fast - which eliminated the potential gain in speed from the fact that the words would not need to be collected piece by piece, + the API lacked the possibility of ‚Äústep by step‚Äù navigating the tree in an arbitrary way, which was necessary for I-no-no-remember-for-what, probably, for proper support of the letter .  Then there was a vague recollection of the fact that in aot.ru all the words were somehow remembered at once. <br><br>  So, I began to re-read the <a href="http://aot.ru/docs/sokirko/Dialog2004.htm">description of the</a> morphological analyzer with aot.ru, and in particular - the paragraph ‚ÄúBinary dictionary representation‚Äù.  Returned, so to speak, to where it all began. <br><br>  In an article on aot.ru, everything is called ‚Äúautomaton‚Äù. <br><br>  The variant with the ‚Äúalienable‚Äù data structure (which can be used for something else - well, like a datrie or marisa-trie) I architecturally liked (and like) a much more specialized automaton with a dictionary.  A specialized automaton would be ‚Äúburied‚Äù in pymorphy2 - from there nothing could be reused, it could be debugged only within the framework of pymorphy, and the complexity would be accumulated within the framework of pymorphy.  I usually ran through the eyes of that paragraph, I thought, ‚Äúhm-hm, not good‚Äù and thought out a way to do something different. <br><br>  But things can be viewed from different angles.  Probably, this should have been obvious from the very beginning, but only after I tried to make a ‚Äúmystem clone‚Äù it came to me that the automaton used in aot is in fact exactly the same as the <a href="https://en.wikipedia.org/wiki/Deterministic_acyclic_finite_state_automaton">DAWG</a> data structure in which words are written as keys (with annotations attached to the end).  And that all the operations described there, fall perfectly on the same API that was in datrie and marisa-trie. <br><br>  That's really true: ‚ÄúI‚Äôm not sure about‚Äú cache invalidation, ‚Äùbut I‚Äôve got a lot of‚Äú naming things ‚Äùhere. <br><br>  You seem to yourself a complete idiot at such moments;  everything was simple, everything was before our eyes from the very beginning, and they talked to me about it many times (both about automata and DAWG). <br><br>  So, the new plan was: on the beaten path <br><br><ol><li>  find DAWG implementation in C / C ++; </li><li>  make a wrapper for it with the same API as datrie and marisa-trie; </li><li>  write all words in DAWG (immediately with information about their grammatical forms) </li><li>  Bonus - make a python library exclusively for reading DAWGs, to simplify installation. </li></ol><br><br>  Morphological analysis of vocabulary words in this case is simply to get information about a word from the DAWG;  in the simplest case (assuming that a word can have only 1 parsing, and that only grammatical information is needed), it can be one line of code. <br><br>  A good library for DAWG ( <a href="https://code.google.com/p/dawgdic/">dawgdic</a> ) was found in the same Susumu Yata, who wrote marisa-trie;  I made a python wrapper for it: <a href="https://github.com/kmike/DAWG">github.com/kmike/DAWG</a> and a python "reader" of this format: <a href="https://github.com/kmike/DAWG-Python">github.com/kmike/DAWG-Python</a> (for installation which does not need a compiler). <br><br>  The entire dictionary without annotations occupied about 3Mb in DAWG, and with annotations (= with information about how to parse each word) - about 7Mb.  For comparison, if you load all this data into the pythonium dictionary "as is", then it takes several gigabytes of memory. <br><br>  Further it was necessary only to write pymorphy2 :) <br><br>  The article and so it turns out some kind of infinite, but about how the pymorphy2 inside is arranged now - until almost nothing happened.  So as not to inflate the text even more, nothing will happen;  how pymorphy2 is arranged inside is rather (unnecessarily?) described in detail in the <a href="http://pymorphy2.readthedocs.org/">documentation</a> , even with pictures.  I would still note that pymorphy2 is not a Python lemmatizer clone, everything is different there, but the information from <a href="http://aot.ru/">aot.ru</a> , of course, helped a lot. <br><br>  Here are two funny benchmarks (1.8 Ghz notebook processor, under Python 3.3 using DAWG and under PyPy 1.9 using DAWG-Python): <br><br><pre><code class="hljs pgsql">Memory <span class="hljs-keyword"><span class="hljs-keyword">usage</span></span>: <span class="hljs-number"><span class="hljs-number">13.8</span></span>M <span class="hljs-keyword"><span class="hljs-keyword">dictionary</span></span>, <span class="hljs-number"><span class="hljs-number">25.4</span></span>M total (<span class="hljs-keyword"><span class="hljs-keyword">load</span></span> <span class="hljs-type"><span class="hljs-type">time</span></span> <span class="hljs-number"><span class="hljs-number">0.14</span></span>s) py33 runtests: commands[<span class="hljs-number"><span class="hljs-number">1</span></span>] benchmarking MorphAnalyzer(): morph.parse(w) <span class="hljs-number"><span class="hljs-number">29950</span></span> words/sec morph.parse(w) <span class="hljs-number"><span class="hljs-number">47474</span></span> words/sec (considering word frequencies) morph.word_is_known(w) <span class="hljs-number"><span class="hljs-number">244816</span></span> words/sec [p.normal_form <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> p <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> morph.parse(w)] <span class="hljs-number"><span class="hljs-number">27807</span></span> words/sec [p.normalized <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> p <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> morph.parse(w)] <span class="hljs-number"><span class="hljs-number">18231</span></span> words/sec [p.lexeme <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> p <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> morph.parse(w)] <span class="hljs-number"><span class="hljs-number">3421</span></span> words/sec [{<span class="hljs-string"><span class="hljs-string">'NOUN'</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> p.tag <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> p <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> morph.parse(w)] <span class="hljs-number"><span class="hljs-number">23862</span></span> words/sec [p.tag.POS == <span class="hljs-string"><span class="hljs-string">'NOUN'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> p <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> morph.parse(w)] <span class="hljs-number"><span class="hljs-number">22157</span></span> words/sec morph.tag(w): <span class="hljs-number"><span class="hljs-number">96342</span></span> words/sec (considering word frequencies) morph.tag(w): <span class="hljs-number"><span class="hljs-number">46767</span></span> words/sec morph.tag(w): <span class="hljs-number"><span class="hljs-number">46935</span></span> words/sec (umlauts removed <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> <span class="hljs-keyword"><span class="hljs-keyword">input</span></span>) morph.tag(w): <span class="hljs-number"><span class="hljs-number">36331</span></span> words/sec (str(tag) <span class="hljs-keyword"><span class="hljs-keyword">called</span></span>) benchmarking MorphAnalyzer(result_type=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>): morph.parse(w) <span class="hljs-number"><span class="hljs-number">35088</span></span> words/sec morph.parse(w) <span class="hljs-number"><span class="hljs-number">62431</span></span> words/sec (considering word frequencies)</code> </pre> <br><br><pre> <code class="hljs pgsql">Memory <span class="hljs-keyword"><span class="hljs-keyword">usage</span></span>: <span class="hljs-number"><span class="hljs-number">40.5</span></span>M <span class="hljs-keyword"><span class="hljs-keyword">dictionary</span></span>, <span class="hljs-number"><span class="hljs-number">84.8</span></span>M total (<span class="hljs-keyword"><span class="hljs-keyword">load</span></span> <span class="hljs-type"><span class="hljs-type">time</span></span> <span class="hljs-number"><span class="hljs-number">0.39</span></span>s) pypy runtests: commands[<span class="hljs-number"><span class="hljs-number">1</span></span>] benchmarking MorphAnalyzer(): morph.parse(w) <span class="hljs-number"><span class="hljs-number">41360</span></span> words/sec morph.parse(w) <span class="hljs-number"><span class="hljs-number">108858</span></span> words/sec (considering word frequencies) morph.word_is_known(w) <span class="hljs-number"><span class="hljs-number">243742</span></span> words/sec [p.normal_form <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> p <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> morph.parse(w)] <span class="hljs-number"><span class="hljs-number">51728</span></span> words/sec [p.normalized <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> p <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> morph.parse(w)] <span class="hljs-number"><span class="hljs-number">37551</span></span> words/sec [p.lexeme <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> p <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> morph.parse(w)] <span class="hljs-number"><span class="hljs-number">14612</span></span> words/sec [{<span class="hljs-string"><span class="hljs-string">'NOUN'</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> p.tag <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> p <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> morph.parse(w)] <span class="hljs-number"><span class="hljs-number">44878</span></span> words/sec [p.tag.POS == <span class="hljs-string"><span class="hljs-string">'NOUN'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> p <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> morph.parse(w)] <span class="hljs-number"><span class="hljs-number">45129</span></span> words/sec morph.tag(w): <span class="hljs-number"><span class="hljs-number">133086</span></span> words/sec (considering word frequencies) morph.tag(w): <span class="hljs-number"><span class="hljs-number">60049</span></span> words/sec morph.tag(w): <span class="hljs-number"><span class="hljs-number">62567</span></span> words/sec (umlauts removed <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> <span class="hljs-keyword"><span class="hljs-keyword">input</span></span>) morph.tag(w): <span class="hljs-number"><span class="hljs-number">52632</span></span> words/sec (str(tag) <span class="hljs-keyword"><span class="hljs-keyword">called</span></span>) benchmarking MorphAnalyzer(result_type=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>): morph.parse(w) <span class="hljs-number"><span class="hljs-number">60777</span></span> words/sec morph.parse(w) <span class="hljs-number"><span class="hljs-number">124226</span></span> words/sec (considering word frequencies)</code> </pre><br><br>  It is amusing here that I find that (a) the numbers are very similar, despite the fact that the ‚Äúinside‚Äù actions are very different (C ++ wrapper + interpreter vs jit compiler), and (b) that PyPy is faster. <br><br>  By itself, the implementation of DAWG in C ++ (if used from python, considering Cython wrappers) is several times faster than DAWG-Python under PyPy, and early versions of pymorphy2 (which made everything less) were faster under CPython.  Over time, the features became larger, the code was more complicated, pymorphy2 was slower (earlier versions and 200 + thousand words / sec could be disassembled - though not very well and not very convenient);  the faster basic data structure "outweigh" has ceased - and now it works faster under PyPy pymorphy2.  On the other hand, to speed up the CPython version is clear how to rewrite something else on Cython (by the way: I do not plan to do this);  with PyPy it's not so obvious. <br><br>  You may notice that the analysis of 100 thousand words / sec in the real text is quite possible to get with Python 3.3 (obtaining grammatical information), and with PyPy (full analysis, with the initial form and user-friendly API).  For comparison: the ‚Äúreal‚Äù mystem (written, apparently, in C ++) worked (according to my tests sometime in the early stages) one and a half to two times faster and demanded as much memory;  from this I made a conclusion for myself - perhaps wrong, - that the mystem still uses the approach with several trie;  if the words were stored entirely, the C ++ implementation would have to be torn off even if mystem and something else makes it tricky.  Well this is so, again, nothing backed chatter. <br><br>  If neither PyPy nor C ++ implementation of DAWG is used, pymorphy2 will still work many times faster (by estimates - a couple of dozen times) than pymorphy1 with all included accelerations - well, it's better to disassemble. <br><br><h4>  How can I help </h4><br>  An interesting question remains - what to do with pymorphy1.  There now there are features that are not in pymorphy2 (for example, integration with django, matching words with numbers and declining surnames), but in the version on the beatbeat I broke backward compatibility.  To release another backward incompatible version of pymorphy is somehow stupid :) so everything hangs out in limbo.  I have absolutely no hands on everything. <br><br><ul><li>  It would be great if someone would help with porting the missing features from pymorphy1; </li><li>  any help from OpenCorpora is help and pymorphy2 too; </li><li>  There are a lot of open bugs in the <a href="https://github.com/kmike/pymorphy2/issues%3Fstate%3Dopen">tracker</a> (various degrees of hardcore); </li><li>  A very primitive tokenizer is now built into pymorphy2 - you can think up a better tokenizer;  it‚Äôs still a good idea to understand how classes from pymorphy2.units.by_shape.py relate to tokenization; </li><li>  CLI can be improved so that pymorphy2 can be used as a full command line utility; </li><li>  you can try to write more predictors for pymorphy2 (see how the current ones are made in pymorphy2.units) - you need more eyes to see if the interface is good for expansion (+ maybe you can improve parsing and add something new to the standard distribution); </li><li>  well, you can just start using pymorphy2. </li></ul><br><br>  A large number of people took part in the development of pymorphy, many of whom actually made a serious contribution, and not all of this was transferred to pymorphy2;  I would not want work to be lost (I think that it will not be lost).  Without this help, I would never have had the motivation to support pymorphy, nor the motivation to rewrite everything as pymorphy2. <br><br>  Yes, and in pymorphy2 several people have already made a serious contribution, by the way :) <br><br><h4>  Links </h4><br><ul><li>  Source code: <a href="https://github.com/kmike/pymorphy2">github</a> , <a href="https://bitbucket.org/kmike/pymorphy2">bitbucket</a> </li><li>  Documentation: <a href="http://pymorphy2.rtfd.org/">pymorphy2.rtfd.org</a> </li><li>  Bug tracker: <a href="https://github.com/kmike/pymorphy2/issues">github.com/kmike/pymorphy2/issues</a> </li><li>  Talk / Support: <a href="https://groups.google.com/forum/%3Ffromgroups">Google Group</a> </li><li>  Python data structures generated by the process: <a href="https://github.com/kmike/DAWG">DAWG</a> , <a href="https://github.com/kmike/DAWG-Python">DAWG-Python</a> , <a href="https://github.com/kmike/marisa-trie">marisa-trie</a> , <a href="https://github.com/kmike/datrie">datrie</a> , <a href="https://github.com/kmike/hat-trie">hat-trie</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/176575/">https://habr.com/ru/post/176575/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../176561/index.html">Mobilefest 2013 - Short Report</a></li>
<li><a href="../176567/index.html">Cube U30GT2 - Budget Quad-Core Tablet</a></li>
<li><a href="../176569/index.html">65 statistics from the world of mobile games to impress friends</a></li>
<li><a href="../176571/index.html">The three most hated things in computers</a></li>
<li><a href="../176573/index.html">Premiere of the domestic fan-movie on the game "Half-life 2"</a></li>
<li><a href="../176577/index.html">Distributed attack on WordPress sites</a></li>
<li><a href="../176579/index.html">Open Java project for those who are looking for first development experience</a></li>
<li><a href="../176581/index.html">Want to block common passwords? Sorry, it's patented.</a></li>
<li><a href="../176583/index.html">The digest of news from the world of mobile development for the last week ‚Ññ9 (April 8 - 14, 2013)</a></li>
<li><a href="../176585/index.html">Flying Phone - 2. Stage Phone</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>