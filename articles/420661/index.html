<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Continuous Cloud Infrastructure</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Demonstrate the use of open source tools, such as Packer and Terraform, for the continuous delivery of infrastructure changes to the users' favorite c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Continuous Cloud Infrastructure</h1><div class="post__text post__text-html js-mediator-article">  Demonstrate the use of open source tools, such as Packer and Terraform, for the continuous delivery of infrastructure changes to the users' favorite cloud environment. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/FJvl13EYtBU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  The material is based on Paul Stack‚Äôs speech at our <a href="https://devoops.ru/">DevOops</a> 2017 autumn conference <a href="https://devoops.ru/">.</a> Paul is an infrastructure developer who used to work at HashiCorp and participated in the development of tools used by millions of people (for example, Terraform).  He often speaks at conferences and communicates practice from the leading edge of CI / CD implementations, the principles of proper organization of operations-parts, and is able to clearly communicate why administrators do this at all.  Further in the article the narration is conducted in the first person. <br><a name="habracut"></a><br>  So let's start right away with a few key findings. <br><br><h3>  Long-lasting servers - sucks </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/7f5/1dc/232/7f51dc232bf87288dc7f5c1bf7140a0d.jpg">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      I previously worked in an organization where we deployed Windows Server 2003 back in 2008, and today they are still in production.  And such a company is not alone.  Using remote desktop on these servers, they install software on them manually by downloading binary files from the Internet.  This is a very bad idea, because the servers are not typical.  You cannot guarantee that the same thing happens in production as in your development environment, in an intermediate environment, in a QA environment. <br><br><h3>  Immutable infrastructure </h3><br>  In 2013, an article appeared in Chad Foiler‚Äôs blog ‚ÄúThrow away your servers and burn the code: unchanging infrastructure and disposable components‚Äù (Chad Foiler <a href="http://chadfowler.com/2013/06/23/immutable-deployments.html">‚ÄúTrash your servers and burn your infrastructure and disposable components‚Äù</a> ).  This is a conversation mostly about the fact that unchanging infrastructure is the way forward.  We have created an infrastructure, and if we need to change it, we create a new infrastructure.  This approach is very common in the cloud, because here it is fast and cheap.  If you have physical data centers, it is a bit more complicated.  Obviously, if you run data center virtualization, everything becomes easier.  However, if you still run physical servers every time, it takes a little longer to enter a new one than to change an existing one. <br><br><h3>  One-time infrastructure </h3><br>  According to functional programmers, ‚Äúunchangeable‚Äù is actually the wrong term for this phenomenon.  Because to be truly immutable, your infrastructure needs a read-only file system: no files will be written locally, no one will be able to use SSH or RDP, etc.  Thus, it seems that the infrastructure is actually not immutable. <br><br>  Terminology has been discussed on Twitter for six or even eight days by several people.  As a result, they agreed that ‚Äúone-time infrastructure‚Äù is a more appropriate formulation.  When the life cycle of a ‚Äúone-time infrastructure‚Äù ends, it can be easily destroyed.  You do not need to hold on to it. <br><br>  I will give an analogy.  Cows on farms are usually not considered as pets. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/be9/a17/baf/be9a17baf223a45e34c63684197cc2ce.png"><br><br>  When you have cattle on your farm, you do not give it individual names.  Each individual has a number and tag.  Same with servers.  If you still have production servers manually created in 2006, they have meaningful names, for example, ‚ÄúSQL database on production 01‚Äù.  And they have a very specific meaning.  And if one of the servers falls, hell begins. <br><br>  If one of the animal herds dies, the farmer simply buys a new one.  This is the "one-time infrastructure." <br><br><h3>  Continuous delivery </h3><br>  So how to combine this with Continuous Delivery? <br>  Everything that I am talking about now exists for quite some time.  I'm just trying to combine the ideas of infrastructure development and software development. <br><br>  Software developers have been practicing continuous delivery and continuous integration for a long time.  For example, Martin Fowler ( <a href="https://twitter.com/martinfowler">Martin Fowler</a> ) wrote about continuous integration in his blog in the early 2000s.  Jez Humble has been pushing for a continuous supply for a long time. <br><br>  If you take a closer look, there is nothing created specifically for the software source code.  There is a standard definition from Wikipedia: <i>continuous delivery is a set of practices and principles aimed at creating, testing and releasing software as quickly as possible</i> . <br><br>  The definition does not speak about web applications or APIs, it is about software as a whole.  Creating a working software requires many pieces of the puzzle.  In this way, you can also practice continuous delivery for infrastructure code in the same way. <br><br>  Development of infrastructure and applications are pretty close directions.  And people who write application code also write infrastructure code (and vice versa).  These worlds are beginning to unite.  There is no longer such a separation and specific traps of each of the worlds. <br><br><h3>  Continuous Delivery Principles and Practices </h3><br>  Continuous delivery has a number of principles: <br><br><ul><li>  The software release / deployment process must be repeatable and reliable. <br></li><li>  Automate everything! <br></li><li>  If a procedure is difficult or painful, do it more often. <br></li><li>  Keep everything in a version control system. <br></li><li>  Made - means "released". <br></li><li>  Integrate work with quality! <br></li><li>  Everyone is responsible for the release process. <br></li><li>  Increase continuity. <br></li></ul><br>  But more importantly, continuous delivery has four practices.  Take them and transfer directly to the infrastructure: <br><br><ul><li>  Create binary files only once.  Create your server once.  Here we are talking about ‚Äúdisposability‚Äù from the very beginning. <br></li><li>  Use the same deployment mechanism in each environment.  Do not practice different deploy in development and production.  You must use the same path in each environment.  It is very important. <br></li><li>  Test your patch.  I have created many applications.  I created a lot of problems because I did not follow the deployment mechanism.  You should always check what happens.  And I'm not saying that you should spend five or six hours on extensive testing.  Enough "smoke test".  You have a key part of the system that, as you know, allows you and your company to make money.  Do not be lazy to start testing.  If you do not do this, there may be interruptions that will cost your company money. <br></li><li>  And finally, the most important thing.  If something breaks, stop and fix it immediately!  You cannot let the problem grow and get worse and worse.  You need to fix this.  This is really important. <br></li></ul><br>  Has anyone read the book <a href="https://www.amazon.com/Continuous-Delivery-Deployment-Automation-Addison-Wesley/dp/0321601912">"Continuous Delivery"</a> ? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cad/9eb/b6f/cad9ebb6f6c3562724d78ef5a0454e7e.png"><br><br>  I am sure your companies will pay you a copy that you can transfer within the team.  I am not saying that you should sit down and spend the day off reading it.  If you do, you probably want to leave IT.  But I recommend periodically learning small pieces of a book, digesting them and thinking about how to transfer this to your environment, to your culture and to your process.  One small piece at a time.  Because continuous delivery is a conversation about continuous improvement.  It‚Äôs not easy to sit in the office with your colleagues and boss and start a conversation with the question: ‚ÄúHow will we introduce a continuous supply?‚Äù, Then write 10 things on the blackboard and in 10 days understand that you implemented it.  It takes a lot of time, causes a lot of protests, as the culture changes as it is introduced. <br><br>  Today we will use two tools: Terraform and Packer (both developed by Hashicorp).  A further story will be about why we should use Terraform and how to integrate it into our environment.  I do not accidentally talk about these two tools.  Until recently, I also worked at Hashicorp.  But even after I left Hashicorp, I still contribute to the code of these tools, because I actually find them very useful. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5d0/4b0/807/5d04b08076e6d28da7cc17589d79de6d.png"><br><br>  Terraform supports interaction with providers.  Providers are clouds, Saas services, etc. <br><br>  Within each cloud service provider there are several resources, such as a subnet, VPC, load balancer, etc. With DSL (domain-specific language), you specify Terraform what your infrastructure will look like. <br><br>  Terraform uses graph theory. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1c7/142/7d2/1c71427d2d18431acfe78d79797f9948.png"><br><br>  You probably know graph theory.  Nodes are parts of our infrastructure, such as a load balancer, subnet, or VPC.  Ribs are the relationships between these systems.  This is all that I personally consider necessary to know about graph theory for using Terraform.  The rest is left to the experts. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8de/db7/fe6/8dedb7fe6a72bd36d371f3576e841cfa.png"><br><br>  Terraform actually uses a directed graph, because it knows not only the relationship, but also their order: that A (suppose A - VPC) must be set before B, which is a subnet.  And B must be created before C (instance), because there is a prescribed order for creating abstractions in Amazon or any other cloud. <br>  Further information on this topic is available on <a href="https://youtu.be/Qv6kFFEAQhM">YouTube by Paul Henze</a> (Paul Hinze), who still works at Hashicorp as infrastructure director.  The link is a great conversation about infrastructure and graph theory. <br><br><h3>  Practice </h3><br>  Write the code, it is much better than discussing the theory. <br><br>  I previously created AMI (Amazon Machine Images).  To create them, I use Packer and I'm going to show you how to do it. <br><br>  AMI is an instance of a virtual server in Amazon, it is predefined (in terms of configuration, applications, etc.) and is created from an image.  I love that I can create new AMIs.  In essence, AMI are my Docker containers. <br><br>  So, I have an AMI, they have an ID.  Going to the Amazon interface, we see that we have only one AMI and nothing more: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4cb/b65/563/4cbb65563af0eea258e8c5cdc936a318.png"><br><br>  I can show you what is in this AMI.  Everything is very simple. <br><br>  I have a JSON file template: <br><br><pre><code class="javascript hljs">{ <span class="hljs-string"><span class="hljs-string">"variables"</span></span>: { <span class="hljs-string"><span class="hljs-string">"source_ami"</span></span>: <span class="hljs-string"><span class="hljs-string">""</span></span>, <span class="hljs-string"><span class="hljs-string">"region"</span></span>: <span class="hljs-string"><span class="hljs-string">""</span></span>, <span class="hljs-string"><span class="hljs-string">"version"</span></span>: <span class="hljs-string"><span class="hljs-string">""</span></span> }, <span class="hljs-string"><span class="hljs-string">"builders"</span></span>: [{ <span class="hljs-string"><span class="hljs-string">"type"</span></span>: <span class="hljs-string"><span class="hljs-string">"amazon-ebs"</span></span>, <span class="hljs-string"><span class="hljs-string">"region"</span></span>: <span class="hljs-string"><span class="hljs-string">"{{user 'region'}}"</span></span>, <span class="hljs-string"><span class="hljs-string">"source_ami"</span></span>: <span class="hljs-string"><span class="hljs-string">"{{user 'source_ami'}}"</span></span>, <span class="hljs-string"><span class="hljs-string">"ssh_pty"</span></span>: <span class="hljs-literal"><span class="hljs-literal">true</span></span>, <span class="hljs-string"><span class="hljs-string">"instance_type"</span></span>: <span class="hljs-string"><span class="hljs-string">"t2.micro"</span></span>, <span class="hljs-string"><span class="hljs-string">"ssh_username"</span></span>: <span class="hljs-string"><span class="hljs-string">"ubuntu"</span></span>, <span class="hljs-string"><span class="hljs-string">"ssh_timeout"</span></span>: <span class="hljs-string"><span class="hljs-string">"5m"</span></span>, <span class="hljs-string"><span class="hljs-string">"associate_public_ip_address"</span></span>: <span class="hljs-literal"><span class="hljs-literal">true</span></span>, <span class="hljs-string"><span class="hljs-string">"ami_virtualization_type"</span></span>: <span class="hljs-string"><span class="hljs-string">"hvm"</span></span>, <span class="hljs-string"><span class="hljs-string">"ami_name"</span></span>: <span class="hljs-string"><span class="hljs-string">"application_instance-{{isotime \"2006-01-02-1504\"}}"</span></span>, <span class="hljs-string"><span class="hljs-string">"tags"</span></span>: { <span class="hljs-string"><span class="hljs-string">"Version"</span></span>: <span class="hljs-string"><span class="hljs-string">"{{user 'version'}}"</span></span> } }], <span class="hljs-string"><span class="hljs-string">"provisioners"</span></span>: [ { <span class="hljs-string"><span class="hljs-string">"type"</span></span>: <span class="hljs-string"><span class="hljs-string">"shell"</span></span>, <span class="hljs-string"><span class="hljs-string">"start_retry_timeout"</span></span>: <span class="hljs-string"><span class="hljs-string">"10m"</span></span>, <span class="hljs-string"><span class="hljs-string">"inline"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"sudo apt-get update -y"</span></span>, <span class="hljs-string"><span class="hljs-string">"sudo apt-get install -y ntp nginx"</span></span> ] }, { <span class="hljs-string"><span class="hljs-string">"type"</span></span>: <span class="hljs-string"><span class="hljs-string">"file"</span></span>, <span class="hljs-string"><span class="hljs-string">"source"</span></span>: <span class="hljs-string"><span class="hljs-string">"application-files/nginx.conf"</span></span>, <span class="hljs-string"><span class="hljs-string">"destination"</span></span>: <span class="hljs-string"><span class="hljs-string">"/tmp/nginx.conf"</span></span> }, { <span class="hljs-string"><span class="hljs-string">"type"</span></span>: <span class="hljs-string"><span class="hljs-string">"file"</span></span>, <span class="hljs-string"><span class="hljs-string">"source"</span></span>: <span class="hljs-string"><span class="hljs-string">"application-files/index.html"</span></span>, <span class="hljs-string"><span class="hljs-string">"destination"</span></span>: <span class="hljs-string"><span class="hljs-string">"/tmp/index.html"</span></span> }, { <span class="hljs-string"><span class="hljs-string">"type"</span></span>: <span class="hljs-string"><span class="hljs-string">"shell"</span></span>, <span class="hljs-string"><span class="hljs-string">"start_retry_timeout"</span></span>: <span class="hljs-string"><span class="hljs-string">"5m"</span></span>, <span class="hljs-string"><span class="hljs-string">"inline"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"sudo mkdir -p /usr/share/nginx/html"</span></span>, <span class="hljs-string"><span class="hljs-string">"sudo mv /tmp/index.html /usr/share/nginx/html/index.html"</span></span>, <span class="hljs-string"><span class="hljs-string">"sudo mv /tmp/nginx.conf /etc/nginx/nginx.conf"</span></span>, <span class="hljs-string"><span class="hljs-string">"sudo systemctl enable nginx.service"</span></span> ] } ] }</code> </pre> <br>  We have variables that we pass, and Packer has a list of so-called Builders for different areas;  many of them.  Builder uses a special AMI source, which I pass to the AMI identifier.  I give him an SSH username and password, and also indicate if he needs a public IP address so that people can access it from outside.  In our case, this does not really matter, because this is the AWS instance for the Packer. <br>  We also set AMI name and tags. <br><br>  You do not need to parse this code.  He is here only to show you how he works.  The most important part here is the version.  It will become relevant later when we enter Terraform. <br><br>  After the builder calls an instance, provisioners run on it.  I actually install NCP and nginx to show you what I can do here.  I copy some files and just configure the nginx configuration.  Everything is very simple.  Then I activate nginx so that it starts when the instance is started. <br><br>  So, I have an application server, and it works.  I can use it in the future.  However, I always check my Packer templates.  Because it is a JSON configuration where you may encounter some problems. <br>  To do this, I run the command: <br><br> <code>make validate <br></code> <br>  I get the answer that the Packer template was verified successfully: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2f0/aa3/1a7/2f0aa31a768c26246877688d0c81cc9e.png"><br><br>  This is just a command, so I can connect it to the CI tool (anyone).  In fact, it will be a process: if the developer changes the template, a pull request is formed, the CI tool checks the request, performs the equivalent of the template validation, and publishes the template if the validation is successful.  All this can be combined in the "Master". <br>  We get the stream for AMI templates - you just need to raise the version. <br><br>  Suppose a developer has created a new version of AMI. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/dc9/3b9/546/dc93b95464ccf7b7e6fbe959d4a5ddf0.png"><br><br>  I'll just fix the version in files from 1.0.0 to 1.0.1 to show you the difference: <br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">html</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">head</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">tittle</span></span></span><span class="hljs-tag">&gt;</span></span>Welcome to DevOops!<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">tittle</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">head</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">body</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">h1</span></span></span><span class="hljs-tag">&gt;</span></span>Welcome!<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">h1</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">p</span></span></span><span class="hljs-tag">&gt;</span></span>Welcome to DevOops!<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">p</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">p</span></span></span><span class="hljs-tag">&gt;</span></span>Version: 1.0.1<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">p</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">body</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">html</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre> <br>  Return to the command line and launch the creation of AMI. <br>  I do not like to run the same commands.  I like to create AMIs quickly, so I use makefiles.  Let's take a look at <code>cat</code> in my makefile: <br><br> <code>cat Makefile <br></code> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/163/0e4/bea/1630e4bea410b10ffcbb5fce2a27d595.png"><br><br>  This is my makefile.  I even provided Help: I ‚Äã‚Äãtype <code>make</code> and press the tab, and it shows me all the target. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/74d/cbb/611/74dcbb6110c7350483735a0f96d9fe90.png"><br><br>  So, we are going to create a new AMI version 1.0.1. <br><br> <code>make ami <br></code> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/879/16c/482/87916c482f6410c8a2fc73c150308409.png"><br><br>  Let's go back to Terraform. <br><br>  I emphasize that this is not a production code.  This is a demonstration.  There are ways to do the same thing better. <br><br>  I use Terraform modules everywhere.  Since I no longer work on Hashicorp, so I can express my opinion about the modules.  For me, the modules are at the level of encapsulation.  For example, I like to encapsulate everything related to VPC: networks, subnets, routing tables, etc. <br><br>  What happens inside?  The developers who work with it may not care about it.  They need to have a general understanding of how the cloud works, what a VPC is.  But it is not necessary to delve into the details.  Only people who really need to change the module should understand it. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1a8/90c/b85/1a890cb85c3fb6a6580e8b6a25492f4f.png"><br><img src="https://habrastorage.org/getpro/habr/post_images/177/a14/a24/177a14a241d80ee6bcc2cf64f194ce4e.png"><br><br>  Here I am going to create an AWS resource and a VPC module.  What's going on here?  <code>cidr_block</code> top-level <code>cidr_block</code> taken and three private subnets and three public subnets are created.  Next, the acailability_zones list is taken.  But we do not know what these access zones are. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/26b/ec1/4ea/26bec14eab18a025c33c2a871c4d3ed4.png"><br><br>  We are going to create a VPN.  Just do not use this VPN module.  This is openVPN, which creates one AWS instance that does not have a certificate.  It uses only the public IP address and is mentioned here only to show you that we can connect to the VPN.  There are more convenient tools for creating a VPN.  It took me about 20 minutes and two beers to write my own. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ae3/a5f/8a6/ae3a5f8a6561b2b9f726e2e129dfcb2f.png"><br><img src="https://habrastorage.org/getpro/habr/post_images/6a8/f0e/7b9/6a8f0e7b965f44255365d50ad64669e0.png"><br><img src="https://habrastorage.org/getpro/habr/post_images/078/e4c/073/078e4c073df61b68e97372c7e4730f5b.png"><br><br>  Then we create an <code>application_tier</code> , which is an auto scaling group - a load balancer.  Some startup configuration is based on AMI-ID, and it combines several subnets and availability zones, and also uses an SSH key. <br>  Let's return to this in a second. <br><br>  I have already mentioned the accessibility zones.  They differ for different AWS accounts.  My US account in the East can have access to zones A, B, and D. Your AWS account can have access to B, C, and E. Thus, fixing these values ‚Äã‚Äãin code, we will encounter problems.  At Hashicorp, we assumed that we could create such data sources so that we could ask Amazon what is available to us.  Under the hood, we request a description of accessibility zones, and then return a list of all zones for your account.  Because of this, we can use data sources for AMI. <br><br>  Now we get to the bottom of my demonstration.  I created an auto scaling group in which three instances are running.  By default, they all have version 1.0.0. <br><br>  When we finish the new version of AMI, I will launch the Terraform configuration again, this will change the launch configuration, and the new service will receive the next version of code, etc. And we can manage it. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/53e/030/d08/53e030d08a8820f28ce8970bdb2bcee8.png"><br><br>  We see that the work of the Packer is over and we have a new AMI. <br>  I go back to Amazon, refresh the page and see the second AMI. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ab7/966/88d/ab796688d7e117c00b12343d54547a3a.png"><br><br>  Let's go back to Terraform. <br><br>  Starting with version 0.10, Terraform has split providers into separate repositories.  And the <code>init terraform</code> command gets a copy of the provider you need to run. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a71/f24/5c1/a71f245c166bffd9cce8d2da21612bb3.png"><br><br>  Providers are loaded.  We are ready to move forward. <br>  Next, we have to run <code>terraform get</code> - load the necessary modules.  They are now on my local machine.  So Terraform will get all the modules at the local level.  In general, modules can be stored in their own repositories on GitHub or elsewhere.  That is why I talked about the VPC module.  You can give the networker team access to make changes.  And this is the API for the development team to work with them.  Really helpful. <br><br>  The next step we want to build a graph. <br><br>  Let's start with <br><br> <code>terraform plan <br></code> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/854/852/ea8/854852ea82a25694dbef99af5832eb27.png"><br><br>  Terraform will take the current local state and compare it with the AWS account, indicating the differences.  In our case, it will create 35 new resources. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3ce/d53/2cc/3ced532cc5cbe65758669980fda88f9e.png"><br><br>  Now we will apply the changes: <br><br> <code>terraform apply <br></code> <br><br>  You do not need to do all this from the local machine.  These are just commands, passing variables to Terraform.  You can transfer this process to CI tools. <br>  If you want to move it to CI, you must use the remote state.  I would like everyone who ever uses Terraform to work with a remote state.  Please do not use local state. <br><br>  One of my buddies noted that even after all the years of working with Terraform, he still discovers something new.  For example, if you create an AWS instance, you need to give it a password, and it can save it in your state.  When I worked in Hashicorp, we assumed that there would be a joint process that changes this password.  So do not try to store everything locally.  And then you can put all this in the CI tools. <br><br>  So, the infrastructure for me is created. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c55/040/c1d/c55040c1d4544a4d7c53f77a494c2be7.png"><br><br>  Terraform can build a graph: <br><br> <code>terraform graph <br></code> <br><br>  As I said, he builds a tree.  In fact, it gives you the opportunity to assess what is happening in your infrastructure.  He will show you the relationship between all the different parts - all nodes and edges.  Since connections have directions, we are talking about a directed graph. <br><br>  The graph will be a JSON list that can be saved in a PNG or DOC file. <br><br>  Return to Terraform.  We really create an auto scaling group. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9fe/253/799/9fe253799368629b6c98d7472cbc3634.png"><br><br>  The Auto scaling group has a capacity of 3. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0dd/2a3/e47/0dd2a3e47ef49ba119c37c6ee1118fe9.png"><br><br>  An interesting question: can we use Vault to manage secrets in Terraform?  Unfortunately no.  No Vault data source for reading secrets in Terraform.  There are other methods, such as environment variables.  With their help, you do not need to enter secrets into the code, you can read them as environment variables. <br><br>  So, we have some infrastructure facilities: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/107/2ef/61e/1072ef61ea789635cb6e9894005734b2.png"><br><br>  I log into my very secret VPN (do not hack my VPN). <br><br>  The most important thing here is that we have three copies of the application.  I, however, should have noted what version of the application is running on them.  It is very important. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/009/bef/b7d/009befb7dcb30e11c9454e3b27f036cf.png"><br>  Everything really is behind the VPN: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b9f/5d3/e43/b9f5d3e436b981cfc21db8e42b782db5.png"><br><br>  If I take it ( <code>application-elb-1069500747.eu-west-1.elb.amazonaws.com</code> ) and paste it into the address bar of the browser, I get the following: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/016/40e/203/01640e2034e1cd80ff454b5c6b74c386.png"><br><br>  Let me remind you, I am connected to a VPN.  If I log out, the specified address will not be available. <br>  We see version 1.0.0.  And no matter how much we refresh the page, we get 1.0.0. <br>  What happens if I change the version from 1.0.0 to 1.0.1 in the code? <br><br><pre> <code class="javascript hljs">filter { name = <span class="hljs-string"><span class="hljs-string">"tag:Version"</span></span> values = [<span class="hljs-string"><span class="hljs-string">"1.0.1"</span></span>] }</code> </pre> <br>  Obviously, CI tools will provide you with the right version. <br>  No manual updates!  We are not perfect, we make mistakes and we can deliver with manual update version 1.0.6 instead of 1.0.1. <br><br><pre> <code class="javascript hljs">filter { name = <span class="hljs-string"><span class="hljs-string">"tag:Version"</span></span> values = [<span class="hljs-string"><span class="hljs-string">"1.0.6"</span></span>] }</code> </pre><br>  But let's move on to our version (1.0.1). <br><br> <code>terraform plan <br></code> <br><br>  Terraform updates the state: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fb8/7f5/221/fb87f522107164e64498825e1e4e0562.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/c39/8ec/61b/c398ec61bfdbd060b526417c56631b14.png"><br><br>  So at this moment he tells me that he is going to change the version in the configuration.  Due to the change of the identifier, it will force the restart of the configuration, and the auto scaling group will change (this is necessary to enable the new launch configuration). <br><br>  This does not change the running instances.  This is really important.  You can follow this process and test it without changing the instances in production. <br><br>  Note: you must always create a new launch configuration before you destroy the old one, otherwise there will be an error. <br><br>  Let's apply the changes: <br><br> <code>terraform apply <br></code> <br><br>  Now back to AWS.  When all changes are applied, we go to the auto scaling group. <br>  Let's go to the AWS configuration.  We see that there are three instances with one launch configuration.  They are the same. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/365/ae7/49c/365ae749cd5c77ea0634247410b58da6.png"><br><br>  Amazon guarantees that if we want to run three instances of the service, they will indeed be running.  That's why we pay them money. <br><br>  Let's move on to the experiments. <br><br>  A new launch configuration has been created.  Therefore, if I delete one of the instances, the rest will not be damaged.  It is important.  However, if you use instances directly, while changing user data, it will destroy the ‚Äúlive‚Äù instances.  Please, do not do that. <br><br>  So, delete one of the instances: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/eb3/373/05a/eb337305af7a4e537d2f5963661f909f.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/244/711/a7c/244711a7c5beb5edd4863239e2997fc1.png"><br><br>  What happens in the auto scaling group when it turns off?  In its place will be a new instance. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/86d/1dd/8f5/86d1dd8f57f1af6e56ddf9054b777e91.png"><br><br>  Here you find yourself in an interesting situation.  The instance will be launched with the new configuration.  That is, in the system you may have several different images (with different configurations).  Sometimes it is better not to immediately delete the old startup configuration in order to connect as needed. <br><br>  Here, everything becomes more interesting.  Why not do it with scripts and CI tools, rather than manually, as I show?  There are tools that can do this, such as the excellent AWS-missing-tools on GitHub. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/948/2b7/96d/9482b796d437daa84cf7c26dc85aa08f.png"><br><br>  And what does this tool do?  This bash script, which runs through all instances in the load balancer, destroys them one by one, ensuring the creation of new ones in their place. <br>  If I lost one of my instances with version 1.0.0 and a new one appeared - 1.1.1, I want to kill all 1.0.0, transferring everything to a new version.  Because I always move forward.  Let me remind you, I do not like it when the application server lives for a long time. <br><br>  In one of the projects, every seven days I had a control script that would destroy all instances in my account.  So the server was no more than seven days.  Another thing (my favorite) is to mark servers as ‚Äústained‚Äù with SSH in a box and destroy them every hour using a script ‚Äî we don‚Äôt want people to do it manually. <br><br>  Such control scripts allow you to always have the latest version with bug fixes and security updates. <br><br>  You can use the script just by running: <br><br> <code>aws-ha-relesae.sh -a my-scaling-group <br></code> <br><br>  <code>-a</code> is your auto scaling group.  The script will go through all instances of your auto scaling group and replace it.  It can be run not only manually, but also from the CI tool. <br>  You can do this in QA or in production.  You can even do this in your local AWS account.  You do whatever you want, each time using the same mechanism. <br><br>  Let's go back to Amazon.  We have a new instance: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f4b/1aa/079/f4b1aa079ec971f111abce895d28c771.png"><br><br>  By refreshing the page in the browser, where we previously saw version 1.0.0, we get: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ff4/923/d78/ff4923d78d16d508c0e8bae753044c15.png"><br><br>  The interesting thing is that since we created the AMI creation script, we can test the creation of AMI. <br><br>  There are some great tools, such as ServerScript or Serverspec. <br><br>  Serverspec allows you to create Ruby-style specifications to check how your application server looks.  For example, below I give a test that verifies that nginx is installed on the server. <br><br><pre> <code class="ruby hljs"><span class="hljs-keyword"><span class="hljs-keyword">require</span></span> <span class="hljs-string"><span class="hljs-string">'spec_helper'</span></span> describe package(<span class="hljs-string"><span class="hljs-string">'nginx'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> it { should be_installed } <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> describe service(<span class="hljs-string"><span class="hljs-string">'nginx'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> it { sould be_enabled } it { sould be_running } <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> describe port(<span class="hljs-number"><span class="hljs-number">80</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> it { should be_listening } <span class="hljs-keyword"><span class="hljs-keyword">end</span></span></code> </pre> <br>  Nginx must be installed and running on the server and listen to port 80.  You can say that user X must be available on the server.  And you can put all these tests in their place.  That way, when you create an AMI, the CI tool can check if the AMI is appropriate for a given purpose.  You will know that AMI is ready for production. <br><br><h3>  Instead of conclusion </h3><br>  Mary Poppendieck is probably one of the most amazing women I have ever heard of.  At one time, she talked about how lean software development has evolved over the years.  And how it was connected with 3M in the 60s, when the company really was engaged in lean development. <br><br>  And she asked the question: How long will your organization need to deploy changes associated with a single line of code?  Can you make this process reliable and easy to repeat? <br><br>  As a rule, this question has always concerned the software code.  How long will it take me to fix one mistake in this application when deploying to production?  But there is no reason why we cannot use the same question as applied to infrastructure or databases. <br><br>  I worked for a company called OpenTable.  In it we called it the duration of the cycle.  And in OpenTable, she was seven weeks.  And this is relatively good.  I know companies that need months when they send a code in production.  At OpenTable, we reviewed the process for four years.  It took a lot of time, since the organization is large - 200 people.  And we reduced the cycle time to three minutes.  This was possible thanks to the measurements of the effect of our transformations. <br><br>  Now everything is scripted.  We have so many tools and examples, there is GitHub.  So take ideas from conferences like DevOops, embed in your organization.  Do not try to implement everything.  Take one tiny thing and sell it.  Show it to someone.  The effect of a small change can be measured, measured and moved on! <br><blockquote>  Paul Stack will arrive in St. Petersburg at the <a href="https://devoops.ru/">DevOops 2018</a> conference with the report <a href="https://devoops.ru/2018/spb/talks/1qvcyo6hye8y8sqeyc8yiw/">‚ÄúSustainable system testing with Chaos‚Äù</a> .  Paul will talk about the Chaos Engineering methodology and show how to use this methodology on real projects. </blockquote></div><p>Source: <a href="https://habr.com/ru/post/420661/">https://habr.com/ru/post/420661/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../420651/index.html">Scientists have unearthed information about the unpleasant precedent associated with climate change</a></li>
<li><a href="../420653/index.html">Five things to do to build a home on Mars</a></li>
<li><a href="../420655/index.html">Robots rent apartments through Airbnbs to learn to better grab items</a></li>
<li><a href="../420657/index.html">Analysis: on whose money Ilon Mask will be able to withdraw Tesla from the stock exchange</a></li>
<li><a href="../420659/index.html">HRF (Human Rights Foundation) USB Campaign ‚ÄúFlash Drives for Freedom‚Äù</a></li>
<li><a href="../420663/index.html">Simple metrics and a way to save time when searching for problems in the infrastructure</a></li>
<li><a href="../420665/index.html">Creating an application on .NET Core and Kubernetes: our experience</a></li>
<li><a href="../420667/index.html">How does the EIGRP protocol work?</a></li>
<li><a href="../420669/index.html">Overview of the enterprise automation market: solutions for construction and management companies in the housing and utilities sector</a></li>
<li><a href="../420671/index.html">[Yekaterinburg, Announcement] UralJS # 9 - three reports on microservices, testing and logging errors on the front</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>