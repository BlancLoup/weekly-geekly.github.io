<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Notes with MBC Symposium: more about saddle points</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Finally, on the second part of the Surya Ganguli report - as a theoretical understanding of the optimization process can help in practice, namely, wha...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Notes with MBC Symposium: more about saddle points</h1><div class="post__text post__text-html js-mediator-article"><p>  Finally, on the second part of the Surya Ganguli report - as a theoretical understanding of the optimization process can help in practice, namely, what role do the saddle points play (the first part is <a href="https://habrahabr.ru/post/282826/">right here</a> , and it is completely optional to read further). </p><br><p><img src="https://u4uvoice.com/wp-content/uploads/2014/09/stanford-huang.jpg" alt="image"></p><a name="habracut"></a><br><p>  <em>Disclaimer: The post is written on the basis of the edited chat logs <a href="http://closedcircles.com/%3Finvite%3Dc2db8785a30164b0a2fb385490c89a8d64eec974">closedcircles.com</a> , hence the style of presentation, and clarifying questions.</em> </p><br><p>  I‚Äôll remind you a bit about saddle points.  In the space of nonlinear functions, there are points with a zero gradient in all coordinates ‚Äî namely, the gradient descent tends to them. <br>  If a point has a gradient in all coordinates of 0, then it can be: <br><img src="http://www.offconvex.org/assets/saddle/minmaxsaddle.png" alt="image"></p><br><ul><li>  The local minimum if in all directions the second derivative is positive. </li><li>  Local maximum if in all directions the second derivative is negative. </li><li>  Saddle point, if in some areas the second derivative is positive, and in others is negative. </li></ul><br><p>  So, we are told good news for gradient descent in a very multidimensional space (which is the optimization of the weights of the deep neural network). </p><br><ul><li>  First, the vast majority of points with a zero gradient are saddle points, not minima. <br>  This can be easily understood intuitively - for a point with a zero gradient to be a local minimum or maximum, the second derivative must have the same sign <strong>in all directions</strong> , but the more measurements, the greater the chance that at least in some direction the sign will be different. <br>  And therefore, the most difficult points that will occur will be saddles. </li><li>  Secondly, with an increase in the number of parameters, it turns out that all local minima are fairly close to each other and to the global minimum. </li></ul><br><p>  Both of these statements were known and theoretically proven for random landscapes in large dimensions. <br>  In collaboration with laboratories Yoshua Bengio, they were experimentally demonstrated for neural networks (theoretically, they have not yet mastered them). </p><br><p><img src="https://habrastorage.org/files/1e5/234/859/1e52348599d14df29e4e25a06dbe6e82.png" alt="image"></p><br><p>  This is a histogram of cost function values ‚Äã‚Äãin local minima, which were obtained by multiple attempts at training from different points - the smaller the parameters, the smaller the spread of values ‚Äã‚Äãin the local minima.  When there are many parameters, the spread decreases sharply and becomes very close to the global minimum. </p><br><p>  The main conclusion is that there is no need to be afraid of local minima, the main problems are with saddle points.  Earlier, when we were not able to train neural networks, we thought that this was due to the fact that the system was slipping into a local minimum.  It turns out, no, we just could not get out of the saddle point. </p><br><p>  And so they came up with a tweak of gradient descent, which well avoids saddle points.  Unfortunately, they use Hessian there. </p><br><blockquote>  For uneducated people like me, Hessian is a matrix of values ‚Äã‚Äãof pairwise second derivatives at a point.  If you think of the gradient as the first derivative of a function of many variables, the Hessian is the second. <br>  <a href="http://avlasov.livejournal.com/">Alexander Vlasov</a> <a href="http://avlasov.livejournal.com/156775.html">wrote a</a> <a href="http://avlasov.livejournal.com/157407.html">good</a> <a href="http://avlasov.livejournal.com/157543.html">tutorial</a> about the so-called second order optimizations, which include working with the Hessian. </blockquote><br><p>  Of course, considering Hessian is terribly impractical for modern neural networks, so I don‚Äôt know how to use their solution in practice. </p><br><p>  The second point is that they invented some theoretically kosher initialization, which in the case of simply linear systems solves the problem of vanishing gradients and allows you to train an arbitrarily deep linear system in the same number of gradient steps. <br>  And they say that it also helps for non-linear systems.  True, I have not yet seen references in the literature of the successful use of this technique. <br>  Links to full articles: <a href="http://papers.nips.cc/paper/5486-identifying-and-attacking-the-saddle-point-problem-in-high-dimensional-non-convex-optimization.pdf">about optimization of gradient descent</a> and <a href="http://arxiv.org/pdf/1312.6120.pdf">about improved initialization</a> . </p><br><p>  In an interesting time we live. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/282900/">https://habr.com/ru/post/282900/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../282884/index.html">My experience setting up an environment for web development</a></li>
<li><a href="../282886/index.html">Attackers use a set of exploits for cyber attacks on Android users</a></li>
<li><a href="../282890/index.html">Sasha Goldshtein, the guru of .NET Performance, will speak at the .NET conference in St. Petersburg</a></li>
<li><a href="../282896/index.html">Multiple vulnerabilities in ImageMagick, one of which leads to RCE</a></li>
<li><a href="../282898/index.html">Upwork has monopoly greed</a></li>
<li><a href="../282902/index.html">Delphi. What does TDictionary</a></li>
<li><a href="../282906/index.html">Administrators of groups in vk have always been in the public domain.</a></li>
<li><a href="../282908/index.html">Virtual IT Infrastructure: Pros and Cons</a></li>
<li><a href="../282910/index.html">NB-IoT: narrow band - broad prospects</a></li>
<li><a href="../282912/index.html">We send data from Arduino to Azure IoT Hub</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>