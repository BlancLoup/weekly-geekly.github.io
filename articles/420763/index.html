<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>KDD 2018, second day, seminars</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Today at the KDD 2018 seminar day - along with a large conference that will start tomorrow, several groups gathered listeners on some specific topics....">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>KDD 2018, second day, seminars</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/b65/759/87a/b6575987a22b09aa6b5b18b5537e075c.jpg" alt="image"><br><br>  Today at the KDD 2018 seminar day - along with a large conference that will start tomorrow, several groups gathered listeners on some specific topics.  I visited two such get-togethers. <br><a name="habracut"></a><br><h3>  Time Series Analysis </h3><br>  In the morning I wanted to go to a seminar on analyzing <a href="http://www.mlgworkshop.org/2018/">graphs</a> , but he was detained for 45 minutes, so I switched to the next one, according to the analysis of time series.  Unexpectedly, a <a href="http://www-personal.umich.edu/~wiensj/">blonde professor</a> from California opens the seminar with the theme ‚ÄúArtificial Intelligence in Medicine‚Äù.  Strange, because for this separate track in the next room.  Then it turns out that she has several graduate students who will talk about time series here.  But, in fact, to the point. <br><br><h4>  Artificial Intelligence in Medicine </h4><br>  Medical errors - the cause of 10% of deaths in the United States, this is one of the three main causes of death in the country.  The problem is that there are not enough doctors;  those that are there are overloaded, and computers, rather, create problems for doctors than they do, at least as doctors perceive it.  However, most of the data is not really used for decision making.  All this must be fought.  For example, one bacterium <a href="https://en.wikipedia.org/wiki/Clostridium_difficile_infection">Clostridium difficile</a> is characterized by high virulence and drug resistance.  Over the past year she has done $ 4 billion in damage.  Let's try to assess the risk of infection based on a time series of medical records.  Unlike previous works, we take a lot of signs (10k-vector for each day) and build individual models for each hospital (in many respects, apparently, a necessary measure, since all hospitals have their own data set).  As a result, we obtain an accuracy of about 0.82 AUC with a CDI risk prediction after 5 days. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      It is important that the model is accurate, interpretable and stable, it is necessary to show what we can do to prevent the disease.  Such a model can be constructed by actively using knowledge from the subject area.  It is the desire for interpretability that often reduces the number of features and leads to the creation of simple models.  But even a simple model with a large feature space loses its interpretability, and the use of L1-regularization often leads to the fact that the model randomly selects one of the collinear features.  As a result, doctors do not believe the model, despite the good AUC.  The authors propose to use a different type of <a href="https://arxiv.org/pdf/1711.03190">EYE</a> (expert yield estimate) regularization.  Taking into account whether there is known data on the impact on the result, it turns out to focus the model on the necessary features.  It gives good results, even if the expert screwed up, moreover - by comparing the quality with standard regularizations, you can assess how much the expert is right. <br><br><img src="https://habrastorage.org/webt/rb/zk/85/rbzk85lodxs506-d1o8eimiyzdu.png"><br><br>  Next, go to the analysis of time series.  It turns out that in order to improve the quality in them, it is important to look for invariants (in fact, to lead to some canonical form).  In a <a href="http://www-personal.umich.edu/~wiensj/papers/MLHC2018_Oh.pdf">recent article, a</a> group of professors proposed an approach based on two convolutional networks.  The first, Sequence Transformer, brings the series to a canonical form, and the second, the Sequence Decoder, solves the classification problem. <br><br><img src="https://habrastorage.org/webt/o8/z2/8b/o8z28bg4ten602freh6pluq37qq.png"><br><br>  The use of CNN, and not RNN, is explained by the fact that they work with rows of fixed length.  They checked MIMIC <a href="https://mimic.physionet.org/">datasets</a> , tried to predict death in the hospital for 48 hours.  As a result, an improvement of 0.02 AUC was obtained as compared to simple CNN with additional layers, but the confidence intervals overlap. <br><br><img src="https://habrastorage.org/webt/o4/ig/6o/o4ig6olnulg0yfzxt8_rcxvl9v4.png"><br><br>  Now, another task: we will predict exclusively on the basis of the series itself, without external signals (that they ate, etc.).  Here, the team proposed replacing the RNN for forecasting a few steps ahead with a grid with several outputs, without recursion between them.  The explanation for this solution is that the error does not accumulate during recursion.  Combine this technique with the previous one (search for invariants).  Immediately after the professor‚Äôs speech, the postdoc talked in detail about this model, so here we‚Äôll end here, noting only that during validation it‚Äôs important to look not only at the general error, but also at the classification of dangerous cases of too high or low glucose. <br><br>  He threw a question about the model's feedback: for now this is a sore open-ended question, they say that it is necessary to try to understand what changes in the distribution of signs occur as a result of the fact of intervention, and which - natural changes caused by external factors.  Actually, the presence of such shifts greatly complicates the situation: it is impossible not to retrain the model, as the quality degrades, mix it up randomly (it‚Äôs not ethical to check someone‚Äôs death and check if it was all treated according to the recommendation of the model - this is guaranteed bias ... <br><br><h4>  Sample path generation </h4><br>  An example of how not to make presentations: very quickly, it's hard to even hear, and getting an idea is almost impossible.  The work itself is available <a href="https://milets18.github.io/papers/milets18_paper_14.pdf">here</a> . <br><br>  The guys develop their previous forecasting result a few steps ahead.  There are two main ideas in the previous work: instead of TIN, we use a network with several outputs for different points in time, plus instead of concrete numbers, we try to predict distributions and estimate quantiles.  This is called all <a href="https://arxiv.org/pdf/1711.11053.pdf">MQ-RNN / CNN</a> (Multi-Horizont forecasting Quantile regression). <br><br><img src="https://habrastorage.org/webt/yr/vo/jw/yrvojwr3lrvmrzqr56pypvu8wjo.png"><br><br>  This time they tried to clarify the forecast using postprocessing.  Considered two approaches.  As part of the first, we are trying to ‚Äúcalibrate‚Äù the distribution of the neural network using a posteriori data and learning the covariance matrix of outputs and observations, the so-called Covariance Shrinkage.  The method is quite simple and working, but I want more.  The second approach was to use generative models to build a ‚Äúsample path‚Äù: using a generative approach for prediction (GAN, VAE).  Good, but unstable results were obtained using <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">WaveNet, which was</a> developed to generate sound. <br><br><h4>  Graph Structured Networks </h4><br>  <a href="https://milets18.github.io/papers/milets18_paper_6.pdf">An interesting job is</a> to transfer the ‚Äúdomain knowledge‚Äù to the neural network.  Shown on the example of forecasting the level of crime in space (by regions of the city) and time (by day and time).  The main difficulty: a strong sparseness of data and the presence of rare local events.  As a result, many methods work poorly, on average for the day, it is still possible to guess, but for specific areas and hours - no.  Let's try to combine high-level structure and micro-patterns in one neural network. <br><br>  We build a communication graph by postal codes and determine the effects of one on the other using the <a href="http://mathworld.wolfram.com/HawkesProcess.html">Multivariate Hawkes Process</a> .  Further, on the basis of the graph obtained, we build the topology of the neural network, linking the blocks of regions of the city, the crime in which showed a correlation. <br><br><img src="https://habrastorage.org/webt/pz/at/qa/pzatqaxp8kfgatfdqqcwcjmcwhc.png"><br><br>  We compared this approach with two others: train on a grid per area or on a grid per group of areas with similar crime rates, showed an increase in accuracy.  For each region, a two-layer LSTM with two fully connected layers is introduced. <br><br>  In addition to the crimes, examples of work on traffic prediction were shown.  Here, the graph for building the network is taken geographically according to kNN.  It is not completely clear how far their results can be compared with others (they changed the metrics in the analysis quite freely), but in general, the heuristics for building a network looks adequate. <br><br><h4>  Nonparametric approach for ensemble forecasting </h4><br>  Ensembles are a very popular topic, but how to deduce the results from individual predictions is not always obvious.  In their work, the authors propose a <a href="https://milets18.github.io/papers/milets18_paper_18.pdf">new approach</a> . <br><br>  Often the simple average in ensembles works well, even better.  than newfangled <a href="https://arxiv.org/pdf/1509.08864">Bayesian model averaging</a> and averaging-nn.  Regression is also not bad, but often gives strange results in terms of the choice of weights (for example, some predictions will give a negative weight, etc.).  In fact, the reason for this is often that the aggregation method uses some assumptions about how the prediction error is distributed (for example, according to Gauss or normal), but when it is applied, it is forgotten to check the fulfillment of this assumption.  The authors attempted to propose an approach free of assumptions. <br><br>  We consider two random processes: the Data Generation Process (DGP) models reality and may depend on time, and the Forecast Generation Process (FGP) models forecasting (there are many of them, one for each member of the ensemble).  The difference of these two processes is also a random process, which we will try to analyze. <br><ol><li>  Collect historical data and build error distribution density for predictors using <a href="https://en.wikipedia.org/wiki/Kernel_density_estimation">Kernel Density Estimation.</a> </li><li> Next, we build the forecast and turn it into a random variable by adding the constructed error. </li><li>  Then we solve the problem of maximizing likelihood. </li></ol><br>  The resulting method is almost like EMOS (Ensemble Model Output Statistics) with a Gaussian error and much better with a non-Gaussian one.  Often in reality, for example, ( <a href="https://aws.amazon.com/ru/datasets/wikipedia-page-traffic-statistics/">Wikipedia Page Traffic Dataset</a> ) is a non-Gaussian error. <br><br><h4>  Nested LSTM: Modeling Taxonomy and Temporal Dynamics in a Location-Based Social Network </h4><br>  <a href="https://milets18.github.io/papers/milets18_paper_8.pdf">The work</a> is presented by authors from Google.  We are trying to predict the next user check.  using his recent chekin history and metadata of places, first of all their relationship to tags / categories.  The three-level categories use the two upper levels: the parent category describes the user's intention (for example, the desire to eat), and the child one - the user's preferences (for example, the user likes Spanish food).  The child category of the next checkin must be shown to generate more revenue from online advertising. <br><br>  We use two nested LSTMs: the top one, as usual, according to the sequence of check-ins, and the nested one - according to the transitions in the category tree from parent to child. <br><br><img src="https://habrastorage.org/webt/dd/7i/md/dd7imd6tviupjn7ca0t6qoxi3l8.png"><br><br>  It turns out 5-7% better compared to simple LSTM with raw category embedings.  In addition, we have shown that LSTM transition embeddings in the category tree look more beautiful than simple ones and cluster better. <br><br><h4>  Identifying Shifts in Evolutionary Semantic Space </h4><br>  Enough vigorous performance of the <a href="http://www-personal.umich.edu/~qmei/">Chinese professor</a> .  The point is to try to understand how words change their meaning. <br><br>  Now everyone successfully trains embedings of words, they work well, but if they are trained at different times, they are not suitable for comparison - it is necessary to do alling. <br><br><ul><li>  You can take the old for initialization, but it does not guarantee. </li><li>  You can learn the transformation function for aligning, but this is not always the case, since the dimensions are not always separated in the same way. </li><li>  And you can use topological, not vector space! </li></ul><br>  As a result, the essence of the decision: we build a kNN-graph by the neighbors of the word in different periods to assess the change in meaning, and we are trying to understand whether there is a significant change here.  For this, we use the <a href="http://papers.nips.cc/paper/2822-bayesian-surprise-attracts-human-attention.pdf">Bayesian Surprise</a> model.  In fact, we are looking at the <a href="https://en.wikipedia.org/wiki/Kullback%25E2%2580%2593Leibler_divergence">KL-divergence of the</a> distribution of the hypothesis (prior) and the hypotheses under the condition of observations (posterior) - this is a surprise.  With words and KNN-graphs, we use Dirichlet based on the frequencies of neighbors in the past as the prior distribution and compare it with the real multinomial in recent history.  Total: <br><br><ul><li>  We cut the story. </li><li>  We build embeddings (LINE with preservation of initialization). </li><li>  We consider kNN on embeddings. </li><li>  We appreciate the surprise. </li></ul><br>  Validating, taking two random words with the same frequency, and changing each other - the increase in quality from a surprise is 80%.  Then we take 21 words with known drifts of meaning and see if we can find them automatically.  In open sources, while a detailed presentation of this approach is not, but <a href="https://dl.acm.org/citation.cfm%3Fid%3D3210040%26dl%3DACM%26coll%3DDL">there is a SIGIR 2018</a> . <br><br><h3>  AdKDD &amp; TargetAd </h3><br>  After lunch, moved to a seminar on online advertising.  There are much more speakers from the industry and everyone is thinking about how to make more money. <br><br><h4>  Ad Tech at AirBnB </h4><br>  Being a large company with a large DS-team, AirBnB invests quite a lot in correctly advertising itself and its internal proposals on external sites.  One of the developers talked a bit about the problems that arise. <br><br>  Let's start with advertising in a search engine: when searching hotels on Google, the first two pages are ads: (. But the user often doesn‚Äôt even understand this, because advertising is very relevant. Standard scheme: match requests for advertising by keywords and extract meaning from patterns / patterns ( city, cheap or luxury, etc.) <br><br>  After the candidates are selected, we arrange an auction between them (now <a href="https://en.wikipedia.org/wiki/Generalized_second-price_auction">Generalized Second Price</a> is used everywhere).  When participating in an auction, the goal is to maximize the effect with a fixed budget, using a model with a combination of probability of click and income: Bid = P (click | search query) * booking value.  An important point: do not spend all the money too quickly, so add Spend pacer. <br><br>  In AirBnB, a powerful system for A / B tests, but here it does not apply, as it drives most of the Google process.  They promised to add more tools for advertisers, the big players are waiting. <br><br>  Separate problem: contact of the user with advertising in several places.  We travel on average a couple of times a year, the trip preparation and booking cycle is very long (weeks or even months), there are several channels where we can reach the user, and we need to divide the budget by channel.  This topic is very patient, there are simple methods (linear, balanced, by the last click or by the results of the <a href="https://en.wikipedia.org/wiki/Uplift_modelling">uplift test</a> ).  AirBnB tried two new approaches: based on Markov models and <a href="https://en.wikipedia.org/wiki/Shapley_value">Shapley models</a> . <br><br>  Everything is more or less clear with the Markov model: we build a discrete chain, the nodes at which correspond to the points of contact with advertising, there is also a node for conversion.  According to the data we select the weights for the transitions; we give more nodes to those nodes where the transition probability is greater.  He threw the question to them: why use a simple Markov chain, whereas it is more logical to use MDP;  They said that they were working on this topic. <br><br>  More interesting with Shapley: in fact, this is a long-known scheme for evaluating an additive effect, in which different combinations of effects are considered, the effect from each of them is evaluated, and then an aggregate is determined for each individual impact.  The difficulty is that between synergies there can be synergy (more rarely antagonism), and the result from the sum is not equal to the sum of the results.  In general, quite an interesting and beautiful theory, I <a href="https://en.wikipedia.org/wiki/Shapley_value">advise you to read</a> . <br><br>  In the case of AirBnB, the use of the Shapley model looks like this: <br><br><ul><li>  We have in the observed data examples with different combinations of effects and the actual result. </li><li>  We fill in data gaps (not all combinations are represented) with the help of ML. </li><li>  We calculate the loan for each type of impact on Shapley. </li></ul><br><h4>  Microsoft: Pushing {AI} Boundaries </h4><br>  A little more about that.  how they are engaged in advertising at Microsoft, now from the side of the platform, first of all, Bing.  A few nobles: <br><br><ul><li>  The advertising market is growing very fast (exponentially). </li><li>  Advertising on one page cannibalizes each other, it is necessary to analyze the entire page. </li><li>  Conversion on some pages above, despite the fact that STR is worse. </li></ul><br>  The Bing advertising engine has about 70 models, 2000 experiments offline, 400 online.  One major change in the platform every week.  In general, they work tirelessly.  What are the changes in the platform: <br><br><ul><li>  The myth of a single metric: it does not work out so that metrics grow and compete. </li><li>  They remade the system of ad-matching to queries from NLP to DL, which is cheated on FPGA. </li><li>  Federal models and context gangsters are used: internal models produce probability and uncertainty, the gangster from above makes the decision.  They talked a lot about gangsters, use them to start models and launch at cruising speed, bypassing with them the fact that often an improvement in the model leads to lower incomes :( </li><li>  It is very important to assess the uncertainty (well, yes, without it the gangster cannot be built). </li><li>  For small advertisers, the institution of advertising through bandits does not work, there is little statistics, it is necessary to make separate models for a cold start. </li><li>  It is important to monitor the performance on different cohorts of users, they have an automatic system for slicing according to the results of the experiment. </li></ul><br>  We talked a little about the analysis of the outflow.  It is not always hypothesis salespeople about the causes of the outflow are correct, it is necessary to dig deeper.  To do this, you have to build interpreted models (or a special model to explain the predictions) and think a lot.  And then conduct experiments.  But with outflow, experiments are always difficult to do, they recommend using second-order metrics and <a href="https://static.googleusercontent.com/media/research.google.com/ru//pubs/archive/43887.pdf">an article from Google</a> . <br><br>  They also use such a thing as Commercial Knowledge Graph, representing the description of the subject area: brands, products, etc.  Build a graph completely automatically, unsupervised.  Brands are marked by categories, this is important, as in general it is not always possible to unsupervised to single out the brand as a whole, but within the framework of a category-topic the signal is stronger.  Unfortunately, I did not find any open works according to their method. <br><br><h4>  Google Ads </h4><br>  The same dude says that yesterday he was talking about the graphs, still sad and arrogant.  Walked on several topics. <br><br>  Part One: Robust Stochastic Ad Allocation.  We have budget nodes (ads) and online nodes (users), and there are also some weights between them.  Now you have to choose which advertisements to show the new node.  You can do this greedily (always at maximum weight), but then we risk prematurely working out a budget and getting an inefficient solution (the theoretical limit is 1/2 of the optimum).  You can fight this differently, in fact, here we have the traditional conflict of revenu and wellfair. <br><br>  When choosing an allocation method, one can assume a random order of appearance of online nodes in accordance with a certain distribution, but in practice there can also be an adversarial order (that is, elements of some opposing influence).  The methods in these cases are different, give references to their latest articles: <a href="https://arxiv.org/abs/1803.09353">1</a> and <a href="http://proceedings.mlr.press/v80/agrawal18b/agrawal18b.pdf">2</a> . <br><br>  Part Two: Inceptive-aware learning / Robust Pricing.  Now we are trying to resolve the issue of choosing the price of a reservation to increase the income of advertising sites.  We also consider the use of other auctions such as <a href="http://www.eecs.harvard.edu/cs286r/courses/spring07/papers/myerson.pdf">Myerson auction</a> , <a href="http://www-bcf.usc.edu/~nazerzad/pdf/bintac.pdf">BINTAC</a> , rollback to the auction of the first price when it hits the reservation.  Do not go into details, send to <a href="https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID3177913_code2336200.pdf%3Fabstractid%3D3144034%26mirid%3D1%26type%3D2">your article</a> . <br><br>  Part Three: Online Bundling.  Again we solve the problem with an increase in income, but now we are entering from the other side.  If you could buy wholesale advertising (offline bundling), then in many situations you can offer a more optimal solution.  But it is impossible to do this in an online auction, it is necessary to build complex models with a memory, and in the harsh conditions of RTB this cannot be crammed. <br><br>  Then a magic model appears, where all memory is reduced to a single digit (bank account), but time is running out, and the speaker starts frantically flipping through the slides.  For answers, in his best style, sends to the <a href="https://www.cs.cmu.edu/~nhaghtal/mlstrat/papers/songzuo2.pdf">article</a> . <br><br><h4>  Deep Policy optimization by Alibaba </h4><br>  We work with "sponsored search".  We decided to use RL, as the name implies - deep.  Detailed information can be found in the <a href="https://arxiv.org/pdf/1803.07347.pdf">article</a> . <br><br>  They told about one of the important points about the separation of the offline-part of the training, where the deep network itself is trained, and the online-part, where the signs are adapted. <br><br><img src="https://habrastorage.org/webt/8-/lx/b9/8-lxb9hhpif4yk-cxocaz3ucv4o.png"><br><br>  As a reward, a mixture of CTR and income is used, the <a href="https://arxiv.org/pdf/1509.02971">DDPG</a> model is <a href="https://arxiv.org/pdf/1509.02971">used</a> . <br><br><img src="https://habrastorage.org/webt/4-/wp/5e/4-wp5ejowiwekgdafew5ffucipq.png"><br><br>  At the end, the speaker addressed three open-ended questions to the audience, ‚Äúto think about‚Äù: <br><br><ul><li>  There is no real environment for RL. </li><li>  A lot of noise in reality, which greatly complicates the analysis. </li><li>  How to work with a changing environment? </li></ul><br><h4>  Criteo Large Scale Benchmark for Uplift Modeling </h4><br>  Again we return to the task of identifying aplift (the effect of a particular impact among many).  In Criteo, to analyze the approaches to solving the problem, they made a new dataset <a href="http://ailab.criteo.com/criteo-uplift-prediction-dataset/">Criteo-UPLIFT1</a> (450 MB) and drove the benchmark. <br><br>  The idea is quite simple.  There is an initial desire to buy something, perhaps the user would have bought without treatment (he recovered).  We look at the difference of two conditional probabilities with and without treatment - this is uplift (remember that we are looking at a specific user). <br><br>  How to evaluate such a model?  Let's look at the situation as a ranking task.  For some reason, going to the ranking, we introduce a strange estimation model - take the uplift cumulative in rank with our ranking and random order, compare the area between the curves (AUUC).  We also look at the <a href="https://www.maths.ed.ac.uk/~mthdat25/uplift/MesallesNaranjoOscar-1">Qini-coefficient</a> (generalization of <a href="https://en.wikipedia.org/wiki/Gini_coefficient">Gini</a> for uplift), can be compared with the ideal according to Qini, and not random. <br><br>  Tested in the framework of the benchmark two approaches.  In the first case, we train two models: one on the treatment group, the other on the non-treatment group to predict probability.  We rank by delta forecasts. <br><br>  Another approach was called revert label.  Redesigned datasets in such a way that the units were those who received treatment and converted, as well as those who did not receive and did not convert;  the remaining zeros.  We rank, putting up those who look like units. <br><br>  According to the benchmark results, in terms of visits, the model with reverser works better.  With conversions, improvement is achieved as data increases. <br><br><h4>  Deep Net with Attention for Multi-Touch Attribution </h4><br>  How to work with many channels, this time <a href="https://docs.wixstatic.com/ugd/b6ac34_ea6e112992544bd08f637fc41b812ae6.pdf">from Adobe</a> .  We immediately discard simple models and begin to learn, of course, the grid!  And not just a grid, but with a attention-layer on top of LSTM, just to simulate the contribution of individual sources.  To simulate static features, add a fully connected mesh of several layers next to LSTM, and get the best result. <br><br><img src="https://habrastorage.org/webt/py/nj/0i/pynj0izs54ljvb3x6byf0-vexo8.png"><br><br>  In general, the model does not look so crazy, and the attention-layer really gives a chance for the model to be interpretable. <br><br><h3>  Conclusion </h3><br>  Then there was a pretentious opening session with an IMAX-roller in the best traditions of trailers for blockbusters, many thanks to everyone who helped organize this - a record KDD in all respects (including $ 1.2 million sponsorship), a farewell from Lord Byts (Minister of Innovation) UK) and poster session, which has no strength left.  We must prepare for tomorrow. </div><p>Source: <a href="https://habr.com/ru/post/420763/">https://habr.com/ru/post/420763/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../420741/index.html">Cheat sheet for programmers or "we google for you"</a></li>
<li><a href="../420749/index.html">GitLab for Continuous Delivery project on InterSystems technologies: Containers</a></li>
<li><a href="../420753/index.html">Microservice Frontend - Modern Approach to Front Sharing</a></li>
<li><a href="../420757/index.html">Programming Contest: Trading (Results)</a></li>
<li><a href="../420761/index.html">TypeScript 3.0</a></li>
<li><a href="../420765/index.html">Impressions of the Gemini PDA. Pocket dual-boot combine or useless toy?</a></li>
<li><a href="../420767/index.html">Rostelecom Demands to Recognize Sputnik Bankrupt</a></li>
<li><a href="../420769/index.html">Looking.House - more than 150 points of Looking Glass on one site</a></li>
<li><a href="../420775/index.html">Own game in 72 hours: rakes, crutches and alpacas</a></li>
<li><a href="../420777/index.html">Logging to Android Studio without code</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>