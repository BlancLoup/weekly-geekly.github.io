<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Autopilot on your own. Part 1 - we collect the training data</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr. This is a post-report-tutorial about unmanned vehicles - how to (start) make your own without spending on equipment. All code is available o...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Autopilot on your own. Part 1 - we collect the training data</h1><div class="post__text post__text-html js-mediator-article"><p>  Hi, Habr.  This is a post-report-tutorial about unmanned vehicles - how to (start) make your own without spending on equipment.  All code is available <a href="https://github.com/waiwnf/pilotguru">on github</a> , and among other things, you will learn how to easily generate such cool images: </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/01-steering/slam-map-trajectory-example.png" alt="SLAM trajectory + map example"></p><br><p>  Go! <a name="habracut"></a></p><br><h2 id="vkratce">  In short </h2><br><p>  Summary for those familiar with the topic: Traditionally, a <a href="https://medium.com/udacity/were-building-an-open-source-self-driving-car-ac3e973cd163">specially equipped car</a> with a <a href="http://blog.caranddriver.com/why-ford-lincoln-and-lexus-testers-rule-the-self-driving-roost/">sufficiently informative CAN bus</a> and interface to it was expensive to recruit a <a href="https://medium.com/udacity/open-sourcing-223gb-of-mountain-view-driving-data-f6b5593fbfa5">training set</a> for the autopilot based on machine learning.  We will do it easier and for free - we will type the same data in essence just from the smartphone on the windshield.  Fits any car, no equipment modifications.  In this series, we calculate the turn of the steering wheel at each moment in time by video.  If everything is clear in this paragraph, you can jump over the introduction <a href="https://habr.com/ru/post/325704/">to the essence of the approach</a> . </p><br><h2 id="chto-zachem-pochemu-bolee-podrobno">  What-why-why in more detail </h2><br><p>  So, a couple of years ago, without serious resources of a large corporation on the topic of autopilots, it was impossible to turn around - the <a href="https://ru.wikipedia.org/wiki/%25D0%259B%25D0%25B8%25D0%25B4%25D0%25B0%25D1%2580">LIDAR</a> sensor alone was worth <a href="http://content.usatoday.com/communities/driveon/post/2012/06/google-discloses-costs-of-its-driverless-car-tests/1">tens of thousands of dollars</a> , but the recent revolution in neural networks changed everything.  <a href="http://comma.ai/">Startups of several people</a> with the simplest sensor sets from a pair of webcams <a href="http://newatlas.com/geohot-comma-ai-openpilot-open-source/46722/">compete</a> on equal terms <a href="http://newatlas.com/geohot-comma-ai-openpilot-open-source/46722/">in terms of the quality of the result</a> with famous brands.  Why not try us too, especially since so many high-quality <a href="https://github.com/commaai/openpilot">components are</a> already in the <a href="https://github.com/udacity/self-driving-car/tree/master/steering-models/community-models">public</a> <a href="https://github.com/commaai/neo">domain</a> . </p><br><p>  The autopilot converts sensor data into control actions ‚Äî the steering turn and the required acceleration / deceleration.  In a laser rangefinder system like Google‚Äôs, it might look like this: </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/01-steering/autopilot-sensors.jpg" alt="Sensors to direcrtions"></p><br><p>  The simplest version of the sensor is a video camera ‚Äúlooking‚Äù through the windshield.  We will work with him, because everyone already has a camera on the phone. </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/01-steering/lane-from-video.jpg" alt="Lane from video"></p><br><p>  For calculating control signals from raw video, <a href="https://habrahabr.ru/post/309508/">convolutional neural networks</a> work well, but, like any other machine learning approach, they need to be taught to predict the correct result.  For training, you need (a) to choose a model architecture and (b) to form a <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B0%25D1%2588%25D0%25B8%25D0%25BD%25D0%25BD%25D0%25BE%25D0%25B5_%25D0%25BE%25D0%25B1%25D1%2583%25D1%2587%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">training set</a> that will demonstrate models of different input situations and "correct answers" (for example, the steering angle and the position of the gas pedal) for each of them.  Data for a training sample is usually recorded from races where the machine is operated by a person.  That is, the driver demonstrates the robot how to operate the machine. </p><br><p>  There are enough good neural networks architectures in the <a href="https://github.com/udacity/self-driving-car/tree/master/steering-models/community-models">public domain</a> , but the situation is more sad with data: firstly, there is simply not enough data, secondly, almost all samples come <a href="https://medium.com/udacity/open-sourcing-223gb-of-mountain-view-driving-data-f6b5593fbfa5">from the USA</a> , and on our roads <a href="https://www.youtube.com/watch%3Fv%3DitMdLTd1l4E">there are a lot of differences from those places</a> . </p><br><p>  The lack of open data is easily explained.  First of all, data is no less valuable asset than expertise in algorithms and models, so no one is in a hurry to share: </p><br><blockquote>  The rocket engine is the data. <br>  <a href="https://www.wired.com/brandlab/2015/05/andrew-ng-deep-learning-mandate-humans-not-just-machines/">Andrew Ng</a> </blockquote><p>  Secondly, the process of collecting data is not cheap, especially if you act head-on.  A good example is <a href="https://medium.com/udacity/were-building-an-open-source-self-driving-car-ac3e973cd163">Udacity</a> .  They specially selected a car model where the steering and gas / brake are tied to a digital bus, made an interface to the bus, and read data from there directly.  Plus approach - high quality data.  Minus - a serious cost, cutting off the vast majority of non-professionals.  After all, not every modern car even writes all the information we need to <a href="https://ru.wikipedia.org/wiki/Controller_Area_Network">CAN</a> , and we‚Äôll have to tinker with the interface. </p><br><p>  We will proceed easier.  We record the "raw" data (for now it will be just a video) with a smartphone on the windshield as a DVR, then use the software to "squeeze" the necessary information from there - the speed of movement and turns, on which the autopilot can already be trained.  As a result, we get an almost free solution - if there is a phone holder on the windshield, just press a button to type training data on the way to work. </p><br><p>  In this series - "squeezer" angle of rotation from the video.  All steps are easy to repeat on your own using the code <a href="https://github.com/waiwnf/pilotguru">on github</a> . </p><br><h2 id="zadacha">  Task </h2><br><p>  We solve the problem: </p><br><ul><li>  There is a video from the camera, rigidly fixed to the car (ie, the camera does not hang out). </li><li>  It is required for each frame to find out the current steering angle. </li></ul><br><p>  Expected Result: </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/gMXn0IMcX-k" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  Immediately just simplify - instead of the steering angle, we calculate the angular velocity in the horizontal plane.  This is roughly equivalent information if we know the translational speed, which we will deal with in the next series. </p><br><h2 id="reshenie">  Decision </h2><br><p>  The solution can be assembled from publicly available components by slightly modifying them: </p><br><h3 id="vosstanavlivaem-traektoriyu-kamery">  We restore the trajectory of the camera </h3><br><p>  The first step is to restore the camera's trajectory in three-dimensional space using <a href="https://github.com/raulmur/ORB_SLAM2">the SLAM library</a> for video (simultaneous localization and mapping, simultaneous localization and mapping).  At the output for each (almost, see the nuances) of the frame, we get 6 position parameters: 3D offset and 3 <a href="https://ru.wikipedia.org/wiki/%25D0%25A3%25D0%25B3%25D0%25BB%25D1%258B_%25D0%25AD%25D0%25B9%25D0%25BB%25D0%25B5%25D1%2580%25D0%25B0">orientation angles</a> . </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/01-steering/slam-trajectory-only-example.png" alt="SLAM trajectory only example"></p><br><p> In the code, the <a href="https://github.com/waiwnf/pilotguru"><code>optical_trajectories</code></a> module is responsible for this part <a href="https://github.com/waiwnf/pilotguru"><code>optical_trajectories</code></a> </p><br><p>  Nuances: </p><br><ul><li>  When recording video, do not chase the maximum resolution - beyond a certain threshold, it only hurts.  My settings around 720x480 work well. </li><li>  The camera will need to be calibrated ( <a href="https://github.com/waiwnf/pilotguru">instructions</a> , <a href="https://habrahabr.ru/post/130300/">theory ‚Äî parts 1 and 2 are relevant</a> ) <strong>at the same settings with which the video from the race was recorded</strong> . </li><li>  The SLAM system needs a "good" sequence of frames, for which you can "hook" as a starting point, so part of the video at the beginning, until the system is "hooked" will not be annotated.  If localization on your video does not work at all, either calibration problems are likely (try to calibrate several times and look at the variation of the results), or video quality problems (too high resolution, too much compression, etc.). </li><li>  There may be breakdowns in the tracking system by the SLAM system, if too many key points are lost between adjacent frames (for example, glass was momentarily flooded with a splash from a puddle).  In this case, the system will be reset to its original non-localized state and will be localized again.  Therefore, from one video you can get several trajectories (not intersecting in time).  The coordinate systems in these trajectories will be completely different. </li><li>  The specific library ORB_SLAM2, which I used, gives not very reliable results on translational displacements, so we ignore them for now, but determine the rotation quite well, we leave them. </li></ul><br><h3 id="opredelyaem-ploskost-dorogi">  Determine the plane of the road </h3><br><p>  The trajectory of the camera in three-dimensional space is good, but it does not directly answer the final question whether to turn left or right, and how quickly.  After all, the SLAM system does not have the concepts of "road plane", "up-down", etc.  This information must also be obtained from the "raw" 3D trajectory. </p><br><p>  A simple observation will help here: highways <em>are usually</em> stretched far further horizontally than vertically.  There are of course <a href="">exceptions</a> , they will have to be neglected.  And if so, you can take the nearest plane (ie, the plane, the projection on which gives the minimal reconstruction error) of our trajectory beyond the horizontal plane of the road. </p><br><p>  We select the horizontal plane by an excellent <a href="http://www.chemometrics.ru/materials/textbooks/pca.htm">method of principal components</a> along all 3D points of the trajectory ‚Äî we remove the direction with the smallest eigenvalue, and the remaining two will give the optimal plane. </p><br><p><img src="http://www.chemometrics.ru/materials/textbooks/pca/fig03.gif" alt="Dominant plane"></p><br><p>  The <a href="https://github.com/waiwnf/pilotguru"><code>optical_trajectories</code></a> module is also responsible for the plane selection logic <a href="https://github.com/waiwnf/pilotguru"><code>optical_trajectories</code></a> </p><br><p>  Nuance: </p><br><ul><li><p>  From the essence of the main components, it is clear that apart from mountain roads, the selection of the main plane will work poorly if the car has been traveling in a straight line all the time, because then only one direction of a real horizontal plane will have a large range of values, and the range along the remaining perpendicular horizontal direction and vertically will be comparable. </p><br><p>  In order not to pollute the data with large errors from such trajectories, we check that the spread on the last main component is significantly (100 times) smaller than on the last but one.  Non-past trajectories are simply thrown away. </p><br></li></ul><br><h3 id="vychislyaem-ugol-povorota">  Calculate the angle of rotation </h3><br><p>  Knowing the basis vectors of the horizontal plane v <sub>1</sub> and v <sub>2</sub> (the two main components with the largest eigenvalues ‚Äã‚Äãfrom the previous part), we project the optical axis of the camera onto the horizontal plane: </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/01-steering/horizontal-projection-equation.gif" alt="Horizontal projection equation"></p><br><p>  Thus, from the three-dimensional orientation of the camera, we obtain the vehicle heading angle (up to an unknown constant, since the axis of the camera and the axis of the car generally do not match).  Since we are only interested in the intensity of rotation (i.e. angular velocity), this constant is not needed. </p><br><p>  The angle of rotation between adjacent frames is given by school trigonometry (the first factor is the absolute value of the turn, the second is the sign that determines the direction to the left / right).  Here, a <sub>t</sub> means the projection vector a <sub>horizontal</sub> at time t: </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/01-steering/rotation-magnitude.gif" alt="Horizontal projection equation"></p><br><p>  This part of the computation is also done by the <a href="https://github.com/waiwnf/pilotguru"><code>optical_trajectories</code></a> module.  At the output we get a JSON file of the following format: </p><br><pre> <code class="hljs json">{ <span class="hljs-attr"><span class="hljs-attr">"plane"</span></span>: [ [ <span class="hljs-number"><span class="hljs-number">0.35</span></span>, <span class="hljs-number"><span class="hljs-number">0.20</span></span>, <span class="hljs-number"><span class="hljs-number">0.91</span></span>], [ <span class="hljs-number"><span class="hljs-number">0.94</span></span>, <span class="hljs-number"><span class="hljs-number">-0.11</span></span>, <span class="hljs-number"><span class="hljs-number">-0.33</span></span>] ], <span class="hljs-attr"><span class="hljs-attr">"trajectory"</span></span>: [ ..., { <span class="hljs-attr"><span class="hljs-attr">"frame_id"</span></span>: <span class="hljs-number"><span class="hljs-number">6710</span></span>, <span class="hljs-attr"><span class="hljs-attr">"planar_direction"</span></span>: [ <span class="hljs-number"><span class="hljs-number">0.91</span></span>, <span class="hljs-number"><span class="hljs-number">-0.33</span></span> ], <span class="hljs-attr"><span class="hljs-attr">"pose"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"rotation"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"w"</span></span>: <span class="hljs-number"><span class="hljs-number">0.99</span></span>, <span class="hljs-attr"><span class="hljs-attr">"x"</span></span>: <span class="hljs-number"><span class="hljs-number">-0.001</span></span>, <span class="hljs-attr"><span class="hljs-attr">"y"</span></span>: <span class="hljs-number"><span class="hljs-number">0.001</span></span>, <span class="hljs-attr"><span class="hljs-attr">"z"</span></span>: <span class="hljs-number"><span class="hljs-number">0.002</span></span> }, <span class="hljs-attr"><span class="hljs-attr">"translation"</span></span>: [ <span class="hljs-number"><span class="hljs-number">-0.005</span></span>, <span class="hljs-number"><span class="hljs-number">0.009</span></span>, <span class="hljs-number"><span class="hljs-number">0.046</span></span> ] }, <span class="hljs-attr"><span class="hljs-attr">"time_usec"</span></span>: <span class="hljs-number"><span class="hljs-number">223623466</span></span>, <span class="hljs-attr"><span class="hljs-attr">"turn_angle"</span></span>: <span class="hljs-number"><span class="hljs-number">0.0017</span></span> }, ..... }</code> </pre><br><p>  Component values: </p><br><ul><li>  <code>plane</code> - the basis vectors of the horizontal plane. </li><li>  <code>trajectory</code> - a list of items, one for each frame successfully tracked by the SLAM system. <br><ul><li>  <code>frame_id</code> - frame number in the source video (starting from 0). </li><li>  <code>planar_direction</code> - projection of the off-axis on the horizontal plane </li><li>  <code>pose</code> - camera position in 3D space <br><ul><li>  <code>rotation</code> - orientation of the optical axis in the <a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25B2%25D0%25B0%25D1%2582%25D0%25B5%25D1%2580%25D0%25BD%25D0%25B8%25D0%25BE%25D0%25BD%25D1%258B_%25D0%25B8_%25D0%25B2%25D1%2580%25D0%25B0%25D1%2589%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%25BF%25D1%2580%25D0%25BE%25D1%2581%25D1%2582%25D1%2580%25D0%25B0%25D0%25BD%25D1%2581%25D1%2582%25D0%25B2%25D0%25B0">unit quaternion</a> format. </li><li>  <code>translation</code> - offset. </li></ul></li><li>  <code>time_use</code> - time from the beginning of the video in microseconds </li><li>  <code>turn_angle</code> - horizontal rotation relative to the previous frame in radians. </li></ul></li></ul><br><h3 id="ubiraem-shum">  Remove noise </h3><br><p>  We are almost there, but the problem remains.  Let's look at the resulting (for now) graph of angular velocity: </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/01-steering/rotations-unsmoothed-plot.png" alt="Raw rotations between frames"></p><br><p>  We visualize on video: </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/y3rKvrGasOI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  It is seen that in general, the direction of rotation is determined correctly, but a lot of high-frequency noise.  We remove it with <a href="https://en.wikipedia.org/wiki/Gaussian_blur">Gaussian blur</a> , which is a low-pass filter. </p><br><p>  Smoothing in the code is done by the <a href="https://github.com/waiwnf/pilotguru"><code>smooth_heading_directions</code></a> module </p><br><p>  Result after the filter: </p><br><p><img src="https://raw.githubusercontent.com/waiwnf/pilotguru/master/blog/habr/01-steering/rotations-with-smoothed-plot.png" alt="Smoothed between frames"></p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/wWIcygYK4HY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  It is already possible to feed the learning model and rely on adequate results. </p><br><h3 id="vizualizaciya">  Visualization </h3><br><p>  For clarity, according to the data from the JSON files of the trajectories, you can impose a virtual steering wheel on the original video, as in the demos above, and check whether it is spinning correctly.  This is the <a href="https://github.com/waiwnf/pilot"><code>render_turning</code></a> module. </p><br><p>  It is also easy to build a frame schedule.  For example, in an IPython laptop with matplotlib installed: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib %matplotlib inline <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> json json_raw = json.load(open(<span class="hljs-string"><span class="hljs-string">'path/to/trajectory.json'</span></span>)) rotations = [x[<span class="hljs-string"><span class="hljs-string">'turn_angle'</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> json_raw[<span class="hljs-string"><span class="hljs-string">'trajectory'</span></span>]] plt.plot(rotations, label=<span class="hljs-string"><span class="hljs-string">'Rotations'</span></span>) plt.show()</code> </pre> <br><p>  That's all for now.  In the next series, we define the translational speed in order to train also speed control, but for now pull-requests are welcome. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/325704/">https://habr.com/ru/post/325704/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../325690/index.html">Why I abandoned multiple monitors</a></li>
<li><a href="../325692/index.html">MakiseGUI - a free graphical user interface library for microcontrollers</a></li>
<li><a href="../325694/index.html">How IT professionals work. Fedor Bykov, Director of Research and Development at PROMT</a></li>
<li><a href="../325696/index.html">Introduction to the magical world of payment mediation</a></li>
<li><a href="../325700/index.html">Practical experience using this HotSwap</a></li>
<li><a href="../325706/index.html">5 Tips for Creating Impressive Motion Graphics</a></li>
<li><a href="../325708/index.html">Some useful commands for working in terminal Linux</a></li>
<li><a href="../325710/index.html">How to set up the next generation extended protection in the corporate network</a></li>
<li><a href="../325714/index.html">Cross a hedgehog (Marathon) with a snake (Spring Cloud). Episode 1</a></li>
<li><a href="../325716/index.html">Life without SDL. Winter 2017</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>