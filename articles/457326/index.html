<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Failover cluster PostgreSQL + Patroni. Implementation experience</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the article I will tell you how we approached the issue of PostgreSQL fault tolerance, why it became important for us and what happened in the end....">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Failover cluster PostgreSQL + Patroni. Implementation experience</h1><div class="post__text post__text-html js-mediator-article">  In the article I will tell you how we approached the issue of PostgreSQL fault tolerance, why it became important for us and what happened in the end. <br><br>  We have a high-loaded service: 2.5 million users worldwide, 50K + active users every day.  The servers are located in Amazone in one region of Ireland: there are always 100+ different servers in operation, of which almost 50 are with databases. <br><br>  The entire backend is a large monolithic stateful Java application that keeps a constant websocket connection with the client.  With simultaneous work of several users on one board, they all see changes in real time, because we record every change in the database.  We have about 10K requests per second to our databases.  At peak load in Redis, we write 80-100K requests per second. <br><img src="https://habrastorage.org/webt/ef/pn/er/efpner_0rhtuim1zt0gwrhff3b8.png"><br><a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  Why we switched from Redis to PostgreSQL </h2><br>  Initially, our service worked with Redis, a key-value repository that stores all data in the server‚Äôs RAM. <br><br>  Redis pros: <br><br><ol><li>  High response rate, because  everything is stored in memory; </li><li>  Convenience of backup and replication. </li></ol><br>  Cons Redis for us: <br><br><ol><li>  There are no real transactions.  We tried to imitate them at the level of our application.  Unfortunately, this did not always work well and required the writing of very complex code. </li><li>  The amount of data is limited by the amount of memory.  As the amount of data increases, memory will grow, and eventually we will rest on the characteristics of the selected instance, which in AWS requires stopping our service to change the type of instance. </li><li>  It is necessary to constantly maintain a low latency level, since  we have a very large number of requests.  The optimal delay level for us is 17-20 ms.  At a level of 30-40 ms, we receive long responses to requests from our application and degradation of the service.  Unfortunately, this happened in September 2018, when for some reason one of the instances with Redis received latency 2 times more than usual.  To solve the problem, we stopped the service in the middle of the working day for unscheduled maintenance and replaced the problem instance Redis. </li><li>  It is easy to obtain inconsistency of data even with minor errors in the code and then spend a lot of time writing code to correct this data. </li></ol><br>  We took into account the disadvantages and realized that we need to move to something more convenient, with normal transactions and less dependent on latency.  We conducted a study, analyzed a variety of options and chose PostgreSQL. <br><br>  We have been moving to the new database for 1.5 years and have transported only a small part of the data, so now we are working simultaneously with Redis and PostgreSQL.  More information about the stages of moving and switching data between the database is written in the <a href="https://habr.com/ru/company/miro/blog/437826/">article of my colleague</a> . <br><br>  When we started to move, our application worked directly from the database and addressed the Redis and PostgreSQL wizard.  The PostgreSQL cluster consisted of a master and a replica with asynchronous replication.  So the scheme of work with bases looked: <br><img src="https://habrastorage.org/webt/wc/wg/ef/wcwgefzqham9mw7hm-5xc37pfp0.png"><br><br><h2>  PgBouncer implementation </h2><br>  While we were moving, the product was also developing: the number of users and the number of servers that worked with PostgreSQL increased, and we began to lack connections.  PostgreSQL creates a separate process for each connection and consumes resources.  You can increase the number of connections until a certain point, otherwise there is a chance to get a non-optimal database operation.  The ideal option in such a situation would be the choice of the connection manager, who will face the base. <br><br>  We had two options for the connection manager: Pgpool and PgBouncer.  But the first one does not support transactional operation with the base, so we chose PgBouncer. <br><br>  We set up the following operation scheme: our application refers to one PgBouncer, followed by PostgreSQL masters, and for each master, one replica with asynchronous replication. <br><img src="https://habrastorage.org/webt/ql/uq/vh/qluqvh64yeyzwvow79wc3tt0nm0.png"><br><br>  At the same time, we could not store the entire amount of data in PostgreSQL and the speed of working with the database was important for us, so we started to sharpen PostgreSQL at the application level.  The scheme described above is relatively convenient for this: when adding a new shard to PostgreSQL, it is enough to update the PgBouncer configuration and the application can immediately work with the new shard. <br><br><h3>  Fault tolerance PgBouncer </h3><br>  This scheme worked until the only instance of the PgBouncer died.  We are in AWS, where all instances are running on hardware that periodically dies.  In such cases, the instance simply moves to a new hardware and works again.  It happened with PgBouncer, however, it became unavailable.  The result of this fall was the inaccessibility of our service for 25 minutes.  AWS for such situations recommends the use of redundancy on the user's side, which was not implemented by us at that time. <br><br>  After that, we seriously thought about the resiliency of PgBouncer and PostgreSQL clusters, because this situation could be repeated with any instance in our AWS account. <br><br>  We built the PgBouncer fault tolerance scheme as follows: all application servers access the Network Load Balancer, which is supported by two PgBouncer.  Each PgBouncer looks at the same PostgreSQL master of each shard.  In case of a repetition of the situation with the AWS instance crash, all traffic is redirected through another PgBouncer.  Fault tolerance Network Load Balancer provides AWS. <br><br>  This scheme allows you to easily add new PgBouncer servers. <br><img src="https://habrastorage.org/webt/05/uc/da/05ucdayudomunfsxjc_gggs5abe.png"><br><br><h2>  Creating a PostgreSQL Failover Cluster </h2><br>  In solving this problem, we considered various options: self-signed failover, repmgr, AWS RDS, Patroni. <br><br><h3>  Custom scripts </h3><br>  They can monitor the wizard and, in the event of a crash, push the cue to the wizard and update the PgBouncer configuration. <br><br>  The advantages of this approach are maximum simplicity, because you write the scripts yourself and understand exactly how they work. <br><br>  Minuses: <br><br><ul><li>  The master could not die, a network failure could occur instead.  Failover, without knowing it, will advance the cue to the master, and the old master will continue to work.  As a result, we will receive two servers in the master role and will not know which of them contains the latest actual data.  This situation is also called split-brain; </li><li>  We were left without a replica.  In our configuration, the master and one replica, after switching the replica moves to the master and we no longer have replicas, so we have to manually add a new replica; </li><li>  We need additional monitoring of work failover, while we have 12 PostgreSQL shards, which means we need to monitor 12 clusters.  If you increase the number of shards, you must still remember to update failover. </li></ul><br>  Handwritten failover looks very difficult and requires nontrivial support.  With a single PostgreSQL cluster, this will be the easiest option, but it doesn‚Äôt scale, so it‚Äôs not suitable for us. <br><br><h3>  Repmgr </h3><br>  Replication Manager for PostgreSQL clusters that can manage the operation of a PostgreSQL cluster.  In this case, there is no automatic failover ‚Äúout of the box‚Äù, so for the work you will need to write your ‚Äúwrapper‚Äù over the finished solution.  So everything can turn out even more complicated than with samopisny scripts, so we didn‚Äôt even try Repmgr. <br><br><h3>  AWS RDS </h3><br>  It supports everything you need for us, can make backups and maintains a pool of connections.  It has automatic switching: when the master dies, the replica becomes the new master, and AWS changes the dns entry to the new master, and the replicas can be located in different AZ. <br><br>  The disadvantages are the lack of fine-tuning.  As an example of fine-tuning: on our instances there are restrictions for tcp connections, which, unfortunately, cannot be done in RDS: <br><br><pre><code class="python hljs">net.ipv4.tcp_keepalive_time=<span class="hljs-number"><span class="hljs-number">10</span></span> net.ipv4.tcp_keepalive_intvl=<span class="hljs-number"><span class="hljs-number">1</span></span> net.ipv4.tcp_keepalive_probes=<span class="hljs-number"><span class="hljs-number">5</span></span> net.ipv4.tcp_retries2=<span class="hljs-number"><span class="hljs-number">3</span></span></code> </pre> <br>  In addition, at AWS RDS, the price is almost twice as expensive as the normal price of the instance, which was the main reason for rejecting this decision. <br><br><h3>  Patroni </h3><br>  This is a python template for managing PostgreSQL with good documentation, automatic failover and source code on github. <br><br>  Advantages of Patroni: <br><br><ul><li>  Each configuration parameter is listed, it is clear how what works; </li><li>  Automatic failover works out of the box; </li><li>  It is written in python, and since we write a lot in python ourselves, it will be easier for us to deal with problems and, possibly, even help the development of the project; </li><li>  Completely controls PostgreSQL, allows you to change the configuration at once on all the nodes of the cluster, and if the cluster is restarted to apply the new configuration, this can be done again using Patroni. </li></ul><br>  Minuses: <br><br><ul><li>  From the documentation it is not clear how to work with PgBouncer correctly.  Although it is difficult to call it a minus, because the task of Patroni is to manage PostgreSQL, and how connections to Patroni will go is our problem; </li><li>  There are few examples of the implementation of Patroni on large volumes, with many examples of implementation from scratch. </li></ul><br>  As a result, we chose Patroni to create a failover cluster. <br><br><h2>  Patroni implementation process </h2><br>  Before Patroni, we had 12 PostgreSQL shards in the configuration of one master and one replica with asynchronous replication.  The application servers accessed the databases through the Network Load Balancer, which was preceded by two instances with the PgBouncer, and all PostgreSQL servers were behind them. <br><img src="https://habrastorage.org/webt/05/uc/da/05ucdayudomunfsxjc_gggs5abe.png"><br><br>  To implement Patroni, we needed to choose a distributed storage cluster configuration.  Patroni works with distributed configuration storage systems such as etcd, Zookeeper, onsul.  We just have a full-fledged Consul cluster, which works in conjunction with Vault and we don‚Äôt use it anymore.  An excellent reason to start using Consul for its intended purpose. <br><br><h3>  How Patroni works with Consul </h3><br>  We have a Consul cluster, which consists of three nodes and a Patroni cluster, which consists of a leader and a replica (in Patroni, the master is called the cluster leader, and the slaves are replicas).  Each Patroni cluster instance constantly sends information about the cluster status to the Consul.  Therefore, from onsul you can always find out the current configuration of the Patroni cluster and who is the leader at the moment. <br><br><img src="https://habrastorage.org/webt/jx/j5/is/jxj5ispkzoegn8x80dw-qtynw3q.png"><br><br>  To connect Patroni to onsul, it is enough to study the official documentation, in which it is written that it is necessary to specify a host in the format of http or https, depending on how we work with Consul, and the connection scheme, optionally: <br><br><pre> <code class="plaintext hljs">host: the host:port for the Consul endpoint, in format: http(s)://host:port scheme: (optional) http or https, defaults to http</code> </pre> <br>  It looks easy, but then the pitfalls begin.  With onsul we work on a secure connection via https and our connection config will look like this: <br><br><pre> <code class="python hljs">consul: host: https://server.production.consul:<span class="hljs-number"><span class="hljs-number">8080</span></span> verify: true cacert: {{ consul_cacert }} cert: {{ consul_cert }} key: {{ consul_key }}</code> </pre> <br>  But that doesn't work.  At startup, Patroni cannot connect to onsul, because it tries to go via http anyway. <br><br>  The Patroni source code helped to deal with the problem.  Good thing it is written in python.  It turns out the host parameter does not parse, and the protocol must be specified in the scheme.  This is what a working configuration unit for working with Consul looks like with us: <br><br><pre> <code class="python hljs">consul: host: server.production.consul:<span class="hljs-number"><span class="hljs-number">8080</span></span> scheme: https verify: true cacert: {{ consul_cacert }} cert: {{ consul_cert }} key: {{ consul_key }}</code> </pre> <br><h3>  Consul-template </h3><br>  So, we selected storage for a configuration.  Now you need to understand how PgBouncer will switch its configuration when the leader changes in the Patroni cluster.  In the documentation for this question there is no answer, because  There, in principle, work with PgBouncer is not described. <br><br>  In search of a solution, we found an article (the name, unfortunately, I don‚Äôt remember), where it was written that the Consul-template helped a lot with PgBouncer and Patroni.  This prompted us to research the work of the Consul-template. <br><br>  It turned out that the Consul-template constantly monitors the configuration of the PostgreSQL cluster in the Consul.  When changing the leader, it updates the PgBouncer configuration and sends the command to reload it. <br><br><img src="https://habrastorage.org/webt/iv/_f/j-/iv_fj-sjbnqtfabqlnmu986sa0o.png"><br><br>  A big plus of the template is that it is stored as a code, so when adding a new shard, it is enough to make a new commit and update the template automatically, supporting the principle of Infrastructure as code. <br><br><h3>  New architecture with Patroni </h3><br>  As a result, we got the following work scheme: <br><img src="https://habrastorage.org/webt/7b/-m/-v/7b-m-vyorrbbuognzt2qx2-fymm.png"><br><br>  All application servers access the balancer ‚Üí there are two instances of PgBouncer behind it. On each instance, a Consul-template is running, which monitors the status of each Patroni cluster and monitors the relevance of the PgBouncer config, which sends requests to the current leader of each cluster. <br><br><h3>  Manual testing </h3><br>  Before launching the prod, we launched this scheme on a small test environment and checked the operation of automatic switching.  They opened the board, moved the sticker and at that moment ‚Äúkilled‚Äù the cluster leader.  In AWS, all you have to do is turn off the instance through the console. <br><br><img src="https://habrastorage.org/webt/ly/yf/aj/lyyfaj6bxodfoaqrsds5j00ycnw.gif"><br><br>  The sticker for 10-20 seconds came back, and then again began to move normally.  This means that the Patroni cluster worked correctly: changed the leader, sent the information to onsul, and onsul-template immediately picked up this information, replaced the PgBouncer configuration and sent the command to reload. <br><br><h2>  How to survive under high load and keep the minimum downtime? </h2><br>  Everything works perfectly!  But new questions appear: How does this work under high loads?  How to quickly and safely roll out everything in production? <br><br>  The test environment in which we conduct load testing helps us to answer the first question.  It is completely identical to production by architecture and has generated test data that is approximately equal in volume to production.  We decide to just ‚Äúkill‚Äù one of the PostgreSQL masters during the test and see what happens.  But before that, it is important to check the automatic rolling, because in this environment we have several PostgreSQL shards, so we will get excellent testing of the configuration scripts before selling. <br><br>  Both tasks are ambitious, but we have PostgreSQL 9.6.  Can we immediately upgrade to 11.2? <br><br>  We decide to do it in 2 stages: first upgrade to version 11.2, then launch Patroni. <br><br><h3>  PostgreSQL update </h3><br>  To quickly update the PostgreSQL version, you need to use the <b>-k</b> option, in which you create hard links on the disk and do not need to copy your data.  On bases of 300-400 GB, the update takes 1 second. <br><br>  We have a lot of shards, so the update needs to be done automatically.  To do this, we wrote Ansible playbook, which performs the whole update process for us: <br><br><pre> <code class="plaintext hljs">/usr/lib/postgresql/11/bin/pg_upgrade \ &lt;b&gt;--link \&lt;/b&gt; --old-datadir='' --new-datadir='' \ --old-bindir='' --new-bindir='' \ --old-options=' -c config_file=' \ --new-options=' -c config_file='</code> </pre> <br>  It is important to note here that before launching the upgrade, it is necessary to execute it with the <b>--check</b> parameter in order to be sure of the possibility of an upgrade.  Also, our script makes configs substitution at the time of the upgrade.  The script was completed in 30 seconds, this is an excellent result. <br><br><h3>  Running patroni </h3><br>  To solve the second problem, just look at the configuration of Patroni.  The official repository has an example configuration with initdb, which is responsible for initializing the new database when Patroni is first launched.  But since we have a ready base, we simply removed this section from the configuration. <br><br>  When we started to install Patroni on a ready-made PostgreSQL cluster and launch it, we faced a new problem: both servers were launched as leader.  Patroni knows nothing about the early state of the cluster and tries to start both servers as two separate clusters with the same name.  To solve this problem, you need to delete the data directory on the slave: <br><br><pre> <code class="plaintext hljs">rm -rf /var/lib/postgresql/</code> </pre> <br>  <b>This must be done only on the slave!</b> <br><br>  When a clean replica is connected, Patroni makes a basebackup leader and restores it to a replica, and then catches up with the current state of the wal-logs. <br><br>  Another difficulty we encountered is that all PostgreSQL clusters are called main by default.  When each cluster knows nothing about the other - this is normal.  But when you want to use Patroni, all clusters must have a unique name.  The solution is to change the cluster name in the PostgreSQL configuration. <br><br><h3>  Load test </h3><br>  We have launched a test that simulates the work of users on the boards.  When the load reached our daily average, we repeated the exact same test, we turned off one instance with the leader PostgreSQL.  The automatic failover worked as we expected: Patroni changed the leader, the Consul-template updated the PgBouncer configuration and sent the command to reload.  According to our graphs in Grafana it was clear that there are delays for 20-30 seconds and a small amount of errors from the servers associated with the connection to the database.  This is a normal situation, such values ‚Äã‚Äãare valid for our failover and definitely better than the downtime of the service. <br><br><h2>  Patroni output to production </h2><br>  As a result, we got the following plan: <br><br><ul><li>  Deploy the Consul-template to the PgBouncer server and launch; </li><li>  PostgreSQL updates to version 11.2; </li><li>  Cluster name change; </li><li>  Launch Patroni Cluster. </li></ul><br>  At the same time, our scheme allows you to make the first item at almost any time; we can remove each PgBouncer from work in turn and execute it with deploy and launch the consul-template.  So we did. <br><br>  For fast rolling, we used Ansible, since we already checked the entire playbook on the test environment, and the execution time of the full script was from 1.5 to 2 minutes for each shard.  We could roll everything one by one on each shard without stopping our service, but we would have to shut down every PostgreSQL for a few minutes.  In this case, users whose data is on this shard, could not fully work at this time, and this is unacceptable for us. <br><br>  The way out of this situation was planned maintenance, which runs with us every 3 months.  This is the window for scheduled work when we completely turn off our service and update the database instances.  One week remained until the next window, and we decided to just wait and prepare further.  During the wait, we additionally secured ourselves: for each shard, PostgreSQL was picked up in a spare replica in case of failure, to save the latest data, and added a new instance for each shard, which should become a new replica in the Patroni cluster, so as not to execute the command to delete data .  All this helped to minimize the risk of error. <br><img src="https://habrastorage.org/webt/ps/ge/yh/psgeyhvrazg-zd1nrochwhl1hag.png"><br><br>  We restarted our service, everything worked as it should, users continued to work, but on the charts we noticed an abnormally high load on the Consul server. <br><img src="https://habrastorage.org/webt/uk/b9/gu/ukb9guaj4wfabq60v262qe098am.png"><br><br>  Why we did not see it on the test environment?  This problem illustrates very well that it is necessary to follow the principle of Infrastructure as code and refine the entire infrastructure, starting with test environments and ending with production.  Otherwise, it is very easy to get such a problem that we got.  What happened?  Sonsul first appeared in production, and then in test environments, and as a result, Consul's version was higher in test environments than in production.  Just in one of the releases, a CPU leak was solved when working with the consul-template.  Therefore, we simply updated Consul, thus solving the problem. <br><br><h3>  Restart Patroni cluster </h3><br>  However, we received a new problem, which was not even suspected.  When updating Consul, we simply remove the Consul node from the cluster using the consul leave command ‚Üí Patroni connects to another Consul server ‚Üí everything works.  But when we reached the last instance of the Consul cluster and sent the consul leave command to it, all Patroni clusters just restarted, and we saw the following error in the logs: <br><br><pre> <code class="plaintext hljs">ERROR: get_cluster Traceback (most recent call last): ... RetryFailedError: 'Exceeded retry deadline' ERROR: Error communicating with DCS &lt;b&gt;LOG: database system is shut down&lt;/b&gt;</code> </pre> <br>  The cluster Patroni was unable to obtain information about its cluster and restarted. <br><br>  To find a solution, we turned to the authors of Patroni through an issue on github.  They suggested improvements to our configuration files: <br><br><pre> <code class="python hljs">consul: consul.checks: [] bootstrap: dcs: retry_timeout: <span class="hljs-number"><span class="hljs-number">8</span></span></code> </pre> <br>  We were able to repeat the problem on the test environment and tested these parameters there, but, unfortunately, they did not work. <br><br>  The problem still remains unresolved.  We plan to try the following solutions: <br><br><ul><li>  Use the Consul-agent on each instance of the Patroni cluster; </li><li>  Fix the problem in the code. </li></ul><br>  We understand the location of the error: the problem is probably in the use of default timeout, which is not overridden by the configuration file.  When the last onsul server is removed from the cluster, the entire Consul cluster hangs, which lasts longer than a second, because of this Patroni cannot get the cluster status and completely restarts the entire cluster. <br><br>  Fortunately, no more errors, we have not met. <br><br><h2>  Results of using Patroni </h2><br>  After the successful launch of Patroni, we added an additional replica in each cluster.  Now in each cluster there is a quorum similarity: one leader and two replicas, - for safety in case of split-brain when switching. <br><img src="https://habrastorage.org/webt/ef/pn/er/efpner_0rhtuim1zt0gwrhff3b8.png"><br><br>  On production Patroni works more than three months.  During this time he has already managed to help us out.  Recently, the leader of one of the clusters died in AWS, automatic failover worked and users continued to work.  Patroni completed his main task. <br><br>  <b>A small summary of the use of Patroni:</b> <br><br><ul><li>  Convenience configuration changes.  It is enough to change the configuration on one instance and it will pull up on the entire cluster.  If a reboot is required to apply the new configuration, Patroni will report it.  Patroni can restart the entire cluster with a single command, which is also very convenient. </li><li>  Automatic failover works and has already managed to help us out. </li><li>  PostgreSQL update without downtime application.  You must first upgrade the replicas to the new version, then change the leader in the Patroni cluster and update the old leader.  When this happens, the necessary testing of automatic failover occurs. </li></ul></div><p>Source: <a href="https://habr.com/ru/post/457326/">https://habr.com/ru/post/457326/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../457308/index.html">On the way of Sergei Pavlovich Korolev. Modern Russian manned project. Part 2. Rocket</a></li>
<li><a href="../457310/index.html">Biology of information dependence. Part one</a></li>
<li><a href="../457316/index.html">How is the role-playing game in the real world for guests of Armenia with trips in half of the country arranged?</a></li>
<li><a href="../45732/index.html">Parallel Extensions for .net 3.5</a></li>
<li><a href="../457324/index.html">Digital events in Moscow from June 24 to June 30</a></li>
<li><a href="../457328/index.html">Categories instead of directories, or Semantic Filesystem for Linux</a></li>
<li><a href="../457332/index.html">How to quickly see the interesting warnings that the PVS-Studio analyzer issues for C and C ++ code?</a></li>
<li><a href="../457334/index.html">TacacsGUI, Configuration Manager</a></li>
<li><a href="../457336/index.html">Consequences of late removal of wisdom teeth</a></li>
<li><a href="../457338/index.html">Blitz with Ilya Krasinsky: how to shoot bad hypotheses, why to dismiss the product and how to grow in a minimum of actions?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>