<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The wonderful world of Word Embeddings: what are they and why are they needed?</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="You should start from the stove, that is, with the formulation of the problem. Where does the word embedding task come from? 
 Lyrical digression: Unf...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The wonderful world of Word Embeddings: what are they and why are they needed?</h1><div class="post__text post__text-html js-mediator-article"><p>  You should start from the stove, that is, with the formulation of the problem.  Where does the word embedding task come from? <br>  <em>Lyrical digression:</em> Unfortunately, the Russian-speaking community has not yet developed a single term for this concept, so we will use the English language. <br>  By itself, embedding is a comparison of an arbitrary entity (for example, a node in a graph or a piece of a picture) to some vector. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/3e8/12f/d16/3e812fd164a08f5e4f195000fecf988f.png" alt="image"></p><a name="habracut"></a><br><p>  Today we are talking about words and it is worth discussing how to make such a comparison of the vector of a word. <br>  Let us return to the subject: here we have words and there is a computer that must somehow work with these words.  The question is - how will the computer work with words?  After all, the computer can not read, and in general is designed very differently than a person.  The very first idea that comes to mind is simply to encode words with numbers in the order in which they appear in the dictionary.  The idea is very productive in its simplicity - the natural series is endless and you can renumber all words without fear of problems.  (For a second, let's forget about type restrictions, especially since you can stuff numbers from 0 to 2 ^ 64 - 1 into a 64-bit word, which is significantly more than the number of all words of all known languages.) </p><br><p>  But this idea has a significant drawback: the words in the dictionary follow in alphabetical order, and when you add a word, you need to renumber most of the words.  But even this is not so important, but what is important is that the literal spelling of a word has nothing to do with its meaning (this hypothesis was expressed at the end of the 19th century by the famous linguist Ferdinand de Saussure).  In fact, the words ‚Äúrooster‚Äù, ‚Äúchicken‚Äù and ‚Äúchicken‚Äù have very little in common with each other and stand in the dictionary far from each other, although they obviously mean a male, female and young of one species of bird.  That is, we can distinguish two types of proximity words: lexical and semantic.  As we see in the chicken example, these proximity do not necessarily coincide.  For clarity, it is possible to cite the opposite example of lexically close, but semantically distant words - "ash" and "gold."  (If you never thought, then the name Cinderella comes from the first.) <br>  In order to be able to represent semantic proximity, it was proposed to use embedding, that is, to associate a word with a vector representing its meaning in the ‚Äúspace of meanings‚Äù. </p><br><p>  What is the easiest way to get a vector from a word?  It seems that it will be natural to take the vector of the length of our dictionary and put only one unit in the position corresponding to the number of the word in the dictionary.  This approach is called one-hot encoding (OHE).  OHE still does not have semantic proximity properties: </p><br><p><img src="https://habrastorage.org/web/747/5ad/1b3/7475ad1b3c4349c58c14f063b286e608.png"></p><br><p>  So we need to find another way to convert words into vectors, but OHE will still come in handy. </p><br><p>  Let's step back a bit - the meaning of one word may not be so important to us, because  Speech (both oral and written) consists of sets of words that we call texts.  So if we want to somehow present the texts, then we take the OHE vector of each word in the text and put it together.  Those.  at the output we get just a count of the number of different words in the text in one vector.  This approach is called ‚Äúbag of words‚Äù (BoW), because we lose all the information about the relative position of words within the text. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/b91/46a/5be/b9146a5be315f422479e40d85e995289.jpg" alt="image"></p><br><p> But despite the loss of this information so the texts can already be compared.  For example, using a <a href="https://github.com/madrugado/word2vec-article/blob/master/cosine-documents.ipynb">cosine measure</a> . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/654/403/84a/65440384afca707b6e53e1bc5a354889.gif" alt="image"></p><br><p>  We can go further and present our corpus (set of texts) as a word-document matrix (term-document).  It is worth noting that in the field of information retrieval (information retrieval), this matrix is ‚Äã‚Äãcalled the "reverse index" (inverted index), in the sense that a regular / direct index looks like a "document word" and is very inconvenient for a quick search.  But this again goes beyond the scope of this article. </p><br><p>  This matrix leads us to thematic models, where the word-document matrix is ‚Äã‚Äãattempted to be represented as a word-theme and subject-document product of two matrices.  In the simplest case, we take the matrix and, using the SVD decomposition, we get the representation of words through themes and documents through themes: </p><br><p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f6ce07d37d7cc1057be2b032cccc5cf580945af9" alt="image"></p><br><p>  Here <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1"><span class="MJXp-msubsup" id="MJXp-Span-2"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-3" style="margin-right: 0.05em;">t</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-4" style="vertical-align: -0.4em;">i</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.639ex" height="2.298ex" viewBox="0 -728.2 705.8 989.6" role="img" focusable="false" style="vertical-align: -0.607ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-74" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-69" x="511" y="-213"></use></g></svg></span><script type="math/tex" id="MathJax-Element-1"> t_i </script>  - the words, <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-5"><span class="MJXp-msubsup" id="MJXp-Span-6"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-7" style="margin-right: 0.05em;">d</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-8" style="vertical-align: -0.4em;">i</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.009ex" height="2.419ex" viewBox="0 -780.1 864.8 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-64" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-69" x="736" y="-213"></use></g></svg></span><script type="math/tex" id="MathJax-Element-2"> d_i </script>  - documents.  But this will be the subject of another article, and now we will return to our main topic - the vector representation of words. </p><br><p>  Suppose we have such a case: </p><br><pre><code class="python hljs">s = [<span class="hljs-string"><span class="hljs-string">'Mars has an athmosphere'</span></span>, <span class="hljs-string"><span class="hljs-string">"Saturn 's moon Titan has its own athmosphere"</span></span>, <span class="hljs-string"><span class="hljs-string">'Mars has two moons'</span></span>, <span class="hljs-string"><span class="hljs-string">'Saturn has many moons'</span></span>, <span class="hljs-string"><span class="hljs-string">'Io has cryo-vulcanoes'</span></span>]</code> </pre> <br><p>  Using the SVD transform, select only the first two components, and draw: </p><br><p><img src="https://habrastorage.org/web/cdf/330/1e7/cdf3301e7cb740eba7d1ea787f20b1a6.png"></p><br><p>  What is interesting in this picture?  The fact that Titan and Io are far apart, although both of them are Saturn‚Äôs moons, there‚Äôs nothing in our corps.  The words "atmosphere" and "Saturn" are very close to each other, although they are not synonymous.  At the same time, the ‚Äútwo‚Äù and ‚Äúmany‚Äù stand side by side, which is logical.  But the general meaning of this example is that the results that you get are very dependent on the case with which you work.  All the code for getting the picture above can be viewed <a href="https://github.com/madrugado/word2vec-article/blob/master/svd-example-2.ipynb">here</a> . </p><br><p>  The narration logic introduces the following modification of the term-document matrix, the TF-IDF formula.  This abbreviation means "term frequency - inverse document frequency". </p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-9"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-10">T</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-11">F</span><span class="MJXp-mo" id="MJXp-Span-12" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-13">I</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-14">D</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-15">F</span><span class="MJXp-mo" id="MJXp-Span-16" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-17">w</span><span class="MJXp-mo" id="MJXp-Span-18" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-19">d</span><span class="MJXp-mo" id="MJXp-Span-20" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-21">C</span><span class="MJXp-mo" id="MJXp-Span-22" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-23" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-24">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-25">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-26">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-27">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-28">c</span><span class="MJXp-mrow" id="MJXp-Span-29"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-30">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-31">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-32">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-33">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-34">t</span><span class="MJXp-mo" id="MJXp-Span-35" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-36">w</span><span class="MJXp-mo" id="MJXp-Span-37" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-38">d</span><span class="MJXp-mo" id="MJXp-Span-39" style="margin-left: 0em; margin-right: 0em;">)</span></span><span class="MJXp-mrow" id="MJXp-Span-40"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-41">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-42">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-43">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-44">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-45">t</span><span class="MJXp-mo" id="MJXp-Span-46" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-47">d</span><span class="MJXp-mo" id="MJXp-Span-48" style="margin-left: 0em; margin-right: 0em;">)</span></span><span class="MJXp-mo" id="MJXp-Span-49" style="margin-left: 0.267em; margin-right: 0.267em;">‚àó</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-50">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-51">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-52">g</span><span class="MJXp-mo" id="MJXp-Span-53" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-54">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-55">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-56">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-57">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-58">c</span><span class="MJXp-mrow" id="MJXp-Span-59"><span class="MJXp-mtext" id="MJXp-Span-60">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-61">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-62">u</span><span class="MJXp-msubsup" id="MJXp-Span-63"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-64" style="margin-right: 0.05em;">m</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-65" style="vertical-align: -0.4em;"><span class="MJXp-msup" id="MJXp-Span-66"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-67" style="margin-right: 0.05em;">d</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-68" style="vertical-align: 0.5em;">‚Ä≤</span></span><span class="MJXp-mtext" id="MJXp-Span-69">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-70">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-71">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-72">C</span></span></span><span class="MJXp-mrow" id="MJXp-Span-73"><span class="MJXp-mtext" id="MJXp-Span-74">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-75">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-76">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-77">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-78">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-79">b</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-80">b</span><span class="MJXp-mrow" id="MJXp-Span-81"><span class="MJXp-mn" id="MJXp-Span-82">1</span></span><span class="MJXp-mo" id="MJXp-Span-83" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-84">w</span><span class="MJXp-mo" id="MJXp-Span-85" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-msup" id="MJXp-Span-86"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-87" style="margin-right: 0.05em;">d</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-88" style="vertical-align: 0.5em;">‚Ä≤</span></span><span class="MJXp-mo" id="MJXp-Span-89" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MJXp-mrow" id="MJXp-Span-90"><span class="MJXp-mrow" id="MJXp-Span-91"><span class="MJXp-mo" id="MJXp-Span-92" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-93">C</span><span class="MJXp-mrow" id="MJXp-Span-94"><span class="MJXp-mo" id="MJXp-Span-95" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span></span><span class="MJXp-mo" id="MJXp-Span-96" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processed" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="88.711ex" height="2.78ex" viewBox="0 -883.9 38194.9 1197.1" role="img" focusable="false" style="vertical-align: -0.728ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-54" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-46" x="704" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-2212" x="1676" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-49" x="2676" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-44" x="3181" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-46" x="4009" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-28" x="4759" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-77" x="5148" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-2C" x="5865" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-64" x="6310" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-2C" x="6834" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-43" x="7279" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-29" x="8039" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-3D" x="8707" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-66" x="10013" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-72" x="10563" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-61" x="11015" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-63" x="11544" y="0"></use><g transform="translate(11978,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-63" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-6F" x="433" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-75" x="919" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-6E" x="1491" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-74" x="2092" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-28" x="2453" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-77" x="2843" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-2C" x="3559" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-64" x="4004" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-29" x="4528" y="0"></use></g><g transform="translate(16896,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-63" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-6F" x="433" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-75" x="919" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-6E" x="1491" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-74" x="2092" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-28" x="2453" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-64" x="2843" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-29" x="3366" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-2217" x="20874" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-6C" x="21596" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-6F" x="21895" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-67" x="22380" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-28" x="22861" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-66" x="23500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-72" x="24051" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-61" x="24502" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-63" x="25032" y="0"></use><g transform="translate(25465,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-73" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-75" x="719" y="0"></use><g transform="translate(1292,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-6D" x="0" y="0"></use><g transform="translate(878,-235)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-64" x="0" y="0"></use><use transform="scale(0.5)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-2032" x="741" y="596"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-69" x="1172" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-6E" x="1518" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-43" x="2118" y="0"></use></g></g><g transform="translate(4306,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-6D" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-61" x="1128" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-74" x="1658" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-68" x="2019" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-62" x="2596" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-62" x="3025" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-31" x="3455" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-28" x="3955" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-77" x="4345" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-2C" x="5061" y="0"></use><g transform="translate(5506,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-64" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-2032" x="741" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-29" x="6325" y="0"></use></g></g><g transform="translate(36487,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-7C" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-43" x="278" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-7C" x="1039" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMAIN-29" x="37805" y="0"></use></g></svg></span></div><script type="math/tex;mode=display" id="MathJax-Element-3"> TF-IDF (w, d, C) = \ frac {count (w, d)} {count (d)} * log (\ frac {\ sum_ {d '\ in C} {\ mathbb {1} ( w, d ')}} {| C |}) </script></p><br><p>  Let's try to figure out what it is.  So TF is the frequency of the word. <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-97"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-98">w</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.664ex" height="1.455ex" viewBox="0 -520.7 716.5 626.5" role="img" focusable="false" style="vertical-align: -0.246ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-77" x="0" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-4"> w </script>  in the text <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-99"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-100">d</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.216ex" height="2.057ex" viewBox="0 -780.1 523.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-64" x="0" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-5"> d </script>  , there is nothing complicated.  But IDF is a much more interesting thing: it is the logarithm of the inverse of the frequency of the word <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-101"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-102">w</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.664ex" height="1.455ex" viewBox="0 -520.7 716.5 626.5" role="img" focusable="false" style="vertical-align: -0.246ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-77" x="0" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-6"> w </script>  in the case <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-103"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-104">C</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.766ex" height="2.057ex" viewBox="0 -780.1 760.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/329410/&amp;xid=25657,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhPDgERWwqU_bmdVPLwKrSY65raig#MJMATHI-43" x="0" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-7"> C </script>  .  The prevalence is the ratio of the number of texts in which the search word is found to the total number of texts in the corpus.  With the help of TF-IDF, texts <a href="https://github.com/madrugado/word2vec-article/blob/master/cosine-documents-tf-idf.ipynb">can</a> also <a href="https://github.com/madrugado/word2vec-article/blob/master/cosine-documents-tf-idf.ipynb">be compared</a> , and this can be done with less apprehension than when using normal frequencies. </p><br><h1 id="novaya-epoha">  New era </h1><br><p>  The approaches described above were (and remain) good for times (or areas), where the number of texts is small and the dictionary is limited, although, as we have seen, there also have their own difficulties.  But with the advent of the Internet into our life, everything became both more complicated and simpler: a great variety of texts appeared in the access, and these texts with a changing and expanding vocabulary.  It was necessary to do something with this, and previously known models could not cope with such a volume of texts.  The number of words in English is very roughly a million - the matrix of joint occurrences of only pairs of words will be 10 ^ 6 x 10 ^ 6.  Even now, such a matrix doesn‚Äôt really crawl into the memory of computers, and, say, 10 years ago, one could not dream of such a thing.  Of course, many methods were invented to simplify or parallelize the processing of such matrices, but all of these were palliative methods. </p><br><p>  And then, as is often the case, a way out was proposed on the principle ‚Äúthe one who bothers us will help us!‚Äù Namely, in 2013 then little-known Czech graduate student Tomash Mikolov proposed his own approach to word embedding, which he called word2vec .  His approach is based on another important hypothesis, which in science is called the locality hypothesis - ‚Äúwords that occur in the same environments have similar meanings‚Äù.  Intimacy in this case is understood very broadly, as the fact that only matching words can stand next to each other.  For example, the phrase "clockwork alarm clock" is familiar to us.  And we cannot say ‚Äúa clockwork orange‚Äù * - these words are not combined. </p><br><p>  Based on this hypothesis, Tomash Mikolov proposed a new approach that did not suffer from large amounts of information, but on the contrary won [1]. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/79c/7ea/4a0/79c7ea4a0541e33e768b5ae2df6205f7.jpg" alt="image"></p><br><p>  The model proposed by Mikolov is very simple (and therefore so good) - we will predict the probability of a word according to its environment (context).  That is, we will learn such word vectors so that the probability assigned by the model to a word is close to the probability of meeting this word in this environment in a real text. </p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-105"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-106">P</span><span class="MJXp-mo" id="MJXp-Span-107" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-108"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-109" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-110" style="vertical-align: -0.4em;">o</span></span><span class="MJXp-mrow" id="MJXp-Span-111"><span class="MJXp-mo" id="MJXp-Span-112" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-msubsup" id="MJXp-Span-113"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-114" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-115" style="vertical-align: -0.4em;">c</span></span><span class="MJXp-mo" id="MJXp-Span-116" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-117" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-118">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-119">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-120">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-121">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-122">c</span><span class="MJXp-mrow" id="MJXp-Span-123"><span class="MJXp-msubsup" id="MJXp-Span-124"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-125" style="margin-right: 0.05em;">e</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-126" style="vertical-align: 0.5em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-127">s</span><span class="MJXp-mo" id="MJXp-Span-128">(</span><span class="MJXp-msubsup" id="MJXp-Span-129"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-130" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-131" style="vertical-align: -0.4em;">o</span></span><span class="MJXp-mo" id="MJXp-Span-132">,</span><span class="MJXp-msubsup" id="MJXp-Span-133"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-134" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-135" style="vertical-align: -0.4em;">c</span></span><span class="MJXp-mo" id="MJXp-Span-136">)</span></span></span></span><span class="MJXp-mrow" id="MJXp-Span-137"><span class="MJXp-mtext" id="MJXp-Span-138">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-139">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-140">u</span><span class="MJXp-msubsup" id="MJXp-Span-141"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-142" style="margin-right: 0.05em;">m</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-143" style="vertical-align: -0.4em;"><span class="MJXp-msubsup" id="MJXp-Span-144"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-145" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-146" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-mtext" id="MJXp-Span-147">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-148">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-149">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-150">V</span></span></span><span class="MJXp-msubsup" id="MJXp-Span-151"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-152" style="margin-right: 0.05em;">e</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-153" style="vertical-align: 0.5em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-154">s</span><span class="MJXp-mo" id="MJXp-Span-155">(</span><span class="MJXp-msubsup" id="MJXp-Span-156"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-157" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-158" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-mo" id="MJXp-Span-159">,</span><span class="MJXp-msubsup" id="MJXp-Span-160"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-161" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-162" style="vertical-align: -0.4em;">c</span></span><span class="MJXp-mo" id="MJXp-Span-163">)</span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-8-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-8"> P (w_o | w_c) = \ frac {e ^ {s (w_o, w_c)}} {\ sum_ {w_i \ in V} e ^ {s (w_i, w_c)}} </script></p><br><p>  Here <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-164"><span class="MJXp-msubsup" id="MJXp-Span-165"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-166" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-167" style="vertical-align: -0.4em;">o</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-9-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-9"> w_o </script>  - vector of the target word, <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-168"><span class="MJXp-msubsup" id="MJXp-Span-169"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-170" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-171" style="vertical-align: -0.4em;">c</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-10-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-10"> w_c </script>  Is a certain vector of context calculated (for example, by averaging) from the vectors surrounding the desired word of other words.  BUT <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-172"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-173">s</span><span class="MJXp-mo" id="MJXp-Span-174" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-175"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-176" style="margin-right: 0.05em;">w</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-177" style="vertical-align: -0.4em;">1</span></span><span class="MJXp-mo" id="MJXp-Span-178" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-msubsup" id="MJXp-Span-179"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-180" style="margin-right: 0.05em;">w</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-181" style="vertical-align: -0.4em;">2</span></span><span class="MJXp-mo" id="MJXp-Span-182" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-11-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-11"> s (w_1, w_2) </script>  Is a function that maps one number to two vectors.  For example, this may be the cosine distance mentioned above. </p><br><p>  The above formula is called softmax, that is, ‚Äúsoft maximum‚Äù, soft - in the sense of differentiable.  This is necessary so that our model can learn using backpropagation, that is, the process of back propagation of an error. </p><br><p>  The training process is organized as follows: we take sequentially (2k + 1) words, the word in the center is the word that should be predicted.  And the surrounding words are the context of length k on each side.  Each word in our model is associated with a unique vector, which we change in the process of learning our model. </p><br><p>  In general, this approach is called CBOW - continuous bag of words, continuous, because we feed our models successively sets of words from text, and BoW because the order of words in the context is not important. </p><br><p>  Mikolov also immediately proposed another approach - directly opposite to CBOW, which he called skip-gram, that is, ‚Äúa phrase with a pass‚Äù.  We are trying to guess its context from a word given to us (more precisely, a context vector).  The rest of the model remains unchanged. </p><br><p>  What is worth noting: although there is clearly no semantics in the model, but only the statistical properties of the corpus of the texts, it turns out that the trained word2vec model can capture some semantic properties of words.  A classic example from the work of the author: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/9dd/1dc/5ea/9dd1dc5eabaa9a645a12a0a272dd5769.png" alt="image"></p><br><p>  The word "man" refers to the word "woman" in the same way as the word "uncle" to the word "aunt", which is completely natural and understandable for us, but in other models the same ratio of vectors can be achieved only with the help of special tricks.  Here - it comes naturally from the body of texts.  By the way, in addition to semantic links, syntactic ones are also captured, on the right, the ratio of singular and plural numbers is shown. </p><br><h2 id="bolee-slozhnye-veschi">  More complicated things </h2><br><p>  In fact, since then, improvements have been proposed to the already classic Word2Vec model.  The two most common will be described below.  But this section may be omitted without prejudice to the understanding of the article as a whole, if it seems too complicated. </p><br><h3 id="negative-sampling">  Negative sampling </h3><br><p>  In the standard CBoW model discussed above, we predict word probabilities and optimize them.  The function for optimization (minimization in our case) is Kullback-Leibler divergence: </p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-183"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-184">K</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-185">L</span><span class="MJXp-mo" id="MJXp-Span-186" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-187">p</span><span class="MJXp-mrow" id="MJXp-Span-188"><span class="MJXp-mo" id="MJXp-Span-189" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mrow" id="MJXp-Span-190"><span class="MJXp-mo" id="MJXp-Span-191" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-192">q</span><span class="MJXp-mo" id="MJXp-Span-193" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-194" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-195">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-196">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-197">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-198">t</span><span class="MJXp-mrow" id="MJXp-Span-199"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-200">p</span><span class="MJXp-mo" id="MJXp-Span-201" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-202">x</span><span class="MJXp-mo" id="MJXp-Span-203" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-204">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-205">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-206">g</span><span class="MJXp-mtext" id="MJXp-Span-207">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-208">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-209">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-210">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-211">c</span><span class="MJXp-mrow" id="MJXp-Span-212"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-213">p</span><span class="MJXp-mo" id="MJXp-Span-214" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-215">x</span><span class="MJXp-mo" id="MJXp-Span-216" style="margin-left: 0em; margin-right: 0em;">)</span></span><span class="MJXp-mrow" id="MJXp-Span-217"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-218">q</span><span class="MJXp-mo" id="MJXp-Span-219" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-220">x</span><span class="MJXp-mo" id="MJXp-Span-221" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-222">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-223">x</span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-12-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-12"> KL (p || q) = \ int {p (x) log \ frac {p (x)} {q (x)}} dx </script></p><br><p>  Here <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-224"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-225">p</span><span class="MJXp-mo" id="MJXp-Span-226" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-227">x</span><span class="MJXp-mo" id="MJXp-Span-228" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-13-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-13"> p (x) </script>  - the probability distribution of words, which we take from the body, <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-229"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-230">q</span><span class="MJXp-mo" id="MJXp-Span-231" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-232">x</span><span class="MJXp-mo" id="MJXp-Span-233" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-14-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-14"> q (x) </script>  - the distribution that our model generates.  Divergence is literally a ‚Äúdivergence,‚Äù as far as one distribution is different.  Since  our distributions in words, i.e.  are discrete, we can replace the integral with the sum in this formula: </p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-234"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-235">K</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-236">L</span><span class="MJXp-mo" id="MJXp-Span-237" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-238">p</span><span class="MJXp-mrow" id="MJXp-Span-239"><span class="MJXp-mo" id="MJXp-Span-240" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mrow" id="MJXp-Span-241"><span class="MJXp-mo" id="MJXp-Span-242" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-243">q</span><span class="MJXp-mo" id="MJXp-Span-244" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-245" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-246">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-247">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-248">u</span><span class="MJXp-msubsup" id="MJXp-Span-249"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-250" style="margin-right: 0.05em;">m</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-251" style="vertical-align: -0.4em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-252">x</span><span class="MJXp-mtext" id="MJXp-Span-253">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-254">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-255">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-256">V</span></span></span><span class="MJXp-mrow" id="MJXp-Span-257"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-258">p</span><span class="MJXp-mo" id="MJXp-Span-259" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-260">x</span><span class="MJXp-mo" id="MJXp-Span-261" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-262">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-263">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-264">g</span><span class="MJXp-mtext" id="MJXp-Span-265">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-266">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-267">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-268">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-269">c</span><span class="MJXp-mrow" id="MJXp-Span-270"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-271">p</span><span class="MJXp-mo" id="MJXp-Span-272" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-273">x</span><span class="MJXp-mo" id="MJXp-Span-274" style="margin-left: 0em; margin-right: 0em;">)</span></span><span class="MJXp-mrow" id="MJXp-Span-275"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-276">q</span><span class="MJXp-mo" id="MJXp-Span-277" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-278">x</span><span class="MJXp-mo" id="MJXp-Span-279" style="margin-left: 0em; margin-right: 0em;">)</span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-15-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-15"> KL (p || q) = \ sum_ {x \ in V} {p (x) log \ frac {p (x)} {q (x)}} </script></p><br><p>  It turned out that it is quite difficult to optimize this formula.  First of all, due to the fact that <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-280"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-281">q</span><span class="MJXp-mo" id="MJXp-Span-282" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-283">x</span><span class="MJXp-mo" id="MJXp-Span-284" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-16-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-16"> q (x) </script>  calculated using softmax throughout the dictionary.  (As we remember, in English now it is of the order of a million words.) It is worth noting that many words do not occur together, as we noted above, so most of the softmax calculations are redundant.  An elegant workaround called Negative Sampling was proposed.  The essence of this approach is that we maximize the probability of encountering the right word in a typical context (one that is often found in our corpus) and at the same time minimize the probability of a meeting in an atypical context (one that is rarely or not at all).  The formula above is written like this: </p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-285"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-286">N</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-287">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-288">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-289">S</span><span class="MJXp-mo" id="MJXp-Span-290" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-291"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-292" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-293" style="vertical-align: -0.4em;">o</span></span><span class="MJXp-mo" id="MJXp-Span-294" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-295" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-296">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-297">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-298">u</span><span class="MJXp-msubsup" id="MJXp-Span-299"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-300" style="margin-right: 0.05em;">m</span><span class="MJXp-script-box" style="height: 2.132em; vertical-align: -0.912em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mrow" id="MJXp-Span-319"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-320">i</span><span class="MJXp-mo" id="MJXp-Span-321">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-322">k</span></span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-301"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-302">i</span><span class="MJXp-mo" id="MJXp-Span-303">=</span><span class="MJXp-mn" id="MJXp-Span-304">1</span><span class="MJXp-mo" id="MJXp-Span-305">,</span><span class="MJXp-msubsup" id="MJXp-Span-306"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-307" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-308" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-mtext" id="MJXp-Span-309">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-310">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-311">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-312">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-313">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-314">k</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-315">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-316">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-317">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-318">D</span></span></span></span></span></span></span><span class="MJXp-mrow" id="MJXp-Span-323"><span class="MJXp-mo" id="MJXp-Span-324" style="margin-left: 0em; margin-right: 0.111em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-325">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-326">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-327">g</span><span class="MJXp-mo" id="MJXp-Span-328" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mn" id="MJXp-Span-329">1</span><span class="MJXp-mo" id="MJXp-Span-330" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-msubsup" id="MJXp-Span-331"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-332" style="margin-right: 0.05em;">e</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-333" style="vertical-align: 0.5em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-334">s</span><span class="MJXp-mo" id="MJXp-Span-335">(</span><span class="MJXp-msubsup" id="MJXp-Span-336"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-337" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-338" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-mo" id="MJXp-Span-339">,</span><span class="MJXp-msubsup" id="MJXp-Span-340"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-341" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-342" style="vertical-align: -0.4em;">o</span></span><span class="MJXp-mo" id="MJXp-Span-343">)</span></span></span><span class="MJXp-mo" id="MJXp-Span-344" style="margin-left: 0em; margin-right: 0em;">)</span></span><span class="MJXp-mo" id="MJXp-Span-345" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mtext" id="MJXp-Span-346">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-347">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-348">u</span><span class="MJXp-msubsup" id="MJXp-Span-349"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-350" style="margin-right: 0.05em;">m</span><span class="MJXp-script-box" style="height: 2.132em; vertical-align: -0.912em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mrow" id="MJXp-Span-371"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-372">j</span><span class="MJXp-mo" id="MJXp-Span-373">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-374">l</span></span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-351"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-352">j</span><span class="MJXp-mo" id="MJXp-Span-353">=</span><span class="MJXp-mn" id="MJXp-Span-354">1</span><span class="MJXp-mo" id="MJXp-Span-355">,</span><span class="MJXp-msubsup" id="MJXp-Span-356"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-357" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-358" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mtext" id="MJXp-Span-359">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-360">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-361">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-362">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-363">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-364">k</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-365">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-366">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-367">m</span><span class="MJXp-msup" id="MJXp-Span-368"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-369" style="margin-right: 0.05em;">D</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-370" style="vertical-align: 0.5em;">‚Ä≤</span></span></span></span></span></span></span></span><span class="MJXp-mrow" id="MJXp-Span-375"><span class="MJXp-mo" id="MJXp-Span-376" style="margin-left: 0em; margin-right: 0.111em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-377">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-378">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-379">g</span><span class="MJXp-mo" id="MJXp-Span-380" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mn" id="MJXp-Span-381">1</span><span class="MJXp-mo" id="MJXp-Span-382" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-msubsup" id="MJXp-Span-383"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-384" style="margin-right: 0.05em;">e</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-385" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-386">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-387">s</span><span class="MJXp-mo" id="MJXp-Span-388">(</span><span class="MJXp-msubsup" id="MJXp-Span-389"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-390" style="margin-right: 0.05em;">x</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-391" style="vertical-align: -0.4em;">j</span></span><span class="MJXp-mo" id="MJXp-Span-392">,</span><span class="MJXp-msubsup" id="MJXp-Span-393"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-394" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-395" style="vertical-align: -0.4em;">o</span></span><span class="MJXp-mo" id="MJXp-Span-396">)</span></span></span><span class="MJXp-mo" id="MJXp-Span-397" style="margin-left: 0em; margin-right: 0em;">)</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-17-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-17"> NegS (w_o) = \ sum_ {i = 1, x_i \ thicksim D} ^ {i = k} {- log (1 + e ^ {s (x_i, w_o)})} + \ sum_ {j = 1, x_j \ thicksim D '} ^ {j = l} {- log (1 + e ^ {- s (x_j, w_o)})} </script></p><br><p>  Here <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-398"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-399">s</span><span class="MJXp-mo" id="MJXp-Span-400" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-401">x</span><span class="MJXp-mo" id="MJXp-Span-402" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-403">w</span><span class="MJXp-mo" id="MJXp-Span-404" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-18-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-18"> s (x, w) </script>  - Exactly the same as in the original formula, but the rest is somewhat different.  First of all, you should pay attention to the fact that the formula now consists of two parts: positive ( <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-405"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-406">s</span><span class="MJXp-mo" id="MJXp-Span-407" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-408">x</span><span class="MJXp-mo" id="MJXp-Span-409" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-410">w</span><span class="MJXp-mo" id="MJXp-Span-411" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-19-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-19"> s (x, w) </script>  ) and negative ( <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-412"><span class="MJXp-mo" id="MJXp-Span-413" style="margin-left: 0em; margin-right: 0.111em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-414">s</span><span class="MJXp-mo" id="MJXp-Span-415" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-416">x</span><span class="MJXp-mo" id="MJXp-Span-417" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-418">w</span><span class="MJXp-mo" id="MJXp-Span-419" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-20-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-20"> -s (x, w) </script>  ).  The positive part is responsible for typical contexts, and <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-420"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-421">D</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-21-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-21"> D </script>  here is the distribution of co-occurrence of the word <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-422"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-423">w</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-22-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-22"> w </script>  and the rest of the words of the body.  The negative part - this is perhaps the most interesting - this is a set of words that are rarely found with our target word.  This set is generated from the distribution <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-424"><span class="MJXp-msup" id="MJXp-Span-425"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-426" style="margin-right: 0.05em;">D</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-427" style="vertical-align: 0.5em;">‚Ä≤</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-23-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-23"> D '</script>  which in practice is taken as uniform over all the words of the hull dictionary.  It was shown that such a function leads, with its optimization, to a result similar to the standard softmax [2]. </p><br><h3 id="hierarchical-softmax">  Hierarchical softmax </h3><br><p>  Also, people came in from the other side - you can not change the original formula, but try to count the softmax itself more efficiently.  For example, using a binary tree [3].  For all the words in the dictionary is built Huffman tree.  In the resulting tree <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-428"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-429">V</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-24-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-24"> V </script>  Words are located on the leaves of the tree. </p><br><p><img src="https://habrastorage.org/web/92d/ceb/034/92dceb0345cc459585d94b3295c0b3fb.png" alt="image"></p><br><p>  The figure shows an example of such a binary tree.  Bold highlighted path from the root to the word <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-430"><span class="MJXp-msubsup" id="MJXp-Span-431"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-432" style="margin-right: 0.05em;">w</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-433" style="vertical-align: -0.4em;">2</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-25-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-25"> w_2 </script>  .  The length of the path denote <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-434"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-435">L</span><span class="MJXp-mo" id="MJXp-Span-436" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-437">w</span><span class="MJXp-mo" id="MJXp-Span-438" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-26-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-26"> L (w) </script>  , but <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-439"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-440">j</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-27-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-27"> j </script>  top on the way to the word <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-441"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-442">w</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-28-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-28"> w </script>  denote by <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-443"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-444">n</span><span class="MJXp-mo" id="MJXp-Span-445" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-446">w</span><span class="MJXp-mo" id="MJXp-Span-447" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-448">j</span><span class="MJXp-mo" id="MJXp-Span-449" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-29-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-29"> n (w, j) </script>  .  It can be proved that inner vertices (not leaves) <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-450"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-451">V</span><span class="MJXp-mo" id="MJXp-Span-452" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mn" id="MJXp-Span-453">1</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-30-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-30"> V - 1 </script>  . </p><br><p>  Using hierarchical softmax vector <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-454"><span class="MJXp-msubsup" id="MJXp-Span-455"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-456" style="margin-right: 0.05em;">v</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-457" style="vertical-align: -0.4em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-458">n</span><span class="MJXp-mo" id="MJXp-Span-459">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-460">w</span><span class="MJXp-mo" id="MJXp-Span-461">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-462">j</span><span class="MJXp-mo" id="MJXp-Span-463">)</span></span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-31-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-31"> v_ {n (w, j)} </script>  predicted for <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-464"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-465">V</span><span class="MJXp-mo" id="MJXp-Span-466" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mn" id="MJXp-Span-467">1</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-32-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-32"> V-1 </script>  internal vertices.  And the probability that the word <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-468"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-469">w</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-33-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-33"> w </script>  will be the output word (depending on what we predict: a word from the context or a given word according to the context) is calculated by the formula: </p><br><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-470"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-471">p</span><span class="MJXp-mo" id="MJXp-Span-472" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-473">w</span><span class="MJXp-mo" id="MJXp-Span-474" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-475"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-476" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-477" style="vertical-align: -0.4em;">o</span></span><span class="MJXp-mo" id="MJXp-Span-478" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-479" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-480">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-481">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-482">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-483">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-484">d</span><span class="MJXp-mtext" id="MJXp-Span-485">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-486">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-487">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-488">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-489">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-490">t</span><span class="MJXp-msubsup" id="MJXp-Span-491"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-492" style="margin-right: 0.05em;">s</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mrow" id="MJXp-Span-497"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-498">L</span><span class="MJXp-mo" id="MJXp-Span-499">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-500">w</span><span class="MJXp-mo" id="MJXp-Span-501">)</span><span class="MJXp-mo" id="MJXp-Span-502">‚àí</span><span class="MJXp-mn" id="MJXp-Span-503">1</span></span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-493"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-494">j</span><span class="MJXp-mo" id="MJXp-Span-495">=</span><span class="MJXp-mn" id="MJXp-Span-496">1</span></span></span></span></span></span></span><span class="MJXp-mtext" id="MJXp-Span-504">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-505">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-506">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-507">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-508">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-509">a</span><span class="MJXp-mo" id="MJXp-Span-510" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mo" id="MJXp-Span-511" style="margin-left: 0em; margin-right: 0em;">[</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-512">n</span><span class="MJXp-mo" id="MJXp-Span-513" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-514">w</span><span class="MJXp-mo" id="MJXp-Span-515" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-516">j</span><span class="MJXp-mo" id="MJXp-Span-517" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mn" id="MJXp-Span-518">1</span><span class="MJXp-mo" id="MJXp-Span-519" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-520" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-521">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-522">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-523">h</span><span class="MJXp-mo" id="MJXp-Span-524" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-525">n</span><span class="MJXp-mo" id="MJXp-Span-526" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-527">w</span><span class="MJXp-mo" id="MJXp-Span-528" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-529">j</span><span class="MJXp-mo" id="MJXp-Span-530" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-531" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-532" style="margin-left: 0em; margin-right: 0em;">]</span><span class="MJXp-msubsup" id="MJXp-Span-533"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-534" style="margin-right: 0.05em;">v</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-542">T</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-535"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-536">n</span><span class="MJXp-mo" id="MJXp-Span-537">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-538">w</span><span class="MJXp-mo" id="MJXp-Span-539">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-540">j</span><span class="MJXp-mo" id="MJXp-Span-541">)</span></span></span></span></span></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-543">u</span><span class="MJXp-mo" id="MJXp-Span-544" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-34-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-34"> p (w = w_o) = \ prod \ limits_ {j = 1} ^ {L (w) -1} \ sigma ([n (w, j + 1) = lch (n (w, j))] v_ {n (w, j)} ^ T u) </script></p><br><p>  Where <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-545"><span class="MJXp-mtext" id="MJXp-Span-546">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-547">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-548">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-549">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-550">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-551">a</span><span class="MJXp-mo" id="MJXp-Span-552" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-553">x</span><span class="MJXp-mo" id="MJXp-Span-554" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-35-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-35"> \ sigma (x) </script>  - softmax function; <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-555"><span class="MJXp-mo" id="MJXp-Span-556" style="margin-left: 0em; margin-right: 0em;">[</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-557">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-558">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-559">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-560">e</span><span class="MJXp-mo" id="MJXp-Span-561" style="margin-left: 0em; margin-right: 0em;">]</span><span class="MJXp-mo" id="MJXp-Span-562" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mn" id="MJXp-Span-563">1</span><span class="MJXp-mo" id="MJXp-Span-564" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mo" id="MJXp-Span-565" style="margin-left: 0em; margin-right: 0em;">[</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-566">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-567">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-568">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-569">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-570">e</span><span class="MJXp-mo" id="MJXp-Span-571" style="margin-left: 0em; margin-right: 0em;">]</span><span class="MJXp-mo" id="MJXp-Span-572" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-573" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mn" id="MJXp-Span-574">1</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-36-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-36"> [true] = 1, [false] = - 1 </script>  ; <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-575"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-576">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-577">c</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-578">h</span><span class="MJXp-mo" id="MJXp-Span-579" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-580">n</span><span class="MJXp-mo" id="MJXp-Span-581" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-37-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-37"> lch (n) </script>  - left son of the top <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-582"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-583">n</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-38-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-38"> n </script>  ; <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-584"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-585">u</span><span class="MJXp-mo" id="MJXp-Span-586" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-587"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-588" style="margin-right: 0.05em;">v</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-589" style="vertical-align: -0.4em;"><span class="MJXp-msubsup" id="MJXp-Span-590"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-591" style="margin-right: 0.05em;">w</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-592" style="vertical-align: -0.4em;">I</span></span></span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-39-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-39"> u = v_ {w_I} </script>  , if skip-gram is used, <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-593"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-594">u</span><span class="MJXp-mo" id="MJXp-Span-595" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-596">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-597">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-598">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-599">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-600">c</span><span class="MJXp-mrow" id="MJXp-Span-601"><span class="MJXp-mn" id="MJXp-Span-602">1</span></span><span class="MJXp-mrow" id="MJXp-Span-603"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-604">h</span></span><span class="MJXp-mtext" id="MJXp-Span-605">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-606">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-607">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-608">m</span><span class="MJXp-mtext" id="MJXp-Span-609">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-610">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-611">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-612">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-613">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-614">t</span><span class="MJXp-msubsup" id="MJXp-Span-615"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-616" style="margin-right: 0.05em;">s</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mrow" id="MJXp-Span-621"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-622">h</span></span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-617"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-618">k</span><span class="MJXp-mo" id="MJXp-Span-619">=</span><span class="MJXp-mn" id="MJXp-Span-620">1</span></span></span></span></span></span></span><span class="MJXp-msubsup" id="MJXp-Span-623"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-624" style="margin-right: 0.05em;">v</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-625" style="vertical-align: -0.4em;"><span class="MJXp-msubsup" id="MJXp-Span-626"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-627" style="margin-right: 0.05em;">w</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-628" style="vertical-align: -0.4em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-629">I</span><span class="MJXp-mo" id="MJXp-Span-630">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-631">k</span></span></span></span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-40-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-40"> u = \ frac {1} {h} \ sum \ limits_ {k = 1} ^ {h} v_ {w_ {I, k}} </script>  that is, the averaged context vector, if CBOW is used. </p><br><p>  The formula can be intuitively understood by imagining that at every step we can go left or right with probabilities: </p><br><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-632"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-633">p</span><span class="MJXp-mo" id="MJXp-Span-634" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-635">n</span><span class="MJXp-mo" id="MJXp-Span-636" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-637">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-638">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-639">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-640">t</span><span class="MJXp-mo" id="MJXp-Span-641" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-642" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-643">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-644">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-645">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-646">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-647">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-648">a</span><span class="MJXp-mo" id="MJXp-Span-649" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-650"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-651" style="margin-right: 0.05em;">v</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-653">T</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-652">n</span></span></span></span></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-654">u</span><span class="MJXp-mo" id="MJXp-Span-655" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-41-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-41"> p (n, left) = \ sigma (v_n ^ T u) </script><br><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-656"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-657">p</span><span class="MJXp-mo" id="MJXp-Span-658" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-659">n</span><span class="MJXp-mo" id="MJXp-Span-660" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-661">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-662">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-663">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-664">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-665">t</span><span class="MJXp-mo" id="MJXp-Span-666" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-667" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mn" id="MJXp-Span-668">1</span><span class="MJXp-mo" id="MJXp-Span-669" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-670">p</span><span class="MJXp-mo" id="MJXp-Span-671" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-672">n</span><span class="MJXp-mo" id="MJXp-Span-673" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-674">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-675">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-676">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-677">t</span><span class="MJXp-mo" id="MJXp-Span-678" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-679" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mn" id="MJXp-Span-680">1</span><span class="MJXp-mo" id="MJXp-Span-681" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mtext" id="MJXp-Span-682">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-683">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-684">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-685">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-686">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-687">a</span><span class="MJXp-mo" id="MJXp-Span-688" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-689"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-690" style="margin-right: 0.05em;">v</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-692">T</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-691">n</span></span></span></span></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-693">u</span><span class="MJXp-mo" id="MJXp-Span-694" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-695" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-696">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-697">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-698">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-699">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-700">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-701">a</span><span class="MJXp-mo" id="MJXp-Span-702" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mo" id="MJXp-Span-703" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-msubsup" id="MJXp-Span-704"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-705" style="margin-right: 0.05em;">v</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-707">T</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-706">n</span></span></span></span></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-708">u</span><span class="MJXp-mo" id="MJXp-Span-709" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-42-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-42"> p (n, right) = 1-p (n, left) = 1- \ sigma (v_n ^ T u) = \ sigma (-v_n ^ T u) </script></p><br><p>  Then, at each step, the probabilities are multiplied together ( <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-710"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-711">L</span><span class="MJXp-mo" id="MJXp-Span-712" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-713">w</span><span class="MJXp-mo" id="MJXp-Span-714" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-715" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mn" id="MJXp-Span-716">1</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-43-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-43"> L (w) -1 </script>  steps) and it turns out the desired formula. </p><br><p>  When using simple softmax to calculate the probability of a word, it was necessary to calculate the normalizing sum for all words from the dictionary, it was required <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-717"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-718">O</span><span class="MJXp-mo" id="MJXp-Span-719" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-720">v</span><span class="MJXp-mo" id="MJXp-Span-721" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-44-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-44"> O (v) </script>  operations.  Now, the probability of the word can be calculated using successive calculations that require <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-722"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-723">O</span><span class="MJXp-mo" id="MJXp-Span-724" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-725">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-726">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-727">g</span><span class="MJXp-mo" id="MJXp-Span-728" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-729">V</span><span class="MJXp-mo" id="MJXp-Span-730" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-731" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-45-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-45"> O (log (V)) </script>  . </p><br><h1 id="drugie-modeli">  Other models </h1><br><p>  In addition to word2vec, other word embedding models were proposed, of course.  It is worth noting the model proposed by the laboratory of computational linguistics at Stanford University, called Global Vectors (GloVe), combining the features of SVD decomposition and word2vec [4]. </p><br><p>  Also it is necessary to mention that since  Initially, all the described models were proposed for the English language, then the inflection problem typical for synthetic languages ‚Äã‚Äã(this is a <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B8%25D0%25BD%25D1%2582%25D0%25B5%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B8%25D0%25B9_%25D1%258F%25D0%25B7%25D1%258B%25D0%25BA">linguistic term</a> ), such as Russian, is not so acute.  Everywhere above, it was implicitly assumed that we either consider different forms of the same word as different words - and then we hope that our corps will be enough for the model to learn their syntactic proximity, or we use the mechanisms of stemming or lemmatization.  Stemming is the cutting off of the end of a word, leaving only the stem (for example, a ‚Äúred apple‚Äù will turn into ‚Äúred apples‚Äù).  And lemmatization - replacing a word with its initial form (for example, ‚Äúwe run‚Äù will turn into ‚ÄúI run‚Äù).  But we can not lose this information, and use it - by coding OHE into a new vector, and converting it with a vector for a basis or a lemma. <br>  It is also worth saying that what we started with - the literal representation of the word - also did not sink into oblivion: models for using the literal representation of the word for the word embedding were proposed [5]. </p><br><h1 id="prakticheskoe-primenenie">  Practical use </h1><br><p>  We talked about the theory, it's time to see what all of the above is applicable in practice.  After all, any of the most beautiful theory without practical application is nothing more than a play of the mind.  Consider the use of Word2Vec in two tasks: <br>  1) The task of classification, it is necessary to identify the user by the sequence of visited sites <br>  2) The task of regression, it is necessary in the text of the article to determine its rating on Habrahabr. </p><br><h2 id="klassifikaciya">  Classification </h2><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#      from __future__ import division, print_function #    Anaconda import warnings warnings.filterwarnings('ignore') #%matplotlib inline import numpy as np import pandas as pd from sklearn.metrics import roc_auc_score</span></span></code> </pre> <br><p>  You can download data for the first task from the page of the competition <a href="https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking">"Catch Me If You Can"</a> </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#      train_df = pd.read_csv('data/train_sessions.csv')#,index_col='session_id') test_df = pd.read_csv('data/test_sessions.csv')#, index_col='session_id') #   time1, ..., time10    times = ['time%s' % i for i in range(1, 11)] train_df[times] = train_df[times].apply(pd.to_datetime) test_df[times] = test_df[times].apply(pd.to_datetime) #     train_df = train_df.sort_values(by='time1') #      train_df.head()</span></span></code> </pre> <br><p><img src="https://habrastorage.org/web/c92/477/565/c924775650e44c469e20c3eaa9705bfa.png" alt="image"></p><br><pre> <code class="python hljs">sites = [<span class="hljs-string"><span class="hljs-string">'site%s'</span></span> % i <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">11</span></span>)] <span class="hljs-comment"><span class="hljs-comment"># nan  0 train_df[sites] = train_df[sites].fillna(0).astype('int').astype('str') test_df[sites] = test_df[sites].fillna(0).astype('int').astype('str') #     word2vec train_df['list'] = train_df['site1'] test_df['list'] = test_df['site1'] for s in sites[1:]: train_df['list'] = train_df['list']+","+train_df[s] test_df['list'] = test_df['list']+","+test_df[s] train_df['list_w'] = train_df['list'].apply(lambda x: x.split(',')) test_df['list_w'] = test_df['list'].apply(lambda x: x.split(','))</span></span></code> </pre> <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#      ,    #      , ..        . train_df['list_w'][10]</span></span></code> </pre> <br><pre> <code class="hljs 1c">['229', '<span class="hljs-number"><span class="hljs-number">1500</span></span>', '33', '<span class="hljs-number"><span class="hljs-number">1500</span></span>', '391', '35', '29', '<span class="hljs-number"><span class="hljs-number">2276</span></span>', '<span class="hljs-number"><span class="hljs-number">4030</span></span>5', '23']</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  word2vec from gensim.models import word2vec</span></span></code> </pre> <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#            #    6=3*2 (  10 )     300,  workers     test_df['target'] = -1 data = pd.concat([train_df,test_df], axis=0) model = word2vec.Word2Vec(data['list_w'], size=300, window=3, workers=4) #        w2v = dict(zip(model.wv.index2word, model.wv.syn0))</span></span></code> </pre> <br><p>  Since  Now we have compared a vector to each word, then we need to decide what to associate with the whole sentence of words. <br>  One of the possible options is simply to average all the words in the sentence and get some meaning for the whole sentence (if the word is not in the text, then we take the zero vector). </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">mean_vectorizer</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, word2vec)</span></span></span><span class="hljs-function">:</span></span> self.word2vec = word2vec self.dim = len(next(iter(w2v.values()))) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fit</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, X)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">transform</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, X)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.array([ np.mean([self.word2vec[w] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> w <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> w <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.word2vec] <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> [np.zeros(self.dim)], axis=<span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> words <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> X ])</code> </pre> <br><pre> <code class="python hljs">data_mean=mean_vectorizer(w2v).fit(train_df[<span class="hljs-string"><span class="hljs-string">'list_w'</span></span>]).transform(train_df[<span class="hljs-string"><span class="hljs-string">'list_w'</span></span>]) data_mean.shape</code> </pre> <br><pre> <code class="hljs lisp">(<span class="hljs-number"><span class="hljs-number">253561</span></span>, <span class="hljs-number"><span class="hljs-number">300</span></span>)</code> </pre> <br><p>  Since  we got distributed representation, then no number individually means nothing, which means best linear algorithms will show themselves.  Let's try neural networks, LogisticRegression and check the non-linear method XGBoost. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   def split(train,y,ratio): idx = round(train.shape[0] * ratio) return train[:idx, :], train[idx:, :], y[:idx], y[idx:] y = train_df['target'] Xtr, Xval, ytr, yval = split(data_mean, y,0.8) Xtr.shape,Xval.shape,ytr.mean(),yval.mean()</span></span></code> </pre> <br><pre> <code class="hljs lisp">((<span class="hljs-number"><span class="hljs-number">202849</span></span>, <span class="hljs-number"><span class="hljs-number">300</span></span>), (<span class="hljs-number"><span class="hljs-number">50712</span></span>, <span class="hljs-number"><span class="hljs-number">300</span></span>), <span class="hljs-number"><span class="hljs-number">0.009726446765820882</span></span>, <span class="hljs-number"><span class="hljs-number">0.006389020350212968</span></span>)</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   keras from keras.models import Sequential, Model from keras.layers import Dense, Dropout, Activation, Input from keras.preprocessing.text import Tokenizer from keras import regularizers</span></span></code> </pre> <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    model = Sequential() model.add(Dense(128, input_dim=(Xtr.shape[1]))) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(1)) model.add(Activation('sigmoid')) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])</span></span></code> </pre> <br><pre> <code class="python hljs">history = model.fit(Xtr, ytr, batch_size=<span class="hljs-number"><span class="hljs-number">128</span></span>, epochs=<span class="hljs-number"><span class="hljs-number">10</span></span>, validation_data=(Xval, yval), class_weight=<span class="hljs-string"><span class="hljs-string">'auto'</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre> <br><pre> <code class="python hljs">classes = model.predict(Xval, batch_size=<span class="hljs-number"><span class="hljs-number">128</span></span>) roc_auc_score(yval, classes)</code> </pre> <br><pre> <code class="hljs css">0<span class="hljs-selector-class"><span class="hljs-selector-class">.91892341356995644</span></span></code> </pre> <br><p>  Got a good result.  So Word2Vec was able to identify the dependencies between the sessions. <br>  Let's see what happens with the XGBoost algorithm. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> xgboost <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> xgb</code> </pre> <br><pre> <code class="python hljs">dtr = xgb.DMatrix(Xtr, label= ytr,missing = np.nan) dval = xgb.DMatrix(Xval, label= yval,missing = np.nan) watchlist = [(dtr, <span class="hljs-string"><span class="hljs-string">'train'</span></span>), (dval, <span class="hljs-string"><span class="hljs-string">'eval'</span></span>)] history = dict()</code> </pre> <br><pre> <code class="python hljs">params = { <span class="hljs-string"><span class="hljs-string">'max_depth'</span></span>: <span class="hljs-number"><span class="hljs-number">26</span></span>, <span class="hljs-string"><span class="hljs-string">'eta'</span></span>: <span class="hljs-number"><span class="hljs-number">0.025</span></span>, <span class="hljs-string"><span class="hljs-string">'nthread'</span></span>: <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-string"><span class="hljs-string">'gamma'</span></span> : <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-string"><span class="hljs-string">'alpha'</span></span> : <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-string"><span class="hljs-string">'subsample'</span></span>: <span class="hljs-number"><span class="hljs-number">0.85</span></span>, <span class="hljs-string"><span class="hljs-string">'eval_metric'</span></span>: [<span class="hljs-string"><span class="hljs-string">'auc'</span></span>], <span class="hljs-string"><span class="hljs-string">'objective'</span></span>: <span class="hljs-string"><span class="hljs-string">'binary:logistic'</span></span>, <span class="hljs-string"><span class="hljs-string">'colsample_bytree'</span></span>: <span class="hljs-number"><span class="hljs-number">0.9</span></span>, <span class="hljs-string"><span class="hljs-string">'min_child_weight'</span></span>: <span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-string"><span class="hljs-string">'scale_pos_weight'</span></span>:(<span class="hljs-number"><span class="hljs-number">1</span></span>)/y.mean(), <span class="hljs-string"><span class="hljs-string">'seed'</span></span>:<span class="hljs-number"><span class="hljs-number">7</span></span> }</code> </pre> <br><pre> <code class="python hljs">model_new = xgb.train(params, dtr, num_boost_round=<span class="hljs-number"><span class="hljs-number">200</span></span>, evals=watchlist, evals_result=history, verbose_eval=<span class="hljs-number"><span class="hljs-number">20</span></span>)</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Training</b> <div class="spoiler_text"><pre> <code class="hljs css"><span class="hljs-selector-attr"><span class="hljs-selector-attr">[0]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">train-auc</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:0.954886</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">eval-auc</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:0.85383</span></span> <span class="hljs-selector-attr"><span class="hljs-selector-attr">[20]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">train-auc</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:0.989848</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">eval-auc</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:0.910808</span></span> <span class="hljs-selector-attr"><span class="hljs-selector-attr">[40]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">train-auc</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:0.992086</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">eval-auc</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:0.916371</span></span> <span class="hljs-selector-attr"><span class="hljs-selector-attr">[60]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">train-auc</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:0.993658</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">eval-auc</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:0.917753</span></span> <span class="hljs-selector-attr"><span class="hljs-selector-attr">[80]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">train-auc</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:0.994874</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">eval-auc</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:0.918254</span></span> <span class="hljs-selector-attr"><span class="hljs-selector-attr">[100]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">train-auc</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:0.995743</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">eval-auc</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:0.917947</span></span> <span class="hljs-selector-attr"><span class="hljs-selector-attr">[120]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">train-auc</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:0.996396</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">eval-auc</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:0.917735</span></span> <span class="hljs-selector-attr"><span class="hljs-selector-attr">[140]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">train-auc</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:0.996964</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">eval-auc</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:0.918503</span></span> <span class="hljs-selector-attr"><span class="hljs-selector-attr">[160]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">train-auc</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:0.997368</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">eval-auc</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:0.919341</span></span> <span class="hljs-selector-attr"><span class="hljs-selector-attr">[180]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">train-auc</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:0.997682</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">eval-auc</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:0.920183</span></span></code> </pre> </div></div><br><p>  We see that the algorithm is strongly adapted to the training sample, so it is possible that our assumption about the need to use linear algorithms has been confirmed. <br>  Let's see what the usual LogisticRegression will show. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.linear_model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LogisticRegression <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_auc_lr_valid</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X, y, C=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">, seed=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">7</span></span></span></span><span class="hljs-function"><span class="hljs-params">, ratio = </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.8</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#       idx = round(X.shape[0] * ratio) #   lr = LogisticRegression(C=C, random_state=seed, n_jobs=-1).fit(X[:idx], y[:idx]) #     y_pred = lr.predict_proba(X[idx:, :])[:, 1] #   score = roc_auc_score(y[idx:], y_pred) return score</span></span></code> </pre> <br><pre> <code class="python hljs">get_auc_lr_valid(data_mean, y, C=<span class="hljs-number"><span class="hljs-number">1</span></span>, seed=<span class="hljs-number"><span class="hljs-number">7</span></span>, ratio = <span class="hljs-number"><span class="hljs-number">0.8</span></span>)</code> </pre> <br><pre> <code class="hljs css">0<span class="hljs-selector-class"><span class="hljs-selector-class">.90037148150108237</span></span></code> </pre> <br><p>  Let's try to improve the results. </p><br><p>  Now, instead of the usual average, to take into account the frequency with which the word occurs in the text, take the weighted average.  We take IDF as weights.  Accounting for IDF reduces the weight of widely used words and increases the weight of more rare words that can quite accurately indicate which class the text belongs to.  In our case, who owns the sequence of visited sites. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   tfidf . from sklearn.feature_extraction.text import TfidfVectorizer from collections import defaultdict class tfidf_vectorizer(object): def __init__(self, word2vec): self.word2vec = word2vec self.word2weight = None self.dim = len(next(iter(w2v.values()))) def fit(self, X): tfidf = TfidfVectorizer(analyzer=lambda x: x) tfidf.fit(X) max_idf = max(tfidf.idf_) self.word2weight = defaultdict( lambda: max_idf, [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()]) return self def transform(self, X): return np.array([ np.mean([self.word2vec[w] * self.word2weight[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0) for words in X ])</span></span></code> </pre> <br><pre> <code class="python hljs">data_mean = tfidf_vectorizer(w2v).fit(train_df[<span class="hljs-string"><span class="hljs-string">'list_w'</span></span>]).transform(train_df[<span class="hljs-string"><span class="hljs-string">'list_w'</span></span>])</code> </pre> <br><p>  Check whether the quality of the LogisticRegression has changed. </p><br><pre> <code class="python hljs">get_auc_lr_valid(data_mean, y, C=<span class="hljs-number"><span class="hljs-number">1</span></span>, seed=<span class="hljs-number"><span class="hljs-number">7</span></span>, ratio = <span class="hljs-number"><span class="hljs-number">0.8</span></span>)</code> </pre> <br><pre> <code class="hljs css">0<span class="hljs-selector-class"><span class="hljs-selector-class">.90738924587178804</span></span></code> </pre> <br><p>  we see a gain of 0.07, which means that the weighted average probably helps to better reflect the meaning of the whole sentence through word2vec. </p><br><h2 id="predskazanie-populyarnosti">  Prediction of popularity </h2><br><p>  Let's try Word2Vec already in a text task - predicting the popularity of an article on Khabrhabr. </p><br><p>  Let's try the algorithm forces directly on the text data of Habr's articles.  We converted the data to a csv table.  You can download them here: <a href="https://yadi.sk/d/hAhCuetI3JPouk">train</a> , <a href="https://yadi.sk/d/mLMZZtN63JPouc">test</a> . </p><br><pre> <code class="python hljs">Xtrain = pd.read_csv(<span class="hljs-string"><span class="hljs-string">'data/train_content.csv'</span></span>) Xtest = pd.read_csv(<span class="hljs-string"><span class="hljs-string">'data/test_content.csv'</span></span>) print(Xtrain.shape,Xtest.shape) Xtrain.head()</code> </pre> <br><p><img src="https://habrastorage.org/web/dfe/d64/ff8/dfed64ff82394b1b94a5a50ad5a8674b.png" alt="image"></p><br><div class="spoiler">  <b class="spoiler_title">Sample text</b> <div class="spoiler_text"><p>  'Good habradnya! <br>  \ r \ n <br>  \ r \ nI will go straight to the point.  Recently, I have been entrusted with the task of developing a contextual network of text ads.   -  ,    .  , 90% -    .     ,     . <br> \r\n <br> \r\n   -   ,       ,    .      ,      AdSense,     .    :   ,      ,     ,     . <br> \r\n <br> \r\n     ?' </p></div></div><br><p>       .       . </p><br><p>  ,             Word2Vec. <br>   ,      . </p><br><p> 1)            ; </p><br><p> 2)      ; </p><br><p> 3)       , ..       ; </p><br><p> 4) ,      ‚Äî   () . </p><br><p>     . </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    from sklearn.metrics import mean_squared_error import re from nltk.corpus import stopwords import pymorphy2 morph = pymorphy2.MorphAnalyzer() stops = set(stopwords.words("english")) | set(stopwords.words("russian")) def review_to_wordlist(review): #1) review_text = re.sub("[^--a-zA-Z]"," ", review) #2) words = review_text.lower().split() #3) words = [w for w in words if not w in stops] #4) words = [morph.parse(w)[0].normal_form for w in words ] return(words)</span></span></code> </pre><br><p>    ,         . </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   Xtrain['date'] = Xtrain['date'].apply(pd.to_datetime) Xtrain['year'] = Xtrain['date'].apply(lambda x: x.year) Xtrain['month'] = Xtrain['date'].apply(lambda x: x.month)</span></span></code> </pre> <br><p>    2015 ,     4  2016, ..         4  2017 .     ,   ,              </p><br><pre> <code class="python hljs">Xtr = Xtrain[Xtrain[<span class="hljs-string"><span class="hljs-string">'year'</span></span>]==<span class="hljs-number"><span class="hljs-number">2015</span></span>] Xval = Xtrain[(Xtrain[<span class="hljs-string"><span class="hljs-string">'year'</span></span>]==<span class="hljs-number"><span class="hljs-number">2016</span></span>)&amp; (Xtrain[<span class="hljs-string"><span class="hljs-string">'month'</span></span>]&lt;=<span class="hljs-number"><span class="hljs-number">4</span></span>)] ytr = Xtr[<span class="hljs-string"><span class="hljs-string">'favs_lognorm'</span></span>] yval = Xval[<span class="hljs-string"><span class="hljs-string">'favs_lognorm'</span></span>] Xtr.shape,Xval.shape,ytr.mean(),yval.mean()</code> </pre> <br><pre> <code class="hljs lisp">((<span class="hljs-number"><span class="hljs-number">23425</span></span>, <span class="hljs-number"><span class="hljs-number">15</span></span>), (<span class="hljs-number"><span class="hljs-number">7556</span></span>, <span class="hljs-number"><span class="hljs-number">15</span></span>), <span class="hljs-number"><span class="hljs-number">3.4046228249071526</span></span>, <span class="hljs-number"><span class="hljs-number">3.304679829935242</span></span>)</code> </pre> <br><pre> <code class="python hljs">data = pd.concat([Xtr,Xval],axis = <span class="hljs-number"><span class="hljs-number">0</span></span>,ignore_index = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   nan,      data['content_clear'] = data['content'].apply(str)</span></span></code> </pre> <br><pre> <code class="python hljs">%%time data[<span class="hljs-string"><span class="hljs-string">'content_clear'</span></span>] = data[<span class="hljs-string"><span class="hljs-string">'content_clear'</span></span>].apply(review_to_wordlist)</code> </pre> <br><pre> <code class="python hljs">model = word2vec.Word2Vec(data[<span class="hljs-string"><span class="hljs-string">'content_clear'</span></span>], size=<span class="hljs-number"><span class="hljs-number">300</span></span>, window=<span class="hljs-number"><span class="hljs-number">10</span></span>, workers=<span class="hljs-number"><span class="hljs-number">4</span></span>) w2v = dict(zip(model.wv.index2word, model.wv.syn0))</code> </pre> <br><p>    : </p><br><pre> <code class="python hljs">model.wv.most_similar(positive=[<span class="hljs-string"><span class="hljs-string">'open'</span></span>, <span class="hljs-string"><span class="hljs-string">'data'</span></span>,<span class="hljs-string"><span class="hljs-string">'science'</span></span>,<span class="hljs-string"><span class="hljs-string">'best'</span></span>])</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Result</b> <div class="spoiler_text"><p> [('massive', 0.6958945393562317), <br> ('mining', 0.6796239018440247), <br> ('scientist', 0.6742461919784546), <br> ('visualization', 0.6403135061264038), <br> ('centers', 0.6386666297912598), <br> ('big', 0.6237790584564209), <br> ('engineering', 0.6209672689437866), <br> ('structures', 0.609510600566864), <br> ('knowledge', 0.6094595193862915), <br> ('scientists', 0.6050446629524231)] </p></div></div><br><p>    ,    : </p><br><pre> <code class="python hljs">data_mean = mean_vectorizer(w2v).fit(data[<span class="hljs-string"><span class="hljs-string">'content_clear'</span></span>]).transform(data[<span class="hljs-string"><span class="hljs-string">'content_clear'</span></span>]) data_mean.shape</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">split</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(train,y,ratio)</span></span></span><span class="hljs-function">:</span></span> idx = ratio <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> train[:idx, :], train[idx:, :], y[:idx], y[idx:] y = data[<span class="hljs-string"><span class="hljs-string">'favs_lognorm'</span></span>] Xtr, Xval, ytr, yval = split(data_mean, y,<span class="hljs-number"><span class="hljs-number">23425</span></span>) Xtr.shape,Xval.shape,ytr.mean(),yval.mean()</code> </pre> <br><pre> <code class="hljs lisp">((<span class="hljs-number"><span class="hljs-number">23425</span></span>, <span class="hljs-number"><span class="hljs-number">300</span></span>), (<span class="hljs-number"><span class="hljs-number">7556</span></span>, <span class="hljs-number"><span class="hljs-number">300</span></span>), <span class="hljs-number"><span class="hljs-number">3.4046228249071526</span></span>, <span class="hljs-number"><span class="hljs-number">3.304679829935242</span></span>)</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.linear_model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Ridge <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mean_squared_error model = Ridge(alpha = <span class="hljs-number"><span class="hljs-number">1</span></span>,random_state=<span class="hljs-number"><span class="hljs-number">7</span></span>) model.fit(Xtr, ytr) train_preds = model.predict(Xtr) valid_preds = model.predict(Xval) ymed = np.ones(len(valid_preds))*ytr.median() print(<span class="hljs-string"><span class="hljs-string">'  '</span></span>,mean_squared_error(ytr, train_preds)) print(<span class="hljs-string"><span class="hljs-string">'  '</span></span>,mean_squared_error(yval, valid_preds)) print(<span class="hljs-string"><span class="hljs-string">'    '</span></span>,mean_squared_error(yval, ymed))</code> </pre> <br><pre> <code class="hljs css">   0<span class="hljs-selector-class"><span class="hljs-selector-class">.734248488422</span></span>    0<span class="hljs-selector-class"><span class="hljs-selector-class">.665592676973</span></span>      1<span class="hljs-selector-class"><span class="hljs-selector-class">.44601638512</span></span></code> </pre> <br><pre> <code class="python hljs">data_mean_tfidf = tfidf_vectorizer(w2v).fit(data[<span class="hljs-string"><span class="hljs-string">'content_clear'</span></span>]).transform(data[<span class="hljs-string"><span class="hljs-string">'content_clear'</span></span>])</code> </pre> <br><pre> <code class="python hljs">y = data[<span class="hljs-string"><span class="hljs-string">'favs_lognorm'</span></span>] Xtr, Xval, ytr, yval = split(data_mean_tfidf, y,<span class="hljs-number"><span class="hljs-number">23425</span></span>) Xtr.shape,Xval.shape,ytr.mean(),yval.mean()</code> </pre> <br><pre> <code class="hljs lisp">((<span class="hljs-number"><span class="hljs-number">23425</span></span>, <span class="hljs-number"><span class="hljs-number">300</span></span>), (<span class="hljs-number"><span class="hljs-number">7556</span></span>, <span class="hljs-number"><span class="hljs-number">300</span></span>), <span class="hljs-number"><span class="hljs-number">3.4046228249071526</span></span>, <span class="hljs-number"><span class="hljs-number">3.304679829935242</span></span>)</code> </pre> <br><pre> <code class="python hljs">model = Ridge(alpha = <span class="hljs-number"><span class="hljs-number">1</span></span>,random_state=<span class="hljs-number"><span class="hljs-number">7</span></span>) model.fit(Xtr, ytr) train_preds = model.predict(Xtr) valid_preds = model.predict(Xval) ymed = np.ones(len(valid_preds))*ytr.median() print(<span class="hljs-string"><span class="hljs-string">'  '</span></span>,mean_squared_error(ytr, train_preds)) print(<span class="hljs-string"><span class="hljs-string">'  '</span></span>,mean_squared_error(yval, valid_preds)) print(<span class="hljs-string"><span class="hljs-string">'    '</span></span>,mean_squared_error(yval, ymed))</code> </pre> <br><pre> <code class="hljs css">   0<span class="hljs-selector-class"><span class="hljs-selector-class">.743623730976</span></span>    0<span class="hljs-selector-class"><span class="hljs-selector-class">.675584372744</span></span>      1<span class="hljs-selector-class"><span class="hljs-selector-class">.44601638512</span></span></code> </pre> <br><p>   . </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   keras from keras.models import Sequential, Model from keras.layers import Dense, Dropout, Activation, Input from keras.preprocessing.text import Tokenizer from keras import regularizers from keras.wrappers.scikit_learn import KerasRegressor</span></span></code> </pre> <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   . def baseline_model(): model = Sequential() model.add(Dense(128, input_dim=Xtr.shape[1], kernel_initializer='normal', activation='relu')) model.add(Dropout(0.2)) model.add(Dense(64, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(1, kernel_initializer='normal')) model.compile(loss='mean_squared_error', optimizer='adam') return model estimator = KerasRegressor(build_fn=baseline_model,epochs=20, nb_epoch=20, batch_size=64,validation_data=(Xval, yval), verbose=2)</span></span></code> </pre> <br><pre> <code class="python hljs">estimator.fit(Xtr, ytr)</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Training</b> <div class="spoiler_text"><p> Train on 23425 samples, validate on 7556 samples <br> Epoch 1/20 <br> 1s ‚Äî loss: 1.7292 ‚Äî val_loss: 0.7336 <br> Epoch 2/20 <br> 0s ‚Äî loss: 1.2382 ‚Äî val_loss: 0.6738 <br> Epoch 3/20 <br> 0s ‚Äî loss: 1.1379 ‚Äî val_loss: 0.6916 <br> Epoch 4/20 <br> 0s ‚Äî loss: 1.0785 ‚Äî val_loss: 0.6963 <br> Epoch 5/20 <br> 0s ‚Äî loss: 1.0362 ‚Äî val_loss: 0.6256 <br> Epoch 6/20 <br> 0s ‚Äî loss: 0.9858 ‚Äî val_loss: 0.6393 <br> Epoch 7/20 <br> 0s ‚Äî loss: 0.9508 ‚Äî val_loss: 0.6424 <br> Epoch 8/20 <br> 0s ‚Äî loss: 0.9066 ‚Äî val_loss: 0.6231 <br> Epoch 9/20 <br> 0s ‚Äî loss: 0.8819 ‚Äî val_loss: 0.6207 <br> Epoch 10/20 <br> 0s ‚Äî loss: 0.8634 ‚Äî val_loss: 0.5993 <br> Epoch 11/20 <br> 1s ‚Äî loss: 0.8401 ‚Äî val_loss: 0.6093 <br> Epoch 12/20 <br> 1s ‚Äî loss: 0.8152 ‚Äî val_loss: 0.6006 <br> Epoch 13/20 <br> 0s ‚Äî loss: 0.8005 ‚Äî val_loss: 0.5931 <br> Epoch 14/20 <br> 0s ‚Äî loss: 0.7736 ‚Äî val_loss: 0.6245 <br> Epoch 15/20 <br> 0s ‚Äî loss: 0.7599 ‚Äî val_loss: 0.5978 <br> Epoch 16/20 <br> 1s ‚Äî loss: 0.7407 ‚Äî val_loss: 0.6593 <br> Epoch 17/20 <br> 1s ‚Äî loss: 0.7339 ‚Äî val_loss: 0.5906 <br> Epoch 18/20 <br> 1s ‚Äî loss: 0.7256 ‚Äî val_loss: 0.5878 <br> Epoch 19/20 <br> 1s ‚Äî loss: 0.7117 ‚Äî val_loss: 0.6123 <br> Epoch 20/20 <br> 0s ‚Äî loss: 0.7069 ‚Äî val_loss: 0.5948 </p></div></div><br><p>         . </p><br><h1 id="zaklyuchenie">  Conclusion </h1><br><p> Word2Vec        , -              ‚Äî    ‚Äî GloVe.   ,      ,   ,         ,   word2vec. </p><br><p>       <a href="https://github.com/madrugado/word2vec-article"></a> .    ‚Äî  <a href="https://github.com/Yorko/mlcourse_open/blob/master/jupyter_notebooks/tutorials/word2vec_demonzheg.ipynb"></a> . </p><br><p>     <a href="https://habr.com/users/demonzheg/" class="user_link">demonzheg</a> . </p><br><h2 id="literatura">  Literature </h2><br><ol><li> Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. <em>Efficient estimation <br> of word representations in vector space.</em> CoRR, abs/1301.3781, </li><li> Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. <em>Distributed representations of words and phrases and their compositionality.</em> In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pages 3111‚Äì3119, 2013. </li><li> Morin, F., &amp; Bengio, Y. <em>Hierarchical Probabilistic Neural Network Language Model</em> . Aistats, 5, 2005. </li><li> Jeffrey Pennington, Richard Socher, and Christopher D. Manning. <em>GloVe: Global Vectors for Word Representation.</em> 2014. </li><li> Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. <em>Enriching word vectors <br> with subword information.</em> arXiv preprint arXiv:1607.04606, 2016. </li></ol><br><p> * ,        . </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/329410/">https://habr.com/ru/post/329410/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../329398/index.html">Intel introduced the 18-core Core i9 Extreme Core X line</a></li>
<li><a href="../329400/index.html">Microsoft will introduce DNA storage in one of its data centers</a></li>
<li><a href="../329402/index.html">Building the right alert system - respond only to business critical issues</a></li>
<li><a href="../329404/index.html">Why did we write CRM to the SEO department and what did it lead to?</a></li>
<li><a href="../329408/index.html">We do GraphQL API on PHP and MySQL. Part 3: Solving the N + 1 Query Problem</a></li>
<li><a href="../329412/index.html">Evaluation of the quality of face recognition algorithms</a></li>
<li><a href="../329414/index.html">Google I / O 2017: Android developer notes</a></li>
<li><a href="../329416/index.html">Cloud Load Balancing</a></li>
<li><a href="../329418/index.html">Testing with –°odeception for dummies: 3 types of tests</a></li>
<li><a href="../329420/index.html">Proctoring in online exams: how does it work?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>