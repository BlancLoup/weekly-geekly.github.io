<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>First model: Fashion MNIST dataset</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Full course in Russian can be found at this link . 
 The original English course is available here . 

 The release of new lectures is scheduled every...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>First model: Fashion MNIST dataset</h1><div class="post__text post__text-html js-mediator-article">  Full course in Russian can be found at <a href="https://www.youtube.com/playlist%3Flist%3DPLfdVzZl6HHg9y9l6U5xUjqKS13rWoQPF4">this link</a> . <br>  The original English course is available <a href="https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187">here</a> . <br><img src="https://habrastorage.org/webt/ry/3a/55/ry3a55ljajwq9gp5jwwhztrxyxo.png"><br>  <i>The release of new lectures is scheduled every 2-3 days.</i> <br><a name="habracut"></a><br><h2>  Interview with Sebastian Trun, CEO Udacity </h2><br>  - So, we are again with you and with us is still Sebastian.  We just want to discuss fully connected layers, those same Dense layers.  Before that, I would like to ask one question.  What are the boundaries and what are the main obstacles that will stand in the way of the development of deep learning and will have the greatest impact on it in the next 10 years?  Everything changes so fast!  What do you think will be the very next ‚Äúbig thing‚Äù (big thing - breakthrough)? <br>  - I would call two things.  The first is general artificial intelligence (general AI) for performing more than one task.  This is great!  People can do more than one task and never have to do the same thing.  The second is the introduction of technology to the market.  For me, a feature of machine learning is that it provides computers with the opportunity to observe and find patterns in the data, helping people become the best in the field - at the expert level!  Machine learning can be used in jurisprudence, medicine, autonomous cars.  Develop such applications, because they can bring a huge amount of money, but what is most important in this all - you have the opportunity to make the world much better. <br>  - I really like the way everything you said is formed into a single picture of the deep learning and its application - this is just a tool that can help you solve a specific task. <br>  - Yes it is!  Incredible tool, right? <br>  - Yes, yes, I completely agree with you! <br>  - Almost like a human brain! <br>  - You mentioned medical applications in our first interview, in the first part of the video course.  In which applications, in your opinion, the use of deep learning is the most delightful and surprising? <br>  - Lots of!  Highly!  Medicine is in the short list of areas that are actively using deep learning.  I lost my sister a few months ago, she was sick with cancer, which is very sad.  I think there are many diseases that could be detected earlier - in the early stages, giving them the opportunity to cure or slow down their development.  The idea, in essence, is to transfer some tools to the house (smart home) so that it is possible to detect such deviations in health long before the moment when the person himself sees them.  I would also add - everything is repetitive, any office work where you perform the same type of actions over and over again, for example, bookkeeping.  Even I, as the CEO, do a lot of repetitive actions.  It would be great to automate them, even work with postal correspondence! <br>  - I can not disagree with you!  In this lesson we will introduce students to the course with a layer of a neural network called a dense-layer.  Could you tell us more about what you think about full connected layers? <br>  - So, let's start with the fact that each network can be connected in different ways.  Some of them may have very dense connectivity, which allows you to get some benefit when scaling and "win" from large networks.  Sometimes you don't know how many links you need, so you connect everything with everything ‚Äî this is called a fully connected layer.  I add that this approach has much more power and potential than something more structured. <br>  - Totally agree with you!  Thanks for helping us learn a little more about full connected layers.  I look forward to the moment when we finally get down to their implementation and writing code. <br>  - Enjoy!  It will be really fun! <br><br><h2>  Introduction </h2><br>  - Welcome back!  In the last lesson, you figured out how to build your first neural network using TensorFlow and Keras, how neural networks work and how the workout (training) process is organized.  In particular, we saw how to train a model to convert degrees Celsius to degrees Fahrenheit. <br><br><img src="https://habrastorage.org/webt/7h/jc/jq/7hjcjqzg5rz1qzpbncjes5ipor8.jpeg">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      - We also got acquainted with the concept of fully connected layers (dense-layers), the most important layer in neural networks.  But in this lesson we will do much cooler things!  In this lesson we will develop a neural network that can recognize elements of clothing and images.  As we mentioned earlier, machine learning uses input data called ‚Äúattributes‚Äù (features, properties) and output data called ‚Äúlabels‚Äù (labels) by which the model learns and finds the transformation algorithm.  Therefore, first, we need many examples to train the neural network to recognize the various elements of clothing.  Let me remind you that the example for learning is a pair of values ‚Äã‚Äã- the input feature and output label, which are fed to the input of the neural network.  In our new example, the image will be the input data, and the output label should be the category of clothing to which the item of clothing shown in the picture belongs.  Fortunately, such a data set already exists.  It is called Fashion MNIST.  We will take a closer look at this dataset in the next section. <br><br><h2>  Fashion MNIST dataset </h2><br>  Welcome to the world of the MNIST dataset!  So, our set consists of 28x28 images, each pixel of which is a shade of gray. <br><br><img src="https://habrastorage.org/webt/ua/mr/f6/uamrf6n8gci7qi2c1t_ganxtai8.jpeg"><br><br>  The data set contains images of T-shirts, tops, sandals, and even shoes.  Here is a complete list of what our MNIST dataset contains: <br><br><img src="https://habrastorage.org/webt/3i/ce/7n/3ice7nwlkok2g_n-trodker5s7e.jpeg"><br><br>  Each input image corresponds to one of the above tags.  The MNIST Fashion dataset contains 70,000 images, so we have a place to start and work with.  Of these 70,000, we will use 60,000 to train the neural network. <br><br><img src="https://habrastorage.org/webt/4b/ur/60/4bur602odizkfsdpt0fds-3fnxk.png"><br><br>  And we will use the remaining 10,000 elements in order to check how well our neural network has learned to recognize the elements of clothing.  A little later, we will explain why we divided the data set into a training set and into a testing set. <br><br>  So, here is our Fashion MNIST dataset. <br><br><img src="https://habrastorage.org/webt/mx/lw/dz/mxlwdzjrfhviwmgsliwdcy6tbwq.png"><br><br>  Remember, each image in the dataset is a 28x28 image in grayscale, which means that each image is 784 bytes in size.  Our task is to create a neural network that receives these 784 bytes at the input, and returns to which category of clothing out of 10 that the input element belongs to. <br><br><h2>  Neural network </h2><br>  In this lesson, we will use a deep neural network that will learn to classify images from the Fashion MNIST dataset. <br><br><img src="https://habrastorage.org/webt/xg/cr/h_/xgcrh_cowdhfz-owx34wp-kqzi0.png"><br><br>  The image above shows what our neural network will look like.  Let's take a closer look at it. <br><br>  The input value of our neural network is a one-dimensional array with a length of 784, an array of exactly that length, for the reason that each image is 28x28 pixels (= 784 pixels in the image), which we transform into a one-dimensional array.  The process of converting a 2D image into a vector is called flattening and is implemented by means of a smoothing layer ‚Äî a flatten layer. <br><br><img src="https://habrastorage.org/webt/7d/wu/d_/7dwud_tt2qctnaigzc8my3pz1j0.png"><br><br>  Smoothing can be done by creating the appropriate layer: <br><br><pre><code class="python hljs">tf.keras.layers.Flatten(input_shape=[<span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>])</code> </pre> <br>  This layer converts a 2D image of 28x28 pixels (for each pixel there is 1 byte for shades of gray) into a 1D array consisting of 784 pixels. <br><br>  Input values ‚Äã‚Äãwill be fully connected with our first <code>dense</code> network layer, the size of which we have chosen to be equal to 128 neurons. <br><br><img src="https://habrastorage.org/webt/mk/n_/3w/mkn_3wrocxruhbwhil0fmh5wh_8.png"><br><br>  Here is how the creation of this layer in the code will look like: <br><br><pre> <code class="python hljs">tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">128</span></span>, activation=tf.nn.relu)</code> </pre><br>  Stop!  What is this <code>tf.nn.relu</code> ?  We did not use this in our previous example with a neural network when converting degrees Celsius to degrees Fahrenheit!  The bottom line is that the current task is much more complicated than the one that was used as an introductory example - converting degrees Celsius to degrees Fahrenheit. <br><br>  <code>ReLU</code> is a mathematical function that we add to our fully connected layer and that gives more power to our network.  In fact, this is a small extension for our fully connected layer, which allows our neural network to solve more complex problems.  We will not go into details, but you will find more detailed information below. <br><br>  Finally, our last layer, also known as the output layer, consists of 10 neurons.  It consists of 10 neurons because our Fashion MNIST dataset contains 10 categories of clothing.  Each of these 10 output values ‚Äã‚Äãwill represent the probability that the image fed to the input belongs to this category of clothing.  In other words, these values ‚Äã‚Äãreflect the ‚Äúconfidence‚Äù of the model in the correctness of the prediction and the correlation of the submitted image with a certain of 10 categories of clothing at the output.  For example, what is the probability that the image of a dress, sneakers, shoes, etc. <br><br><img src="https://habrastorage.org/webt/fo/2b/3v/fo2b3vakws6ubmiwtj9rrctltla.png"><br><br>  For example, if an image of a shirt is submitted to the input of our neural network, the model can give us results like the ones you see in the image above - the probability of matching the input image with the output label. <br><br>  If you pay attention, you will notice that the greatest probability - 0.85 refers to the mark 6, which corresponds to the shirt.  The model is 85% sure that the image on the shirt.  Usually those things that will look like shirts will also have a high estimate of probability, and the least similar things will have the lowest estimate of probability. <br><br>  Since all 10 output values ‚Äã‚Äãcorrespond to probabilities, when summing all these values ‚Äã‚Äãwe get 1. These 10 values ‚Äã‚Äãare also called probability distributions. <br><br>  Now we need an output layer to calculate the same probabilities for each label. <br><br><img src="https://habrastorage.org/webt/v5/tt/hk/v5tthkilik-9reer8owxjpv-x3m.png"><br><br>  And we will do this with the following command: <br><br><pre> <code class="python hljs">tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">10</span></span>, activation=tf.nn.softmax)</code> </pre><br>  In fact, whenever we create neural networks that solve classification problems, we always use a fully connected layer as the last layer of the neural network.  The last layer of the neural network must contain the number of neurons equal to the number of classes, belonging to which we define and use the softmax activation function. <br><br><h3>  <code>ReLU</code> - neuron activation function </h3><br>  In this lesson we talked about <code>ReLU</code> as something that expands the capabilities of our neural network and gives it extra power. <br><br>  <code>ReLU</code> is a mathematical function that looks like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/691/c04/e7b/691c04e7b270706458daf61c4b38cf22.png" alt="image"><br><br>  The <code>ReLU</code> function returns 0 if the input value was a negative value or zero, in all other cases the function will return the original input value. <br><br>  <code>ReLU</code> makes it possible to solve non-linear problems. <br><br>  The conversion of degrees Celsius to degrees Fahrenheit is a linear problem, because the expression <code>f = 1.8*c + 32</code> is an equation of a straight line - <code>y = m*x + b</code> .  But most of the tasks that we want to solve are non-linear.  In such cases, adding the activation function of the ReLU to our fully connected layer can help to cope with such tasks. <br><br>  <code>ReLU</code> is just one type of activation function.  There are activation functions such as sigmoid, ReLU, ELU, tanh, but it is <code>ReLU</code> that <code>ReLU</code> most often used as the default activation function.  To build and use models that include a ReLU, you do not need to understand how it works inside.  If you still want to understand better, we recommend <a href="https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning">this article</a> . <br><br>  Let's go over the new terms introduced in this lesson: <br><br><ul><li>  <b>Smoothing</b> - the process of converting a 2D image to a 1D vector; </li><li>  <b>ReLU</b> is an activation function that allows the model to solve non-linear problems; </li><li>  <b>Softmax</b> is a function that calculates probabilities for each possible output class; </li><li>  <b>Classification</b> is a class of machine learning tasks used to distinguish between two or more categories (classes). </li></ul><br><h2>  Training and Testing </h2><br>  When training a model any model in machine learning it is always necessary to divide the data set into at least two different sets - the data set used for training and the data set used for testing.  In this part, we will understand why it is worth doing so. <br><br>  Let's recall how we distributed our data set from Fashion MNIST consisting of 70,000 copies. <br><br><img src="https://habrastorage.org/webt/4b/ur/60/4bur602odizkfsdpt0fds-3fnxk.png"><br><br>  We suggested dividing 70,000 into two parts - leaving 60,000 for training in the first part and 10,000 for testing in the second part.  The need for such an approach is caused by the following fact: after the model has been trained in 60,000 copies, it is necessary to check the results and the effectiveness of its work on such examples that were not yet in the data set on which the model was trained. <br><br>  In a way, it is like passing an exam at school.  Before you take the exam, you are diligently engaged in solving problems of a particular class.  Then, in the exam, you encounter the same class of problems, but other input data.  There is no point in giving the same data to the input that was used during the training session, otherwise the task will be to memorize solutions, and not to find a solution model.  That is why in the exams you are confronted with such tasks that were not previously in the training program.  Only in this way can you check whether the model has learned the general solution or not. <br><br>  The same thing happens with machine learning.  You show some data that represents a certain class of problems that you want to learn to solve.  In our case, with the data set from Fashion MNIST, we want the neural network to be able to determine the category to which the item of clothing in the image belongs.  That is why we are training our model with 60,000 examples that contain all categories of clothing items.  After a workout, we want to test the model‚Äôs effectiveness, so we feed the remaining 10,000 items of clothing that the model hasn‚Äôt yet seen.  If we decided not to do this, do not test on 10,000 examples, we would not be able to say with confidence whether our model had actually learned to determine the class of an item of clothing or she remembered all pairs of input + output values. <br><br>  That is why in machine learning we always have a data set for training and a data set for testing. <br><br>  <a href="https://medium.com/tensorflow/introducing-tensorflow-datasets-c7f01f7e19f3">The TensorFlow Kit Kit</a> provides a collection of ready-to-use training data. <br><br>  Data sets are usually divided into several blocks, each of which is used at a certain stage of training and testing the effectiveness of the neural network.  In this part we are talking about: <br><br><ul><li>  <b>training dataset</b> : a dataset designed to train a neural network; </li><li>  <b>test dataset</b> : a dataset designed to test the performance of a neural network; </li></ul><br>  Consider another set of data, which I call the validation dataset.  This data set is not used <b>to</b> train a model, only <b>during a</b> workout.  So, after our model has gone through several training cycles, we feed our test data set to it and look at the results.  For example, if during a workout the value of the loss function decreases, and the accuracy deteriorates on the test data set, then this means that our model simply remembers the pairs of input-output values. <br><br>  The test data set is reused at the very end of the workout to measure the final accuracy of the model predictions. <br><br>  More <a href="https://developers.google.com/machine-learning/crash-course/training-and-test-sets/video-lecture">information about the training and test data sets can be found in Google Crash Course</a> . <br><br><h2>  Practical part in CoLab </h2><br>  <a href="https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l03c01_classifying_images_of_clothing.ipynb">Link to the original CoLab in English</a> and <a href="https://colab.research.google.com/drive/1Q8qC-3SGUq9kZjJGFM9xsJh5cYGb2LUI">link to Russian CoLab</a> . <br><br><h2>  Classification of images of clothing items </h2><br>  In this part of the lesson, we will build and train the neural network to classify images of items of clothing, such as dresses, sneakers, shirts, t-shirts, etc. <br><br>  All right, if some moments will be incomprehensible.  The purpose of this course is to introduce you to TensorFlow and in parallel to explain the algorithms of its work and develop a common understanding of projects using TensorFlow, and not go into the details of implementation. <br><br>  In this part, we use <code>tf.keras</code> - a high-level API for building and training models in TensorFlow. <br><br><h3>  Installing and importing dependencies </h3><br>  We need <a href="https://www.tensorflow.org/datasets/">a TensorFlow dataset</a> , an API that simplifies loading and accessing datasets provided by several services.  We will also need several auxiliary libraries. <br><br><pre> <code class="python hljs">!pip install -U tensorflow_datasets</code> </pre><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> __future__ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> absolute_import, division, print_function, unicode_literals <span class="hljs-comment"><span class="hljs-comment">#  TensorFlow    TensorFlow import tensorflow as tf import tensorflow_datasets as tfds tf.logging.set_verbosity(tf.logging.ERROR) #   import math import numpy as np import matplotlib.pyplot as plt #    import tqdm import tqdm.auto tqdm.tqdm = tqdm.auto.tqdm print(tf.__version__) tf.enable_eager_execution()</span></span></code> </pre><br><h3>  Importing the MNIST dataset </h3><br>  This example uses the MNIST Fashion dataset, which contains 70,000 images of clothing items in 10 categories in grayscale.  The images contain clothes in low resolution (28x28 pixels), as shown below: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/18d/2c1/da3/18d2c1da3b5c7dbff14ea81077d9ed24.png" alt="image"><br><br>  Fashion MNIST is used as a replacement for the classic MNIST data set ‚Äî most often used as ‚ÄúHello, World!‚Äù In machine learning and computer vision.  The MNIST dataset contains handwritten numbers (0, 1, 2, etc.) in the same format as the clothing elements in our example. <br><br>  In our example, we use Fashion MNIST because of the diversity and because this task is more interesting from the point of view of implementation than the solution of a typical problem on the MNIST data set.  Both data sets are small enough, therefore, they are used to verify the correct operation of the algorithm.  Excellent data sets for starting learning machine learning, testing and debugging code. <br><br>  We will use 60,000 images to train the network and 10,000 images to test the accuracy of training and image classification.  You can directly access the MNIST Fashion dataset via TensorFlow using the API: <br><br><pre> <code class="python hljs">dataset, metadata = tfds.load(<span class="hljs-string"><span class="hljs-string">'fashion_mnist'</span></span>, as_supervised=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, with_info=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) train_dataset, test_dataset = dataset[<span class="hljs-string"><span class="hljs-string">'train'</span></span>], dataset[<span class="hljs-string"><span class="hljs-string">'test'</span></span>]</code> </pre><br>  By loading a data set we get metadata, a training data set and a test data set. <br><br><ul><li>  The model is trained on a data set from `train_dataset` </li><li>  Model tested on dataset from `test_dataset` </li></ul><br>  The images are two-dimensional arrays <code>2828</code> , where the values ‚Äã‚Äãin each cell can be in the interval <code>[0, 255]</code> .  Labels - an array of integers, where each value in the interval <code>[0, 9]</code> .  These labels correspond to the output image class as follows: <br><br><div class="scrollable-table"><table><tbody><tr><th>  Tag </th><th>  Class </th></tr><tr><td>  0 </td><td>  T-shirt / top </td></tr><tr><td>  one </td><td>  Shorts </td></tr><tr><td>  2 </td><td>  Sweater </td></tr><tr><td>  3 </td><td>  The dress </td></tr><tr><td>  four </td><td>  Cloak </td></tr><tr><td>  five </td><td>  Sandals </td></tr><tr><td>  6 </td><td>  Shirt </td></tr><tr><td>  7 </td><td>  Sneaker </td></tr><tr><td>  eight </td><td>  A bag </td></tr><tr><td>  9 </td><td>  The boot </td></tr></tbody></table></div><br><br>  Each image belongs to one tag.  Since the class names are not contained in the original data set, let's save them for later use when we draw the images: <br><br><pre> <code class="python hljs">class_names = [<span class="hljs-string"><span class="hljs-string">' / '</span></span>, <span class="hljs-string"><span class="hljs-string">""</span></span>, <span class="hljs-string"><span class="hljs-string">""</span></span>, <span class="hljs-string"><span class="hljs-string">""</span></span>, <span class="hljs-string"><span class="hljs-string">""</span></span>, <span class="hljs-string"><span class="hljs-string">""</span></span>, <span class="hljs-string"><span class="hljs-string">""</span></span>, <span class="hljs-string"><span class="hljs-string">""</span></span>, <span class="hljs-string"><span class="hljs-string">""</span></span>, <span class="hljs-string"><span class="hljs-string">""</span></span>]</code> </pre><br><h4>  Examine the data </h4><br>  Let's examine the format and structure of the data presented in the training set before training the model.  The following code shows that 60,000 images are in the training dataset, and 10,000 images are in the test set: <br><br><pre> <code class="python hljs">num_train_examples = metadata.splits[<span class="hljs-string"><span class="hljs-string">'train'</span></span>].num_examples num_test_examples = metadata.splits[<span class="hljs-string"><span class="hljs-string">'test'</span></span>].num_examples print(<span class="hljs-string"><span class="hljs-string">'  : {}'</span></span>.format(num_train_examples)) print(<span class="hljs-string"><span class="hljs-string">'  : {}'</span></span>.format(num_test_examples))</code> </pre><br><h3>  Data preprocessing </h3><br>  The value of each pixel in the image is in the interval <code>[0,255]</code> .  In order for the model to work correctly, these values ‚Äã‚Äãneed to be normalized - lead to values ‚Äã‚Äãin the interval <code>[0,1]</code> .  Therefore, just below, we declare and implement the normalization function, and then apply it to each image in the training and test data sets. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">normalize</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(images, labels)</span></span></span><span class="hljs-function">:</span></span> images = tf.cast(images, tf.float32) images /= <span class="hljs-number"><span class="hljs-number">255</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> images, labels <span class="hljs-comment"><span class="hljs-comment">#  map         #      train_dataset = train_dataset.map(normalize) test_dataset = test_dataset.map(normalize)</span></span></code> </pre><br><h4>  We study the processed data </h4><br>  Let's draw an image to take a look at it: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#          #   reshape() for image, label in test_dataset.take(1): break; image = image.numpy().reshape((28, 28)) #   plt.figure() plt.imshow(image, cmap=plt.cm.binary) plt.colorbar() plt.grid(False) plt.show()</span></span></code> </pre><br><img src="https://habrastorage.org/webt/ce/se/hw/cesehwjbca_ol0s1dcpxnaxyu2i.png"><br><br>  We will display the first 25 images from the training data set and under each image we will indicate to which class it belongs. <br><br>  Make sure that the data is in the correct format and we are ready to start creating and training the network. <br><br><pre> <code class="python hljs">plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>,<span class="hljs-number"><span class="hljs-number">10</span></span>)) i = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (image, label) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> test_dataset.take(<span class="hljs-number"><span class="hljs-number">25</span></span>): image = image.numpy().reshape((<span class="hljs-number"><span class="hljs-number">28</span></span>,<span class="hljs-number"><span class="hljs-number">28</span></span>)) plt.subplot(<span class="hljs-number"><span class="hljs-number">5</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>,i+<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.xticks([]) plt.yticks([]) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) plt.imshow(image, cmap=plt.cm.binary) plt.xlabel(class_names[label]) i += <span class="hljs-number"><span class="hljs-number">1</span></span> plt.show()</code> </pre><br><img src="https://habrastorage.org/webt/4h/_v/s7/4h_vs7mj97mmqknia5mpnzaqfis.png"><br><br><h4>  Build a model </h4><br>  Building a neural network requires adjusting layers, and then building a model with optimization and loss functions. <br><br><h4>  Customize layers </h4><br>  The basic element in the construction of a neural network is a layer.  The layer extracts the view from the data it received at the input.  The result of the work of several related layers, we get an idea that makes sense to solve the problem. <br><br>  Most of the time in deep learning, you will be creating links between simple layers.  Most layers, for example, such as tf.keras.layers.Dense have a set of parameters that can be ‚Äútailored‚Äù during the learning process. <br><br><pre> <code class="python hljs">model = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape=(<span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)), tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">128</span></span>, activation=tf.nn.relu), tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">10</span></span>, activation=tf.nn.softmax) ])</code> </pre><br>  The network consists of three layers: <br><br><ul><li>  <b>input</b> <code>tf.keras.layers.Flatten</code> - this layer converts images of 28x28 pixels into a 1D array of size 784 (28 * 28).  On this layer, we have no parameters for training, since this layer is only concerned with the conversion of input data. </li><li>  <code>tf.keras.layers.Dense</code> <b>hidden layer</b> is a densely connected layer of 128 neurons.  Each neuron (node) takes as input all 784 values ‚Äã‚Äãfrom the previous layer, changes the input values ‚Äã‚Äãaccording to internal weights and offsets during training, and returns a single value to the next layer. </li><li>  <b>output layer</b> <code>ts.keras.layers.Dense</code> - <code>softmax</code> consists of 10 neurons, each of which represents a particular class of item of clothing.  As in the previous layer, each neuron accepts the input values ‚Äã‚Äãof all 128 neurons of the previous layer.  The weights and displacements of each neuron on this layer change during training in such a way that the resulting value is in the interval <code>[0,1]</code> and represents the probability that the image belongs to this class.  The sum of all output values ‚Äã‚Äãof 10 neurons is 1. </li></ul><br><h4>  Compile the model </h4><br>  Before we begin to train the model, it is worth making a few adjustments.  These settings are made during model building when calling the compile method: <br><br><ul><li>  <b>the loss function</b> is an algorithm for measuring how far the desired value is from the predicted value. </li><li>  <b>the optimization function</b> ‚Äî an agroitm of ‚Äúfitting‚Äù the internal parameters (weights and displacements) of the model to minimize the loss function; </li><li>  <b>metrics</b> - used to monitor the process of training and testing.  The example below uses such a metric as <code></code> , the percentage of images that were correctly classified. </li></ul><br><pre> <code class="python hljs">model.compile(optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre><br><h3>  We train model </h3><br>  First, we determine the sequence of actions during training at the training data set: <br><br><ol><li>  Repeat an infinite number of times the input data set using the <code>dataset.repeat()</code> method (the <code>epochs</code> parameter, which is described below, determines the number of all training iterations to perform) </li><li>  The <code>dataset.shuffle(60000)</code> method <code>dataset.shuffle(60000)</code> all images so that the order of input data does not affect the learning of our model. </li><li>  The <code>dataset.batch(32)</code> method tells the training method <code>model.fit</code> use blocks of 32 images and tags when updating internal model variables. </li></ol><br>  Training takes place by calling the <code>model.fit</code> method: <br><br><ul><li>  Sends <code>train_dataset</code> to model input. </li><li>  The model learns to match the input image with the label. </li><li>  The parameter <code>epochs=5</code> limits the number of trainings to 5 complete training iterations on the data set, which ultimately gives us a workout on 5 * 60000 = 300,000 examples. </li></ul><br>  (you can ignore the <code>steps_per_epoch</code> parameter, soon this parameter will be excluded from the method). <br><br><pre> <code class="python hljs">BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">32</span></span> train_dataset = train_dataset.repeat().shuffle(num_train_examples).batch(BATCH_SIZE) test_dataset = test_dataset.batch(BATCH_SIZE)</code> </pre><br><pre> <code class="python hljs">model.fit(train_dataset, epochs=<span class="hljs-number"><span class="hljs-number">5</span></span>, steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE))</code> </pre><br>  And here is the conclusion: <br><br> <code>Epoch 1/5 <br> 1875/1875 [==============================] - 26s 14ms/step - loss: 0.4921 - acc: 0.8267 <br> Epoch 2/5 <br> 1875/1875 [==============================] - 20s 11ms/step - loss: 0.3652 - acc: 0.8686 <br> Epoch 3/5 <br> 1875/1875 [==============================] - 20s 11ms/step - loss: 0.3341 - acc: 0.8782 <br> Epoch 4/5 <br> 1875/1875 [==============================] - 19s 10ms/step - loss: 0.3111 - acc: 0.8858 <br> Epoch 5/5 <br> 1875/1875 [==============================] - 16s 8ms/step - loss: 0.2911 - acc: 0.8922 <br></code> <br>  In the process of training the model, the value of the loss function and the accuracy metric are displayed for each training iteration.  This model achieves an accuracy of around 0.88 (88%) on training data. <br><br><h4>  Check accuracy </h4><br>  Check what accuracy gives the model on the test data.  We will use all the examples that we have in the test dataset to verify accuracy. <br><br><pre> <code class="python hljs">test_loss, test_accuracy = model.evaluate(test_dataset, steps=math.ceil(num_test_examples/BATCH_SIZE)) print(<span class="hljs-string"><span class="hljs-string">"    : "</span></span>, test_accuracy)</code> </pre><br>  Conclusion: <br><br> <code>313/313 [==============================] - 1s 5ms/step - loss: 0.3440 - acc: 0.8793 <br>     : 0.8793 <br></code> <br><br>  As you can see, the accuracy on the test dataset turned out to be less accurate on the training dataset.  This is quite normal, since the model was trained on train_dataset data.  When the model detects images that it has never seen before (from the train_dataset dataset), it is quite obvious that the classification efficiency will decrease. <br><br><h3>  Predict and explore </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> We can use the trained model to get predictions for some images. </font></font><br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> test_images, test_labels <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> test_dataset.take(<span class="hljs-number"><span class="hljs-number">1</span></span>): test_images = test_images.numpy() test_labels = test_labels.numpy() predictions = model.predict(test_images)</code> </pre><br><pre> <code class="python hljs">predictions.shape</code> </pre><br>  Conclusion: <br><br> <code>(32, 10) <br></code> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the example above, the model predicted the labels for each test input image. </font><font style="vertical-align: inherit;">Let's look at the first prediction:</font></font><br><br><pre> <code class="python hljs">predictions[<span class="hljs-number"><span class="hljs-number">0</span></span>]</code> </pre><br>  Conclusion: <br><br><pre> <code class="python hljs">array([<span class="hljs-number"><span class="hljs-number">3.1365351e-05</span></span>, <span class="hljs-number"><span class="hljs-number">9.0029374e-08</span></span>, <span class="hljs-number"><span class="hljs-number">5.0016739e-03</span></span>, <span class="hljs-number"><span class="hljs-number">6.3597057e-05</span></span>, <span class="hljs-number"><span class="hljs-number">6.8342477e-02</span></span>, <span class="hljs-number"><span class="hljs-number">1.0856857e-08</span></span>, <span class="hljs-number"><span class="hljs-number">9.2655218e-01</span></span>, <span class="hljs-number"><span class="hljs-number">1.8982398e-09</span></span>, <span class="hljs-number"><span class="hljs-number">8.4999456e-06</span></span>, <span class="hljs-number"><span class="hljs-number">1.0296091e-09</span></span>], dtype=float32)</code> </pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Recall that model predictions are an array of 10 values. </font><font style="vertical-align: inherit;">These values ‚Äã‚Äãdescribe the ‚Äúconfidence‚Äù of the model that the input image belongs to a certain class (clothing item). </font><font style="vertical-align: inherit;">We can see the maximum value as follows:</font></font><br><br><pre> <code class="python hljs">np.argmax(predictions[<span class="hljs-number"><span class="hljs-number">0</span></span>])</code> </pre><br>  Conclusion: <br><br><pre> <code class="python hljs"><span class="hljs-number"><span class="hljs-number">6</span></span></code> </pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This means that the model showed the greatest confidence that this image belongs to the class with label 6 (class_names [6]). </font><font style="vertical-align: inherit;">We can check and make sure that the result is true and correct:</font></font><br><br><pre> <code class="python hljs">test_labels[<span class="hljs-number"><span class="hljs-number">0</span></span>]</code> </pre><br><pre> <code class="python hljs"><span class="hljs-number"><span class="hljs-number">6</span></span></code> </pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> We can display all input images and corresponding model predictions in 10 classes: </font></font><br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plot_image</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(i, predictions_array, true_labels, images)</span></span></span><span class="hljs-function">:</span></span> predictions_array, true_label, img = predictions_array[i], true_label[i], images[i] plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) plt.xticks([]) plt.yticks([]) plt.imshow(img[...,<span class="hljs-number"><span class="hljs-number">0</span></span>], cmap=plt.cm.binary) predicted_label = np.argmax(predictions_array) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> predicted_label == true_label: color = <span class="hljs-string"><span class="hljs-string">'blue'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: color = <span class="hljs-string"><span class="hljs-string">'red'</span></span> plt.xlabel(<span class="hljs-string"><span class="hljs-string">"{} {:2.0f}% ({})"</span></span>.format(class_names[predicted_label], <span class="hljs-number"><span class="hljs-number">100</span></span> * np.max(predictions_array), class_names[true_label]), color=color) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plot_value_array</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(i, predictions_array, true_label)</span></span></span><span class="hljs-function">:</span></span> predictions_array, true_label = predictions_array[i], true_label[i] plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) plt.xticks([]) plt.yticks([]) thisplot = plt.bar(range(<span class="hljs-number"><span class="hljs-number">10</span></span>), predictions_array, color=<span class="hljs-string"><span class="hljs-string">"#777777"</span></span>) plt.ylim([<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>]) predicted_label = np.argmax(predictions_array) thisplot[predicted_label].set_color(<span class="hljs-string"><span class="hljs-string">'red'</span></span>) thisplot[true_label].set_color(<span class="hljs-string"><span class="hljs-string">'blue'</span></span>)</code> </pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Let's take a look at the 0th image, the result of the model prediction and the array of predictions. </font></font><br><br><pre> <code class="python hljs">i = <span class="hljs-number"><span class="hljs-number">0</span></span> plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">6</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>)) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>) plot_image(i, predictions, test_labels, test_images) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>) plot_value_array(i, predictions, test_labels)</code> </pre><br><img src="https://habrastorage.org/webt/fc/7i/ef/fc7iefucuvtopx4_avluy-rq1ei.png"><br><br><pre> <code class="python hljs">i = <span class="hljs-number"><span class="hljs-number">12</span></span> plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">6</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>)) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>) plot_image(i, predictions, test_labels, test_images) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>) plot_value_array(i, predictions, test_labels)</code> </pre><br><img src="https://habrastorage.org/webt/n0/2y/tj/n02ytjjdkeubvkqvjdusbkwoemy.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's now display some images with their respective predictions. </font><font style="vertical-align: inherit;">Correct predictions are blue, incorrect ones are red. </font><font style="vertical-align: inherit;">The value under the image reflects the percentage of confidence in the model that the input image corresponds to this class. </font><font style="vertical-align: inherit;">Please note that the result may be incorrect even if the value of ‚Äúconfidence‚Äù is large.</font></font><br><br><pre> <code class="python hljs">num_rows = <span class="hljs-number"><span class="hljs-number">5</span></span> num_cols = <span class="hljs-number"><span class="hljs-number">3</span></span> num_images = num_rows * num_cols plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">2</span></span>*<span class="hljs-number"><span class="hljs-number">2</span></span>*num_cols, <span class="hljs-number"><span class="hljs-number">2</span></span>*num_rows)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(num_images): plt.subplot(num_rows, <span class="hljs-number"><span class="hljs-number">2</span></span>*num_cols, <span class="hljs-number"><span class="hljs-number">2</span></span>*i + <span class="hljs-number"><span class="hljs-number">1</span></span>) plot_image(i, predictions, test_labels, test_images) plt.subplot(num_rows, <span class="hljs-number"><span class="hljs-number">2</span></span>*num_cols, <span class="hljs-number"><span class="hljs-number">2</span></span>*i + <span class="hljs-number"><span class="hljs-number">2</span></span>) plot_value_array(i, predictions, test_labels)</code> </pre><br><img src="https://habrastorage.org/webt/m1/11/je/m111jevw7ptxblu2ccmlwmtonva.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Use the trained model to predict the label for a single image: </font></font><br><br><pre> <code class="python hljs">img = test_images[<span class="hljs-number"><span class="hljs-number">0</span></span>] print(img.shape)</code> </pre><br>  Conclusion: <br><br><pre> <code class="python hljs">(<span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Models in </font></font><code>tf.keras</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">optimized for prediction blocks (collections). </font><font style="vertical-align: inherit;">Therefore, despite the fact that we use a single element, it is necessary to add it to the list:</font></font><br><br><pre> <code class="python hljs">img = np.array([img]) print(img.shape)</code> </pre><br>  Conclusion: <br><br> <code>(1, 28, 28, 1)</code> <br> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Now let's predict the result: </font></font><br><br><pre> <code class="python hljs">predictions_single = model.predict(img) print(predictions_single)</code> </pre><br>  Conclusion: <br><br><pre> <code class="python hljs">[[<span class="hljs-number"><span class="hljs-number">3.1365438e-05</span></span> <span class="hljs-number"><span class="hljs-number">9.0029722e-08</span></span> <span class="hljs-number"><span class="hljs-number">5.0016833e-03</span></span> <span class="hljs-number"><span class="hljs-number">6.3597123e-05</span></span> <span class="hljs-number"><span class="hljs-number">6.8342514e-02</span></span> <span class="hljs-number"><span class="hljs-number">1.0856857e-08</span></span> <span class="hljs-number"><span class="hljs-number">9.2655218e-01</span></span> <span class="hljs-number"><span class="hljs-number">1.8982469e-09</span></span> <span class="hljs-number"><span class="hljs-number">8.4999692e-06</span></span> <span class="hljs-number"><span class="hljs-number">1.0296091e-09</span></span>]]</code> </pre><br><pre> <code class="python hljs">plot_value_array(<span class="hljs-number"><span class="hljs-number">0</span></span>, predictions_single, test_labels) _ = plt.xticks(range(<span class="hljs-number"><span class="hljs-number">10</span></span>), class_names, rotation=<span class="hljs-number"><span class="hljs-number">45</span></span>)</code> </pre><br><img src="https://habrastorage.org/webt/eo/vw/sl/eovwslxcn_ldtj2abz870ninw4g.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The model.predict method returns a list of lists (array of arrays), each for an image from a block of input data. </font><font style="vertical-align: inherit;">We get the only result for our single input image:</font></font><br><br><pre> <code class="python hljs">np.argmax(predictions_single[<span class="hljs-number"><span class="hljs-number">0</span></span>])</code> </pre><br>  Conclusion: <br><br><pre> <code class="python hljs"><span class="hljs-number"><span class="hljs-number">6</span></span></code> </pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> As previously, the model predicted label 6 (shirt). </font></font><br><br><h3>  Exercises </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Experiment with different models and see how the accuracy will change. </font><font style="vertical-align: inherit;">In particular, try changing the following parameters:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> set the epochs parameter to 1; </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> change the number of neurons in the hidden layer, for example, from a low value of 10 to 512 and see how the accuracy of the model prediction will change; </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> add additional layers between the flatten-layer (smoothing layer) and the final dense-layer, experiment with the number of neurons on this layer; </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> do not normalize pixel values ‚Äã‚Äãand see what happens. </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Do not forget to activate the GPU so that all the calculations are faster ( </font></font><code>Runtime -&gt; Change runtime type -&gt; Hardware accelertor -&gt; GPU</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">). </font><font style="vertical-align: inherit;">Also, if in the process of work you have any problems, then try to reset the global environment settings:</font></font><br><br><ul><li> <code>Edit -&gt; Clear all outputs</code> </li> <li> <code>Runtime -&gt; Reset all runtimes</code> </li> </ul><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Degrees Celsius VS MNIST </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- At this stage, we have already encountered two types of neural networks. Our first neural network has learned to convert degrees Celsius to degrees Frenheit, returning a single value that can be in a wide range of numerical values. </font></font><br><br><img src="https://habrastorage.org/webt/o8/ag/_t/o8ag_trkedahoa0ftg3pstkirt4.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Our second neural network returns 10 probability values ‚Äã‚Äãthat represent the network‚Äôs confidence that the image fed to the input corresponds to a certain class. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Neural networks can be used to solve various kinds of problems. </font></font><br><br><img src="https://habrastorage.org/webt/no/0v/jo/no0vjoulnrva_-bky0uauc3tesi.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The first class of problems that we solved with the prediction of a single value is called </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">regression.</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. The conversion of degrees Celsius to degrees Fahrenheit is one example of the task of this class. Another example of this class of tasks may be the problem of determining the value of a house by room number, total area, location, and other characteristics. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The second class of problems, which we considered in this lesson by classifying images into existing categories, is called </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">classification</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . According to the input data, the model will return the probability distribution (the ‚Äúconfidence‚Äù of the model that the input value belongs to this class). In this lesson, we developed a neural network that classified items of clothing into 10 categories, and in the next lesson we will learn to determine who the dog or cat is depicted in the photo, this task also applies to the classification task.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's summarize and note the difference between these two classes of problems - </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">regression</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">classification</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><img src="https://habrastorage.org/webt/_c/wj/qu/_cwjquy9ivk-s3zma34qamyakoq.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Congratulations, you learned two types of neural networks! Get ready for the next lecture, where we will study a new type of neural networks - convolutional neural networks (CNN, convolutional neural networks).</font></font><br><br><h3>  Results </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In this lesson, we taught the neural network to classify images with items of clothing. To do this, we used the MNIST Fashion dataset, which contains 70,000 images of clothing items. 60,000 of which we used to train the neural network, and the remaining 10,000 to test its performance. In order to feed these images to the input of our neural network, we needed to convert them (smooth out) from a 28x28 2D format to a 1D format of 784 elements. Our network consisted of a fully connected layer of 128 neurons and an output layer of 10 neurons, corresponding to the number of tags (classes, categories of elements of clothing). These 10 output values ‚Äã‚Äãrepresented the probability distribution for each class. </font><i><font style="vertical-align: inherit;">Softmax</font></i><font style="vertical-align: inherit;"> activation </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">function</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">counted the probability distribution. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We also learned about the differences between </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">regression</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">classification</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br><br><ul><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Regression</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : a model that returns a single value, for example, the value of a house.</font></font></li><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Classification</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : a model that returns a probability distribution among several categories. </font><font style="vertical-align: inherit;">For example, in our problem with Fashion MNIST, the output values ‚Äã‚Äãwere 10 probability values, each of which was associated with a particular class (category of clothing item). </font><font style="vertical-align: inherit;">I remind you that we used the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">softmax</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> activation function </font><font style="vertical-align: inherit;">just to get a probability distribution on the last layer.</font></font></li></ul><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Video version of the article</font></font></b> <div class="spoiler_text"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The video comes out a few days after publication and is added to the article. </font></font><br></div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ... and standard call-to-action - subscribe, put a plus and share :) </font></font><br>  <a href="https://www.youtube.com/c/%25D0%2590%25D0%25BD%25D0%25B4%25D1%2580%25D0%25B5%25D0%25B9%25D0%25A8%25D0%25BC%25D0%25B8%25D0%25B3%3Fsub_confirmation%3D1">YouTube</a> <br>  <a href="https://t.me/ashmig">Telegram</a> <br>  <a href="https://vk.com/ashmig">In contact with</a> </div><p>Source: <a href="https://habr.com/ru/post/454034/">https://habr.com/ru/post/454034/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../45402/index.html">Page navigation</a></li>
<li><a href="../454024/index.html">MPPT charge controller on STM32F334C8T6</a></li>
<li><a href="../454028/index.html">Sketches with PHP Russia 2019: clean code, dark magic</a></li>
<li><a href="../454030/index.html">Odaydzhest: interesting for designers for the week</a></li>
<li><a href="../454032/index.html">Router and Data Passing Architecture Clean Swift</a></li>
<li><a href="../454036/index.html">6 ways to go to hell ready-made solutions and lower a million or two</a></li>
<li><a href="../454038/index.html">Ilya Zverev: Over the years, OpenStreetMap has been overgrown with such a serious infrastructure that you can draw a map without leaving home</a></li>
<li><a href="../45404/index.html">Let's remember the good old mechanical devices and try to guess their purpose.</a></li>
<li><a href="../454040/index.html">Conference React Russia 2019 is June 1</a></li>
<li><a href="../454042/index.html">Pay what you want: how this model manifested itself in music, and who tried to earn it</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>