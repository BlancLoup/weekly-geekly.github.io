<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Reinforcement training in PyBullet</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Many who study machine learning are familiar with the OpenAI project, of which Ilon Mask is one of the founders, and use the OpenAI Gym platform as a ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Reinforcement training in PyBullet</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/0g/x5/ai/0gx5aizowrlyrgkvekcxlxh9pge.png" alt="image"><br><br>  Many who study machine learning are familiar with the OpenAI project, of which Ilon Mask is one of the founders, and use the <a href="https://gym.openai.com/">OpenAI Gym</a> platform as a medium to train their neural network models. <br><br>  Gym contains a huge set of environments, some of them - various kinds of physical simulations: the movement of animals, humans, <a href="https://blog.openai.com/ingredients-for-robotics-research/">robots</a> .  These simulations are based on the <a href="http://www.mujoco.org/">MuJoCo</a> physics engine, which is free for educational and scientific purposes. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In this article, we will create an extremely simple physical simulation similar to the OpenAI Gym environment, but based on the Bullet ( <a href="https://pybullet.org/wordpress/">PyBullet</a> ) free physics engine.  And also create an agent to work with this environment. <br><a name="habracut"></a><br>  PyBullet is a python module for creating a physical simulation environment based on the <a href="http://bulletphysics.org/wordpress/">Bullet Physics</a> engine.  He, like MuJoCo, is quite often used as stimulation of various robots, who are interested in habr there is <a href="https://habr.com/company/mailru/blog/343008/">an article</a> with real examples. <br><br>  For PyBullet there is a pretty good <a href="https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit">QuickStartGuide</a> , which contains links to examples on the page with the source code on <a href="https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/examples">GitHub</a> . <br><br>  PyBullet allows you to upload already created models in URDF, SDF or MJCF format.  The source has a library of <a href="https://github.com/bulletphysics/bullet3/tree/master/data">models</a> in these formats, as well as fully prepared environments of simulators of <a href="https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/gym/pybullet_envs/bullet">real robots.</a> <br><br>  In our case, we ourselves will create an environment using PyBullet tools.  The environment interface will be <a href="https://gym.openai.com/docs/">similar to</a> the OpenAI Gym interface.  In this way, we will be able to train our agents both in our environment and in the Gym environment. <br><br>  All code (iPython), as well as the work of the program can be seen in the <a href="https://colab.research.google.com/drive/1GE2hlm-swF6M51XKMOCDnPnCLdj1mWbb">Google Laboratory</a> . <br><br><h2>  Environment </h2><br>  Our environment will consist of a ball that can move along a vertical axis within a certain range of heights.  The ball has a mass, and it is acted upon by gravity, and the agent must, by controlling the vertical force applied to the ball, bring it to the goal.  Target height changes with every experience restart. <br><br><img src="https://habrastorage.org/webt/w-/wy/jr/w-wyjrj1zphr8aqndhutrnno1po.png" alt="image"><br><br>  The simulation is very simple, and in fact can be considered as a simulation of some elementary propulsion. <br><br>  Three methods are used to work with the environment: <i><b>reset</b></i> (restarting the experience and creating all the objects of the environment), <i><b>step</b></i> (applying the selected action and obtaining the resulting state of the environment), <i><b>render</b></i> (visual display of the environment). <br><br>  When you initialize the environment, you need to connect our object to the physical simulation.  Two connection options are possible: with a graphical interface (GUI) and without (DIRECT). In our case, this is DIRECT. <br><br><pre><code class="python hljs">pb.connect(pb.DIRECT)</code> </pre> <br><h4>  reset </h4><br>  With each new experience, we reset the <i>pb.resetSimulation ()</i> simulation and create all the environment objects again. <br><br>  In PyBullet, objects have 2 forms: a collision shape ( <i>Collision Shape</i> ), and a visual shape ( <i>Visual Shape</i> ).  The first is used by the physics engine to calculate collisions of objects and, to speed up the calculation of physics, usually has a simpler form than a real object.  The second is optional, and is used only when forming an image of an object. <br><br>  Forms are collected in a single object (body) - <i>MultiBody</i> .  The body can be composed of one form (a pair of <i>CollisionShape / Visual Shape</i> ), as in our case, or several. <br><br>  In addition to the forms that make up the body, it is necessary to determine its mass, position and orientation in space. <br><br><div class="spoiler">  <b class="spoiler_title">A few words about multi-object bodies.</b> <div class="spoiler_text">  As a rule, in real cases, for the simulation of various mechanisms, bodies consisting of many forms are used.  When creating a body, in addition to the basic form of collisions and visualization, chains of forms of child objects ( <i>Links</i> ), their position and orientation relative to the previous object, as well as types of connections (joints) of objects between themselves ( <i>Joint</i> ) are transmitted to the body.  Types of connections can be fixed, prismatic (sliding within one axis) or rotational (rotating about one axis).  The last 2 types of connections allow you to set the parameters of the respective types of motors ( <i>JointMotor</i> ), such as acting force, speed of movement or torque, thus simulating the engines of the ‚Äújoints‚Äù of the robot.  Read more in the <a href="https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit">documentation</a> . <br></div></div><br>  We will create 3 bodies: Ball, Plane (earth) and Pointer to the target.  The last object will have only the form of visualization and zero mass, so it will not participate in the physical interaction between the bodies: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  floorColShape = pb.createCollisionShape(pb.GEOM_PLANE) #   (GEOM_PLANE), visualShape -    ,   GEOM_BOX floorVisualShapeId = pb.createVisualShape(pb.GEOM_BOX,halfExtents=[100,100,0.0001], rgbaColor=[1,1,.98,1]) pb_floorId = pb.createMultiBody(0,floorColShape,floorVisualShapeId, [0,0,0], [0,0,0,1]) #  PB_BallRadius = 0.2 PB_BallMass = 1 ballPosition = [0,0,5] ballOrientation=[0,0,0,1] ballColShape = pb.createCollisionShape(pb.GEOM_SPHERE,radius=PB_BallRadius) ballVisualShapeId = pb.createVisualShape(pb.GEOM_SPHERE,radius=PB_BallRadius, rgbaColor=[1,0.27,0,1]) pb_ballId = pb.createMultiBody(PB_BallMass, ballColShape, ballVisualShapeId, ballPosition, ballOrientation) #   TARGET_Z = 8 targetPosition = [0,0,TARGET_Z] targetOrientation=[0,0,0,1] targetVisualShapeId = pb.createVisualShape(pb.GEOM_BOX,halfExtents=[1,0.025,0.025], rgbaColor=[0,0,0,1]) pb_targetId = pb.createMultiBody(0,-1, targetVisualShapeId, targetPosition, targetOrientation)</span></span></code> </pre><br>  Determine the gravity and time step of the simulation. <br><br><pre> <code class="python hljs">pb.setGravity(<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">-10</span></span>) pb.setTimeStep(<span class="hljs-number"><span class="hljs-number">1.</span></span>/<span class="hljs-number"><span class="hljs-number">60</span></span>)</code> </pre> <br>  In order for the ball not to fall immediately after running the simulation, we balance gravity. <br><br><pre> <code class="python hljs">pb_force = <span class="hljs-number"><span class="hljs-number">10</span></span> * PB_BallMass pb.applyExternalForce(pb_ballId, <span class="hljs-number"><span class="hljs-number">-1</span></span>, [<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,pb_force], [<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>], pb.LINK_FRAME)</code> </pre> <br><br><h4>  step </h4><br>  The agent selects actions based on the current state of the environment, then calls the <i>step</i> method and gets a new state. <br><br>  Two types of actions are defined: increase and decrease of the force acting on the ball.  Limit values ‚Äã‚Äãof force are limited. <br><br>  After changing the force acting on the ball, a new physical simulation step <i>pb.stepSimulation () is launched</i> , and the following parameters are returned to the agent: <br><br>  <i>observation</i> - observations (state of the environment) <br>  <i>reward</i> - reward for perfect action <br>  <i>done</i> - flag ending the experience <br>  <i>info</i> - additional information <br><br>  Three values ‚Äã‚Äãare returned as states of the environment: the distance to the target, the current force applied to the ball, and the speed of the ball.  The values ‚Äã‚Äãare returned normalized (0..1), since the environmental parameters that determine these values ‚Äã‚Äãmay vary depending on our desire. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     (     Z curPos[2]) curPos, curOrient = pb.getBasePositionAndOrientation(pb_ballId) #     (      Z lin_vel[2]) lin_vel, ang_vel= pb.getBaseVelocity(self.pb_ballId)</span></span></code> </pre> <br>  The reward for the committed action is 1 if the ball is close to the target (target height plus / minus is an acceptable rolling value <i>TARGET_DELTA</i> ) and 0 otherwise. <br>  The experiment ends if the ball goes outside the zone (falls to the ground or flies high).  If the ball reaches the goal, the experience also ends, but only after a certain time has <i>passed</i> ( <i>STEPS_AFTER_TARGET</i> experience steps).  Thus, our agent learns not only to move towards the goal, but also to stop and is located next to it.  Taking into account the fact that the reward when staying close to the goal is equal to 1, a fully successful experience should have a total reward equal to the value of <i>STEPS_AFTER_TARGET</i> . <br><br>  As additional information for displaying statistics, the total number of steps performed as part of the experience, as well as the number of steps performed per second are returned. <br><br><h4>  render </h4><br>  PyBullet has 2 image rendering capabilities - OpenGL based GPU and TinyRenderer based CPU rendering.  In our case, only CPU implementation is possible. <br><br>  To obtain the current frame of the simulation, it is necessary to determine the <a href="http://www.opengl-tutorial.org/ru/beginners-tutorials/tutorial-3-matrices/">view matrix</a> and <a href="http://www.opengl-tutorial.org/ru/beginners-tutorials/tutorial-3-matrices/">projection matrix</a> , and then obtain an <i>rgb</i> image of the specified dimensions from the camera. <br><br><pre> <code class="python hljs">camTargetPos = [<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>] <span class="hljs-comment"><span class="hljs-comment">#   ()  camDistance = 10 #     yaw = 0 #     pitch = 0 #     roll=0 #      upAxisIndex = 2 #    (z) fov = 60 #    nearPlane = 0.01 #      farPlane = 20 #      pixelWidth = 320 #   pixelHeight = 200 #   aspect = pixelWidth/pixelHeight; #    #   viewMatrix = pb.computeViewMatrixFromYawPitchRoll(camTargetPos, camDistance, yaw, pitch, roll, upAxisIndex) #   projectionMatrix = pb.computeProjectionMatrixFOV(fov, aspect, nearPlane, farPlane); #     img_arr = pb.getCameraImage(pixelWidth, pixelHeight, viewMatrix, projectionMatrix, shadow=0, lightDirection=[0,1,1],renderer=pb.ER_TINY_RENDERER) w=img_arr[0] #width of the image, in pixels h=img_arr[1] #height of the image, in pixels rgb=img_arr[2] #color data RGB dep=img_arr[3] #depth data</span></span></code> </pre> <br>  At the end of each experiment, a video is generated based on the collected images. <br><br><pre> <code class="python hljs">ani = animation.ArtistAnimation(plt.gcf(), render_imgs, interval=<span class="hljs-number"><span class="hljs-number">10</span></span>, blit=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>,repeat_delay=<span class="hljs-number"><span class="hljs-number">1000</span></span>) display(HTML(ani.to_html5_video()))</code> </pre><br><h2>  Agent </h2><br>  The GitHub <a href="https://github.com/jaara/AI-blog/blob/master/CartPole-basic.py">jaara</a> user code was taken as the basis for the Agent, as a simple and clear example of the implementation of learning with reinforcement for the Gym environment. <br><br>  The agent contains 2 objects: <i>Memory</i> - a repository for the formation of training examples and <i>Brain itself is</i> a neural network, which he trains. <br><br>  The trained neural network is created on TensorFlow using the Keras library, which has recently been fully <a href="https://www.tensorflow.org/guide/keras">incorporated</a> into TensorFlow. <br>  The neural network has a simple structure - 3 layers, i.e.  Only 1 hidden layer. <br><br>  The first layer contains 512 neurons and has a number of inputs equal to the number of parameters of the state of the environment (3 parameters: distance to the target, force and speed of the ball).  The hidden layer has a dimension equal to the first layer - 512 neurons, at the output it is connected to the output layer.  The number of neurons in the output layer corresponds to the number of actions performed by the Agent (2 actions: decrease and increase of the acting force). <br><br>  Thus, the state of the system is fed to the network input, and we have a benefit for each of the actions at the output. <br><br>  For the first two layers, the <i>ReLU</i> (rectified linear unit) is used as the activation functions, for the latter, the <i>linear function</i> (the sum of the input values ‚Äã‚Äãis simple). <br>  As a function of the error <i>MSE</i> (root-mean-square error), as an optimization algorithm, <i>RMSprop</i> (Root Mean Square Propagation). <br><br><pre> <code class="python hljs">model = Sequential() model.add(Dense(units=<span class="hljs-number"><span class="hljs-number">512</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, input_dim=<span class="hljs-number"><span class="hljs-number">3</span></span>)) model.add(Dense(units=<span class="hljs-number"><span class="hljs-number">512</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(Dense(units=<span class="hljs-number"><span class="hljs-number">2</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'linear'</span></span>)) opt = RMSprop(lr=<span class="hljs-number"><span class="hljs-number">0.00025</span></span>) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'mse'</span></span>, optimizer=opt)</code> </pre><br>  After each simulation step, the Agent saves the results of this step in the form of a list <i>(s, a, r, s_)</i> : <br>  <i>s</i> - the previous observation (the state of the environment) <br>  <i>a</i> - performed action <br>  <i>r</i> - reward received for the action performed <br>  <i>s_</i> - final observation after performing the action <br><br>  After that, the Agent gets a random collection of examples from previous periods from memory and forms a training packet ( <i>batch</i> ). <br><br>  The initial state <i>s</i> from random steps selected from the memory is taken as the input values ‚Äã‚Äã( <i>X</i> ) of the packet. <br><br>  The actual values ‚Äã‚Äãof the output for learning ( <i>Y '</i> ) are calculated as follows: At the output ( <i>Y</i> ) of the neural network, for s there will be values ‚Äã‚Äãof the <a href="https://ru.wikipedia.org/wiki/Q-%25D0%25BE%25D0%25B1%25D1%2583%25D1%2587%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">Q-function</a> for each of the actions <i>Q (s)</i> .  From this set, the agent chose the action with the highest value <i>Q (s, a) = MAX (Q (s))</i> , performed it and received the award <i>r</i> .  The new value of <i>Q</i> for the selected action <i>a</i> will be <i>Q (s, a) = Q (s, a) + DF * r</i> , where <i>DF</i> is the discounting factor.  The remaining values ‚Äã‚Äãof the outputs will remain the same. <br><br><pre> <code class="python hljs">STATE_CNT = <span class="hljs-number"><span class="hljs-number">3</span></span> ACTION_CNT = <span class="hljs-number"><span class="hljs-number">2</span></span> batchLen = <span class="hljs-number"><span class="hljs-number">32</span></span> <span class="hljs-comment"><span class="hljs-comment">#     states = numpy.array([ o[0] for o in batch ]) #     states_ = numpy.array([ o[3] for o in batch ]) #     p = agent.brain.predict(states) #     p_ = agent.brain.predict(states_) #     x = numpy.zeros((batchLen, STATE_CNT)) y = numpy.zeros((batchLen, ACTION_CNT)) #   for i in range(batchLen): o = batch[i] s = o[0]; a = o[1]; r = o[2]; s_ = o[3] t = p[i] #      #      ,       t[a] = r + GAMMA * numpy.amax(p_[i]) #            #    batch x[i] = s y[i] = t #      self.brain.train(x, y)</span></span></code> </pre> <br>  Formed by the pack is network training <br><br><pre> <code class="python hljs">self.model.fit(x, y, batch_size=<span class="hljs-number"><span class="hljs-number">32</span></span>, epochs=<span class="hljs-number"><span class="hljs-number">1</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre> <br>  After completing the experience, a video is generated. <br><br><img src="https://habrastorage.org/webt/om/ox/rk/omoxrkmnrigllf9hu_8x9ofbd7m.gif" alt="image"><br><br>  and statistics are displayed <br><br><img src="https://habrastorage.org/webt/0s/ed/p2/0sedp2zvwqmiiku6emmhp2yxf7m.png" alt="image"><br><br>  It took the agent 1,200 experiments to achieve about 95 percent of the result (the number of successful steps).  And by the 50th experience, the Agent had learned to move the ball towards the goal (unsuccessful experiments disappear). <br><br>  To improve the results, you can try to change the size of the network layers (LAYER_SIZE), the parameter of the discounting factor (GAMMA) or the rate of decrease in the probability of choosing a random action (LAMBDA). <br><br>  Our Agent has the simplest architecture - DQN (Deep Q-Network).  On such a simple task it is enough to get an acceptable result. <br><br>  Using, for example, the DDQN (Double DQN) architecture should provide smoother, more accurate learning.  And the RDQN network (Recurrent DQN) will be able to trace the patterns of the environment change over time, which will give the opportunity to get rid of the ball speed parameter, reducing the number of input network parameters. <br><br>  You can also expand our simulation by adding the ball's variable mass or the angle of its motion. <br><br>  But this is the next time. </div><p>Source: <a href="https://habr.com/ru/post/420897/">https://habr.com/ru/post/420897/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../420887/index.html">Tele2 hackathon report</a></li>
<li><a href="../420889/index.html">Military mine detection technology helps robots to navigate on any roads</a></li>
<li><a href="../420891/index.html">Migration to JUnit 5 in 10 minutes Test Time Measurement with Extensions</a></li>
<li><a href="../420893/index.html">Franchise Packing A to B</a></li>
<li><a href="../420895/index.html">How I revived the device (BH-USB-560v2 JTAG emulator) via U-Boot</a></li>
<li><a href="../420901/index.html">How I study the Spring framework (help for beginners is the work of the beginners themselves)</a></li>
<li><a href="../420903/index.html">ERP implementation: how not to fail</a></li>
<li><a href="../420905/index.html">How in Russia they introduce smart lighting and how long it will take</a></li>
<li><a href="../420907/index.html">From NOKLA to Xiaomi: the evolution of Chinese mobile phones</a></li>
<li><a href="../420909/index.html">Russian TV companies accused Yandex of piracy</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>