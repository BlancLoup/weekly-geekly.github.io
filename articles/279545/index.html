<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>MCMC-sampling for those who studied, but did not understand</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Talking about probabilistic programming and Bayesian statistics, I usually do not pay much attention to how, in fact, probabilistic inference is perfo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>MCMC-sampling for those who studied, but did not understand</h1><div class="post__text post__text-html js-mediator-article">  Talking about probabilistic programming and Bayesian statistics, I usually do not pay much attention to how, in fact, probabilistic inference is performed, considering it as a kind of ‚Äúblack box‚Äù.  The beauty of probabilistic programming is that, in fact, in order to build models, it is <i>not necessary</i> to understand exactly how the conclusion is drawn.  But this knowledge is certainly very useful. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/656/d2c/caa/656d2ccaa73a45008425471cd7846462.jpg"></div><br>  Once I talked about a new Bayesian model to a person who did not particularly understand the subject, but really wanted to understand everything.  He asked me something that I usually don‚Äôt touch.  ‚ÄúThomas,‚Äù he said, ‚Äúand how, in fact, is probabilistic inference performed?  How do these mysterious samples come from a posteriori probability? ‚Äù <br><a name="habracut"></a><br>  Then I could say: ‚ÄúEverything is very simple.  The MCMC generates samples from the a posteriori probability distribution, creating a reversible Markov chain, the equilibrium distribution of which is the target a posteriori distribution.  Questions? <br><br>  The explanation is correct, but how much benefit does it have for the uninitiated?  What annoys me most is how they teach mathematics and statistics that nobody ever talks about the intuitive ideas that underlie all kinds of concepts.  And these ideas are usually pretty simple.  From such occupations you can take out three-storey mathematical formulas, but not an understanding of how everything works.  I was taught that way, and I wanted to understand.  For countless hours I was hitting my head against the mathematical walls, before the moment of the next epiphany came, before the next ‚ÄúEureka!‚Äù Broke from my lips.  After I managed to figure out what was previously difficult, it turned out to be surprisingly simple.  What used to be scary with a jumble of mathematical signs became a useful tool. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      This material is an attempt to provide an intuitive explanation of MCMC sampling (in particular, <a href="https://ru.wikipedia.org/wiki/%25D0%2590%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC_%25D0%259C%25D0%25B5%25D1%2582%25D1%2580%25D0%25BE%25D0%25BF%25D0%25BE%25D0%25BB%25D0%25B8%25D1%2581%25D0%25B0_%25E2%2580%2594_%25D0%2593%25D0%25B0%25D1%2581%25D1%2582%25D0%25B8%25D0%25BD%25D0%25B3%25D1%2581%25D0%25B0">the Metropolis algorithm</a> ).  We will use code examples instead of formulas or mathematical calculations.  Ultimately, one cannot do without formulas, but personally I think that it is best to start with examples and achieve an intuitive understanding of the issue before moving on to the language of mathematics. <br><br><h2>  <font color="#c75733">The problem and its non-intuitive solution</font> </h2><br>  Let's look at the <a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B5%25D0%25BE%25D1%2580%25D0%25B5%25D0%25BC%25D0%25B0_%25D0%2591%25D0%25B0%25D0%25B9%25D0%25B5%25D1%2581%25D0%25B0">Bayes</a> formula: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/46e/26a/199/46e26a19913b4f2ca4614774f6045694.png"></div><br>  We have P (Œ∏ | x) - what we are interested in, namely, the a posteriori probability, or the probability distribution of the parameters of the model Œ∏, calculated after taking the x data taken from the observations into account.  In order to get what we need, we need to multiply the a priori probability P (Œ∏), that is, what we think about Œ∏ before conducting the experiments, before receiving the data, and the likelihood function P (x | Œ∏), that is - what we think about how data is distributed.  Numerator fraction is very easy to find. <br><br>  Now let's look at the denominator P (x), which is also called evidence, that is, evidence that the x data was generated by this model.  You can calculate it by integrating all possible values ‚Äã‚Äãof the parameters: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/28c/0e6/eda/28c0e6edafff4e3e8c152d761a8be92e.png"></div><br>  This is the main complexity of the Bayes formula.  It looks quite innocent itself, but even for a slightly non-trivial model, the posterior probability is not calculated in a finite number of steps. <br><br>  Here you can say: ‚ÄúWell, if it is not directly solved, can we take the path of approximation?  For example, if we could somehow get samples of the data from this very posterior probability, they could be approximated by the <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%259C%25D0%25BE%25D0%25BD%25D1%2582%25D0%25B5-%25D0%259A%25D0%25B0%25D1%2580%25D0%25BB%25D0%25BE">Monte Carlo</a> method. but also invert it, so this approach is even more difficult. <br><br>  Continuing the reflections, one can state: "Ok, then let's construct a Markov chain, the equilibrium distribution of which coincides with our a posteriori distribution."  I, of course, joke.  Most people do not say so, very much it sounds crazy.  If it is impossible to calculate directly, it is also impossible to take samples from the distribution, then building a Markov chain with the above-mentioned properties is a very difficult task. <br><br>  And here we are waiting for an amazing discovery.  This is actually very easy to do.  There is a whole class of algorithms for solving such problems: the Monte Carlo methods in Markov Chains ( <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Markov Chain Monte Carlo</a> , MCMC).  The essence of these algorithms is the creation of a Markov chain for performing Monte-Carlo approximation. <br><br><h2>  <font color="#c75733">Formulation of the problem</font> </h2><br>  First we import the necessary modules.  The code blocks marked as ‚ÄúListing‚Äù can be inserted into <a href="http://jupyter.org/">Jupyter Notebook</a> and, as you read the material, try it out.  See the full source <a href="https://github.com/twiecki/WhileMyMCMCGentlySamples/blob/master/content/downloads/notebooks/MCMC-sampling-for-dummies.ipynb">here</a> . <br><br><h3>  <font color="#c75733">‚ñç</font> Listing 1 </h3><br><pre><code class="hljs pgsql">%matplotlib <span class="hljs-keyword"><span class="hljs-keyword">inline</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> scipy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sp <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy.stats <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> norm sns.set_style(<span class="hljs-string"><span class="hljs-string">'white'</span></span>) sns.set_context(<span class="hljs-string"><span class="hljs-string">'talk'</span></span>) np.random.seed(<span class="hljs-number"><span class="hljs-number">123</span></span>)</code> </pre> <br>  Now we will generate the data.  These will be 100 points normally distributed around zero.  Our goal is to estimate the posterior distribution of the mean <i>mu</i> (assuming we know that the standard deviation is 1). <br><br><h3>  <font color="#c75733">‚ñç</font> Listing 2 </h3><br><pre> <code class="hljs haskell"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class"> = np.random.randn(20)</span></span></code> </pre> <br>  Now we will build a histogram. <br><br><h3>  <font color="#c75733">‚ñç</font> Listing 3 </h3><br><pre> <code class="hljs pgsql">ax = plt.subplot() sns.distplot(data, kde=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, ax=ax) _ = ax.<span class="hljs-keyword"><span class="hljs-keyword">set</span></span>(title=<span class="hljs-string"><span class="hljs-string">'Histogram of observed data'</span></span>, xlabel=<span class="hljs-string"><span class="hljs-string">'x'</span></span>, ylabel=<span class="hljs-string"><span class="hljs-string">'# observations'</span></span>);</code> </pre> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c8d/53a/d9f/c8d53ad9f392d74f592771312bbc9e25.png"></div><br>  <i><font color="#999999">Histogram of observable data</font></i> <br><br>  Now we need to describe the model.  We consider a simple case, so we assume that the data is distributed normally, that is, the likelihood function of the model is also normally distributed.  As you must know, the normal distribution has two parameters ‚Äî it is the mean (Œº) Œº and the standard deviation œÉ.  For simplicity, we assume that we know that œÉ = 1.  We want to derive the a posteriori probability for Œº.  For each desired parameter, it is necessary to select an a priori probability distribution.  For simplicity, assume that the normal distribution is the prior distribution for Œº.  Thus, in the language of statistics, the model will look like this: <br><br><pre> <code class="hljs 1c">Œº‚àºNormal(<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>) x<span class="hljs-string"><span class="hljs-string">|Œº‚àºNormal(x;Œº,1)</span></span></code> </pre> <br>  What is especially convenient is that for this model, in fact, it is possible to calculate the posterior probability distribution analytically.  The fact is that for a normal likelihood function with a known standard deviation, the normal a priori probability distribution for <i>mu</i> is a <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25BE%25D0%25BF%25D1%2580%25D1%258F%25D0%25B6%25D1%2591%25D0%25BD%25D0%25BD%25D0%25BE%25D0%25B5_%25D0%25B0%25D0%25BF%25D1%2580%25D0%25B8%25D0%25BE%25D1%2580%25D0%25BD%25D0%25BE%25D0%25B5_%25D1%2580%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">conjugate a</a> priori distribution.  That is, our a posteriori distribution will follow the same distribution as the a priori distribution.  In summary, we know that the posterior distribution for Œº is also normal.  In Wikipedia, it is easy to find a method for calculating the parameters for the a posteriori distribution.  A mathematical description of this can be found <a href="http://www.bcs.rochester.edu/people/robbie/jacobslab/cheat_sheet/bayes_normal_normal.pdf">here</a> . <br><br><h3>  <font color="#c75733">‚ñç</font> Listing 4 </h3><br><pre> <code class="hljs haskell"><span class="hljs-title"><span class="hljs-title">def</span></span> calc_posterior_analytical(<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class">, x, mu_0, sigma_0):   sigma = 1.   n = len(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">data</span></span></span><span class="hljs-class">)   mu_post = (</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">mu_0</span></span></span><span class="hljs-class"> / </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">sigma_0</span></span></span><span class="hljs-class">**2 + </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">data</span></span></span><span class="hljs-class">.</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">sum</span></span></span><span class="hljs-class">() / sigma**2) / (1. / </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">sigma_0</span></span></span><span class="hljs-class">**2 + </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">n</span></span></span><span class="hljs-class"> / </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">sigma</span></span></span><span class="hljs-class">**2)   sigma_post = (1. / </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">sigma_0</span></span></span><span class="hljs-class">**2 + </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">n</span></span></span><span class="hljs-class"> / </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">sigma</span></span></span><span class="hljs-class">**2)**-1   return norm(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">mu_post</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">np</span></span></span><span class="hljs-class">.</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">sqrt</span></span></span><span class="hljs-class">(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">sigma_post</span></span></span><span class="hljs-class">)).pdf(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">x</span></span></span><span class="hljs-class">) ax = plt.subplot() x = np.linspace(-1, 1, 500) posterior_analytical = calc_posterior_analytical(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">data</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">x</span></span></span><span class="hljs-class">, 0., 1.) ax.plot(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">x</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">posterior_analytical</span></span></span><span class="hljs-class">) ax.set(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">xlabel</span></span></span><span class="hljs-class">='</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">mu'</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ylabel</span></span></span><span class="hljs-class">='</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">belief'</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">title</span></span></span><span class="hljs-class">='</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">Analytical</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">posterior'</span></span></span><span class="hljs-class">); sns.despine()</span></span></code> </pre> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/902/f15/e53/902f15e538e20ddf84bfd95335f4dec2.png"></div><br>  <i><font color="#999999">Analytical posterior distribution found</font></i> <br><br>  Here we are interested in what we are interested in, that is, the probability distribution for Œº values ‚Äã‚Äãafter taking into account the available data, taking into account a priori information.  Suppose, however, that the prior distribution is not conjugate and we cannot solve the problem, so to speak, manually.  In practice, this is usually the case. <br><br><h2>  <font color="#c75733">Code, as a way to understand the essence of MCMC-sampling</font> </h2><br>  Now let's talk about sampling.  First, you need to find the initial position of the parameter (you can choose it randomly).  Let's set it arbitrarily like this: <br><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">mu_current</span></span> = <span class="hljs-number"><span class="hljs-number">1</span></span>.</code> </pre> <br>  Then it is proposed to move, "jump" from this position to some other place (this already applies to Markov's chains).  This new position can be chosen and "at random", and guided by some deep considerations.  The sampler in the Metropolis algorithm is as simple as five kopecks: it selects a sample from a normal distribution with a center in the current <i>mu</i> value ( <i>mu_current</i> variable) with a certain standard deviation ( <i>proposal_width</i> ) that determines the width of the range from which the proposed values ‚Äã‚Äãare selected.  Here we use <i>scipy.stats.norm</i> .  It must be said that the above normal distribution has nothing to do with what is used in the model. <br><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">proposal</span></span> = norm(mu_current, proposal_width).rvs()</code> </pre> <br>  The next step is to assess whether a suitable location has been selected for the transition.  If the obtained normal distribution with the proposed <i>mu</i> explains the data better than the previous <i>mu</i> , then in the chosen direction, it is certainly worth moving.  But what does ‚Äúdata explain better‚Äù?  We define this concept by calculating the probability of the data, taking into account the likelihood function (normal) with the proposed values ‚Äã‚Äãof the parameters (proposed by <i>mu</i> and <i>sigma</i> , the value of which is fixed and equal to 1).  The calculations here are simple.  It is enough to calculate the probability for each data point using the <i>scipy.stats.normal (mu, sigma) .pdf (data)</i> command and then multiply the individual probabilities, that is, to find the value of the likelihood function.  Usually in this case, use the logarithmic probability distribution, but here we omit it. <br><br><pre> <code class="hljs haskell"><span class="hljs-title"><span class="hljs-title">likelihood_current</span></span> = norm(mu_current, <span class="hljs-number"><span class="hljs-number">1</span></span>).pdf(<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class">).prod() likelihood_proposal = norm(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">mu_proposal</span></span></span><span class="hljs-class">, 1).pdf(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">data</span></span></span><span class="hljs-class">).prod() #         mu       prior_current = norm(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">mu_prior_mu</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">mu_prior_sd</span></span></span><span class="hljs-class">).pdf(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">mu_current</span></span></span><span class="hljs-class">) prior_proposal = norm(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">mu_prior_mu</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">mu_prior_sd</span></span></span><span class="hljs-class">).pdf(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">mu_proposal</span></span></span><span class="hljs-class">) #    p_current = likelihood_current * prior_current p_proposal = likelihood_proposal * prior_proposal</span></span></code> </pre> <br>  Up to this point, we, in fact, used the ascent to the top algorithm, which only suggests moving in random directions and adopts a new position only if <i>mu_proposal</i> has a higher likelihood level than <i>mu_cu</i> rrent.  In the end, we come to the variant mu = 0 (or to a value close to this), from where there will be nowhere to move.  However, we want to get a posterior distribution, so we must sometimes agree to move in another direction.  The main secret here is that by dividing <i>p_proposal</i> by <i>p_current</i> , we get the probability of accepting an offer. <br><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">p_accept</span></span> = p_proposal / p_current</code> </pre> <br>  It can be noted that if <i>p_proposal is</i> greater than <i>p_current</i> , the resulting probability will be greater than 1, and we definitely will accept such a proposal.  However, if <i>p_current is</i> more than <i>p_proposal</i> , say, twice, the transition chance will be 50% already: <br><br><pre> <code class="hljs perl"><span class="hljs-keyword"><span class="hljs-keyword">accept</span></span> = np.random.rand() &lt; p_accept <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">accept</span></span>:   <span class="hljs-comment"><span class="hljs-comment">#     cur_pos = proposal</span></span></code> </pre> <br>  This simple procedure gives us samples from the posterior distribution. <br><br><h2>  <font color="#c75733">Why is this all about?</font> </h2><br>  Let us look back and note that the introduction into the system of the probability of accepting the proposed transition position is the reason why all this works and allows us to avoid integration.  This is clearly seen when calculating the level of acceptance of a sentence for a normalized a posteriori distribution and observe how it relates to the level of acceptance of a sentence for a non-normalized posterior distribution (say, Œº0 is the current position, and Œº is the proposed new position): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/a5b/a71/a56/a5ba71a56e1041bea7c0c16333e45e19.png"></div><br>  If this is explained, then, by dividing the a posteriori probability that turned out for the proposed parameter to the one calculated for the current parameter, we get rid of the already annoying P (x), which we are not able to calculate.  It can be intuitively understood that we divide the full posterior probability distribution in one position by the same in another position (quite a normal operation).  Thus, we visit areas with a higher level of a posteriori probability distribution <i>relatively</i> more often than areas with a low level. <br><br><h2>  <font color="#c75733">Fold the full picture</font> </h2><br>  Putting the above together. <br><br><h3>  <font color="#c75733">‚ñç</font> Listing 5 </h3><br><pre> <code class="hljs mel">def sampler(data, samples=<span class="hljs-number"><span class="hljs-number">4</span></span>, mu_init=<span class="hljs-number"><span class="hljs-number">.5</span></span>, proposal_width=<span class="hljs-number"><span class="hljs-number">.5</span></span>, plot=False, mu_prior_mu=<span class="hljs-number"><span class="hljs-number">0</span></span>, mu_prior_sd=<span class="hljs-number"><span class="hljs-number">1.</span></span>):   mu_current = mu_init   posterior = [mu_current]   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(samples):       #          mu_proposal = norm(mu_current, proposal_width).rvs()       #  ,             likelihood_current = norm(mu_current, <span class="hljs-number"><span class="hljs-number">1</span></span>).pdf(data).prod()       likelihood_proposal = norm(mu_proposal, <span class="hljs-number"><span class="hljs-number">1</span></span>).pdf(data).prod()             #         mu              prior_current = norm(mu_prior_mu, mu_prior_sd).pdf(mu_current)       prior_proposal = norm(mu_prior_mu, mu_prior_sd).pdf(mu_proposal)             p_current = likelihood_current * prior_current       p_proposal = likelihood_proposal * prior_proposal             #   ?       p_accept = p_proposal / p_current             #        ,            accept = np.random.<span class="hljs-keyword"><span class="hljs-keyword">rand</span></span>() &lt; p_accept             <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> plot:           plot_proposal(mu_current, mu_proposal, mu_prior_mu, mu_prior_sd, data, accept, posterior, i)             <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> accept:           #             mu_current = mu_proposal             posterior.append(mu_current)         <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> posterior #   def plot_proposal(mu_current, mu_proposal, mu_prior_mu, mu_prior_sd, data, accepted, <span class="hljs-keyword"><span class="hljs-keyword">trace</span></span>, i):   from copy import copy   <span class="hljs-keyword"><span class="hljs-keyword">trace</span></span> = copy(<span class="hljs-keyword"><span class="hljs-keyword">trace</span></span>)   fig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=<span class="hljs-number"><span class="hljs-number">4</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">16</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>))   fig.suptitle(<span class="hljs-string"><span class="hljs-string">'Iteration %i'</span></span> % (i + <span class="hljs-number"><span class="hljs-number">1</span></span>))   x = np.linspace(<span class="hljs-number"><span class="hljs-number">-3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">5000</span></span>)   <span class="hljs-keyword"><span class="hljs-keyword">color</span></span> = <span class="hljs-string"><span class="hljs-string">'g'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> accepted <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">'r'</span></span>         #       prior_current = norm(mu_prior_mu, mu_prior_sd).pdf(mu_current)   prior_proposal = norm(mu_prior_mu, mu_prior_sd).pdf(mu_proposal)   prior = norm(mu_prior_mu, mu_prior_sd).pdf(x)   ax1.plot(x, prior)   ax1.plot([mu_current] * <span class="hljs-number"><span class="hljs-number">2</span></span>, [<span class="hljs-number"><span class="hljs-number">0</span></span>, prior_current], <span class="hljs-keyword"><span class="hljs-keyword">marker</span></span>=<span class="hljs-string"><span class="hljs-string">'o'</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">color</span></span>=<span class="hljs-string"><span class="hljs-string">'b'</span></span>)   ax1.plot([mu_proposal] * <span class="hljs-number"><span class="hljs-number">2</span></span>, [<span class="hljs-number"><span class="hljs-number">0</span></span>, prior_proposal], <span class="hljs-keyword"><span class="hljs-keyword">marker</span></span>=<span class="hljs-string"><span class="hljs-string">'o'</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">color</span></span>=<span class="hljs-keyword"><span class="hljs-keyword">color</span></span>)   ax1.<span class="hljs-keyword"><span class="hljs-keyword">annotate</span></span>(<span class="hljs-string"><span class="hljs-string">""</span></span>, xy=(mu_proposal, <span class="hljs-number"><span class="hljs-number">0.2</span></span>), xytext=(mu_current, <span class="hljs-number"><span class="hljs-number">0.2</span></span>),                arrowprops=dict(arrowstyle=<span class="hljs-string"><span class="hljs-string">"-&gt;"</span></span>, lw=<span class="hljs-number"><span class="hljs-number">2.</span></span>))   ax1.set(ylabel=<span class="hljs-string"><span class="hljs-string">'Probability Density'</span></span>, title=<span class="hljs-string"><span class="hljs-string">'current: prior(mu=%.2f) = %.2f\nproposal: prior(mu=%.2f) = %.2f'</span></span> % (mu_current, prior_current, mu_proposal, prior_proposal))     #    likelihood_current = norm(mu_current, <span class="hljs-number"><span class="hljs-number">1</span></span>).pdf(data).prod()   likelihood_proposal = norm(mu_proposal, <span class="hljs-number"><span class="hljs-number">1</span></span>).pdf(data).prod()   y = norm(loc=mu_proposal, <span class="hljs-keyword"><span class="hljs-keyword">scale</span></span>=<span class="hljs-number"><span class="hljs-number">1</span></span>).pdf(x)   sns.distplot(data, kde=False, norm_hist=True, ax=ax2)   ax2.plot(x, y, <span class="hljs-keyword"><span class="hljs-keyword">color</span></span>=<span class="hljs-keyword"><span class="hljs-keyword">color</span></span>)   ax2.axvline(mu_current, <span class="hljs-keyword"><span class="hljs-keyword">color</span></span>=<span class="hljs-string"><span class="hljs-string">'b'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">'--'</span></span>, label=<span class="hljs-string"><span class="hljs-string">'mu_current'</span></span>)   ax2.axvline(mu_proposal, <span class="hljs-keyword"><span class="hljs-keyword">color</span></span>=<span class="hljs-keyword"><span class="hljs-keyword">color</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">'--'</span></span>, label=<span class="hljs-string"><span class="hljs-string">'mu_proposal'</span></span>)   #ax2.title(<span class="hljs-string"><span class="hljs-string">'Proposal {}'</span></span>.<span class="hljs-keyword"><span class="hljs-keyword">format</span></span>(<span class="hljs-string"><span class="hljs-string">'accepted'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> accepted <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">'rejected'</span></span>))   ax2.<span class="hljs-keyword"><span class="hljs-keyword">annotate</span></span>(<span class="hljs-string"><span class="hljs-string">""</span></span>, xy=(mu_proposal, <span class="hljs-number"><span class="hljs-number">0.2</span></span>), xytext=(mu_current, <span class="hljs-number"><span class="hljs-number">0.2</span></span>),                arrowprops=dict(arrowstyle=<span class="hljs-string"><span class="hljs-string">"-&gt;"</span></span>, lw=<span class="hljs-number"><span class="hljs-number">2.</span></span>))   ax2.set(title=<span class="hljs-string"><span class="hljs-string">'likelihood(mu=%.2f) = %.2f\nlikelihood(mu=%.2f) = %.2f'</span></span> % (mu_current, <span class="hljs-number"><span class="hljs-number">1e14</span></span>*likelihood_current, mu_proposal, <span class="hljs-number"><span class="hljs-number">1e14</span></span>*likelihood_proposal))     #      posterior_analytical = calc_posterior_analytical(data, x, mu_prior_mu, mu_prior_sd)   ax3.plot(x, posterior_analytical)   posterior_current = calc_posterior_analytical(data, mu_current, mu_prior_mu, mu_prior_sd)   posterior_proposal = calc_posterior_analytical(data, mu_proposal, mu_prior_mu, mu_prior_sd)   ax3.plot([mu_current] * <span class="hljs-number"><span class="hljs-number">2</span></span>, [<span class="hljs-number"><span class="hljs-number">0</span></span>, posterior_current], <span class="hljs-keyword"><span class="hljs-keyword">marker</span></span>=<span class="hljs-string"><span class="hljs-string">'o'</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">color</span></span>=<span class="hljs-string"><span class="hljs-string">'b'</span></span>)   ax3.plot([mu_proposal] * <span class="hljs-number"><span class="hljs-number">2</span></span>, [<span class="hljs-number"><span class="hljs-number">0</span></span>, posterior_proposal], <span class="hljs-keyword"><span class="hljs-keyword">marker</span></span>=<span class="hljs-string"><span class="hljs-string">'o'</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">color</span></span>=<span class="hljs-keyword"><span class="hljs-keyword">color</span></span>)   ax3.<span class="hljs-keyword"><span class="hljs-keyword">annotate</span></span>(<span class="hljs-string"><span class="hljs-string">""</span></span>, xy=(mu_proposal, <span class="hljs-number"><span class="hljs-number">0.2</span></span>), xytext=(mu_current, <span class="hljs-number"><span class="hljs-number">0.2</span></span>),                arrowprops=dict(arrowstyle=<span class="hljs-string"><span class="hljs-string">"-&gt;"</span></span>, lw=<span class="hljs-number"><span class="hljs-number">2.</span></span>))   #x3.set(title=r<span class="hljs-string"><span class="hljs-string">'prior x likelihood $\propto$ posterior'</span></span>)   ax3.set(title=<span class="hljs-string"><span class="hljs-string">'posterior(mu=%.2f) = %.5f\nposterior(mu=%.2f) = %.5f'</span></span> % (mu_current, posterior_current, mu_proposal, posterior_proposal))     <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> accepted:       <span class="hljs-keyword"><span class="hljs-keyword">trace</span></span>.append(mu_proposal)   <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:       <span class="hljs-keyword"><span class="hljs-keyword">trace</span></span>.append(mu_current)   ax4.plot(<span class="hljs-keyword"><span class="hljs-keyword">trace</span></span>)   ax4.set(xlabel=<span class="hljs-string"><span class="hljs-string">'iteration'</span></span>, ylabel=<span class="hljs-string"><span class="hljs-string">'mu'</span></span>, title=<span class="hljs-string"><span class="hljs-string">'trace'</span></span>)   plt.tight_layout()   #plt.legend()</code> </pre> <br><h2>  <font color="#c75733">MCMC Visualization</font> </h2><br>  In order to visualize the sampling process, we draw graphs of some calculated values.  Each row of images represents a single pass of the Metropolis sampler. <br><br>  The first column shows the prior probability distribution, that is, our assumptions about Œº before getting acquainted with the data.  As you can see, this distribution does not change, we just show here the proposals of the new value Œº.  Vertical blue lines are current Œº.  And the proposed Œº is displayed in either green or red (these are, respectively, accepted and rejected sentences). <br><br>  In the second column, the likelihood function and what we use to evaluate how well the model explains the data.  Here you can see that the graph changes in accordance with the proposed Œº.  A blue histogram is also displayed there - the data itself.  The solid line is displayed either in green or in red - this is the graph of the likelihood function with <i>mu</i> proposed in the current step.  It is intuitively clear that the stronger the likelihood function overlaps with a graphical display of data, the better the model explains the data and the higher the resulting probability.  The dotted line of the same color is the proposed <i>mu</i> , the dotted blue line is the current <i>mu</i> . <br><br>  The third column is the a posteriori probability distribution.  The normalized a posteriori distribution is shown here, but, as we explained above, you can simply multiply the previous value for the current and proposed Œº by the value of the likelihood function for two Œº in order to get the non-normalized values ‚Äã‚Äãof the posterior distribution (which we use for real calculations), and divide one by the other in order to get the probability of accepting the proposed value. <br><br>  The fourth column presents the trace of the sample, that is, the values ‚Äã‚Äãof Œº, the samples obtained on the basis of the model.  Here we show each sample, regardless of whether it was accepted or rejected (in this case, the line does not change). <br><br>  Pay attention that we always move to relatively more likely values ‚Äã‚Äãof Œº (in terms of the density of their posterior probability distribution) and only sometimes to relatively less likely values ‚Äã‚Äãof Œº.  For example, as in iteration # 1 (numbers of iterations can be seen in the upper part of each row of images in the center. <br><br><h3>  <font color="#c75733">‚ñç</font> Listing 6 </h3><br><pre> <code class="hljs haskell"><span class="hljs-title"><span class="hljs-title">np</span></span>.random.seed(<span class="hljs-number"><span class="hljs-number">123</span></span>) sampler(<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class">, samples=8, mu_init=-1., plot=</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">True</span></span></span><span class="hljs-class">);</span></span></code> </pre> <br><br><div style="text-align:center;"><img src="https://lh4.googleusercontent.com/2pF0waFRNxcllrYWqC50bt-KRR2SWOfBIsTn0u_KuHAPAJpwr37CqYSPWNhSKL92_2d8hN-U6m49P3KJuGARzcx07gB3Tc6dFqo7DAR1M4ddbmHUC1sywPqR9qbSiOi3zH-2ixp-lJKw_Aec9Q"></div><br><div style="text-align:center;"><img src="https://lh6.googleusercontent.com/1qUx-WiEYxbmA-sjdrX7vO47V9_BLfz32P941vzO_FzSLhmhm9ylFYbhJKK7R-Daj_1jGktPlSw1AiV6Wvgwljr7HvDZbfPhZC9OBfSzoIx5VPnaFrEcGVTqWb4Tt-ZYbsknNi1jiQBgt6RtXg"></div><br><div style="text-align:center;"><img src="https://lh6.googleusercontent.com/xzwC-3XKn1EAyihGpsUOBq1Pa4oGkFyoXoU0qqlFt18EuNsnaOBNDMV1YBhnq8TCoWj2Um-QQygrDAQ0dl4Bb279JL44qAKKantEHgHmhuwiuhR744zzdaPkp-lXfa7DEAskv34H95YzJM5MDw"></div><br><div style="text-align:center;"><img src="https://lh6.googleusercontent.com/hWvWec9bU77moctr-FHKkuFlqyIDx10J1vjHSYC0MF1RsPPKB0YnAGg4jaqPKDSAfXDbObQlfxTvooiQmmJC6iGXq2Bt-r6A7juO21e5gN588e8afqKOBEGnJxy2FvCk9rUvPMtLKYiHpxAZIQ"></div><br><div style="text-align:center;"><img src="https://lh3.googleusercontent.com/1vmrUaa_sne6Es-80aqOx0BS1Nsnu3FVpjUTUR1ehQCc0ub7F1i3mtaj6imiH9Z9DUJKXxb5FRshWKfnCkUoAUERER8Hq_tm4EsUtwvDZYxfV8J_jR3s_zc0AHKVQkln-KftBcWxhPbedN19TA"></div><br><div style="text-align:center;"><img src="https://lh6.googleusercontent.com/LgSS-JBVj6jKg7l28c1OpKcyO2T8tUWW6ewCwUVKZeOUCfeqkPRbSemMI15GdWzHgJo_AX2MvC_rCZn3KJgX0GwxSgsEhp6QiVl8gtP8XcbNe31fg1vO3xOCM8GywEC-dCzvgNm7A9tzg1YojA"></div><br><div style="text-align:center;"><img src="https://lh4.googleusercontent.com/nwWlocLpW06PMtrotiMpbsQ_wHDp0pxU7cpI9LxL4_G8vP7vyyWptg19FiIRP1LA7K2D1V46pzT09WlnFP-bykGc8hD_2oSihfXHIs6fajIeJfDjzUkWQWoeyiKPNm_L_SijfuMJECE0gD7YCQ"></div><br><div style="text-align:center;"><img src="https://lh6.googleusercontent.com/o2fCH4N-fOkz4_PskRRwStBy-PS88ix2le9Wmu51VnZ1Ohwgc72ZQcveA5p9xA5ok0erAhOzWvCRAmBP7A5-qELlAWgso6xryhcTR3wy3pAcYXIsknXWl5CwyWrHj-VJD5G4f4ysbEZrImTV6A"></div><br>  <i><font color="#999999">MCMC Visualization</font></i> <br><br>  The remarkable feature of MCMC is that in order to get the desired result, the process described above just needs to be repeated for a long time.  Samples generated in this way are taken from the a posteriori probability distribution of the model under study.  There is a rigorous mathematical proof that ensures that this is exactly what will happen, but here I don‚Äôt want to go into such details. <br><br>  In order to get an idea of ‚Äã‚Äãwhat kind of data the system produces, let's generate a large number of samples and present them graphically. <br><br><h3>  <font color="#c75733">‚ñç</font> Listing 7 </h3><br><pre> <code class="hljs haskell"><span class="hljs-title"><span class="hljs-title">posterior</span></span> = sampler(<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class">, samples=15000, mu_init=1.) fig, ax = plt.subplots() ax.plot(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">posterior</span></span></span><span class="hljs-class">) _ = ax.set(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">xlabel</span></span></span><span class="hljs-class">='</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">sample'</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ylabel</span></span></span><span class="hljs-class">='</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">mu'</span></span></span><span class="hljs-class">);</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/88b/42a/cf1/88b42acf1e99d09342bce1dce393effa.png"></div><br>  <i><font color="#999999">Visualization of 15,000 samples</font></i> <br><br>  This is commonly referred to as a sample trace.  Now, in order to obtain an approximate value of the posterior probability distribution (strictly speaking, this is what we need), it is sufficient to construct a histogram of this data.  It is important to remember that although the data obtained is similar to the ones we were sampling to fit the model, these are two different data sets. <br><br>  The graph below reflects our <b>understanding</b> of what <i>mu</i> should be.  In this case, it turned out that he also demonstrates a normal distribution, but for another model he could have a completely different form than the graph of the likelihood function or a priori probability distribution. <br><br><h3>  <font color="#c75733">‚ñç</font> Listing 8 </h3><br><pre> <code class="hljs matlab">ax = plt.subplot() sns.distplot(posterior[<span class="hljs-number"><span class="hljs-number">500</span></span>:], ax=ax, label=<span class="hljs-string"><span class="hljs-string">'estimated posterior'</span></span>) x = np.<span class="hljs-built_in"><span class="hljs-built_in">linspace</span></span>(<span class="hljs-number"><span class="hljs-number">-.5</span></span>, <span class="hljs-number"><span class="hljs-number">.5</span></span>, <span class="hljs-number"><span class="hljs-number">500</span></span>) post = calc_posterior_analytical(data, x, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) ax.<span class="hljs-built_in"><span class="hljs-built_in">plot</span></span>(x, post, <span class="hljs-string"><span class="hljs-string">'g'</span></span>, label=<span class="hljs-string"><span class="hljs-string">'analytic posterior'</span></span>) _ = ax.set(xlabel=<span class="hljs-string"><span class="hljs-string">'mu'</span></span>, ylabel=<span class="hljs-string"><span class="hljs-string">'belief'</span></span>); ax.<span class="hljs-built_in"><span class="hljs-built_in">legend</span></span>();</code> </pre> <br><div style="text-align:center;"><img src="https://lh5.googleusercontent.com/kGycN1cYLUYnYHcA9firo5eM79NlE3dj0urL8CS0PU5cjaDJ_gy9GqXmTeUPuoLYVl9JJcfqQPh_UvEcr0CmlWj-Nxzs4t5z2D1lfAJToGB06-EL_5Ou1eYH8ujiZe37vZ0JtVlVm_nmoWHyHg"></div><br>  <i><font color="#999999">Analytical and estimated a posteriori probability distributions</font></i> <br><br>  As you can see, following the above method, we obtained samples from the same probability distribution that was obtained analytically. <br><br><h2>  <font color="#c75733">The width of the range for sampling the proposed values</font> </h2><br>  Above, we set the interval from which the proposed values ‚Äã‚Äãare selected to be 0.5.  It is this value that turned out to be very successful.  In general, the width of the range should not be too small, otherwise the sampling will be ineffective, since it will take too much time to study the parameter space and the model will show typical behavior for random walk. <br><br>  Here, for example, what we got by setting the <i>proposal_width</i> parameter to 0.01. <br><br><h3>  <font color="#c75733">‚ñç</font> Listing 9 </h3><br><pre> <code class="hljs haskell"><span class="hljs-title"><span class="hljs-title">posterior_small</span></span> = sampler(<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class">, samples=5000, mu_init=1., proposal_width=.01) fig, ax = plt.subplots() ax.plot(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">posterior_small</span></span></span><span class="hljs-class">); _ = ax.set(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">xlabel</span></span></span><span class="hljs-class">='</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">sample'</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ylabel</span></span></span><span class="hljs-class">='</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">mu'</span></span></span><span class="hljs-class">);</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://lh4.googleusercontent.com/au5nlMdj9_blDFnKnDjidA54HeU98U7VheRIN2A2FkAL_kat9f8bBzjC68-OVpPgzYIdPklzcyYAyk4Jxc9umhcUGp50rjcX3YAiFIiAfDZIcQaADHxfbTVpX7rxCPCwnP1apZEKCDRsRX3cXA"></div><br>  <i><font color="#999999">Result of using too small a range.</font></i> <br><br>  Too long a spacing will not work either - the values ‚Äã‚Äãproposed for the transition will be constantly rejected.  This is what happens if you set the <i>proposal_width</i> to 3. <br><br><h3>  <font color="#c75733">‚ñç</font> Listing 10 </h3><br><pre> <code class="hljs haskell"><span class="hljs-title"><span class="hljs-title">posterior_large</span></span> = sampler(<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class">, samples=5000, mu_init=1., proposal_width=3.) fig, ax = plt.subplots() ax.plot(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">posterior_large</span></span></span><span class="hljs-class">); plt.xlabel('</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">sample'</span></span></span><span class="hljs-class">); plt.ylabel('</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">mu'</span></span></span><span class="hljs-class">); _ = ax.set(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">xlabel</span></span></span><span class="hljs-class">='</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">sample'</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ylabel</span></span></span><span class="hljs-class">='</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">mu'</span></span></span><span class="hljs-class">);</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9aa/b60/76a/9aab6076a9bda9beb453e03845325637.png"></div><br>  <i><font color="#999999">Result of using too much range</font></i> <br><br>  However, pay attention to the fact that we here continue to take samples from the target a posteriori distribution, as guaranteed by mathematical proof.  But this approach is less effective. <br><br><h3>  <font color="#c75733">‚ñç</font> Listing 11 </h3><br><pre> <code class="hljs vhdl">sns.distplot(posterior_small[<span class="hljs-number"><span class="hljs-number">1000</span></span>:], <span class="hljs-keyword"><span class="hljs-keyword">label</span></span>=<span class="hljs-symbol"><span class="hljs-symbol">'Small</span></span> step size') sns.distplot(posterior_large[<span class="hljs-number"><span class="hljs-number">1000</span></span>:], <span class="hljs-keyword"><span class="hljs-keyword">label</span></span>=<span class="hljs-symbol"><span class="hljs-symbol">'Large</span></span> step size'); _ = plt.legend();</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/52f/52e/fd1/52f52efd1169d2778637234b4d47c3c1.png"></div><br>  <i><font color="#999999">Small and large stride size</font></i> <br><br>  Using a large number of iterations, what we have, in the end, will look like a real a posteriori distribution.  The main thing is that we need the samples to be independent of each other.  In this case, this is obviously not the case.  In order to evaluate the effectiveness of our sampler, you can use the autocorrelation index.  That is, find out how sample <i>i</i> correlates with sample <i>i-1</i> , <i>i-2</i> , and so on. <br><br><h3>  <font color="#c75733">‚ñç</font> Listing 12 </h3><br><pre> <code class="hljs delphi">from pymc3.stats import autocorr lags = np.arange(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) fig, ax = plt.subplots() ax.plot(lags, [autocorr(posterior_large, l) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> l <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> lags], <span class="hljs-keyword"><span class="hljs-keyword">label</span></span>=<span class="hljs-string"><span class="hljs-string">'large step size'</span></span>) ax.plot(lags, [autocorr(posterior_small, l) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> l <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> lags], <span class="hljs-keyword"><span class="hljs-keyword">label</span></span>=<span class="hljs-string"><span class="hljs-string">'small step size'</span></span>) ax.plot(lags, [autocorr(posterior, l) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> l <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> lags], <span class="hljs-keyword"><span class="hljs-keyword">label</span></span>=<span class="hljs-string"><span class="hljs-string">'medium step size'</span></span>) ax.legend(loc=<span class="hljs-number"><span class="hljs-number">0</span></span>) _ = ax.<span class="hljs-keyword"><span class="hljs-keyword">set</span></span>(xlabel=<span class="hljs-string"><span class="hljs-string">'lag'</span></span>, ylabel=<span class="hljs-string"><span class="hljs-string">'autocorrelation'</span></span>, ylim=(-.<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>))</code> </pre> <br><div style="text-align:center;"><img src="https://lh4.googleusercontent.com/gYtU-Bni5W2Sr0tDLOwQoCWCiLUyKntqCnvGWgAbZIni0GoRyM3yv1eh_TvdekpWIckxjImvBZ8xPm-RLApF5Qg8MP8Hb15NKqTkiFRtY07jfWMiymWXemiE6IyE2Ir1zwoTu3eIKTT4-SFSBw"></div><br>  <i><font color="#999999">Autocorrelation for small, medium and large step sizes</font></i> <br><br>  Obviously, I would like to have an intelligent way to automatically determine the appropriate step size.  One of the most commonly used methods is to adjust the interval from which values ‚Äã‚Äãare selected so that approximately 50% of the sentences are discarded.  Interestingly, other MCMC algorithms, like the Monte-Carlo Hybrid Method (Hamiltonian Monte Carlo) are very similar to the one we talked about.  Their main feature is that they behave "smarter" when they put forward proposals for the next "jump." <br><br><h2>  <font color="#c75733">Go to the more complex models.</font> </h2><br>  It is now easy to imagine that we could add the <i>sigma</i> parameter for the standard deviation and follow the same procedure for the second parameter.  In this case, the sentences for <i>mu</i> and <i>sigma</i> would be generated, but the logic of the algorithm would hardly change.  Or we could take data from very different probability distributions, such as the binomial one, and, continuing to use the same algorithm, we would get the correct a posteriori probability distribution.  It‚Äôs just great, it‚Äôs one of the huge benefits of probabilistic programming: it‚Äôs enough to determine the desired model and allow MCMC to do Bayesian output. <br><br>  For example, the model below can be very easily described using PyMC3.  Also, in this example, we use the Metropolis sampler (with an automatically adjustable width of the range from which the proposed values ‚Äã‚Äãare taken) and see that the results are almost identical.  You can experiment with this, change the distribution, for example.  In the <a href="http://pymc-devs.github.io/pymc3/getting_started/">PyMC3 documentation</a> you can find more complex examples and information on how this all works. <br><br><h3>  <font color="#c75733">‚ñç</font> Listing 13 </h3><br><pre> <code class="hljs markdown">import pymc3 as pm with pm.Model():   mu = pm.Normal('mu', 0, 1)   sigma = 1.   returns = pm.Normal('returns', mu=mu, sd=sigma, observed=data)     step = pm.Metropolis()   trace = pm.sample(15000, step)  sns.distplot(trace[<span class="hljs-string"><span class="hljs-string">2000:</span></span>][<span class="hljs-symbol"><span class="hljs-symbol">'mu'</span></span>], label='PyMC3 sampler'); sns.distplot(posterior[500:], label='Hand-written sampler'); plt.legend();</code> </pre> <br><div style="text-align:center;"><img src="https://lh4.googleusercontent.com/EVPdo-IdTybdVPDZV-t5_akIahGmIUTx8dTZU0KZXCt_6v2GQkfgTxuVEGrltgeTwo3Oahfn8FS3C8A1qcAYvh5yXiLvC3NZcn0RQ0mljJSgMStwYCUSgNAKQxmwTiBRzhAkfwYRTavaW2OzpQ"></div><br>  <i><font color="#999999">The results of the various samplers</font></i> <br><br><h2>  <font color="#c75733">Let's sum up</font> </h2><br>  Hopefully, we were able to convey to you the essence of MCMC, the Metropolis sampler, the probabilistic conclusion, and now you have an intuitive understanding of all this.  So, you are ready to read the materials on this issue, saturated with mathematical formulas.       ,       ,       . <br><br><blockquote><div class="spoiler"> <b class="spoiler_title">,     ?</b>  <b class="spoiler_title">:)</b> <div class="spoiler_text"> <a href="http://wunderfund.io/"><b>wunderfund.io</b></a> ‚Äî  ,   <a href="https://en.wikipedia.org/wiki/High-frequency_trading"> </a> .   ‚Äî         .   ,      . <br><br>           low latency      .     ,       . <br><br>    : <a href="http://wunderfund.io/">wunderfund.io</a> </div></div></blockquote></div><p>Source: <a href="https://habr.com/ru/post/279545/">https://habr.com/ru/post/279545/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../279535/index.html">Testing. Fundamental theory</a></li>
<li><a href="../279537/index.html">Creating a nanoCAD distribution with custom settings</a></li>
<li><a href="../279539/index.html">Popular WordPress plugin contains backdoor</a></li>
<li><a href="../279541/index.html">Ubiquiti LiteBeam AC‚Äì we build radio networks in a new and budgetary way</a></li>
<li><a href="../279543/index.html">Webinar 4: Docker Container Management System</a></li>
<li><a href="../279547/index.html">How cloud technologies affect data centers</a></li>
<li><a href="../279549/index.html">We invite you to a series of lectures "Creating online games: game design, monetization, operating and promotion"</a></li>
<li><a href="../279551/index.html">ABBYY: environmental friendliness ++. Four years later</a></li>
<li><a href="../279553/index.html">Is there life without google play? Alternatives and Application Updates</a></li>
<li><a href="../279555/index.html">Shared on shared-hosting: pain and suffering or a simple routine?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>