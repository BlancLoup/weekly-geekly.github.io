<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Kubernetes Network Performance Comparison</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Kubernetes requires that each container in a cluster has a unique, routable IP. Kubernetes does not assign IP addresses by itself, leaving this task t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Kubernetes Network Performance Comparison</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/web/396/e84/39b/396e8439b2a64ab58f17714b679e1f69.png"><br><br>  Kubernetes requires that each container in a cluster has a unique, routable IP.  Kubernetes does not assign IP addresses by itself, leaving this task to third-party solutions. <br><br>  The purpose of this study is to find a solution with the lowest latency, the highest throughput, and the smallest tuning cost.  Since our load depends on delays, we measure delays of high <a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25B2%25D0%25B0%25D0%25BD%25D1%2582%25D0%25B8%25D0%25BB%25D1%258C">percentiles</a> with a fairly active network load.  In particular, we focused on performance in the region of 30-50 percent of the maximum load, since this best reflects typical situations for non-overloaded systems. <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  Options </h2><br>
<h3>  Docker with <code>--net=host</code> </h3><br>  Our exemplary installation.  All other options were compared with her. <br><br>  The option <a href="https://docs.docker.com/engine/reference/run/"><code>--net=host</code></a> means that containers inherit the IP addresses of their host machines, i.e.  there is no network containerization. <br><br>  Lack of network containerization a priori provides better performance than the presence of any implementation - for this reason we used this installation as a reference. <br><br><h3>  Flannel </h3><br>  <a href="https://github.com/coreos/flannel">Flannel</a> is a virtual network solution supported by the <a href="https://coreos.com/">CoreOS</a> project.  Well tested and ready for production, so the cost of implementation is minimal. <br><br>  When you add a flannel machine to a cluster, the flannel does three things: <br><br><ol><li>  Assigns a subnet to a new machine using <a href="https://github.com/coreos/etcd">etcd</a> . </li><li>  Creates a virtual <a href="http://www.linuxfoundation.org/collaborate/workgroups/networking/bridge">bridge interface</a> on the machine ( <code>docker0 bridge</code> ). </li><li>  Configures the packet forwarding <a href="https://github.com/coreos/flannel">backend</a> : <br><br><ul><li>  <code>aws-vpc</code> - registers a machine's subnet in the Amazon AWS instance table.  The number of entries in this table is limited to 50, i.e.  You cannot have more than 50 machines in a cluster if you are using a flannel with <code>aws-vpc</code> .  In addition, this backend only works with Amazon AWS; </li><li>  <code>host-gw</code> - creates IP routes to subnets via the IP addresses of the remote machine.  Requires direct L2 connectivity between hosts running the flannel; </li><li>  <code>vxlan</code> - creates a virtual <a href="https://en.wikipedia.org/wiki/Virtual_Extensible_LAN">interface VXLAN</a> . </li></ul></li></ol><br>  Because flannel uses the bridge interface to forward packets, each packet passes through two network stacks when sent from one container to another. <br><br><h3>  Ipvlan </h3><br>  <a href="https://www.kernel.org/doc/Documentation/networking/ipvlan.txt">IPvlan</a> is a driver in the Linux kernel that allows you to create virtual interfaces with unique IP addresses without the need to use a bridge interface. <br><br>  To assign an IP address to a container using IPvlan requires: <br><br><ol><li>  Create a container with no network interface at all. </li><li>  Create an ipvlan interface in a standard network namespace. </li><li>  Move the interface to the container namespace. </li></ol><br>  IPvlan is a relatively new solution, so there are no tools to automate this process yet.  Thus, IPvlan deploy on multiple machines and containers becomes more complicated, that is, the implementation cost is high.  However, IPvlan does not require a bridge interface and forwards packets directly from the NIC to the virtual interface, so we expected better performance than the flannel. <br><br><h2>  Load test script </h2><br>  For each option we have done the following steps: <br><br><ol><li>  <a href="http://machinezone.github.io/research/networking-solutions-for-kubernetes/">Set up a network</a> on two physical machines. </li><li>  We launched <a href="https://github.com/MachineZone/tcpkali">tcpkali</a> in a container on the same machine, setting it up to send requests at a constant speed. </li><li>  Run nginx in a container on another machine, setting it up to respond with a fixed-size file. </li><li>  They removed the system metrics and tcpkali results. </li></ol><br>  We ran this test with different numbers of requests: from 50,000 to 450,000 requests per second (RPS). <br><br>  For each request, nginx responded with a fixed-size static file: 350 bytes (contents of 100 bytes and headers of 250 bytes) or 4 kilobytes. <br><br><h2>  results </h2><br><ol><li>  IPvlan showed the lowest latency and best maximum throughput.  Flannel with <code>host-gw</code> and <code>aws-vpc</code> follows it with similar metrics, with <code>host-gw</code> better under maximum load. </li><li>  Flannel with <code>vxlan</code> showed the worst results in all tests.  However, we suspect that its exceptionally bad percentile of 99,999 was caused by a bug. </li><li>  The results for the 4-kilobyte response are similar to the 350-byte case, but there are two noticeable differences: <br><br><ul><li>  the maximum RPS is significantly lower, since 4  270 thousand RPS were required for 4-kilobyte responses to fully load the 10-gigabit NIC; </li><li>  IPvlan is much closer to <code>--net=host</code> when approaching a bandwidth limit. </li></ul></li></ol><br>  Our current choice is flannel with <code>host-gw</code> .  It has few dependencies (in particular, it does not require AWS or a new version of the Linux kernel), it is easy to install compared to IPvlan and offers sufficient performance.  IPvlan is our fallback.  If at some point the flannel gets support for IPvlan, we will switch to that option. <br><br>  Despite the fact that the performance of <code>aws-vpc</code> turned out to be a little better than <code>host-gw</code> , the limitation of 50 machines and the fact that it was tied to Amazon AWS became decisive factors for us. <br><br><h2>  50,000 RPS, 350 bytes </h2><br><img src="https://habrastorage.org/web/50b/362/2d8/50b3622d84b9462b96c253a3b2712e64.png"><br>  At 50,000 requests per second, all candidates showed acceptable performance.  You can already notice the main trend: IPvlan shows the best results, <code>host-gw</code> and <code>aws-vpc</code> follow it, and <code>vxlan</code> - the worst. <br><br><h2>  150,000 RPS, 350 bytes </h2><br><img src="https://habrastorage.org/web/7b1/77a/6ec/7b177a6eceda4e178615dae9fc5566d9.png"><br><br>  <b>Percentages of delay for 150 000 RPS (‚âà30% of the maximum RPS), ms</b> <br><img src="https://habrastorage.org/web/ab3/8e9/22a/ab38e922af584df9a6b96633b406f759.png"><br><br>  IPvlan is slightly better than <code>host-gw</code> and <code>aws-vpc</code> , but it has the worst percentile of 99.99.  <code>host-gw</code> slightly better performance than <code>aws-vpc</code> . <br><br><h2>  250,000 RPS, 350 bytes </h2><br><img src="https://habrastorage.org/web/bd9/c60/e0b/bd9c60e0b1b24f98ae1dca02d65906d4.png"><br><br>  It is assumed that such a load is normal for production, so the results are especially important. <br><br>  <b>Percentile delays for 250,000 RPS (‚âà50% of maximum RPS), ms</b> <br><img src="https://habrastorage.org/web/355/eca/dca/355ecadca054489fa9036f85acd4685a.png"><br><br>  IPvlan again shows better performance, but <code>aws-vpc</code> best result in percentiles of 99.99 and 99.999.  <code>host-gw</code> superior to <code>aws-vpc</code> in percentiles 95 and 99. <br><br><h2>  350,000 RPS, 350 bytes </h2><br><img src="https://habrastorage.org/web/25b/7c2/34f/25b7c234f1e442b1b0748d2d7739daf9.png"><br><br>  In most cases, the delay is close to the results for 250,000 RPS (350 bytes), but it is growing rapidly after the 99.5 percentile, which means an approximation to the maximum RPS. <br><br><h2>  450,000 RPS, 350 bytes </h2><br><img src="https://habrastorage.org/web/4be/0b5/810/4be0b58109af4a4dba33e4a2689386de.png"><br><img src="https://habrastorage.org/web/ac6/1d7/446/ac61d7446daa4f818d382bfedea842d8.png"><br><br>  Interestingly, <code>host-gw</code> shows much better performance than <code>aws-vpc</code> : <br><img src="https://habrastorage.org/web/223/9e7/bf3/2239e7bf3db34a9aadafbef9880cfb1c.png"><br><br><h2>  500,000 RPS, 350 bytes </h2><br>  With a load of 500,000 RPS, only IPvlan still works and even surpasses <code>--net=host</code> , but the delay is so high that we cannot call it valid for applications that are sensitive to delays. <br><img src="https://habrastorage.org/web/335/e7d/452/335e7d452dd04dfb823f8eb48fb38c08.png"><br><br><h2>  50,000 RPS, 4 kilobytes </h2><br><img src="https://habrastorage.org/web/228/ca7/181/228ca71816ce4d3191752009afeb17ba.png"><br><br>  Large results of requests <i>(4 kilobytes against 350 bytes tested earlier)</i> lead to a greater network load, but the list of leaders practically does not change: <br><br>  <b>Percentile delays at 50,000 RPS (‚âà20% of maximum RPS), ms</b> <br><img src="https://habrastorage.org/web/9e6/3e4/35c/9e63e435cc2640c3b753eab7ba9eca82.png"><br><br><h2>  150,000 RPS, 4 kilobytes </h2><br><img src="https://habrastorage.org/web/e56/255/e0a/e56255e0a87241caa72384af28767645.png"><br><br>  <code>host-gw</code> a surprisingly poor percentile of 99.999, but it still shows good results for smaller percentiles. <br><br>  <b>Percentages of delay for 150,000 RPS (‚âà60% of maximum RPS), ms</b> <br><img src="https://habrastorage.org/web/72f/052/352/72f052352a0849029fa08ccd13cd3d26.png"><br><br><h2>  250,000 RPS, 4 kilobytes </h2><br><img src="https://habrastorage.org/web/e3b/622/e7f/e3b622e7f40949778127f9cd1e4f6f30.png"><br><br>  This is the maximum RPS with the greatest answer (4 Kb).  <code>aws-vpc</code> far superior to <code>host-gw</code> , unlike in the case with a small response (350 bytes). <br><br>  <code>Vxlan</code> was again excluded from the schedule. <br><br><h2>  Environment for testing </h2><br><h3>  The basics </h3><br>  To better understand this article and reproduce our test environment, you need to be familiar with the basics of high performance. <br><br>  These articles contain useful information on this topic: <br><br><ul><li>  <a href="https://blog.cloudflare.com/how-to-receive-a-million-packets/">How to receive a million packets per second</a> from CloudFlare; </li><li>  <a href="https://blog.cloudflare.com/how-to-achieve-low-latency/">How to achieve low latency with 10Gbps Ethernet</a> from CloudFlare; </li><li>  <a href="https://www.kernel.org/doc/Documentation/networking/scaling.txt">Scaling in the Linux Networking Stack</a> from the Linux kernel documentation. </li></ul><br><h3>  Cars </h3><br><ul><li>  We used two instances of <a href="https://aws.amazon.com/ec2/instance-types/">c4.8xlarge</a> in Amazon AWS EC2 with CentOS 7. </li><li>  <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html">Enhanced networking is</a> enabled on both machines. </li><li>  Each machine is <a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">NUMA</a> with 2 processors, each processor has 9 cores, each core has 2 threads (hyperthreads), which ensures the effective launch of 36 threads on each machine. </li><li>  Each machine on the network card 10Gbps (NIC) and 60 GB of RAM. </li><li>  To support enhanced networking and IPvlan, we installed <a href="http://elrepo.org/linux/kernel/el7/x86_64/RPMS/">Linux 4.3.0 kernel</a> with Intel ixgbevf driver. </li></ul><br><h3>  Configuration </h3><br>  Modern NICs use <a href="https://msdn.microsoft.com/en-us/library/windows/hardware/ff556942(v%3Dvs.85).aspx">Receive Side Scaling (RSS)</a> over multiple interrupt request ( <a href="https://en.wikipedia.org/wiki/Interrupt_request_(PC_architecture)">IRQ</a> ) lines.  EC2 offers only two such lines in a virtualized environment, so we tested several configurations with RSS and <a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Performance_Tuning_Guide/network-rps.html">Receive Packet Steering (RPS)</a> and came up with the following settings, partly recommended by the Linux kernel documentation: <br><br><ul><li>  <b>IRQ</b> .  The first core of each of the two NUMA nodes is configured to receive interrupts from the NIC.  To map a CPU to a NUMA node, use <code>lscpu</code> : <br><br><pre> <code class="bash hljs">$ lscpu | grep NUMA NUMA node(s): 2 NUMA node0 CPU(s): 0-8,18-26 NUMA node1 CPU(s): 9-17,27-35</code> </pre> <br>  This setting is done by writing <code>0</code> and <code>9</code> to <code>/proc/irq/&lt;num&gt;/smp_affinity_list</code> , where the IRQ numbers are obtained via <code>grep eth0 /proc/interrupts</code> : <br><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> 0 &gt; /proc/irq/265/smp_affinity_list $ <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> 9 &gt; /proc/irq/266/smp_affinity_list</code> </pre> <br></li><li>  <b>Receive Packet Steering (RPS)</b> .  Several combinations have been tested for RPS.  To reduce the delay, we unloaded the processors from IRQ processing, using only CPU numbers 1‚Äì8 and 10‚Äì17.  Unlike <code>smp_affinity</code> in the IRQ, the <code>rps_cpus</code> sysfs file <code>rps_cpus</code> not have a <code>rps_cpus</code> postfix, so <code>rps_cpus</code> are <code>_list</code> to enumerate CPUs to which RPS can send traffic <i>(see <a href="https://www.kernel.org/doc/Documentation/networking/scaling.txt">Linux kernel documentation: RPS Configuration</a> )</i> : <br><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"00000000,0003fdfe"</span></span> &gt; /sys/class/net/eth0/queues/rx-0/rps_cpus $ <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"00000000,0003fdfe"</span></span> &gt; /sys/class/net/eth0/queues/rx-1/rps_cpus</code> </pre> <br></li><li>  <b>Transmit Packet Steering (XPS)</b> .  All NUMA 0 processors (including HyperThreading, i.e. CPUs numbered 0-8, 18-26) were tuned to tx-0, and NUMA 1 processors (9-17, 27-37) were tuned to tx-1 <i>(more see <a href="https://www.kernel.org/doc/Documentation/networking/scaling.txt">Linux kernel documentation: XPS Configuration</a> )</i> : <br><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"00000000,07fc01ff"</span></span> &gt; /sys/class/net/eth0/queues/tx-0/xps_cpus $ <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"0000000f,f803fe00"</span></span> &gt; /sys/class/net/eth0/queues/tx-1/xps_cpus</code> </pre> <br></li><li>  <b>Receive Flow Steering (RFS)</b> .  We planned to use 60 thousand permanent connections, and official documentation recommends that this number be rounded to the nearest power of two: <br><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> 65536 &gt; /proc/sys/net/core/rps_sock_flow_entries $ <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> 32768 &gt; /sys/class/net/eth0/queues/rx-0/rps_flow_cnt $ <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> 32768 &gt; /sys/class/net/eth0/queues/rx-1/rps_flow_cnt</code> </pre> <br></li><li>  <b>Nginx</b>  Nginx used 18 workflows, each with its own CPU (0-17).  This is configured using <a href="http://nginx.org/en/docs/ngx_core_module.html"><code>worker_cpu_affinity</code></a> : <br><br><pre> <code class="bash hljs">workers 18; worker_cpu_affinity 1 10 100 1000 10000 ...;</code> </pre> <br></li><li>  <a href="https://github.com/MachineZone/tcpkali"><b>Tcpkali</b></a>  Tcpkali does not have built-in support for binding to specific CPUs.  To use RFS, we ran tcpkali on the <code>taskset</code> and set up the scheduler for the rare reassignment of threads: <br><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> 10000000 &gt; /proc/sys/kernel/sched_migration_cost_ns $ taskset -ac 0-17 tcpkali --threads 18 ...</code> </pre> </li></ul><br>  This configuration allowed us to evenly distribute the interrupt load across the processor cores and achieve better throughput while maintaining the same latency as in the other tested configurations. <br><br>  Kernels 0 and 9 serve only network interrupt (NIC) and do not work with packets, but they remain the most busy: <br><br><img src="https://habrastorage.org/web/c7d/4b4/f6d/c7d4b4f6df0741aea7689b8b8e016366.png"><br><br>  I also used <a href="https://fedorahosted.org/tuned/">tuned</a> from Red Hat with the network-latency profile enabled. <br><br>  To minimize the effect of nf_conntrack, <a href="http://serverfault.com/a/536303/99164">NOTRACK rules</a> have been added. <br><br>  The sysctl configuration has been configured to support a large number of TCP connections: <br><br><pre> <code class="bash hljs">fs.file-max = 1024000 net.ipv4.ip_local_port_range = <span class="hljs-string"><span class="hljs-string">"2000 65535"</span></span> net.ipv4.tcp_max_tw_buckets = 2000000 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_tw_recycle = 1 net.ipv4.tcp_fin_timeout = 10 net.ipv4.tcp_slow_start_after_idle = 0 net.ipv4.tcp_low_latency = 1</code> </pre> <br>  <i><b>From the translator</b> : Many thanks to my colleagues from <a href="https://www.mz.com/">Machine Zone, Inc.</a> for testing!</i>  <i>It helped us, so we wanted to share it with others.</i> <i><br><br></i>  <i>PS Perhaps you will also be interested in our article " <a href="https://habrahabr.ru/company/flant/blog/329830/">Container Networking Interface (CNI) - network interface and standard for Linux-containers</a> ."</i> </div><p>Source: <a href="https://habr.com/ru/post/332432/">https://habr.com/ru/post/332432/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../332418/index.html">Create a UWP application in SPL</a></li>
<li><a href="../332422/index.html">Integration of HostTracker with Slack. Site stability: how to keep everyone up to date</a></li>
<li><a href="../332426/index.html">Cisco CDR and Asterisk Telephony Analysis with Splunk</a></li>
<li><a href="../332428/index.html">TI SensorTag, Eclipse kura and web parts integration via Apache Camel</a></li>
<li><a href="../332430/index.html">VKontakte data center</a></li>
<li><a href="../332434/index.html">Systems Management Analysis</a></li>
<li><a href="../332436/index.html">Bitfury Group conducted the first successful multi-hop transaction on the Lightning Network</a></li>
<li><a href="../332438/index.html">‚ÄúUltimate‚Äù blockchain digest: useful materials on Habr√© and other sources on the topic</a></li>
<li><a href="../332442/index.html">ML Grid - Apache Ignite machine learning library</a></li>
<li><a href="../332444/index.html">Doctor Web: MEDoc contains a backdoor giving attackers access to a computer</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>