<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to solve 90% of NLP tasks: a step-by-step guide to natural language processing</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="It doesn't matter who you are a reputable company, or just about to launch your first service - you can always use text data to test your product, imp...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to solve 90% of NLP tasks: a step-by-step guide to natural language processing</h1><div class="post__text post__text-html js-mediator-article">  It doesn't matter who you are a reputable company, or just about to launch your first service - you can always use text data to test your product, improve it and expand its functionality. <br><br>  <strong>Natural language processing (NLP)</strong> is an actively developing scientific discipline that searches for meaning and learns from textual data. <br><br><h2>  How can this article help you </h2><br>  Over the past year, the <a href="https://blog.insightdatascience.com/">Insight</a> team took part in work on several hundred projects, combining the knowledge and experience of leading companies in the United States.  They summarized the results of this work in an article whose translation is now in front of you, and derived approaches to solving the <strong>most common applied problems of machine learning</strong> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      We will start with <strong>the simplest method</strong> that can work - and gradually move to more subtle approaches, such as <a href="https://en.wikipedia.org/wiki/Feature_engineering">feature engineering</a> , word vectors and in-depth training. <br><br>  After reading the article, you will know how: <br><br><ul><li>  collect, prepare, and inspect data; </li><li>  build simple models and, if necessary, make the transition to deep learning; </li><li>  interpret and understand your models to make sure that you interpret the information, not the noise. </li></ul><br>  The post is written in a walkthrough format;  It can also be considered as a review of highly efficient standard approaches. <br><a name="habracut"></a><br><br>  <strong><em>An original <a href="https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb">Jupyter Notepad is</a> attached to the original post, demonstrating the use of all the mentioned techniques.</em></strong>  <strong><em>We encourage you to use it as you read the article.</em></strong> <br><br><h2>  Apply machine learning to understand and use text. </h2><br>  Processing natural language allows you to get <a href="https://arxiv.org/abs/1704.01444">new</a> <a href="https://arxiv.org/abs/1711.00043">amazing</a> <a href="https://arxiv.org/abs/1708.04729">results</a> and is a very wide area.  However, <a href="https://blog.insightdatascience.com/">Insight has</a> identified the following key aspects of practical application, which are much more common: <br><br><ul><li>  Identifying different cohorts of users or customers (for example, predicting customer churn, total customer profits, product preferences) </li><li>  Accurate detection and extraction of various categories of feedback (positive and negative opinions, references to individual attributes like clothing size, etc.) </li><li>  Classification of the text in accordance with its meaning (request for elementary assistance, urgent problem). </li></ul><br>  Despite the presence of a large number of scientific publications and tutorials on the topic of NLP on the Internet, today there are practically no full recommendations and tips on how to <strong>effectively</strong> deal with NLP tasks, while considering solutions to these problems from the very foundations. <br><br><h3>  Step 1: Collect your data </h3><br><h4>  Sample data sources </h4><br>  Any machine learning task begins with the data ‚Äî be it a list of email addresses, posts, or tweets.  Common sources of textual information are: <br><br><ul><li>  Product Reviews (Amazon, Yelp and various app stores). </li><li>  User created content (tweets, Facebook posts, questions on StackOverflow). </li><li>  Diagnostic information (user requests, support tickets, chat logs). </li></ul><br><h4>  Dataset "Disaster in social media" </h4><br>  To illustrate the approaches described, we will use the ‚ÄúDisasters in Social Media‚Äù <a href="https://www.crowdflower.com/data-for-everyone/">data</a> courtesy of <a href="https://www.crowdflower.com/data-for-everyone/">CrowdFlower</a> . <br><br><blockquote>  The authors reviewed over 10,000 tweets that were selected using various search queries such as "on fire", "quarantine" and "pandemonium."  They then marked whether the tweet was related to a catastrophe event (as opposed to jokes using these words, movie reviews, or anything not related to disasters). <br></blockquote><br>  We set ourselves the task of determining which tweets are related to <strong>a catastrophe event</strong> as opposed to those tweets that relate to <strong>irrelevant topics</strong> (for example, films).  Why do we need to do this?  A potential use would be to notify officials of emergencies that require urgent attention ‚Äî an overview of Adam Sandler‚Äôs latest film would have been ignored.  The particular difficulty of this task lies in the fact that both of these classes contain the same search criteria, so we will have to use more subtle differences to separate them. <br><br>  Further we will refer to catastrophe tweets as a <em>‚Äúcatastrophe‚Äù</em> , and tweets about everything else as <em>‚Äúirrelevant‚Äù</em> . <br><br><h4>  Labels </h4><br>  Our data is tagged, so we know which categories the tweets belong to.  As Richard Socher emphasizes, it‚Äôs usually faster, easier and cheaper to <strong>find and mark enough data</strong> on which the model will be trained - instead of trying to optimize a complex teaching method without a teacher. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-0" style="position: static; visibility: visible; display: block; transform: rotate(0deg); width: 500px; margin: 10px auto; max-width: 100%; min-width: 220px;" data-tweet-id="840333380130553856"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  <sup>Instead of spending a month on formulating a machine-learning task without a teacher, just spend a week sorting out the data and train the classifier.</sup> <br><br><h3>  Step 2. Clear your data </h3><br><blockquote>  Rule number one: "Your model can only become so good <br>  how good is your data <br></blockquote><br>  One of the key skills of Data Scientist pro is knowing what the next step should be - working on a model or on data.  As practice shows, it is better to first look at the data itself, and only then clean it up. <br>  <strong>Net dataset will allow the model to learn significant signs and not to retrain on irrelevant noise.</strong> <br><br>  This is followed by the checklist that is used when clearing our data (details can be found in the <a href="https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb">code</a> ). <br><br><ol><li>  Delete all irrelevant characters (for example, any non-alphanumeric characters). <br></li><li>  <a href="https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html">Tokenize</a> text by dividing it into individual words. </li><li>  Delete irrelevant words - for example, tweeting or URLs. <br></li><li>  Translate all characters in lower case so that the words ‚Äúhello‚Äù, ‚ÄúHello‚Äù and ‚ÄúHELLO‚Äù are considered the same word. <br></li><li>  Consider combining words that are misspelled or have alternative spellings (for example, cool / cool / kruuto) <br></li><li>  Consider the possibility of carrying out <a href="https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html">lemmatization</a> , i.e., the information of various forms of one word to the dictionary form (for example, ‚Äúmachine‚Äù instead of ‚Äúmachine‚Äù, ‚Äúby machine‚Äù, ‚Äúmachines‚Äù, etc.) </li></ol><br>  After we go through these steps and check for additional errors, we can start using clean, tagged data to train the models. <br><br><h3>  Step 3. Choose a good data presentation. </h3><br>  As an input, machine learning models take numeric values.  For example, models working with images take a matrix representing the intensity of each pixel in each color channel. <br><br><img src="https://habrastorage.org/webt/pn/vo/ne/pnvonepkxguu1it2qb-nme0rjm0.png"><br><br>  <em>Smiling face represented as an array of numbers</em> <br><br>  Our dataset is a list of sentences, so in order for our algorithm to extract patterns from data, we first need to find a way to present it in such a way that our algorithm can understand it. <br><br><h4>  One-hot encoding ("Bag of words") </h4><br>  The natural way to display text in computers is to encode each character individually as a number (an example of such an approach is <a href="https://en.wikipedia.org/wiki/ASCII">ASCII</a> encoding).  If we ‚Äúfeed‚Äù such a simple representation to the classifier, he will have to study the structure of words from scratch, based only on our data, which is impossible on most datasets.  Therefore, we must use a higher level approach. <br><br>  For example, we can build a dictionary of all unique words in our dataset, and associate a unique index to each word in the dictionary.  Each sentence can then be displayed as a list, the length of which is equal to the number of unique words in our dictionary, and each index in this list will be stored how many times the word is found in the sentence.  This model is called <em>Bag of Words</em> , since it is a display that completely ignores the word order of a sentence.  Below is an illustration of this approach. <br><br><img src="https://habrastorage.org/webt/uc/f9/sm/ucf9sm_gzdytgyripyurqh7fbbe.png"><br><br>  <em>Presentation of sentences in the form of "bag of words."</em>  <em>The original sentences are on the left, their presentation is on the right.</em>  <em>Each index in the vectors represents one particular word.</em> <br><br><h4>  Visualize vector views. </h4><br>  The Social Media Crash Dictionary contains about 20,000 words.  This means that each sentence will be reflected by a vector of length 20,000. This vector will contain <strong>mostly zeros</strong> , since each sentence contains only a small subset from our dictionary. <br><br>  In order to find out whether our vector representations ( <em>embeddings</em> ) capture the information <strong>relevant to our task</strong> (for example, whether tweets are related to disasters or not), you should try to visualize them and see how well these classes are separated.  Since dictionaries are usually very large and data cannot be visualized for 20,000 measurements, approaches like <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%25B3%25D0%25BB%25D0%25B0%25D0%25B2%25D0%25BD%25D1%258B%25D1%2585_%25D0%25BA%25D0%25BE%25D0%25BC%25D0%25BF%25D0%25BE%25D0%25BD%25D0%25B5%25D0%25BD%25D1%2582">the principal component method</a> (PCA) help project the data into two dimensions. <br><br><img src="https://habrastorage.org/webt/bu/a0/jk/bua0jkpnurkzdifvwunfhapbdom.png"><br><br>  <em>Visualization of vector representations for the "bag of words"</em> <br><br>  Judging by the resulting graphics, it does not seem that the two classes are divided as it should be - this may be a feature of our presentation or simply the effect of reducing the dimension.  In order to find out whether the ‚Äúbag of words‚Äù possibilities are useful for us, we can train a classifier based on them. <br><br><h3>  Step 4. Classification </h3><br>  When you start a task for the first time, it‚Äôs common practice to start with the simplest method or tool that can solve this problem.  When it comes to classifying data, the most common way is <a href="https://ru.wikipedia.org/wiki/%25D0%259B%25D0%25BE%25D0%25B3%25D0%25B8%25D1%2581%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F_%25D1%2580%25D0%25B5%25D0%25B3%25D1%2580%25D0%25B5%25D1%2581%25D1%2581%25D0%25B8%25D1%258F">logistic regression</a> because of its versatility and ease of interpretation.  It is very easy to train, and its results can be interpreted, since you can easily extract all the most important coefficients from the model. <br><br>  We divide our data into a training sample, which we will use to train our model, and a test sample - in order to see how well our model generalizes to data that we have not seen before.  After training, we get an accuracy of 75.4%.  Not so bad!  Guessing the most frequent class (‚Äúirrelevant‚Äù) would give us only 57%. <br><br>  However, even if the result with 75% accuracy would be enough for our needs, we should never use the model in production without trying to understand it. <br><br><h3>  Step 5. Inspection </h3><br><h4>  Error matrix </h4><br>  The first step is to understand what types of errors our model makes, and with what types of errors we would like to see less often in the future.  In the case of our example, <strong>false-positive</strong> results classify an irrelevant tweet as a catastrophe, <strong>false-negative ones</strong> classify a catastrophe as an irrelevant tweet.  If our priority is to respond to each potential event, then we will want to reduce our false negative responses.  However, if we are limited in resources, then we can prioritize a lower false-negative response rate to reduce the likelihood of a false alarm.  A good way to visualize this information is to use <a href="https://en.wikipedia.org/wiki/Confusion_matrix">the error matrix</a> , which compares the predictions made by our model with actual labels.  Ideally, this matrix would be a diagonal line running from the upper left to the lower right corner (this would mean that our predictions coincided perfectly with the truth). <br><br><img src="https://habrastorage.org/webt/ks/u6/nl/ksu6nlmjjzzydnetbppnc04vb98.png"><br><br>  Our classifier creates more false-negative than false-positive results (proportionally).  In other words, the most frequent mistake of our model is inaccurate classification of catastrophes as irrelevant.  If false positives reflect a high cost for law enforcement, then this could be a good option for our classifier. <br><br><h3>  Explanation and interpretation of our model </h3><br>  To validate our model and interpret its predictions, it is important to look at the words it uses to make decisions.  If our data is biased, our classifier will produce accurate predictions on sample data, but the model will not be able to generalize them well enough in the real world.  The diagram below shows the most significant words for classes of disasters and irrelevant tweets.  Drawing up diagrams reflecting the significance of words is not difficult in the case of using the ‚Äúbag of words‚Äù and logistic regression, since we simply extract and rank the coefficients that the model uses for its predictions. <br><br><img src="https://habrastorage.org/webt/oq/ij/ko/oqijkopfrt-ataefzjonby8jpoq.png"><br><br>  <em>"Bag of words": the importance of words</em> <br><br>  Our classifier correctly found several patterns ( <em>hiroshima</em> - "Hiroshima", <em>massacre</em> - "massacre"), but it is clear that he retrained in some meaningless terms ("heyoo", "x1392").  So, now our ‚Äúbag of words‚Äù deals with a huge dictionary of various words and all these words are equivalent for him.  However, some of these words are very common, and only add noise to our predictions.  Therefore, we will further try to find a way to present sentences in such a way that they can take into account the frequency of words, and see if we can get more useful information from our data. <br><br><h3>  Step 6. Consider the structure of the dictionary </h3><br><h4>  TF-IDF </h4><br>  To help our model focus on meaningful words, we can use <a href="https://ru.wikipedia.org/wiki/TF-IDF">TF-IDF</a> ( <em>Term Frequency, Inverse Document Frequency</em> ) scoring on top of our word bag model.  TF-IDF weighs on the basis of how rare they are in our dataset, lowering the priority of words that occur too often and just add noise.  Below is the projection of the principal component method, allowing us to evaluate our new presentation. <br><br><img src="https://habrastorage.org/webt/7r/w1/zw/7rw1zwzjbk15mlftvwttgiwyx0e.png"><br><br>  <em>Visualize a vector view using TF-IDF.</em> <br><br>  We can observe a clearer separation between the two colors.  This indicates that our classifier should be easier to separate the two groups.  Let's see how our results improve.  By teaching another logistic regression on our new vector representations, we get an <strong>accuracy of 76.2%</strong> . <br><br>  Very minor improvement.  Maybe our model at least began to choose more important words?  If the result obtained for this part is better, and we do not allow the model to "cheat", then this approach can be considered an improvement. <br><br><img src="https://habrastorage.org/webt/_w/kf/sn/_wkfsncbbhoxushgyarr8rxgk9g.png"><br><br>  <em>TF-IDF: Significance of Words</em> <br><br>  The words chosen by the model really look much more relevant.  Despite the fact that the metrics on our test set increased only slightly, we now have <strong>much more confidence in using the model</strong> in a real system that will interact with customers. <br><br><h3>  Step 7. Apply semantics </h3><br><h4>  Word2vec </h4><br>  Our latest model was able to ‚Äúsnatch‚Äù the words that carry the most meaning.  However, most likely, when we release her in production, she will encounter words that have not been encountered in the training set ‚Äî and will not be able to accurately classify these tweets, <strong>even if she saw very similar words during the training</strong> . <br><br>  To solve this problem, we need to capture the <strong>semantic (semantic) meaning of words</strong> - this means that it is important for us to understand that the words ‚Äúgood‚Äù and ‚Äúpositive‚Äù are closer to each other than the words ‚Äúapricot‚Äù and ‚Äúcontinent‚Äù.  We will use the Word2Vec tool that will help us match the meanings of words. <br><br><h4>  Use of pre-training results </h4><br>  <a href="">Word2Vec</a> is a technique for finding continuous mappings for words.  Word2Vec learns to read a huge amount of text and then memorize which word appears in similar contexts.  After learning on enough data, Word2Vec generates a vector of 300 dimensions for each word in the dictionary, in which words with a similar meaning are located closer to each other. <br><br>  The authors of the <a href="">publication</a> on the topic of continuous vector representations of words laid out in an open access model that was previously trained on a very large amount of information, and we can use it in our model to contribute knowledge about the semantic meaning of words.  Pre-trained vectors can be taken in the repository mentioned in the <a href="https://github.com/hundredblocks/concrete_NLP_tutorial">article by reference</a> . <br><br><h4>  Offer Level Display </h4><br>  A quick way to get suggestions for our classifier is to average Word2Vec scores for all words in our sentence.  This is still the same approach as with the ‚Äúbag of words‚Äù earlier, but this time we lose only the syntax of our sentence, while preserving the semantic (semantic) information. <br><br><img src="https://habrastorage.org/webt/xk/ns/ra/xknsraialkgrmqkmcyaas6njfj0.png"><br><br>  <em>Word2Vec Vector Offerings</em> <br><br>  Here is a visualization of our new vector concepts after using the listed techniques: <br><br><img src="https://habrastorage.org/webt/g4/_j/ez/g4_jezattmxftmnyzzg8-4c9a4s.png"><br><br>  <em>Visualization of vector representations Word2Vec.</em> <br><br>  Now the two groups of colors look separated even more, and this should help our classifier to find the difference between the two classes.  After learning the same model for the third time (logistic regression), <strong>we get an accuracy of 77.7%</strong> - and this is our best result at the moment!  It is time to explore our model. <br><br><h4>  Compromise between complexity and explicability </h4><br>  Since our vector representations are no longer represented as a single-dimension vector per word, as was the case in previous models, it is now harder to understand which words are most relevant to our classification.  Despite the fact that we still have access to the coefficients of our logistic regression, they relate to the 300 dimensions of our investments, and not to the indexes of words. <br><br>  For such a small increase in accuracy, the complete loss of the ability to explain the work of the model is too hard a compromise.  Fortunately, when working with more complex models, we can use interpreters like <a href="https://homes.cs.washington.edu/~marcotcr/blog/lime/">LIME</a> , which are used to get some idea of ‚Äã‚Äãhow the classifier works. <br><br><h4>  LIME </h4><br>  LIME is available on Github as an open package.  This black-box interpreter allows users to explain the solutions of any classifier <strong>on one specific example</strong> by changing the input (in our case, removing the word from the sentence) and observing how the prediction changes. <br><br>  Let's take a look at a couple of explanations for sentences from our dataset. <br><br><img src="https://habrastorage.org/webt/xm/nm/sq/xmnmsqj59wld3wi89m3adqzotmk.png"><br><br>  <em>The correct words of catastrophes are selected for classification as ‚Äúrelevant‚Äù.</em> <br><br><img src="https://habrastorage.org/webt/pv/-e/dd/pv-eddblb1ticidm4oyqoncwwfy.png"><br><br>  <em>Here the contribution of words to the classification looks less obvious.</em> <br><br>  However, we do not have enough time to investigate thousands of examples from our dataset.  Instead, let's run LIME on a representative sample of test data, and see which words occur regularly and make the most contribution to the final result.  Using this approach, we can get estimates of the significance of words in the same way as we did for previous models, and validate the predictions of our model. <br><br><img src="https://habrastorage.org/webt/-2/kk/q4/-2kkq41wxnasygjyka6q6kogcre.png"><br><br>  It seems that the model chooses highly relevant words and accordingly makes clear decisions.  Compared to all previous models, she chooses the most relevant words, so it would be better to send her to the production. <br><br><h3>  Step 8. Using syntax with end-to-end approaches </h3><br>  We considered fast and efficient approaches for generating compact vector representations of sentences.  However, omitting the word order, we discard all syntactic information from our sentences.  If these methods do not provide sufficient results, you can use a more complex model that accepts whole expressions as input and predicts labels, without the need to construct an intermediate representation.  A common way to do this is to consider a sentence as a <strong>sequence of individual word vectors</strong> using either Word2Vec, or more recent approaches like <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a> or <a href="https://arxiv.org/abs/1708.00107">CoVe</a> .  This is what we will do next. <br><br><img src="https://habrastorage.org/webt/gm/ba/rw/gmbarwqrbnmoh925k8oqdeflfco.png"><br><br>  <em>High-performance model learning architecture without additional prior and post-processing (end-to-end, <a href="">source</a> )</em> <br><br>  <a href="">Convolutional neural networks for proposal classification</a> ( <a href="https://github.com/yoonkim/CNN_sentence">CNNs for Sentence Classification</a> ) learn very quickly and can do an excellent service as an input level in the deep learning architecture.  Despite the fact that convolutional neural networks (CNN) are mainly known for their high performance on image data, they show excellent results when working with textual data, and are usually much faster to learn than most complex NLP approaches (for example, <a href="https://habrahabr.ru/company/wunderfund/blog/331310/">LSTM</a> networks and architecture <a href="https://www.tensorflow.org/tutorials/seq2seq">encoder / decoder</a> ).  This model preserves word order and learns valuable information about which word sequences serve as prediction of our target classes.  Unlike previous models, she is aware of the existence of a difference between the phrases ‚ÄúLesha eats plants‚Äù and ‚ÄúPlants eat Lesha‚Äù. <br><br>  Training this model will not require much more effort compared to previous approaches (see the <a href="https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb">code</a> ), and, as a result, we will get a model that works much better than the previous one, allowing us to obtain an <strong>accuracy of 79.5%</strong> .  As with the models we reviewed earlier, the next step should be to research and explain the predictions using the methods we described above to make sure that the model is the best option we can offer to users.  By this point, you should already feel confident enough to handle the next steps yourself. <br><br><h2>  Finally </h2><br>  So, a summary of the approach that we have successfully applied in practice: <br><br><ul><li>  We start with a quick and simple model; </li><li>  explain her predictions; </li><li>  we understand what kinds of mistakes she makes; </li><li>  We use our knowledge to make a decision about the next step - whether it is working on data or on a more complex model. </li></ul><br>  We considered these approaches on a concrete example using models that are designed to recognize, understand, and use short texts ‚Äî for example, tweets;  however, the same ideas <strong>are widely applicable to many different tasks</strong> . <br><br><blockquote>  As already noted in the article, anyone can benefit by applying methods of machine learning, especially in the Internet world, with all the variety of analytical data.  Therefore, the topics of artificial intelligence and machine learning are certainly discussed at our conferences <a href="http://ritfest.ru/">RIT ++</a> and <a href="http://www.highload.ru/siberia/">Highload ++</a> , and from a completely practical point of view, as in this article.  Here, for example, is a video of several performances last year: <br><br><ul><li>  <a href="https://youtu.be/OSk83rfWh1Q">Search for signs of fraud in medical insurance losses</a> / Vasily Ryazanov (Allianz) <br></li><li>  <a href="https://youtu.be/hMIXTiKAVoA">Ranking of candidate responses using machine learning</a> / Sergey Saigushkin (Superjob) <br></li><li>  <a href="https://youtu.be/cpvjp2mEy30">Machine learning in e-commerce</a> / Alexander Serbul (1C-Bitrix) <br></li><li>  <a href="https://youtu.be/ZCNcK2fmkBo">Application of machine learning for generating structured snippets</a> / Nikita Spirin (Datastars) <br></li></ul><br>  The program of the May festival RIT ++ and June <a href="http://www.highload.ru/siberia/">Highload ++ Siberia</a> is on the way, you can follow the current status on conference sites or <a href="http://ritfest.ru/">subscribe to the newsletter</a> , and we will periodically send announcements of approved reports so that you don‚Äôt miss anything. <br></blockquote></div><p>Source: <a href="https://habr.com/ru/post/352614/">https://habr.com/ru/post/352614/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../352604/index.html">Cryptobankomat, contemporary creativity and Russian officials</a></li>
<li><a href="../352606/index.html">The digest of fresh materials from the world of the frontend for the last week ‚Ññ308 (March 26 - April 1, 2018)</a></li>
<li><a href="../352608/index.html">What we should build Scrum: an interview with Agile-coach Vasily Savunov</a></li>
<li><a href="../352610/index.html">As I did Hot-Spot through Virtual Box</a></li>
<li><a href="../352612/index.html">Virtual server on VPS.house: performance review</a></li>
<li><a href="../352616/index.html">Ansible is not so simple</a></li>
<li><a href="../352618/index.html">How to switch to microservices and not break production</a></li>
<li><a href="../352620/index.html">We open the history of the Bolshoi Theater. Part one</a></li>
<li><a href="../352622/index.html">Performance analysis of the drive Intel Optane SSD 750GB</a></li>
<li><a href="../352624/index.html">The Metrix has you ...</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>