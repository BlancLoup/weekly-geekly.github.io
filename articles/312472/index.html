<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Why super-mega-about machine learning in 15 minutes still does not become</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Yesterday I published an article about machine learning and NVIDIA DIGITS. As promised, today's article - why everything is not so good + an example o...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Why super-mega-about machine learning in 15 minutes still does not become</h1><div class="post__text post__text-html js-mediator-article">  Yesterday I published <a href="https://habrahabr.ru/post/311832/">an article</a> about machine learning and NVIDIA DIGITS.  As promised, today's article - why everything is not so good + an example of selecting objects in a frame on DIGITS. <br><br>  NVIDIA raised a wave of public relations about the DetectNet mesh developed and implemented in DIGITS.  The grid is positioned as a solution for searching for identical / similar objects in the image. <br><br><img src="https://habrastorage.org/files/b5b/e64/7c5/b5be647c520b4d609605ad641cc16c4e.jpg"><br><a name="habracut"></a><br><h3>  What it is </h3><br>  At the beginning of the year, I <a href="https://habrahabr.ru/post/277069/">mentioned</a> several times about the <a href="http://pjreddie.com/darknet/yolo/">Yolo</a> fun net.  In general, all the people with whom I spoke, treated her rather negatively, with the words that Faster-RCNN is much faster and simpler.  But, NVIDIA engineers were <a href="https://devblogs.nvidia.com/parallelforall/detectnet-deep-neural-network-object-detection-digits/">inspired by</a> it and assembled their Caffe grid, calling it DetectNet. <br>  The grid principle is the same as in Yolo.  The network output for an image (N * a * N * a) is an N * N * 5 array, in which for each region of the original image of a * a size, 5 parameters are entered: the presence of the object and its size: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e73/e2e/a01/e73e2ea01516a487d8b27d1b76ed2f2c.png" alt="image"></div><br>  Plus grids: <br><br><ul><li>  Quickly counts.  I did it for 10-20ms per frame.  At a time when Faster-RCNN spent 100-150. </li><li>  Just learning and customizing.  With Faster-RCNN it was necessary to potter for a long time. </li></ul><br>  Minus one: there are solutions with better detection. <br><br><h3>  Common words before I start the story </h3><br>  Unlike category recognition, which I wrote about yesterday, the detection of objects is done poorly.  Not user friendly.  Most of the article will be on how to start this miracle.  Unfortunately, this approach kills the original idea of ‚Äã‚ÄãDIGITS, that you can do something without understanding the logic of the system and its mathematics. <br>  But if you still run - it is convenient to use. <br><br><h3>  What we will recognize </h3><br>  A couple of years ago we had a completely crazy <a href="https://habrahabr.ru/company/recognitor/blog/222539/">idea</a> with license plates.  Which resulted in a whole series of <a href="https://habrahabr.ru/company/recognitor/blog/222539/">articles</a> on it.  Including was a decent <a href="https://habrahabr.ru/company/recognitor/blog/243919/">database of</a> photos that we posted. <br><br>  I decided to use part of the developments and re-detect the numbers through DIGITS.  So we will use them. <br><br>  The base is properly labeled, I had a very small, for other <a href="https://habrahabr.ru/company/recognitor/blog/277781/">purposes</a> .  But it was enough to train. <br><br><h3>  Go </h3><br>  Selecting in the main menu ‚ÄúNew Dataset-&gt; Images-&gt; Object Detection‚Äù we get to the menu for creating datasets.  Here you must specify: <br><br><ul><li>  Training image folder - a folder with images </li><li>  Training label folder - folder with textual captions for images </li><li>  Validation image folder - a folder with images for verification </li><li>  Validation label folder - a folder with textual signatures to them </li><li>  Pad image - If the image is smaller than the one specified here, then it will be supplemented with a black background.  If more - the creation of the base will fall ¬Ø \ _ („ÉÑ) _ / ¬Ø </li><li>  Resize image - to what size to resize the image </li><li>  Minimum box size - it‚Äôs best to set this value.  This is the minimum object size during validation. </li></ul><br>  There is a difficulty.  How to make a text-caption to the image with its description?  <a href="https://github.com/NVIDIA/DIGITS/tree/master/examples/object-detection">The example</a> on GitHub from NVIDIA in the official DIGITS repository is silent about this, mentioning only that it is the same as in the kitti.  I was somewhat surprised by this approach to users of a ready-made box of the framework.  But ok.  I went, downloaded the database and the docks to it, read it.  File format: <br><br><pre><code class="bash hljs">Car 0.00 0 1.95 96.59 181.90 405.06 371.40 1.52 1.61 3.59 -3.49 1.62 7.68 1.53 Car 0.00 0 1.24 730.55 186.66 1028.77 371.36 1.51 1.65 4.28 2.61 1.69 8.27 1.53 Car 0.00 0 1.77 401.35 177.13 508.22 249.68 1.48 1.64 3.95 -3.52 1.59 16.82 1.57</code> </pre> <br>  File Description: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#Values Name Description ---------------------------------------------------------------------------- 1 type Describes the type of object: 'Car', 'Van', 'Truck', 'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram', 'Misc' or 'DontCare' 1 truncated Float from 0 (non-truncated) to 1 (truncated), where truncated refers to the object leaving image boundaries 1 occluded Integer (0,1,2,3) indicating occlusion state: 0 = fully visible, 1 = partly occluded 2 = largely occluded, 3 = unknown 1 alpha Observation angle of object, ranging [-pi..pi] 4 bbox 2D bounding box of object in the image (0-based index): contains left, top, right, bottom pixel coordinates 3 dimensions 3D object dimensions: height, width, length (in meters) 3 location 3D object location x,y,z in camera coordinates (in meters) 1 rotation_y Rotation ry around Y-axis in camera coordinates [-pi..pi] 1 score Only for results: Float, indicating confidence in detection, needed for p/r curves, higher is better.</span></span></code> </pre><br>  Naturally, most of the parameters are not needed here.  In reality, you can leave only the ‚Äúbbox‚Äù parameter, the rest will not be used anyway. <br><br>  As it turned out, for DIGITS there was also a second <a href="https://devblogs.nvidia.com/parallelforall/deep-learning-object-detection-digits/">tutorial</a> , where the file format was still signed.  But he was not in the repository DIGITS ¬Ø \ _ („ÉÑ) _ / ¬Ø <br><br>  It confirmed that my guesses about what to use were correct: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4fe/3c3/fe1/4fe3c3fe1b6995c4d9c86f4b85f5f331.png" alt="image"><br><br><h4>  Begin to teach </h4><br>  The class.  The base is made, we begin to train.  For training you need to set the same settings as specified in the <a href="https://github.com/NVIDIA/DIGITS/tree/master/examples/object-detection">example</a> : <br><br><ul><li>  Subtract Mean to None </li><li>  base learning rate at 0.0001 </li><li>  ADAM solver </li><li>  Choose your base </li><li>  Select the ‚ÄúCustom Network‚Äù tab.  Copy the text from the file "/caffe-caffe-0.15/examples/kitti/detectnet_network.prototxt" into it (this is in the caffe from nvidia, of course). </li><li>  It is also recommended to download a pre-trained GoogleNet model <a href="https://github.com/BVLC/caffe/tree/rc3/models/bvlc_googlenet">here</a> .  Specify it in ‚ÄúPretrained model (s)‚Äù </li></ul><br>  Also, I did the following.  For the ‚Äúdetectnet_network.prototxt‚Äù copied mesh, I replaced all image size values ‚Äã‚Äã‚Äú1248, 352‚Äù with image sizes from my base.  Without this, learning fell.  Well, naturally, there is no such thing in one tutorial ... ¬Ø \ _ („ÉÑ) _ / ¬Ø <br><br>  Loss chart falls, training has gone.  But ... The accuracy graph is at zero.  What?! <br>  None of the two tutorials that I found answered this question.  Went to dig into the description of the grid.  Where to dig, it was clear right away.  Once the loss falls, the training is on.  Error in validation pipeline.  And really.  In the network configuration there is a block: <br><br><pre> <code class="bash hljs">layer { name: <span class="hljs-string"><span class="hljs-string">"cluster"</span></span> <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>: <span class="hljs-string"><span class="hljs-string">"Python"</span></span> bottom: <span class="hljs-string"><span class="hljs-string">"coverage"</span></span> bottom: <span class="hljs-string"><span class="hljs-string">"bboxes"</span></span> top: <span class="hljs-string"><span class="hljs-string">"bbox-list"</span></span> python_param { module: <span class="hljs-string"><span class="hljs-string">"caffe.layers.detectnet.clustering"</span></span> layer: <span class="hljs-string"><span class="hljs-string">"ClusterDetections"</span></span> param_str: <span class="hljs-string"><span class="hljs-string">"1024, 640, 16, 0.05, 1, 0.02, 5, 1"</span></span> } }</code> </pre><br>  It looks suspicious.  Opening the description of the <b>clustering</b> layer you can find a comment: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># parameters - img_size_x, img_size_y, stride, # gridbox_cvg_threshold,gridbox_rect_threshold,gridbox_rect_eps,min_height,num_classes</span></span></code> </pre><br>  It becomes clear that these are thresholds.  Zarandomil there 3 numbers without delving into the essence.  Training went + began to grow validation.  Hours for 5 reached some reasonable thresholds. <br><br><img width="700" src="https://habrastorage.org/files/880/342/07e/88034207ed1846d4a3705ebfee43aead.png"><br><br>  But here's a bummer.  With successful training, 100% of the pictures were not distributed.  I had to dig and understand what this layer means. <br><br>  The layer implements the collection of the obtained hypotheses into a single solution.  As the main tool, the OpenCV module ‚Äúcv.groupRectangles‚Äù is used here.  This is a function that associates groups of rectangles into one rectangle.  As you remember, the network has such a structure that in the vicinity of the object there should be a lot of positives.  They need to be collected in a single solution.  The collection algorithm has a bunch of parameters. <br><br><ul><li>  gridbox_cvg_threshold (0.05) - object detection threshold.  Essentially the credibility of the fact that we found a number.  The smaller - the more detections. </li><li>  gridbox_rect_threshold (1) - how many detectors should work for a ‚Äúhave a number‚Äù decision </li><li>  gridbox_rect_eps (0.02) - how many times the size of rectangles can be different to combine them into one hypothesis </li><li>  min_height - the minimum height of the object </li></ul><br>  Now it‚Äôs enough just to pick them up so that it all works.  And now humor.  Taki was also the <a href="https://devblogs.nvidia.com/parallelforall/detectnet-deep-neural-network-object-detection-digits/">third</a> tutorial, where part of the whole thing is described. <br>  But not all ¬Ø \ _ („ÉÑ) _ / ¬Ø <br><br><h4>  What is the result </h4><br>  As a result, you can see what the grid has allocated: <br><br><div style="text-align:center;"><img width="600" src="https://habrastorage.org/files/7d7/593/844/7d7593844d734900a988c8e24fc85045.jpg"></div><br>  It works well.  At first glance, better than the Haar that we used.  But it immediately became clear that a small training base (~ 1500 frames) - makes itself felt.  The database did not take into account dirty numbers =&gt; they are not detected.  The database did not take into account the strong perspective numbers =&gt; they are not detected.  Not considered too large / too small.  Well, you understand.  In short, you need not be lazy and mark out thousands of 5 numbers normally. <br><br>  When recognizing, you can see funny pictures with activation cards ( <a href="">1</a> , <a href="">2</a> , <a href="">3</a> ).  It can be seen that at each next level the number is seen more and more clearly. <br><br><h3>  How to start </h3><br>  Pleasant moment - the result can be run with a code of ~ 20 lines.  And it will be a ready-made number detector: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys caffe_root = <span class="hljs-string"><span class="hljs-string">'../'</span></span> <span class="hljs-comment"><span class="hljs-comment">#     sys.path.insert(0, caffe_root + 'python') import caffe caffe.set_mode_cpu() #   . : #caffe.set_device(0) #caffe.set_mode_gpu() model_def = caffe_root + 'models/DetectNet/deploy.prototxt' #  model_weights = caffe_root + 'models/DetectNet/DetectNet.caffemodel' #  net = caffe.Net(model_def, # defines the structure of the model model_weights, # contains the trained weights caffe.TEST) # use test mode (eg, don't perform dropout) #       mean=np.array([128.0,128.0,128.0]) transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape}) transformer.set_transpose('data', (2,0,1)) # move image channels to outermost dimension transformer.set_mean('data', mean) # subtract the dataset-mean value in each channel transformer.set_raw_scale('data', 255) # rescale from [0, 1] to [0, 255] transformer.set_channel_swap('data', (2,1,0)) # swap channels from RGB to BGR #        net.blobs['data'].reshape(1, # batch size 3, # 3-channel (BGR) images 640, 1024) # image size is 227x227 image = caffe.io.load_image('/media/anton/Bazes/ReInspect/CARS/test/0.jpg')#    transformed_image = transformer.preprocess('data', image)#      output = net.forward() #  output_prob = output['bbox-list'][0] #       print output_prob[0]</span></span></code> </pre><br>  Here I posted a file for the grid and the weight of the trained network, if anyone needs it. </div><p>Source: <a href="https://habr.com/ru/post/312472/">https://habr.com/ru/post/312472/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../312450/index.html">Neural networks for beginners. Part 1</a></li>
<li><a href="../312456/index.html">1C in the clouds</a></li>
<li><a href="../312458/index.html">Facebook and Google have released Yarn, a new JavaScript package manager.</a></li>
<li><a href="../312460/index.html">ONLYOFFICE or Libre: about the battle of formats and co-editing</a></li>
<li><a href="../312470/index.html">Storing user passwords in Google Chrome on Android</a></li>
<li><a href="../312476/index.html">How Asana makes it easier to work with the team, documents and brings more profit</a></li>
<li><a href="../312482/index.html">Hackathon according to criminal statistics</a></li>
<li><a href="../312484/index.html">How to love mbed, and then screw it up twice</a></li>
<li><a href="../312490/index.html">How to assemble bigrams for a body of any size on a home computer</a></li>
<li><a href="../312492/index.html">OpenShift v3. Part II. Continue dating. ROR4</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>