<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Information theory in the problem of testing the hypothesis of independence of values ‚Äã‚Äãtaken by a random variable, for example, the DJI index</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Let us try to test the hypothesis of whether the increments of the values ‚Äã‚Äãof the DJI index are statistically independent. At the same time, as a ref...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Information theory in the problem of testing the hypothesis of independence of values ‚Äã‚Äãtaken by a random variable, for example, the DJI index</h1><div class="post__text post__text-html js-mediator-article">  Let us try to test the hypothesis of whether the increments of the values ‚Äã‚Äãof the DJI index are statistically independent.  At the same time, as a reference source of data with which we will make a comparison, we take an artificial time series generated from the actual increments of the original series, but at the same time randomly mixed.  As a measure of statistical independence, we use statistics of mutual information. <br><br><a name="habracut"></a><br><br>  As an experimental data source, we take the data from the DJI for 30 years (daily closing prices), from 1981-08-31 to 2011-08-26 (Source: <a href="http://finance.yahoo.com/">finance.yahoo.com</a> ). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Dow Jones Industrial Average (DJI) Index Values <br><img src="https://habrastorage.org/getpro/habr/post_images/a47/c60/fc7/a47c60fc7640d8cf2be5aadf779e78d2.jpg" alt="image"><br><br>  The number of percentage increments of quotes calculated by the formula X [t] / X [t-1] - 1 <br><img src="https://habrastorage.org/getpro/habr/post_images/879/bb6/3f6/879bb63f6ffe3ae30aa06102ac15e07f.jpg" alt="image"><br><br>  To bring the investigated continuous, in its essence, variable to a discrete type, we proceed to a series of percentage increments, rounded to 0.01 (1%).  Counting mutual information for continuous variables, although technically possible, but not informative, due to the very large value of n - a finite set of values ‚Äã‚Äãof the characteristic taken by a random variable. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/57d/0b7/cfd/57d0b7cfd47084b7a5b2fffa1ed291b6.jpg" alt="image"><br><br><h5>  Basic concepts of information and theoretical ideas used in the article </h5><br><br>  (All formulas and theory are borrowed from: <a href="http://ru.wikipedia.org/">ru.wikipedia.org</a> as well as from a number of monographs that can be searched by keywords.) <br><br>  The theory of information has evolved inextricably with the theory of communication, I will not deviate from this tradition. <br><br><h6>  What is information? </h6><br><br>  Imagine that there is a data transmitter and receiver.  The transmitter transmits a discrete variable X, which accepts a limited number of possible values ‚Äã‚Äãof x (this is also called the alphabet).  The probability of the implementation of each specific value differs from zero, otherwise this value is simply excluded from the analysis.  The form of the probability density function on the space of values ‚Äã‚Äãaccepted by a variable can be arbitrary.  The sum of all probabilities for each possible value is 1 (if the sum is 0, then the further course of thoughts does not make sense). <br><br>  The receiver perceives the transmitted values ‚Äã‚Äãof X, or it can be said that an event occurs at the point of reception of values ‚Äã‚Äã‚Äî the variable X has assumed the value x.  And the less we, that is, the observers, know what kind of event will occur (that is, what value the receiver will take), the more entropy this system has, and the more information the implementation of this event will bring. <br><br>  Hence, informational entropy (concepts borrowed from entropy in theoretical physics) is a quantitative measure of uncertainty in an abstract system consisting of the possibility of realizing an event and its direct implementation.  Hmm, sounds really abstract.  But this is also the strength of this theory: it can be applied to the widest class of phenomena. <br><br>  But what is information?  It is also a quantitative measure characterizing the amount of entropy, or uncertainty, that has left the system during the implementation of a specific event.  Information, therefore, is quantitatively equal to entropy. <br><br>  If they talk about the whole range of values ‚Äã‚Äãthat are implemented in the system, then they talk about average information or informational entropy.  This value is calculated by the formula: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c1d/710/d85/c1d710d85dd108df3b21b4faebe87298.png" alt="image"><br><br>  If they talk about the information of a single implementation of a random variable, they talk about their own information: <br><img src="https://habrastorage.org/getpro/habr/post_images/4cf/e00/568/4cfe005688c53c29d6e24db0993698a7.png" alt="image"><br><br>  For example, an experiment with multiple fair coin popping is a system with an average information equal to 1 Bit (when substituting the base 2 logarithm into the formula).  At the same time, before each flip we expect a tail or an eagle with equal probability (these events are independent from each other) and the uncertainty is always 1. And what will be the information entropy of this system with an unequal probability of falling sides of a coin?  Say, the eagle falls with a probability of 0.6, and tails - with a probability of 0.4.  Calculate and get: 0.971 bits.  The entropy of the system has decreased, since the uncertainty of the experiment is less: we expect the eagle more often than tails. <br><br>  Returning to the example of the transmitter and receiver, if the connection between them is ideally good, then the information (in a broad sense) will always be transmitted 100% correct.  In other words, the mutual information between the transmitter and the receiver will be equal to the average information of the receiver itself (symbolizing the realization of the event), and if the data from the transmitter <i>is</i> not related to the data received by the receiver, then the mutual information between them will be equal to 0. In other words, The transmitter transmits nothing about what the receiver is receiving.  If there is some loss of information, the mutual information will be from 0 to the average receiver information. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fb9/f86/340/fb9f863403e58d2ef0a7b69696f011eb.png" alt="image"><br><br>  In the context of the task about which I wrote in this article, mutual information is a tool for finding an arbitrary type of dependence between a receiver (dependent variable) and a transmitter (independent variable).  The maximization of mutual information between a pair of variables indicates the presence of a certain determinism of the implementation of a random value in relation to its past implementations.  You can, of course, take anything as independent variables, from the composition of singing birds in the morning to the frequency of certain words in online publications on the topic of stock trading.  "The truth is somewhere near." <br><br>  So, we calculate the entropy of the data source (http://ru.wikipedia.org/): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c1d/710/d85/c1d710d85dd108df3b21b4faebe87298.png" alt="image"><br><br>  The average information (or just entropy) of this data source (calculated from the logarithm with base 2) is 2.098 bits. <br><br>  Mutual information between random variables is calculated through the concept of informational entropy (http://ru.wikipedia.org/): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fb9/f86/340/fb9f863403e58d2ef0a7b69696f011eb.png" alt="image"><br><br>  A histogram of mutual information values ‚Äã‚Äãbetween the dependent variable ‚Äî the percentage increment of the index calculated by the closing prices ‚Äî and its values ‚Äã‚Äãwith a shift from 1 to 250 steps back in time. <br><img src="https://habrastorage.org/getpro/habr/post_images/2a4/5e2/f49/2a45e2f4903f8e5484e955db815dcef9.jpg" alt="image"><br><br>  In particular, it can be seen that the maximum mutual information is calculated with a variable with a lag of 5, that is, with a value having a place one trading week ago.  Also, it is obvious that the amount of mutual information decreases when immersed in the lag space. <br><br>  The type of probability density distribution function for the obtained set of values ‚Äã‚Äãof the quantity of mutual information: <br><img src="https://habrastorage.org/getpro/habr/post_images/4c4/365/290/4c4365290ca0cf58f5c02864b3c59ca8.jpg" alt="image"><br><br>  Generate an artificial time series for reference purposes.  The source of a series of integers that specify the sequence of values ‚Äã‚Äãof the attribute was selected site <a href="http://www.random.org/">www.random.org</a> .  According to the information on the site, they provide really random numbers (unlike PRNG, a pseudo-random number generator). <br><br>  The resulting series of increments, with randomly mixed chronological order <br><img src="https://habrastorage.org/getpro/habr/post_images/b55/bc9/7bc/b55bc97bc281e84867073c10d7c7a2ea.jpg" alt="image"><br><br>  By eye, one can note how much more stationary the data has become. <br><br>  The same row with rounded values. <br><img src="https://habrastorage.org/getpro/habr/post_images/7a7/48c/4d2/7a748c4d2d049214ef66f0d54c360636.jpg" alt="image"><br><br>  The histogram of mutual information values ‚Äã‚Äãbetween the dependent variable and its values ‚Äã‚Äãwith a shift from 1 to 250 steps back in time along an artificial time series of increments (while maintaining the same type of probability density function on the space of feature values) <br><img src="https://habrastorage.org/getpro/habr/post_images/e64/f1a/d0b/e64f1ad0b209c68e456015acebbbeedb.jpg" alt="image"><br><br>  Type of probability density distribution function for a given sample: <br><img src="https://habrastorage.org/getpro/habr/post_images/57b/9f4/66d/57b9f466df6e468698bff3267787fca5.jpg" alt="image"><br><br>  Comparison of 2 considered cases of mutual information calculation <br><img src="https://habrastorage.org/getpro/habr/post_images/863/b50/1e0/863b501e0a6e1e0598a0389a9a28ad03.jpg" alt="image"><br><br>  You can see by how much different samples of the values ‚Äã‚Äãof the amount of mutual information are different. <br><br>  Let us test the hypothesis about the significance of the difference (the difference in the form of the probability density function of the two) of two samples of calculated values ‚Äã‚Äãof mutual information - for the original and artificial time series.  Using non-parametric tests, we calculate the statistics using the Kolmogorov-Smirnov method (the Kolmogorov-Smirnov test is used to compare two independent samples of values ‚Äã‚Äãin order to determine the statistical significance of the differences between sample values. The U-test of Mann and Whitney is used for the same purpose). <br><br>  Result: p = 0.00 at the accepted threshold level of significance of 0.05. <br><br>  The result of the U-test by the method of Mann and Whitney: p = 0.00. <br><br>  We see that in both cases the hypothesis about the difference between the samples of the characteristic values ‚Äã‚Äãis accepted (p is less than 0.05). <br><br>  It can be concluded that in the natural financial data (at least the DJI index) there are statistically significant dependencies of an arbitrary type between increments of quotations.  That is, such a series of data can not be considered random.  Theoretically, there is a space of possibilities for predicting the future values ‚Äã‚Äãof such a series, for example, using neural networks. <br><br>  PS: I would welcome comments, criticism. </div><p>Source: <a href="https://habr.com/ru/post/127394/">https://habr.com/ru/post/127394/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../127387/index.html">Confetti - simple and fast configuration of your project</a></li>
<li><a href="../127390/index.html">Google redesigned Google Docs editor</a></li>
<li><a href="../127391/index.html">Why do freelancers and customers often consider each other idiots</a></li>
<li><a href="../127392/index.html">Wanted! Eric Lippert old hits</a></li>
<li><a href="../127393/index.html">Microsoft showed Windows 8 Explorer with a ribbon interface</a></li>
<li><a href="../127395/index.html">Windows 8 will have native support for ISO and VHD</a></li>
<li><a href="../127396/index.html">Learning to do "Yandex"</a></li>
<li><a href="../127397/index.html">Stanford course on digital photography</a></li>
<li><a href="../127398/index.html">HP TouchPad will produce until October 31</a></li>
<li><a href="../127400/index.html">What to do if you are stuck with a large and complex programmer task?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>