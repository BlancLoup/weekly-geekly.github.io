<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How computers learned to recognize images incredibly well</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A landmark scientific work from 2012 transformed the field of software image recognition 


 Today I can, for example, open Google Photos, write ‚Äúthe ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How computers learned to recognize images incredibly well</h1><div class="post__text post__text-html js-mediator-article"><h3>  A landmark scientific work from 2012 transformed the field of software image recognition </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/0d6/b8d/67e/0d6b8d67e771c94acab7f0ed50a54ba4.jpg"><br><br>  Today I can, for example, open Google Photos, write ‚Äúthe beach‚Äù, and see a bunch of my photos from various beaches that I have visited over the last decade.  And I never signed my photos - Google recognizes beaches on them based on their content.  This seemingly boring feature is based on the technology called ‚Äúdeep convolutional neural network‚Äù, which allows programs to understand images using a complex method that is inaccessible to technologies of previous generations. <br><br>  In recent years, researchers have found that software accuracy becomes better as they create deeper neural networks (NNs) and train them on ever larger data sets.  This created an insatiable need for computing power, and enriched GPU manufacturers such as Nvidia and AMD.  A few years ago, Google developed its own special chips for the National Assembly, while other companies are trying to keep up with it. <br><a name="habracut"></a><br>  In Tesla, for example, Andrei Karpati, an expert on deep learning, was appointed head of the Autopilot project.  Now the automaker is developing its own chip to speed up the work of the National Assembly in future versions of the autopilot.  Or take Apple: in the A11 and A12 chips, central to the latest iPhones, there is the Neural Engine ‚Äú <a href="https://ru.wikipedia.org/wiki/%25D0%259D%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25BF%25D1%2580%25D0%25BE%25D1%2586%25D0%25B5%25D1%2581%25D1%2581%25D0%25BE%25D1%2580">neural processor</a> ‚Äù, which accelerates the work of the National Assembly and allows the applications for image and voice recognition to work better. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The experts I interviewed for this article track the beginning of the depth learning boom to a specific work: AlexNet, named after its main author, Alex Krizhevsky.  ‚ÄúI believe that 2012 was a landmark year when AlexNet‚Äôs work came out,‚Äù said Sean Gerrish, an MO expert and author of <a href="https://mitpress.mit.edu/books/how-smart-machines-think">How Smart Machines Think</a> . <br><br>  Until 2012, deep neural networks (GNS) were something of a remote place in the world of MO.  But then Krizhevsky and his colleagues from the University of Toronto filed an application for a prestigious competition in image recognition, and she radically surpassed in accuracy everything that was developed before her.  Almost instantly, the HNS has become the leading technology in image recognition.  Other researchers who used this technology soon demonstrated further improvements in recognition accuracy. <br><br>  In this article, we delve into deep learning.  I will explain what NS are, how they are taught, and why they require such computational resources.  And then I will explain why a certain type of NA ‚Äî deep convolutional networks ‚Äî is so well understood by images.  Do not worry, there will be a lot of pictures. <br><br><h2>  Simple example with one neuron </h2><br>  The concept of "neural network" may seem vague to you, so let's start with a simple example.  Suppose you want the NA to decide whether the car should go, on the basis of green, yellow and red traffic lights.  NA can solve this problem with a single neuron. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c00/2d0/dda/c002d0dda45d5fef70ab7a156ad8e7cd.png"><br><br>  The neuron receives input data (1 is on, 0 is off), multiplies by the corresponding weight, and adds all the values ‚Äã‚Äãof the weights.  Then the neuron adds an offset that defines the threshold value for the "activation" of the neuron.  In this case, if the output is positive, we consider that the neuron is activated - and vice versa.  The neuron is equivalent to the inequality "green - red - 0.5&gt; 0".  If it turns out to be true - that is, the green is on, and the red is off - then the car must go. <br><br>  In a real NS, artificial neurons take another step.  Summing the weighted input and adding the offset, the neuron applies a nonlinear activation function.  Often used sigmoid, S-shaped function, always producing a value from 0 to 1. <br><br>  Using the activation function will not change the result of our simple traffic light model (we just need to use a threshold value of 0.5, not 0).  But the nonlinearity of the activation functions is necessary in order for the NNs to model more complex functions.  Without the activation function, each arbitrarily complex NA is reduced to a linear combination of input data.  A linear function can not simulate the complex phenomena of the real world.  Nonlinear activation function allows the NA to approximate <a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B5%25D0%25BE%25D1%2580%25D0%25B5%25D0%25BC%25D0%25B0_%25D0%25A6%25D1%258B%25D0%25B1%25D0%25B5%25D0%25BD%25D0%25BA%25D0%25BE">any mathematical function</a> . <br><br><h2>  Network example </h2><br>  Of course, there are many ways to approximate a function.  NNs are distinguished by the fact that we know how to ‚Äútrain‚Äù them using a little algebra, a bunch of data and a sea of ‚Äã‚Äãcomputing power.  Instead of the programmer developing directly the NA for a specific task, we can create software that starts with a fairly common NA, examines a bunch of marked examples, and then modifies the NA so that it gives the correct label for as many examples as possible.  The expectation is that the final NA will summarize the data and will issue the correct labels for examples that were not previously in the database. <br><br>  The process leading to this goal began long before AlexNet.  In 1986, a trio of researchers published a <a href="https://www.nature.com/articles/323533a0">landmark work</a> on backpropagation ‚Äî a technology that helped make the mathematical teaching of complex NAs a reality. <br><br>  To get an idea of ‚Äã‚Äãhow backpropagation works, let's take a look at a simple NA described by Michael Nielsen in his excellent <a href="http://neuralnetworksanddeeplearning.com/chap1.html">online GO tutorial</a> .  The purpose of the network is to process the image of a handwritten digit in the resolution of 28x28 pixels and correctly determine whether the digit is written 0, 1, 2, etc. <br><br>  Each image is 28 * 28 = 784 input values, each of which is a real number from 0 to 1, indicating how bright or dark the pixel is.  Nielsen created this type of NA: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fbf/85e/fe1/fbf85efe11ae6dc62ccd0257e2325229.png"><br><br>  Each circle in the center and in the right column is a neuron, similar to the one we discussed in the previous section.  Each neuron takes a weighted average of the input data, adds an offset, and applies an activation function.  The circles on the left are not neurons, they denote network input data.  And although the picture shows only 8 input circles, in fact there are 784 ‚Äî one for each pixel. <br><br>  Each of the 10 neurons on the right should ‚Äúwork‚Äù on its own number: the top one should turn on when handwritten 0 is input (and only in this case), the second one when the network sees handwritten 1 (and only her), and so on. <br><br>  Each neuron perceives input from each neuron of the previous layer.  So each of the 15 neurons in the middle receives 784 input values.  Each of these 15 neurons has a weight parameter for each of the 784 input values.  This means that only this layer has 15 * 784 = 11,760 weight parameters.  Similarly, the output layer contains 10 neurons, each of which accepts input from all 15 neurons of the middle layer, which adds another 15 * 10 = 150 weight parameters.  In addition, the network has 25 variable offsets - one for each of the 25 neurons. <br><br><h2>  Neural network training </h2><br>  The goal of the training is to adjust these 11 935 parameters to maximize the likelihood that the desired output neuron ‚Äî and only he ‚Äî is activated when the networks give an image of a handwritten digit.  We can do this with the help of the famous MNIST image set, where there are 60,000 labeled images with a resolution of 28x28 pixels. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/16d/dab/2ee/16ddab2ee3bcd9e22d96f267e473a2f4.png"><br>  <i>160 of 60,000 images from the MNIST set</i> <br><br>  Nielsen demonstrates how to train a network using 74 lines of regular python code - without any libraries for MO.  Training begins with the selection of random values ‚Äã‚Äãfor each of these 11 935 parameters, weights and offsets.  The program then iterates over the sample images, going through two stages with each of them: <br><ol><li>  The forward propagation step calculates the network output based on the input image and current parameters. </li><li>  The backward propagation step calculates the deviation of the result from the correct output data and modifies the network parameters so as to slightly improve its performance on the image. </li></ol><br><br>  Example.  Suppose the network received the following image: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e22/129/af7/e22129af76f6376aaa204ae02164a790.png"><br><br>  If it is well calibrated, then output ‚Äú7‚Äù should tend to 1, and the other nine conclusions should tend to 0. But suppose that instead, the network at output ‚Äú0‚Äù gives a value of 0.8.  It's too much!  The learning algorithm changes the input weights of the neuron responsible for "0", so that it gets closer to 0 the next time it processes this image. <br><br>  For this, the backpropagation algorithm calculates the error gradient for each input weight.  This is a measure of how the output error changes for a given change in input weight.  Then the algorithm uses a gradient to decide how much to change each input weight - the larger the gradient, the stronger the change. <br><br>  In other words, the learning process "teaches" the neurons of the output layer to pay less attention to those inputs (neurons in the middle layer) that push them to the wrong answer, and more to those inputs that push in the right direction. <br><br>  The algorithm repeats this step for all other output neurons.  It reduces the input weights for neurons "1", "2", "3", "4", "5", "6", "8" and "9" (but not "7") in order to lower the value of these output neurons.  The higher the output value, the greater the gradient of the output error relative to the input weight - and the stronger its weight decreases. <br><br>  Conversely, the algorithm increases the weight of the input data for output ‚Äú7‚Äù, which causes the neuron to issue a higher value the next time it is given this image.  Again, inputs with larger values ‚Äã‚Äãwill increase the weight more, which will force the output neuron ‚Äú7‚Äù to pay more attention to these inputs in the following times. <br><br>  Then the algorithm must perform the same calculations for the middle layer: change each input weight in the direction that will reduce network errors - again, bringing the output ‚Äú7‚Äù closer to 1, and the rest to 0. But each middle neuron has a connection with all 10 weekends, which complicates matters in two ways. <br><br>  First, the error gradient for each average neuron depends not only on the input value, but also on the error gradients in the next layer.  The algorithm is called backpropagation because the error gradients of the later layers of the network propagate in the opposite direction and are used to calculate gradients in the earlier layers. <br><br>  Also, each middle neuron is an input for all ten weekends.  Therefore, the learning algorithm has to calculate the error gradient, which reflects how a change in a certain input weight affects the average error across all outputs. <br><br>  Back propagation is an algorithm for climbing a hill: each of its passes brings the output values ‚Äã‚Äãcloser to the correct ones for a given image, but only a little.  The more examples the algorithm scans, the higher it climbs up the hill, towards the optimal set of parameters that correctly classify the maximum number of training examples.  Thousands of examples are required to achieve high accuracy, and the algorithm may need to cycle through each image in this set dozens of times before its efficiency stops growing. <br><br>  Nielsen shows how to implement these 74 lines in python.  Surprisingly, a network trained with such a simple program is able to recognize more than 95% of handwritten numbers from the MNIST database.  With additional enhancements, a simple two-layer network is able to recognize more than 98% of the numbers. <br><br><h2>  AlexNet Breakthrough </h2><br>  You might think that the development of the theme of backward proliferation should have gone through in the 1980s, and to generate rapid progress in the Defense Ministry based on the National Assembly - but this did not happen.  In the 1990s and the early 2000s, some people worked on this technology, but interest in the National Assembly did not gain momentum until the early 2010s. <br><br>  This can be tracked by the results of <a href="https://arxiv.org/pdf/1409.0575.pdf">the ImageNet competition</a> - an annual competition in MO organized by a computer scientist from Stanford Fey-Fey Lee.  Each year, opponents are given the same set of more than a million images for training, each of which is manually labeled in categories of more than 1000 - from the ‚Äúfire engine‚Äù and ‚Äúmushroom‚Äù to ‚Äúcheetah‚Äù.  Software participants are judged by the possibility of classifying other images that were not in the set.  The program can give out several guesses, and its work is considered successful if at least one of the five first guesses coincides with the mark made by the person. <br><br>  The competition began in 2010, and deep NAs did not play a big role in it in the first two years.  The best teams used different MO techniques, and achieved rather average results.  In 2010, the team won with an error rate of 28. In 2011, with an error of 25%. <br><br>  And then came 2012.  A team from the University of Toronto made a <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">bid</a> - which AlexNet was later christened in honor of the lead author of the work, Alex Krizhevsky - and left the rivals far behind.  Using deep NA, the team achieved a 16% error rate.  For the nearest competitor, this figure was 26. <br><br>  The NS described in the article for handwriting recognition has two layers, 25 neurons and almost 12,000 parameters.  AlexNet was much larger and more complex: eight trained layers, 650,000 neurons and 60 million parameters. <br><br>  To train an NS of this size requires enormous computational power, and AlexNet has been designed to take advantage of the massive parallelization available on modern GPUs.  The researchers thought of how to divide the network training work into two GPUs, which doubled the power.  And still, despite the hard optimization, the network training took 5-6 days on the hardware that was available in 2012 (on a pair of Nvidia GTX 580 with 3 Gb of memory). <br><br>  It is useful to study the examples of the results of AlexNet‚Äôs work in order to understand how serious this breakthrough was.  Here is a picture from a scientific work, which shows examples of images and the first five network guesses according to their classification: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d36/1ee/3b6/d361ee3b61cc19de787edda63d6e1e65.png"><br><br>  AlexNet was able to recognize on the first picture of the tick, although from it there is only a small form in the very corner.  The software not only correctly identified the leopard, but also gave other close options - jaguar, cheetah, snow leopard, Egyptian mau.  AlexNet tagged a photo of hornbeams as a ‚Äúmushroom‚Äù.  Just "mushroom" was the second version of the network. <br><br>  AlexNet's ‚Äúbugs‚Äù are also impressive.  She noted a photo with a Dalmatian, standing behind a bunch of cherries, as "Dalmatian", although the official label was "cherry".  AlexNet recognized that there was a berry in the photo - among the first five options were "grapes" and "elderberry" - she simply did not recognize the cherry.  To a photo of a Madagascar lemur sitting in a tree, AlexNet has listed a list of small mammals living in trees.  I think that many people (including me) would have put the wrong signature here. <br><br>  The quality of work was impressive, and demonstrated that the software is able to recognize ordinary objects in a wide range of their orientations and environments.  GNS quickly became the most popular technique for image recognition, and since then the world of MoD has not refused it. <br><br>  ‚ÄúOn the wave of success in 2012 of the method based on civil defense, most of the participants of the 2013 competition switched to deep convolutional neural networks,‚Äù <a href="https://arxiv.org/pdf/1409.0575.pdf">wrote</a> sponsors of ImageNet.  In the following years, this trend continued, and subsequently the winners worked on the basis of basic technologies, first used by the AlexNet team.  By 2017, rivals, using deeper NAs, seriously reduced the percentage of errors to less than three.  Given the complexity of the problem, computers in some way have learned to solve it better than many people. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/433/872/5e1/4338725e10f14ecf679878fb267394c9.png"><br>  <i>The percentage of errors in the classification of images in different years</i> <br><br><h2>  Convolution networks: concept </h2><br>  Technically, AlexNet was a convolutional NA.  In this section, I will explain what makes the convolutional neural network (SNS), and why this technology has become crucial for modern pattern recognition algorithms. <br><br>  The previously considered simple handwriting recognition network was completely connected: each neuron of the first layer was an input for each neuron of the second layer.  This structure works quite well on simple tasks with recognition of numbers on images of 28x28 pixels.  But it doesn‚Äôt scale well. <br><br>  In the MNIST handwritten digit database, all characters are centered.  This seriously simplifies learning, since, let's say, the seven will always have several dark pixels at the top and right, and the bottom left corner is always white.  Zero will almost always have a white spot in the middle and dark pixels along the edges.  A simple and fully connected network can recognize such patterns fairly easily. <br><br>  But, let's say, you wanted to create an NA capable of recognizing numbers that can be placed anywhere on a larger image.  A fully connected network will not work just as well with such a task, since it does not have an effective way to recognize similar features of forms located in different parts of the image.  If in your training dataset most of the sevens are located in the upper left corner, then your network will better recognize the sevens in the upper left corner than in any other part of the image. <br><br>  Theoretically, this problem can be solved by ensuring that there are many examples of each digit in each of the possible positions in your set.  But in practice it will be a huge waste of resources.  With increasing image size and network depth, the number of links ‚Äî and the number of weight parameters ‚Äî will grow explosively.  You will need much more training images (and computing power) to achieve adequate accuracy. <br><br>  When a neural network learns to recognize a form located in one place of the image, it should be able to apply this knowledge to recognize the same form in other parts of the image.  SNS provide an elegant solution to this problem. <br><br>  ‚ÄúIt‚Äôs like if you took a stencil and put it on all the places in the image,‚Äù said AI researcher Jai Ten.  - You have a stencil with a dog image, and you first attach it to the upper right corner of the image to see if there is a dog there?  If not, you move the stencil slightly.  And so for the whole image.  No matter where the picture of the dog is.  Stencil will match with it.  You do not need every part of the network to learn its own classification of dogs. ‚Äù <br><br>  Imagine that we took a large image, and smashed into squares 28x28 pixels.  Then we will be able to feed each small square of a fully connected web that recognizes handwriting, which we have studied before.  If at least one of the squares will work output "7", it will be a sign that in general there is a seven in the image.  This is what convolution networks do. <br><br><h2>  How convolution networks worked in AlexNet </h2><br>  In convolutional networks, such "stencils" are known as feature detectors, and the area they are studying is known by the receptive field.  Real feature detectors work with much smaller margins than a square with a side of 28 pixels.  In AlexNet, the feature detectors in the first convolutional layer worked with a receptive field of 11x11 pixels in size.  In subsequent layers, receptive fields were 3-5 units wide. <br><br>  In the process of traversing, the feature detector of the input image produces a feature map: a two-dimensional lattice on which it is noted how strongly the detector was activated in different parts of the image.  Convolutional layers usually have more than one detector, and each of them scans an image in search of different patterns.  AlexNet had 96 sign detectors on the first layer, which produced 96 sign cards. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/94f/1a0/c3d/94f1a0c3deacf7903606f4ecaf040b20.png"><br><br>  To better understand this, consider the visual representation of the patterns studied by each of the 96 detectors of the first AlexNet layer after network training.  There are detectors looking for horizontal or vertical lines, transitions from light to dark, chess patterns and many other forms. <br><br>  A color image is usually represented as a pixel map with three numbers for each pixel: the value of red, green, and blue.  The first layer of AlexNet takes this representation and turns it into a representation using 96 numbers.  Each ‚Äúpixel‚Äù in this image has 96 values, one for each feature detector. <br><br>  In this example, the first of 96 values ‚Äã‚Äãindicates whether some image point matches this pattern: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/260/3fa/933/2603fa9332cbf509d9afd9dc1870e53a.png"><br><br>  The second value indicates whether some image point coincides with this pattern: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a0b/736/b01/a0b736b01ab683e1dfbd229e29904500.png"><br><br>  The third value indicates whether some image point coincides with this pattern: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bd9/a72/09c/bd9a7209cb808f24e18d54327974a0e7.png"><br><br>  And so on for 93 more feature detectors in the first AlexNet layer.  The first layer produces a new image representation, where each pixel is a vector in 96 dimensions (later I will explain that this representation is reduced 4 times). <br><br>  Such is the first layer of AlexNet.  Then there are four more convolutional layers, each of which accepts the output of the previous one as input. <br><br>  As we have seen, the first layer detects basic patterns, such as horizontal and vertical lines, transitions from light to dark and curves.  The second level uses them as a building block for recognizing slightly more complex forms.  For example, the second layer could have a feature detector that finds circles using a combination of the outputs of the first layer feature detectors that find the curves.  The third layer finds even more complex forms, combining features from the second layer.  The fourth and fifth find even more complex patterns. <br><br>  Researchers Matthew Seiler and Rob Fergus published an <a href="https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf">excellent work</a> in 2014, which provides very useful ways to visualize patterns recognized by a five-layer neural network similar to ImageNet. <br><br>  In the next slideshow, taken from their work, each image, except the first, has two halves.  On the right, you will see examples of thumbnails that strongly activated a specific feature detector.  They are collected by nine - and each group corresponds to its detector.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">On the left is a map showing which particular pixels in this miniature are most responsible for the coincidence. This is especially well seen on the fifth layer, because there are feature detectors that react strongly to dogs, logos, wheels, and so on. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/a2c/beb/74e/a2cbeb74e2f71a9b1b84fbea587294b2.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">First layer - simple patterns and forms </font></font></i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/701/ebd/501/701ebd5013d881b6892c66c3fb63d77f.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">the second layer - begin to show fine structure </font></font></i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/34d/283/91a/34d28391a4bb2d0804e15d12992208ba.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">detectors signs on the third layer can recognize more complex shapes, such as the wheels of vehicles, cells and even people silhouettes </font></font></i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/41a/ee2/cb6/41aee2cb61324edc33313a0b01c874b5.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">fourth layer is able to distinguish between complex shapes, such as mugs dogs or bird feet </font></font></i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/04b/96a/baa/04b96abaa00cf8ed2dcb994d56a5af3f.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">fifth layer may recognize very complex shapes</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Looking through the images, you can see how each subsequent layer is able to recognize more and more complex patterns. The first layer recognizes simple patterns that are not similar to anything. The second recognizes textures and simple shapes. By the third layer, recognizable shapes like wheels and red-orange spheres (tomatoes, ladybugs, something else) become visible. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the first layer, the side of the receptive field is 11, and in the later, from three to five. But remember, later layers recognize feature maps generated by earlier layers, so each of their ‚Äúpixels‚Äù means several pixels of the original image. Therefore, the receptive field of each layer includes a larger part of the first image than the previous layers. This is part of the reason that miniatures in later layers look more complicated than in earlier ones.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The fifth, last layer of the network is able to recognize an impressively large range of elements. For example, look at this image that I selected from the top right corner of the image corresponding to the fifth layer: The </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/e82/3fb/895/e823fb8956c1b12d92b32607c41837ec.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">nine pictures on the right may not look like each other. But if you look at the nine heat maps on the left, you will see that this feature detector does not concentrate on the objects in the foreground of the photos. Instead, it concentrates on the grass against the background of each!</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Obviously, a grass detector is useful if one of the categories you are trying to determine is ‚Äúgrass‚Äù, but it can be useful for many other categories. After five convolutional layers, AlexNet has three layers that are fully connected, like our handwriting recognition network. These layers look at each of the feature maps produced by the five convolutional layers, trying to classify the image into one of 1000 possible categories.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So if there is grass in the background, then a wild animal will most likely appear on the image. </font><font style="vertical-align: inherit;">On the other hand, if there is grass in the background, it is less likely to be the image of furniture in the house. </font><font style="vertical-align: inherit;">These and other fifth layer feature detectors provide a lot of information about the likely photo content. </font><font style="vertical-align: inherit;">The last layers of the network synthesize this information to produce a conjecture supported by the facts about what is generally shown in the picture.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> What distinguishes convolutional layers: total input weights </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We have seen that the feature detectors in convolutional layers exhibit impressive pattern recognition, but so far I have not explained how convolutional networks actually work. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The convolutional layer (SS) consists of neurons. They, like any neurons, take a weighted average per input and use the activation function. Parameters are trained using reverse propagation techniques. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But, unlike previous NA, the SS is not fully connected. Each neuron accepts input from a small fraction of the neurons from the previous layer. And, importantly, convolutional neurons have common input weights.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's consider the first neuron of the first SS AlexNet in more detail. The receptive field of this layer has a size of 11x11 pixels, so the first neuron examines the square of 11x11 pixels in one corner of the image. This neuron receives input data from this 121 pixels, and each pixel has three values ‚Äã‚Äã‚Äî red, green, and blue. Therefore, in general, the neuron has 363 input parameters. Like any neuron, this one takes a weighted average of 363 parameters, and applies an activation function to them. And, since the input parameters are 363, the weighting parameters also need 363.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The second neuron of the first layer is similar to the first. He also studies squares of 11x11 pixels, but his receptive field is shifted four pixels from the first. Two fields have an overlap of 7 pixels, so the network does not lose sight of interesting patterns that fall at the junction of two squares. The second neuron also takes 363 parameters describing a 11x11 square, multiplies each of them by weight, adds and applies an activation function. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But instead of using a separate set of 363 weights, the second neuron uses the same weights as the first. The top left pixel of the first neuron uses the same weights as the top left pixel of the second. Therefore, both neurons are looking for the same pattern; just their receptive fields are shifted 4 pixels relative to each other.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Naturally, there are more than two neurons there: 3025 neurons are in the 55x55 grid. Each of them uses the same set of 363 weights as the first two. Together, all neurons form a feature detector, a ‚Äúscanning‚Äù image for the desired pattern, which can be located anywhere. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Remember that the first AlexNet layer has 96 feature detectors. The 3025 neurons I just mentioned constitute one of these 96 detectors. Each of the other 95s is a separate group of 3025 neurons. Each group of 3025 neurons uses a common set of 363 scales - however, for each of the 95 groups it has its own.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SNs are trained using the same backward propagation that is used for fully connected networks, but the convolutional structure makes the learning process more efficient and effective. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúThe use of bundles really helps - the parameters can be reused,‚Äù the MO expert and author Sean Gerrish told us. </font><font style="vertical-align: inherit;">This drastically reduces the number of input weights that a network has to learn, which allows it to produce better results with fewer teaching examples. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Training on one part of the image translates into improved recognition of the same pattern in other parts of the image. </font><font style="vertical-align: inherit;">This allows the network to achieve high performance on a much smaller number of training examples.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> People quickly realized the power of deep convolutional networks </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">AlexNet's work became a sensation in the academic community of the MoD, but its importance was quickly understood in the IT industry. She became particularly interested in Google. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In 2013, Google acquired a startup founded by the authors of AlexNet. The company used this technology to add new photo search feature in Google Photos. ‚ÄúWe took an advanced study and launched it in a little more than six months,‚Äù wrote Chuck Rosenberg of Google. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Meanwhile, in the </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">work of</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 2013, it was described how Google uses the GSS for recognizing addresses from Google Street View photos. ‚ÄúOur system helped us extract nearly 100 million physical addresses from these images,‚Äù the authors wrote.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The researchers found that the effectiveness of the NA increases with increasing depth. ‚ÄúWe found that the effectiveness of this approach grows with increasing depth of the SNS, and the best results show the deepest of all the architectures we have trained,‚Äù wrote the Google Street View team. ‚ÄúOur experiments suggest that deeper architectures can produce greater accuracy, but with a slower increase in efficiency.‚Äù </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So after AlexNet, the networks started to get deeper. The Google team came up with a winning bid in 2014 ‚Äî just two years after AlexNet won in 2012. It was also based on deep SNS, but Goolge used a much deeper network of 22 layers to achieve an error rate in 6.7% - this was a significant improvement compared with 16% for AlexNet.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But at the same time, deeper networks worked better only with larger sets of training data. Therefore, Gerrish said that the ImageNet data set and the competition played a major role in the success of the SNA. Recall that in an ImageNet competition, participants are given a million images each and asked to sort them into 1000 categories. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúIf you have a million images to learn, then each class includes 1000 images,‚Äù said Gerrish. Without such a large data set, he said, ‚Äúyou would have too many parameters for network training.‚Äù</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In recent years, experts have increasingly focused on collecting a huge amount of data for learning deeper and more accurate networks. </font><font style="vertical-align: inherit;">That is why companies developing robomobils concentrate on running along public roads ‚Äî images and videos of these trips are sent to headquarters and used to train corporate NAs.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Computational deep learning boom </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The discovery of the fact that deeper networks and more voluminous data sets can improve the performance of the NA has created an insatiable thirst for ever-increasing computing power. One of the main components of AlexNet‚Äôs success was the idea that matrix operations are used in NA training, which can be effectively performed on well-parallelized graphics processors. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúThe National Assembly is well parallelized,‚Äù said a researcher in the field of Defense Jai Ten. Graphic cards - providing enormous parallel computing power for video games - have proven to be useful for NN as well. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"The central part of the work of the GPU, the very fast matrix multiplication, turned out to be the central part for the work of the National Assembly," said Ten.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">All this was successful for the leading manufacturers of GPU, Nvidia and AMD. Both companies have developed new chips specifically tailored to the needs of MO applications, and now AI applications are responsible for a significant portion of the GPU sales of these companies. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In 2016, Google announced the creation of a special chip, the Tensor Processing Unit (TPU), designed to operate the National Assembly. ‚ÄúAlthough Google considered the possibility of creating special-purpose integrated circuits (ASIC) back in 2006, this situation became urgent in 2013,‚Äù a </font><font style="vertical-align: inherit;">company spokesman </font></font><a href="https://cloud.google.com/blog/products/gcp/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wrote</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> last year. ‚ÄúIt was then that we realized that the rapidly growing NS requirements for computing power may require us to double the number of data centers we have.‚Äù</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">At first, only Google‚Äôs own services had access to TPU, but later the company allowed everyone to use this technology through a cloud computing platform. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Of course, Google is not the only company working on AI chips. </font><font style="vertical-align: inherit;">Just a few examples: in the latest versions of chips for the iPhone </font></font><a href="https://www.wired.com/story/apples-neural-engine-infuses-the-iphone-with-ai-smarts/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">there is a</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚Äúneural core‚Äù optimized for operations with NA. </font><font style="vertical-align: inherit;">Intel is </font></font><a href="https://www.theverge.com/circuitbreaker/2017/10/17/16488414/intel-ai-chips-nervana-neural-network-processor-nnp"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">developing</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> its own line of chips optimized for GO. </font><font style="vertical-align: inherit;">Tesla recently </font></font><a href="https://arstechnica.com/cars/2018/08/tesla-says-its-dumping-nvidia-chips-for-a-homebrew-alternative/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">announced</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> the failure of Nvidia's chips in favor of its own NS chips. </font><font style="vertical-align: inherit;">Amazon is also rumored to be </font></font><a href="https://www.theverge.com/2018/2/12/17004734/amazon-custom-alexa-echo-ai-chips-smart-speaker"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">working</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> on its own AI chips.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Why deep neural networks are hard to understand </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I explained how neural networks work, but did not explain why they work so well. It is rather incomprehensible how exactly an immense amount of matrix calculations allows a computer system to distinguish a jaguar from a cheetah, and an elderberry from a currant. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Perhaps the most remarkable quality of the NA is that they do not. Convolutions allow the NA to understand hyphenation - they can tell whether the picture from the upper right corner of the image is similar to the image in the upper left corner of another image. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But at the same time, the SNA has no idea about geometry. They cannot recognize the similarity of two pictures if they are rotated 45 degrees or doubled. SNA does not try to understand the three-dimensional structure of objects, and can not take into account different lighting conditions.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> But at the same time, NSs can recognize photos of dogs taken both from the front and from the side, and it does not matter whether the dog occupies a small part of the image, or a large one. </font></font> How do they do it?<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It turns out that with enough data available, the statistical brute force approach can do the job. </font><font style="vertical-align: inherit;">SNA is not designed so that it can ‚Äúpresent‚Äù how a certain image would look from another angle or in other conditions, but with a sufficient number of marked examples, it can learn all possible variants of the image by simple repetition. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There is evidence that the visual system of people works in a similar way. </font><font style="vertical-align: inherit;">Look at a couple of pictures - first carefully examine the first one, and then open the second one. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/d43/861/b2d/d43861b2ddeebcec572cab31c9ddb81a.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">First photo</font></font></i> <br><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Second photo</font></font></b> <div class="spoiler_text"><img src="https://habrastorage.org/getpro/habr/post_images/529/451/bde/529451bde2016793fd9cfe4ec092b46a.png"><br></div></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The creator of the image took someone's photo and turned his eyes and mouth upside down. The picture seems relatively normal when you look at it upside down, because the human visual system is used to seeing the eyes and mouths in this position. But if you look at the picture in the correct orientation, it is immediately clear that the face is strangely distorted. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This suggests that the human visual system is based on the same coarse techniques for recognizing patterns as NA. If we look at something that is almost always seen in one orientation - the human eye - we can recognize it much better in its normal orientation.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">NAs recognize images well, using all the context available to them. For example, cars usually drive on roads. Dresses are usually put on the body of a woman or hang in the closet. Planes are usually shot against the sky or they are driving along the runway. Nobody specifically teaches the National Assembly with these correlations, but with a sufficient number of marked examples, the network itself can learn them.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In 2015, researchers from Google tried to better understand the NA, ‚Äúlaunching them backwards‚Äù. Instead of using pictures to teach the NA, they used trained NAs to change the pictures. For example, they started with an image containing random noise, and then gradually changed it so that it strongly activated one of the output neurons of the NA ‚Äî in fact, they asked the NA to ‚Äúdraw‚Äù one of the categories it was taught to recognize. In one interesting case, they forced the NA to generate pictures that activate the NA, trained to recognize dumbbells. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/ece/817/a18/ece817a18e8c2fc63281cb0a1773ac91.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúHere, of course, there are dumbbells, but not a single image of dumbbells seems complete without the presence of muscular roll on it, lifting them,‚Äù wrote researchers from Google.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">At first glance, this looks strange, but in fact it is not so very different from what people do. </font><font style="vertical-align: inherit;">If we see a small or blurred object in the picture, we are looking for a hint in his environment in order to understand what may happen there. </font><font style="vertical-align: inherit;">People obviously argue about the pictures differently, taking advantage of a complex conceptual understanding of the world around them. </font><font style="vertical-align: inherit;">But in the end, the STS is well recognized by the pictures, because they enjoy all the advantages of the entire context depicted on them, and this is not very different from how people do it.</font></font></div><p>Source: <a href="https://habr.com/ru/post/455331/">https://habr.com/ru/post/455331/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../455312/index.html">One-bit full adder on unusual microcircuits</a></li>
<li><a href="../455314/index.html">What is a service network</a></li>
<li><a href="../455316/index.html">We modify the Bluetooth stack to improve the sound on headphones without AAC, aptX and LDAC codecs</a></li>
<li><a href="../455319/index.html">Workflow 3D artist. How not to drown in a ton of information. Part 1</a></li>
<li><a href="../455321/index.html">Home automation do-it-yourself</a></li>
<li><a href="../455335/index.html">Little Python Fun # 3: Poetry</a></li>
<li><a href="../455337/index.html">Who added Python to the latest Windows update?</a></li>
<li><a href="../455341/index.html">What is known about ITIL 4 certification</a></li>
<li><a href="../455343/index.html">Training Cisco 200-125 CCNA v3.0. Day 9. The physical world of switches. Part 2</a></li>
<li><a href="../455345/index.html">Caution Doctor</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>