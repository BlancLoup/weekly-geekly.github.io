<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Neural network optimization methods</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the overwhelming majority of sources of information about neural networks, ‚Äúnow let's educate our network‚Äù means ‚Äúfeed the objective function to th...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Neural network optimization methods</h1><div class="post__text post__text-html js-mediator-article"><p>  In the overwhelming majority of sources of information about neural networks, ‚Äúnow let's educate our network‚Äù means ‚Äúfeed the objective function to the optimizer‚Äù with only the minimum learning speed setting.  It is sometimes said that the weights of the network can be updated not only by stochastic gradient descent, but without any explanation what the other algorithms are notable for and what the mysterious ones mean <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cbeta" alt="\ inline \ beta">  and <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cgamma" alt="\ inline \ gamma">  in their parameters.  Even teachers in machine learning courses often do not focus on this.  I would like to correct the lack of information in runet about various optimizers that you may encounter in modern <a href="http://lasagne.readthedocs.io/en/latest/modules/updates.html">machine learning packages</a> .  I hope my article will be useful to people who want to deepen their understanding of machine learning or even invent something of their own. </p><br><p><img src="https://habrastorage.org/files/93c/2ee/e3f/93c2eee3fad440a8b8204befec4b06bf.jpg" alt="image"></p><br><p>  Under the cut a lot of pictures, including animated gif. </p><a name="habracut"></a><br><p>  The article is aimed at a reader familiar with neural networks.  It is assumed that you already understand the essence of <a href="https://habrahabr.ru/post/271563/">backpropagation</a> and <a href="https://habrahabr.ru/post/272679/">SGD</a> .  I will not go into a rigorous proof of the convergence of the algorithms presented below, but on the contrary, I will try to convey their ideas in simple language and show that the formulas are open for further experiments.  The article lists not all the complexities of machine learning and not all the ways to overcome them. </p><br><h2>  Why do we need tricks </h2><br><p>  Let me remind you what formulas look like for ordinary gradient descent: </p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(1)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%5CDelta%20%5Ctheta%20%3D%20-%5Ceta%20%5Cnabla_%5Ctheta%20J(%20%5Ctheta)" alt="\ Delta \ theta = - \ eta \ nabla_ \ theta J (\ theta)"></div><p></p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(2)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%5Ctheta%20%3D%20%5Ctheta%20%2B%20%5CDelta%20%5Ctheta%20%3D%20%5Ctheta%20-%20%5Ceta%20%5Cnabla_%5Ctheta%20J(%20%5Ctheta)" alt="\ theta = \ theta + \ Delta \ theta = \ theta - \ eta \ nabla_ \ theta J (\ theta)"></div><p></p><br><p>  Where <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Ctheta" alt="\ inline \ theta">  - network settings <img src="https://tex.s2cms.ru/svg/%5Cinline%20J(%20%5Ctheta)" alt="\ inline J (\ theta)">  - the objective function or loss function in the case of machine learning, and <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Ceta" alt="\ inline \ eta">  - learning speed.  It looks amazingly simple, but a lot of the magic is hidden in <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cnabla_%5Ctheta%20J(%20%5Ctheta)" alt="\ inline \ nabla_ \ theta J (\ theta)">  - update the parameters of the output layer is quite simple, but to get to the parameters of the layers behind it, you have to go through nonlinearities, the derivatives of which contribute to.  This is the familiar principle of the reverse propagation of error - backpropagation. </p><br><p>  Explicitly written formulas for updating the scales somewhere in the middle of the network look ugly, because each neuron depends on all the neurons with which it is connected, and those from all the neurons with which they are connected, and so on.  At the same time, even in ‚Äútoy‚Äù neural networks, there can be about 10 layers, and among the <a href="http://image-net.org/challenges/LSVRC/2016/results">networks</a> that keep modern classifications of modern datasets - much, much more.  Each weight is variable in <img src="https://tex.s2cms.ru/svg/%5Cinline%20J(%20%5Ctheta)" alt="\ inline J (\ theta)">  .  Such an incredible amount of degrees of freedom allows you to build very complex mappings, but brings researchers a headache: </p><br><ul><li>  Jam at local minima or saddle points, which for a function of <img src="https://tex.s2cms.ru/svg/%5Cinline%3E%2010%5E6" alt="\ inline &amp; gt;  10 ^ 6">  There may be a lot of variables. </li><li>  The complex landscape of the objective function: the plateau alternates with regions of strong nonlinearity.  The derivative on the plateau is almost zero, and a sudden break, on the contrary, can send us too far. </li><li>  Some parameters are updated much less frequently than others, especially when there are informative, but rare signs in the data, which have a bad effect on the nuances of the generalizing network rule.  On the other hand, giving too much importance to all rarely encountered symptoms in general can lead to retraining. </li><li>  Too small learning rate makes the algorithm converge for a very long time and get stuck in local minima, too large - to ‚Äúfly‚Äù narrow global minima or to completely diverge </li></ul><br><p>  Computational mathematics known advanced algorithms of the second order, which is able to find a good minimum and on a complex landscape, but then the amount of weights hits again.  To use the honest method of the second order "in the forehead," you will have to calculate the Hessian <img src="https://tex.s2cms.ru/svg/%5Cinline%20J(%20%5Ctheta)" alt="\ inline J (\ theta)">  - the matrix of derivatives for each pair of parameters of a pair of parameters (already bad) - and, say, for the Newton method, also the inverse of it.  We have to invent all sorts of tricks to cope with the problems, leaving the task of computationally lifting.  Second-order working optimizers <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3D%25D0%2590%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC_%25D0%259B%25D0%25B5%25D0%25B2%25D0%25B5%25D0%25BD%25D0%25B1%25D0%25B5%25D1%2580%25D0%25B3%25D0%25B0-%25D0%259C%25D0%25B0%25D1%2580%25D0%25BA%25D0%25B2%25D0%25B0%25D1%2580%25D0%25B4%25D1%2582%25D0%25B0">exist</a> , but for now let's concentrate on what we can achieve without considering the second derivatives. </p><br><h2>  Nesterov Accelerated Gradient </h2><br><p>  By itself, the idea of ‚Äã‚Äãmethods with the accumulation of momentum is obviously simple: "If we move for a while in a certain direction, then we probably should go there for some time in the future."  To do this, you need to be able to refer to the recent change history of each parameter.  You can store the latest <img src="https://tex.s2cms.ru/svg/%5Cinline%20n" alt="\ inline n">  copies <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5CDelta%20%5Ctheta" alt="\ inline \ Delta \ theta">  and at each step it is fair to assume an average, but this approach takes too much memory for large <img src="https://tex.s2cms.ru/svg/%5Cinline%20n" alt="\ inline n">  .  Fortunately, we do not need an exact average, but only an estimate, so we use an exponential moving average. </p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(3)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/v_t%20%3D%20%5Cgamma%20v_%7Bt-1%7D%20%2B%20(1-%5Cgamma)%20x" alt="v_t = \ gamma v_ {t-1} + (1- \ gamma) x"></div><p></p><br><p>  To accumulate something, we will multiply the accumulated value by the conservation factor <img src="https://tex.s2cms.ru/svg/%5Cinline%200%20%3C%20%5Cgamma%20%3C%201%20" alt="\ inline 0 &amp; lt;  \ gamma &amp; lt;  one">  and add another value multiplied by <img src="https://tex.s2cms.ru/svg/%5Cinline%201-%5Cgamma" alt="\ inline 1- \ gamma">  .  The closer <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cgamma" alt="\ inline \ gamma">  to one, the larger the accumulation window and the stronger the smoothing is history <img src="https://tex.s2cms.ru/svg/%5Cinline%20x" alt="\ inline x">  begins to influence more than every next <img src="https://tex.s2cms.ru/svg/%5Cinline%20x" alt="\ inline x">  .  If a <img src="https://tex.s2cms.ru/svg/%5Cinline%20x%3D0" alt="\ inline x = 0">  from a certain moment <img src="https://tex.s2cms.ru/svg/%5Cinline%20v_t" alt="\ inline v_t">  decay exponentially exponentially, hence the name.  We apply the exponential running average to accumulate the gradient of the objective function of our network: </p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(4)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Av_t%20%3D%20%5Cgamma%20v_%7Bt-1%7D%20%2B%20%5Ceta%20%5Cnabla_%5Ctheta%20J(%20%5Ctheta)%0A" alt="v_t = \ gamma v_ {t-1} + \ eta \ nabla_ \ theta J (\ theta)"></div><p></p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(5)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Ctheta%20%3D%20%5Ctheta%20-%20v_t%0A" alt="\ theta = \ theta - v_t"></div><p></p><br><p>  Where <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cgamma" alt="\ inline \ gamma">  usually takes order <img src="https://tex.s2cms.ru/svg/%5Cinline%200.9" alt="\ inline 0.9">  .  note that <img src="https://tex.s2cms.ru/svg/%5Cinline%201-%5Cgamma" alt="\ inline 1- \ gamma">  not lost, but included in <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Ceta" alt="\ inline \ eta">  ;  Sometimes you can find a variant of the formula with an explicit multiplier.  The smaller <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cgamma" alt="\ inline \ gamma">  , the more the algorithm behaves like a normal SGD.  To get a popular physical interpretation of the equations, imagine a ball rolling on a hilly surface.  If at the moment <img src="https://tex.s2cms.ru/svg/%5Cinline%20t" alt="\ inline t">  under the ball there was a nonzero bias ( <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cnabla_%5Ctheta%20J(%20%5Ctheta)" alt="\ inline \ nabla_ \ theta J (\ theta)">  ), and then he hit the plateau, he will continue to roll on this plateau anyway.  Moreover, the ball will continue to move a couple of updates in the same direction, even if the bias has changed to the opposite.  However, viscous friction acts on the ball and every second it loses <img src="https://tex.s2cms.ru/svg/%5Cinline%201-%5Cgamma" alt="\ inline 1- \ gamma">  its speed.  Here is what the accumulated impulse looks like for different <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cgamma" alt="\ inline \ gamma">  (hereinafter, epochs are plotted along the X axis, and the <em>gradient</em> value and the accumulated values ‚Äã‚Äãare along the Y axis): </p><br><p><img src="https://habrastorage.org/files/1e3/740/c6f/1e3740c6fbe242f58a9c3624d333cb67.png" alt="image"><br><img src="https://habrastorage.org/files/36e/8a9/902/36e8a99023e84d9abc302cf4f6d36f7a.png" alt="image"><br><img src="https://habrastorage.org/files/879/ac0/be5/879ac0be573343fca6e97bec91e3a429.png" alt="image"><br><img src="https://habrastorage.org/files/e2d/852/6c8/e2d8526c8af046fea2c159859e49f90e.png" alt="image"></p><br><p>  Note that the accumulated in <img src="https://tex.s2cms.ru/svg/%5Cinline%20v_t" alt="\ inline v_t">  the value can greatly exceed the value of each of <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Ceta%20%5Cnabla_%5Ctheta%20J(%20%5Ctheta)" alt="\ inline \ eta \ nabla_ \ theta J (\ theta)">  .  A simple accumulation of momentum already gives a good result, but Nesterov goes further and applies the well-known idea in computational mathematics: looking ahead along the update vector.  Since we're still going to shift to <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cgamma%20v_%7Bt-1%7D" alt="\ inline \ gamma v_ {t-1}">  then let's calculate the loss function gradient not at the point <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Ctheta" alt="\ inline \ theta">  and in <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Ctheta%20-%20%5Cgamma%20v_%7Bt-1%7D" alt="\ inline \ theta - \ gamma v_ {t-1}">  .  From here: </p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(6)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Av_t%20%3D%20%5Cgamma%20v_%7Bt-1%7D%20%2B%20%5Ceta%20%5Cnabla_%5Ctheta%20J(%20%5Ctheta%20-%20%5Cgamma%20v_%7Bt-1%7D%20)%0A" alt="v_t = \ gamma v_ {t-1} + \ eta \ nabla_ \ theta J (\ theta - \ gamma v_ {t-1})"></div><p></p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(7)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Ctheta%20%3D%20%5Ctheta%20-%20v_t%0A" alt="\ theta = \ theta - v_t"></div><p></p><br><p>  Such a change allows you to ‚Äúroll‚Äù faster if, aside, where we are going, the derivative increases, and more slowly, if vice versa.  This is especially evident for <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cgamma%3D0.975" alt="\ inline \ gamma = 0.975">  for a graph with a sine. </p><br><p><img src="https://habrastorage.org/files/f89/b7f/bd3/f89b7fbd359f4353a7da3e31658b6406.png" alt="image"><br><img src="https://habrastorage.org/files/edb/f80/571/edbf80571550485fa690e97b060b643e.png" alt="image"></p><br><p>  Looking ahead can play a cruel joke on us if you set too large <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cgamma" alt="\ inline \ gamma">  and <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Ceta" alt="\ inline \ eta">  : we look so far as to miss the areas with the opposite gradient sign: </p><br><p><img src="https://habrastorage.org/files/0e4/035/13b/0e403513bbf94efdb7ef62039a038951.png" alt="image"></p><br><p>  However, sometimes this behavior may be desirable.  Once again I draw your attention to the idea - looking ahead - and not to execution.  Nesterov's method (6) is the most obvious option, but not the only one.  For example, you can use another technique from computational mathematics ‚Äî stabilization of the gradient by averaging over several points along the line along which we move.  So to speak: </p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(8)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Av_t%20%3D%20%5Cgamma%20v_%7Bt-1%7D%20%2B%20%5Cfrac%7B%5Ceta%7D%7B3%7D%20%5Cbig(%20%5Cnabla_%5Ctheta%20J(%20%5Ctheta%20-%20%5Cfrac%7B%5Cgamma%20v_%7Bt-1%7D%7D%7B3%7D%20)%20%2B%20%5Cnabla_%5Ctheta%20J(%20%5Ctheta%20-%20%5Cfrac%7B2%5Cgamma%20v_%7Bt-1%7D%7D%7B3%7D%20)%20%2B%20%5Cnabla_%5Ctheta%20J(%20%5Ctheta%20-%20%5Cgamma%20v_%7Bt-1%7D%20)%20%5Cbig)%0A" alt="v_t = \ gamma v_ {t-1} + \ frac {\ eta} {3} \ big (\ nabla_ \ theta J (\ theta - \ frac {\ gamma v_ {t-1}} {3}) + \ nabla_ \ theta J (\ theta - \ frac {2 \ gamma v_ {t-1}} {3}) + \ nabla_ \ theta J (\ theta - \ gamma v_ {t-1}) \ big)"></div><p></p><br><p>  Or so: </p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(9)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Av_t%20%3D%20%5Cgamma%20v_%7Bt-1%7D%20%2B%20%5Cfrac%7B%5Ceta%7D%7B7%7D%20%5Cbig(%20%5Cnabla_%5Ctheta%20J(%20%5Ctheta%20-%20%5Cfrac%7B%5Cgamma%20v_%7Bt-1%7D%7D%7B2%7D%20)%20%2B%202%5Cnabla_%5Ctheta%20J(%20%5Ctheta%20-%20%5Cfrac%7B3%5Cgamma%20v_%7Bt-1%7D%7D%7B4%7D%20)%20%2B%204%5Cnabla_%5Ctheta%20J(%20%5Ctheta%20-%20%5Cgamma%20v_%7Bt-1%7D%20)%20%5Cbig)%0A" alt="v_t = \ gamma v_ {t-1} + \ frac {\ eta} {7} \ big (\ nabla_ \ theta J (\ theta - \ frac {\ gamma v_ {t-1}} {2}) + 2 \ nabla_ \ theta J (\ theta - \ frac {3 \ gamma v_ {t-1}} {4}) + 4 \ nabla_ \ theta J (\ theta - \ gamma v_ {t-1}) \ big)"></div><p></p><br><p>  Such a technique can help in the case of noisy target functions. </p><br><p>  We will not manipulate the argument of the objective function in the subsequent methods (although, of course, no one bothers to experiment).  Further, for brevity </p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(10)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Ag_t%20%5Cequiv%20%5Cnabla_%5Ctheta%20J(%20%5Ctheta_t%20)%20%0A" alt="g_t \ equiv \ nabla_ \ theta J (\ theta_t)"></div><p></p><br><h2>  Adagrad </h2><br><p>  How many methods work with the accumulation of momentum  Let us turn to more interesting optimization algorithms.  Let's start with a relatively simple <a href="http://jmlr.org/papers/v12/duchi11a.html">Adagrad</a> - adaptive gradient. </p><br><p>  Some signs may be extremely informative, but rarely occur.  Exotic high-paying profession, a fancy word in the spam database - they will easily drown in the noise of all other updates.  It is not only about rarely encountered input parameters.  Say, you may well encounter rare graphic patterns that even become a sign only after passing through several layers of a convolutional network.  It would be nice to be able to update the parameters with an eye on how typical they sign.  To achieve this is not difficult: let's store for each network parameter the sum of the squares of its updates.  It will act as a proxy for typicality: if the parameter belongs to a chain of frequently activated neurons, it is constantly pulled back and forth, which means the amount quickly accumulates.  Rewrite the update formula like this: </p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(11)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0AG_%7Bt%7D%20%3D%20G_%7Bt%7D%20%2B%20g_%7Bt%7D%5E2%0A" alt="G_ {t} = G_ {t} + g_ {t} ^ 2"></div><p></p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(12)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%5Ctheta_%7Bt%2B1%7D%20%3D%20%5Ctheta_%7Bt%7D%20-%20%5Cfrac%7B%5Ceta%7D%7B%5Csqrt%7BG_%7Bt%7D%20%2B%20%5Cepsilon%7D%7D%20g_%7Bt%7D%0A" alt="\ theta_ {t + 1} = \ theta_ {t} - \ frac {\ eta} {\ sqrt {G_ {t} + \ epsilon}} g_ {t}"></div><p></p><br><p>  Where <img src="https://tex.s2cms.ru/svg/%5Cinline%20G_%7Bt%7D" alt="\ inline G_ {t}">  - the sum of the squares of the updates, and <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cepsilon" alt="\ inline \ epsilon">  - smoothing parameter necessary to avoid dividing by 0. The parameter that was frequently updated in the past is large <img src="https://tex.s2cms.ru/svg/%5Cinline%20G_t" alt="\ inline G_t">  , is the big denominator in (12).  Parameter changed only one or two will be updated in full force. <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cepsilon" alt="\ inline \ epsilon">  take order <img src="https://tex.s2cms.ru/svg/%5Cinline%2010%5E%7B-6%7D" alt="\ inline 10 ^ {- 6}">  or <img src="https://tex.s2cms.ru/svg/%5Cinline%2010%5E%7B-8%7D" alt="\ inline 10 ^ {- 8}">  for a completely aggressive update, but, as can be seen from the graphs, this plays a role only at the beginning, towards the middle, the training begins to outweigh <img src="https://tex.s2cms.ru/svg/%5Cinline%20G_t" alt="\ inline G_t">  : </p><br><p><img src="https://habrastorage.org/files/b8f/4a9/f7d/b8f4a9f7d49e481cb5cc185d2f31c877.png" alt="image"><br><img src="https://habrastorage.org/files/84d/962/020/84d9620206f6481ca12c247407d0b348.png" alt="image"></p><br><p>  So, the idea of ‚Äã‚ÄãAdagrad is to use <em>something</em> that would reduce updates for the elements that we update so often.  Nobody forces us to use this particular formula, therefore Adagrad is sometimes called a <em>family of</em> algorithms.  Let's say we can remove the root or accumulate not the squares of updates, but their modules, or even replace the multiplier with something like <img src="https://tex.s2cms.ru/svg/%5Cinline%20e%5E%7B-G_%7Bt%7D%7D" alt="\ inline e ^ {- G_ {t}}">  . </p><br><p>  (Another thing is that this requires experimentation. If you remove the root, updates will start to decrease too quickly, and the algorithm will get worse) </p><br><p>  Another advantage of Adagrad is the absence of the need to accurately select the speed of learning.  It is enough to set it moderately large to provide a good margin, but not so huge that the algorithm diverges.  In fact, we automatically get the decay rate of learning (learning rate decay). </p><br><h2>  RMSProp and Adadelta </h2><br><p>  The disadvantage of Adagrad is that <img src="https://tex.s2cms.ru/svg/%5Cinline%20G_%7Bt%7D" alt="\ inline G_ {t}">  in (12) it can increase as much as necessary, which after a while leads to too small updates and paralysis of the algorithm.  RMSProp and Adadelta are designed to correct this flaw. </p><br><p>  Modifying the idea of ‚Äã‚ÄãAdagrad: we are still going to update less weight, which are updated too often, but instead of the total amount of updates, we will use the history-averaged square of the gradient.  Again we use the exponentially decaying running average. <br>  (four).  Let be <img src="https://tex.s2cms.ru/svg/%5Cinline%20E%5Bg%5E2%5D_t" alt="\ inline E [g ^ 2] _t">  - running average at the moment <img src="https://tex.s2cms.ru/svg/%5Cinline%20t" alt="\ inline t"></p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(13)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0AE%5Bg%5E2%5D_t%20%3D%20%5Cgamma%20E%5Bg%5E2%5D_%7Bt-1%7D%20%2B%20(1%20-%20%5Cgamma)%20g%5E2_t%0A" alt="E [g ^ 2] _t = \ gamma E [g ^ 2] _ {t-1} + (1 - \ gamma) g ^ 2_t"></div><p></p><br><p>  then instead of (12) we get </p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(14)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Ctheta_%7Bt%2B1%7D%20%3D%20%5Ctheta_%7Bt%7D%20-%20%5Cfrac%7B%5Ceta%7D%7B%5Csqrt%7BE%5Bg%5E2%5D_t%20%2B%20%5Cepsilon%7D%7D%20g_%7Bt%7D%0A" alt="\ theta_ {t + 1} = \ theta_ {t} - \ frac {\ eta} {\ sqrt {E [g ^ 2] _t + \ epsilon}} g_ {t}"></div><p></p><br><p>  The denominator is the root of the mean squares of the gradients, hence RMSProp - root mean square propagation </p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(15)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0ARMS%5Bg%5D_t%20%3D%20%5Csqrt%7BE%5Bg%5E2%5D_t%20%2B%20%5Cepsilon%7D%0A" alt="RMS [g] _t = \ sqrt {E [g ^ 2] _t + \ epsilon}"></div><p></p><br><p><img src="https://habrastorage.org/files/bea/afe/eb2/beaafeeb20314c839aad1c07e0b9a36e.png" alt="image"><br><img src="https://habrastorage.org/files/ee3/9a7/bff/ee39a7bff76241519ae209f184b6b981.png" alt="image"><br><img src="https://habrastorage.org/files/afd/35d/400/afd35d400f734de99b52344cfcfedb05.png" alt="image"></p><br><p>  Notice how the refresh rate is restored on the chart with long teeth for different <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cgamma" alt="\ inline \ gamma">  .  Also compare the graphs with the meander for Adagrad and RMSProp: in the first case, the updates are reduced to zero, and in the second - they reach a certain level. </p><br><p>  That's the whole RMSProp.  <a href="">Adadelta differs</a> from it in that we add to the numerator (14) the stabilizing term proportional <img src="https://tex.s2cms.ru/svg/%5Cinline%20RMS" alt="\ inline RMS">  from <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5CDelta%5Ctheta_t" alt="\ inline \ Delta \ theta_t">  .  On the move <img src="https://tex.s2cms.ru/svg/%5Cinline%20t" alt="\ inline t">  we don't know the meaning yet <img src="https://tex.s2cms.ru/svg/%5Cinline%20RMS%5B%5CDelta%20%5Ctheta%5D_%7Bt%7D" alt="\ inline RMS [\ Delta \ theta] _ {t}">  therefore, the parameters are updated in three stages, not two: we first accumulate the square of the gradient, then update <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Ctheta" alt="\ inline \ theta">  after which we update <img src="https://tex.s2cms.ru/svg/%5Cinline%20RMS%5B%5CDelta%20%5Ctheta%5D" alt="\ inline RMS [\ Delta \ theta]">  . </p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(16)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5CDelta%20%5Ctheta%20%3D%20-%5Cfrac%7BRMS%5B%5CDelta%20%5Ctheta%5D_%7Bt-1%7D%7D%7BRMS%5Bg%5D_%7Bt%7D%7Dg_%7Bt%7D%0A" alt="\ Delta \ theta = - \ frac {RMS [\ Delta \ theta] _ {t-1}} {RMS [g] _ {t}} g_ {t}"></div><p></p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(17)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Ctheta_%7Bt%2B1%7D%20%3D%20%5Ctheta_%7Bt%7D%20-%20%5Cfrac%7BRMS%5B%5CDelta%20%5Ctheta%5D_%7Bt-1%7D%7D%7BRMS%5Bg%5D_%7Bt%7D%7Dg_%7Bt%7D%0A" alt="\ theta_ {t + 1} = \ theta_ {t} - \ frac {RMS [\ Delta \ theta] _ {t-1}} {RMS [g] _ {t}} g_ {t}"></div><p></p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(18)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0AE%5B%5CDelta%20%5Ctheta%5E2%5D_t%20%3D%20%5Cgamma%20E%5B%5CDelta%20%5Ctheta%5E2%5D_%7Bt-1%7D%20%2B%20(1%20-%20%5Cgamma)%20%5CDelta%20%5Ctheta%5E2_t%0A" alt="E [\ Delta \ theta ^ 2] _t = \ gamma E [\ Delta \ theta ^ 2] _ {t-1} + (1 - \ gamma) \ Delta \ theta ^ 2_t"></div><p></p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(19)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0ARMS%5B%5CDelta%20%5Ctheta%5D_%7Bt%7D%20%3D%20%5Csqrt%7BE%5B%5CDelta%20%5Ctheta%5E2%5D_t%20%2B%20%5Cepsilon%7D%0A" alt="RMS [\ Delta \ theta] _ {t} = \ sqrt {E [\ Delta \ theta ^ 2] _t + \ epsilon}"></div><p></p><br><p>  Such a change is made from considerations that the dimensions <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Ctheta" alt="\ inline \ theta">  and <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5CDelta%20%5Ctheta" alt="\ inline \ Delta \ theta">  must match.  Notice that the learning rate has no dimension, which means that in all the algorithms before that, we added the dimensional quantity with the dimensionless one.  Physicists in this place will be horrified, and we shrug our shoulders: it works the same. </p><br><p>  Note that we need a nonzero <img src="https://tex.s2cms.ru/svg/%5Cinline%20RMS%5B%5CDelta%20%5Ctheta%5D_%7B-1%7D" alt="\ inline RMS [\ Delta \ theta] _ {- 1}">  for the first step, otherwise all subsequent <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5CDelta%20%5Ctheta" alt="\ inline \ Delta \ theta">  and therefore <img src="https://tex.s2cms.ru/svg/%5Cinline%20RMS%5B%5CDelta%20%5Ctheta%5D_%7Bt%7D" alt="\ inline RMS [\ Delta \ theta] _ {t}">  will be zero.  But we solved this problem even earlier, adding to <img src="https://tex.s2cms.ru/svg/%5Cinline%20RMS" alt="\ inline RMS"><img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cepsilon" alt="\ inline \ epsilon">  .  Another thing is that without an obvious big <img src="https://tex.s2cms.ru/svg/%5Cinline%20RMS%5B%5CDelta%20%5Ctheta%5D_%7B-1%7D" alt="\ inline RMS [\ Delta \ theta] _ {- 1}">  we get the <em>opposite</em> behavior of Adagrad and RMSProp: we will be stronger (to a certain extent) update weights that are used <em>more often</em> .  After all, now to <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5CDelta%20%5Ctheta" alt="\ inline \ Delta \ theta">  became significant, the parameter must accumulate a large amount in the numerator of the fraction. </p><br><p>  Here are the graphics for the zero starting <img src="https://tex.s2cms.ru/svg/%5Cinline%20RMS%5B%5CDelta%20%5Ctheta%5D" alt="\ inline RMS [\ Delta \ theta]">  : </p><br><p><img src="https://habrastorage.org/files/e98/8d0/a38/e988d0a38f974992bfdc0276e61c745e.png" alt="image"><br><img src="https://habrastorage.org/files/52c/323/a75/52c323a7557840528b899efd0ddf75a3.png" alt="image"></p><br><p>  But for the big one: </p><br><p><img src="https://habrastorage.org/files/eee/fe2/280/eeefe228078e4626894e3c8db731fa62.png" alt="image"><br><img src="https://habrastorage.org/files/c88/fc6/78f/c88fc678febe4e2a88bae09ac82230ba.png" alt="image"></p><br><p>  However, it seems, the authors of the algorithm and sought such an effect.  For RMSProp and Adadelta, as well as for Adagrad, it is not necessary to select the learning rate very precisely - just a rough value.  Usually advised to start trimming. <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Ceta" alt="\ inline \ eta">  c <img src="https://tex.s2cms.ru/svg/%5Cinline%200.1%20-%201" alt="\ inline 0.1 - 1">  a <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cgamma" alt="\ inline \ gamma">  so leave <img src="https://tex.s2cms.ru/svg/%5Cinline%200.9" alt="\ inline 0.9">  .  The closer <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cgamma" alt="\ inline \ gamma">  to <img src="https://tex.s2cms.ru/svg/%5Cinline%201" alt="\ inline 1">  the longer RMSProp and Adadelta with more <img src="https://tex.s2cms.ru/svg/%5Cinline%20RMS%5B%5CDelta%20%5Ctheta%5D_%7B-1%7D" alt="\ inline RMS [\ Delta \ theta] _ {- 1}">  It will greatly update the little used weight.  If <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cgamma%20%5Capprox%201" alt="\ inline \ gamma \ approx 1">  and <img src="https://tex.s2cms.ru/svg/%5Cinline%20RMS%5B%5CDelta%20%5Ctheta%5D_%7B-1%7D%20%3D%200" alt="\ inline RMS [\ Delta \ theta] _ {- 1} = 0">  , the Adadelta will treat the rarely used weights for a long time ‚Äúwith distrust‚Äù.  The latter can lead to paralysis of the algorithm, and can cause intentionally "greedy" behavior, when the algorithm first updates the neurons that encode the best signs. </p><br><h2>  Adam </h2><br><p>  <a href="">Adam</a> - adaptive moment estimation, one more optimization algorithm.  He combines the idea of ‚Äã‚Äãthe accumulation of movement and the idea of ‚Äã‚Äãa weaker updating of the scales for typical signs.  Remember again (4): </p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(20)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Am_t%20%3D%20%5Cbeta_1%20m_%7Bt-1%7D%20%2B%20(1%20-%20%5Cbeta_1)%20g_t%0A" alt="m_t = \ beta_1 m_ {t-1} + (1 - \ beta_1) g_t"></div><p></p><br><p>  Adam differs from Nesterov in that we accumulate not <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5CDelta%20%5Ctheta" alt="\ inline \ Delta \ theta">  , and gradient values, although this is a purely cosmetic change, see (23).  In addition, we want to know how often the gradient changes.  The authors of the algorithm proposed for this to evaluate also the average non-centered variance: </p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(21)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Av_t%20%3D%20%5Cbeta_2%20v_%7Bt-1%7D%20%2B%20(1%20-%20%5Cbeta_2)%20g_t%5E2%0A" alt="v_t = \ beta_2 v_ {t-1} + (1 - \ beta_2) g_t ^ 2"></div><p></p><br><p>  It is easy to notice that this is already familiar to us. <img src="https://tex.s2cms.ru/svg/%5Cinline%20E%5Bg%5E2%5D_t" alt="\ inline E [g ^ 2] _t">  , so in fact there is no difference from RMSProp. </p><br><p>  An important difference is in initial calibration. <img src="https://tex.s2cms.ru/svg/%5Cinline%20m_t" alt="\ inline m_t">  and <img src="https://tex.s2cms.ru/svg/%5Cinline%20v_t" alt="\ inline v_t">  : they suffer from the same problem as <img src="https://tex.s2cms.ru/svg/%5Cinline%20E%5Bg%5E2%5D_t" alt="\ inline E [g ^ 2] _t">  in RMSProp: if you set a zero initial value, then they will accumulate for a long time, especially with a large accumulation window ( <img src="https://tex.s2cms.ru/svg/%5Cinline%200%20%5Cll%20%5Cbeta_1%20%3C%201" alt="\ inline 0 \ ll \ beta_1 &amp; lt; one">  , <img src="https://tex.s2cms.ru/svg/%5Cinline%200%20%5Cll%20%5Cbeta_2%20%3C%201" alt="\ inline 0 \ ll \ beta_2 &amp; lt; one">  ), and some initial values ‚Äã‚Äãare two more hyperparameters.  No one wants two more hyperparameters, so we artificially increase <img src="https://tex.s2cms.ru/svg/%5Cinline%20m_t" alt="\ inline m_t">  and <img src="https://tex.s2cms.ru/svg/%5Cinline%20v_t" alt="\ inline v_t">  in the first steps (approximately <img src="https://tex.s2cms.ru/svg/%5Cinline%200%20%3C%20t%20%3C%2010" alt="\ inline 0 &amp; lt; t &amp; lt; ten">  for <img src="https://tex.s2cms.ru/svg/%5Cinline%20m_t" alt="\ inline m_t">  and <img src="https://tex.s2cms.ru/svg/%5Cinline%200%20%3C%20t%20%3C%201000" alt="\ inline 0 &amp; lt; t &amp; lt; 1000">  for <img src="https://tex.s2cms.ru/svg/%5Cinline%20v_t" alt="\ inline v_t">  ) </p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(22)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Chat%7Bm%7D_t%20%3D%20%5Cfrac%7Bm_t%7D%7B1%20-%20%5Cbeta%5Et_1%7D%2C%20%5C%3B%0A%5Chat%7Bv%7D_t%20%3D%20%5Cfrac%7Bv_t%7D%7B1%20-%20%5Cbeta%5Et_2%7D%0A" alt="\ hat {m} _t = \ frac {m_t} {1 - \ beta ^ t_1}, \; \ hat {v} _t = \ frac {v_t} {1 - \ beta ^ t_2}"></div><p></p><br><p>  In summary, the update rule is: </p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(23)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Ctheta_%7Bt%2B1%7D%20%3D%20%5Ctheta_%7Bt%7D%20-%20%5Cdfrac%7B%5Ceta%7D%7B%5Csqrt%7B%5Chat%7Bv%7D_t%20%2B%20%5Cepsilon%7D%7D%20%5Chat%7Bm%7D_t%0A" alt="\ theta_ {t + 1} = \ theta_ {t} - \ dfrac {\ eta} {\ sqrt {\ hat {v} _t + \ epsilon}} \ hat {m} _t"></div><p></p><br><p><img src="https://habrastorage.org/files/b8d/f90/277/b8df902776274196b466d31f5bf6c198.png" alt="image"><br><img src="https://habrastorage.org/files/b8a/3ca/8e7/b8a3ca8e7aa54197a8f2aad522c15459.png" alt="image"><br><img src="https://habrastorage.org/files/f8a/636/157/f8a636157aaf40c684ad50fc44d0114e.png" alt="image"></p><br><p>  Here you should take a close look at how quickly the values ‚Äã‚Äãof updates synchronized on the first teeth of graphs with rectangles and on the smoothness of the update curve on a graph with a sine ‚Äî we received it ‚Äúfor free‚Äù.  With recommended setting <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cbeta_1" alt="\ inline \ beta_1">  on the graph with spikes, it is clear that sudden gradient bursts do not cause an instantaneous response in the accumulated value, so a well-tuned Adam does not need gradient clipping. </p><br><p>  The authors of the algorithm derive (22) by expanding the recursive formulas (20) and (21).  For example, for <img src="https://tex.s2cms.ru/svg/%5Cinline%20v_t" alt="\ inline v_t">  : </p><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0AE%5Bv_t%5D%20%26%3D%20E%5CBigg%5B%20(1%20-%20%5Cbeta_2)%20%5Csum_%7Bi%3D1%7D%5E%7Bt%7D%7B%5Cbeta_2%5E%7Bt-i%7Dg_i%5E2%7D%20%5CBigg%5D%0A" alt="E [v_t] &amp; amp; = E \ Bigg [(1 - \ beta_2) \ sum_ {i = 1} ^ {t} {\ beta_2 ^ {t-i} g_i ^ 2} \ Bigg]"></div><p></p><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%3D%20E%5Bg_t%5E2%5D%20(1%20-%20%5Cbeta_2)%20%5Csum_%7Bi%3D1%7D%5E%7Bt%7D%5Cbeta_2%5E%7Bt-i%7D%20%2B%20%5Czeta%0A" alt="= E [g_t ^ 2] (1 - \ beta_2) \ sum_ {i = 1} ^ {t} \ beta_2 ^ {t-i} + \ zeta"></div><p></p><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%3D%20E%5Bg_t%5E2%5D%20(1%20-%20%5Cbeta_2)%20%5Cfrac%7B1%20-%20%5Cbeta_2%5Et%7D%7B1%20-%20%5Cbeta_2%7D%20%2B%20%5Czeta%0A" alt="= E [g_t ^ 2] (1 - \ beta_2) \ frac {1 - \ beta_2 ^ t} {1 - \ beta_2} + \ zeta"></div><p></p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(24)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%3D%20E%5Bg_t%5E2%5D(1%20-%20%5Cbeta_2%5Et)%20%2B%20%5Czeta%0A" alt="= E [g_t ^ 2] (1 - \ beta_2 ^ t) + \ zeta"></div><p></p><br><p>  Term <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Czeta" alt="\ inline \ zeta">  close to <img src="https://tex.s2cms.ru/svg/%5Cinline%200" alt="\ inline 0">  with stationary distribution <img src="https://tex.s2cms.ru/svg/%5Cinline%20p(g)" alt="\ inline p (g)">  that is not true in cases practically of interest to us.  but we still move the bracket with <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cbeta_2%5Et" alt="\ inline \ beta_2 ^ t">  to the left.  Informally, one can imagine that with <img src="https://tex.s2cms.ru/svg/%5Cinline%20t%3D0" alt="\ inline t = 0">  we have an endless history of identical updates: </p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(25)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Chat%7Bv%7D_1%20%3D%20v_%7B1%7D%20%2B%20%5Cbeta_2%20v_%7B1%7D%20%2B%20%5Cbeta_2%5E2%20v_%7B1%7D%20%2B%20%5Cldots%20%3D%20%5Cfrac%7Bv_%7B1%7D%7D%7B1%20-%20%5Cbeta_2%7D%0A" alt="\ hat {v} _1 = v_ {1} + \ beta_2 v_ {1} + \ beta_2 ^ 2 v_ {1} + \ ldots = \ frac {v_ {1}} {1 - \ beta_2}"></div><p></p><br><p>  When we get closer to the correct value <img src="https://tex.s2cms.ru/svg/%5Cinline%20v" alt="\ inline v">  , we force the ‚Äúvirtual‚Äù part of the series to decay faster: </p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(26)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Chat%7Bv%7D_2%20%3D%20v_%7B2%7D%20%2B%20%5Cbeta_2%5E2%20v_%7B2%7D%20%2B%20%5Cbeta_2%5E4%20v_%7B2%7D%20%2B%20%5Cldots%20%3D%20%5Cfrac%7Bv_%7B2%7D%7D%7B1%20-%20%5Cbeta_2%5E2%7D%0A" alt="\ hat {v} _2 = v_ {2} + \ beta_2 ^ 2 v_ {2} + \ beta_2 ^ 4 v_ {2} + \ ldots = \ frac {v_ {2}} {1 - \ beta_2 ^ 2}"></div><p></p><br><p>  Adam authors propose default values. <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cbeta_1%20%3D%200.9%2C%20%5Cbeta_2%20%3D%200.999%2C%20%5Cepsilon%20%3D%2010%5E%7B-8%7D" alt="\ inline \ beta_1 = 0.9, \ beta_2 = 0.999, \ epsilon = 10 ^ {- 8}">  and argue that the algorithm performs better or about the same as all previous algorithms on a wide range of datasets due to the initial calibration.  Notice, again, that equations (22) are not set in stone.  We have some theoretical justification for why attenuation should look like this, but no one forbids experimenting with calibration formulas.  In my opinion, here it is simply suggested to apply peering forward, as in the Nesterov method. </p><br><h2>  Adamax </h2><br><p>  Adamax is just such an experiment proposed in the same article.  Instead of dispersion in (21), we can assume the inertial moment of the distribution of gradients of arbitrary <img src="https://tex.s2cms.ru/svg/%5Cinline%20p" alt="\ inline p">  .  This can lead to instability to the calculations.  However the case <img src="https://tex.s2cms.ru/svg/%5Cinline%20p" alt="\ inline p">  tending to infinity works surprisingly well. </p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(27)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Av_t%20%3D%20%5Cbeta_2%5Ep%20v_%7Bt-1%7D%20%2B%20(1%20-%20%5Cbeta_2%5Ep)%20%7Cg_t%7C%5Ep%0A" alt="v_t = \ beta_2 ^ p v_ {t-1} + (1 - \ beta_2 ^ p) | g_t | ^ p"></div><p></p><br><p>  Notice that instead <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cbeta_2" alt="\ inline \ beta_2">  using the appropriate dimension <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cbeta_2%5Ep" alt="\ inline \ beta_2 ^ p">  .  In addition, note that to use in the Adam formulas the value obtained in (27), you need to extract the root from it: <img src="https://tex.s2cms.ru/svg/%5Cinline%20u_t%20%3D%20v_t%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D" alt="\ inline u_t = v_t ^ {\ frac {1} {p}}">  .  We derive a decisive rule in return (21), taking <img src="https://tex.s2cms.ru/svg/%5Cinline%20p%20%5Crightarrow%20%5Cinfty" alt="\ inline p \ rightarrow \ infty">  by unfolding under the root <img src="https://tex.s2cms.ru/svg/%5Cinline%20v_t" alt="\ inline v_t">  with the help of (27): </p><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Au_t%20%3D%20%5Clim_%7Bp%20%5Crightarrow%20%5Cinfty%7D%7B%20v_t%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D%20%7D%0A" alt="u_t = \ lim_ {p \ rightarrow \ infty} {v_t ^ {\ frac {1} {p}}}"></div><p></p><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%20%0A%3D%20%5Clim_%7Bp%20%5Crightarrow%20%5Cinfty%7D%7B%20%5CBig%5B%20%5Cbeta_2%5Ep%20v_%7Bt-1%7D%20%2B%20(1%20-%20%5Cbeta_2%5Ep)%20%7Cg_t%7C%5Ep%20%5CBig%5D%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D%20%7D%20%0A" alt="= \ lim_ {p \ rightarrow \ infty} {\ Big [\ beta_2 ^ p v_ {t-1} + (1 - \ beta_2 ^ p) | g_t | ^ p \ Big] ^ {\ frac {1} {p }}}"></div><p></p><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%3D%20%5Clim_%7Bp%20%5Crightarrow%20%5Cinfty%7D%7B%20%5CBig%5B%20(1%20-%20%5Cbeta_2%5Ep)%20%5Csum_%7Bi%3D1%7D%5E%7Bt%7D%5Cbeta_2%5E%7Bp(t-i)%7D%20%7Cg_i%7C%5E%7Bp%7D%20%5CBig%5D%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D%20%7D%20%0A" alt="= \ lim_ {p \ rightarrow \ infty} {\ Big [(1 - \ beta_2 ^ p) \ sum_ {i = 1} ^ {t} \ beta_2 ^ {p (ti)} | g_i | ^ {p} \ Big] ^ {\ frac {1} {p}}}"></div><p></p><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%3D%20%5Clim_%7Bp%20%5Crightarrow%20%5Cinfty%7D%7B%20(1%20-%20%5Cbeta_2%5Ep)%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D%20%20%5CBig(%20%5Csum_%7Bi%3D1%7D%5E%7Bt%7D%5Cbeta_2%5E%7Bp(t-i)%7D%7Cg_i%7C%5E%7Bp%7D%20%5CBig)%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D%20%7D%20%0A" alt="= \lim_{p \rightarrow \infty}{ (1 - \beta_2^p)^{\frac{1}{p}}  \Big( \sum_{i=1}^{t}\beta_2^{p(t-i)}|g_i|^{p} \Big)^{\frac{1}{p}} }"></div><p></p><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%3D%20%5Clim_%7Bp%20%5Crightarrow%20%5Cinfty%7D%7B%20%5CBig(%20%5Csum_%7Bi%3D1%7D%5E%7Bt%7D%5Cbeta_2%5E%7Bp(t-i)%7D%7Cg_i%7C%5E%7Bp%7D%20%5CBig)%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D%20%7D%20%0A" alt="= \lim_{p \rightarrow \infty}{ \Big( \sum_{i=1}^{t}\beta_2^{p(t-i)}|g_i|^{p} \Big)^{\frac{1}{p}} }"></div><p></p><br><p><img align="right" src="https://tex.s2cms.ru/svg/(28)"></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%3D%20%20max(%5Cbeta_2%5E%7Bt-1%7D%20%7Cg_1%7C%2C%20%5Cbeta_2%5E%7Bt-2%7D%20%7Cg_2%7C%2C%20%5Cldots%20%2C%20%5Cbeta_2%7Cg_%7Bt-1%7D%7C%2C%20%7Cg_t%7C%20)%0A" alt="=  max(\beta_2^{t-1} |g_1|, \beta_2^{t-2} |g_2|, \ldots , \beta_2|g_{t-1}|, |g_t| )"></div><p></p><br><p>      <img src="https://tex.s2cms.ru/svg/%5Cinline%20p%20%5Crightarrow%20%5Cinfty" alt="\inline p \rightarrow \infty">    (28)    . ,   ,   ,      <img src="https://tex.s2cms.ru/svg/%5Cinline%20p" alt="\inline p">  : <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Csqrt%5B100%5D%7B10%5E%7B100%7D%20%2B%209%5E%7B100%7D%7D%20%3D%2010%5Csqrt%5B100%5D%7B1%20%2B%20%5Cfrac%7B9%7D%7B10%7D%5E%7B100%7D%7D%20%3D%2010%5Csqrt%5B10%5D%7B1.00002%7D%20%5Capprox%2010%20" alt="\inline \sqrt[100]{10^{100} + 9^{100}} = 10\sqrt[100]{1 + \frac{9}{10}^{100}} = 10\sqrt[10]{1.00002} \approx 10">  .   . </p><br><p>         Adam. </p><br><h2>  Experiments </h2><br><p>        .   ,           . ,     ‚Äî      ,                 ,       .          ,    . ,          ,   . </p><br><p>           <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cgamma" alt="\inline \gamma">  . ,    ,          ,  Adam  Adamax. </p><br><p><img src="https://habrastorage.org/files/21d/c52/2d1/21dc522d1bb6424481751fe41bcd911c.gif" alt="image"><br><img src="https://habrastorage.org/files/7b4/733/660/7b4733660d1243b68098a025bc42563b.png" alt="image"></p><br><p>         ,     -.   <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cgamma" alt="\inline \gamma">      SGD,         .    <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cgamma" alt="\inline \gamma"> ,     ,     ¬´¬ª.   :    ,         . </p><br><p><img src="https://habrastorage.org/files/af7/816/b93/af7816b9324446d28ee7ce97080d7352.gif" alt="image"><br><img src="https://habrastorage.org/files/29b/fcf/65a/29bfcf65ab4442918642334a01f8dc6d.png" alt="image"></p><br><p>  :    ,        .    <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cgamma" alt="\inline \gamma">   ,    <em></em>     ,   ,    ,        . </p><br><p><img src="https://habrastorage.org/files/8be/c1e/0e1/8bec1e0e10d3456f8f418e8604069232.gif" alt="image"><br><img src="https://habrastorage.org/files/153/c86/002/153c86002d7348bd9540446c0cc557f9.png" alt="image"></p><br><p>    ,    . </p><br><p><img src="https://habrastorage.org/files/7bc/de8/670/7bcde86703484105b581e1137054af07.gif" alt="image"><br><img src="https://habrastorage.org/files/760/db9/a95/760db9a95aba4cbda17b54d87a68667c.png" alt="image"></p><br><p><img src="https://habrastorage.org/files/b42/5b1/dd5/b425b1dd5ae546d7818c5584978648eb.gif" alt="image"><br><img src="https://habrastorage.org/files/070/539/d9b/070539d9bffc4e86ba74f3a5817e3335.png" alt="image"></p><br><p>  ,      (    ).    ,     Adam  RMSProp   .                (   )     (14)  (23)  .    : </p><br><p><img src="https://habrastorage.org/files/aab/459/d64/aab459d643cd40679a927a68e414f8e4.gif" alt="image"><br><img src="https://habrastorage.org/files/fb3/edf/21a/fb3edf21a046442cbd1adf3380ef21db.png" alt="image"></p><br><p>  Adam,      .     , , RMSProp   .   ,   <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cgamma" alt="\inline \gamma"> ,    ,    ,       - . RMSProp    ¬´¬ª.        (14) ‚Äî          .     , ,   (  <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Ceta" alt="\inline \eta"> ,   )  <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cepsilon" alt="\inline \epsilon"> ( ,  ). Adagrad    ,          ,    .    ,     ,         ,  -,        <em></em> . </p><br><p> , ,                       ,       ,       : </p><br><p><img src="https://habrastorage.org/files/f9c/ecc/c2a/f9ceccc2a7414226870338c74aebabd9.gif" alt="image"><br><img src="https://habrastorage.org/files/0f4/718/e89/0f4718e894c44193a7a51ed4e19d2fce.png" alt="image"></p><br><p><img src="https://habrastorage.org/files/f37/324/104/f37324104fbd4dfcbff324a1f3ecf704.gif" alt="image"><br><img src="https://habrastorage.org/files/8c6/c8d/ba4/8c6c8dba4c744fb08fcb16d76bab0b1a.png" alt="image"></p><br><h2>  Conclusion </h2><br><p> ,          . ,           ,       ,       . </p><br><p> ,      :               .      SGD  -   ,       ,   ,          - : </p><br><p><img src="https://habrastorage.org/files/270/4f7/4f5/2704f74f52764a2d83f519c16dd3bc9c.png" alt="image"></p><br><p> ‚Ä¶   .      ¬´ ¬ª Adam,          .    - ,      .        ,        . -        learning rate.            ,     -  . </p><br><p>       ,      ( python &gt; 3.4, numpy  matplotlib): </p><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> math <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ceil, floor <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">linear_interpolation</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X, idx)</span></span></span><span class="hljs-function">:</span></span> idx_min = floor(idx) idx_max = ceil(idx) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> idx_min == idx_max <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> idx_max &gt;= len(X): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X[idx_min] <span class="hljs-keyword"><span class="hljs-keyword">elif</span></span> idx_min &lt; <span class="hljs-number"><span class="hljs-number">0</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X[idx_max] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X[idx_min] + (idx - idx_min)*X[idx_max] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">EDM</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X, gamma, lr=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.25</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> Y = [] v = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> X: v = gamma*v + lr*x Y.append(v) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.asarray(Y) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">NM</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X, gamma, lr=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.25</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> Y = [] v = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(X)): v = gamma*v + lr*(linear_interpolation(X, i+gamma*v) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i+gamma*v &lt; len(X) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span>) Y.append(v) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.asarray(Y) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">SmoothedNM</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X, gamma, lr=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.25</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> Y = [] v = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(X)): lookahead4 = linear_interpolation(X, i+gamma*v/<span class="hljs-number"><span class="hljs-number">4</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i+gamma*v/<span class="hljs-number"><span class="hljs-number">4</span></span> &lt; len(X) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> lookahead3 = linear_interpolation(X, i+gamma*v/<span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i+gamma*v/<span class="hljs-number"><span class="hljs-number">2</span></span> &lt; len(X) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> lookahead2 = linear_interpolation(X, i+gamma*v*<span class="hljs-number"><span class="hljs-number">3</span></span>/<span class="hljs-number"><span class="hljs-number">4</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i+gamma*v*<span class="hljs-number"><span class="hljs-number">3</span></span>/<span class="hljs-number"><span class="hljs-number">4</span></span> &lt; len(X) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> lookahead1 = linear_interpolation(X, i+gamma*v) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i+gamma*v &lt; len(X) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> v = gamma*v + lr*(lookahead4 + lookahead3 + lookahead2 + lookahead1)/<span class="hljs-number"><span class="hljs-number">4</span></span> Y.append(v) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.asarray(Y) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Adagrad</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X, eps, lr=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">2.5</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> Y = [] G = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> X: G += x*x v = lr/np.sqrt(G + eps)*x Y.append(v) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.asarray(Y) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">RMSProp</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X, gamma, lr=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.25</span></span></span></span><span class="hljs-function"><span class="hljs-params">, eps=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.00001</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> Y = [] EG = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> X: EG = gamma*EG + (<span class="hljs-number"><span class="hljs-number">1</span></span>-gamma)*x*x v = lr/np.sqrt(EG + eps)*x Y.append(v) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.asarray(Y) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Adadelta</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X, gamma, lr=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">50.0</span></span></span></span><span class="hljs-function"><span class="hljs-params">, eps=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.001</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> Y = [] EG = <span class="hljs-number"><span class="hljs-number">0</span></span> EDTheta = lr <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> X: EG = gamma*EG + (<span class="hljs-number"><span class="hljs-number">1</span></span>-gamma)*x*x v = np.sqrt(EDTheta + eps)/np.sqrt(EG + eps)*x Y.append(v) EDTheta = gamma*EDTheta + (<span class="hljs-number"><span class="hljs-number">1</span></span>-gamma)*v*v <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.asarray(Y) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">AdadeltaZeroStart</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X, gamma, eps=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.001</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Adadelta(X, gamma, lr=<span class="hljs-number"><span class="hljs-number">0.0</span></span>, eps=eps) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">AdadeltaBigStart</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X, gamma, eps=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.001</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Adadelta(X, gamma, lr=<span class="hljs-number"><span class="hljs-number">50.0</span></span>, eps=eps) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Adam</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X, beta1, beta2=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.999</span></span></span></span><span class="hljs-function"><span class="hljs-params">, lr=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.25</span></span></span></span><span class="hljs-function"><span class="hljs-params">, eps=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.0000001</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> Y = [] m = <span class="hljs-number"><span class="hljs-number">0</span></span> v = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(X): m = beta1*m + (<span class="hljs-number"><span class="hljs-number">1</span></span>-beta1)*x v = beta2*v + (<span class="hljs-number"><span class="hljs-number">1</span></span>-beta2)*x*x m_hat = m/(<span class="hljs-number"><span class="hljs-number">1</span></span>- pow(beta1, i+<span class="hljs-number"><span class="hljs-number">1</span></span>) ) v_hat = v/(<span class="hljs-number"><span class="hljs-number">1</span></span>- pow(beta2, i+<span class="hljs-number"><span class="hljs-number">1</span></span>) ) dthetha = lr/np.sqrt(v_hat + eps)*m_hat Y.append(dthetha) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.asarray(Y) np.random.seed(<span class="hljs-number"><span class="hljs-number">413</span></span>) X = np.arange(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">300</span></span>) D_Thetha_spikes = np.asarray( [int(x%<span class="hljs-number"><span class="hljs-number">60</span></span> == <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> X]) D_Thetha_rectangles = np.asarray( [<span class="hljs-number"><span class="hljs-number">2</span></span>*int(x%<span class="hljs-number"><span class="hljs-number">40</span></span> &lt; <span class="hljs-number"><span class="hljs-number">20</span></span>) - <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> X]) D_Thetha_noisy_sin = np.asarray( [np.sin(x/<span class="hljs-number"><span class="hljs-number">20</span></span>) + np.random.random() - <span class="hljs-number"><span class="hljs-number">0.5</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> X]) D_Thetha_very_noisy_sin = np.asarray( [np.sin(x/<span class="hljs-number"><span class="hljs-number">20</span></span>)/<span class="hljs-number"><span class="hljs-number">5</span></span> + np.random.random() - <span class="hljs-number"><span class="hljs-number">0.5</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> X]) D_Thetha_uneven_sawtooth = np.asarray( [ x%<span class="hljs-number"><span class="hljs-number">20</span></span>/(<span class="hljs-number"><span class="hljs-number">15</span></span>*int(x &gt; <span class="hljs-number"><span class="hljs-number">80</span></span>) + <span class="hljs-number"><span class="hljs-number">5</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> X]) D_Thetha_saturation = np.asarray( [ int(x % <span class="hljs-number"><span class="hljs-number">80</span></span> &lt; <span class="hljs-number"><span class="hljs-number">40</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> X]) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> method_label, method, parameter_step <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> [ (<span class="hljs-string"><span class="hljs-string">"GRAD_Simple_Momentum"</span></span>, EDM, [<span class="hljs-number"><span class="hljs-number">0.25</span></span>, <span class="hljs-number"><span class="hljs-number">0.9</span></span>, <span class="hljs-number"><span class="hljs-number">0.975</span></span>]), (<span class="hljs-string"><span class="hljs-string">"GRAD_Nesterov"</span></span>, NM, [<span class="hljs-number"><span class="hljs-number">0.25</span></span>, <span class="hljs-number"><span class="hljs-number">0.9</span></span>, <span class="hljs-number"><span class="hljs-number">0.975</span></span>]), (<span class="hljs-string"><span class="hljs-string">"GRAD_Smoothed_Nesterov"</span></span>, SmoothedNM, [<span class="hljs-number"><span class="hljs-number">0.25</span></span>, <span class="hljs-number"><span class="hljs-number">0.9</span></span>, <span class="hljs-number"><span class="hljs-number">0.975</span></span>]), (<span class="hljs-string"><span class="hljs-string">"GRAD_Adagrad"</span></span>, Adagrad, [<span class="hljs-number"><span class="hljs-number">0.0000001</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, <span class="hljs-number"><span class="hljs-number">10.0</span></span>]), (<span class="hljs-string"><span class="hljs-string">"GRAD_RMSProp"</span></span>, RMSProp, [<span class="hljs-number"><span class="hljs-number">0.25</span></span>, <span class="hljs-number"><span class="hljs-number">0.9</span></span>, <span class="hljs-number"><span class="hljs-number">0.975</span></span>]), (<span class="hljs-string"><span class="hljs-string">"GRAD_AdadeltaZeroStart"</span></span>, AdadeltaZeroStart, [<span class="hljs-number"><span class="hljs-number">0.25</span></span>, <span class="hljs-number"><span class="hljs-number">0.9</span></span>, <span class="hljs-number"><span class="hljs-number">0.975</span></span>]), (<span class="hljs-string"><span class="hljs-string">"GRAD_AdadeltaBigStart"</span></span>, AdadeltaBigStart, [<span class="hljs-number"><span class="hljs-number">0.25</span></span>, <span class="hljs-number"><span class="hljs-number">0.9</span></span>, <span class="hljs-number"><span class="hljs-number">0.975</span></span>]), (<span class="hljs-string"><span class="hljs-string">"GRAD_Adam"</span></span>, Adam, [<span class="hljs-number"><span class="hljs-number">0.25</span></span>, <span class="hljs-number"><span class="hljs-number">0.9</span></span>, <span class="hljs-number"><span class="hljs-number">0.975</span></span>]), ]: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> label, D_Thetha <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> [(<span class="hljs-string"><span class="hljs-string">"spikes"</span></span>, D_Thetha_spikes), (<span class="hljs-string"><span class="hljs-string">"rectangles"</span></span>, D_Thetha_rectangles), (<span class="hljs-string"><span class="hljs-string">"noisy sin"</span></span>, D_Thetha_noisy_sin), (<span class="hljs-string"><span class="hljs-string">"very noisy sin"</span></span>, D_Thetha_very_noisy_sin), (<span class="hljs-string"><span class="hljs-string">"uneven sawtooth"</span></span>, D_Thetha_uneven_sawtooth), (<span class="hljs-string"><span class="hljs-string">"saturation"</span></span>, D_Thetha_saturation), ]: fig = plt.figure(figsize=[<span class="hljs-number"><span class="hljs-number">16.0</span></span>, <span class="hljs-number"><span class="hljs-number">9.0</span></span>]) ax = fig.add_subplot(<span class="hljs-number"><span class="hljs-number">111</span></span>) ax.plot(X, D_Thetha, label=<span class="hljs-string"><span class="hljs-string">"gradient"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> gamma <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> parameter_step: Y = method(D_Thetha, gamma) ax.plot(X, Y, label=<span class="hljs-string"><span class="hljs-string">"param="</span></span>+str(gamma)) ax.spines[<span class="hljs-string"><span class="hljs-string">'bottom'</span></span>].set_position(<span class="hljs-string"><span class="hljs-string">'zero'</span></span>) full_name = method_label + <span class="hljs-string"><span class="hljs-string">"_"</span></span> + label plt.xticks(np.arange(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">300</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>)) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) plt.title(full_name) plt.xlabel(<span class="hljs-string"><span class="hljs-string">'epoch'</span></span>) plt.ylabel(<span class="hljs-string"><span class="hljs-string">'value'</span></span>) plt.legend() <span class="hljs-comment"><span class="hljs-comment"># plt.show(block=True) #Uncoomment and comment next line if you just want to watch plt.savefig(full_name) plt.close(fig)</span></span></code> </pre> </div></div><br><p>  If you want to experiment with the parameters of the algorithms and your own functions, use this to create your own animation of the trajectory of the minimizer (you also need theano / lasagne): </p><br><div class="spoiler">  <b class="spoiler_title">More code</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib matplotlib.use(<span class="hljs-string"><span class="hljs-string">"Agg"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.animation <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> animation <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> theano <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> theano.tensor <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> T <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> lasagne.updates <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> nesterov_momentum, rmsprop, adadelta, adagrad, adam <span class="hljs-comment"><span class="hljs-comment">#For reproducibility. Comment it out for randomness np.random.seed(413) #Uncoomment and comment next line if you want to try random init # clean_random_weights = scipy.random.standard_normal((2, 1)) clean_random_weights = np.asarray([[-2.8], [-2.5]]) W = theano.shared(clean_random_weights) Wprobe = T.matrix('weights') levels = [x/4.0 for x in range(-8, 2*12, 1)] + [6.25, 6.5, 6.75, 7] + \ list(range(8, 20, 1)) levels = np.asarray(levels) O_simple_quad = (W**2).sum() O_wobbly = (W**2).sum()/3 + T.abs_(W[0][0])*T.sqrt(T.abs_(W[0][0]) + 0.1) + 3*T.sin(W.sum()) + 3.0 + 8*T.exp(-2*((W[0][0] + 1)**2+(W[1][0] + 2)**2)) O_basins_and_walls = (W**2).sum()/2 + T.sin(W[0][0]*4)**2 O_ripple = (W**2).sum()/3 + (T.sin(W[0][0]*20)**2 + T.sin(W[1][0]*20)**2)/15 O_giant_plateu = 4*(1-T.exp(-((W[0][0])**2+(W[1][0])**2))) O_hills_and_canyon = (W**2).sum()/3 + \ 3*T.exp(-((W[0][0] + 1)**2+(W[1][0] + 2)**2)) + \ T.exp(-1.5*(2*(W[0][0] + 2)**2+(W[1][0] -0.5)**2)) + \ 3*T.exp(-1.5*((W[0][0] -1)**2+2*(W[1][0] + 1.5)**2)) + \ 1.5*T.exp(-((W[0][0] + 1.5)**2+3*(W[1][0] + 0.5)**2)) + \ 4*(1 - T.exp(-((W[0][0] + W[1][0])**2))) O_two_minimums = 4-0.5*T.exp(-((W[0][0] + 2.5)**2+(W[1][0] + 2.5)**2))-3*T.exp(-((W[0][0])**2+(W[1][0])**2)) nesterov_testsuit = [ (nesterov_momentum, "nesterov momentum 0.25", {"learning_rate": 0.01, "momentum": 0.25}), (nesterov_momentum, "nesterov momentum 0.9", {"learning_rate": 0.01, "momentum": 0.9}), (nesterov_momentum, "nesterov momentum 0.975", {"learning_rate": 0.01, "momentum": 0.975}) ] cross_method_testsuit = [ (nesterov_momentum, "nesterov", {"learning_rate": 0.01}), (rmsprop, "rmsprop", {"learning_rate": 0.25}), (adadelta, "adadelta", {"learning_rate": 100.0}), (adagrad, "adagrad", {"learning_rate": 1.0}), (adam, "adam", {"learning_rate": 0.25}) ] for O, plot_label in [ (O_wobbly, "Wobbly"), (O_basins_and_walls, "Basins_and_walls"), (O_giant_plateu, "Giant_plateu"), (O_hills_and_canyon, "Hills_and_canyon"), (O_two_minimums, "Bad_init") ]: result_probe = theano.function([Wprobe], O, givens=[(W, Wprobe)]) history = {} for method, history_mark, kwargs_to_method in cross_method_testsuit: W.set_value(clean_random_weights) history[history_mark] = [W.eval().flatten()] updates = method(O, [W], **kwargs_to_method) train_fnc = theano.function(inputs=[], outputs=O, updates=updates) for i in range(125): result_val = train_fnc() print("Iteration " + str(i) + " result: "+ str(result_val)) history[history_mark].append(W.eval().flatten()) print("-------- DONE {}-------".format(history_mark)) delta = 0.05 mesh = np.arange(-3.0, 3.0, delta) X, Y = np.meshgrid(mesh, mesh) Z = [] for y in mesh: z = [] for x in mesh: z.append(result_probe([[x], [y]])) Z.append(z) Z = np.asarray(Z) print("-------- BUILT MESH -------") fig, ax = plt.subplots(figsize=[12.0, 8.0]) CS = ax.contour(X, Y, Z, levels=levels) plt.clabel(CS, inline=1, fontsize=10) plt.title(plot_label) nphistory = [] for key in history: nphistory.append( [np.asarray([h[0] for h in history[key]]), np.asarray([h[1] for h in history[key]]), key] ) lines = [] for nph in nphistory: lines += ax.plot(nph[0], nph[1], label=nph[2]) leg = plt.legend() plt.savefig(plot_label + '_final.png') def animate(i): for line, hist in zip(lines, nphistory): line.set_xdata(hist[0][:i]) line.set_ydata(hist[1][:i]) return lines def init(): for line, hist in zip(lines, nphistory): line.set_ydata(np.ma.array(hist[0], mask=True)) return lines ani = animation.FuncAnimation(fig, animate, np.arange(1, 120), init_func=init, interval=100, repeat_delay=0, blit=True, repeat=True) print("-------- WRITING ANIMATION -------") # plt.show(block=True) #Uncoomment and comment next line if you just want to watch ani.save(plot_label + '.mp4', writer='ffmpeg_file', fps=5) print("-------- DONE {} -------".format(plot_label))</span></span></code> </pre> </div></div><br><p>  ‚Üí <a href="https://yadi.sk/i/kwUDVyQd36s4g2">PDF version of the article</a> </p><br><p>  Thanks to Roman Parpalak for the <a href="https://habrahabr.ru/post/264709/">tool</a> thanks to which working with formulas on the habr is not so painful.  You can see the <a href="http://sebastianruder.com/optimizing-gradient-descent/">article</a> that I took for the basis of my article - there is a lot of useful information that I omitted in order to better focus on optimization algorithms.  To further delve into the neural networks, I recommend <a href="http://www.deeplearningbook.org/">this</a> book. </p><br><p>  Have a good New Year's Eve weekend, Habr! </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/318970/">https://habr.com/ru/post/318970/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../318960/index.html">Scala type classes (with a little overview of the cats library)</a></li>
<li><a href="../318962/index.html">About string formatting in modern C ++</a></li>
<li><a href="../318964/index.html">Towers of Hanoi - a theoretical solution without recursion</a></li>
<li><a href="../318966/index.html">Features of the development of mobile MMO RTS. Part 2</a></li>
<li><a href="../318968/index.html">Security in IoT: Security Architecture</a></li>
<li><a href="../318972/index.html">Adaptive jQuery without window.matchMedia</a></li>
<li><a href="../318976/index.html">How IT professionals work. Ilya Safonov, Senior Research Scientist at Schlumberger</a></li>
<li><a href="../318980/index.html">Theoretical Foundations of VMware Virtual SAN 6.5</a></li>
<li><a href="../318982/index.html">The history of one bug (# 1653967)</a></li>
<li><a href="../318984/index.html">Here you are, grandmother, and namespaces</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>