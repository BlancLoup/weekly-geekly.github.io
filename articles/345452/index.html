<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Machine learning and chocolates</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="They say that merchandisers have an unspoken rule: never put Nesquik and Snickers bars next to them. Who knows whether it is a myth or not, technologi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Machine learning and chocolates</h1><div class="post__text post__text-html js-mediator-article">  They say that merchandisers have an unspoken rule: never put Nesquik and Snickers bars next to them.  Who knows whether it is a myth or not, technologies that allow you to check the conditions of storage and display of chocolates on display cases exist.  In this article, we dive into them and talk about a machine learning model designed specifically for this purpose. <br><br><img src="https://habrastorage.org/webt/8d/7k/jk/8d7kjky21xn-qa8jpe49k9ahhmq.png"><a name="habracut"></a><br><br><blockquote><h2>  Series of Digital Transformation articles </h2><br>  Technological articles: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      1. <a href="https://habrahabr.ru/company/microsoft/blog/341854/">Start</a> . <br>  2. <a href="https://habrahabr.ru/company/microsoft/blog/342364/">Blockchain in the bank</a> . <br>  3. We <a href="https://habrahabr.ru/company/microsoft/blog/343604/">learn the car to understand human genes</a> . <br>  4. <a href="https://habrahabr.ru/company/microsoft/blog/345452/">Machine learning and chocolates</a> . <br>  5. Loading ... <br><br>  A series of interviews with Dmitry Zavalishin on <a href="https://www.youtube.com/channel/UCJuwCFeP6rhSUqk9gRMAwuw/feed">DZ Online</a> : <br><br>  1. <a href="https://habrahabr.ru/company/microsoft/blog/342596/">Alexander Lozhechkin from Microsoft: Do we need developers in the future?</a> <br>  2. <a href="https://habrahabr.ru/company/microsoft/blog/345662/">Alexey Kostarev from ‚ÄúRobot Vera‚Äù: How to replace HR with a robot?</a> <br>  3. <a href="https://habrahabr.ru/company/microsoft/blog/346870/">Fedor Ovchinnikov from Dodo Pizza: How to replace the restaurant director with a robot?</a> <br>  4. <a href="https://habrahabr.ru/company/microsoft/blog/348670/">Andrei Golub from ELSE Corp Srl: How to stop spending a lot of time on shopping trips?</a> </blockquote><br><h2>  Situation </h2><br>  In the company with which we worked, there is a huge distribution network through supermarket chains covering more than fourteen countries.  Each distributor must arrange the chocolates on windows in accordance with standard policies.  These policies indicate which shelf a particular type of candy should be placed on, and also determine the rules for warehousing. <br><br>  Procedures to verify compliance with these policies invariably entail very high costs.  SMART Business sought to develop a system with which the inspector or store manager could look at the image and immediately understand how well and efficiently the goods are placed on the shelf - as in the image below. <br><br><img src="https://habrastorage.org/webt/o6/v6/t_/o6v6t_iuzt1jlr_lwpnlq-qc0bu.jpeg"><br><br>  Effective policy (left), ineffective policy (right) <br><br><h2>  Study </h2><br>  In determining the range of tasks, we examined a number of image classification techniques, including the Microsoft <a href="https://www.customvision.ai/">Custom Vision Service</a> , transfer training using <a href="https://docs.microsoft.com/en-us/cognitive-toolkit/Build-your-own-image-classifier-using-Transfer-Learning">CNTK ResNet,</a> and object detection using <a href="https://docs.microsoft.com/en-us/cognitive-toolkit/Object-Detection-using-Fast-R-CNN">CNTK Fast-RCNN</a> .  Although the <a href="https://en.wikipedia.org/wiki/Object_detection">technology of</a> detecting objects using Fast-RCNN ultimately showed the best result, we also found out during the research that each approach differs in complexity and each has its own pros and cons. <br><br><h4>  Custom Vision Service </h4><br>  Teaching a REST-based service and working with it is much easier than teaching, deploying and updating a custom model of computer vision.  As a result, the first service we started using was the Microsoft Custom Vision Service.  Custom Vision Service is a tool for creating custom image classifiers and their continuous optimization.  For training the model, we used a set of samples from 882 images, on which individual shelves with chocolate candy display were presented (at that, 505 images showed the appropriate display of goods and 377 showed not corresponding). <br><br><img src="https://habrastorage.org/webt/_4/tg/35/_4tg35kactawqm4oaem7l6tyuco.jpeg"><br><br>  As a result of the training, we obtained a relatively efficient base model using the Custom Vision Service along with the following performance tests: <br><br><img src="https://habrastorage.org/webt/kx/vw/-t/kxvw-tkxwj0_tqo1h0dokfghnac.jpeg"><br>  In addition, we tested models on a set of 500 hidden images to supplement this data and get guaranteed baseline indicators. <br><br>  For more information about performance testing and using the Custom Vision Service model in a production environment, see the previous code example: Food <a href="https://www.microsoft.com/developerblog/2017/05/12/food-classification-custom-vision-service/">Classification</a> Using the Custom Vision Service.  Detailed explanations of standard classification indicators are provided in the Indicators section when evaluating machine learning algorithms in Python. <br><table><tbody><tr><th width="160" align="center">  Tag </th><th align="center">  Accuracy </th><th align="center">  Full return </th><th align="center">  F-1 Score </th><th align="center">  Support </th></tr><tr><td>  Does not meet the requirements </td><td align="center">  0.71 </td><td align="center">  0.74 </td><td align="center">  0.72 </td><td align="center">  170 </td></tr><tr><td>  Meets requirements </td><td align="center">  0.87 </td><td align="center">  0.85 </td><td align="center">  0.86 </td><td align="center">  353 </td></tr><tr><td>  Average / total </td><td align="center">  0.82 </td><td align="center">  0.81 </td><td align="center">  0.82 </td><td align="center">  523 </td></tr></tbody></table><br>  <b>Inaccuracy Matrix</b> <br><table><tbody><tr><td width="70" align="center">  125 </td><td width="70" align="center">  45 </td></tr><tr><td align="center">  52 </td><td align="center">  301 </td></tr></tbody></table><br>  Despite the fact that the Custom Vision Service showed excellent results in this scenario and proved to be a powerful tool for image classification, this service revealed a number of limitations that prevent its effective use in a production environment. <br><br>  These limitations are most fully described in the following piece of <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/home">documentation</a> on the Custom Vision Service: <br><br>  <i>The methods used in the Custom Vision Service, ways to effectively identify differences, which allows you to start creating a prototype, even with a small amount of data.</i>  <i>In theory, a small number of images are required to create a classifier ‚Äî 30 images for each class is enough to create a prototype.</i>  <i>However, this means that the Custom Vision Service is usually poorly prepared to implement scenarios aimed at identifying the most minor differences.</i> <br><br>  The Custom Vision Service showed excellent results after we found out that we need to process one policy and separate shelves with display of chocolate products.  Nevertheless, the limit of 1000 images used for training the service did not allow us to fine-tune the model to work with some borderline cases within this policy. <br><br>  For example, the Custom Vision Service has proven itself in terms of detecting gross policy violations, and these were the majority in our data set, for example, the following cases: <br><br><img src="https://habrastorage.org/webt/hz/sy/wh/hzsywhjavg6x1ulvzhetvlmerbw.jpeg"><br><br>  Nevertheless, this service constantly failed to recognize less obvious, albeit systematic violations in cases where the difference consisted of just one candy - like, for example, on the first shelf in this image: <br><br><img src="https://habrastorage.org/webt/iw/xw/bk/iwxwbkig7xo2coyq5rdooghnz-u.jpeg"><br><br>  To overcome the limitations of the Custom Vision Service, we decided to create several models, and then combine the results using <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html">the</a> majoritarian <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html">classifier</a> .  And although this would undoubtedly improve the results of using the model and, possibly, would allow using it in other scenarios, the costs for the API and the execution time would increase.  In addition, the model would still not be able to scale to serve more than one or two policies, since in the Custom Vision Service there is a limit on the number of models in each account ‚Äî no more than nineteen. <br><br><h4>  Transfer Training with CNTK and ResNet </h4><br>  To avoid the limitations of the Custom Vision Service associated with the data set, we decided to create an image recognition model using CNTK and ResNet <a href="https://en.wikipedia.org/wiki/Transfer_learning">training transfer</a> technology, following the instructions in the next <a href="https://docs.microsoft.com/en-us/cognitive-toolkit/Build-your-own-image-classifier-using-Transfer-Learning">tutorial</a> .  <a href="https://arxiv.org/abs/1512.03385">ResNet</a> is a deep convolutional neural network (GNSS) architecture developed by Microsoft as part of the 2015 ImageNet competition. <br><br>  In this case, our training data set contained two sets of 795 images that represented an effective and inefficient policy. <br><br><img src="https://habrastorage.org/webt/wj/xd/e2/wjxde2ztckdcgk8vj_83yb1mhbc.png"><br>  <i>Fig.</i>  <i>3</i>  ResNet CNN with image from ImageNet.  The input data is the RGB image of the cat, the output data is the probability vector, the maximum value of which corresponds to the label ‚Äústriped cat‚Äù. <br><br>  Since we did not have enough data (tens of thousands of samples) and enough computational power to train the large-scale CNN model from scratch, we decided to use ResNet as part of re-training at the output level of the training data set. <br><br><h4>  results </h4><br>  We launched the ResNet learning transfer model three times: for 20, 200 and 2000 superframes, respectively.  The best result was obtained for a test data set at startup for 2000 superframes. <br><table><tbody><tr><th width="160" align="center">  Tag </th><th align="center">  Accuracy </th><th align="center">  Full return </th><th align="center">  F-1 Score </th><th align="center">  Support </th></tr><tr><td>  Does not meet the requirements </td><td align="center">  0.38 </td><td align="center">  0.96 </td><td align="center">  0.54 </td><td align="center">  171 </td></tr><tr><td>  Meets requirements </td><td align="center">  0.93 </td><td align="center">  0.23 </td><td align="center">  0.37 </td><td align="center">  353 </td></tr><tr><td>  Average / total </td><td align="center">  0.75 </td><td align="center">  0.47 </td><td align="center">  0.43 </td><td align="center">  524 </td></tr></tbody></table><br>  <b>Inaccuracy Matrix</b> <br><table><tbody><tr><td width="70" align="center">  165 </td><td width="70" align="center">  6 </td></tr><tr><td align="center">  272 </td><td align="center">  81 </td></tr></tbody></table><br>  As you can see, the technology transfer learning has shown significantly worse performance results in comparison with the Customer Vision Service. <br><br>  However, learning transfer to ResNet is a powerful tool for teaching image recognition systems that use limited data sets.  However, if the model is used for new images that are too different from the original 1000 ImageNet classes, then it cannot obtain new representative elements using abstract elements ‚Äúlearned‚Äù from the <a href="http://www.image-net.org/">ImageNet</a> training set. <br><br><h4>  findings </h4><br>  We have seen promising results in terms of classifying individual policies using object recognition techniques (for example, the Custom Vision Service).  Given the large number of brands and the possible options for their placement on the shelves, it was absolutely impossible to determine whether a policy was being observed, based on images only, using the standard object recognition process based on the available data. <br><br>  Taking into account the high complexity of the problems arising during the test, as well as the desire of SMART Business to create new models for each policy as quickly and easily as possible based on standard object recognition methods, we decided to tackle the problem creatively. <br><br><h2>  Decision </h2><br><h4>  Object Detection and Fast R-CNN </h4><br>  In order to make the policy more efficient, without reducing the classification accuracy, we decided to use <a href="https://docs.microsoft.com/en-us/cognitive-toolkit/Object-Detection-using-Fast-R-CNN">object detection</a> technology <a href="https://docs.microsoft.com/en-us/cognitive-toolkit/Object-Detection-using-Fast-R-CNN">and Fast R-CNN</a> along with <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">AlexNet</a> to detect shelf images with display of goods that meet the requirements.  If the image contains shelves with a display that meets the requirements, then the entire showcase is considered to meet the requirements.  Thus, we could not only classify images, but also reuse previously classified shelves to create new customizable policies.  We chose Fast R-CNN instead of other alternatives (for example, Faster R-CNN), because the implementation and evaluation process has already proven its effectiveness in using CNTK (see the section <a href="https://www.microsoft.com/developerblog/2017/04/10/object-detection-using-cntk/">Object Detection Using CNTK</a> ). <br><br>  First, we used the new image support function ‚Äî the Visual Item Tagging Tool (VoTT) ‚Äîthe marking of effective policies within a larger data set (2,600 images).  For instructions on tagging image catalogs using VOTT, see <a href="https://github.com/Microsoft/VoTT">Marking an Image Catalog</a> . <br><br><img src="https://habrastorage.org/webt/eu/x7/nd/eux7nd7x5cxuitrm6fidlrd0qce.jpeg"><br><br>  Please note that on all three shelves the layout meets the requirements, therefore, the image demonstrates the work of an effective policy. <br><br>  By changing the filtering proportions, the number, and the minimum size of the focal area, we were able to obtain a high-quality result using the existing data set. <br><br><h2>  results </h2><br><img src="https://habrastorage.org/webt/j4/xz/zv/j4xzzvlf84pcpe-c6l0ytx62zcq.jpeg"><br><br>  Although at first glance, the results for this model look much worse than when using a solution based on the Custom Vision Service, the presence of a modular structure and the possibility of generalization within individual, constantly recurring problems inspired SMART Business to continue research in the field of advanced object detection techniques. <br><br><h4>  Use cases </h4><br>  Further, the advantages and disadvantages of the studied contextual methods of classifying images in order of increasing complexity are considered. <br><table><tbody><tr><th width="130" align="center">  Technique </th><th width="150" align="center">  Benefits </th><th width="160" align="center">  disadvantages </th><th>  Application area </th></tr><tr><td>  Custom Vision Service </td><td>  ‚Ä¢ Ability to start using even with small datasets.  A graphical user interface is required. <br>  ‚Ä¢ Checked images can be remarked to improve the model. <br>  ‚Ä¢ Ability to implement the service in a production environment with just one click. </td><td>  ‚Ä¢ Ability to detect the most minor changes. <br>  ‚Ä¢ Impossibility of local launch of the model. <br>  ‚Ä¢ Limited training set: only 1000 images. </td><td>  ‚Ä¢ Cloud services (for example, Custom Vision Service) are great for solving problems related to the classification of objects, in the presence of a limited training set.  This is the easiest method available. <br>  ‚Ä¢ In our study, the service showed the best results when using the existing data set, but did not cope with scaling within several policies and with the detection of constantly recurring problems. </td></tr><tr><td>  CNN / transfer training </td><td>  ‚Ä¢ Effective use of existing levels of the model, so that the model does not have to be trained from scratch. <br>  ‚Ä¢ Simple training - just select the sorted image catalogs and apply the training script to them. <br>  ‚Ä¢ The size of the training set is not limited, it is possible to launch the model offline. </td><td>  ‚Ä¢ Does not cope with the classification of data, the abstract elements of which differ from the elements that participated in the training based on the ImageNet data set. <br>  ‚Ä¢ Training requires a graphical user interface. <br>  ‚Ä¢ Implementation in a production environment is much more difficult compared to the introduction of a computer vision service. </td><td>  ‚Ä¢ Transferring CNN training on pre-trained models (for example, ResNet or Inception) shows the best results when using medium-sized datasets with properties similar to ImageNet categories.  Please note that in the presence of a large data set (at least several tens of thousands of samples), it is recommended to conduct re-training at all levels of the model. <br>  ‚Ä¢ Of all the methods we studied, the transfer of training showed the worst result in relation to our complex classification scenario. </td></tr><tr><td>  Object Detection with VoTT </td><td>  ‚Ä¢ Better for detecting minor differences between image classes. <br>  ‚Ä¢ The detection areas have a modular structure, they can be reused when changing the criteria for complex classification. <br>  ‚Ä¢ The size of the training set is not limited, it is possible to launch the model offline. </td><td>  ‚Ä¢ Annotation of the frame for all images is required (although using VoTT greatly simplifies the task). <br>  ‚Ä¢ Training requires a graphical user interface. <br>  ‚Ä¢ Implementation in a production environment is much more difficult compared to the introduction of a computer vision service. <br>  ‚Ä¢ Algorithms like Fast R-CNN are not able to detect areas of small size. </td><td>  ‚Ä¢ By combining object detection techniques with heuristic image classification technologies, you can use scenarios that support working with medium-sized datasets in cases where minor differences are required to differentiate the image classes. <br>  ‚Ä¢ Of all the considered techniques, this turned out to be the most difficult in terms of implementation, however, it demonstrated the most accurate result for the existing test data set.  SMART Business chose this very method. </td></tr></tbody></table><br>  The ecosystem of deep learning is developing rapidly; fundamentally new algorithms are being developed and improved every day.  After you become familiar with high performance data in standard performance testing, you may be tempted to immediately use the latest DNN algorithm to solve classification problems.  However, it is no less important (and maybe more) to evaluate such new technologies in the context of their application.  Too often, the novelty of machine learning algorithms overshadows the importance of well-designed and balanced techniques. <br><br>  The techniques that we studied in the course of cooperation with SMART Business provide a huge selection of classification methods of varying degrees of complexity, and also show possible shortcomings that should be taken into account when building image classification systems. <br><br>  Our study shows how important it is to take into account all possible drawbacks (implementation complexity, scalability and optimization possibilities) when using different size data sets, variability of class instances, similarity of classes, and different performance requirements. <br><br><img src="https://habrastorage.org/webt/59/e3/5d/59e35dcea0345205971169.jpeg" align="left" width="60">  <b>PS We</b> thank Kostya Kichinsky ( <a href="https://t.me/quantumquintum">Quantum Quintum</a> ) for the illustration of this article. </div><p>Source: <a href="https://habr.com/ru/post/345452/">https://habr.com/ru/post/345452/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../345440/index.html">3CX V15.5 SP3 alpha and 3CX Session Border Controller Update</a></li>
<li><a href="../345442/index.html">Machine training in sewers (in a good way)</a></li>
<li><a href="../345446/index.html">Create a Q & A bot: step by step instructions</a></li>
<li><a href="../345448/index.html">Common examples of using advanced JQL queries</a></li>
<li><a href="../345450/index.html">Technoporn with WebAssembly</a></li>
<li><a href="../345454/index.html">The digest of fresh materials from the world of the frontend for the last week ‚Ññ294 (December 18 - 24, 2017)</a></li>
<li><a href="../345458/index.html">The story of the victory at the annual competition Russian AI Cup 2017</a></li>
<li><a href="../345460/index.html">We comprehend C deeper using assembler. Part 2 (conditions)</a></li>
<li><a href="../345462/index.html">PHP Digest number 122 (December 11 - 25, 2017)</a></li>
<li><a href="../345464/index.html">DevDay on functional. Record of reports</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>