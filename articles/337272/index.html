<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to create a racist AI, without even trying. Part 2</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the first article, we managed to realize how easily and naturally AI absorbs human prejudices into the logic of its models. As I promised, I post t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to create a racist AI, without even trying. Part 2</h1><div class="post__text post__text-html js-mediator-article">  In the <a href="https://habrahabr.ru/company/microsoft/blog/336358/">first article,</a> we managed to realize how easily and naturally AI absorbs human prejudices into the logic of its models.  As I promised, I post the second part of the translation, in which we will figure out how to measure and weaken the influence of racism in AI using simple methods. <br><br><img src="https://habrastorage.org/web/8e5/b33/578/8e5b3357875141c3b2ba81abfc581e01.jpg"><br><blockquote>  Let me remind you: we ended up with the fact that our classifier considered the idea of ‚Äã‚Äãgoing to an Italian restaurant 5 times better than a Mexican one. </blockquote><a name="habracut"></a><h2>  We carry out a quantitative analysis of the problem. </h2><br>  I would like to understand how to avoid a similar situation in the future.  Let's process our system with additional data and statistically measure the magnitude of prejudice. <br><br>  We make four lists of names that are associated with people (residents of the United States) of different ethnic origin.  The first two lists are the common names of white and black people taken from an <a href="https://www.princeton.edu/~aylinc/papers/caliskan-islam_semantics.pdf">article in Princeton University</a> .  I add the Spanish-speaking names and names that are common in Islamic culture (mainly from Arabic and Urdu), that is, two more groups of names that are strongly associated with their ethnic group. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Now this data is used to test prejudice in the process of forming <code>ConceptNet</code> .  They are contained in the <code>conceptnet5.vectors.evaluation.bias</code> module.  I would like to add other ethnic groups.  For this, it may be necessary to take into account not only names, but also surnames. <br><br><div class="spoiler">  <b class="spoiler_title">Here are the lists</b> <div class="spoiler_text"><pre> <code class="python hljs">NAMES_BY_ETHNICITY = { <span class="hljs-comment"><span class="hljs-comment"># The first two lists are from the Caliskan et al. appendix describing the # Word Embedding Association Test. 'White': [ 'Adam', 'Chip', 'Harry', 'Josh', 'Roger', 'Alan', 'Frank', 'Ian', 'Justin', 'Ryan', 'Andrew', 'Fred', 'Jack', 'Matthew', 'Stephen', 'Brad', 'Greg', 'Jed', 'Paul', 'Todd', 'Brandon', 'Hank', 'Jonathan', 'Peter', 'Wilbur', 'Amanda', 'Courtney', 'Heather', 'Melanie', 'Sara', 'Amber', 'Crystal', 'Katie', 'Meredith', 'Shannon', 'Betsy', 'Donna', 'Kristin', 'Nancy', 'Stephanie', 'Bobbie-Sue', 'Ellen', 'Lauren', 'Peggy', 'Sue-Ellen', 'Colleen', 'Emily', 'Megan', 'Rachel', 'Wendy' ], 'Black': [ 'Alonzo', 'Jamel', 'Lerone', 'Percell', 'Theo', 'Alphonse', 'Jerome', 'Leroy', 'Rasaan', 'Torrance', 'Darnell', 'Lamar', 'Lionel', 'Rashaun', 'Tyree', 'Deion', 'Lamont', 'Malik', 'Terrence', 'Tyrone', 'Everol', 'Lavon', 'Marcellus', 'Terryl', 'Wardell', 'Aiesha', 'Lashelle', 'Nichelle', 'Shereen', 'Temeka', 'Ebony', 'Latisha', 'Shaniqua', 'Tameisha', 'Teretha', 'Jasmine', 'Latonya', 'Shanise', 'Tanisha', 'Tia', 'Lakisha', 'Latoya', 'Sharise', 'Tashika', 'Yolanda', 'Lashandra', 'Malika', 'Shavonn', 'Tawanda', 'Yvette' ], # This list comes from statistics about common Hispanic-origin names in the US. 'Hispanic': [ 'Juan', 'Jos√©', 'Miguel', 'Lu√≠s', 'Jorge', 'Santiago', 'Mat√≠as', 'Sebasti√°n', 'Mateo', 'Nicol√°s', 'Alejandro', 'Samuel', 'Diego', 'Daniel', 'Tom√°s', 'Juana', 'Ana', 'Luisa', 'Mar√≠a', 'Elena', 'Sof√≠a', 'Isabella', 'Valentina', 'Camila', 'Valeria', 'Ximena', 'Luciana', 'Mariana', 'Victoria', 'Martina' ], # The following list conflates religion and ethnicity, I'm aware. So do given names. # # This list was cobbled together from searching baby-name sites for common Muslim names, # as spelled in English. I did not ultimately distinguish whether the origin of the name # is Arabic or Urdu or another language. # # I'd be happy to replace it with something more authoritative, given a source. 'Arab/Muslim': [ 'Mohammed', 'Omar', 'Ahmed', 'Ali', 'Youssef', 'Abdullah', 'Yasin', 'Hamza', 'Ayaan', 'Syed', 'Rishaan', 'Samar', 'Ahmad', 'Zikri', 'Rayyan', 'Mariam', 'Jana', 'Malak', 'Salma', 'Nour', 'Lian', 'Fatima', 'Ayesha', 'Zahra', 'Sana', 'Zara', 'Alya', 'Shaista', 'Zoya', 'Yasmin' ] }</span></span></code> </pre> <br></div></div><br>  With the help of Pandas, we transform these data (names, the most characteristic origin and the tonality estimate obtained for them) into a table. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">name_sentiment_table</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> frames = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> group, name_list <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sorted(NAMES_BY_ETHNICITY.items()): lower_names = [name.lower() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> name <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> name_list] sentiments = words_to_sentiment(lower_names) sentiments[<span class="hljs-string"><span class="hljs-string">'group'</span></span>] = group frames.append(sentiments) <span class="hljs-comment"><span class="hljs-comment"># Put together the data we got from each ethnic group into one big table return pd.concat(frames) name_sentiments = name_sentiment_table()</span></span></code> </pre><br>  Now it is possible to visualize the distribution of tonality assessments that the system issues for each group of names. <br><br><pre> <code class="python hljs">plot = seaborn.swarmplot(x=<span class="hljs-string"><span class="hljs-string">'group'</span></span>, y=<span class="hljs-string"><span class="hljs-string">'sentiment'</span></span>, data=name_sentiments) plot.set_ylim([<span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>])</code> </pre> <br><img src="https://habrastorage.org/web/a35/851/b3c/a35851b3c6e24acfaee614b5f676fd5e.PNG"><br><br>  Distributions can be converted to bar charts with a confidence interval of 95% around averages. <br><br><pre> <code class="python hljs">plot = seaborn.barplot(x=<span class="hljs-string"><span class="hljs-string">'group'</span></span>, y=<span class="hljs-string"><span class="hljs-string">'sentiment'</span></span>, data=name_sentiments, capsize=<span class="hljs-number"><span class="hljs-number">.1</span></span>)</code> </pre> <br><img src="https://habrastorage.org/web/1d9/a6d/12f/1d9a6d12f4a14c8aa0889003b5bab500.PNG"><br><br>  Finally, we can process this data with the powerful statistical tools of the <code>statsmodels</code> package to find out, among other things, how pronounced the observed effect is. <br><br><pre> <code class="python hljs">ols_model = statsmodels.formula.api.ols(<span class="hljs-string"><span class="hljs-string">'sentiment ~ group'</span></span>, data=name_sentiments).fit() ols_model.fvalue <span class="hljs-comment"><span class="hljs-comment"># 13.041597745167659</span></span></code> </pre> <br>  F-measure (F-statistic) is a metric that allows you to simultaneously evaluate the accuracy and completeness of the model (more <a href="https://chrisalbon.com/machine-learning/precision_recall_and_F1_scores.html">here</a> ).  It can be used to assess overall prejudice against various ethnic groups. <br><br>  Our task is to improve the value of the F-measure.  The lower it is, the better. <br><br><h2>  We correct the data </h2><br>  So, we learned to measure the level of prejudice that is hidden in the set of vector meanings of words.  Let's try to improve this value.  To do this, repeat a series of operations. <br><br>  If it were important for us to write a good and easy-to-support code, then using global variables (for example, <code>model</code> and <code>embeddings</code> ) would not be worth it.  But the raw research code has a great advantage: it allows us to track the results of each stage and draw conclusions.  We will try not to do too much work, we will write a function that will repeat some of the operations performed. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">retrain_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(new_embs)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" Repeat the steps above with a new set of word embeddings. """</span></span> <span class="hljs-keyword"><span class="hljs-keyword">global</span></span> model, embeddings, name_sentiments embeddings = new_embs pos_vectors = embeddings.loc[pos_words].dropna() neg_vectors = embeddings.loc[neg_words].dropna() vectors = pd.concat([pos_vectors, neg_vectors]) targets = np.array([<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> entry <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> pos_vectors.index] + [<span class="hljs-number"><span class="hljs-number">-1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> entry <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> neg_vectors.index]) labels = list(pos_vectors.index) + list(neg_vectors.index) train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \ train_test_split(vectors, targets, labels, test_size=<span class="hljs-number"><span class="hljs-number">0.1</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">0</span></span>) model = SGDClassifier(loss=<span class="hljs-string"><span class="hljs-string">'log'</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">0</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">100</span></span>) model.fit(train_vectors, train_targets) accuracy = accuracy_score(model.predict(test_vectors), test_targets) print(<span class="hljs-string"><span class="hljs-string">"Accuracy of sentiment: {:.2%}"</span></span>.format(accuracy)) name_sentiments = name_sentiment_table() ols_model = statsmodels.formula.api.ols(<span class="hljs-string"><span class="hljs-string">'sentiment ~ group'</span></span>, data=name_sentiments).fit() print(<span class="hljs-string"><span class="hljs-string">"F-value of bias: {:.3f}"</span></span>.format(ols_model.fvalue)) print(<span class="hljs-string"><span class="hljs-string">"Probability given null hypothesis: {:.3}"</span></span>.format(ols_model.f_pvalue)) <span class="hljs-comment"><span class="hljs-comment"># Show the results on a swarm plot, with a consistent Y-axis plot = seaborn.swarmplot(x='group', y='sentiment', data=name_sentiments) plot.set_ylim([-10, 10])</span></span></code> </pre> <br><h4>  Word2vec check </h4><br>  You can assume that the problem is in the <code>GloVe</code> .  This archive is based on all the sites processed by <a href="http://commoncrawl.org/">Common Crawl</a> robot (including a lot of highly questionable, and about 20 more copies of the <a href="http://www.urbandictionary.com/">Urban Dictionary</a> , a dictionary of urban jargon).  Maybe the problem is this?  What if you take the good old <code>word2vec</code> , the result of processing Google News? <br><br>  The most reliable source of <code>word2vec</code> files that we managed to find is this <a href="https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit%3Fusp%3Dsharing">file in Google Drive</a> .  Download and save it as <code>data/word2vec-googlenews-300.bin.gz</code> . <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Use a ConceptNet function to load word2vec into a Pandas frame from its binary format from conceptnet5.vectors.formats import load_word2vec_bin w2v = load_word2vec_bin('data/word2vec-googlenews-300.bin.gz', nrows=2000000) # word2vec is case-sensitive, so case-fold its labels w2v.index = [label.casefold() for label in w2v.index] # Now we have duplicate labels, so drop the later (lower-frequency) occurrences of the same label w2v = w2v.reset_index().drop_duplicates(subset='index', keep='first').set_index('index') retrain_model(w2v) # Accuracy of sentiment: 94.30% # F-value of bias: 15.573 # Probability given null hypothesis: 7.43e-09</span></span></code> </pre> <br><img src="https://habrastorage.org/web/d06/6a8/5ef/d066a85efaed492d8acb8762d7637d57.PNG"><br><br>  So, the results for <code>word2vec</code> even worse.  F-measure for it exceeds 15, the differences in tone for ethnic groups are more pronounced. <br><br>  If you think about it, a dataset based on news releases can hardly be free from prejudice. <br><br><h4>  We try ConceptNet Numberbatch </h4><br>  Now is the time to introduce you to my own project to create vector meanings of words. <br><br>  <code>ConceptNet</code> is a knowledge graph with built-in functions for calculating vector values ‚Äã‚Äãof words.  In his learning process, a special stage is used to identify and eliminate some sources of algorithmic racism and sexism by adjusting numerical values.  The idea of ‚Äã‚Äãthis stage is based on the <a href="https://arxiv.org/abs/1607.06520">article Debiasing Word Embeddings</a> .  It is summarized to take into account several forms of prejudice.  As far as I know, other semantic systems with a similar function do not yet exist. <br><br>  From time to time, we export the pre-computed <code>ConceptNet</code> vectors and publish a package called <a href="https://github.com/commonsense/conceptnet-numberbatch">ConceptNet Numberbatch</a> .  The phase of eliminating human bias was added in April 2017. Let's load the vector meanings of English words and retrain our tonality analysis model on them. <br><br>  Download the <code>numberbatch-en-17.04b.txt.gz</code> , save it in the <code>data/</code> folder and re <code>numberbatch-en-17.04b.txt.gz</code> model. <br><br><pre> <code class="python hljs">retrain_model(load_embeddings(<span class="hljs-string"><span class="hljs-string">'data/numberbatch-en-17.04b.txt'</span></span>)) <span class="hljs-comment"><span class="hljs-comment"># Accuracy of sentiment: 97.46% # F-value of bias: 3.805 # Probability given null hypothesis: 0.0118</span></span></code> </pre> <br><img src="https://habrastorage.org/web/b52/4f0/480/b524f04805f74a939d1864607be645ec.PNG"><br>  Have we managed to completely solve the problem by switching to ConceptNet Numberbatch?  Can the problem of algorithmic racism be considered closed?  Not. <br><br>  Did we manage to significantly weaken it?  Yes, definitely. <br><br>  The ranges of pitch values ‚Äã‚Äãoverlap much more than for vector values ‚Äã‚Äãof words taken directly from <code>GloVe</code> or <code>word2vec</code> .  The value of the metric has decreased by more than 3 times relative to <code>GloVe</code> and approximately 4 times relative to <code>word2vec</code> .  In general, the fluctuations of tonality with the change in the text of the names significantly decreased, which is what we wanted, because the tone of the text should not depend on the names at all. <br><br>  However, a slight correlation persists.  Probably, I could find some data or training parameters for which the problem would look completely solved.  But it would be very ugly of me, because the problem is not solved until the end.  <code>ConceptNet</code> takes into account and eliminates only part of the sources of algorithmic racism.  But this is a good start. <br><br><h4>  Pros without cons </h4><br>  Please note: when we switched to <code>ConceptNet Numberbatch</code> , the accuracy of the forecast of tonality increased. <br><br>  One would assume that in order to counter algorithmic racism one would have to sacrifice something.  But we donate nothing.  It turns out that you can get data that will be both better and less racist.  The data can be better precisely because racism is less pronounced.  Those racism imprinted in the <code>word2vec</code> and <code>GloVe</code> data have nothing to do with accuracy. <br><br><h2>  Other approaches </h2><br>  Of course, there are other methods for analyzing the tonality of the text.  All the operations we used here are very common, but you can do something differently.  If you are applying your own approach, check to see if you are adding any prejudices and prejudices to your model. <br><br>  Instead of (or at the same time) changing the source of vector meanings of words, you can try to solve the problem directly at the output, for example, change the model so that it does not assign a tonality to the names and names of groups of people, or refuse to extrapolate the tonality altogether words and take into account only those words that are in the list.  This is perhaps the most common form of tonality analysis that does not use machine learning at all.  Then the model will show no more prejudice than is reflected in the list.  However, if you do not use machine learning, the model will become very rigid, and you can only change the data set by manual editing. <br><br>  You can also use a hybrid approach: calculate predicted tonality values ‚Äã‚Äãfor a set of words, and then attract an expert who will carefully check them and make a list of exception words for which you need to set the value 0. The minus of this approach is additional work, plus - the opportunity to see estimates, which the system gives based on your data.  It seems to me that in machine learning this should be given more attention. <br><br><hr><br><h2>  Outcome (from translator) </h2><br>  In conclusion, I would like to speak at once on the topic of comments on the previous post (the topic, expectedly, touched the feelings of many readers).  Often there was a thought that racism in the data is not evil, but a correct and reliable part of them, which should not be fought, because this is a reflection of public opinion, which cannot be ignored. <br><br>  I fundamentally disagree with this thought for several reasons.  As <a href="https://habrahabr.ru/company/microsoft/blog/336358/">vedenin1980</a> correctly <a href="https://habrahabr.ru/company/microsoft/blog/336358/">noted</a> , algorithms do not analyze objective reality.  They analyze texts written by people.  To begin with, we will think about the nature of these texts - who, when and why wrote them.  I hope there is an obvious bias in the sample.  Most texts are written by white people.  This means that we do not really take into account the opinions of others.  Most news texts are written about the horrors of this world - it means that we do not really take into account the good things that are in it. <br><br>  Finally, the main problem is that the text is tied to the current public opinion at the time of writing, and the AI ‚Äã‚Äãbased on it will be used in the future.  That is, he will think outdated views.  If it seems to you that I am dramatizing, <a href="http://visualhistory.livejournal.com/180412.html">let</a> me remind you that <a href="http://visualhistory.livejournal.com/180412.html">African Americans were allowed to sit on buses on some benches with the rest only in 1955</a> . <br><br><img src="https://habrastorage.org/web/2b7/c63/977/2b7c63977b2049b6844fd9294a3c0c8d.jfif" width="400" align="left">  In terms of history, it was a couple of hours ago.  Then there were long riots, people protested.  Today it seems unthinkable to whom it could even occur to divide the shops into white and color.  How do you think, how many unjust thoughts exist in today's world?  Those that we take for granted, and in 50 years we will be horrified by them.  Are you sure you should try to purposely train the AI ‚Äã‚Äãon this data, without even trying to improve the situation? <br><br>  Me not. </div><p>Source: <a href="https://habr.com/ru/post/337272/">https://habr.com/ru/post/337272/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../337260/index.html">Visual Explanation of Floating Numbers</a></li>
<li><a href="../337262/index.html">Data Ensemble Workflow Visualization with InterSystems DeepSee</a></li>
<li><a href="../337264/index.html">Poker AI: how to teach algorithms to bluff</a></li>
<li><a href="../337268/index.html">The largest manufacturer of avionics, Rockwell Collins will buy for $ 30 billion</a></li>
<li><a href="../337270/index.html">Top 10 internships for IT professionals</a></li>
<li><a href="../337274/index.html">Abbreviated Properties</a></li>
<li><a href="../337280/index.html">Software Asset Management at Raiffeisenbank - process and result</a></li>
<li><a href="../337282/index.html">Footer sections</a></li>
<li><a href="../337284/index.html">How to embed svg</a></li>
<li><a href="../337286/index.html">Why do you need BEM</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>