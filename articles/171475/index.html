<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Performance test: amazing and simple</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="It so happened that for the last six months I have been actively engaged in performance tests and it seems to me that in this area of ‚Äã‚ÄãIT there is an...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Performance test: amazing and simple</h1><div class="post__text post__text-html js-mediator-article">  It so happened that for the last six months I have been actively engaged in performance tests and it seems to me that in this area of ‚Äã‚ÄãIT there is an absolute lack of understanding of what is happening.  Nowadays, when the growth of computing power has decreased (vertical scalability), and the volume of tasks is growing at the same speed, the problem of performance is becoming more acute.  But before you rush into the fight against performance, you need to get a quantitative description. <br><br>  Summary of the article: <br><ul><li>  <a href="https://habr.com/ru/post/171475/">The simplest test</a> : <a href="https://habr.com/ru/post/171475/">test</a> measurement methods, choice of statistics (quantiles, median, average). </li><li>  <a href="https://habr.com/ru/post/171475/">Parameterized test</a> : assessment of the complexity of the algorithm, the application of OLS to the assessment of the linearity of the test. </li><li>  <a href="https://habr.com/ru/post/171475/">Tests on multi-core machines</a> : the difficulty of extrapolating the results of tests on multi-core machines, the Amdal law and the expediency of measurements. </li><li>  <a href="https://habr.com/ru/post/171475/">Behavioral test</a> : how, for a given model of user behavior, there will be a time waiting for a request and that can lead to a system collapse.  Throughput and how to calculate it. </li><li>  <a href="https://habr.com/ru/post/171475/">Amazing static distribution of</a> performance test results. </li></ul><br><br><h3>  Prehistory </h3><br>  Once, traveling in a train, I wanted to calculate the distance between the power poles.  Armed with the usual clock and estimating the average train speed of 80-100km / h (25 m / s), I marked the time between 2 pillars.  Oddly enough, this naive method gave a very depressing result, up to 1.5-2 times the difference.  Naturally the method was easy to fix, which I did, it was enough to detect 1 minute and count the number of pillars.  And it does not matter that the instantaneous speed over a minute can vary and it doesn‚Äôt even matter that we count the last pillar or minute in the middle, because measurements are quite enough for the desired result. <br>  The meaning of the test is to get convincing for yourself and for other measurements. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h5>  Tests "on the knee" </h5><br>  This story reminds me of what happens with software engineering performance testing.  A rather frequent phenomenon is the launch of 1-2 tests, the construction of graphs and the drawing of conclusions about the scalability of the system.  Even if it is possible to use MNCs or find out a standard error, this is not done because of ‚Äúuselessness.‚Äù The situation is especially interesting when, after these 2 measurements, people discuss how fast the system is, how it scales and compare it with other systems for personal sensations. <br>  Of course, it is not difficult to estimate how fast the team is executed.  On the other hand, faster does not mean better.  Software systems have over 10 different parameters, from hardware on which they operate to input, which the user enters at different points in time.  And often 2 equivalent algorithms can produce completely different scalability parameters in different conditions, which makes the choice not at all obvious. <br><br><h5>  Mistrust test </h5><br>  On the other hand, measurement results always remain a source of speculation and distrust. <br>  - Yesterday we measured was X, and today 1.1 * X.  Someone changed something?  - 10% - this is normal, we now have more records in the database. <br>  - During the test, the antivirus, skype, screensaver animation was disabled? <br>  ‚ÄúNo-no, for normal tests we need to purchase a cluster of servers, set up microsecond time synchronization between them ... remove the OS, run in protected mode ... <br>  - How many users do we support?  We have 5000 registered users, suddenly 20% of them will log in, it is necessary to run tests with 1000 parallel agents. <br><br><a name="habracut"></a><br><h5>  What do we do? </h5><br>  First, we have to admit that we already have the iron that we have, and we need to get maximum results on it.  Another question is how we can explain the behavior on other machines (production / quality testing).  Those who advocate for the experiments of "clean room" simply do not trust you, since you do not give enough explanations or data, "clean room" is an illusion. <br>  Secondly, the most important advantage in testing programs is the low cost of tests.  If physicists could conduct 100 tests of a simplified situation instead of 5 full-fledged ones, they would definitely choose 100 (and for checking the results 5 full-fledged :)).  In no case, you can not lose the cheapness of tests, run them at home, at a colleague, on the server, in the morning and at lunch.  Do not be tempted by the ‚Äúreal‚Äù hourly tests with thousands of users, it‚Äôs more important to understand how the system behaves than to know a couple of absolute numbers. <br>  Third, store the results.  The value of the tests are the measurements themselves, not the conclusions.  The findings are far more often wrong than the measurements themselves. <br><br>  So, <b>our task is</b> to conduct a sufficient number of tests in order to <b>predict the</b> behavior of the system in a given situation. <br><br><a name="SimpleTest"></a><br><h3>  Simplest test </h3><br>  Immediately make a reservation that we consider the system as a "black box".  Since systems can work under virtual machines, with or without a database, it does not make sense to dismember the system and take measurements in the middle (this usually leads to unrecorded important points).  But if there is an opportunity to test the system, in different configurations, this should definitely be done. <br><br>  For testing, even a simple test, it is better to have an independent system like JMeter, which can calculate the results, average.  Of course, setting up the system takes time, but I want to get the result much faster and we write something like that. <br><br><pre><code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">long</span></span> t = - System.currentTimeMillis(); <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> count = <span class="hljs-number"><span class="hljs-number">100</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> (count -- &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>) operation(); <span class="hljs-keyword"><span class="hljs-keyword">long</span></span> time = (t + System.currentTimeMillis()) / <span class="hljs-number"><span class="hljs-number">100</span></span>;</code> </pre> <br><br><h5>  Record measurement result </h5><br>  Running once we get the number X. Run again we get 1.5 * X, 2 * X, 0.7 * X.  In fact, it is possible and necessary to stop here if we take a temporary measurement and we do not need to insert this number into the report.  If we want to share this number with someone, it is important for us that it converges with other dimensions and does not arouse suspicion. <br>  The first ‚Äúlogical‚Äù step seems to be to put the numbers X and average them, but, in fact, averaging the averages is nothing more than increasing the count for one test.  Oddly enough, increasing the count can lead to even more unstable results, especially if you run the test and do something at the same time. <br><br><h5>  Minimum execution time as the best statistics </h5><br>  The problem is that the <b>average</b> is not a stable characteristic, one piled test that was performed 10 times longer would be enough for your results not to coincide with others.  As it is not paradoxical for simple performance tests, it is desirable to take the <b>minimum execution time</b> .  Naturally, the operation () should be measured in this context, usually 100 ms - 500 ms for the system is more than enough.  Of course, the minimum time will not be 1 to 1 coincide with the observed effect or with the real one, but this number will be comparable with other measurements. <br><br><h5>  Quantiles </h5><br>  Quantiles 10%, 50% (median), 90% are more stable than average, and much more informative.  We can find out with what probability the request will execute time 0.5 * X or 0.7 * X.  There is another problem with them, it is much more difficult to count quantiles on the knee than to take min, max. <br>  JMeter provides a measurement of the median and 90% in the Aggregate Report out of the box, which naturally should be used. <br><br><a name="ParameterTest"></a><br><h3>  Parameterized test </h3><br>  As a result of measurements, we get a certain number (median, minimum or other), but what can we say about our system (function) by one number?  Imagine you are automating the process of obtaining this number and every day you will check it when you need to sound the alarm and look for the guilty?  For example, here is a typical schedule of the same test every day. <br><br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text"><img src="https://habrastorage.org/storage2/8e5/eff/67b/8e5eff67bf4cec9a5881207301816441.png" alt="Daily test"></div></div><br><br>  If we want to write a performance report, with one number it will look empty.  Therefore, we will definitely test for different parameters.  Tests for different parameters will give us a beautiful graph and an idea of ‚Äã‚Äãthe scalability of the system. <br><br><h5>  Single parameter test </h5><br>  Consider a system with one numeric parameter.  First of all, you need to select the parameter values.  It does not make sense to choose numbers from different ranges, like [1, 100, 10000], you simply carry out 3 completely different incoherent tests and it will be impossible to find dependencies on such numbers.  Imagine you want to build a graph, which numbers would you choose?  Something similar to [X * 1, X * 2, X * 3, X * 4, X * 5,]. <br><br>  So, choose 5-10 (7 best) control points, for each point we make 3-7 measurements, take the minimum number for each point and build a graph. <br><img src="https://habrastorage.org/storage2/46f/29c/a13/46f29ca136931d3ce06924e3e55e0129.png" alt="Scalability parameter day graph"><br><br><h5>  Algorithm complexity </h5><br>  As you can see, the points are located exactly around the imaginary straight line.  Now we come to the most interesting part of the measurements, we can determine the <b>complexity of the algorithm</b> based on the measurements.  Determining the complexity of an algorithm based on tests is much easier for complex programs than analyzing the text of source programs.  With the help of the test, you can even find special points when the difficulty changes, for example, the launch of Full GC. <br><br>  Determining the complexity of the program on the test results is a direct task of the <b><a href="http://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B5%25D0%25B3%25D1%2580%25D0%25B5%25D1%2581%25D1%2581%25D0%25B8%25D0%25BE%25D0%25BD%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7">Regression analysis</a></b> .  Obviously, there is no general way to find a function only by a point; therefore, for the beginning, an assumption is made about some dependence, and then it is checked.  In our case, and in most others, we assume that the function is a linear relationship. <br><br><h5>  Least squares method (linear model) </h5><br>  To search for linear dependence coefficients, there is a simple and optimal <a href="http://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%25BD%25D0%25B0%25D0%25B8%25D0%25BC%25D0%25B5%25D0%25BD%25D1%258C%25D1%2588%25D0%25B8%25D1%2585_%25D0%25BA%25D0%25B2%25D0%25B0%25D0%25B4%25D1%2580%25D0%25B0%25D1%2582%25D0%25BE%25D0%25B2">OLS</a> method.  The advantage of this method is that the formulas can be programmed in 10 lines and they are absolutely clear. <br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">y</span></span> = a + bx a = ([xy] - b[x])/[x^<span class="hljs-number"><span class="hljs-number">2</span></span>] b = ([y] - a[x])/ n</code> </pre><br><br>  Let's present our calculations in the table. <br><img src="https://habrastorage.org/storage2/e21/e99/941/e21e999410ab197cffd1f79186fdaea8.png" alt="Table results"><br><br>  In the highlighted line we see the linear coefficient of our model, it is equal to 95.54, the free coefficient is 2688. Now we can do a simple but not obvious focus, we can attach importance to these numbers.  95.54 is measured in milliseconds (as are our measurements), 95.54 is the time we spend on each component, and 2688 ms the time we spend on the system itself does not depend on the number of components.  This method allowed us to allocate accurately enough the time of the external system, in this case the database, although it is ten times longer than the time of the 1st component.  If we used the Time_with_N_component / N formula, we would have to measure for N&gt; 1000 in order for the error to be less than 10%. <br><br>  The linear coefficient of the model is the most important number of our measurements and oddly enough it is the most stable number of our measurements.  The linear coefficient also makes it possible to adequately measure and interpret operations that occur in nanoseconds, and by separating the overhead of the system itself. <br><br>  The graph shows the results of independent test launches at different times, which confirms the greater stability of the linear coefficient than individual tests. <br><img src="https://habrastorage.org/storage2/80d/48c/837/80d48c837fdcacec9c8974937632c82e.png" alt="Scalability parameter many days graph"><br><br><h5>  Evaluation of the linearity of the model and the Pearson coefficient </h5><br>  Using the graph, we can visually see that our measurements really satisfy the linear model, but this method is far from accurate and we cannot automate it.  In this case, <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3D%25D0%259A%25D0%25BE%25D1%258D%25D1%2584%25D1%2584%25D0%25B8%25D1%2586%25D0%25B8%25D0%25B5%25D0%25BD%25D1%2582_%25D0%25BA%25D0%25BE%25D1%2580%25D1%2580%25D0%25B5%25D0%25BB%25D1%258F%25D1%2586%25D0%25B8%25D0%25B8_%25D0%259F%25D0%25B8%25D1%2580%25D1%2581%25D0%25BE%25D0%25BD%25D0%25B0">the Pearson coefficient</a> can help us.  This method does show deviations from a straight line, but for 5-10 measurements it is clearly not enough.  Below is an example of a clearly non-linear relationship, but the Pearson coefficient is quite high. <br><br>  Returning to our table, knowing the ovehead of the system (free coefficient), we can calculate the linear coefficient for each measurement, which is done in the table.  As we can see the numbers (100, 101, 93, 96, 102, 81, 94, 100, 93, 98) are rather randomly distributed around 95, which gives us weighty reasons to believe that the dependence is linear.  Following the letter of mathematics, in fact, the deviations from the average value should satisfy the normal distribution, and to check the normal distribution, it suffices to check the Kolmogorov-Smirnov criterion, but we leave this for the sophisticated testers. <br><br>  Oddly enough, not all dependencies are linear.  The first thing that comes to mind is a quadratic dependence.  In fact, quadratic dependency is very dangerous, it kills performance first slowly and then very quickly.  Even if you have everything done for a fraction of a second for 1000 elements, then for 10,000 it will already be tens of seconds (multiplied by 100).  Another example is sorting, which cannot be solved in linear time.  Calculate how applicable the linear analysis method is for algorithms with complexity O (n * log n) <br><br><pre> <code class="hljs pgsql">(<span class="hljs-number"><span class="hljs-number">10</span></span>n <span class="hljs-keyword"><span class="hljs-keyword">log</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span>n )/ (n <span class="hljs-keyword"><span class="hljs-keyword">log</span></span> n)= <span class="hljs-number"><span class="hljs-number">10</span></span> + (<span class="hljs-number"><span class="hljs-number">10</span></span>*<span class="hljs-keyword"><span class="hljs-keyword">log</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span>)/(<span class="hljs-keyword"><span class="hljs-keyword">log</span></span> n)</code> </pre><br><br>  That is, for n&gt; = 1000, the deviation will be within 10%, which is significant, but in some cases it can be applied, especially if the coefficient at log is large enough. <br><br>  Consider an example of nonlinear dependence. <br><div class="spoiler">  <b class="spoiler_title">Nonlinear complexity of the algorithm</b> <div class="spoiler_text"><img src="https://habrastorage.org/storage2/1e2/4be/5c9/1e24be5c90900ff2ee70598722b746b7.png" alt="Table results: not linear dependency"><br></div></div><br>  The first thing to note is negative overhead, for obvious reasons the system cannot have a negative time spent on preparation.  The second, looking at the column of coefficients, you can notice a pattern, the linear coefficient falls, and then grows.  This is reminiscent of a parabola chart. <br><br><h5>  Test with two or more parameters </h5><br>  Naturally, most systems do not consist of one parameter.  For example, a program for reading from a table already consists of 2 parameters, columns and lines.  Using parameterized tests, we can calculate what the cost of reading a row of 100 columns (let it be 100ms) and what are the costs of reading a column for 100 rows (let it be 150ms).  100 * 100ms! = 100 * 150ms and this is not a paradox, we just do not take into account that in the speed of reading lines there is already an overhead reading of 100 columns.  That is, 100ms / 100 columns = 1ms - this is not the speed of reading a single cell!  In fact, 2 measurements will not be enough to calculate the speed of reading a single cell.  The general formula is as follows: where A is the cost per cell, B is one line, C is per column: <br> <code>Time(row, column) = A * row * column + B * row + C * column + Overhead <br></code> <br><br>  Let's make a system of equations, taking into account the available values ‚Äã‚Äãand one more necessary measurement: <br><pre> <code class="hljs pgsql"> <span class="hljs-type"><span class="hljs-type">Time</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">row</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) = <span class="hljs-number"><span class="hljs-number">100</span></span> * A * <span class="hljs-keyword"><span class="hljs-keyword">row</span></span> + B * <span class="hljs-keyword"><span class="hljs-keyword">row</span></span> + o = <span class="hljs-number"><span class="hljs-number">100</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">row</span></span> + o. <span class="hljs-type"><span class="hljs-type">Time</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">row</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>) = <span class="hljs-number"><span class="hljs-number">50</span></span> * A * <span class="hljs-keyword"><span class="hljs-keyword">row</span></span> + B * <span class="hljs-keyword"><span class="hljs-keyword">row</span></span> + o = <span class="hljs-number"><span class="hljs-number">60</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">row</span></span> + o. <span class="hljs-type"><span class="hljs-type">Time</span></span>(<span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">column</span></span>) = <span class="hljs-number"><span class="hljs-number">100</span></span> * B * <span class="hljs-keyword"><span class="hljs-keyword">column</span></span> + C * <span class="hljs-keyword"><span class="hljs-keyword">column</span></span> + o = <span class="hljs-number"><span class="hljs-number">150</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">column</span></span> + o. <span class="hljs-number"><span class="hljs-number">100</span></span> * A + B = <span class="hljs-number"><span class="hljs-number">100.</span></span> <span class="hljs-number"><span class="hljs-number">50</span></span> * A + B = <span class="hljs-number"><span class="hljs-number">55.</span></span> <span class="hljs-number"><span class="hljs-number">100</span></span> * B + C = <span class="hljs-number"><span class="hljs-number">150.</span></span></code> </pre><br><br>  From here, we get A = 0.9 ms, B = 10 ms, C = 50 ms. <br><br>  Of course, having a formula for the similarity of the LMIC for this case, all calculations would be simplified and automated.  In general, the general LMIC does not apply to us, because a function that is linear in each of the arguments is not a multidimensional linear function (hyperplane in N-1 space).  It is possible to use the gradient method of finding the coefficients, but this will not be an analytical method. <br><br><a name="CoreTest"></a><br><h3>  Multi-user tests and multi-core systems tests </h3><br><h5>  Myth "throughput per core" </h5><br>  One of the favorite activities with performance tests is extrapolation.  Especially people like to extrapolate to an area for which they cannot get values.  For example, having 2 cores or 1 core in the system, I would like to extrapolate how the system behaved with 10 cores.  And of course, the first wrong assumption is that the dependence is linear.  For a normal definition of a linear relationship, it is necessary from 5 points, which of course is impossible to obtain on 2 cores. <br><br><h5>  Amdal's Law </h5><br>  One of the closest approximations is the <b><a href="http://ru.wikipedia.org/wiki/%25D0%2597%25D0%25B0%25D0%25BA%25D0%25BE%25D0%25BD_%25D0%2590%25D0%25BC%25D0%25B4%25D0%25B0%25D0%25BB%25D0%25B0">law of Amdal</a></b> . <br><img src="https://habrastorage.org/storage2/d64/84f/25b/d6484f25bca0fcf3e197141acf744382.png" alt="Amdahl law"><br>  It is based on the calculation of the percentage of the parallelized code Œ± (outside the synchronization block) and the synchronized code (1 - Œ±).  If one process takes time T on one core, then for multiple running tasks N time will be T '= T * Œ± + (1-Œ±) * N * T.  On average, of course, N / 2, but Amdahl admits N. Parallel acceleration, respectively, S = N * T / T '= N / (Œ± + (1-Œ±) * N) = 1 / (Œ± / N + (1 - Œ± )). <br><br>  Of course, the above graph is not so dramatic in reality (there is a logarithmic scale in X).  However, a significant drawback of synchronization blocks is the asymptote.  Relatively speaking, it is not possible by increasing the power to overcome the acceleration limit lim S = 1 / (1 - Œ±).  And this limit is quite hard, that is, for 10% of synchronized code, the limit is 10, for 5% (which is very good) the limit is 20. <br><br>  The function has a limit, but it is constantly growing, and from this arises, at first glance, a strange task: to evaluate which hardware is optimal for our algorithm.  In reality, it is quite difficult to increase the percentage of paralized.  Let us return to the formula T '= T * Œ± + (1-Œ±) * N * T, optimal from the point of view of efficiency: if the kernel is idle, as much time as it will work.  That is, T * Œ± = (1-Œ±) * N * T, hence we get N = <br>  Œ± / (1-Œ±).  For 10% - 9 cores, for 5% - 19 cores. <br><br><h5>  Communication of the number of users and the number of cores.  Perfect schedule. </h5><br>  The model of acceleration of calculations is theoretically possible, but not always real.  Consider a client-server situation where N clients are constantly running some task on the server, one by one, and we do not experience any costs of synchronizing the results, since the clients are not completely dependent!  Keeping statistics for example introduces a synchronization element.  Having a M-core machine, we expect the average query time T when N &lt;M is the same, and when N&gt; M the query time increases in proportion to the number of cores and users.  Calculate throughput as the number of requests processed per second and get the following "ideal" schedule. <br><br><div class="spoiler">  <b class="spoiler_title">Perfect graphics</b> <div class="spoiler_text"><img src="https://habrastorage.org/storage2/3b6/e4e/887/3b6e4e88751ebbca883ae89ed099c57a.png" alt="Idea graph"><br><img src="https://habrastorage.org/storage2/68c/8c6/e57/68c8c6e578de883b70a63a68953a5eb5.png" alt="Idea throughput graph"><br></div></div><br><br><h5>  Experimental measurements </h5><br>  Naturally perfect graphics are achievable if we have 0% synchronized blocks (critical sections).  Below are the actual measurements of one algorithm with different parameter values. <br><img src="https://habrastorage.org/storage2/b5e/743/26f/b5e74326fe083e030b6bf7a825744681.png" alt="Core / throughput"><br><br>  We can calculate the linear coefficient and build a graph.  Also having a machine with 8 cores, it is possible to conduct a series of experiments for 2, 4, 8 cores.  From the test results, you can see that a system with 4 cores and 4 users behaves just like a system with 8 cores and 4 users.  Of course, this was expected, and gives us the opportunity to carry out only one series of tests for the machine with the maximum number of cores. <br><div class="spoiler">  <b class="spoiler_title">Experimental measurements using linear coefficient</b> <div class="spoiler_text"><img src="https://habrastorage.org/storage2/033/f6b/778/033f6b778050eeeaf34453fc46298a58.png" alt="Core / parameter throughput"><br></div></div><br><br>  The measurement plots are close in value to the Amdal law, but still significantly different.  Having measurements for 1, 2, 4, 8 cores, you can calculate the amount of non-parallelized code using the formula <br><img src="https://habrastorage.org/getpro/habr/post_images/08e/863/89f/08e86389f22d1948487c55578c5a7e38.png" alt="Amdahl estimation"><br><br>  Where NP is the number of cores, SU = Through NP core / Throughput 1 core.  According to Amdal's law, this number should be constant, but in all dimensions this number falls, although not significantly (91% - 85%). <br><br>  The throughput per core chart is not always a continuous line.  For example, when there is a shortage of memory or when GC is running, deviations in throughput can be very significant. <br><div class="spoiler">  <b class="spoiler_title">Significant throughput fluctuations with Full GC</b> <div class="spoiler_text"><img src="https://habrastorage.org/storage2/5f2/083/e69/5f2083e699b1e22198a01b3850cd47a3.png" alt="Core / throughput with Full GC"><br></div></div><br><br><a name="BehaviorTest"></a><br><h3>  Behavioral test </h3><br><h5>  Throughput </h5><br>  When measuring the load on multi-user systems, we introduced the definition of <b>Throughput = NumberOfConcurrentUsers / AverageTimeResponse</b> .  To interpret the results. Throughput is the system capacity, how many users the system can serve at a time, and this definition is closely related to <a href="http://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B8%25D1%2581%25D1%2582%25D0%25B5%25D0%25BC%25D0%25B0_%25D0%25BC%25D0%25B0%25D1%2581%25D1%2581%25D0%25BE%25D0%25B2%25D0%25BE%25D0%25B3%25D0%25BE_%25D0%25BE%25D0%25B1%25D1%2581%25D0%25BB%25D1%2583%25D0%25B6%25D0%25B8%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D1%258F">queuing theory</a> .  The difficulty of measuring throughput is that the value depends on the input stream.  For example, in queuing systems, it is assumed that the waiting time for a response does not depend on how many applications are in the queue.  In a simple software implementation, without a queue of requests, the time between all threads will be divided and, accordingly, the system will degrade.  It is important to note that <b>JMeter</b> does not need to trust throughput measurements, it was noticed that Jmeter averages throughput between all test steps, which is not correct. <br><br>  Consider the following case: with a load with 1 user, the system issues Throughput = 10, with 2 - Throughput = 5.  This situation is quite possible, since the typical Throughput / User graph is a function that has one local (and global maximum). <br>  Therefore, with an incoming stream of 1 user every 125ms, the system will process 8 users per second (input throughput). <br>  With an incoming stream of 2 users every 125ms, the system will begin to collapse, since 8 (input throughput)&gt; 5 (possible throughput). <br><br><h5>  Collapsing system </h5><br>  Consider an example of a collapsing system in more detail.  After conducting the Throughput / Users test, we have the following results. <br><img src="https://habrastorage.org/storage2/beb/30d/cf5/beb30dcf55abb5a348ca5cf1109ce9bd.png" alt="Throughput measurements"><br><br>  Users - the number of simultaneous users in the system; Progress per user - what amount of work as a percentage the user performs in one second.  Mentally simulating the situation that 10 users come every second and begin to perform the same operation, the question is whether the system will be able to serve this stream. <br><img src="https://habrastorage.org/storage2/f3a/981/394/f3a98139419072a92cc7037f54a60778.png" alt="System progress"><br><br>  In 1 second, 10 users on average will perform only 20% of their work, assuming that on average they perform all in 5. In the 2nd second there will already be 20 users in the system and they will share resources between 20 threads, which will further reduce percentage of work performed.  We will continue the row until the moment when the first 10 users finish the work (theoretically they should finish it as the 1/2 / 1/3 + 1/4 row ... divergent). <br><img src="https://habrastorage.org/storage2/959/379/a30/959379a3015bc4f4ad8dc4a4484b68e3.png" alt="Collapse evolution"><br><br>  It is quite obvious that the system will collapse, because after 80 seconds there will be 800 users in the system, and at this point only the first 10 users can finish. <br><br>  This test shows that for any distribution of the incoming stream, if the incoming throughput (mathematical expectation) is greater than the maximum <br>  measured throughput for the system, the system will begin to degrade and will fall at constant load.  But the opposite is not true, in the 1st example, the maximum measured throughput (= 10)&gt; incoming (8), but the system also cannot cope. <br><br><h5>  Stationary system </h5><br>  An interesting case is a workable system.  Taking our measurements as a basis, we will test that every 1 second there are 2 new users.  Since the minimum response time exceeds one second, at first the number of users will accumulate. <br><img src="https://habrastorage.org/storage2/afd/050/f08/afd050f081da6325587fb3b40ccc0acb.png" alt="Stabilized system"><br><br>  As we can see, the system will enter the stationary mode exactly at that point, the graphics when the measured throughput coincides with the input throughput.  The average number of users in the system will be 10. <br><br>  From this we can conclude that the Throughput / Users graph on the one hand represents the number of users processed per second (throughput) and the number of users in the system (users).  The fall of the graph to the right of this point characterizes only the stability of the system in stressful situations and dependence on the nature of the incoming distribution.  In any case, conducting tests Throughput / Users, it is not at all superfluous to conduct a behavioral test with approximate characteristics. <br><br><a name="TestDistribution"></a><br><h3>  Amazing distribution </h3><br>  When conducting performance tests, there are almost always measurements that take much longer than expected.  If the testing time is long enough, then you can observe the following picture.  How to choose one number from all these dimensions? <br><img src="https://habrastorage.org/storage2/1e4/a63/971/1e4a6397145bd97f2f4163412496d0e3.png" alt="Timeline results"><br><br><h5>  Selection of statistics </h5><br>  The simplest statistic is the average, but for the above reasons it is not quite stable, especially for a small number of measurements.  To determine the quantile convenient to use the graph of the distribution function.  Thanks to the wonderful <a href="https://code.google.com/p/jmeter-plugins/">JMeter plugin</a> , we have this opportunity.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> After the first 10-15 measurements, the picture is usually not clear enough; therefore, a detailed study of the function will require 100 measurements. </font></font><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">First measurements</font></font></b> <div class="spoiler_text"><img src="https://habrastorage.org/storage2/a2b/58a/d42/a2b58ad42582ddbb76099554789a436a.png" alt="First measurements cum distribution"><br></div></div><br><img src="https://habrastorage.org/storage2/ede/843/3c4/ede8433c4a571a14b31411f8e324e4f3.png" alt="Final measurements cum distribution"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It is very simple to get the N-quantile from this graph; it is enough to take the value of the function at this point. </font><font style="vertical-align: inherit;">The function shows a rather strange behavior around the minimum, but then it grows quite stably and only a sharp rise begins only near 95% of quantile.</font></font><br><br><h5><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Distribution? </font></font></h5><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Is this distribution analytical or known? </font><font style="vertical-align: inherit;">To do this, you can plot the density distribution and try to see the known functions, fortunately there are not so </font></font><a href="http://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%25B2%25D0%25B5%25D1%2580%25D0%25BE%25D1%258F%25D1%2582%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D0%25B5%25D0%25B9"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">many of them</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><img src="https://habrastorage.org/storage2/568/796/3ef/5687963ef51f3a9cd01ccd4f7e9f1809.png" alt="Final measurements distribution"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To be honest, this method did not bring any results, at first glance, similar distribution functions: Beta distribution, Maxwell distribution, turned out to be quite far from the statistics.</font></font><br><br><h5><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Minimum and exponential distribution </font></font></h5><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It is important to note that the range of our random variable is not at all [0, + ‚àû [, but some [Min, + ‚àû [. </font><font style="vertical-align: inherit;">We cannot expect the program to execute faster than the theoretical minimum. </font><font style="vertical-align: inherit;">If we assume that the measured minimum converges to the theoretical one and subtract this number from all the statistics, then a rather interesting regularity can be observed.</font></font><br><img src="https://habrastorage.org/storage2/4a3/ea8/868/4a3ea886815ff2994503e557ecf0b243.png" alt="Table results"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It turns out Minimum + StdDev = Mean (Average), and this is typical for all measurements. </font><font style="vertical-align: inherit;">There is one known distribution of which, the expectation coincides with the variance, this exponential distribution. </font><font style="vertical-align: inherit;">Although the distribution density graph is different at the beginning of the domain, the main quantiles completely coincide. </font><font style="vertical-align: inherit;">Quite possibly, the random variable is the sum of the exponential distribution and the normal one, which fully explains the oscillations around the theoretical minimum point.</font></font><br><br><h5><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Variance and mean </font></font></h5><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Unfortunately, I was not able to find a theoretical justification for why the test results satisfy the exponential distribution. Most often, the exponential distribution appears in the problems of device failure time and even human lifetime. It is also unclear whether this is platform specific (Windows) or Java GC or any physical processes occurring in a computer.</font></font><br><br> , ,      2  (   )    performance test ,  ,   peformance ,      2 .  ‚Äî   ,  ‚Äî    . ,   ,  ,   ,      , . <br><br>   -  ,             ,  .    ,        ,   . 2       ( sleep),     . </div><p>Source: <a href="https://habr.com/ru/post/171475/">https://habr.com/ru/post/171475/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../171461/index.html">Chrome for Android plans to "speed up" by passing traffic through a remote server</a></li>
<li><a href="../171463/index.html">Why lay CloudFlare</a></li>
<li><a href="../171467/index.html">The second issue of the magazine TsODy.RF</a></li>
<li><a href="../171471/index.html">Boost Signals - Signals and Slots for C ++</a></li>
<li><a href="../171473/index.html">Qualcomm claims Snapdragon 800 beats Nvidia Tegra 4 easily</a></li>
<li><a href="../171477/index.html">Webrtc, Peer Connection - creating a full-fledged video chat in the browser</a></li>
<li><a href="../171479/index.html">About modularity, good architecture, dependency injection in C / C ++ and multicolored circles</a></li>
<li><a href="../171481/index.html">Review NOOK Simple Touch with GlowLight</a></li>
<li><a href="../171483/index.html">PoolLiveAid shows the trajectory of the billiard ball</a></li>
<li><a href="../171485/index.html">Videobook Ultrabook ASUSPRO BU400V</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>