<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Anatomy of recommendation systems. Part one</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I work as a data scientist at CleverDATA . We are involved in machine learning projects, and one of the most frequent requests for the development of ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Anatomy of recommendation systems. Part one</h1><div class="post__text post__text-html js-mediator-article">  I work as a data scientist at <a href="http://cleverdata.ru/">CleverDATA</a> .  We are involved in machine learning projects, and one of the most frequent requests for the development of machine learning-based marketing solutions is the development of recommendatory models. <br><br>  In this article I will talk about recommender systems, I will try to give the most complete overview of existing approaches and explain the principles of the algorithms on my fingers.  Part of the material is based on a good course on the recommendation systems of the <a href="https://grouplens.org/datasets/movielens/">MovieLens</a> laboratory (which is most familiar with the same name for testing recommendations), the rest is from personal experience.  The article consists of two parts.  The first describes the formulation of the problem and gives an overview of simple (but popular) recommendation algorithms.  In the second article I will talk about more advanced methods and some practical aspects of implementation. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/652/779/d2d/652779d2ded29db0c8412ad892d0df72.jpg"></div>  <a href="">A source</a> <br><a name="habracut"></a><br><h3>  <font color="#0057b6">Review and problem statement</font> </h3><br>  The task of the recommender system is to inform the user about the product that he may be most interested in at a given time.  The client receives information, and the service earns on the provision of quality services.  Services are not necessarily direct sales of the product offered.  The service can also earn commissions or simply increase user loyalty, which then translates into advertising and other income. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Depending on the business model, recommendations may be its basis, such as at TripAdvisor, or it may simply be a convenient additional service (as, for example, in some online clothing store), designed to improve Customer Experience and make navigation through the catalog more comfortable. <br><br>  Personalization of online marketing is an obvious trend of the last decade.  According to <a href="https://www.mckinsey.com/industries/retail/our-insights/how-retailers-can-keep-up-with-consumers">McKinsey</a> , 35% of Amazon‚Äôs revenue, or 75% of Netflix, comes from recommended products, and this percentage is likely to increase.  Recommender systems are about what to offer a client to make him happy. <br><br>  To illustrate all the variety of recommendation services, I will give a list of basic characteristics with which you can describe any recommendation system. <br><br><ol><li>  <b>The subject of the recommendation</b> is what is recommended. <br><br>  There is a big variety here - it can be products (Amazon, Ozon), articles (Arxiv.org), news (Surfingbird, Yandex.Dazen), images (500px), videos (YouTube, Netflix), people (Linkedin, LonelyPlanet), music (Last.fm, Pandora), playlists and more.  In general, you can recommend anything. <br></li><li>  <b>The purpose of the recommendation</b> is why it is recommended. <br><br>  For example: buying, informing, training, making contacts. <br></li><li>  <b>The context of the recommendation</b> is what the user is doing at this moment. <br><br>  For example: looking goods, listening to music, talking to people. <br></li><li>  <b>Source of recommendation</b> - who recommends: <br><br>  - audience (average rating of a restaurant on TripAdvisor), <br>  - similar users, <br>  - expert community (happens when it comes to a complex product, such as, for example, wine). <br></li><li>  <b>Degree of personalization</b> . <br><br>  Non-personal recommendations - when you recommend the same thing to everyone else.  They allow targeting by region or time, but do not take into account your personal preferences. <br><br>  A more advanced option is when recommendations use data from your current session.  You have looked at several products, and at the bottom of the page you are offered similar ones. <br><br>  Personal recommendations use all available information about the client, including the history of his purchases. <br></li><li>  <b>Transparency</b> . <br><br>  People trust the recommendations more if they understand exactly how it was received.  So there is less risk of running into ‚Äúunscrupulous‚Äù systems promoting the paid goods or putting more expensive products higher in the rating.  In addition, a good recommender system itself must be able to deal with the purchased reviews and salesman‚Äôs cheats. <br><br>  Manipulations by the way are unintentional.  For example, when a new blockbuster comes out, the first thing to do is to go to the fans, respectively, the first couple of months the rating can be greatly overestimated. <br></li><li>  <b>The format of the recommendation</b> . <br><br>  This can be a pop-up window, a sorted list appearing in a specific section of the site, a ribbon at the bottom of the screen, or something else. <br></li><li>  <b>Algorithms</b> . <br><br>  Despite the many existing algorithms, they all boil down to a few basic approaches, which will be described later.  The most classic are the algorithms Summary-based (non-personal), Content-based (models based on the product description), Collaborative Filtering (collaborative filtering), Matrix Factorization (methods based on matrix decomposition) and some others. <br></li></ol><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6d4/db2/906/6d4db2906e4c73fe2dfc75325e55157b.png" width="400"></div>  <a href="">A source</a> <br><br>  At the center of any recommendation system is the so-called preference matrix.  This is the matrix, on one of the axes of which all the clients of the service (Users) are laid, and on the other - the recommendation objects (Items).  At the intersection of some pairs (user, item), this matrix is ‚Äã‚Äãfilled with ratings (Ratings) - this is a known indicator of user interest in this product, expressed on a given scale (for example, from 1 to 5). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b9a/1cd/f0d/b9a1cdf0d459d5bd938dcd269dc04678.jpg"></div><br>  Users usually evaluate only a small part of the goods that are in the catalog, and the task of the recommender system is to summarize this information and predict the customer‚Äôs attitude to other goods, about which nothing is known.  In other words, you need to fill in all the blank cells in the picture above. <br><br>  People's consumption patterns are different, and new products do not have to be recommended.  You can show repeated positions, for example, to replenish stock.  According to this principle, there are two groups of goods. <br><br><ul><li>  <b>Repeatable</b> .  For example, shampoos or razors that are always needed. <br></li><li>  <b>Unrepeatable</b> .  For example, books or films that are rarely purchased repeatedly. <br></li></ul><br>  If the product cannot be clearly attributed to one of the classes, it makes sense to determine the admissibility of repeated purchases individually (someone goes to the store only for a certain brand of peanut butter, and it is important for someone to try everything in the catalog). <br><br>  The concept of "interesting" is also subjective.  Some users need things only from their favorite category (conservative recommendations), and someone, on the contrary, responds more to non-standard products or product groups (risky recommendations).  For example, video hosting may recommend the user only new series of the favorite series, and may periodically throw new shows on it or new genres in general.  Ideally, you should choose a strategy for displaying recommendations for each client separately, using modeling the client category. <br><br>  User ratings can be obtained in two ways: <br><br><ul><li>  clearly (explicit ratings) - the user himself puts the rating of the product, leaves a review, likes the page, <br></li><li>  implicitly (implicit ratings) - the user obviously does not express his attitude, but an indirect conclusion can be made from his actions: bought a product means he likes it, read the description for a long time means interest, etc. <br></li></ul><br>  Of course, explicit preferences are better - the user himself says that he liked it.  However, in practice, not all sites provide an opportunity to clearly express their interest, and not all users have the desire to do so.  Both types of ratings are most often used at once and complement each other well. <br><br>  It is also important to distinguish between the terms Prediction (prediction of the degree of interest) and the Recommendation itself (showing the recommendation).  What and how to show is a separate task that uses the estimates obtained in the Prediction step, but can be implemented in different ways. <br><br>  Sometimes the term ‚Äúrecommendation‚Äù is used in a wider sense and means any optimization, whether it is selection of clients for advertising distribution, determination of the optimal offer price, or simply selection of the best communication strategy with the client.  In the article I will confine myself to the classical definition of this term, denoting the choice of the most interesting product for the client. <br><br><h3>  <font color="#0057b6">Non-personalized recommendations</font> </h3><br>  Let's start with non-personalized recommendations, as they are the easiest to implement.  In them, the potential interest of the user is determined simply by the average product rating: ‚ÄúEverybody likes it - it means that you will like it too.‚Äù  According to this principle, most services work when the user is not authorized in the system, for example, the same TripAdvisor. <br><br>  Recommendations can be shown differently - as a banner on the side of the product description (Amazon), as a result of a request, sorted by a certain parameter (TripAdvisor), or something else. <br><br>  Product rating can also be depicted in different ways.  These can be asterisks next to the product, the number of likes, the difference between positive and negative votes (as is usually done on forums), the proportion of high marks, or even a histogram of marks.  Histograms are the most informative method, but they have one drawback - it is difficult to compare them with each other or sort them when you need to display goods in the list. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b73/0ea/d52/b730ead5266ac0ad83e4a22aef73e91f.jpg"></div><br><h4>  <font color="#0057b6">Cold start problem</font> </h4><br>  A cold start is a typical situation when a sufficient amount of data has not yet been accumulated for the correct operation of the recommender system (for example, when a product is new or just is rarely bought).  If the average rating is estimated by the estimates of only three users (igor92, xyz_111 and oleg_s), such an assessment will clearly not be reliable, and users understand this.  Often in such situations, ratings are artificially adjusted. <br><br>  The first way is to show not the average value, but the smoothed average (Damped Mean).  The meaning is as follows: with a small number of ratings, the displayed rating is more to a certain safe ‚Äúaverage‚Äù indicator, and as soon as a sufficient number of new ratings are typed, the ‚Äúaveraging‚Äù adjustment no longer works. <br><br>  Another approach is to calculate confidence intervals for each rating.  Mathematically, the more estimates, the smaller the variation of the average and, therefore, more confidence in its correctness.  And as a rating, you can display, for example, the lower limit of the interval (Low CI Bound).  At the same time, it is clear that such a system will be quite conservative, with a tendency to underestimate estimates for new products (if, of course, this is not a hit). <br><br>  Since the estimates are limited to a certain scale (for example, from 0 to 1), the usual method of calculating the confidence interval is poorly applicable here: because of the distribution tails, going to infinity and the symmetry of the interval itself.  There is an alternative and more accurate way to calculate it - the <a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval">Wilson Confidence Interval</a> .  At the same time asymmetrical intervals of approximately this type are obtained. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/b3/ks/rt/b3ksrtynyspkk3lnfpawslijoou.jpeg"></div><br>  In the picture above, the estimate of the average value of the rating is plotted horizontally, and the variation around the mean value is displayed vertically.  The color indicates different sample sizes (obviously, the larger the sample, the smaller the confidence interval). <br><br>  The problem of cold start is also relevant for non-personalized recommendations.  The general approach here is to replace what currently cannot be calculated with different heuristics (for example, replace it with an average rating, use a simpler algorithm, or not use the product at all until the data is collected). <br><br><h4>  <font color="#0057b6">Relevance of recommendations</font> </h4><br>  In some cases it is also important to consider the ‚Äúfreshness‚Äù of the recommendation.  This is especially true for articles or posts on the forums.  Fresh entries should often get to the top.  To do this, use correction factors (damping factors).  Below are a couple of formulas for calculating the rating of articles on media sites. <br><br>  An example of rating calculation in the Hacker news magazine: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/vc/yr/sn/vcyrsnoylizvctaq3ilbafaosbq.jpeg"></div>  where U = upvotes, D = downvotes, and P (Penalty) - an additional adjustment for the implementation of other business rules <br><br>  Reddit rating calculation: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/g-/zj/fg/g-zjfghqsstjvuryzd-b9c92hjk.jpeg"></div>  where U = number of votes "for", D = number of votes "against", T = recording time.  The first item assesses the ‚Äúquality of the recording‚Äù, and the second makes a correction for the time. <br><br>  Obviously, there is no universal formula, and each service invents the formula that best solves its problem ‚Äî it is empirically tested. <br><br><h3>  <font color="#0057b6">Content-based recommendations</font> </h3><br>  Personal recommendations suggest the maximum use of information about the user himself, primarily about his previous purchases.  One of the first was the content-based filtering approach.  In the framework of this approach, the description of the product (content) is compared with the interests of the user, obtained from his previous ratings.  The more the product meets these interests, the higher the potential interest of the user is estimated.  The obvious requirement here is that all products in the catalog should have a description. <br><br>  Historically, the subject of Content-based recommendations were more often products with unstructured descriptions: films, books, articles.  Such signs may be, for example, text descriptions, reviews, cast, and so on.  However, nothing prevents the use of ordinary numerical or categorical features. <br><br>  Unstructured signs are described in a text-typical way - vectors in the space of words ( <a href="https://en.wikipedia.org/wiki/Vector_space_model">Vector-Space model</a> ).  Each element of such a vector is a sign that potentially characterizes the interest of the user.  Similarly, a product is a vector in the same space. <br><br>  As the user interacts with the system (for example, he buys films), the vector descriptions of the goods purchased by him are combined (summed up and normalized) into a single vector and, thus, the vector of his interests is formed.  Next, it suffices to find the product, the description of which is closest to the vector of interests, i.e  Solve the problem of finding n closest neighbors. <br><br>  Not all elements are equally significant: for example, allied words obviously do not carry any payload.  Therefore, when determining the number of coinciding elements in two vectors, all dimensions must be weighed by their significance.  This task is solved by the well-known <a href="https://en.wikipedia.org/wiki/Tf%25E2%2580%2593idf">TF-IDF</a> transformation in <a href="https://en.wikipedia.org/wiki/Text_mining">Text Mining</a> , which assigns more weight to more rare interests.  The coincidence of such interests is more important in determining the proximity of two vectors than the coincidence of the popular ones. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ns/yf/-l/nsyf-lljlfb0r7bjrc_l3xonto0.jpeg"></div><br>  The principle of TF-IDF here is equally applicable to ordinary nominal attributes, such as, for example, genre, director, language.  TF is a measure of the significance of an attribute for a user; IDF is a measure of the ‚Äúrarity‚Äù of an attribute. <br><br>  There is a whole family of similar transformations (for example, BM25 and similar), but they all contain the same logic that TF-IDF: rare attributes should have more weight when comparing products.  The picture below illustrates exactly how the weight of the TF-IDF depends on the TF and IDF values.  The near horizontal axis is DF: the frequency of the attribute among all products, the far horizontal axis is TF: the logarithm of the frequency of the attribute of the user. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/248/b71/1f2/248b711f2b0f9c813c8e88ced69bafbd.png"></div><br>  Some points that can be considered when implementing. <br><br><ul><li>  When forming the vector space of product presentation, instead of individual words, you can use shingles or <a href="https://en.wikipedia.org/wiki/N-gram">n-grams</a> (successive pairs of words, triples, etc.).  This will make the model more detailed, but more data will be required for training. <br></li><li>  In different places of the description of the product, the weight of keywords may differ (for example, the description of the film may consist of a title, a brief description and a detailed description). <br></li><li>  Product descriptions from different users can be weighed differently.  For example, we can give more weight to active users who have a lot of ratings. <br></li><li>  Similarly, you can weigh by item.  The greater the average rating of an object, the greater its weight (similar to <a href="https://en.wikipedia.org/wiki/PageRank">PageRank</a> ). <br></li><li>  If the product description allows links to external sources, then you can get confused and also analyze all third-party information related to the product. <br></li></ul><br>  It can be seen that content-based filtering almost completely repeats the query-document matching mechanism used in search engines such as Yandex and Google.  The only difference is in the form of a search query - here it is a vector that describes the interests of the user, and there are the keywords of the requested document.  When search engines began to add personalization, the distinction was erased even more. <br><br>  The cosine distance is most often used as a measure of proximity of two vectors. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0a6/136/c9b/0a6136c9bd4144fabc84b53b4e0b6f49.png"></div><br>  When adding a new assessment, the vector of interests is updated incrementally (only for those elements that have changed).  In recalculation, it makes sense to give new estimates a little more weight, since preferences may change. <br><br><h3>  <font color="#0057b6">Collaborative filtering (User-based option)</font> </h3><br>  This class of systems began to actively develop in the 90s.  Under the approach, recommendations are generated based on the interests of other similar users.  Such recommendations are the result of the ‚Äúcollaboration‚Äù of multiple users.  Hence the name of the method. <br><br>  The classic implementation of the algorithm is based on the principle of k nearest neighbors.  On the fingers - for each user we look for the k most similar to him (in terms of preferences) and supplement the information about the user with known data on his neighbors.  So, for example, if it is known that your neighbors in interest are delighted with the film ‚ÄúBlood and Concrete‚Äù, and you have not watched it for some reason, this is an excellent reason to offer you this film for Saturday viewing. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/al/it/fv/alitfv0yszejlvhji0agoy01amg.jpeg"></div><br>  The picture above illustrates the principle of the method.  In the preference matrix, the user is highlighted in yellow, for which we want to determine estimates for new products (question marks).  His three closest neighbors are highlighted in blue. <br><br>  ‚ÄúSimilarity‚Äù is in this case a synonym for ‚Äúcorrelation‚Äù of interests and can be considered in many ways (besides the Pearson correlation, there is also a cosine distance, there is a Jacquard distance, Hamming distance, etc.). <br><br>  The classical implementation of the algorithm has one obvious disadvantage - it is poorly applicable in practice because of the quadratic complexity.  Indeed, like any nearest neighbor method, it requires the calculation of all pairwise distances between users (and there may be millions of users).  It is easy to calculate that the complexity of calculating the distance matrix will be <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>n</mi><mn>2</mn></msup><mi>m</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="8.072ex" height="2.901ex" viewBox="0 -935.7 3475.4 1249" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/lanit/blog/420499/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjB_P_9ZsdZqpWVIREAQd18Y0DAZA#MJMATHI-4F" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/lanit/blog/420499/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjB_P_9ZsdZqpWVIREAQd18Y0DAZA#MJMAIN-28" x="763" y="0"></use><g transform="translate(1153,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/lanit/blog/420499/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjB_P_9ZsdZqpWVIREAQd18Y0DAZA#MJMATHI-6E" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/lanit/blog/420499/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjB_P_9ZsdZqpWVIREAQd18Y0DAZA#MJMAIN-32" x="849" y="513"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/lanit/blog/420499/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjB_P_9ZsdZqpWVIREAQd18Y0DAZA#MJMATHI-6D" x="2207" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/lanit/blog/420499/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjB_P_9ZsdZqpWVIREAQd18Y0DAZA#MJMAIN-29" x="3085" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mi>m</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-1"> O (n ^ 2m) </script>  where n is the number of users, and m is the number of products.  With millions of users, a minimum of 4TB is required to store the matrix of distances in raw form. <br><br>  This problem can be partly solved by purchasing high-performance iron.  But if you approach it wisely, then it is better to introduce corrections into the algorithm: <br><br><ul><li>  update distances not with every purchase, but with batches (for example, once a day), <br></li><li>  do not recalculate the distance matrix completely, but update it incrementally, <br></li><li>  opt for iterative and approximate algorithms (for example, ALS). <br></li></ul><br>  In order for the algorithm to be effective, it is important that several assumptions be fulfilled. <br><br><ul><li>  The tastes of people do not change over time (or they change, but they are the same for everyone). <br></li><li>  If people's tastes are the same, then they are the same in everything. <br><br>  For example, if two clients prefer the same films, then they also like the same book.  This is often the case when the recommended products are homogeneous (for example, films only).  If this is not the case, then the couple of clients may well have the same eating habits, and the political views will be exactly the opposite - here the algorithm will be less efficient. <br></li></ul><br>  The neighborhood of the user in the space of preferences (his neighbors), which we will analyze to generate new recommendations, can be chosen in different ways.  We can work in general with all users of the system, we can set a certain proximity threshold, we can choose several neighbors at random or take n most similar neighbors (this is the most popular approach). <br><br>  Authors from MovieLens as the optimal number of neighbors give figures in 30-50 neighbors for films and 25-100 for arbitrary recommendations.  Here it is clear that if we take too many neighbors, we will get a greater chance of random noise. Conversely, if we take too little, we will get more accurate recommendations, but fewer goods can be recommended. <br><br>  An important stage of data preparation is the normalization of estimates. <br><br><h3>  <font color="#0057b6">Data Standardization (scaling)</font> </h3><br>  Since all users evaluate differently - someone puts all fives in a row, and it‚Äôs rare for someone to wait for fours - it‚Äôs better to normalize the data before calculating, i.e.  lead to a single scale so that the algorithm can correctly compare them with each other. <br><br>  Naturally, the predicted estimate will then need to be converted to the original scale by inverse transformation (and, if necessary, rounded to the nearest whole number). <br><br>  There are several ways to normalize: <br><br><ul><li>  by centering (mean-centering) - from the user's ratings just subtract his average rating, <br><br>  <i>* relevant only for non-binary matrices</i> <br></li><li>  standardization (z-score) - in addition to centering, we divide its assessment by the standard deviation of the user, <br><br>  <i>* Here, after the inverse transformation, the rating can go beyond the scale (for example, 6 on a five-point scale), but such situations are quite rare and are solved simply by rounding off to the nearest acceptable estimate.</i> <br></li><li>  double standardization - for the first time we normalize with user ratings, and the second time with product ratings. <br><br>  If the film ‚ÄúThe Best Movie‚Äù has an average rating of 2.5, and the user puts it at 5, then this is a strong factor indicating that such films clearly suit their taste. <br></li></ul><br>  ‚ÄúSimilarity‚Äù or correlation of preferences of two users can be considered in different ways.  In fact, we just need to compare two vectors.  We list some of the most popular. <br><br><ol><li>  Pearson correlation is a classical coefficient, which is quite applicable when comparing vectors. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/110/e8c/698/110e8c698e7cf685c3bc2041afb25cec.png"></div><br>  Its main disadvantage is when the intersection is estimated to be low, the correlation may be high simply by accident. <br><br>  To combat accidentally high correlation, you can multiply by a factor of 50 / min (50, Rating intersection) or any other damping factor, the effect of which decreases with an increase in the number of estimates. <br></li><li>  Spearman Correlation <br><br>  The main difference is the rank factor, i.e.  does not work with absolute values ‚Äã‚Äãof ratings, but with their sequence numbers.  In general, the result is very close to the Pearson correlation. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fdd/59e/325/fdd59e32550c21f7007bd4242a9a7b29.png"></div></li><li>  Cosine distance <br><br>  Another classic factor.  If you look closely, the cosine of the angle between the standardized vectors is the Pearson correlation, the same formula. <br><br><img src="https://habrastorage.org/webt/jo/s3/vn/jos3vnkilzsvrkevdfsm8xl22gq.png"><br><br>  Why cosine - because if two vectors are co-directed (i.e. the angle between them is zero), then the cosine of the angle between them is equal to one.  Conversely, the cosine of the angle between perpendicular vectors is zero. <br></li></ol><br>  An interesting development of the collaborative approach is the so-called Trust-based recommendations, which take into account not only the proximity of people according to their interests, but also their ‚Äúsocial‚Äù proximity and the degree of trust between them.  If, for example, we see that on Facebook a girl occasionally visits a page with her friend‚Äôs audio recordings, then she trusts her musical taste.  Therefore, in the recommendation of the girl, you can completely mix in new songs from a friend's playlist. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/63b/62c/55e/63b62c55e1f68e966d6b4928617cc406.jpg"></div><br><h3>  <font color="#0057b6">Rationale for recommendations</font> </h3><br>  It is important that the user trusted the recommendation system, and for this it should be simple and straightforward.  If necessary, a clear explanation of the recommendation should always be available (in English terminology explanation). <br><br>  As part of the explanation, it is not bad to show the evaluation of the goods by the neighbors, by which attribute (for example, actor or director) was a coincidence, and also to deduce the confidence of the system in the evaluation (confidence).  In order not to overload the interface, you can put all this information into the ‚ÄúTell me more‚Äù button. <br><br>  For example: <br><br><ul><li>  "You may like the film ... because it plays there ... and ...". </li><li>  "Users with similar musical tastes have rated the album ... 4.5 out of 5". </li></ul><br><h3>  <font color="#0057b6">Summary</font> </h3><br>  This concludes the first part of the article.  We reviewed the general formulation of the problem, talked about non-personal recommendations, described two classical approaches (content-based and collaborative filtering), and also touched upon the rationale for recommendations.  In general, these two approaches are quite enough to build a production-ready recommender system.  In the next part, I will continue the review and talk about more modern methods, including those involving neural networks and deep learning, as well as about hybrid models. <br><br><div class="spoiler">  <b class="spoiler_title">In the meantime, look at our jobs.</b> <div class="spoiler_text"><ul><li>  <a href="https://job.lanit.ru/vacancy/Pages/CD-03.aspx%3Futm_source%3Dhabr%26utm_medium%3Dpost-2018-08-21%26utm_campaign%3Dcleverdata">Frontend architect</a> </li><li>  <a href="https://job.lanit.ru/vacancy/Pages/CD-15.aspx%3Futm_source%3Dhabr%26utm_medium%3Dpost-2018-08-21%26utm_campaign%3Dcleverdata">Java Developer (Big Data)</a> </li><li>  <a href="https://job.lanit.ru/vacancy/Pages/CD-14.aspx%3Futm_source%3Dhabr%26utm_medium%3Dpost-2018-08-21%26utm_campaign%3Dcleverdata">Linux system engineer</a> </li></ul></div></div></div><p>Source: <a href="https://habr.com/ru/post/420499/">https://habr.com/ru/post/420499/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../420489/index.html">New ARM processors will be able to contend with the Core i5</a></li>
<li><a href="../420491/index.html">My way is a warrior, or how I prepared an application for life in Sailfish</a></li>
<li><a href="../420493/index.html">Can the American food ordering service become Amazon in the world of restaurants?</a></li>
<li><a href="../420495/index.html">Water rendering in screen space</a></li>
<li><a href="../420497/index.html">Vegetable Singularity: Kroger Launches Robocouriers for Fruit and Vegetable Customers in Arizona</a></li>
<li><a href="../420501/index.html">Linux in RAM: debirf way 2018</a></li>
<li><a href="../420503/index.html">JS Developer Day, different cities and communities - one holiday</a></li>
<li><a href="../420505/index.html">Will OpenAI Five win the professional team at The International?</a></li>
<li><a href="../420507/index.html">Reference: global Internet for all and its creators</a></li>
<li><a href="../420509/index.html">Unobvious problem of using assert</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>