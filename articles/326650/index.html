<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Introduction to machine learning with tensorflow</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="If in the next five years we build a machine with the intellectual abilities of one person, then its successor will already be more intelligent than a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Introduction to machine learning with tensorflow</h1><div class="post__text post__text-html js-mediator-article"><blockquote>  <i>If in the next five years we build a machine with the intellectual abilities of one person, then its successor will already be more intelligent than all humanity together.</i>  <i>After one or two generations, they simply stop paying attention to us.</i>  <i>Just as you do not pay attention to the ants in your yard.</i>  <i>You do not destroy them, but you do not tame them, they have practically no effect on your daily life, but they are there.</i> <br>  <b>Seth Shostak</b> </blockquote><br><h3>  Introduction </h3><br>  A series of my articles is an extended version of what I wanted to see when I decided to get acquainted with neural networks.  It is designed primarily for programmers who want to get acquainted with tensorflow and neural networks.  I do not know, fortunately or unfortunately, but this topic is so extensive that even a little bit informative description requires a large amount of text.  Therefore, I decided to divide the narration into 4 parts: <br><br><ol><li>  Introduction, familiarity with tensorflow and basic algorithms (this article) </li><li>  First neural networks </li><li>  Convolutional neural networks </li><li>  Recurrent Neural Networks </li></ol><br>  The first part set out below is aimed at explaining the basics of working with tensorflow and in passing telling how machine learning works in principle, using the example of tensorfolw.  In the second part, we will finally begin to design and train neural networks, incl.  multilayer and pay attention to some of the nuances of preparing training data and the choice of hyperparameters.  Since convolutional networks are now very popular, the third part is dedicated to a detailed explanation of their work.  Well, in the final part of the story about recurrent models is planned, in my opinion, this is the most difficult and interesting topic. <br><a name="habracut"></a><br><h3>  Tensorflow installation </h3><br>  Although the description of the installation of tensorflow is not the goal of the article, I will briefly describe the installation process of the cpu-version for 64-bit windows systems and add-ons used later in the text.  <a href="https://www.tensorflow.org/install/">In general, the installation procedure can be viewed on the website tensorflow.</a> <br><br><ol><li>  Download and install python version 3.5. * (The <a href="">latest version at the time of writing 3.5.3</a> ).  During the installation, check the box ‚ÄúAdd Python 3.5 to PATH‚Äù.  If you do not want to add directories of this version of python to environment variables, for example, due to the active use of another version of the interpreter, then you should perform the further steps from the Scripts folder of the specified version of the distribution kit (cd "path to python 3.5 / Scripts"). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    </li><li>  After installation, run the command line (exactly after installation, otherwise the python directories will not get into the PATH environment variable). <br><br></li><li>  Next, run the commands: <br><br><ol><li>  pip update: "pip install --upgrade pip" </li><li>  setuptools update: "pip install -U pip setuptools" </li><li>  Installing tensorflow 1.0.1 under CPU: "pip install --ignore-installed --upgrade <a href="https://ci.tensorflow.org/view/Nightly/job/nightly-win/DEVICE%3Dcpu">ci.tensor.org/view/Nightly/job/nightly-win/DEVICE=cpu</a> , OS = windows / lastSuccessfulBuild / artifact / cmake_build / tf_python /dist/tensorflow-1.0.1-cp35-cp35m-win_amd64.whl ¬ª </li><li>  install matplotlib (for charts): ‚Äúpip install matplotlib‚Äù </li><li>  Jupyter installation: "pip install jupyter" <br></li></ol></li><li>  The installation is complete, to launch Jupyter, execute the ‚Äújupyter notebook‚Äù command and in the opened tab you can open the ipynb version of the article ( <a href="https://gist.githubusercontent.com/ins2718/1d56104d3abf746f90c90678b3c77842/raw/d942a6a0e0bb6bedd72622485eb74c489175a263/1.ipynb">take here</a> ). </li></ol><br>  Below is a <a href="">script for vbs</a> , if you just want to quickly install all the necessary software, without going into details, then just run it and follow the instructions: <br><br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text"><pre><code class="vbscript hljs"><span class="hljs-comment"><span class="hljs-comment">'   - Function GetPythonVersion() On Error Resume Next Err.Clear GetPythonVersion = vbNullString Set WshShell = CreateObject("WScript.Shell") Set WshExec = WshShell.Exec("python --version") If Err.Number = 0 Then '     Set TextStream = WshExec.StdOut Str = vbNullString While Not TextStream.AtEndOfStream Str = Str &amp; Trim(TextStream.ReadLine()) &amp; vbCrLf Wend Set objRegExp = CreateObject("VBScript.RegExp") objRegExp.Pattern = "(\d+\.?)+" objRegExp.Global = True Set objMatches = objRegExp.Execute(Str) PythonVersion = "0" For i=0 To objMatches.Count-1 '      PythonVersion = objMatches.Item(i).Value Next GetPythonVersion = PythonVersion Else Err.Clear End If End Function Function DownloadPython() Err.Clear Set x = CreateObject("WinHttp.WinHttpRequest.5.1") call x.Open("GET", "https://www.python.org/ftp/python/3.5.3/python-3.5.3-amd64-webinstall.exe", 0) x.Send() Set s = CreateObject("ADODB.Stream") s.Mode = 3 s.Type = 1 s.Open() s.Write(x.responseBody) call s.SaveToFile("python-3.5.3-amd64-webinstall.exe", 2) DownloadPython = "python-3.5.3-amd64-webinstall.exe" End Function Function InstallPython() InstallPython = False PythonVersion = GetPythonVersion() If Mid(PythonVersion, 1, 3)="3.5" Then InstallPython = True Else txt = vbNullString If Len(PythonVersion) &gt; 0 Then txt = "    " Else txt = "  " End If If MsgBox(txt &amp; vbCrLf &amp; "  ?", 4) = 6 Then MsgBox("      'Add Python 3.5 to PATH'") Set WshShell = WScript.CreateObject("WScript.Shell") WshShell.Run DownloadPython(), 0, True MsgBox("  ,      ") End If End If End Function If InstallPython() Then Set WshShell = WScript.CreateObject("WScript.Shell") ' tensorflow WshShell.Run "pip install --upgrade pip", 1, True WshShell.Run "pip install --ignore-installed --upgrade https://ci.tensorflow.org/view/Nightly/job/nightly-win/DEVICE=cpu,OS=windows/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tensorflow-1.0.1-cp35-cp35m-win_amd64.whl", 1, True WshShell.Run "pip install -U pip setuptools", 1, True WshShell.Run "pip install matplotlib" , 1, True WshShell.Run "pip install jupyter" , 1, True If MsgBox(" ,  Jupyter notebook?", 4) = 6 Then WshShell.Run "jupyter notebook" , 1, False End If End If</span></span></code> </pre> <br></div></div><br><h3>  Introduction to tensorflow </h3><br>  The principles of working with tensorflow are quite simple.  We must compile a graph of operations, then transfer data to this graph and give the command to make calculations.  In the picture below you can see 3 examples of such graphs: <br><img src="https://habrastorage.org/files/cbe/56b/106/cbe56b10650e48148625e1e2f9ce2a95.png" alt="image"><br>  The graph on the left contains only one vertex representing a constant with a value of 1. Hereinafter, in such illustrations, vertices with constants will be indicated by circles with gray hatching, and without hatching vertices with operations.  The central graph illustrates the addition operation.  If we ask tensorflow to calculate the value of the vertex representing the operation of addition, it will calculate the values ‚Äã‚Äãof the edges of the graph directed to it and sum them (that is, it will return 3).  In the right column we have two vertices with operations - subtraction and squaring.  If we try to calculate the vertex representing the squaring, then the tensorflow will first perform the subtraction.  I think the concept of computing graphs will not cause anyone any difficulties. <br><br>  You can create an empty graph with the tf.Graph () function, besides, the graph is created by default when the library is connected and if you do not explicitly specify the graph, then it will be used.  The example below shows how to create two constants in two different columns. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-comment"><span class="hljs-comment">#       #   -   default_graph = tf.get_default_graph() #     - c1 = tf.constant(1.0) #    second_graph = tf.Graph() with second_graph.as_default(): #         c2 = tf.constant(101.0) print(c2.graph is second_graph, c1.graph is second_graph) # True, False print(c2.graph is default_graph, c1.graph is default_graph) # False, True</span></span></code> </pre> <br>  Data transfer and operations take place in sessions.  A session is started by calling tf.Session, and its closing by calling the close method on the session object.  You can use the with clause, which automatically closes the session: <br><br><pre> <code class="python hljs">default_graph = tf.get_default_graph() c1 = tf.constant(<span class="hljs-number"><span class="hljs-number">1.0</span></span>) second_graph = tf.Graph() <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> second_graph.as_default(): c2 = tf.constant(<span class="hljs-number"><span class="hljs-number">101.0</span></span>) session = tf.Session() <span class="hljs-comment"><span class="hljs-comment">#     - print(c1.eval(session=session)) # print(c2.eval(session=session)) #  ,    session.close() #  : with tf.Session() as session: print(c1.eval()) #       eval #   : with tf.Session(graph=second_graph) as session: print(c2.eval()) #       eval #: # 1.0 # 1.0 # 101.0</span></span></code> </pre> <br>  I hope about the graphs and sessions in general, clearly, their functionality will not be understood in detail here, those who want to thoroughly understand these mechanisms should get acquainted directly with the documentation.  And then we proceed to the construction of graphs.  In the previous examples, constants were added to the graph, and it is time to find out what they are and how they differ from placeholders and variables.  In the example below, a more complex graph is constructed representing the expression <math> </math> $ inline $ a \ cdot x + b $ inline $   . <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   a.           #       ,      : # a = tf.constant(2.0) #  : # value ( ) -   # shape - . : [] - , [5] -   5 , [2, 3] -  2x3(2   3 ) # dtype -   ,     https://www.tensorflow.org/api_docs/python/tf/DType # name -  .            a = tf.constant(2.0, shape=[], dtype=tf.float32, name="a") #   x #         #       ,    : # initial_value -     # dtype - , name - ,     x = tf.Variable(initial_value=3.0, dtype=tf.float32) #           ,      #       placeholder #          ,     #       b = tf.placeholder(tf.float32, shape=[]) #     ,        f = tf.add(tf.multiply(a, x), b) #     f = a*x + b with tf.Session() as session: #        #      x tf.global_variables_initializer().run() #     f   #   feed_dict    placeholder' #    b = -5 #      ,    result_f, result_a, result_x, result_b = session.run([f, a, x, b], feed_dict={b: -5}) print("f = %.1f * %.1f + %.1f = %.1f" % (result_a, result_x, result_b, result_f)) print("a = %.1f" % a.eval()) #   ,    #  eval    run  ,       ( feed_dict) #      ,   : x = x.assign_add(1.0) print("x = %.1f" % x.eval()) # : # f = 2.0 * 3.0 + -5.0 = 1.0 # a = 2.0 # x = 4.0</span></span></code> </pre> <br>  So, a placeholder is a node through which new data will be transferred to the model, and a variable (Variable) is a node that may change as the graph runs.  I hope that the above material is clear to everyone, because  it is just enough to start learning the first model.  In the previous code snippet we compiled a linear function graph <math> </math> $ inline $ a \ cdot x + b $ inline $   , now let's go a little further and approximate the function <math> </math> $ inline $ a \ cdot x + b $ inline $   over a set of points.  Yes, I know that everyone has already been bothered by this task, as well as the recognition of characters and a series of cliche examples, but put up with it, you have to go through all of them ... <br><br><h3>  First learning algorithm </h3><br>  In order for tensorflow to train the model, we need to add 2 more things: the loss function and the optimization algorithm itself. <br><br>  The loss function is a function that takes the value of the function predicted by the model and the actual value, and returns the distance between them (we will call this value an error).  For example, if we predict a real value, then as the loss function, we can take the square of the difference of the arguments or the modulus of their difference.  If we have a classification task, then the loss function can return 0 for the correct answer and 1 for errors.  Roughly speaking, the loss function should return a non-negative real number and it should be the greater, the more the model makes a mistake, and then the task of training the model is reduced to minimization.  And although the last sentence is not entirely correct, it fully reflects the idea of ‚Äã‚Äãmachine learning. <br><br>  Of the optimization methods, we consider only the classical gradient descent.  Much has already been written about him, so I will not disassemble it "brick by brick" and go into details (the material is not so small anyway).  However, it must be understood, so I will try to explain the method briefly and clearly with the help of visualizations.  Below are 2 options of the same schedule - <math> </math> $ inline $ \ sin \ left (\ frac12x ^ 2- \ frac14y ^ 2 \ right) + \ cos (2x + 1) $ inline $   .  The task of the method is to find a local minimum, i.e.  from a point (taken at random, on a chart <math> </math> $ inline $ \ left (\ frac12; \ frac12 \ right) $ inline $   ) get into the recess (blue zone in the graphs). <br><br><div class="spoiler">  <b class="spoiler_title">Pictures</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/958/455/4b7/9584554b7b36499b9ee53a3a2b04ecb2.png" alt="image"><br><img src="https://habrastorage.org/files/06f/f7a/fd7/06ff7afd796447c49f009acf65a6a34e.png" alt="image"><br></div></div><br>  The essence of the method is to go in the direction opposite to the gradient function at the current point.  The gradient is a vector that points in the direction of the largest growth of the function.  Mathematically, this is a vector of derivatives for all arguments - <math> </math> $ inline $ \ mathrm {grad} (f) = \ nabla f = \ left (\ frac {\ partial f} {\ partial x}, \; \ frac {\ partial f} {\ partial y} \ right) $ inline $   .  The function is taken at random and we will not carry out calculations on it, for practice we have a simpler example, first look at the visualization of several steps of the algorithm: <br><br><div class="spoiler">  <b class="spoiler_title">Gif</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/ad9/135/65a/ad913565a76a416bbdda885a3f3c05d3.gif" alt="image"><br></div></div><br>  Separately, it is worth mentioning the speed with which you need to move to a minimum (with reference to the machine learning task, this will be called the learning rate).  To get the first results, we just need to pick a fixed speed.  However, it is often a good idea to lower it in the course of the algorithm, i.e.  move less and less steps.  While this is sufficient, we will analyze the method in more detail with practice, as necessary. <br><br>  In the following example, we will try to restore the value of the function <math> </math> $ inline $ 2x-3 $ inline $   on the interval from -2 to 2 by 50 points with normally distributed noise.  We will train the model in sets (packages) of 5 points each using a stochastic gradient descent (eng. SGD - Stochastic Gradient Descent).  Let's go straight to the code. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf %matplotlib inline <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt samples = <span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-comment"><span class="hljs-comment">#   packetSize = 5 #   def f(x): return 2*x-3 #   x_0 = -2 #   x_l = 2 #   sigma = 0.5 #    np.random.seed(0) #    (          ) data_x = np.arange(x_0,x_l,(x_l-x_0)/samples) #  [-2, -1.92, -1.84, ..., 1.92, 2] np.random.shuffle(data_x) # ,    data_y = list(map(f, data_x)) + np.random.normal(0, sigma, samples) #      print(",".join(list(map(str,data_x[:packetSize])))) #    print(",".join(list(map(str,data_y[:packetSize])))) #     tf_data_x = tf.placeholder(tf.float32, shape=(packetSize,)) #        tf_data_y = tf.placeholder(tf.float32, shape=(packetSize,)) #        weight = tf.Variable(initial_value=0.1, dtype=tf.float32, name="a") bias = tf.Variable(initial_value=0.0, dtype=tf.float32, name="b") model = tf.add(tf.multiply(tf_data_x, weight), bias) loss = tf.reduce_mean(tf.square(model-tf_data_y)) #  ,    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss) #  ,     with tf.Session() as session: tf.global_variables_initializer().run() for i in range(samples//packetSize): feed_dict={tf_data_x: data_x[i*packetSize:(i+1)*packetSize], tf_data_y: data_y[i*packetSize:(i+1)*packetSize]} _, l = session.run([optimizer, loss], feed_dict=feed_dict) #     "" print(": %f" % (l, )) print("a = %f, b = %f" % (weight.eval(), bias.eval())) plt.plot(data_x, list(map(lambda x: weight.eval()*x+bias.eval(), data_x)), data_x, data_y, 'ro')</span></span></code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Conclusion</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/3d4/435/103/3d44351039f04d35935543e447f9c596.png" alt="image"><br></div></div><br>  Our graph looks like this: <br><img src="https://habrastorage.org/files/0e5/461/cb2/0e5461cb281f4aec9ff8a05db0c99fab.png" alt="image"><br>  input nodes are highlighted in green, and variables to be optimized in red. <br><br>  The first thing that should be noticed is the discrepancy between the dimensions of the input nodes and the variables.  Input nodes accept arrays of 5 elements each, and variables are numbers.  This is called <a href="https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">batch computing (broadcasting)</a> .  Roughly speaking, when it is necessary to perform calculations on arrays, one of which has an extra dimension, calculations are performed separately for each element of a larger array and the result will be an array of greater dimension.  Those.  [1,2,3,4,5] + 1 = [2,3,4,5,6], it is rather difficult to formulate, but it should be intuitively clear. <br><br>  Let's manually recalculate the actions of the algorithm, I think this is the best way to understand what is happening.  So, the arguments are passed to the inputs - [0.24, -1.12, -1.2, 1.28, -1.84] and the values ‚Äã‚Äã[-2.72, -5.65, -5.61, -0.70, -6.27] (rounded to hundredths).  First, we batchly calculate the value of the function, let me remind you that after initializing the variables, the function looks like <math> </math> $ inline $ 0.1 \ cdot x + 0 $ inline $   .  We substitute each argument: <br><br><p><math> </math> $$ display $$ \ left [\ begin {matrix} 0.1 \ cdot 0.24 + 0 = 0.024 \\ 0.1 \ cdot -1.12 + 0 = -0.112 \\ 0.1 \ cdot -1.2 + 0 = -0.12 \\ 0.1 \ cdot 1.28 + 0 = 0.128 \\ 0.1 \ cdot -1.84 + 0 = -0.184 \ end {matrix} \ right. $$ display $$ </p><br>  Further, the obtained values ‚Äã‚Äãare subtracted from the reference values, squared and the average value is calculated: <br><p><math> </math> $$ display $$ \ left [\ begin {matrix} (0.024 - (- 2.72)) ^ 2 \ approx7.53 \\ (-0.112 - (- 5.65)) ^ 2 \ approx30.67 \\ (- 0.12- (-5.61)) ^ 2 \ approx30.14 \\ (0.128-0.7) ^ 2 \ approx0.69 \\ (- 0.184 - (- 6.27)) ^ 2 \ approx37.04 \ end {matrix} \ right. \ Rightarrow \ frac {7.53 + 30.67 + 30.14 + 0.69 + 37.04} 5 \ approx21.21 $$ display $$ </p><br>  a difference of about one hundredth from the value displayed in the log is caused by rounding to hundredths during calculations.  So, we considered a mistake, now it's time to figure out the optimization.  In the graph above, dotted arrows indicate that the optimizer changes variables.  You should already have an intuitive understanding of how gradient descent works.  In this example, a stochastic gradient descent with a speed of 0.5 is used.  Let's take it in order, we optimize the variables a and b, so by them we find the gradient: <br><p><math> </math> $$ display $$ f = (a \ cdot x + b - y) ^ 2 \ Rightarrow \ left \ {\ begin {matrix} \ frac {\ partial f} {\ partial a} = 2x (ax + by) \ \\ frac {\ partial f} {\ partial b} = 2 (ax + by) \ end {matrix} \ right. $$ display $$ </p><br>  We need to improve the value over the entire set of points, so we calculate the average value of the gradient, for convenience, for each variable separately: <br><p><math> </math> $$ display $$ \ begin {matrix} a \ Rightarrow \ frac {1.31712 + (- 12.4051) + (- 13.176) +2.11968 + (- 22.3965)} {5} = - 8.90816 \\ b \ Rightarrow \ frac {5.488 + 11.076 + 10.98 + 1.656 + 12.172} {5} = 8.2744 \ end {matrix} $$ display $$ </p><br>  And finally, we change the values ‚Äã‚Äãof the variables to be optimized, taking into account the given speed: <br><p><math> </math> $$ display $$ \ begin {matrix} a_ {new} = a_ {old} -0.5 \ cdot-8.90816 = 0.1-0.5 \ cdot (-8.90816) = 4.55 \\ b_ {new} = b_ {old} -0.5 \ cdot8.2744 = 0-0.5 \ cdot (-8.90816) = - 4.14 \ end {matrix} $$ display $$ </p><br>  Meanings <math> </math> $ inline $ a_ {new} $ inline $   and <math> </math> $ inline $ b_ {new} $ inline $   and there are the desired values ‚Äã‚Äãof variables.  These calculations are repeated in a loop on each set of points.  Why is this method called stochastic?  Because we calculate the gradient only on a small piece of data (package), and not on all points at once.  Thus, stochastic descent requires much less computation, but does not guarantee a reduction in error at each iteration.  Oddly enough, this ‚Äúnoise‚Äù in terms of the convergence in time can even be useful, since  allows to "get out" from local minima. <br><br>  Actually, this is still possible to finish.  The article turned out even relatively small, which is good.  I am writing this kind of material for the first time, so if you think that the article doesn‚Äôt discuss some of the points in sufficient detail or in the wrong way, please write about it - the article will definitely be updated and corrected based on constructive criticism.  In addition, it will help to better prepare for publication subsequent parts, which will necessarily be posted here (of course, except for the scenario in which the article will be met negatively). <br><br>  In conclusion, I would very much like to thank my friend <a href="https://vk.com/saganenko_nikolai">Nikolai Saganeniko</a> for help in preparing the material.  It is thanks to him that my small cheat sheet for personal use turned into the foregoing stream of consciousness. </div><p>Source: <a href="https://habr.com/ru/post/326650/">https://habr.com/ru/post/326650/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../326640/index.html">How to cram your sensor in Android OS</a></li>
<li><a href="../326642/index.html">Recipe interface</a></li>
<li><a href="../326644/index.html">Scientists from ITMO University have proposed a new system for the transmission of energy over a distance</a></li>
<li><a href="../326646/index.html">Simplify converters for WPF</a></li>
<li><a href="../326648/index.html">St. Petersburg homeless - in Prague. Continuing the history of self-taught developer</a></li>
<li><a href="../326652/index.html">Processing of personal data? No, not to us. Elegant solution from Golos</a></li>
<li><a href="../326654/index.html">About multitenancy</a></li>
<li><a href="../326656/index.html">Data science and quality code</a></li>
<li><a href="../326658/index.html">Resources for startups: action plan in links</a></li>
<li><a href="../326660/index.html">Recover Group Policy Objects (GPOs) with Veeam Explorer for Active Directory</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>