<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Kaggle: how our nets considered sea lions in the Aleutian Islands</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello colleagues! 


 On June 27, the Kaggle Sea Lion (Sea Lion) counting competition on NOAA Fisheries Steller Sea Lions Population Count ended. It c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Kaggle: how our nets considered sea lions in the Aleutian Islands</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/web/69d/2e2/4e3/69d2e24e3a4b42af8cbaa18297b474c1.jpg" alt="header_im"></p><br><p>  Hello colleagues! </p><br><p>  On June 27, the Kaggle Sea Lion (Sea Lion) counting competition on <a href="https://www.kaggle.com/c/noaa-fisheries-steller-sea-lion-population-count">NOAA Fisheries Steller Sea Lions Population Count</a> ended.  It competed 385 teams.  I want to share with you the story of our participation in Challenger and (almost) victory in it. </p><a name="habracut"></a><br><h2 id="nebolshoe-liricheskoe-otstuplenie">  A small lyrical digression. </h2><br><p>  As many already know, Kaggle is a platform for conducting online competitions in Data Science.  And recently, there began to appear more and more tasks from the field of computer vision.  For me, this type of task is the most fascinating.  And the Steller Sea Lions Population Count competition is one of them.  I will narrate with the expectation of a reader who knows the basics of deep learning in relation to pictures, so I will not explain many things in detail. </p><br><p>  A few words about yourself.  I study in graduate school at the University of Heidelberg in Germany.  I am engaged in research in the field of deep learning and computer vision.  The page of our group <a href="https://hci.iwr.uni-heidelberg.de/compvis">CompVis</a> . </p><br><p>  I wanted to participate in the rating competition on Kaggle with prizes.  On this case, I was also shot down by <a href="https://www.kaggle.com/chelovekparohod">Dmitry Kotovenko</a> , who at that time was doing an internship in our research group.  It was decided to participate in the competition in computer vision. </p><br><p>  At that time I had a certain experience of participating in competitions at Kaggle, but only in non-ranking ones, for which they do not give either medals or experience points (Ranking Points).  But I had a fairly extensive experience with imaging through deep learning.  <a href="https://www.kaggle.com/chelovekparohod">Dima</a> had experience at Kaggle in rating competitions, and had 1 bronze medal, but he was just starting to work with computer vision. </p><br><p>  We faced a difficult choice of 3 competitions: the <a href="https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening">prediction of uterine cancer on medical images</a> , the <a href="https://www.kaggle.com/c/planet-understanding-the-amazon-from-space">classification of satellite images from the forests of the Amazon,</a> and the calculation of sea lions on aerial photographs.  The first was dropped due to visually not very pleasant pictures, and between the second and the third was chosen the third because of the earlier deadline. </p><br><h2 id="postanovka-zadachi">  Formulation of the problem </h2><br><p>  In connection with a significant decrease in the sea lions population in the western Aleutian Islands (owned by the USA) over the past 30 years, scientists from the NOAA Fisheries Alaska Fisheries Science Center keep a constant record of the number of individuals using aerial photographs from drones.  Until that time, the individuals were counted on photographs by hand.  It took biologists up to 4 months to calculate the number of sea lions in the thousands of photographs received by NOAA Fisheries every year.  The task of this competition is to develop an algorithm for automatic calculation of sea lions on aerial photographs. </p><br><p>  All Sivuchi are divided into 5 classes: </p><br><ol><li>  <em>adult_males</em> - adult males ( <img src="https://habrastorage.org/web/8df/a53/fca/8dfa53fca0e04c26a334275852bfcf84.png">  ), </li><li>  <em>subadult_males</em> - young males ( <img src="https://habrastorage.org/web/1c6/e57/bd2/1c6e57bd23ed496fb1fb2253e0b02733.png">  ), </li><li>  <em>adult_females</em> - adult females ( <img src="https://habrastorage.org/web/63a/4ee/36c/63a4ee36ca7544adbd7704423a90d3a6.png">  ), </li><li>  <em>juveniles</em> - teenagers ( <img src="https://habrastorage.org/web/02c/a8f/85b/02ca8f85b7ae48058d46119ea86583da.png">  ), </li><li>  <em>pups</em> - cubs ( <img src="https://habrastorage.org/web/b32/fef/efd/b32fefefdfa24b359b14dc8f62c11fad.png">  ). </li></ol><br><p>  948 training pictures are given, for each of which Ground Truth knows the number of individuals of each class.  It is required to predict the number of individuals by classes on each of the 18641 test images.  Here is an example of some parts from dataset. </p><br><p><img src="https://habrastorage.org/web/a85/77d/a0c/a8577da0cd12411f844dc1738a470a01.png"></p><br><p>  Pictures of different resolutions: 4608x3456 to 5760x3840.  The quality and scale is very diverse, as can be seen from the example above. </p><br><p>  The position on the leaderboard is determined by the RMSE error averaged over all test images and over the classes. </p><br><p>  As a bonus, the organizers provided copies of training images with sea lions marked with dots of different colors.  Each color corresponded to a specific class.  All these points were hand-labeled by someone else (I hope, by biologists), and they were not always clearly in the center of the animal.  That is, in fact, we have a rough position of each individual, given by one point.  It looks like this. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/270/f99/f88/270f99f8857c8443f76c7089ece1eb6b.png" alt="image"><br>  <sub><sup>(image credits to bestfitting)</sup></sub> </p><br><p>  The most frequent sea lion classes are females ( <img src="https://habrastorage.org/web/63a/4ee/36c/63a4ee36ca7544adbd7704423a90d3a6.png">  ), teenagers ( <img src="https://habrastorage.org/web/02c/a8f/85b/02ca8f85b7ae48058d46119ea86583da.png">  ) and cubs ( <img src="https://habrastorage.org/web/b32/fef/efd/b32fefefdfa24b359b14dc8f62c11fad.png">  ). <br><img src="https://habrastorage.org/web/e4b/9be/555/e4b9be555e5e419ca0a8ab253c99a702.png"><br><img src="https://habrastorage.org/web/bb4/9fd/313/bb49fd313eec47caa3a22c85b172b619.png"></p><br><h2 id="problemy">  Problems </h2><br><p>  Here I will briefly list what were the problems with the data, and with the task as a whole. </p><br><ul><li>  Noisy markup.  Not all training pictures were marked up with all sea lions (often biologists missed those swimming in the water). </li><li>  There is no clear separation between pairs of classes <em>adult_males</em> and <em>subadult_males</em> , <em>adult_females and juveniles</em> .  We do not always even with our eyes can understand where the female is and where the teenager is.  The same problem with adults and young males.  The biologist from NOAA answered the question ‚Äúhow did you mark them?‚Äù At the forum that classes were often distinguished only by behavioral signs.  For example, adult males are surrounded by a multitude of females (Sivuchi live in harems), while young, not yet successful, are forced to huddle alone away from everyone. </li><li>  It is difficult to distinguish a cub from a wet stone.  They are several times smaller than other individuals. </li><li>  No segmentation masks - only the rough position of the animals.  Frontal approach to the segmentation of objects is not applicable. </li><li>  Different scales of images in general.  Including, by eye, the scale on the training images is less than in the test images. </li><li>  The task of counting random objects (not people) is not very covered in scientific articles.  Usually everyone counts people in the photos.  About animals, and especially from several classes, no publications were found. </li><li>  A huge number of test images (18641) of high resolution.  The prediction took from 10 to 30 hours on one Titan X. <br>  Moreover, most of the test images are added solely in order to avoid that participants manually annotate the test.  That is, the predictions on some of them did not affect the final score. </li></ul><br><h2 id="kak-my-reshali">  How we solved </h2><br><p> In Germany, as in Russia, this year there was a big weekend for May 1.  The free days from Saturday to Monday turned out to be more suitable than ever in order to begin to dive into the task.  The competition has lasted for more than a month.  It all started when <a href="https://www.kaggle.com/chelovekparohod">Dima Kotovenko</a> and I read the condition on Saturday. </p><br><p>  The first impression was controversial.  It is a lot of data, there is no settled way how to solve such problems.  But it fueled interest.  Well, not everything is ‚Äúto pack <em>xgboost</em> .‚Äù The goal I set myself was rather modest - just get into the top 100 and get a bronze medal. Although then the goals changed. </p><br><p>  The first 3 days were spent on data processing and writing the first version of the pipeline.  One kind man, <a href="https://www.kaggle.com/radustoicescu">Radu Stoicescu</a> , laid out a kernel that converted points on training images into coordinates and a sea lion class.  It's great that you didn't have to waste your time on this.  I made the first submit only one week after the start. </p><br><p>  Obviously, this problem cannot be solved head-on with semantic segmentation, as there are no Ground Truth masks.  It is necessary either to generate coarse masks by yourself or to train in the spirit of weak supervision.  I wanted to start with something simpler. </p><br><p>  The task of counting the number of objects / people is not new, and we began to look for similar articles.  Several relevant works were found, but all about people counting <a href="https://arxiv.org/abs/1608.06197">CrowdNet</a> , <a href="https://arxiv.org/pdf/1612.00220.pdf">Fully Convolutional Crowd Counting</a> , <a href="http://www.ee.cuhk.edu.hk/~xgwang/papers/zhangLWYcvpr15.pdf">Cross-scene Crowd Counting via Deep Convolutional Neural Networks</a> .  They all had one common idea, based on <em>Fully Convolutional Networks</em> and regressions.  I started with something similar. </p><br><h4 id="ideya-crowd-counting-na-palcah">  The idea of ‚Äã‚Äãcrowd counting on the fingers </h4><br><p>  We want to learn how to predict hitmaps (2D matrices) for each class, but such that it would be possible to sum up the values ‚Äã‚Äãin each of them and get the number of class objects. </p><br><p>  To do this, generate the Grount Truth hitmaps as follows: in the center of each object, draw a Gaussian.  This is convenient because the integral of the Gaussians is 1. We get 5 hitmaps (one for each of the 5 classes) for each picture from the training sample.  It looks like this. <br><img src="https://habrastorage.org/web/30f/cae/593/30fcae5935af4028932187098c7ddc45.png"><br>  <a href="">Zoom</a> </p><br><p>  The standard deviation of the Gaussians for different classes put it in the eye.  For males - more, for young - less.  The neural network (here and further in the text, I mean the convolutional neural network) takes the input images, cut into pieces (tiles) of 256x256 pixels, and spits out 5 hitmaps for each tile.  The loss function is the Frobenius norm of the difference between the predicted hitmaps and the Ground Truth hitmaps, which is equivalent to the L2 norm of the vector obtained by vectoring the difference of the hitmaps.  This approach is sometimes called <em>Density Map Regression</em> .  To get the total number of individuals in each class, we summarize the values ‚Äã‚Äãin each hitmap at the output. </p><br><table><thead><tr><th>  Method </th><th>  Public Leaderboard RMSE </th></tr></thead><tbody><tr><td>  Baseline 1: predict everywhere 0 </td><td>  08.29704 </td></tr><tr><td>  Baseline 2: predict the average for every train </td><td>  26.83658 </td></tr><tr><td>  My <em>Density Map Regression</em> </td><td>  25.51889 </td></tr></tbody></table><br><p>  My decision, based on <em>Density Map Regression</em> , was a little better than baseline and yielded 25.5.  It turned out somehow not very. </p><br><br><p><img src="https://habrastorage.org/web/9fd/c67/2d4/9fdc672d4bb949f296eb48f994702a50.jpg"></p><br><p>  In tasks on sight, it is very useful to look with the eyes on what your network has created, revelations happen.  That's exactly what I did.  I looked at the network predictions - they degenerate to zero in all but one class.  The total number of animals was predicted all the way, but all the sea lions were nets of the same class. </p><br><p>  The original problem that was solved in the articles is to count the number of people in a crowd, that is, there was only one class of objects.  Probably, <em>Density Map Regression is</em> not a good choice for a multi-class task.  And everything is aggravated by a huge variation in the density and scale of objects.  I tried changing L2 to L1 loss function and weighing classes, all of which did not greatly affect the result. </p><br><p>  There was a feeling that the L2 and L1 loss functions do something wrong in case of mutually exclusive classes, and that the pixel-by-pixel cross-entropy loss function may work better.  This gave me the idea to train the network to segment individuals with a pixel-by-pixel cross-entropy loss function.  As a Ground Truth mask, I drew squares centered on previously obtained object coordinates. </p><br><p>  But then a new problem appeared.  How to get the number of individuals from segmentation?  In the ODS chatika, <a href="https://www.kaggle.com/lopuhin">Konstantin Lopukhin</a> admitted that he uses <em>xgboost</em> to regress the number of sea lions over a set of features, calculated by masks.  We wanted to figure out how to make everything end-to-end using neural networks. </p><br><p>  In the meantime, while I was doing crowd counting and segmentation, <a href="https://www.kaggle.com/chelovekparohod">Dima</a> earned a simple orange approach.  He took the <em>VGG-19</em> , trained on the Imagenet classification, and predicted the number of sea lions on the tile.  He used the usual L2 loss function.  It turned out as always - <em>the simpler the method, the better the result</em> . </p><br><p>  So, it became clear that ordinary regression does its job and does well.  The idea of ‚Äã‚Äãsegmentation was happily postponed until better times.  I decided to train <em>VGG-16</em> for regression.  Prisobachil at the end of the output layer for regression on 5 classes of sea lions.  Each output neuron predicted the number of individuals of the corresponding class. </p><br><p>  I came out sharply in the top 20 with RMSE 20.5 on the public leaderboard. </p><br><p><img src="https://habrastorage.org/web/bfd/274/c26/bfd274c265714c5db850609a6bdccb89.png"></p><br><p>  By this time, goal-setting has undergone minor changes.  It became clear that it makes sense to aim not at the top 100, but at least at the top 10.  It did not seem to be something unattainable. </p><br><p>  It turned out that on the test sample, many of the pictures were of a different scale; the sea lions looked larger on them than on the train.  <a href="https://www.kaggle.com/lopuhin">Kostya Lopukhin</a> (thanks to him for this) wrote in the ODS slak that reducing the test images for each dimension by 2 times gave a significant increase in the public leaderboard. </p><br><p>  But <a href="https://www.kaggle.com/chelovekparohod">Dima,</a> too, does not sew a hoop, he <em>twirled</em> something in his <em>VGG-19</em> , reduced the pictures and came out on the <strong>2nd place</strong> with ~ 16 soon. </p><br><h2 id="podbor-arhitektury-i-giperparametrov-seti">  Selection of network architecture and hyperparameters </h2><br><p><img src="https://habrastorage.org/web/2fa/542/608/2fa542608c9c47cabb4c74b2214e9f7d.png"><br>  <sub><sup>(image credits to Konstantin Lopuhin)</sup></sub> </p><br><p>  With the loss function, we all understand.  Time to start experimenting with deeper networks.  <em>VGG-19, ResnetV2-101, ResnetV2-121, ResnetV2-152</em> and heavy artillery - <em>Inception-Resnet-V2 were used</em> . </p><br><p>  <em>Inception-Resnet-V2</em> is an architecture invented by Google, which is a combination of tricks from <em>Inception</em> architectures (inception blocks) and from <em>ResNet</em> architectures (residual connections).  This network is pretty much deeper than the previous ones and this monster looks like this. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/720/df6/320/720df632077d98bc79489d9d7c9cef4d.png"><br>  <sub><sup>(image from research.googleblog.com)</sup></sub> </p><br><p>  In the article <a href="https://arxiv.org/pdf/1602.07261.pdf">"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning", the</a> guys from Google show that this architecture gave at that time a state of the art on Imagenet without using ensembles. </p><br><p>  In addition to the architecture itself, I had to sort out: </p><br><ul><li>  various sizes of input tiles: from 224x224 to 512x512 pixels; </li><li>  type of pooling after convolutional layers: <em>sum-pooling</em> or <em>average-pooling</em> ; </li><li>  the number of additional <abbr title="Fully connected">FC</abbr> -sheets before the final: 0, 1 or 2; </li><li>  number of neurons in additional <abbr title="Fully connected">FC</abbr> layers: 128 or 256. </li></ul><br><p>  The best combination turned out to be: <em>Inception-Resnet-V2-BASE</em> + <em>average-pooling</em> + <abbr title="Fully connected">FC ‚Äî</abbr> layer for 256 neurons + <em>Dropout</em> + final <abbr title="Fully connected">FC ‚Äî</abbr> layer for 5 neurons.  <em>Inception-Resnet-V2-BASE</em> denotes a portion of the original network from the first to the last convolutional layer. </p><br><p>  The best size of the input tile was 299x299 pixels. </p><br><h2 id="augmentacii-izobrazheniy">  Image augmentation </h2><br><p>  For training pictures, we did a typical set of augmentations for CV tasks. <br>  Each tile was applied: </p><br><ul><li>  Random flips from left to right and top to bottom; </li><li>  random rotations at angles that are multiples of 90 degrees; </li><li>  random scaling 0.83 - 1.25 times. <br>  We did not augment the color, as it is a rather slippery track.  Frequently sea lions could only be distinguished from the landscape by color. </li></ul><br><p>  <em>Test time augmentation</em> we did not.  Because the prediction on all test pictures and so took half a day. </p><br><h2 id="prodolzhenie-istorii">  Continuation of a story </h2><br><p>  At some point, while I went through the hyperparameters and network architectures, we teamed up with <a href="https://www.kaggle.com/chelovekparohod">Dima Kotovenko</a> .  <a href="https://www.kaggle.com/asanakoev">At</a> that moment I was the 2nd place, <a href="https://www.kaggle.com/chelovekparohod">Dima is</a> on the 3rd.  To challenge the Chinese in the wrong direction, the team was called <em>"DL Sucks"</em> . <br><img src="https://habrastorage.org/web/f4a/fc3/c30/f4afc3c30510429f8ad84e89b4a7d9a8.png"></p><br><p>  We were united, because it would be unfair to take a medal from someone, because with <a href="https://www.kaggle.com/chelovekparohod">Dima</a> we actively discussed our decisions and exchanged ideas.  <a href="https://www.kaggle.com/lopuhin">Kostya</a> was very happy about this event, we freed him a prize.  From the 4th he got to the 3rd. </p><br><p>  For the last 3-4 weeks of the competition, we kept tightly on the 2nd place on the public leaderboard, one-tenth each, improving one-thousandth quickly by going through hyperparameters and stacking models. </p><br><p>  It seemed to us that a 2-fold reduction in all test images was rough and rough.  I wanted to do everything beautifully so that the network would predict how to scale each of the images.  Dima invested in this idea quite a lot of time.  In short, he tried to predict the scale of sea lions according to the picture, which is equivalent to predicting the altitude of the drone during the shooting.  Unfortunately, due to the lack of time and a number of problems that we encountered, this was not brought to an end.  For example, many pictures contain only one sea lion, and most of the space is sea and stones.  Therefore, not always, looking only at the rocks or the sea, it is possible to understand from what height the picture was taken. </p><br><p>  A couple of days before the deadline, we collected all the best models and made an ensemble of 24 neural networks.  All models had the best Inception-Resnet-V2 architecture, which I described earlier.  The models differed only in how aggressively we augmented the pictures, on what scale of test images the predictions were made.  Outputs from different networks were averaged. </p><br><p>  The team <em>"DL Sucks"</em> finished the competition on the 2nd place on the public leaderboard, which could not but rejoice, since we were "in the money."  We understood that everything can change on the private leaderboard and we can be thrown out of the top ten altogether.  We had a decent gap with the 4th and 5th place, and this added to our confidence.  This is how the position on the leaderboard looked: </p><br><blockquote>  <em>1st place 10.98445 <a href="https://www.kaggle.com/outrunner">outrunner</a> (Chinese 1)</em> <em><br></em>  <em><strong>2nd place 13.29065 We (DL Sucks)</strong></em> <em><br></em>  <em>3rd place 13.36938 <a href="https://www.kaggle.com/lopuhin">Kostya Lopukhin</a></em> <em><br></em>  <em>4th place 14.03458 <a href="https://www.kaggle.com/bestfitting">bestfitting</a> (Chinese 2)</em> <em><br></em>  <em>5th place 14.47301 LeiLei-WeiWei (Team of two Chinese)</em> </blockquote><p>  It remained to wait for the final results ... </p><br><p>  And what would you think?  The Chinese have bypassed us!  We were moved from 2nd to 4th place.  Well, nothing, but got a gold medal;) </p><br><p>  The first place, as it turned out, was taken by another Chinese, alpha goose <a href="https://www.kaggle.com/outrunner">outrunner</a> .  And his decision was almost like ours.  He trained <em>VGG-16</em> with an additional fully connected layer for 1024 neurons to predict the number of individuals in classes.  What brought him into the first place was the ad-hoc increase in the number of teenagers by 50% and the decrease in the number of females by the same number, multiplying the number of cubs by 1.2.  Such a trick would lift us up several positions. </p><br><p>  Final position of places: </p><br><blockquote>  <em>1st place 10.85644 <a href="https://www.kaggle.com/outrunner">outrunner</a> (Chinese 1)</em> <em><br></em>  <em>2nd place 12.50888 <a href="https://www.kaggle.com/lopuhin">Kostya Lopukhin</a></em> <em><br></em>  <em>3rd place 13.03257 (Chinese 2)</em> <em><br></em>  <em><strong>4th place 13.18968 We (DL Sucks)</strong></em> <em><br></em>  <em>5th place 14.47301 <a href="https://www.kaggle.com/dmytropoplavskiy">Dmitro Poplavsky</a> (also in ODS) in a team with 2 others</em> </blockquote><br><h2 id="paru-slov-o-drugih-resheniyah">  A few words about other solutions. </h2><br><p>  There is a reasonable question - is it possible to train the detector and then count the bouncing boxes of each class?  The answer is - you can.  Some guys did.  <a href="https://www.kaggle.com/albuslaev">Alexander Buslaev</a> (13th place) trained <em>SSD</em> , and <a href="https://www.kaggle.com/iglovikov">Vladimir Iglovikov</a> (49th place) - <em>Faster RCNN</em> . </p><br><p>  Example prediction of <a href="https://www.kaggle.com/iglovikov">Vladimir</a> : </p><br><p><img src="https://habrastorage.org/web/ec6/2cc/5df/ec62cc5dfba5480aad46e8f060a708fe.jpg"><br>  <sub><sup>(image credits to Vladimir Iglovikov)</sup></sub> </p><br><p>  The disadvantage of this approach is that he is greatly mistaken when the sea lions in the photo are very close to each other.  And the presence of several different classes also aggravates the situation. </p><br><p>  The decision based on segmentation with the help of <em>UNet</em> also has a place to be and brought <a href="https://www.kaggle.com/lopuhin">Konstantin</a> to the 2nd place.  He predicted the small squares that he drew inside each sea lion.  Next - dancing with a tambourine.  According to the predicted hitmaps, <a href="https://www.kaggle.com/lopuhin">Kostya</a> calculated various features (areas above specified thresholds, the number and probabilities of blobs) and fed them to <em>xgboost</em> to predict the number of individuals. </p><br><p><img src="https://habrastorage.org/web/689/cc5/606/689cc5606f644dc6b47136bf9e2866e2.png"><br>  <sub><sup>(image credits to Konstantin Lopuhin)</sup></sub> </p><br><p>  More information about his decision can be viewed on <a href="https://www.youtube.com/watch%3Fv%3DPZfD-StZltk">youtube</a> . </p><br><p>  At the 3rd place ( <a href="https://www.kaggle.com/bestfitting">bestfitting</a> ), the decision is also based on UNet.  I will describe it in a nutshell.  The guy manually marked out the segmentation masks for 3 images, trained <em>UNet</em> and predicted the masks on another 100 images.  I corrected the masks of these 100 images with my hands and re-trained the network on them.  In his words, it gave a very good result.  Here is an example of his prediction. </p><br><p><img src="https://habrastorage.org/web/59e/b0f/2b4/59eb0f2b4ead42c79a0ab23c19bc443a.png"><br>  <sub><sup>(image credits to bestfitting)</sup></sub> </p><br><p>  To obtain the number of individuals by masks, he used morphological operations and a blob detector. </p><br><h2 id="zaklyuchenie">  Conclusion </h2><br><p>  So, the initial goal was to get at least in the top 100.  We have fulfilled the goal and even exceeded it.  Many different approaches, architectures and augmentations have been tried.  It turned out that the simpler method is better.  And for networks, oddly enough, deeper is better.  <em>Inception-Resnet-V2</em> after dopilivaniya, trained to predict the number of individuals by class, gave the best result. </p><br><p>  In any case, it was a useful experience in creating a good solution to a new task in a short time. </p><br><p>  In graduate school, I mainly <em>research Unsupervised Learning</em> and <em>Similarity Learning</em> .  And to me, even though I do computer vision every day, it was interesting to work with some new task, not related to my main focus.  Kaggle gives you the opportunity to better study different Deep Learning frameworks and try them in practice, as well as implement known algorithms, see how they work on other tasks.  Does Kaggle interfere?  It is unlikely that it interferes, rather it helps, broadens the mind.  Although he takes enough time.  I can say that I spent behind this competition 40 hours a week (just like the second job), doing every day in the evenings and on weekends.  But it was worth it. </p><br><p>  Who read, thank you for your attention and success in future competitions! </p><br><hr><br><p>  My profile on Kaggle: <a href="https://www.kaggle.com/asanakoev">Artem.Sanakoev</a> <br>  A brief technical description of our Kaggle solution: <a href="https://www.kaggle.com/c/noaa-fisheries-steller-sea-lion-population-count/discussion/35442">link</a> <br>  Solution code on github: <a href="https://github.com/asanakoy/kaggle_sea_lions_counting">link</a> </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/337548/">https://habr.com/ru/post/337548/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../337534/index.html">Fads Stream API</a></li>
<li><a href="../337536/index.html">Fix 7 common exception handling errors in java</a></li>
<li><a href="../337538/index.html">‚ÄúYoung people want to buy shares of familiar start-ups on the stock exchange‚Äù: finance trends according to the founder of the Robinhood service</a></li>
<li><a href="../337540/index.html">Setting up a Webpack 3 + Angular 4 development environment: from complex to simple</a></li>
<li><a href="../337546/index.html">Why don't CRM work</a></li>
<li><a href="../337550/index.html">Learn OpenGL. Part 2.4. - Texture Cards</a></li>
<li><a href="../337554/index.html">The path of the Jedi. From a small user to an employee of an IT company</a></li>
<li><a href="../337556/index.html">SAMBA File Server Based on Linux CentOS 7</a></li>
<li><a href="../337558/index.html">What is LinkedList under the hood?</a></li>
<li><a href="../337560/index.html">Why did we do VOD on WebRTC</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>