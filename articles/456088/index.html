<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Big Data Analysis Problems</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="What are the problems of analyzing Big Data? 
 Big Data creates prominent features that are not shared by traditional data sets. These features create...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Big Data Analysis Problems</h1><div class="post__text post__text-html js-mediator-article"><h3>  What are the problems of analyzing Big Data? </h3><br>  Big Data creates prominent features that are not shared by traditional data sets.  These features create significant problems for data analysis and motivate the development of new statistical methods.  Unlike traditional data sets, where the sample size is usually larger than the measurement, Big Data is characterized by a huge sample size and high dimensionality.  First, we will discuss the effect of large sample size on understanding heterogeneity: on the one hand, a large sample size allows us to uncover hidden patterns associated with small subgroups of the population and a weak community among the entire population.  On the other hand, modeling the internal heterogeneity of Big Data requires more complex statistical methods.  Secondly, we will discuss several unique phenomena associated with high dimensionality, including noise accumulation, false correlation, and random endogeneity.  These unique features make traditional statistical procedures null and void. <br><a name="habracut"></a><br><h3>  Heterogeneity </h3><br>  Big Data is often created by combining multiple data sources corresponding to different subgroups.  Each subgroup may exhibit some unique features that are not shared by others.  In classical conditions, when the sample size is small or moderate, data points from small subpopulations are usually classified as ‚Äúdeviations‚Äù, and they are systematically difficult to model due to the insufficient number of observations.  However, in the era of Big Data, the large sample size allows us to better understand heterogeneity, shedding light on research, such as studying the relationship between certain covariates (for example, genes or SNP) and rare results (for example, rare diseases or diseases in small populations) and understanding that why certain treatments (for example, chemotherapy) benefit one population and harm another.  To better illustrate this point, we introduce the following model for the population: <br><br><p><math> </math> $$ display $$ Œª1p1 (y; Œ∏1 (x)) + + Œªmpm (y; Œ∏m (x)), Œª1p1 (y; Œ∏1 (x)) + + Œªmpm (y; Œ∏m (x)), ( 1) $$ display $$ </p><br>  Where Œªj ‚â• 0 represents the fraction of the jth subgroup, pj (y; Œ∏j (x)) is the probability distribution of the response of the jth subgroup, taking into account the covariates of x with Œ∏j (x) as the parameter vector.  In practice, many subpopulations are rarely observed, that is, Œªj is very small.  When the sample size n is moderate, nŒªj may be small, which makes it impossible to derive covariant-dependent parameters Œ∏j (x) due to lack of information.  However, since Big Data is characterized by a large sample size n, the sample size nŒªj for the j-th population can be moderately large, even if Œªj is very small.  This allows us to more accurately draw a conclusion about the parameters of the subpopulation Œ∏j (¬∑).  In short, the main advantage of Big Data is the understanding of the heterogeneity of subpopulations, such as the advantages of certain personalized treatments that are impossible with a small or moderate sample size. <br><br>  Big Data also allows us, due to the large sample size, to reveal a weak community among the entire population.  For example, assessing the benefits of the heart of one glass of red wine a day can be difficult without a large sample size.  Similarly, the health risks associated with exposure to certain environmental factors can be assessed more convincingly only when the sample sizes are large enough. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In addition to the advantages mentioned above, the heterogeneity of Big Data also creates significant problems for statistical inference.  The derivation of a mixture model in (1) for large data sets requires complex statistical and computational methods.  In small dimensions, standard methods can be applied, such as the expect-maximization algorithm for models of finite mixtures.  In large sizes, however, we need to carefully streamline the evaluation procedure to avoid retraining or noise accumulation and develop good computational algorithms. <br><br><h3>  Noise accumulation </h3><br>  Big Data Analysis requires us to simultaneously evaluate and check many parameters.  Estimation errors accumulate when the decision or rule of forecasting depends on a large number of such parameters.  Such an effect of noise accumulation is especially serious in large dimensions and may even dominate the true signals.  This is usually handled by the rarefaction assumption. <br><br>  Take, for example, a multidimensional classification.  Bad classification is due to the presence of many weak points that do not contribute to the reduction of classification errors.  As an example, consider the classification problem when data comes from two classes: <br><br><p><math> </math> $$ display $$ X1, and Y1, ‚Ä¶‚Ä¶, Xn‚àºNd (Œº1, Id), Yn‚àºNd (Œº2, Id) .X1, ..., Xn‚àºNd (Œº1, Id) and Y1,‚Ä¶, Yn‚àº Nd (Œº2, Id).  (2) $$ display $$ </p><br>  We want to build a classification rule that classifies a new observation Z‚ààRdZ‚ààRd either in the first or second class.  To illustrate the effect of noise accumulation in the classification, we set n = 100 and d = 1000. We set Œº1 = 0Œº1 = 0 and Œº2 as sparse, i.e.  only the first 10 Œº2 entries are nonzero with a value of 3, and all other entries are zero.  Figure 1 shows the first two main components using the first m = 2, 40, 200 elements and as many as 1000 elements.  As shown in these graphs, when m = 2, we get a high degree of discrimination.  However, the discriminating power becomes very low when m is too large due to noise accumulation.  The first 10 functions contribute to the classification, and the others do not.  Therefore, when m&gt; 10, the procedures do not receive any additional signals, but accumulate noise: the more m, the more noise accumulates, which worsens the classification procedure due to dimensionality.  When m = 40, the accumulated signals compensate for the accumulated noise, so the first two main components still have good recognition capability.  When m = 200, the accumulated noise exceeds the signal gain. <br><br>  The above discussion motivates the use of sparse models and the choice of variables to overcome the effect of noise accumulation.  For example, in the classification model (2), instead of using all the functions, we could choose a subset of features that achieve the best signal-to-noise ratio.  Such a sparse model provides higher classification efficiency.  In other words, the choice of variables plays a key role in overcoming the accumulation of noise in the classification and prediction of regression.  However, the choice of variables in large dimensions is challenging due to spurious correlation, random endogeneity, heterogeneity, and measurement errors. <br><br><h3>  False correlation </h3><br>  The high dimension also contains a false correlation, referring to the fact that many uncorrelated random variables may have high sampling correlations in large dimensions.  False correlation can lead to erroneous scientific discoveries and incorrect statistical conclusions. <br><br>  Consider the problem of estimating the coefficient vector Œ≤ of the linear model <br><br><p><math> </math> $$ display $$ y = XŒ≤ + œµ, Var (œµ) = œÉ2Id, y = XŒ≤ + œµ, Var () = œÉ2Id, (3) $$ display $$ </p><br>  where y‚ààRny‚ààRn represents the response vector, X = [x1, ..., xn] T‚ààRn √ó dX = [x1, ..., xn] T‚ààRn √ó d represents the design matrix,, ‚ààRnœµ‚ààRn represents the independent vector of the random noise and Id is the d √ó d unit matrix.  To cope with the problem of noise accumulation, when size d is comparable to or larger than sample size n, it is considered that only a small number of variables give the answer, that is, Œ≤ is a sparse vector.  In accordance with this rarefaction assumption, the choice of a variable can be made to avoid noise accumulation, improve prediction performance, and improve the interpretability of a model with a conservative representation. <br><br>  For large sizes, even for such a simple model as (3), the choice of variables is difficult due to the presence of a false correlation.  In particular, with a high dimension, important variables can be strongly correlated with several false variables that are not scientifically related.  Consider a simple example illustrating this phenomenon.  Let x1, ..., xn be independent observations of the d-dimensional Gaussian random vector X = (X1, ..., Xd) T‚àºNd (0, Id) X = (X1, ..., Xd) T‚àºNd (0, Id) .  We repeatedly model data with n = 60 and d = 800 and 6400 1000 times.  Figure 2a shows the empirical distribution of the maximum absolute sample correlation coefficient between the first variable, and the others are defined as <br><br><p><math> </math> $$ display $$ rÀÜ = maxj‚â•2 | CorrÀÜ (X1, Xj) |, r ^ = maxj‚â•2 | Corr ^ (X1, Xj) |, (4) $$ display $$ </p><br>  where Corr ^ (X1, Xj) Corr ^ (X1, Xj) is the selective correlation between the variables X1 and Xj.  We see that the maximum absolute correlation of the sample becomes higher with increasing dimension. <br><br>  In addition, we can calculate the maximum absolute multiple correlation between X1 and linear combinations of several irrelevant side variables: <br><br><p><math> </math> $$ display $$ RÀÜ = max | S | = 4max {Œ≤j} 4j = 1‚à£‚à£‚à£‚à£Corr (X1, j‚ààSŒ≤jXj) ‚à£‚à£‚à£‚à£.R ^ = max | S | = 4max {Œ≤j} j = 14 | Corr ^ (X1, ‚àà j‚ààSŒ≤jXj) |.  (5) $$ display $$ </p><br>  Using the standard configuration, the empirical distribution of the maximum absolute coefficient of the sample correlation between X1 and ‚àëj ‚àà SŒ≤jXj is given, where S is any fourth-size set of {2, ..., d} and Œ≤j is the least squares regression coefficient of Xj by {Xj} regression j ‚àà S. Again, we see that, although X1 is completely independent of X2, ..., Xd, the correlation between X1 and the nearest linear combination of any four variables from {Xj} j ‚â† 1 to X1 can be very high. <br><br>  False correlation has a significant impact on the choice of variables and can lead to erroneous scientific discoveries.  Let XS = (Xj) j ‚àà S be a random vector indexed by S, and let SÀÜS ^ be the chosen set, which has a higher parasitic correlation with X1, as in Fig.  2. For example, when n = 60 and d = 6400, we see that X1 is practically indistinguishable from XSÀÜXS ^ for the set SÀÜS ^ c |  SÀÜ |  = 4 |  S ^ |  = 4‚Å†.  If X1 represents the level of expression of the gene responsible for the disease, we cannot distinguish it from the other four genes in SÀÜS ^, which have similar predictive power, although they are scientifically irrelevant. <br><br>  In addition to the choice of variables, a false correlation can also lead to an incorrect statistical inference.  We explain this by considering again the same linear model as in (3).  Here we would like to evaluate the standard error œÉ of the remainder, which is noticeably manifested in the statistical conclusions of the regression coefficients, the choice of model, the test of conformity and the limit regression.  Let SÀÜS ^ be the set of selected variables, and PSÀÜPS ^ be the projection matrix of the column space XSÀÜXS ^ ‚Å†.  Standard residual variance estimate based on selected variables: <br><br><p><math> </math> $$ display $$ œÉ2 = yT (In ‚àí PS) yn‚àí | SÀÜ | .œÉ ^ 2 = yT (In ‚àí PS ^) yn‚àí | S ^ |.  (6) $$ display $$ </p><br>  Estimator (6) is impartial when variables are not selected by data and the model is correct.  However, the situation is completely different when variables are selected based on data.  In particular, the authors showed that when there are many false variables, œÉ2 is seriously underestimated, this leads to erroneous statistical conclusions, including the choice of models or tests of significance, and erroneous scientific discoveries, such as searching for the wrong genes for molecular mechanisms.  They also offer an improved cross-validation method to ease the problem. <br><br><h3>  Random endogeneity </h3><br>  Random endogeneity is another subtle problem arising from the high dimensionality.  In the regression setting Y = ‚àëdj = 1Œ≤jXj + ŒµY = ‚àëj = 1dŒ≤jXj + Œµ‚Å†, the term ‚Äúendogeneity‚Äù means that some predictors {Xj} correlate with residual noise Œµ.  The usual sparse model suggests <br><br><p><math> </math> $$ display $$ Y = Œ≤jŒ≤jXj + Œµ, and E (ŒµXj) = 0 for j = 1, ..., d, Y = ‚àëjŒ≤jXj + Œµ, and E (ŒµXj) = 0 for j = 1, ..., d , (7) $$ display $$ </p><br>  with a small set S = {j: Œ≤j ‚â† 0}.  The exogenous assumption (7) that the residual noise Œµ does not correlate with all predictors is crucial for the accuracy of most existing statistical methods, including the consistency of the choice of variables.  Although this assumption looks innocent, it is easy to break in large dimensions, since some variables {Xj} randomly correlate with Œµ, which makes most multidimensional procedures statistically invalid. <br><br>  To explain the problem of endogeneity in more detail, suppose that the unknown answer Y is associated with three covariates as follows: <br><br><p><math> </math> $$ display $$ Y = X1 + X2 + X3 + Œµ, withEŒµXj = 0, for j = 1, 2, 3.Y = X1 + X2 + X3 + Œµ, withEŒµXj = 0, for j = 1, 2, 3 . $$ display $$ </p><br>  At the data collection stage, we do not know the true model and therefore collect as many covariates as possible, potentially related to Y, in the hope of including all the terms in S in (7).  By the way, some of these Xj (for jj 1, 2, 3) can be associated with residual noise Œµ.  This refutes the hypothesis of exogenous modeling in (7).  In fact, the more covariates are collected or measured, the more difficult this assumption is. <br><br>  Unlike spurious correlation, random endogeneity refers to the true existence of correlations between unintended variables.  The first is similar to the fact that two people are similar to each other, but do not have a genetic connection, and the second is similar to an acquaintance that easily happens in a big city.  More generally, endogeneity results from selection bias, measurement errors and missing variables.  These phenomena often occur when analyzing Big Data, mainly for two reasons: <br><br><ul><li>  Thanks to the new high-performance measurement methods, scientists can collect as many functions as possible and strive for it.  This, accordingly, increases the likelihood that some of them may be correlated with residual noise. </li><li>  Big Data is usually combined from several sources with potentially different data generation schemes.  This increases the likelihood of selection bias and measurement errors, which also cause potential random endogeneity. </li></ul><br>  Does random endogeneity appear in real data sets and how can we test this in practice?  We are considering a genomics study in which 148 microchip samples are loaded from the GEO and ArrayExpress database.  These samples were created on the Affymetrix HGU133a platform for people with prostate cancer.  The resulting data set contains 22,283 probes, which corresponds to 12,719 genes.  In this example, we are interested in a gene called the ‚Äúdiscoidin domain receptor family, member 1‚Äù (abbreviated DDR1).  DDR1 encodes receptor tyrosine kinases, which play an important role in the association of cells with their microenvironment.  It is known that DDR1 is closely related to prostate cancer, and we want to explore its relationship with other genes in cancer patients.  We took the DDR1 gene expressions as the response variable Y, and the expressions of all the remaining 12,718 genes as predictors.  On the left panel of fig.  3 shows the empirical distribution of correlations between the response and individual predictors. <br><br>  To illustrate the existence of endogeneity, we fit the L1 least squares regression (Lasso) to the data, and the penalty is automatically selected using a 10-fold cross-check (37 genes are selected).  Then we restore the usual least squares regression for the selected model to calculate the residual vector.  On the right panel fig.  3 we build an empirical distribution of correlations between predictors and residuals.  We see that residual noise correlates strongly with many predictors.  To make sure that these correlations are not caused by a purely false correlation, we introduce a ‚Äúzero distribution‚Äù of false correlations by randomly rearranging the row orders in the project matrix, so the predictors do not really depend on the residual noise.  Comparing these two distributions, we see that the distribution of correlations between predictors and residual noise in the raw data (marked as ‚Äúraw data‚Äù) has a ‚Äúheavier tail‚Äù than in the swapped data (marked as ‚Äúswapped data‚Äù).  This result provides strong evidence of endogeneity. </div><p>Source: <a href="https://habr.com/ru/post/456088/">https://habr.com/ru/post/456088/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../456072/index.html">Dav1d - the fastest AV1 decoder now in Firefox by default.</a></li>
<li><a href="../456078/index.html">Projection of corporate conflict on network connectivity</a></li>
<li><a href="../45608/index.html">Sparko</a></li>
<li><a href="../456080/index.html">Which student needs a magician and which AI do we need?</a></li>
<li><a href="../456086/index.html">iOS Storyboards: analysis of the pros and cons, best practices</a></li>
<li><a href="../456090/index.html">Introduction to unit testing in Unity</a></li>
<li><a href="../456094/index.html">Read datasheets 2: SPI on STM32; PWM, timers and interrupts on STM8</a></li>
<li><a href="../456096/index.html">What does the average reader Geektimes, soaring in the clouds</a></li>
<li><a href="../456098/index.html">Russian ICT as the foundation of national information security</a></li>
<li><a href="../4561/index.html">Internet Explorer 7 caused a wave of criticism in the blogosphere</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>