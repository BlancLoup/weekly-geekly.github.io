<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Inventing JPEG</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="You correctly understood from the title that this is not an ordinary description of the JPEG algorithm (I described the file format in detail in the a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Inventing JPEG</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/a78/291/807/a7829180746c99c987384e4b2b6df7b8.png" align="right"><br>  You correctly understood from the title that this is not an ordinary description of the JPEG algorithm (I described the file format in detail in the article <a href="http://habrahabr.ru/post/102521/">"JPEG decoding for dummies"</a> ).  First of all, the chosen method of presenting the material assumes that we know nothing not only about JPEG, but also about the Fourier transform, and Huffman coding.  And in general, there is little to remember from the lectures.  Just took the picture and began to think how it can be compressed.  Therefore, I tried to expressly express only the essence, but at which the reader will have a sufficiently deep and, most importantly, an intuitive understanding of the algorithm.  Formulas and mathematical calculations - at the very minimum, only those that are important for understanding what is happening. <br><br>  Knowledge of the JPEG algorithm is very useful not only for image compression.  It uses the theory of digital signal processing, mathematical analysis, linear algebra, information theory, in particular, Fourier transform, lossless coding, etc. Therefore, this knowledge can be useful anywhere. <br><br>  If there is a desire, then I propose to go through the same stages independently in parallel with the article.  Check how the above reasoning is suitable for different images, try to make your own modifications to the algorithm.  It is very interesting.  As a tool, I can recommend a wonderful bundle of Python + NumPy + Matplotlib + PIL (Pillow).  Almost all of my work (including graphics and animation) was done using them. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Attention, traffic!  Many illustrations, graphs and animations (~ 10Mb).  Ironically, the article about JPEG only 2 images with this format of fifty. <br><a name="habracut"></a><br>  Whatever the algorithm for compressing information, its principle will always be the same - finding and describing patterns.  The more patterns, the more redundancy, the less information.  Archivers and coders are usually ‚Äúsharpened‚Äù for a specific type of information, and they know where to find them.  In some cases, the pattern is visible immediately, for example, a picture of a blue sky.  Each row of its digital representation can be fairly accurately described as straight. <br><br>  We will train on <s>cats</s> raccoons.  The gray image above is taken as an example.  It well combines both homogeneous areas and contrasting.  And if we learn to compress gray, then there will be no problems with color. <br><br><h4>  Vector presentation </h4><br>  First, let's check how dependent two neighboring pixels are.  It is logical to assume that most likely they will be very similar.  Check it out for all image pairs.  Mark them on the coordinate plane with dots so that the point value along the X axis is the value of the first pixel, along the Y axis it is the second one.  For our image of 256 by 256 we get 256 * 256/2 points: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/df5/580/874/df55808741c57a5dcda54ecda0afa663.png"></div><br>  It is predicted that most of the points are on or near the straight line y = x (and there are even more of them than can be seen in the figure, since they repeatedly overlap each other, and, moreover, they are translucent).  And if so, it would be easier to work by turning them through 45 ¬∞.  To do this, express them in a different coordinate system. <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/744/aa0/7e8/744aa07e806018cb6172b96a7e6d064f.png"></div><br>  The base vectors of the new system are obviously: <img src="https://habrastorage.org/getpro/habr/post_images/74e/41c/773/74e41c7737fe8ff8bbdd43e6c639684e.png">  .  They are forced to divide by the root of two to get an orthonormal system (the lengths of the basis vectors are equal to unity).  Here it is shown that some point p = (x, y) in the new system will be represented as a point (a <sub>0</sub> , a <sub>1</sub> ).  Knowing the new coefficients, we can easily get the old reverse turn.  Obviously, the first (new) coordinate is the average, and the second is the difference between x and y (but divided by the root of 2).  Imagine that you are asked to leave only one of the values: either a <sub>0</sub> or a <sub>1</sub> (i.e., equate another to zero).  It is better to choose a <sub>0</sub> , since the value of a <sub>1</sub> is likely to be around zero.  Here's what happens if we restore the image only at a <sub>0</sub> : <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e4d/8ff/b4e/e4d8ffb4ebe6372726c831efe3a91c07.png"></div><br>  4 times magnification: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ea4/a04/132/ea4a04132fdfc8bcf4490af850712c7c.png"></div><br>  Such compression is not very impressive, to be honest.  It is better to similarly divide the image into triples of pixels and represent them in three-dimensional space. <br><img src="https://habrastorage.org/getpro/habr/post_images/2cd/dae/f94/2cddaef943888e2f5086c3c52da3d679.png"><img src="https://habrastorage.org/getpro/habr/post_images/ccb/7f0/9aa/ccb7f09aac3a89a4ae5a68f024e4d94e.png"><br>  This is the same graph, but from different points of view.  The red lines are the axes that have been asking for themselves.  They correspond to the vector: <img src="https://habrastorage.org/getpro/habr/post_images/3ca/980/887/3ca9808879bf8fc30e7a5ccb0bb9c9cf.png">  .  I remind you that you have to divide into some constants so that the lengths of the vectors become equal to one.  Thus, expanding on such a basis, we get 3 values ‚Äã‚Äãa <sub>0</sub> , a <sub>1</sub> , a <sub>2</sub> , and a <sub>0 is</sub> more important than a <sub>1</sub> , and a <sub>1 is</sub> more important than a <sub>2</sub> .  If we drop a <sub>2</sub> , then the graph ‚Äúflattens out‚Äù in the direction of the vector e <sub>2</sub> .  This and so is not quite thick three-dimensional sheet will be flat.  It will lose not so much, but we will get rid of a third of the values.  Compare the images reconstructed in triples: (a <sub>0</sub> , 0, 0), (a <sub>1</sub> , a <sub>2</sub> , 0) and (a <sub>0</sub> , a <sub>1</sub> , a <sub>2</sub> ).  In the last version, we did not throw anything away, so we get the original. <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/810/1cf/e48/8101cfe482a9eb1e3eb8a6972b630fe6.png"></div><br>  4 times magnification: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b1d/e19/83a/b1de1983a838dab12ae6c896b4efce5f.png"></div><br>  The second picture is already good.  The sharp areas smoothed out a bit, but on the whole the picture was preserved very well.  And now, in the same way, we divide into fours and visually define the basis in four-dimensional space ... Oh, yes.  But one can guess what one of the basis vectors will be: (1,1,1,1) / 2.  Therefore, you can look at the projection of the four-dimensional space on the space perpendicular to the vector (1,1,1,1), in order to identify others.  But this is not the best way. <br>  Our goal is to learn how to transform (x <sub>0</sub> , x <sub>1</sub> , ..., x <sub>n-1</sub> ) to (a <sub>0</sub> , a <sub>1</sub> , ..., a <sub>n-1</sub> ) so that each value of a <sub>i</sub> is less important than previous  If we can do this, then perhaps the last sequence values ‚Äã‚Äãcan be thrown away altogether.  The above experiments hint that you can.  But one cannot do without a mathematical apparatus. <br>  So, you need to convert the points to a new basis.  But first you need to find a suitable basis.  Let us return to the first experiment of splitting into pairs.  We will consider it generically.  We defined the basis vectors: <br><img src="https://habrastorage.org/getpro/habr/post_images/96d/6ff/21d/96d6ff21d78376216d9a74ce8aae98cf.png"><br>  Express through them the vector <b>p</b> : <br><img src="https://habrastorage.org/getpro/habr/post_images/26d/aeb/8fd/26daeb8fdcdb4320b421874b40b242be.png"><br>  or in coordinates: <br><img src="https://habrastorage.org/getpro/habr/post_images/a75/9c7/5a5/a759c75a56a3c44a20bb6d9416134ee5.png"><br>  To find a <sub>0</sub> and a <sub>1,</sub> you need to project <b>p</b> onto <b>e</b> <sub>0</sub> and <b>e</b> <sub>1,</sub> respectively.  And for this you need to find the scalar product <br><img src="https://habrastorage.org/getpro/habr/post_images/3f5/9a4/12e/3f59a412e90f3a12a77cf1e76afe3331.png"><br>  similarly: <br><img src="https://habrastorage.org/getpro/habr/post_images/f6e/1ab/850/f6e1ab850ef68758e84a776bd9f793c0.png"><br>  In coordinates: <br><img src="https://habrastorage.org/getpro/habr/post_images/752/11a/e92/75211ae9278fe1ae6061913ef05fc26e.png"><br>  It is often more convenient to perform the transformation in matrix form. <br><img src="https://habrastorage.org/getpro/habr/post_images/e34/77d/6a9/e3477d6a98f06edbbd48cf6d7564f18f.png"><br>  Then A = EX and X = E <sup>T</sup> A. This is a beautiful and convenient form.  The matrix E is called the transformation matrix and is orthogonal; we will still meet with it. <br><br><h4>  Transition from vectors to functions. </h4><br>  It is convenient to work with vectors of small dimensions.  However, we want to find patterns in larger blocks, so instead of N-dimensional vectors it is more convenient to operate with sequences that represent the image.  I will call such sequences discrete functions, since the following reasoning applies to continuous functions. <br>  Returning to our example, we represent such a function f (i), which is defined only at two points: f (0) = x and f (1) = y.  Similarly, we define the basis functions e <sub>0</sub> (i) and e <sub>1</sub> (i) on the basis of the bases <b>e</b> <sub>0</sub> and <b>e</b> <sub>1</sub> .  We get: <br><img src="https://habrastorage.org/getpro/habr/post_images/09d/7c8/5d6/09d7c85d6a8774d731f48254101ae15c.png"><br>  This is a very important conclusion.  Now in the phrase ‚Äúdecomposition of a vector in orthonormal vectors‚Äù we can replace the word ‚Äúvector‚Äù by ‚Äúfunction‚Äù and get a completely correct expression ‚Äúdecomposition of a function in orthonormal functions‚Äù.  It does not matter that we received such a short function, since the same reasoning works for the N-dimensional vector, which can be represented as a discrete function with N values.  And working with functions is clearer than with N-dimensional vectors.  It is possible, and vice versa, to present such a function as a vector.  Moreover, an ordinary continuous function can be represented by an infinite-dimensional vector, though not in a Euclidean, but a Hilbert space.  But we will not go there, we will be interested only in discrete functions. <br>  And our task of finding a basis turns into the problem of finding a suitable system of orthonormal functions.  In the following reasoning, it is assumed that we have somehow defined a set of basis functions by which we will decompose. <br>  Suppose we have a certain function (represented by, for example, values) that we want to represent as a sum of others.  You can represent this process in vector form.  To decompose a function, you need to ‚Äúproject‚Äù it into basic functions in turn.  In the vector sense, the calculation of the projection gives the minimum approximation of the original vector to another in distance.  Remembering that the distance is calculated using the Pythagorean theorem, then a similar representation in the form of functions gives the best rms approximation of the function to another.  Thus, each coefficient (k) determines the "proximity" of the function.  More formally, k * e (x) is the best rms approximation to f (x) among l * e (x). <br>  The following example shows the process of approximating a function using only two points.  On the right is a vector representation. <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f9e/45a/554/f9e45a5548561514e345c1b6f6e56272.png"></div><br>  With reference to our pairing experiment, we can say that these two points (0 and 1 on the abscissa) are a pair of neighboring pixels (x, y). <br>  The same, but with animation: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/196/030/3db/1960303dbb4de3d50326f993c4f7f8b6.gif"></div><br>  If we take 3 points, then we need to consider three-dimensional vectors, but the approximation will be more accurate.  And for a discrete function with N values, we need to consider N-dimensional vectors. <br>  Having a set of derived coefficients, one can easily obtain the original function by summing the basis functions taken with the corresponding coefficients.  Analysis of these coefficients can provide a lot of useful information (depending on the basis).  A special case of these considerations is the principle of expansion in a Fourier series.  After all, our reasoning applies to any basis, and when decomposing into a Fourier series, it is taken quite concretely. <br><br><h4>  Discrete Fourier Transforms (DFT) </h4><br>  In the previous section, we concluded that it would be nice to decompose a function into components.  In the early 19th century, Fourier also thought about this.  True, he did not have a picture with a raccoon, so he had to investigate the distribution of heat along a metal ring.  Then he found out that it is very convenient to express the temperature (and its change) at each point of the ring as the sum of sinusoids with different periods.  ‚ÄúFourier has established (I recommend to <a href="http://www.ega-math.narod.ru/Nquant/Fourier.htm">read</a> , interestingly) that the second harmonic is damped 4 times faster than the first, and the harmonics of higher orders are damped with even greater speed.‚Äù <br>  In general, it soon turned out that the periodic functions are remarkably decomposed into the sum of sinusoids.  And since there are many objects and processes in nature that are described by periodic functions, a powerful tool for their analysis has appeared. <br>  Perhaps one of the most illustrative periodic processes is sound. <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5ff/180/281/5ff1802819f9e6d19a7f777b2fe2389b.png"></div><br><ul><li>  The 1st graph is a pure tone with a frequency of 2500 hertz. </li><li>  2nd - white noise.  That is, noise with evenly distributed frequencies throughout the range. </li><li>  3rd is the sum of the first two. </li></ul>  If I were given the values ‚Äã‚Äãof the last function at that time when I did not know about the Fourier series, and asked to analyze them, then I would definitely be confused and could not say anything worthwhile.  Well, yes, some function, but how to understand that there is something orderly?  But if I had thought to listen to the last function, the ear would catch a clear tone among the noise.  Although it is not very good, since I specially picked up such parameters during generation so that the signal visually dissolves in the noise on the summary graph.  As I understand it, it is still not exactly fixed, as the hearing aid does.  However, it has recently become clear that he is <a href="http://db.compulenta.ru/veshestvo/fizika/10004301/">not spreading the</a> sound into sinusoids.  Maybe someday we will understand how this happens, and more advanced algorithms will appear.  Well, we are still the old fashioned way. <br>  Why not try to take a sine wave as a basis?  In fact, we have actually done it.  Recall our decomposition into 3 basis vectors and plot them on the graph: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/053/6f4/331/0536f4331f89c092a1570061d281ec72.png"></div><br>  Yes, yes, I know, it looks like a fit, but with three vectors it's hard to expect more.  But now it is clear how to get, for example, 8 basic vectors: <br><img src="https://habrastorage.org/getpro/habr/post_images/937/5fa/7ed/9375fa7ed81a349be30a43ff4c6ddb70.png"><br>  A not very complicated test shows that these vectors are pairwise perpendicular, that is, orthogonal.  This means they can be used as a basis.  Transformation over such a basis is widely known, and is called discrete cosine transform (DCT).  I think from the graphs below it is clear how the DCT-conversion formula is obtained: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/53e/555/16c/53e55516ce3d74cac9bc5013bc85d8f3.png"></div><br>  This is all the same formula: A = EX with a substituted basis.  The basis vectors of the indicated DCT (they are the same row vectors of the matrix E) are orthogonal, but not orthonormal.  This should be remembered during the inverse transformation (I will not dwell on this, but for whom it is interesting, the inverse DCT contains the term 0.5 * a <sub>0</sub> , since the zero basis vector is larger than the others). <br>  The following example shows the process of approximating subtotals to baseline values.  At each iteration, the regular basis is multiplied by the next coefficient and added to the subtotal (that is, just like in the earlier experiments on the raccoon, one third of the values, two thirds). <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3ef/874/757/3ef874757bc85ac97c18567c07707d61.gif"></div><br>  But, nevertheless, despite some arguments about the advisability of choosing such a basis, there are no real arguments yet.  Indeed, unlike sound, the expediency of decomposing an image into periodic functions is much less obvious.  However, the image may indeed be too unpredictable even in a small area.  Therefore, the picture is divided into fairly small pieces, but not quite tiny, so that the decomposition makes sense.  In JPEG, the image is ‚Äúsliced‚Äù into 8x8 squares.  Within the limits of such a slice, photographs are usually very uniform: a background plus small fluctuations.  Such areas are smartly approaching with sinusoids. <br>  Well, let's say this fact is more or less intuitive.  But there is a bad feeling about sharp color transitions, because slowly changing functions will not save us.  It is necessary to add different high-frequency functions that cope with their work, but appear sideways on a uniform background.  Take an image of 256x256 with two contrast areas: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cbc/58c/830/cbc58c830592389e3c2ad047c092c0bc.png"></div><br>  We decompose each row with DCT, thus obtaining 256 coefficients per row. <br>  Then we leave only the first n coefficients, and the rest equate to zero, and, therefore, the image will be represented as a sum of only the first harmonics: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/44b/774/ac6/44b774ac61bf6f8e70a0ad37b8c3dd96.png"></div><br>  The number in the picture is the number of coefficients left.  In the first image, only the mean value remains.  The second has already added one low-frequency sine wave, and so on. By the way, pay attention to the border - despite all the best approximation, 2 bars are clearly visible near the diagonal, one is lighter, the other is darker.  Part of the last image enlarged 4 times: <br><img src="https://habrastorage.org/getpro/habr/post_images/a9c/83d/d90/a9c83dd90310f9f867e83afc33012c20.png"><br>  And generally, if far from the border we see the initial uniform background, then when approaching it, the amplitude begins to grow, finally reaches its minimum value, and then sharply becomes maximum.  This phenomenon is known as the Gibbs effect. <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/17f/a21/8ad/17fa218aded5e3afc54bda4b7c83d827.gif"></div><br>  The height of these humps, appearing near the discontinuities of the function, does not decrease with an increase in the number of the sum of the functions.  In a discrete transformation, it disappears only when almost all coefficients are saved.  More precisely, it becomes invisible. <br>  The following example is completely analogous to the above decomposition of triangles, but already on a real raccoon: <br><img src="https://habrastorage.org/getpro/habr/post_images/f58/765/b50/f58765b50d387766933792e67e1929ec.png"><br>  When studying DCT, one may get the false impression that only the first few (low-frequency) coefficients are always enough.  This is true for many pieces of photos, those whose values ‚Äã‚Äãdo not change dramatically.  However, on the border of contrasting areas, the values ‚Äã‚Äãwill briskly ‚Äúride‚Äù and even the last coefficients will be great.  Therefore, when you hear about the DCT energy conservation property, make a correction that it applies to many types of signals encountered, but not to all.  For example, think about how a discrete function will look, the decomposition coefficients of which are zero, except for the last.  Hint: Imagine decomposition in vector form. <br>  Despite the shortcomings, the chosen basis is one of the best in real photographs.  A little later we will see a small comparison with others. <br><br><h4>  DCT vs everything else </h4><br>  When I studied the issue of orthogonal transformations, then, frankly, I was not very convinced by the arguments that everything around was the sum of harmonic oscillations, therefore, the pictures should also be laid out on sinusoids.  Or maybe some step functions would be better?  Therefore, I was looking for research results on the optimality of DCT on real images.  The fact that ‚ÄúIt is DCT that is most often found in practical applications due to the property of‚Äú energy compacting ‚Äùis written everywhere.  This property means that the maximum amount of information is contained in the first coefficients.  And why?  It is not difficult to conduct a study: we arm ourselves with a bunch of different pictures, different known bases, and begin to consider the standard deviation from the real image for a different number of coefficients.  Found a small study in the <a href="http://scien.stanford.edu/pages/labsite/1997/ee392c/demos/wong/">article</a> (images used <a href="http://scien.stanford.edu/pages/labsite/scien_test_images_videos.php">here</a> ) by this method.  It contains graphs of the dependence of the stored energy on the number of first expansion coefficients over different bases.  If you looked at the charts, you made sure that the DCT is consistently honored ... um ... 3rd place.  How so?  What is KLT conversion?  I praised the DCT, and here ... <br><br><h5>  KLT </h5><br>  All transformations, except KLT, are transformations with a constant basis.  And in KLT (the Karhunen-Loeve transform) the most optimal basis for several vectors is calculated.  It is calculated in such a way that the first coefficients give the smallest root-mean-square error for the total for all vectors.  We carried out similar work earlier manually, visually defining the basis.  At first it seems like a sensible idea.  We could, for example, break an image into small sections and calculate a basis for each one.  But not only is there a concern of storing this basis, so also the operation of calculating it is rather laborious.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> But DCT loses only a little, and besides DCT has fast conversion algorithms. </font></font><br><br><h5><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Dft </font></font></h5><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DFT (Discrete Fourier Transform) is a discrete Fourier transform. Under this name, not only a specific transformation is sometimes mentioned, but also the entire class of discrete transformations (DCT, DST ...). Let's look at the DFT formula: </font></font><br><img src="https://habrastorage.org/getpro/habr/post_images/36f/224/66b/36f22466b401db3e5d47bf98408e214f.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As you might guess, this is an orthogonal transformation with some kind of complex basis. Since such a complex form occurs a little more often than always, it makes sense to study its conclusion. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It may appear that any pure harmonic signal (with an integer frequency) during DCT decomposition will produce only one non-zero coefficient corresponding to this harmonic. This is not the case, because besides the frequency, the phase of this signal is important. For example, the decomposition of a sine in cosines (in a similar way and in discrete decomposition) would be:</font></font><br><img src="https://habrastorage.org/getpro/habr/post_images/cb3/216/34c/cb321634c9309970564a0cbcff3b6500.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So much for pure harmonics. She spawned a bunch of others. The animation shows the DCT coefficients of the sine wave in different phases. </font></font><br><img src="https://habrastorage.org/getpro/habr/post_images/df4/cfc/728/df4cfc7282eb54d9c88dca2b99d37f1a.gif"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If it seemed to you that the columns rotate around an axis, then you did not think. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So now we will expand the function on the sum of the sinusoids not just of different frequencies, but also shifted in some phase. It will be more convenient to consider the phase shift using the example of cosine: </font></font><br><img src="https://habrastorage.org/getpro/habr/post_images/ef6/22a/e8d/ef622ae8d1f651609b180bc4d93c558b.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A simple trigonometric identity gives an important result: the phase shift is replaced by the sum of sine and cosine, taken with the coefficients cos (b) and sin (b). So, you can decompose the functions on the sum of sines and cosines (without any phases). This is a common trigonometric form. However, complex is more often used. To obtain it you need to use the </font></font><a href="http://ru.wikipedia.org/wiki/%25D0%25A4%25D0%25BE%25D1%2580%25D0%25BC%25D1%2583%25D0%25BB%25D0%25B0_%25D0%25AD%25D0%25B9%25D0%25BB%25D0%25B5%25D1%2580%25D0%25B0"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Euler formula.</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Just substitute the derived formulas for sine and cosine, we get: </font></font><br><img src="https://habrastorage.org/getpro/habr/post_images/8f9/220/7d5/8f92207d5956cbe737ee254503163ef1.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Now a small substitution. Upper underscore is the conjugate number. </font></font><br><img src="https://habrastorage.org/getpro/habr/post_images/967/410/937/9674109371b09bf12265b7cd7165917b.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We obtain the final equality: </font></font><br><img src="https://habrastorage.org/getpro/habr/post_images/9e9/5c4/9c7/9e95c49c7562b64f2d21a7cc492bf407.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">c is the complex coefficient, the real part of which is equal to the cosine coefficient, and the imaginary part - to the sine. And the set of points (cos (b), sin (b)) is a circle. In such a record, each harmonic is included in the decomposition with both positive and negative frequencies. Therefore, summation or integration from minus to plus infinity usually occurs in various formulas of Fourier analysis. It is often more convenient to perform calculations in such a complex form.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The transformation decomposes the signal into harmonics with frequencies from one to N oscillations in the signal region. But the sampling rate is N on the signal area. And according to the Kotelnikov theorem (aka the Nyquist ‚Äì Shannon theorem), the sampling frequency should be at least twice the signal frequency. If this is not the case, then the effect of the appearance of a signal with a false frequency is obtained:</font></font><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d88/f86/624/d88f8662403f1e655688197861b92a8a.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dotted lines show an incorrectly restored signal. With this phenomenon you often come across in life. For example, a fun movement of the wheels of a car in the video, or a moire effect. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This leads to the fact that the second half of the N complex amplitudes seem to consist of other frequencies. These false harmonics of the second half are a mirror image of the first and do not carry additional information. Thus, we have N / 2 cosines and N / 2 sines (forming an orthogonal basis).</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Well, there is a basis. Its components are harmonics with an integer number of oscillations on the signal region, and therefore, the extreme values ‚Äã‚Äãof the harmonics are equal. More precisely, they are almost equal, since the latter value is taken not entirely from the edge. Moreover, each harmonic is almost mirror-symmetrical about its center. All these phenomena are especially strong at low frequencies, which are important to us in coding. This is also bad because the block borders will be visible on the compressed image. I illustrate the DFT-basis with N = 8. The first 2 rows are cosine components, the last ones are sinus:</font></font><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/08e/085/6f5/08e0856f53fafab8c2dc1e9e5d26b68c.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pay attention to the appearance of duplicate components with increasing frequency. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">You can think mentally how a signal could be decomposed, the values ‚Äã‚Äãof which gradually decrease from the maximum value at the beginning to the minimum at the end. </font><font style="vertical-align: inherit;">A more or less adequate approximation could only make the harmonics closer to the end, which is not very good for us. </font><font style="vertical-align: inherit;">The figure on the left is the asymmetric signal approximation. </font><font style="vertical-align: inherit;">Right - symmetrical: </font></font><br><img src="https://habrastorage.org/getpro/habr/post_images/357/321/347/35732134721e35149e3956c852325b0b.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">From the first case is extremely bad. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So it can be done like in DCT - to reduce the frequency by 2 or another number of times so that the number of some oscillations is fractional and the boundaries are in different phases? </font><font style="vertical-align: inherit;">Then the components will be non-orthogonal. </font><font style="vertical-align: inherit;">And nothing can be done about it.</font></font><br><br><h5><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> DST </font></font></h5><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What if instead of cosines in DCT use sines? </font><font style="vertical-align: inherit;">We get the Discrete Sine Transform (DST). </font><font style="vertical-align: inherit;">But for our task, they are all uninteresting, since both the whole and the halves of the sinus periods are close to zero at the boundaries. </font><font style="vertical-align: inherit;">That is, we get about the same inappropriate decomposition, as in the DFT.</font></font><br><br><h5><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Returning to DCT </font></font></h5><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> How is he doing at the borders? </font></font> Good.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> There are antiphases and no zeros. </font></font><br><br><h5>  All the rest </h5><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Non-Fourier transform. </font><font style="vertical-align: inherit;">I will not describe. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WHT - the matrix consists only of step components with values ‚Äã‚Äãof -1 and 1. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Haar is partly orthogonal wavelet transform. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">They are inferior to DCT, but easier to compute. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So, the thought came to you to invent your own transformation. </font><font style="vertical-align: inherit;">Remember this:</font></font><br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The basis must be orthogonal. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">With a fixed basis, you cannot outperform KLT in compression quality. </font><font style="vertical-align: inherit;">Meanwhile, on real photos DCT is almost as good.</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> For example, DFT and DST need to remember about the border. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> And remember that DCT has another good advantage - near the boundaries of the components that make up their derivatives are zero, which means that the transition between adjacent blocks will be fairly smooth. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fourier transforms have fast algorithms with complexity O (N * logN), as opposed to head-on: O (N </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ).</font></font></li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It won't be easy, right? </font><font style="vertical-align: inherit;">However, for some types of images you can choose a better basis than the DCT. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This section turned out to be similar to a discrete cosine transform advertisement. </font><font style="vertical-align: inherit;">But it is really cool!</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 2D transformations </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Now we will try to conduct such an experiment. </font><font style="vertical-align: inherit;">Take, for example, a piece of the image.</font></font><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/806/ada/ba0/806adaba036c775aae0ca116056a5411.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> His 3D graph: </font></font><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0b2/ecc/703/0b2ecc703c4b253486b22cfc233601d3.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Go through DCT (N = 32) for each line: </font></font><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cca/3a4/4b7/cca3a44b7ccd49f6ca7f398a0d2fa365.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Now I want you to run your eyes over each column of the obtained coefficients, i.e. from top to bottom. </font><font style="vertical-align: inherit;">Recall that our goal is to leave as few values ‚Äã‚Äãas possible, removing the minor ones. </font><font style="vertical-align: inherit;">Surely you guessed that the values ‚Äã‚Äãof each column of the obtained coefficients can be decomposed in the same way as the values ‚Äã‚Äãof the original image. </font><font style="vertical-align: inherit;">No one restricts us in choosing an orthogonal transformation matrix, but we will do it again using DCT (N = 8):</font></font><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2ec/adc/912/2ecadc912dbffcfe65242301a1d0b804.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The coefficient (0,0) turned out to be too large, so on the graph it is reduced by 4 times. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So what happened? </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The upper left corner is the most significant expansion coefficients of the most significant coefficients. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The lower left corner is the most insignificant coefficients of the expansion of the most significant coefficients. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The upper right corner is the most significant expansion coefficients of the most insignificant coefficients. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The lower right corner is the most insignificant expansion coefficients of the most insignificant coefficients. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It is clear that the significance of the coefficients decreases if you move diagonally from the upper left to the lower right. And which is more important: (0, 7) or (7, 0)? What do they even mean? </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">First line by line: A </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = (EX </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> )</font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = XE </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (transposed, since the formula A = EX columns) then the columns: A = EA </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = EXE </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . If we carefully calculate, we get the formula: </font></font><br><img src="https://habrastorage.org/getpro/habr/post_images/82f/007/f3a/82f007f3a15aa473026c1f37815fbd3d.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Thus, if the vector is decomposed into sinusoids, then the matrix on functions of the form cos (ax) * cos (by). Each block of 8x8 in JPEG is represented as a sum of 64 functions of the form: </font></font><br><img src="https://habrastorage.org/getpro/habr/post_images/76d/369/abf/76d369abf2819b0e1dd7036001cc5d74.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In Wikipedia and other sources, such functions are presented in a more convenient form:</font></font><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/529/558/060/529558060e5742d7551065e729f3a706.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Therefore, the coefficients (0, 7) or (7, 0) are equally useful. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">However, in fact, this is the usual one-dimensional decomposition on 64 64-dimensional basis. </font><font style="vertical-align: inherit;">All of the above applies not only to DCT, but also to any orthogonal decomposition. </font><font style="vertical-align: inherit;">Acting by analogy, in the general case we obtain an N-dimensional orthogonal transform. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And here is a 2-dimensional transformation of a raccoon (DCT 256x256). </font><font style="vertical-align: inherit;">Again with zeroed values. </font><font style="vertical-align: inherit;">Numbers - the number of non-zero coefficients of all (the most significant values ‚Äã‚Äãwere left, which are in a triangular area in the upper left corner).</font></font><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4fd/646/008/4fd6460082b20e71ab19bc41b460b16c.gif"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Remember that the coefficient (0, 0) is called DC, the other 63 is AC. </font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Select block size </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A comrade </font></font><a href="http://math.stackexchange.com/questions/32491/why-is-8x8-matrix-chosen-for-discrete-cosine-transform"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">asks</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : why in JPEG is the 8x8 split used. </font><font style="vertical-align: inherit;">From the spiteful answer:</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It has been the case for the DCT. </font><font style="vertical-align: inherit;">If you take 64x64 blocks, you will have to</font></font></blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">They say that DCT works well only on periodic functions, and if you take a large size, you will most likely get a giant leap at the block boundaries and you will need a lot of high-frequency components to cover it. </font><font style="vertical-align: inherit;">This is not true! </font><font style="vertical-align: inherit;">Such an explanation is very similar to the DFT, but not to the DCT, as it perfectly covers such jumps by the very first components. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">On the same page is the answer from the MPEG FAQ, with the main arguments against the big blocks:</font></font><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Little profit when split into large blocks. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Increase computational complexity. </font></font></li><li>         ,    . </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I suggest to investigate it yourself. Let's start with the </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">first</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><img src="https://habrastorage.org/getpro/habr/post_images/1b8/3c3/0a9/1b83c30a99b939b45da4fa3370950ed4.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The horizontal axis is the fraction of the first non-zero coefficients. Vertical - the standard deviation of pixels from the original. The maximum possible deviation is taken per unit. Of course, one picture is clearly not enough for the verdict. Besides, I'm not acting quite correctly, just zeroing. In real JPEG, depending on the quantization matrix, only small values ‚Äã‚Äãof high-frequency components are zeroed. Therefore, the following experiments and conclusions are intended to superficially identify principles and patterns. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">You can compare the division into different blocks with the left 25 percent of the coefficients (from left to right, then from right to left):</font></font><br><img src="https://habrastorage.org/getpro/habr/post_images/5ff/094/624/5ff0946247113315aa2da487c5fe0c9d.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Large blocks are not shown, as they are visually almost indistinguishable from 32x32. Now let's look at the absolute difference with the original image (amplified 2 times, otherwise nothing really can be seen): </font></font><br><img src="https://habrastorage.org/getpro/habr/post_images/89d/f79/fb6/89df79fb6777f31433567b1823fcc00d.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">8x8 gives a better result than 4x4. A further increase in size does not give a well-marked advantage. Although I would seriously think about 16x16, instead of 8x8: an increase of 33% in complexity (of the complexity in the next paragraph) gives a small but still visible improvement with the same number of coefficients. However, the choice of 8x8 looks quite reasonable and, perhaps, is the golden mean. JPEG was published in 1991. I think that such compression was very difficult for processors of that time. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Second</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">argument. </font><font style="vertical-align: inherit;">It must be remembered that increasing the block size will require more calculations. </font><font style="vertical-align: inherit;">Let's evaluate how much. </font><font style="vertical-align: inherit;">The complexity of the transformation in the forehead, as we already know how to: O (N </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), since each coefficient consists of N terms. </font><font style="vertical-align: inherit;">But in practice, an efficient fast Fourier transform algorithm (FFT, Fast Fourier Transform, FFT) is used. </font><font style="vertical-align: inherit;">His description is beyond the scope of the article. </font><font style="vertical-align: inherit;">Its complexity: O (N * logN). </font><font style="vertical-align: inherit;">For two-dimensional decomposition, you need to use it twice N times. </font><font style="vertical-align: inherit;">Thus, the complexity of the 2D DCT - O (N </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> logN). </font><font style="vertical-align: inherit;">Now let's compare the complexity of computing an image with one block and several small ones:</font></font><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In one block (kN) x (kN): O ((kN) </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> log (kN)) = O (k </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> N </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> log (kN))</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k * k in N * N: O blocks (k </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> N </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> logN)</font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This means that, for example, computing when split into 64x64 is two times more complicated than it is into 8x8. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The third</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> argument. </font><font style="vertical-align: inherit;">If we have a sharp border of flowers on the image, then this will affect the whole block. </font><font style="vertical-align: inherit;">Perhaps this block will be better enough small, because in many neighboring blocks, such a border is likely to be gone. </font><font style="vertical-align: inherit;">However, far from the boundaries, the attenuation occurs fairly quickly. </font><font style="vertical-align: inherit;">In addition, the border itself will look better. </font><font style="vertical-align: inherit;">Let's check with an example with a large number of contrasting transitions, again, with only a quarter of the coefficients: </font></font><br><img src="https://habrastorage.org/getpro/habr/post_images/be9/126/ab9/be9126ab9bc86e7043ff6dfcc4839313.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Although the distortions of the 16x16 blocks extend further than that of 8x8, but the inscription is smoother. </font><font style="vertical-align: inherit;">Therefore, I was convinced only the first two arguments. </font><font style="vertical-align: inherit;">But something more like 16x16 divisions.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Quantization </font></font></h4><br>  At this stage, we have a bunch of 8x8 matrices with cosine transform coefficients.  It's time to get rid of unimportant coefficients.  There is a more elegant solution than simply resetting the last coefficients, as we did above.  We are not satisfied with this method, since non-zero values ‚Äã‚Äãare stored with excessive accuracy, and among those who are not lucky, they could be quite important.  Output - you need to use a quantization matrix.  Losses occur at this stage.  Each Fourier coefficient is divided by the corresponding number in the quantization matrix.  Consider an example.  Take the first block from our raccoon and produce quantization.  The JPEG specification provides the standard matrix: <br><img src="https://habrastorage.org/getpro/habr/post_images/a0c/65b/36f/a0c65b36f1da3265a2e6016e811678e3.png"><br>  The standard matrix corresponds to 50% quality in FastStone and IrfanView.  Such a table was chosen in terms of the balance of quality and degree of compression.  I think that the value for the DC coefficient is greater than the neighboring ones due to the fact that DCT is not normalized and the first value is obtained more than it should.  High-frequency coefficients are rougher because of their lesser importance.  I think that now such matrices are rarely used, since the deterioration is well marked.  Nobody forbids to use your table (with values ‚Äã‚Äãfrom 1 to 255) <br>  When decoding, the inverse process occurs - the quantized coefficients are multiplied by the term of time by the values ‚Äã‚Äãof the quantization matrix.  But since we are rounding the values, we will not be able to accurately restore the original Fourier coefficients.  The greater the quantization number, the greater the error.  Thus, the restored coefficient is only the nearest multiple. <br>  Another example: <br><img src="https://habrastorage.org/getpro/habr/post_images/892/118/f38/892118f385c4ea8690aa948771c957da.png"><br><br>  And for dessert, consider the quality of 5% (when coding in Fast Stone). <br><img src="https://habrastorage.org/getpro/habr/post_images/d82/62c/6af/d8262c6af8ebc6cf48f50cc71d8aa5d3.png"><br>  When restoring this block, we get only the averaged value plus the vertical gradient (due to the preserved value of -1).  But for him only two values ‚Äã‚Äãare stored: 7 and -1.  With other blocks, the situation is no better, here is the restored picture: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/50d/d3a/af5/50dd3aaf512825b72449a217689fd155.png"></div><br><br>  By the way, about 100% quality.  As you might guess, in this case the quantization matrix consists entirely of units, that is, no quantization occurs.  However, due to the rounding of the coefficients to the whole, we cannot exactly restore the original image.  For example, a raccoon kept 96% of pixels exactly, and 4% differed by 1/256.  Of course, such ‚Äúdistortions‚Äù cannot be seen visually. <br>  And <a href="http://www.impulseadventure.com/photo/jpeg-quantization.html">here</a> you can see the quantization matrix of various cameras. <br><br><h4>  Coding </h4><br>  Before proceeding further, we need to understand with simpler examples how to compress the values ‚Äã‚Äãobtained. <br><br>  <b>Example 0</b> (for warm-up) <br>  Imagine such a situation that your friend has forgotten a list with a list in your house and now asks to dictate it by telephone (there are no other means of communication). <br>  List: <br><ul><li>  d9rg3 </li><li>  wfr43gt </li><li>  wfr43gt </li><li>  d9rg3 </li><li>  d9rg3 </li><li>  d9rg3 </li><li>  wfr43gt </li><li>  d9rg3 </li></ul><br>  How would you make your task easier?  You have no particular desire to dictate all these words.  But there are only two of them and they are repeated.  Therefore, you just somehow dictate the first two words and agree that further ‚Äúd9rg3‚Äù will be called the first word, and ‚Äúwfr43gt‚Äù - the second.  Then it will be enough to dictate: 1, 2, 2, 1, 1, 1, 2, 1. <br><br>  We will denote such words as A, B, C ..., and call them symbols.  And under the symbol can hide anything: a letter of the alphabet, a word or a hippopotamus in a zoo.  The main thing is that the same symbols correspond to the same concepts, but different to different.  Since our task is efficient coding (compression), we will work with bits, since these are the smallest units of information representation.  Therefore, we write the list as ABBAAABA.  Instead of ‚Äúfirst word‚Äù and ‚Äúsecond word‚Äù, you can use bits 0 and 1. Then ABBAAABA is encoded as 01100010 (8 bits = 1 byte). <br><img src="https://habrastorage.org/getpro/habr/post_images/bb2/a30/fa5/bb2a30fa5f2e8960d7dbc70ffec3b914.png"><br><br>  <b>Example 1</b> <br>  Encode ABC. <br>  3 different symbols (A, B, C) can not be compared with 2 possible values ‚Äã‚Äãof bits (0 and 1).  And if so, then you can use 2 bits per symbol.  For example: <br><ul><li>  A: 00 </li><li>  B: 01 </li><li>  C: 10 </li></ul><br>  The sequence of bits associated with a symbol will be called a code.  ABC will be encoded as: 000110. <br><img src="https://habrastorage.org/getpro/habr/post_images/e61/818/f90/e61818f90332c02edcebe2ba8fd6bbdf.png"><br><br>  <b>Example 2</b> <br>  Encode AAAAAABC. <br>  Using 2 bits per symbol A seems a bit wasteful.  What if you try this: <br><ul><li>  A: 0 </li><li>  B: 1 </li><li>  C: 00 </li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/661/7de/86f/6617de86f2e044ead814d5773b960bca.png"><br>  Encoded sequence: 000000100. <br>  Obviously, this option is not suitable, since it is not clear how to decode the first two bits of this sequence: as AA or as C?  It is very wasteful to use any separator between the codes, we will think how to get around this obstacle in a different way.  So, the failure occurred because the C code starts with the A code. But we are determined to encode A in one bit, even if B and C are two each.  Proceeding from such a wish, A will give code 0. Then codes B and C cannot begin at 0. But they can by 1: <br><ul><li>  A: 0 </li><li>  B: 10 </li><li>  C: 11 </li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/dfc/1ee/fba/dfc1eefbae8e0ebf6ca09610339a9cdf.png"><br>  The sequence is encoded as: 0000001011. Try to mentally decode it.  You can only do this in one way. <br>  We have developed two coding requirements: <br><ol><li>  The greater the weight of the character, the shorter its code should be.  And vice versa. </li><li>  For unambiguous decoding, the character code cannot begin with the code of any other character. </li></ol><br>  Obviously, the order of characters is not important, we are only interested in the frequency of their occurrence.  Therefore, with each symbol is assigned a number, called weight.  The weight of a symbol can be both a relative value, reflecting the proportion of its occurrence, and absolute, equal to the number of symbols.  The main thing is that the weights should be proportional to the occurrence of characters. <br><br>  <b>Example 3</b> <br>  Consider the general case for 4 characters with any weights. <br><ul><li>  A: pa </li><li>  B: pb </li><li>  C: pc </li><li>  D: pd </li></ul><br>  Without loss of generality, we put pa ‚â• pb ‚â• pc ‚â• pd.  There are only two variant codes that are fundamentally different in length: <br><img src="https://habrastorage.org/getpro/habr/post_images/807/66f/863/80766f8638e9f18a401bd6107eec20c0.png"><br>  Which one is preferable?  To do this, calculate the lengths of the encoded messages: <br>  W1 = 2 * pa + 2 * pb + 2 * pc + 2 * pd <br>  W2 = pa + 2 * pb + 3 * pc + 3 * pd <br>  If W1 is less than W2 (W1-W2 &lt;0), then it is better to use the first option: <br>  W1-W2 = pa - (pc + pd) &lt;0 =&gt; pa &lt;pc + pd. <br>  If C and D together occur more often than others, then their common vertex gets the shortest code of one bit.  Otherwise, one bit goes to symbol A. Hence, the combination of characters behaves as an independent character and has a weight equal to the sum of the incoming characters. <br>  In general, if p is the weight of a character represented by a fraction of its occurrence (from 0 to 1), then the best code length is s = -log <sub>2</sub> p. <br>  Consider this in a simple case (it is easy to imagine it as a tree).  So, you need to encode 2 <sup>s</sup> characters with equal weights (1/2 <sup>s</sup> ).  Due to the equality of the weights, the length of the codes will be the same.  Each character will require s bits.  So, if the weight of the symbol is 1/2 <sup>s</sup> , then its length is s.  If we replace the weight with p, then we get the length of the code <nobr>s = -log <sub>2</sub> p</nobr> .  So, if one character is twice as rare as the other, then the length of its code will be a bit longer.  However, this conclusion is easy to make, if we recall that adding one bit allows you to double the number of possible options. <br>  And one more observation - the two characters with the smallest weights always have the greatest, but equal lengths of codes.  Moreover, their bits, except the last one, coincide.  If this were not true, then at least one code could be shortened by 1 bit without violating the prefix.  This means that the two characters with the smallest weights in the code tree have a common parent a level higher.  You can see this in the example of C and D above. <br><br>  <b>Example 4</b> <br>  Let's try to solve the following example, according to the conclusions obtained in the previous example. <br><ol><li>  All characters are sorted in descending order of weights. </li><li>  The last two characters are grouped together.  This group is assigned a weight equal to the sum of the weights of these elements.  This group participates in the algorithm along with symbols and other groups. </li></ol><br>  The steps are repeated until there is only one group left.  In each group, one character (or subgroup) is assigned bit 0, and the other bit 1. <br>  This algorithm is called Huffman coding. <br>  The illustration shows an example with 5 characters (A: 8, B: 6, C: 5, D: 4, E: 3).  On the right is the weight of the symbol (or group). <br><img src="https://habrastorage.org/getpro/habr/post_images/091/6f8/28f/0916f828f4912831fb27895c02baa15e.png"><br><br><h4>  We code coefficients </h4><br>  Come back.  Now we have a lot of blocks with 64th coefficients in each, which need to somehow be saved.  The simplest solution is to use a fixed number of bits per coefficient ‚Äî obviously, unfortunate.  Construct a histogram of all obtained values ‚Äã‚Äã(i.e., the dependence of the number of coefficients on their value): <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d5c/4fb/951/d5c4fb951ef335a8057635270cf30ee7.png"></div><br>  Pay attention - the logarithmic scale!  Can you explain the reason for the accumulation of values ‚Äã‚Äãgreater than 200?  These are DC coefficients.  Since they are very different from the rest, it is not surprising that they are encoded separately.  Here are just DC: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/466/eec/684/466eec68437f7089bab936cb84359263.png"></div><br>  <i>Notice that the shape of the graph resembles the shape of the graphs from the very early experiments of dividing into pairs and triples of pixels.</i> <br>  In general, the values ‚Äã‚Äãof DC coefficients can vary from 0 to 2047 (more precisely, from -1024 to 1023, since 128 is subtracted from JPEG from all the original values, which corresponds to 1024 subtraction from DC) and distributed fairly evenly with small peaks.  Therefore, Huffman coding is not very helpful.  Imagine how big the coding tree will be!  And during decoding you will have to look for values ‚Äã‚Äãin it.  It is very expensive.  We think further. <br>  DC coefficient - averaged value of 8x8 block.  Imagine a gradient transition (albeit not perfect), which is often found in photographs.  The DC values ‚Äã‚Äãthemselves will be different, but they will represent an arithmetic progression.  Hence, their difference will be more or less constant.  Construct a histogram of differences: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6df/98a/afa/6df98aafaedbc31cb6f5d9cf60dbc0c0.png"></div><br>  Now this is better, because the values ‚Äã‚Äãare generally concentrated around zero (but the Huffman algorithm will again give too large a tree).  Small values ‚Äã‚Äã(in absolute value) are common, large rarely.  And since small values ‚Äã‚Äãtake up little bits (if you remove leading zeros), one of the compression rules is well executed: assign short codes to symbols with large weights (and vice versa).  We are still limited by the failure to comply with another rule: the impossibility of unambiguous decoding.  In general, such a problem is solved in the following ways: confuse with the code separator, specify the length of the code, use prefix codes (you already know them - this is the case when no code starts with another).  Let's go with the simple second variant, i.e. each coefficient (more precisely, the difference of the neighboring ones) will be written like this: (length) (value), with the following label: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a91/070/9c7/a910709c737488ffce32d2b53f34e71e.png"></div><br>  That is, positive values ‚Äã‚Äãare directly encoded by their binary representation, and negative values ‚Äã‚Äãare the same, but with replacement of leading 1 by 0. It remains to decide how to encode lengths.  Since there are 12 possible values, you can use 4 bits to store the length.  But here it is better to use Huffman coding. <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/10c/5f4/320/10c5f4320d2b754e09d73c8255a87f59.png"></div><br>  Values ‚Äã‚Äãwith lengths of 4 and 6 are the most, so they got the shortest codes (00 and 01). <br><br>  <i>JPEG implementation features:</i> <br>  The question may arise: why on the example of the value 9 the code is 1111110, and not 1111111?  After all, you can safely raise "9" to a higher level, next to "0"?  The fact is that in JPEG it is impossible to use a code consisting only of units - such code is reserved. <br>  There is another feature.  Codes obtained by the described Huffman algorithm may not match bits with codes in JPEG, although their lengths will be the same.  Using the Huffman algorithm, get the length of the codes, and the codes themselves are generated (the algorithm is simple - start with short codes and add them in turn to the tree as far as possible to the left, preserving the prefix property).  For example, for the tree above, the list is kept: 0,2,3,1,1,1,1,1.  And, of course, a list of values ‚Äã‚Äãis stored: 4,6,3,5,7,2,8,1,0,9.  When decoding, codes are generated in the same way. <br><br>  Now order.  We figured out how DC is stored: <br>  <b>[Huffman code for DC <sub>diff</sub> length (in bits)] [DC <sub>diff</sub> ]</b> <br>  where DC <sub>diff</sub> = DC <sub>current</sub> - DC <sub>previous</sub> <br><br>  We look AC: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/945/597/57a/94559757a8806f4ffeb7cdc8717fafb7.png"></div><br>  Since the graph is very similar to the graph for DC differences, the principle is the same: [Huffman code for AC length (in bits)] [AC].  But not really!  Since the scale is logarithmic, it is not immediately noticeable that the zero values ‚Äã‚Äãare about 10 times greater than the values ‚Äã‚Äãof 2 - the next in frequency.  This is understandable - not everyone experienced quantization.  Let us return to the matrix of values ‚Äã‚Äãobtained at the quantization stage (using the FastStone quantization matrix, 90%). <br><img src="https://habrastorage.org/getpro/habr/post_images/98b/7ab/de4/98b7abde4a85a0e61255a0c193e259fb.png"><br>  Since there are many groups of consecutive zeros, the idea is to record only the number of zeros in the group.  This compression algorithm is called RLE (Run-length encoding, retransmission coding).  It remains to find out the direction of bypassing the "consecutive march" - who is behind whom?  Writing from left to right and from top to bottom is not very effective, since non-zero coefficients are concentrated near the upper left corner, and the closer to the lower right angle, the more zeros. <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1d4/2b1/9e2/1d42b19e2ddcd5e565dec8d56fa1c8ef.png"></div><br>  Therefore, in JPEG, an order called ‚ÄúZig-zag‚Äù is used; it is shown in the left figure.  This method well identifies groups of zeros.  In the right figure - an alternative way to work around, not related to JPEG, but with a curious name ( <a href="https://www.google.ru/search%3Fq%3D%2522yeltsin%2Bwalk%2522%26tbm%3Dbks">proof</a> ).  It can be used in MPEG when compressing interlaced video.  The choice of the traversal algorithm does not affect the image quality, but may increase the number of coded groups of zeros, which may ultimately affect the file size. <br>  Modifying our record.  For each non-zero AC - coefficient: <br>  <b>[Number of zeros before AC] [Huffman code for AC length (in bits)] [AC]</b> <br>  I think that you immediately say - the number of zeros is also perfectly encoded by Huffman!  This is a very close and good answer.  But you can optimize a little.  Imagine that we have a certain coefficient AC, before which there were 7 zeros (of course, if written out in a zigzag order).  These zeros are a spirit of values ‚Äã‚Äãthat did not withstand quantization.  Most likely, our coefficient was also badly battered and it became small, and, therefore, its length was short.  This means that the number of zeros before AC and the length of AC are dependent values.  Therefore, write this: <br>  <b>[Huffman code for (Number of zeros before AC, length of AC (in bits)] [AC]</b> <br>  The coding algorithm remains the same: those pairs (the number of zeros before AC, the length of AC), which are often found, will receive short codes and vice versa. <br><br>  We build a histogram of the dependence of the number of these pairs and the Huffman tree. <br><img src="https://habrastorage.org/getpro/habr/post_images/e1b/0b3/9aa/e1b0b39aad50fdff60ce6ae38bf6ca4e.png"><br>  The long "mountain range" confirms our assumption. <br><br>  <i>JPEG implementation features:</i> <br>  This pair is 1 byte: 4 bits for the number of zeros and 4 bits for the length of AC.  4 bits are values ‚Äã‚Äãfrom 0 to 15. For AC, there is enough excess, but can there be more than 15 zeros?  Then more pairs are used.  For example, for 20 zeros: (15, 0) (5, AC).  That is, the 16th zero is encoded as a non-zero coefficient.  Since near the end of the block is always full of zeros, then after the last non-zero coefficient a pair (0,0) is used.  If it occurs during decoding, then the remaining values ‚Äã‚Äãare 0. <br><br>  Found out that each block is encoded stored in a file like this: <br>  [Huffman code for DC <sub>diff</sub> ] <br>  [DC <sub>diff</sub> ] <br>  [Huffman code for (number of zeros before AC <sub>1</sub> , length AC <sub>1</sub> ] <br>  [AC <sub>1</sub> ] <br>  ... <br>  [Huffman code for (number of zeros before AC <sub>n</sub> , length AC <sub>n</sub> ] <br>  [AC <sub>n</sub> ] <br>  Where AC <sub>i</sub> - non-zero AC coefficients. <br><br><h4>  Color image </h4><br>  The way a color image is displayed depends on the selected color model.  A simple solution is to use RGB and encode each color channel of the image separately.  Then the coding will not differ from the coding of the gray image, only the work is 3 times more.  But image compression can be increased if we recall that the eye is more sensitive to changes in brightness than colors.  This means that the color can be stored with more losses than the brightness.  RGB has no separate brightness channel.  It depends on the sum of the values ‚Äã‚Äãof each channel.  Therefore, the RGB-cube (this is a representation of all possible values) is simply ‚Äúput‚Äù on the diagonal - the higher, the brighter.  But this is not limited - the cube is slightly pressed laterally, and it turns out rather a parallelepiped, but this is only to take into account the features of the eye.  For example, it is more susceptible to green than blue.  So there was a model YCbCr. <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/14f/be2/f7d/14fbe2f7ddb562eee6233a0a95a7e803.png"></div><br>  <i>(Image from Intel.com)</i> <br>  Y is the luminance component, Cb and Cr are the blue and red color difference components.  Therefore, if they want to compress the image more strongly, then RGB is transferred to YCbCr, and the Cb and Cr channels are punctured.  That is, they break into small blocks, for example 2x2, 4x2, 1x2, and average all values ‚Äã‚Äãof one block.  Or, in other words, reduce the image size for this channel by 2 or 4 times vertically and / or horizontally. <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cd0/5a4/e90/cd05a4e90c4045e87b9a305f11061144.png"></div><br>  Each 8x8 block is encoded (DCT + Huffman), and the encoded sequences are written in this order: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/389/dd0/1c1/389dd01c1fab498c399d8f77979c959b.png"></div><br>  It is curious that the JPEG specification does not limit the choice of model, that is, the implementation of the encoder can arbitrarily divide the image by color components (channels) and each will be saved separately.  I am aware of the use of Grayscale (1 channel), YCbCr (3), RGB (3), YCbCrK (4), CMYK (4).  The first three are supported by almost everyone, but with the latest 4-channel there are problems.  FastStone, GIMP support them correctly, and the regular Windows programs, paint.net, <s>correctly extract all the information, but then emit 4 black channels, therefore</s> ( <a href="https://habrahabr.ru/users/antelle/" class="user_link">Antelle</a> said they don‚Äôt emit, read his comments) show a lighter image.  On the left - classic YCbCr JPEG, on the right CMYK JPEG: <br><img src="https://habrastorage.org/getpro/habr/post_images/1ff/b66/b93/1ffb66b93725bfe036714e2613ac9333.jpg"><img src="https://habrastorage.org/getpro/habr/post_images/483/436/ba3/483436ba384cba412b9b447d0129f7ef.jpg"><br>  If they differ in color, or only one picture is visible, then most likely you have IE (any version) (UPD. In the comments say ‚Äúor Safari‚Äù).  You can try to open the article in different browsers. <br><br><h4>  And something else </h4><br>  In a nutshell about additional features. <br><br><h5>  Progressive mode </h5><br>  We decompose the obtained tables of DCT coefficients into the sum of the tables (approximately like this (DC, -19, -22, 2, 1) = (DC, 0, 0, 0, 0) + (0, -20, -20, 0, 0) + (0, 1, -2, 2, 1)).  First, we encode all the first addends (as we have learned: Huffman and zigzag bypass), then the second ones, etc. This trick is useful with a slow Internet, because at first only the DC coefficients are loaded, which are used to construct a rough picture with 8x8 pixels.  Then rounded AC coefficients to clarify the figure.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Then rough amendments to them, then more exact. </font></font> Well, and so on.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The coefficients are rounded, since in the early stages of loading accuracy is not so important, but rounding has a positive effect on the length of the codes, since each stage uses its own Huffman table. </font></font><br><br><h5><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Lossless mode </font></font></h5><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lossless compression. </font><font style="vertical-align: inherit;">DCT no. </font><font style="vertical-align: inherit;">The 4th point prediction using three adjacent points is used. </font><font style="vertical-align: inherit;">Prediction errors are encoded by Huffman. </font><font style="vertical-align: inherit;">In my opinion, used a little more than ever.</font></font><br><br><h5><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Hierarchical mode </font></font></h5><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The image creates several layers with different resolutions. </font><font style="vertical-align: inherit;">The first coarse layer is encoded as usual, and then only the difference (image refinement) between the layers (pretends to be a Haar wavelet). </font><font style="vertical-align: inherit;">DCT or Lossless is used for encoding. </font><font style="vertical-align: inherit;">In my opinion, it is used a little less than never.</font></font><br><br><h5><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Arithmetic coding </font></font></h5><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The Huffman algorithm creates optimal codes for the weight of characters, but this is true only for a fixed correspondence of characters with codes. </font><font style="vertical-align: inherit;">Arithmetic does not have such a tight binding, which allows the use of codes as if with a fractional number of bits. </font><font style="vertical-align: inherit;">It is alleged that it reduces the file size by an average of 10% compared with Huffman. </font><font style="vertical-align: inherit;">Not common due to patent issues, not supported by all. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I hope that now you understand the JPEG algorithm intuitively.</font></font> Thanks for reading! <br><br>  <b>UPD</b> <br> <a href="https://habrahabr.ru/users/vanwin/" class="user_link"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">vanwin</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> suggested specifying the software used. </font><font style="vertical-align: inherit;">I am pleased to announce that everything is available and free:</font></font><br><ul><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Python + NumPy + Matplotlib + PIL (Pillow)</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">The main tool. </font><font style="vertical-align: inherit;">Found on the issuance of "Matlab free alternative".</font></font> Recommend!<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Even if you are not familiar with Python, then after a couple of hours, learn how to make calculations and build beautiful graphics. </font></font></li><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">JpegSnoop</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Shows detailed information about the jpeg file.</font></font></li><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">yEd</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Graph editor.</font></font></li><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Inkscape</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">He did illustrations in it, such as an example of a Huffman algorithm. </font><font style="vertical-align: inherit;">I read a few lessons, it turned out very cool.</font></font></li><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Daum Equation Editor</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">I was looking for a visual formula editor, as I‚Äôm not very friendly with Latex. </font><font style="vertical-align: inherit;">Daum Equation - plugin for Chrome, seemed to me very convenient. </font><font style="vertical-align: inherit;">In addition to the mouse, you can edit Latex.</font></font></li><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">FastStone</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">I think it is not necessary to represent him.</font></font></li><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Picpick</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Free alternative to SnagIt. </font><font style="vertical-align: inherit;">He sits in the tray, a screenshot that will say where they say. </font><font style="vertical-align: inherit;">Plus all sorts of buns, such as ruler, pipette, protractor, etc.</font></font></li></ul></div><p>Source: <a href="https://habr.com/ru/post/206264/">https://habr.com/ru/post/206264/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../206250/index.html">3D printer Thinker Thing will print your thoughts</a></li>
<li><a href="../206252/index.html">Roofs, water and iron pipes - Nokia Lumia 1020 in the hands of photoextreme</a></li>
<li><a href="../206256/index.html">Python Meetup: November Meeting</a></li>
<li><a href="../206258/index.html">How and why I decided to start my own business</a></li>
<li><a href="../206262/index.html">.vimrc for frontendder</a></li>
<li><a href="../206276/index.html">Creating your own applications for installation through the standard Parallels Cloud Server tools</a></li>
<li><a href="../206278/index.html">New version of HP Vertica: Crane number 7</a></li>
<li><a href="../206280/index.html">For pioneers. On the waves with Jolla (UPD)</a></li>
<li><a href="../206282/index.html">Ubiquiti NanoBridge M5: the first installation experience or wifi bridge out of the box</a></li>
<li><a href="../206288/index.html">Chelyabinsk mathematician published an attempt to prove P = NP</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>