<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Performance comparison of GPU calculations in Python and C</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Python has a number of attractive advantages, which include ease of implementation of software solutions, clarity and conciseness of the code, the pre...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Performance comparison of GPU calculations in Python and C</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/files/708/b36/2bd/708b362bd5a14b8b8f4d5c3ca04561f1.png"></div><br>  Python has a number of attractive advantages, which include ease of implementation of software solutions, clarity and conciseness of the code, the presence of a large number of libraries and numerous active community.  At the same time, the well-known slowness of the python often limits its applicability to "heavy" calculations.  For a number of tasks, it is possible to achieve a significant acceleration of calculations by using the CUDA technology for parallel computing on the GPU.  The purpose of this small study is to analyze the possibilities of effectively using python for calculations on GPU and comparing the performance of various python solutions with implementation on C. <br><a name="habracut"></a><br><h2>  Introduction </h2><br>  By the nature of my work, I often do numerical modeling tasks.  In many cases, we use CUDA technology to speed up calculations by using the parallel computing capabilities of the GPU, while programs are written in C. At the same time, in some cases, we would like to be able to implement calculations on python, because it is convenient, fast , flexible, concise, <s>youth</s> .  In this case, however, it is very important not to lose the speed of program execution, since some calculations can take from several hours to a day.  The article demonstrated the use of the python-libraries Numba and PyCUDA for the implementation of parallel calculations on the GPU and the results of the comparison of their performance on the test task. <br><br><h2>  Test task </h2><br>  The choice of the test problem and the conditions of testing was determined by the nature of the real problems, for which further use of python is planned.  In this case, the simplest problem was chosen, namely the problem of <a href="https://ru.wikipedia.org/wiki/%25D0%25A3%25D1%2580%25D0%25B0%25D0%25B2%25D0%25BD%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D1%2582%25D0%25B5%25D0%25BF%25D0%25BB%25D0%25BE%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B2%25D0%25BE%25D0%25B4%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D0%25B8">solving the two-dimensional heat conduction equation</a> using <a href="http://agora.guru.ru/abrau2010/pdf/346.pdf">an explicit finite-difference scheme</a> .  The task was considered on a square domain with given temperature values ‚Äã‚Äãat the boundaries.  The number of nodes of the computational grid in x and y is the same and is equal to n.  The figure at the beginning of the article shows the steady-state solution to the test problem. <br><br><h2>  Algorithm and testing conditions </h2><br>  The algorithm for solving the problem can be represented by the following pseudocode: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <pre><code class="python hljs">  u0  u,                   u0  u     <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; nstp/<span class="hljs-number"><span class="hljs-number">2</span></span>; i++)   u     ,   u0       u0     ,   u       (u0)         </code> </pre> <br>  In all tests presented below, the number of square grid nodes in each direction (n) ranged from 512 to 4096, and nstp = 5000. <br><br><h2>  Software and hardware </h2><br>  Testing was conducted on a personal computer: <br><br><blockquote>  Intel¬Æ Core (TM) 2 Quad CPU Q9650 @ 3.00GHz, 8 Gb RAM <br>  GPU: Nvidia GTX 580 <br>  Operating system: Ubuntu 16.04 LTS with CUDA 7.5 installed </blockquote><br><h2>  C implementation </h2><br>  All further python implementations were compared with the results obtained using the C program described in this section. <br><br><div class="spoiler">  <b class="spoiler_title">C code</b> <div class="spoiler_text"><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;stdio.h&gt; #include &lt;time.h&gt; #include &lt;cuda_runtime.h&gt; #define BLOCK_SIZE 16 int N = 512; //1024; //2048; double L = 1.0; double h = L/N; double h2 = h*h; double tau = 0.1*h2; double th2 = tau/h2; int nstp = 5000; __constant__ int dN; __constant__ double dth2; __global__ void NextStpGPU(double *u0, double *u) { int i = blockDim.x * blockIdx.x + threadIdx.x; int j = blockDim.y * blockIdx.y + threadIdx.y; double uim1, uip1, ujm1, ujp1, u00, d2x, d2y; if (i &gt; 0) uim1 = u0[(i - 1) + dN*j]; else uim1 = 0.0; if (i &lt; dN - 1) uip1 = u0[(i + 1) + dN*j]; else uip1 = 0.0; if (j &gt; 0) ujm1 = u0[i + dN*(j - 1)]; else ujm1 = 0.0; if (j &lt; dN - 1) ujp1 = u0[i + dN*(j + 1)]; else ujp1 = 1.0; u00 = u0[i + dN*j]; d2x = (uim1 - 2.0*u00 + uip1); d2y = (ujm1 - 2.0*u00 + ujp1); u[i + dN*j] = u00 + dth2*(d2x + d2y); } int main(void) { size_t size = N * N * sizeof(double); double pStart, pEnd, pD; int i; double *h_u0 = (double *)malloc(size); double *h_u1 = (double *)malloc(size); for (i = 0; i &lt; N*N; ++i) { h_u0[i] = 0.0; h_u1[i] = 0.0; } pStart = (double) clock(); double *d_u0 = NULL; cudaMalloc((void **)&amp;d_u0, size); double *d_u1 = NULL; cudaMalloc((void **)&amp;d_u1, size); cudaMemcpy(d_u0, h_u0, size, cudaMemcpyHostToDevice); cudaMemcpy(d_u1, h_u1, size, cudaMemcpyHostToDevice); cudaMemcpyToSymbol(dN, &amp;N, sizeof(int)); cudaMemcpyToSymbol(dth2, &amp;th2, sizeof(double)); dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE, 1); dim3 dimGrid(N/BLOCK_SIZE, N/BLOCK_SIZE, 1); for(i = 0; i &lt; int(nstp/2); i++) { NextStpGPU&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;(d_u0, d_u1); NextStpGPU&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;(d_u1, d_u0); } cudaMemcpy(h_u0, d_u0, size, cudaMemcpyDeviceToHost); cudaFree(d_u0); cudaFree(d_u1); pEnd = (double) clock(); pD = (pEnd-pStart)/CLOCKS_PER_SEC; printf("Calculation time on GPU = %f sec\n", pD); } free(h_u0); free(h_u1); return 0; }</span></span></span></span></code> </pre><br></div></div><br>  Calculations show that for N = 512, the execution time of the C program parallelized on the GPU is 0.27 seconds versus 33.06 seconds for the sequential implementation of the algorithm on the CPU.  That is, the CPU / GPU acceleration is about 120 times.  With increasing N, the magnitude of the acceleration does not decrease. <br><br><h2>  Python with Numba </h2><br>  The <a href="http://numba.pydata.org/">Numba</a> library provides the ability to jit (just-in-time) compile python code into byte-code comparable in performance to C or Fortran code.  Numba supports compiling and running python code not only on the CPU, but also on the GPU, while the style and appearance of the program using the Numba library remains purely Python. <br><br><div class="spoiler">  <b class="spoiler_title">This is what our program looks like in this case.</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> numba <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cuda <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> time n = <span class="hljs-number"><span class="hljs-number">512</span></span> blockdim = <span class="hljs-number"><span class="hljs-number">16</span></span>, <span class="hljs-number"><span class="hljs-number">16</span></span> griddim = int(n/blockdim[<span class="hljs-number"><span class="hljs-number">0</span></span>]), int(n/blockdim[<span class="hljs-number"><span class="hljs-number">1</span></span>]) L = <span class="hljs-number"><span class="hljs-number">1.</span></span> h = L/n dt = <span class="hljs-number"><span class="hljs-number">0.1</span></span>*h**<span class="hljs-number"><span class="hljs-number">2</span></span> nstp = <span class="hljs-number"><span class="hljs-number">5000</span></span> @cuda.jit(<span class="hljs-string"><span class="hljs-string">"void(float64[:,:], float64[:,:])"</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">nextstp_gpu</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(u0, u)</span></span></span><span class="hljs-function">:</span></span> i,j = cuda.grid(<span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>: uim1 = u0[i<span class="hljs-number"><span class="hljs-number">-1</span></span>,j] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: uim1 = <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i &lt; n<span class="hljs-number"><span class="hljs-number">-1</span></span>: uip1 = u0[i+<span class="hljs-number"><span class="hljs-number">1</span></span>,j] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: uip1 = <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> j &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>: ujm1 = u0[i,j<span class="hljs-number"><span class="hljs-number">-1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: ujm1 = <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> j &lt; n<span class="hljs-number"><span class="hljs-number">-1</span></span>: ujp1 = u0[i,j+<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: ujp1 = <span class="hljs-number"><span class="hljs-number">1.</span></span> d2x = (uim1 - <span class="hljs-number"><span class="hljs-number">2.</span></span>*u0[i,j] + uip1) d2y = (ujm1 - <span class="hljs-number"><span class="hljs-number">2.</span></span>*u0[i,j] + ujp1) u[i,j] = u0[i,j] + (dt/h**<span class="hljs-number"><span class="hljs-number">2</span></span>)*(d2x + d2y) u0 = np.full((n,n), <span class="hljs-number"><span class="hljs-number">0.</span></span>, dtype = np.float64) u = np.full((n,n), <span class="hljs-number"><span class="hljs-number">0.</span></span>, dtype = np.float64) st = time() d_u0 = cuda.to_device(u0) d_u = cuda.to_device(u) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(<span class="hljs-number"><span class="hljs-number">0</span></span>, int(nstp/<span class="hljs-number"><span class="hljs-number">2</span></span>)): nextstp_gpu[griddim,blockdim](d_u0, d_u) nextstp_gpu[griddim,blockdim](d_u, d_u0) cuda.synchronize() u0 = d_u0.copy_to_host() <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">'time on GPU = '</span></span>, time() - st</code> </pre><br></div></div><br>  Note here are some nice features.  First, this implementation is much shorter and clearer.  Here we used two-dimensional arrays, which makes the code much more readable.  Secondly, if in the C-implementation we were required to pass all constants (for example, N) by executing functions like <code>cudaMemcpyToSymbol(dN, &amp;N, sizeof(int));</code>  here we simply use global variables as in the usual python functions.  Finally, the implementation does not require any knowledge of the C language and the GPU architecture. <br><br>  This code is easy to rewrite and for the case of using one-dimensional arrays of size n * n, as will be shown later, this significantly affects the execution speed. <br><br><div class="spoiler">  <b class="spoiler_title">Here is the code</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> numba <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cuda <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> time n = <span class="hljs-number"><span class="hljs-number">512</span></span> blockdim = <span class="hljs-number"><span class="hljs-number">16</span></span>, <span class="hljs-number"><span class="hljs-number">16</span></span> griddim = int(n/blockdim[<span class="hljs-number"><span class="hljs-number">0</span></span>]), int(n/blockdim[<span class="hljs-number"><span class="hljs-number">1</span></span>]) L = <span class="hljs-number"><span class="hljs-number">1.</span></span> h = L/n dt = <span class="hljs-number"><span class="hljs-number">0.1</span></span>*h**<span class="hljs-number"><span class="hljs-number">2</span></span> nstp = <span class="hljs-number"><span class="hljs-number">5000</span></span> @cuda.jit(<span class="hljs-string"><span class="hljs-string">"void(float64[:], float64[:])"</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">nextstp_gpu</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(u0, u)</span></span></span><span class="hljs-function">:</span></span> i,j = cuda.grid(<span class="hljs-number"><span class="hljs-number">2</span></span>) u00 = u0[i + n*j] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>: uim1 = u0[i<span class="hljs-number"><span class="hljs-number">-1</span></span> + n*j] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: uim1 = <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i &lt; n<span class="hljs-number"><span class="hljs-number">-1</span></span>: uip1 = u0[i+<span class="hljs-number"><span class="hljs-number">1</span></span> + n*j] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: uip1 = <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> j &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>: ujm1 = u0[i + n*(j<span class="hljs-number"><span class="hljs-number">-1</span></span>)] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: ujm1 = <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> j &lt; n<span class="hljs-number"><span class="hljs-number">-1</span></span>: ujp1 = u0[i + n*(j+<span class="hljs-number"><span class="hljs-number">1</span></span>)] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: ujp1 = <span class="hljs-number"><span class="hljs-number">1.</span></span> d2x = (uim1 - <span class="hljs-number"><span class="hljs-number">2.</span></span>*u00 + uip1) d2y = (ujm1 - <span class="hljs-number"><span class="hljs-number">2.</span></span>*u00 + ujp1) u[i + n*j] = u00 + (dt/h/h)*(d2x + d2y) u0 = np.full(n*n, <span class="hljs-number"><span class="hljs-number">0.</span></span>, dtype = np.float64) u = np.full(n*n, <span class="hljs-number"><span class="hljs-number">0.</span></span>, dtype = np.float64) st = time() d_u0 = cuda.to_device(u0) d_u = cuda.to_device(u) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(<span class="hljs-number"><span class="hljs-number">0</span></span>, int(nstp/<span class="hljs-number"><span class="hljs-number">2</span></span>)): nextstp_gpu[griddim,blockdim](d_u0, d_u) nextstp_gpu[griddim,blockdim](d_u, d_u0) cuda.synchronize() u0 = d_u0.copy_to_host() <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">'time on GPU = '</span></span>, time() - st</code> </pre><br></div></div><br><h2>  PyCUDA </h2><br>  The second python library tested was the <a href="https://documen.tician.de/pycuda/">PyCUDA</a> library.  Unlike Numba, here from the developer you will need to write kernel code in C, therefore you cannot do without knowledge of this language.  On the other hand, apart from the actual C kernel, you do not need to write anything. <br><br><div class="spoiler">  <b class="spoiler_title">PyCUDA code is obtained as follows.</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pycuda.driver <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> cuda <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pycuda.autoinit <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pycuda.compiler <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SourceModule <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> time n = <span class="hljs-number"><span class="hljs-number">512</span></span> blockdim = <span class="hljs-number"><span class="hljs-number">16</span></span>, <span class="hljs-number"><span class="hljs-number">16</span></span> griddim = int(n/blockdim[<span class="hljs-number"><span class="hljs-number">0</span></span>]), int(n/blockdim[<span class="hljs-number"><span class="hljs-number">1</span></span>]) L = <span class="hljs-number"><span class="hljs-number">1.</span></span> h = L/n dt = <span class="hljs-number"><span class="hljs-number">0.1</span></span>*h**<span class="hljs-number"><span class="hljs-number">2</span></span> nstp = <span class="hljs-number"><span class="hljs-number">5000</span></span> mod = SourceModule(<span class="hljs-string"><span class="hljs-string">""" __global__ void NextStpGPU(int* dN, double* dth2, double *u0, double *u) { int i = blockDim.x * blockIdx.x + threadIdx.x; int j = blockDim.y * blockIdx.y + threadIdx.y; double uim1, uip1, ujm1, ujp1, u00, d2x, d2y; uim1 = exp(-10.0); if (i &gt; 0) uim1 = u0[(i - 1) + dN[0]*j]; else uim1 = 0.0; if (i &lt; dN[0] - 1) uip1 = u0[(i + 1) + dN[0]*j]; else uip1 = 0.0; if (j &gt; 0) ujm1 = u0[i + dN[0]*(j - 1)]; else ujm1 = 0.0; if (j &lt; dN[0] - 1) ujp1 = u0[i + dN[0]*(j + 1)]; else ujp1 = 1.0; u00 = u0[i + dN[0]*j]; d2x = (uim1 - 2.0*u00 + uip1); d2y = (ujm1 - 2.0*u00 + ujp1); u[i + dN[0]*j] = u00 + dth2[0]*(d2x + d2y); } """</span></span>) u0 = np.full(n*n, <span class="hljs-number"><span class="hljs-number">0.</span></span>, dtype = np.float64) u = np.full(n*n, <span class="hljs-number"><span class="hljs-number">0.</span></span>, dtype = np.float64) nn = np.full(<span class="hljs-number"><span class="hljs-number">1</span></span>, n, dtype = np.int64) th2 = np.full(<span class="hljs-number"><span class="hljs-number">1</span></span>, dt/h/h, dtype = np.float64) st = time() u0_gpu = cuda.to_device(u0) u_gpu = cuda.to_device(u) n_gpu = cuda.to_device(nn) th2_gpu = cuda.to_device(th2) func = mod.get_function(<span class="hljs-string"><span class="hljs-string">"NextStpGPU"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(<span class="hljs-number"><span class="hljs-number">0</span></span>, int(nstp/<span class="hljs-number"><span class="hljs-number">2</span></span>)): func(n_gpu, th2_gpu, u0_gpu, u_gpu, block=(blockdim[<span class="hljs-number"><span class="hljs-number">0</span></span>],blockdim[<span class="hljs-number"><span class="hljs-number">1</span></span>],<span class="hljs-number"><span class="hljs-number">1</span></span>),grid=(griddim[<span class="hljs-number"><span class="hljs-number">0</span></span>],griddim[<span class="hljs-number"><span class="hljs-number">1</span></span>],<span class="hljs-number"><span class="hljs-number">1</span></span>)) func(n_gpu, th2_gpu, u_gpu, u0_gpu, block=(blockdim[<span class="hljs-number"><span class="hljs-number">0</span></span>],blockdim[<span class="hljs-number"><span class="hljs-number">1</span></span>],<span class="hljs-number"><span class="hljs-number">1</span></span>),grid=(griddim[<span class="hljs-number"><span class="hljs-number">0</span></span>],griddim[<span class="hljs-number"><span class="hljs-number">1</span></span>],<span class="hljs-number"><span class="hljs-number">1</span></span>)) u0 = cuda.from_device_like(u0_gpu, u0) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">'time on GPU = '</span></span>, time() - st</code> </pre><br></div></div><br>  This all looks like a pure python with the exception of a local C-code insert. <br><br><h2>  Performance comparison </h2><br>  Figure 1 and Table 1 show the dependencies of the test program runtime (in seconds) on the mesh size n, obtained when running the C code (CUDA C curve) and python implementations with the Numba library and two-dimensional arrays (Numba 2DArr), with the library Numba and one-dimensional arrays (Numba 1DArr), with the PyCUDA library (PyCUDA curve). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/714/8dc/811/7148dc81141d4289bee3c0ce1607de0b.png" alt="Picture 1"></div><br>  <i>Picture 1</i> <br><table><caption>  Table 1 </caption><tbody><tr><th>  n </th><th>  Cuda c </th><th>  Numba 2DArr </th><th>  Numba 1DArr </th><th>  PyCUDA </th></tr><tr><td>  512 </td><td>  0.25 </td><td>  0.8 </td><td>  0.66 </td><td>  0.216 </td></tr><tr><td>  1024 </td><td>  0.77 </td><td>  3.26 </td><td>  1.03 </td><td>  0.808 </td></tr><tr><td>  2048 </td><td>  2.73 </td><td>  12.23 </td><td>  4.07 </td><td>  2.87 </td></tr><tr><td>  3073 </td><td>  6.1 </td><td>  27.3 </td><td>  9.12 </td><td>  6.6 </td></tr><tr><td>  4096 </td><td>  11.05 </td><td>  55.88 </td><td>  16.27 </td><td>  12.02 </td></tr></tbody></table><br>  Figure 2 shows the relationship between the execution times of various python implementations and the execution time of the C code.  As can be seen from the figures, the slowest of the considered is the implementation using the Numba library using two-dimensional arrays.  At the same time, this approach is the most obvious and simple.  Interestingly, the sweep of two-dimensional arrays in one-dimensional leads to approximately threefold acceleration of the code.  The quickest solution was to use the PyCUDA library.  At the same time, as noted above, using this library is somewhat more laborious, since it requires writing a kernel in C. However, the costs pay off and the execution speed of such a python program is only 5-8% less than a program written entirely in C. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/6ef/bd0/33f/6efbd033fbe043b2a894efdb085e16fd.png" alt="Figure 2"></div><br>  <i>Figure 2</i> <br><br><h2>  findings </h2><br>  Miracles do not happen and the most simple and visual solutions are at the same time the slowest.  At the same time, the available libraries allow you to achieve the speed of execution of python-programs, comparable to the speed of execution of pure C-code.  Existing libraries give the developer a choice between more and less high-level solutions.  This choice, however, there is always a trade-off between the speed of development and the speed of program execution. <br><br><h2>  Links </h2><br>  ¬ª <a href="http://numba.pydata.org/">Numba Library Documentation</a> <br>  ¬ª <a href="http://numba.pydata.org/numba-doc/0.13/CUDAJit.html">Examples of using Numba</a> <br>  ¬ª <a href="https://documen.tician.de/pycuda/">PyCUDA Library Documentation</a> <br>  ¬ª <a href="https://wiki.tiker.net/PyCuda/Examples">PyCUDA usage examples</a> </div><p>Source: <a href="https://habr.com/ru/post/317328/">https://habr.com/ru/post/317328/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../317314/index.html">Progress Tracking in R</a></li>
<li><a href="../317318/index.html">Creating a website as a product or "why is it so expensive?"</a></li>
<li><a href="../317320/index.html">The study of connectivity in the brain based on electrophysiological data. Lecture in Yandex</a></li>
<li><a href="../317322/index.html">Pentest at Global Data Security - passing the 10th Pentestit Lab</a></li>
<li><a href="../317326/index.html">Ruby code coverage analysis</a></li>
<li><a href="../317332/index.html">Communication strategy as a tool for building a career and personal brand</a></li>
<li><a href="../317334/index.html">The digest of interesting materials for the mobile # 183 developer (December 5-11)</a></li>
<li><a href="../317336/index.html">Scrapbook with M * CTF</a></li>
<li><a href="../317338/index.html">Set up Swashbuckle (Swagger) for WebAPI</a></li>
<li><a href="../317340/index.html">How I chose the GTD system for IT using the Wunderlist example</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>