<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Calibrate Kinect v2 with OpenCV in Python</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Not so long ago, we started a couple of projects that require an optical system with a range channel, and decided to use Kinect v2 for this. Since the...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Calibrate Kinect v2 with OpenCV in Python</h1><div class="post__text post__text-html js-mediator-article">  Not so long ago, we started a couple of projects that require an optical system with a range channel, and decided to use Kinect v2 for this.  Since the projects are implemented in Python, you first had to get Kinect to work from Python, and then calibrate it, since Kinect out of the box introduces some geometric distortion in the frames and gives centimeter errors in determining the depth. <br><br>  Before that, I have never dealt with computer vision, or with OpenCV, or with Kinect.  I did not manage to find an exhaustive instruction on how to work with all this farming, so I had to tinker with it in the end.  And I decided that it would not be superfluous to systematize the experience gained in this article.  Perhaps, it will be useful for some suffering, <s>and we also need a popular article for a tick in the reporting</s> . <br><br><img src="https://habrastorage.org/files/fa0/4ae/2d7/fa04ae2d7d414303b1cd224d7416b8f2.png" alt="image"><br><a name="habracut"></a><br>  <u>Minimum system requirements</u> : Windows 8 and higher, Kinect SDK 2.0, USB 3.0. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Table I. Kinect v2 features: <br><table><tbody><tr><td>  RGB camera resolution, pix. </td><td>  1920 x 1080 </td></tr><tr><td>  Infrared (IR) camera resolution, pix. </td><td>  512 x 424 </td></tr><tr><td>  RGB camera viewing angles, ¬∫ </td><td>  84.1 x 53.8 </td></tr><tr><td>  IR camera viewing angles, ¬∫ </td><td>  70.6 x 60.0 </td></tr><tr><td>  Range of measurement range, m </td><td>  0.6 - 8.0 <sup>1</sup> </td></tr><tr><td>  RGB camera shooting frequency, Hz </td><td>  thirty </td></tr><tr><td>  IR shooting frequency, Hz </td><td>  thirty </td></tr></tbody></table><br><div class="spoiler">  <b class="spoiler_title">one</b> <div class="spoiler_text">  Information varies from source to source.  They say about 0.5‚Äì4.5 m., In fact I received ~ 0.6-8.0 m. <br></div></div><br>  Thus, I had the following tasks: <br><br><ol><li>  get Kinect on Python; </li><li>  calibrate RGB and IR cameras; </li><li>  to realize the possibility of combining RGB and IR frames; </li><li>  calibrate the depth channel. </li></ol><br>  And now we will dwell on each item. <br><br><h4>  <b>1. Kinect v2 and Python</b> </h4><br>  As I have already said, I hadn‚Äôt done anything with computer vision before, but I heard rumors that without the OpenCV library there is nowhere.  And since it has a whole camera calibration <a href="http://docs.opencv.org/3.0.0/d9/d0c/group__calib3d.html">module</a> , the first thing I did was assemble OpenCV with Python 3 support under Windows 8.1.  It was not without some trouble, usually accompanying the assembly of open-source projects on Windows, but everything went without any surprises and in general as part of the <a href="http://docs.opencv.org/master/d5/de5/tutorial_py_setup_in_windows.html">instructions</a> from the developers. <br>  I had to tinker a bit longer with Kinect.  The official SDK only supports interfaces for C #, C ++ and JavaScript.  If you go to the other side, you can see that OpenCV <a href="http://docs.opencv.org/3.0-rc1/d7/df5/tutorial_ug_highgui.html">supports</a> input from 3D cameras, but the camera should be compatible with the OpenNI library.  OpenNI supports Kinect, but the relatively recent Kinect v2 does not.  However, good people wrote a <a href="https://github.com/kaorun55/OpenNI2-Kinect2Driver">driver</a> for Kinect v2 under OpenNI.  It even works and allows you to admire the video from the device channels in NiViewer, but when used with OpenCV crashes.  However, other good people wrote a <a href="https://github.com/kinect/PyKinect2">Python wrapper</a> over the official SDK.  I stopped at it. <br><br><h4>  <b>2. Camera Calibration</b> </h4><br>  The cameras are not perfect, they distort the image and need to be calibrated.  To use Kinect for measurements, it would be nice to eliminate these geometric distortions on both the RGB camera and the depth sensor.  Since the IR camera is also the receiver of the depth sensor, we can use IR frames for calibration, and then use the calibration results to eliminate distortions from the depth frames. <br><br>  Calibration of the camera is carried out in order to find out the internal parameters of the camera, namely, the camera matrix and the distortion coefficients. <br><br><img src="https://habrastorage.org/files/43f/3ff/9e5/43f3ff9e56d94ba0bf206ec38ce737a2.png" alt="image" height="400" width="444"><br><br>  The matrix of the camera is called the matrix of the form: <br><br><img src="https://habrastorage.org/files/61d/95f/272/61d95f27296b4a80b058799edc53756d.png">  Where <br><br>  ( <i> <sub>u</sub> , c <sub>v</sub></i> ) - coordinates of the principal point (the point of intersection of the optical axis with the image plane, in the ideal camera to be exactly in the center of the image, in real ones it is slightly offset from the center); <br><br>  <i>f <sub>u</sub> , f <sub>v</sub></i> is the focal length <i>f</i> measured in width and height of a pixel. <br><br>  There are two main types of distortion: radial distortion and tangential distortion. <br><br>  <u>Radial distortion</u> - image distortion as a result of the non-ideality of the parabolic lens shape.  Distortions caused by radial distortion are 0 in the optical center of the sensor and increase towards the edges.  As a rule, radial distortion makes the greatest contribution to image distortion. <br><br>  <u>Tangential distortion</u> - image distortion caused by errors in the installation of the lens parallel to the image plane. <br><br><img src="https://habrastorage.org/files/a2d/34e/f76/a2d34ef76130433586b5106c179e6669.png"><br><br>  To eliminate distortion pixel coordinates can be recalculated using the following <a href="http://docs.opencv.org/2.4/doc/tutorials/calib3d/camera_calibration/camera_calibration.html">equation</a> : <br><br><img src="https://habrastorage.org/files/ffd/b2a/718/ffdb2a718a1b4702930f4af387515157.png"><br><br>  where ( <i>u, v</i> ) is the initial location of the pixel, <br>  ( <i>u <sub>corrected</sub> , v <sub>corrected</sub></i> ) - the location of the pixel after removing geometric distortions, <br>  <i>k <sub>1</sub> , k <sub>2</sub> , k <sub>3</sub></i> - the coefficients of radial distortion, <br>  <i>p <sub>1</sub> p <sub>2</sub></i> - the coefficients of tangential distortion, <br>  <i>r <sup>2</sup> = u <sup>2</sup> + v <sup>2</sup> .</i> <br><br>  The accuracy of the measurement of camera parameters (distortion coefficients, camera matrix) is determined by the average value of the reprojection error ( <i>ReEr, Reprojection Error</i> ).  <i>ReEr</i> is the distance (in pixels) between the projection <i>P '</i> on the image plane of the point <i>P</i> on the object's surface, and the projection <i>P' 'of</i> the same point <i>P</i> , constructed after eliminating distortion using camera parameters. <br><br><img src="https://habrastorage.org/files/83f/15b/255/83f15b25525f48fcbf3e917689a67e1f.png" alt="image" height="356" width="400"><br><br>  The standard camera calibration procedure consists of the following steps: <br><br>  1) make 20-30 photographs with different positions of the <s>object with the known geometry of a</s> chessboard; <br><br><img src="https://habrastorage.org/files/a77/4f0/58d/a774f058dd01405e9391df228a2acea4.png" alt="image" height="572" width="1000"><br><br>  2) identify the key points of the object in the image; <br><br><pre><code class="python hljs">found, corners = cv2.findChessboardCorners(img, <span class="hljs-comment"><span class="hljs-comment"># PATTERN_SIZE,#  ,    6x8 flags)#  </span></span></code> </pre> <br><img src="https://habrastorage.org/files/856/8f8/e09/8568f8e092794e36a902296d0337f931.png"><br><br>  3) find the distortion coefficients that minimize <i>ReEr</i> . <br><br><pre> <code class="python hljs">ReEr, camera_matrix, dist_coefs, rvecs, tvecs = cv2.calibrateCamera(obj_points,<span class="hljs-comment"><span class="hljs-comment">#       #(', y', z'=0) img_points,#    (u,v) (w, h),#  None,#      None, #      criteria = criteria,#   ReEr flags = flags)#     </span></span></code> </pre><br>  In our case, the average <i>ReEr</i> value for the RGB camera was 0.3 pixels, and for the IR camera - 0.15.  The results of the elimination of distortion: <br><br><pre> <code class="python hljs">img = cv2.undistort(img, camera_matrix, dist_coefs)</code> </pre><br><img src="https://habrastorage.org/files/39d/1f7/746/39d1f774669f498aa5d667c9f76e530b.png"><br><br><h4>  <b>3. Combining frames from two cameras</b> </h4><br><br><img src="https://habrastorage.org/files/1a9/330/541/1a93305411f348a8881a1a87b0139b3a.png" height="444" width="400"><br><br>  In order to get both a depth (Z coordinate) for a pixel and a color, first you need to go from pixel coordinates on a depth frame to three-dimensional coordinates of an IR camera [2]: <br><br><img src="https://habrastorage.org/files/eef/3cb/3b3/eef3cb3b37764d6f9f0b8014c84896a7.png"><br><br>  where ( <i>x <sub>1</sub> , y <sub>1</sub> , z <sub>1</sub></i> ) are the coordinates of a point in the coordinate system of the IR camera, <br>  <i>z <sub>1</sub></i> is the result returned by the depth sensor, <br>  ( <i>u <sub>1</sub> , v <sub>1</sub></i> ) - coordinates of the pixel in the depth frame, <br>  <i>c <sub>1, u</sub> , c <sub>1, v</sub></i> - coordinates of the optical center of the IR camera, <br>  <i>f <sub>1, u</sub> , f <sub>1, v</sub></i> are the projections of the focal length of the IR camera. <br><br>  Then you need to go from the coordinate system of the IR camera to the coordinate system of the RGB camera.  To do this, you need to move the origin using the transfer vector <i>T</i> and rotate the coordinate system using the rotation matrix <i>R</i> : <br><br><img src="https://habrastorage.org/files/139/483/292/1394832929d648e891f4920a958a9cb8.png"><br><br>  Then you need to go from the three-dimensional coordinate system of the RGB camera to the pixel coordinates of the RGB frame: <br><br><img src="https://habrastorage.org/files/305/db9/532/305db953214d446d88f25bd712f3233f.png"><br><br>  Thus, after all these transformations, we can get for the pixel ( <i>u <sub>1</sub> , v <sub>1</sub></i> ) of the depth frame the color value of the corresponding pixel of the RGB frame ( <i>u <sub>2</sub> , v <sub>2</sub></i> ). <br><br><img src="https://habrastorage.org/files/04c/17e/1b5/04c17e1b551b4361a3d0b0e30754dc55.png"><br><br>  As seen in the resulting image, the image is twofold.  The same effect can be <a href="http://www.bryancook.net/2014/03/mapping-between-kinect-color-and-depth.html">observed</a> when using the <i>CoordinateMapper</i> class from the official SDK.  However, if only a person is interested in the image, then you can use <i>bodyIndexFrame</i> (the Kinect stream, which allows you to find out which pixels belong to a person and which ones to the background) to highlight the region of interest and eliminate ghosting. <br><br><img src="https://habrastorage.org/files/0f4/477/36e/0f447736ebc747fbbca89ffbd0288ec9.png"><br><br>  To determine the rotation matrix <i>R</i> and the transfer vector <i>T,</i> it is necessary to carry out a joint calibration of the two cameras.  To do this, you need to take 20-30 photographs of an object with known geometries in different positions with both RGB and IR cameras, it is better not to hold the object in your hands, in order to exclude the possibility of its displacement between taking pictures with different cameras.  Then you need to use the <i>stereoCalibrate</i> function from the OpenCV library.  This function determines the position of each camera relative to the calibration object, and then finds such a transformation from the coordinate system of the first camera to the coordinate system of the second camera, which minimizes ReEr. <br><br><pre> <code class="python hljs">retval, cameraMatrix1, distCoeffs1, cameraMatrix2, distCoeffs2, R, T, E, F = cv2.stereoCalibrate(pattern_points, <span class="hljs-comment"><span class="hljs-comment">#  #     (', y', z'=0) ir_img_points,#     (u1, v1) rgb_img_points, #   RGB  (u2, v2) irCamera['camera_matrix'],#   (  calibrateCamera), irCamera['dist_coefs'], #. .   (  calibrateCamera) rgbCamera['camera_matrix'], # RGB  (  calibrateCamera) rgbCamera['dist_coefs'], #. . RGB  (  calibrateCamera) image_size) #    ( )</span></span></code> </pre><br>  And in the end we got <i>ReEr</i> = 0.23. <br><br><h4>  <b>4. Channel depth calibration</b> </h4><br>  The Kinect depth sensor returns the depth (namely, depth, i.e., Z-coordinate, not distance) in mm.  But how accurate are these values?  Judging by the publication [2], the error may be 0.5‚Äì3 cm depending on the distance, so it makes sense to calibrate the depth channel. <br><br>  This procedure is to find the Kinect systematic error (the difference between the reference depth and the depth given by the sensor) depending on the distance to the object.  And for this you need to know the reference depth.  The most obvious way is to position the flat object parallel to the camera plane and measure the distance to it with a ruler.  By gradually moving the object and making a series of measurements at each distance, one can find the average error for each of the distances.  But, firstly, it is not very convenient, and secondly, to find a perfectly flat object of relatively large dimensions and to ensure parallelism of its location relative to the plane of the camera is more difficult than it might seem at first glance.  Therefore, as a reference against which the error will be calculated, we decided to take the depth, which is determined by the known geometry of the object. <br><br>  Knowing the geometry of the object (for example, the size of the cells of a chessboard) and placing it strictly parallel to the plane of the chamber, you can determine the depth to it as follows: <br><br><img src="https://habrastorage.org/files/1f7/52e/e05/1f752ee05f5647298c67cf614d899e9c.png"><br><br>  where <i>f</i> is the focal length, <br>  <i>d</i> is the distance between the projections of the key points on the camera matrix, <br>  <i>D</i> is the distance between the key points of the object, <br>  <i>Z</i> is the distance from the center of the camera projection to the object. <br><br><img src="https://habrastorage.org/files/720/301/c40/720301c40b1d4e839ba8512f31d2d2aa.png" height="270" width="190"><br><br>  If the object is located not strictly parallel, but at a certain angle to the camera plane, the depth can be determined based on the solution of the Perspective-n-Point (PnP) problem [3].  A number of algorithms implemented in the OpenCV library that allow you to find the transformation |  <i>R, T</i> |  between the coordinate system of the calibration object and the coordinate system of the camera, and therefore, determine the depth to the accuracy of the camera parameters. <br><br><pre> <code class="python hljs">retval, R, T = cv2.solvePnP(obj_points[:, [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">42</span></span>, <span class="hljs-number"><span class="hljs-number">47</span></span>]],<span class="hljs-comment"><span class="hljs-comment">#     img_points[:, [0, 5, 42, 47]], #     rgbCameraMatrix,#  rgbDistortion,#  flags= cv2.SOLVEPNP_UPNP)#  PnP R, jacobian = cv2.Rodrigues(R)#       for j in range(0, numberOfPoints): #     point = numpy.dot(rgb_obj_points[j], RT) + TT # !       , #      , ,      computedDistance[j] = point[0][2] * 1000 # Z-  </span></span></code> </pre><br>  To calibrate the depth channel, we made a series of surveys of the calibration object at distances of ~ 0.7-2.6 m with a step of ~ 7 cm.  The calibration object was located in the center of the frame parallel to the plane of the camera, as far as it is possible to do "by eye".  At each distance, one RGB image was taken with a camera and 100 images with a depth sensor.  The data from the sensor were averaged, and the distance determined by the geometry of the object based on the RGB frame was taken as the standard.  The average error in determining the depth of the Kinect sensor at this distance was determined as follows: <br><br><img src="https://habrastorage.org/files/cb7/d88/416/cb7d884169bf400eb10f717a81192e96.png"><br><br>  where <i>z <sub>i</sub> <sup>RGB</sup></i> - the distance to the i-th key point in geometry, <br>  <i>z <sub>i</sub> <sup>depth</sup></i> - the distance averaged over 100 frames to the i-th key point according to the depth sensor, <br>  N - the number of key points on the object (in our case 48). <br><br>  Then we obtained the error function of distance by interpolating the results obtained. <br><br><img src="https://habrastorage.org/files/b9a/913/275/b9a9132758964ba2a2601188f4dfd4d5.png"><br><br>  The figure below shows the distribution of errors before and after the correction on the calibration frames.  A total of 120,000 measurements were made (25 distances, 100 depth frames at each, 48 key points on the object).  The error before correction was 17 ¬± 9.95 mm (mean ¬± standard deviation), after - 0.45 ¬± 8.16 mm. <br><br><img src="https://habrastorage.org/files/1f5/d46/c29/1f5d46c294e14e6da0c94bb15b87de0e.png"><br><br>  Then 25 test frames (RGB and depth) of the calibration object were made in different positions.  Total 1200 measurements (25 frames, 48 ‚Äã‚Äãkey points on each).  The error before correction was 7.41 ¬± 6.32 mm (mean ¬± standard deviation), after - 3.12 ¬± 5.50 mm.  The figure below shows the distribution of errors before and after correction on test frames. <br><br><img src="https://habrastorage.org/files/798/ed0/1f9/798ed01f9f1b4d61aa7befd869ff59f9.png"><br><br><h4>  <b>Conclusion</b> </h4><br>  Thus, we have eliminated the geometric distortions of the RGB camera and the depth sensor, learned how to combine frames, and improved the accuracy of determining the depth.  The code for this project can be found <a href="https://github.com/tataraidze/KinectV2">here</a> .  I hope it will be useful. <br><br>  <i>The study was carried out by a grant from the Russian Science Foundation (project No. 15-19-30012)</i> <br><br><h4>  <b>List of sources</b> </h4><br>  1. Kramer J. Hacking the Kinect / Apress.  2012. P. 130 <br>  2. Lachat E. et al.  Close Range 3D Modeling, International Remote Sensing and Spatial Information Sciences.  2015 <br>  3. Gao XS et al.  IEEE Transactions for Analysis and Machine Intelligence.  Vol.  25. N 8. 2003. P. 930-943. </div><p>Source: <a href="https://habr.com/ru/post/272629/">https://habr.com/ru/post/272629/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../272619/index.html">Self-modifying code</a></li>
<li><a href="../272621/index.html">Material Design Competition: Winners Announced</a></li>
<li><a href="../272623/index.html">Simple but useful plugin for Redmine</a></li>
<li><a href="../272625/index.html">How to write a JS library in ScalaJS</a></li>
<li><a href="../272627/index.html">Installing a virtual environment and agent program on the example of antivirus</a></li>
<li><a href="../272631/index.html">The test from Rostelecom to test knowledge in the field of system administration and user support</a></li>
<li><a href="../272633/index.html">Free hosting control panels. "Coin"</a></li>
<li><a href="../272635/index.html">Heavy armor sound design for the InSomnia project</a></li>
<li><a href="../272639/index.html">Video of Badoo reports from the Highload 2015 conference</a></li>
<li><a href="../272641/index.html">FireEye Specialists Discover New Bootkit</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>