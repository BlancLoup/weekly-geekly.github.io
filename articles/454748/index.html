<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>MongoDB Survival Guide</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="All good startups either die quickly or grow to the need to scale. We will model such a startup, which is first about features, and then about perform...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>MongoDB Survival Guide</h1><div class="post__text post__text-html js-mediator-article">  All good startups either die quickly or grow to the need to scale.  We will model such a startup, which is first about features, and then about performance.  We will improve performance with MongoDB - this is a popular NoSQL solution for data storage.  It is easy to start with MongoDB, and many problems have out-of-the-box solutions.  However, when the load grows, a rake comes out, about which nobody warned you beforehand ... until today! <br><br><img src="https://habrastorage.org/webt/oh/rq/ua/ohrquayfyh04hfgs-gareddfzlk.gif" alt="image"><br><br>  The modeling is carried out by <strong>Sergey Zagursky</strong> , who is responsible for the backend infrastructure in general, and MongoDB in particular, in <a href="https://habr.com/ru/company/joom/">Joom</a> .  Also was seen in the server side of the development of MMORPG Skyforge.  As Sergey describes himself - ‚Äúa professional taper of cones with his own forehead and rake‚Äù.  Under the microscope, a project that uses an accumulation strategy to manage technical debt.  In this text version of the <a href="https://youtu.be/j6TVaEk4x2U">report</a> on HighLoad ++, we will move in chronological order from the appearance of the problem to the solution using MongoDB. <br><a name="habracut"></a><br><h2>  First difficulties </h2><br>  We simulate a startup that fills cones.  The first stage of life - features are launched in our startup and, unexpectedly, users come.  Our small-small MongoDB server is under load, which we didn‚Äôt even dream of.  But we are in the cloud, we are a startup!  We do the simplest things possible: we look at requests - oh, and here we have the whole correction is read for each user, then we will build the indices, we will add the hardware, and here we will cache. <br>  All - we live on! 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <blockquote>  If problems can be solved by such simple means - they should be solved in the same way. </blockquote><br>  But the further way of a successful startup is a slow, painful delay of the moment of horizontal scaling.  I will try to give advice on how to survive this period, get to scale and not step on a rake. <br><br><h2>  Slow recording </h2><br>  This is one of the problems you can face.  What to do if you met her, and the methods above do not help?  Answer: <strong>durability</strong> <strong>guarantee mode</strong> <strong>in MongoDB by default</strong> .  In three words, it is arranged as follows: <br><br><ul><li>  We came to the primary replica and said: "Write!". <br></li><li>  Primary replica recorded. <br></li><li>  After that, they read the secondary replicas from it and said to the primary: ‚ÄúWe recorded it!‚Äù. <br></li></ul><br>  At that moment, when most secondary replicas have done this, the request is considered completed and control is returned to the driver in the application.  Such guarantees allow you to be sure that when control is returned to the application, durability will not go anywhere, even if MongoDB falls, except for some very terrible disasters. <br><br><blockquote>  Fortunately, MongoDB is a database that allows you to reduce the durability guarantee for each individual request. </blockquote><br>  For important requests, we can keep the maximum guarantees for durability by default, and for some requests - reduce. <br><br><h3>  Query classes </h3><br>  The first guarantee layer we can remove is <strong>not to wait for confirmation of the record by the majority of replicas</strong> .  This will save latency, but will not add bandwidth.  But sometimes latency is what you need, especially if the cluster is slightly overloaded and the secondary replicas are not working as fast as we would like. <br><br><pre><code class="javascript hljs">{<span class="hljs-attr"><span class="hljs-attr">w</span></span>:<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-attr"><span class="hljs-attr">j</span></span>:<span class="hljs-literal"><span class="hljs-literal">true</span></span>}</code> </pre> <br>  If we write records with such guarantees, then at the moment when we get control to the application, we no longer know whether the record will be alive after some kind of accident.  But, usually, she is still alive. <br><br>  The next guarantee, which affects bandwidth and latency too, is <strong>disabling confirmation of the entry in the log</strong> .  Logging is written in any case.  The magazine is one of the fundamental mechanisms.  If we turn off the confirmation of entry into it, then we do not do two things: <a href="https://docs.mongodb.com/manual/reference/command/fsync/"><u><strong>fsync</strong></u></a> <strong>on the log</strong> and <strong>do not wait for it to end</strong> .  This can well <strong>save disk system resources</strong> and get a <strong>multiple increase in throughput by</strong> simply changing the durability warranty. <br><br><pre> <code class="javascript hljs">{<span class="hljs-attr"><span class="hljs-attr">w</span></span>:<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-attr"><span class="hljs-attr">j</span></span>:<span class="hljs-literal"><span class="hljs-literal">false</span></span>}</code> </pre> <br>  The most ‚Äútough‚Äù durability guarantees is the <strong>disconnection of any evidence</strong> .  We will only receive confirmation that the request reached the primary replica.  This saves latency and does not increase throughput. <br><br><pre> <code class="javascript hljs">{<span class="hljs-attr"><span class="hljs-attr">w</span></span>:<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-attr"><span class="hljs-attr">j</span></span>:<span class="hljs-literal"><span class="hljs-literal">false</span></span>} ‚Äî   .</code> </pre> <br>  We will also receive various other things, for example, the recording failed due to a conflict with a unique key. <br><br><h3>  To which operations is this applicable? </h3><br>  I'll tell you about the application to the setup in Joom.  In addition to the load from users, in which there are no durability eases, there is a load that can be described as a background batch load: updating, recalculating ratings, collecting analytics data. <br><br>  These background operations can go on for hours, but they are designed so that when a breakaway occurs, for example, a backend crash, they will not lose the result of all their work, but will resume from a point in the recent past.  For such tasks, reducing the durability guarantee is useful, especially since fsync in the log, like any other operation, will increase latency also for reading. <br><br><h2>  Read scaling </h2><br>  The next problem is <strong>insufficient reading throughput</strong> .  Recall that in our cluster there are not only primary replicas, but also secondary ones <strong>from which to read</strong> .  Let's do it. <br><br>  You can read, but there are nuances.  From secondary replicas will come a little outdated data - for 0.5-1 seconds.  In most cases this is normal, but the behavior of the secondary replica is different from the behavior of the primary replicas. <br><br>  On secondary, there is a process of applying oplog, which is not on the primary replica.  This process is not something that is designed for low latency - MongoDB developers just do not bother with this.  Under some conditions, the process of applying oplog from primary to secondary can give delays of up to 10 s. <br><br><blockquote>  For user requests, secondary replicas are not suitable - user experiences a cheerful step goes into the trash can. </blockquote><br>  On non-hardened clusters, these spikes are less noticeable, but they are still there.  Shardirovannye clusters are affected, because the removal of oplog is particularly affected by the use of oplog, and <strong>deletion is part of the work of the balancer</strong> .  Balancer relish, tastefully removes documents in tens of thousands in a short period of time. <br><br><h2>  Number of connections </h2><br>  The next factor to think about is the <strong>limit on the number of connections on MongoDB instances</strong> .  By default, there are no restrictions, <strong>except for OS resources ‚Äî</strong> you can connect as long as it permits. <br><br>  However, the more parallel concurrent requests, the slower they are executed.  <strong>Performance degrades nonlinearly</strong> .  Therefore, if the spike of requests has arrived to us, it is better to serve 80% than not to serve 100%.  The number of connections should be limited directly to MongoDB. <br><br>  But there are bugs that can cause trouble for this.  In particular, the <strong>connection pool on the MongoDB side is common for both user and service intracluster connections</strong> .  If the application has "eaten" all the connections from this pool, then the integrity of the cluster can be broken. <br><br>  We learned about this when we were going to rebuild the index, and since we needed to remove the uniqueness from the index, the procedure went through several stages.  In MongoDB it is impossible to build next to the index the same, but without uniqueness.  Therefore, we wanted: <br><br><ul><li>  build a similar index without uniqueness; <br></li><li>  delete index with uniqueness; <br></li><li>  build an index without uniqueness instead of a remote one; <br></li><li>  remove temporary. <br></li></ul><br>  When the temporary index was still being completed on secondary, we began to delete the unique index.  At this point, the secondary MongoDB announced its blocking.  Some metadata were blocked, and in the majority all the records stopped: they hung in the <a href="https://en.wikipedia.org/wiki/Connection_pool"><u>connection pool</u></a> and waited for them to confirm that the record passed.  All readings on the secondary also stopped, because the global log was captured. <br><br>  A cluster in such an interesting state has also lost its connectedness.  Sometimes it appeared and when two cues were connected to each other, they tried to make in their state a choice that they could not make, because they had a global lock. <br><br><blockquote>  Moral of the story: the number of connections must be monitored. </blockquote><br>  There is a well-known MongoDB rake, which is still being attacked so often that I decided to walk along it briefly. <br><br><h2>  Do not lose documents </h2><br>  If in MongoDB to send a request by index, then the <strong>request may not return all documents</strong> that satisfy the condition, and in completely unexpected cases.  This is due to the fact that when we go to the beginning of the index, the document, which at the end, moves to the beginning for the documents that we have passed.  This is due solely <strong>to the index mutability</strong> .  For reliable iteration, use <strong>indexes on non-stable fields</strong> and there will be no difficulties. <br>  MongoDB has its own views on which indexes to use.  It is solved simply - <strong>with the help of $ hint, we force MongoDB to use the index that was specified</strong> . <br><br><h2>  Sizes of collections </h2><br>  Our startup is developing, there is a lot of data, but I don‚Äôt want to add disks ‚Äî they have already been added three times in the last month.  Let's see what is stored in our data, look at the size of the documents.  How to understand where in the collection you can reduce the size?  According to two parameters. <br><br><ul><li>  <strong>By the size of</strong> <strong>specific documents</strong> , to play <code>Object.bsonsize()</code> with their length: <code>Object.bsonsize()</code> ; <br></li><li>  <strong>By average</strong> <strong>document size in</strong> <strong>collection</strong> : <code>db.c.stats().avgObjectSize</code> . <br></li></ul><br><h3>  How to affect the size of the document? </h3><br>  I have non-specific answers to this question.  The first - the <strong>long field name increases the size of the document.</strong>  In each document, all the field names are copied, so if the document has a long field name, then the size of the name needs to be added to the size of each document.  If you have a collection with a huge number of small documents in several fields, then call the fields short names: "A", "B", "CD" - a maximum of two letters.  <strong>On disk, this is compensated by compression</strong> , but everything is stored in the cache as is. <br><br>  The second tip - sometimes <strong>some fields with low cardinality can be rendered in the name of the collection</strong> .  For example, such a field could be a language.  If we have a collection with translations into Russian, English, French and a field with information about the stored language - the value of this field can be put into the name of the collection.  So we will <strong>reduce the size of the documents</strong> and can <strong>reduce the number and size of indices</strong> - a solid savings!  This is not always possible to crank, because sometimes there are indexes inside the document that will not work if the collection is smashed into different collections. <br><br>  Last tip on document size - <strong>use the _id field</strong> .  If there is a natural unique key in your data, place it directly in the id field.  Even if the key is composite, use the compound id.  It is well indexed.  There is only one small rake - if your marshaller sometimes changes the order of fields, then id with the same field values, but with different order will be considered different id in terms of a unique index in MongoDB.  In some cases, this can happen in Go. <br><br><h2>  Index sizes </h2><br>  <strong>The index stores a copy of the fields that it contains</strong> .  Index size consists of data that is indexed.  If we are trying to index large fields, then get ready for the fact that the size of the index will be large. <br><br>  The second moment greatly inflates the indices: <strong>array fields in the index multiply other fields from the document in this index</strong> .  Be careful with large arrays in the documents: either do not index something else to the array, or play around with the order of listing the fields in the index. <br><br>  <strong>The order of the fields matters</strong> , <strong>especially if one of the index fields is an array</strong> .  If the fields differ in cardinality, and in one field the number of possible values ‚Äã‚Äãis very different from the number of possible values ‚Äã‚Äãin another, then it makes sense to arrange them to increase cardinality.  <strong>You can easily save 50% of the size of the index, if you swap fields with different cardinality.</strong>  Field permutation may also result in a more significant reduction in size. <br><br>  Sometimes, when the field contains a large value, we do not need to compare this value more or less, but rather a comparison on a clear equality.  Then the <strong>index on the field with heavy contents</strong> can be <strong>replaced with the index on hash from this field</strong> .  The index will store copies of hash, not copies of these fields. <br><br><h2>  Deleting documents </h2><br>  I have already mentioned that deleting documents is an unpleasant operation and <strong>it is better not to delete them, if possible.</strong>  When developing the design of the data schema, try to provide for either the minimum removal of individual data, or the removal of entire collections.  You can delete them with entire collections.  Deleting collections is a cheap operation, and deleting thousands of individual documents is a hard operation. <br><br>  If it still happens that you want to delete a lot of documents, be sure to <strong>do throttling</strong> , otherwise mass deletion of documents will affect the latency of reading and will be unpleasant.  This is especially bad for latency on secondary. <br><br>  It is worth making some kind of ‚Äúhandle‚Äù to twist the throttling - from the first time it is very difficult to pick up the level.  We went through it so many times that trotting is guessed from the third, fourth time.  Initially, provide the opportunity to tweak it. <br><br>  <strong>If you delete more than 30% of a large collection, then transfer the live documents to the next collection</strong> , and delete the old collection entirely.  It is clear that there are nuances, because the load is switched from the old to the new collection, but shift it, if possible. <br><br>  Another way to delete documents is the <strong>TTL index</strong> ‚Äî an index that indexes the field in which the Mongo timestamp lies, which contains the date of death of the document.  When that time comes, MongoDB will delete this document automatically. <br><br>  The TTL index is convenient, but <strong>there is no throttling in the implementation.</strong>  MongoDB doesn‚Äôt bother to remove these deletes.  If you try to delete a million documents at the same time, for a few minutes you will have an unworkable cluster that only deals with deletion and nothing else.  To prevent this from happening, add some <strong>random</strong> , <strong>smear the TTL</strong> as much as your business logic and special effects on latency allow.  TTL smearing is required if you have natural reasons in business logic that concentrate removal at one point in time. <br><br><h2>  Sharding </h2><br>  We tried to postpone this moment, but it came - we still have to scale horizontally.  With reference to MongoDB is sharding. <br><br><blockquote>  If you doubt that you need sharding, it means you do not need it. </blockquote><br>  Sharding complicates the life of a developer and devops in a variety of ways.  In the company, we call it sharding tax.  When we shard a collection, the <strong>specific performance of the collection decreases</strong> : MongoDB needs to maintain a separate index for sharding, and you need to pass additional parameters to the query so that it can be executed more efficiently. <br><br>  Some sharding things just don't work well.  For example, a bad idea to use queries with <code>skip</code> , especially if you have a lot of documents.  You give the command: "Skip 100,000 documents." <br><br>  MongoDB thinks so: ‚ÄúThe first, second, third ... hundred thousandth, went further.  And we will return this to the user. ‚Äù <br><br>  In a non-harnessed collection, MongoDB will perform the operation somewhere within itself.  In shardirovannoy - all 100 000 documents, she really reads and transfers to the sharding proxy - in <a href="https://docs.mongodb.com/manual/reference/program/mongos/"><u>mongos</u></a> , which already on its side will somehow filter and discard the first 100 000. An unpleasant feature to be remembered. <br><br>  <strong>With sharding, the code will certainly become more complex</strong> - in many places you have to drag the sharding key.  This is not always convenient, and not always possible.  Some requests will go to either broadcast or multicast, which also does not add scalability.  Approach the choice of the key on which sharding will be more accurate. <br><br>  <strong>In shardirovannye collections the operation <code>count</code> breaks</strong> .  She begins to return a number more than in reality - can lie 2 times.  The reason lies in the balancing process, when documents are poured from one shard to another.  When the documents are transferred to the next shard, but have not yet been deleted on the source one, <code>count</code> will still <code>count</code> them.  MongoDB developers do not call this a bug - it is such a feature.  I do not know whether they will fix it or not. <br><br>  <strong>Shardirovanny cluster is much harder to administer</strong> .  Devopsy will no longer greet you, because the procedure for removing the backup becomes radically more difficult.  When sharding the need to automate the infrastructure flashes like a fire alarm - something that could have been done without earlier. <br><br><h3>  How sharding works in MongoDB </h3><br>  There is a collection, we want to somehow scatter it on shards.  To do this, <strong>MongoDB divides the collection into chunks</strong> using the sharding key, trying to divide them in shard key space into equal pieces.  Then the balancer turns on, which diligently <strong>spreads these chunks along shards in a cluster</strong> .  And the balancer doesn‚Äôt care how much these chunks weigh and how many documents are in them, since the balancing goes in chunks one by one. <br><br><h2>  Sharding key </h2><br>  Did you still decide what to shard?  Well, the first question is how to choose the sharding key.  A good key has several parameters: <strong>high cardinality</strong> , <strong>uncomfortableness</strong> and it <strong>fits well with frequent requests</strong> . <br><br>  The natural selection of the sharding key is the primary key - the id field.  If the id field is suitable for sharding, then it‚Äôs best to shard it directly.  This is an excellent choice - it has a good cardinality, it is not mutable, but how well it fits into frequent requests is your business specificity.  Start from your situation. <br><br>  I will give an example of an unsuccessful sharding key.  I have already mentioned the collection of translations.  It has a language field that stores language.  For example, the collection supports 100 languages, and we shuffle by language.  This is bad - cardinality the number of possible values ‚Äã‚Äãof only 100 pieces, which is small.  But this is not the worst - perhaps cardinality is enough for this purpose.  Worse, as soon as we step up the language, we immediately find out that we have English-speaking users 3 times more than the rest.  At the unfortunate shard, which is English, comes three times more requests than all the others combined. <br><br>  Therefore, it is necessary to take into account that sometimes the shard key naturally causes uneven distribution of the load. <br><br><h3>  Balancing </h3><br>  We come to sharding when we have a need for it - our MongoDB cluster squeaks, crunches its own disks, a processor ‚Äî everything that can be done.  Where to go?  Nowhere, and we heroically weard the five collections.  Shard, run, and suddenly find out that <strong>balancing is not free</strong> . <br><br>  Balancing goes through several stages.  The balancer selects chunks and shards, from where and where it will transfer.  Further work goes in two phases: first, the <strong>documents are copied</strong> from the source to the target, and then the documents that were copied <strong>are deleted</strong> . <br><br>  Our Shard is overloaded, all collections are in it, but the first part of the operation is easy for him.  But the second - the removal - quite unpleasant, because it puts a shard on the shoulder blades and so suffers under load. <br><br>  The problem is aggravated by the fact that if we balance a lot of chunks, for example, thousands, then with the default settings, all these chunks are first copied, and then the uninstaller comes and starts to remove them en masse.  At this point, the procedure is no longer affected, and you only have to watch what is happening. <br><br>  Therefore, if you approach to sharding an overloaded cluster, you need to plan, since <strong>balancing takes time.</strong>  It is desirable to take this time not in prime time, but during periods of low load.  Balancer - switchable spare part.  You can go to primary balancing in manual mode, turn off the balancer in prime time, and turn on when the load has decreased to allow yourself to be bigger. <br><br>  If the capabilities of the cloud still allow it to scale vertically, then it is better to improve the shard source in advance for the gland so that all these special effects can be slightly reduced. <br><br>  <b>For sharding you need to carefully prepare.</b> <br><br><blockquote>  <a href="https://www.highload.ru/siberia/2019">HighLoad ++ Siberia 2019</a> will come in Novosibirsk on June 24 and 25.  HighLoad ++ Siberia is an opportunity for developers from Siberia to listen to the reports, to talk on the high-topic and to plunge into the environment ‚Äúwhere are everyone‚Äù, without flying three thousand kilometers to Moscow or St. Petersburg.  Of the 80 applications, the Program Committee approved 25, and we tell about all other changes in the program, announcements of reports and other news in our newsletter.  <a href="http://eepurl.com/VYVaf">Subscribe</a> to be informed. </blockquote></div><p>Source: <a href="https://habr.com/ru/post/454748/">https://habr.com/ru/post/454748/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../454730/index.html">Allure Server meetup: video reports</a></li>
<li><a href="../454734/index.html">Backup, part 4: zbackup, restic, borgbackup review and testing</a></li>
<li><a href="../454738/index.html">Visual Studio 2019 support in PVS-Studio</a></li>
<li><a href="../454742/index.html">At least one trick of Vim, about which you did not know</a></li>
<li><a href="../454744/index.html">Overview of Java track reports from the RigaDevDays conference</a></li>
<li><a href="../45475/index.html">PostgreSQL horizontal scaling using PL / Proxy.</a></li>
<li><a href="../454750/index.html">Swift UI - at a gallop across Europe</a></li>
<li><a href="../454754/index.html">When is it worth testing the hypothesis of no less effectiveness?</a></li>
<li><a href="../454756/index.html">Checking the effectiveness of the site and advertising settings, the cost of attracting customers wholesale company</a></li>
<li><a href="../454758/index.html">Bypassing Windows Defender cheap and cheerful: Mimikatz obfuscation</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>