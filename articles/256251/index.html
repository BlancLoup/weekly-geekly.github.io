<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>We optimize step by step with the compiler Intel C ++</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Each developer sooner or later faces the problem of optimizing their application, and you want to do it with minimal effort and maximum profit in term...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>We optimize step by step with the compiler Intel C ++</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/files/ab9/b0c/9cc/ab9b0c9ccddf462d83221880b38dfde4.png"></div><br><br>  Each developer sooner or later faces the problem of optimizing their application, and you want to do it with minimal effort and maximum profit in terms of performance.  In this question, the compiler comes to the rescue, which today can do a lot of things automatically, you just need to tell him about it with the help of keys.  Compilation options, as well as types of optimization, divorced quite a lot, so I decided to write a blog about step-by-step optimization of the application using the Intel compiler. <br><br>  So, the whole thorny path of compiling and optimizing our application can be divided into 7 steps.  Let's go! <br><a name="habracut"></a><br>  <b>Step 1. Will we compile the code without any optimization at all?</b> <br>  That's right, in the first step we would like to answer this question.  I often start the optimization process by turning off everything and everything in the compiler.  Why?  Well, first, I want to make sure that my code works correctly without any interference from the compiler and its ingenious transformations.  I disable optimizations with the <i>-O0</i> key (on Windows <i>/ Od</i> ), collect the code and launch the application.  Yes, and debugging non-optimized code is easier. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>Step 2. What can there be "easier" to connect?</b> <br>  We start with the "basic" options. <br><br>  <i>-O1 / -Os</i> <br>  The first, basic level of optimization, in which the compiler does not auto-vectorize ‚Äî that is, it does not even try.  At the same time, an analysis of the data flow, moving the code, reducing the cost of operations, analyzing the lifetime of variables, planning the execution of commands are performed.  Often used to limit the size of our application, somewhat cutting back on optimization.  If the O1 option is enabled, then Os is also implicitly enabled. <br><br>  <i>-O2</i> <br>  The optimization level that is enabled by default, based on the speed of the application.  Starting from this level, the cycle vectorization is enabled.  In addition, a series of basic optimizations are performed with cycles, in-lining, IP (Intra-file interprocedural) optimization, and more. <br><br>  <i>-O3</i> <br>  At this maximum level of optimizations, in addition to what was done on O2, a number of more aggressive transformations are included with cycles, for example, unrolling the outer loop and fusion the inner ones, dividing the cycles into blocks (blocking), combining the IF conditions .  A very good overview of the optimizations themselves is presented <a href="http://habrahabr.ru/post/124910/">here</a> .  If it is critical for your application to save numerical results (for example, scientific calculations), then you need to be careful in using this option.  Often, the numbers ‚Äúfloat‚Äù and you need to limit the optimization, returning to <i>-O2</i> and using the <i>-fp-model</i> options.  In general, after compiling with <i>-O2,</i> no one limits us to try <i>-O3</i> and just see what happens.  In theory, the application should work faster. <br><br>  <i>-no-prec-div</i> <br>  A division operation conforming to the IEEE standard is very labor intensive.  You can sacrifice some accuracy in calculations, but speed up calculations using this option, in which the compiler, for example, will replace <i>A / B</i> expressions with <i>A * (1 / B)</i> . <br><br>  <i>-ansi-alias</i> <br>  This option tells the compiler that we adhere to strict aliasing rules when writing our code in accordance with the ISO C standard. If you follow these rules, when dereferencing pointers to objects of different types, we will never turn to the same memory location. which gives more room to the compiler to perform optimizations.  For a detailed description of aliasing, you can read <a href="http://habrahabr.ru/post/114117/">this</a> article. <br>  It is important to note that starting with the Intel compiler version 15.0 (Intel Parallel Studio XE 2015 Composer Edition) and above, this option is enabled by default, but if we write on earlier versions, we don‚Äôt forget about it. <br><br>  <b>Step 3. We use the specifics of "iron"</b> <br>  You can use the <i>-x'code '</i> option to connect optimizations specific to Intel processors.  It tells the compiler what processor capabilities can be used, including a set of instructions that can be generated.  As <i>'code'</i> you can set <i>SSE2, SSE3, SSSE3, SSE3_ATOM and SSSE3_ATOM, ATOM_SSSE3, ATOM_SSE4.2, SSE4.1, SSE4.2, AVX, CORE-AVX-I, CORE-AVX2, CORE-AVX512, MIC-AVX512 , COMMON-AVX512</i> . <br>  It is clear that the resulting application can be run only on systems with Intel processors that support the generated instructions. <br>  By default, the <i>-xSSE2</i> key is <i>used</i> , which, for example, during vectorization will tell the compiler to use <i>SSE2</i> instructions.  In most cases (Pentium 4 and higher), this guarantees the execution of the application. <br>  If we write under Atom and know for sure that the application will only run on it, then for better performance we can use <i>-xSSSE3_ATOM</i> .  For the Silvermont architecture, you must specify <i>-xATOM_SSE4.2</i> . <br>  Particularly lazy can use the option <i>-xHost</i> , and in this case, the optimization will be done under the iron on which we collect the code. <br><br>  By the way, it is possible to specify not only one specific set of instructions, but several at once - using the <i>-ax'code '</i> key. <br>  At the same time, the auto-selector (dispatcher) will be added to the code, which will be determined by the CPU (by <i>CPUID</i> ) during the launch of the application, and depending on which instruction set it supports, it will go along the desired path.  Of course, this leads to an increase in the size of our application, but it gives more flexibility.  In addition to the explicitly specified set of instructions via <i>'code'</i> , the default version SSE2 is also always created.  For example, specifying <i>-axAVX</i> , we get one default version with SSE2, as well as a separate version for AVX. <br>  In addition, we can specify several sets of instructions in the <i>-ax</i> option, <i>separated</i> by commas.  For example, <i>-axSSE4.2, AVX</i> will tell the compiler to generate versions of SSE4.2, AVX, and do not forget about the default (SSE2) branch, which will always be.  It can also be explicitly set using the <i>-x</i> <i>option</i> in addition to <i>-ax</i> .  For example, specifying the <i>-axSSE4.2</i> keys <i>, AVX-xSSE4.1, the</i> default version will be SSE4.1. <br><br>  For optimizations that are not specific to Intel processors, the <i>-m option is used</i> . <br>  For example, for the Quark SoC X1000, you can specify the <i>-mia32</i> options (generate code for the IA-32 architecture) and <i>-falign-stack = assume-4-byte</i> , which lets you tell the compiler that our stack is 4 bytes aligned.  If necessary, the compiler will be able to align it to 16 bytes.  This may reduce the size of the data required for calling functions. <br><br>  <b>Step 4. IPO</b> <br>  No, we are not going to sell shares on the stock exchange yet.  IPO (Interprocedural Optimization) is an interprocedural analysis and optimization that the compiler does over our code.  It is connected with the <i>-ipo</i> option and allows optimizations not for one single source file, but for all sources at the same time.  In this case, the compiler knows much more and can make much more conclusions and, accordingly, transformations / optimizations.  All the intricacies of the IPO will help to understand <a href="http://habrahabr.ru/company/intel/blog/199112/">this</a> blog.  The specificity of the work lies in the fact that when compiling with <i>-ipo, the</i> usual order of compilation and linking changes, and the object file contains a packed internal representation, so the standard (on Linux) linker <i>ld</i> and the <i>ar</i> utility should be replaced with Intel <i>xiar</i> and <i>xild</i> .  Do not forget that the process of compiling with an IPO itself may take significantly more time, especially for "large" applications. <br><br>  <b>Step 5. Or maybe "profile"?</b> <br>  Nothing can give the compiler more information than running the application itself.  Thanks to him, we can find out exactly which branches we went to, where we spent more time, how often we did not get into the cache, and so on.  Naturally, I conclude that profiling our application can significantly help its optimization. <br>  The compiler has such an option that allows you to profile our application and optimize based on the collected data - PGO (Profile-guided Optimization). <br>  The process of work consists of several steps, and, accordingly, compiler keys. <br><br>  First of all, we need to perform the instrumentation of our application, having collected it with the <i>-prof-gen</i> key.  Next, you need to run our application, while collecting various statistical data (profile) into a separate information file with the .dyn extension.  Well, in the end, use this data during the final compilation with the <i>-prof-use</i> key, in which the compiler will try to optimize the most expensive computationally-intensive code branches. <br><br>  In some cases, you may need to specify a place where to put the same files with the results of the application.  This can be done with the option <i>-prof-dir = 'val'</i> , specifying the path to the folder.  Thus, we can build our code on one machine, then profile it on another, and perform the final compilation again on the first one.  Just take the dyn files and put them in a daddy on the system, where we collect the code, and specify the path through <i>-prof-dir</i> . <br><br>  In order for the profile to be compiled, the application must normally complete and exit. <br>  If our application runs infinitely (for example, a frequent case for embedded systems), you will have to make a couple more gestures: <br>  1. Add an exit point from the application. <br>  2. Add a <i>PGO API call _PGOPTI_Prof_Dump_All ()</i> <br>  3. You can control the dump interval in microseconds through environment variables: <br>  <i>export INTEL_PROF_DUMP_INTERVAL 5000</i> <i><br></i>  <i>export INTEL_PROF_DUMP_CUMULATIVE 1</i> <br><br>  <b>Step 6. Vector games</b> <br>  Specifically decided to focus on vectorization, although it is enabled by default (with the <i>-O2</i> option), and the set of instructions is controlled by the ones already described <i>-x</i> , <i>-ax</i> , etc.  But when talking about the performance of the Intel compiler, it is on vectorization that you need to pay special attention, because it gives the maximum increase in the speed of the application.  We read the corresponding <a href="http://habrahabr.ru/company/intel/blog/205552/">post</a> on how to help the compiler in its hard work.  Well, a set of updated options <i>-opr-report</i> will help. <br><br>  <b>Step 7. Parallel automatically!</b> <br>  The Intel compiler has the most interesting option -parallel, which allows parallelizing loops using OpenMP using compiler tools in automatic mode.  Obviously, not all cycles are equally well parallelized, and the compiler cannot always do this.  But try this option is worth it - from this, we are unlikely to lose something. <br><br>  As a result, here is a set of options worth trying when compiling your code to increase performance: <br><br>  <i>-O2 / O3 -no-prec-div -x'code '-ipo -prof-gen / -prof-use -prof-dir =' val '-parallel</i> <br><br>  By the way, for the lazy they came up with the <i>-fast</i> option, which includes most of these keys: <i>ipo, -O3, -no-prec-div, -static, -fp-model fast = 2, and -xHost</i> . <br>  Well, apart from the options, a good Intel¬Æ Vtune Amplifier XE profiler will always help us, but that's another story. <br><br>  <b>Practice</b> <br>  In addition to theoretical reflections, I want to play around with the listed options on an example of calculating the number of Pi and show how they affect the speed of the application, albeit unpretentious.  In the ‚Äútheory‚Äù I gave the keys for Linux, in the case of Windows they are almost identical, the letter Q is added at the beginning (in most cases).  I will collect an example on Windows to show the corresponding options.  I used the Intel C ++ compiler version 15.0 (15.0.2.179 Build 20150121). <br><br>  So, the code I deliberately broke into two files (so that was the effect of the IPO). <br>  pi.c: <br><br><pre><code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">define</span></span></span><span class="hljs-meta"> N 1000000000 double f( double x ); main() { double sum, pi, x, h; clock_t start, stop; int i; h = (double)1.0/(double)N; sum = 0.0; start = clock(); for ( i=0; i&lt;N ; i++ ){ x = h*(i-0.5); sum = sum + f(x); } stop = clock(); </span><span class="hljs-comment"><span class="hljs-meta"><span class="hljs-comment">// print value of pi to be sure multiplication is correct pi = h*sum; printf(" pi is approximately : %f \n", pi); // print elapsed time printf("Elapsed time = %lf seconds\n",((double)(stop - start)) / CLOCKS_PER_SEC); }</span></span></span></span></code> </pre> <br><br>  In a separate file fx.c, the function f is defined: <br><br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">double</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">f</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">double</span></span></span></span><span class="hljs-function"><span class="hljs-params"> x)</span></span></span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> ret; ret = <span class="hljs-number"><span class="hljs-number">4.0</span></span> / (x*x + <span class="hljs-number"><span class="hljs-number">1.0</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> ret; }</code> </pre><br>  Inklud <i>stdio</i> libraries and <i>time</i> I did not give. <br>  So, we will collect this code with different options and look at the resulting acceleration. <br>  To begin with, we compile without optimizations: <br><br><pre> <code class="cpp hljs">icl /Od pi.c fx.c /o Od_pi.exe</code> </pre><br>  And run Od_pi.exe: <br><br><pre> <code class="cpp hljs">pi is approximately : <span class="hljs-number"><span class="hljs-number">3.141593</span></span> Elapsed time = <span class="hljs-number"><span class="hljs-number">22.828000</span></span> seconds</code> </pre><br>  Something long, let's see what the next level O1 gives: <br><br><pre> <code class="cpp hljs">icl /O1 pi.c fx.c /o O1_pi.exe pi is approximately : <span class="hljs-number"><span class="hljs-number">3.141593</span></span> Elapsed time = <span class="hljs-number"><span class="hljs-number">4.963000</span></span> seconds</code> </pre><br>  Interestingly, by increasing the optimization level to O2 and O3, we no longer gain any speed. <br>  This is quite logical, because the code is quite simple, and besides, it was not vectorized due to a function call defined in another file inside the loop.  So an IPO should help: <br><br><pre> <code class="cpp hljs">icl /O2 pi.c fx.c /Qipo ipo_pi.exe pi is approximately : <span class="hljs-number"><span class="hljs-number">3.141593</span></span> Elapsed time = <span class="hljs-number"><span class="hljs-number">2.562000</span></span> seconds</code> </pre><br>  At the same time, our cycle was vectorized.  If we collect the code without an IPO, but with the keys <i>QxAVX</i> , <i>QxSSE2</i> and others from the same series, we will not notice any difference in speed.  Again it is quite logical, since vectorization will not work: <br><br><pre> <code class="cpp hljs">icl /O2 /QxAVX pi.c fx.c /o xAVX_pi.exe Elapsed time = <span class="hljs-number"><span class="hljs-number">5.065000</span></span> seconds icl /O2 /QxSSE2 pi.c fx.c /o xSSE2_pi.exe Elapsed time = <span class="hljs-number"><span class="hljs-number">5.093000</span></span> seconds</code> </pre><br>  I compile the code and run the application on Haswell, so I use the <i>/ QxHost</i> option and IPO: <br><br><pre> <code class="cpp hljs">icl /O2 /QxHost /Qipo pi.c fx.c /o xHost_ipo_pi.exe Elapsed time = <span class="hljs-number"><span class="hljs-number">2.718000</span></span> seconds</code> </pre><br>  The <i>/ fast</i> option gives the same result: <br><br><pre> <code class="cpp hljs">icl /fast /Qvec-report2 pi.c fx.c /o fast_pi.exe Elapsed time = <span class="hljs-number"><span class="hljs-number">2.718000</span></span> seconds</code> </pre><br>  Profiling allows you to squeeze a little more in terms of optimization: <br><br><pre> <code class="cpp hljs">icl /Qprof-gen pi.c fx.c /o pgen_pi.exe</code> </pre><br>  Run the application, and then compile again: <br><br><pre> <code class="cpp hljs">icl /Qprof-use /O2 /Qipo pi.c fx.c /o puse_pi.exe Elapsed time = <span class="hljs-number"><span class="hljs-number">2.578000</span></span> seconds</code> </pre><br>  Well, we get the most with auto-paralleling: <br><br><pre> <code class="cpp hljs">icl /Qparallel /Qpar-report2 /Qvec-report2 /Qipo pi.c fx.c /o par_ipo_pi.exe Elapsed time = <span class="hljs-number"><span class="hljs-number">1.447000</span></span> seconds</code> </pre><br>  This is how we accelerated significantly, without putting much effort into it.  Simple game options, so to speak. </div><p>Source: <a href="https://habr.com/ru/post/256251/">https://habr.com/ru/post/256251/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../256231/index.html">Does disk space speed up your computer?</a></li>
<li><a href="../256233/index.html">The law on "personal data" will lead to an increase in the volume of the data center market in Russia?</a></li>
<li><a href="../256239/index.html">Auto-tuning IP phones in 3CX - just plug and play</a></li>
<li><a href="../256245/index.html">What they say 100 million letters: Complete instructions for working with email newsletters</a></li>
<li><a href="../256247/index.html">Another project on the ESP8266 and a water meter</a></li>
<li><a href="../256253/index.html">HP low-cost servers for SMB and providers</a></li>
<li><a href="../256255/index.html">In the departments of Google Adsense, were you a little bit shorter with adidas?</a></li>
<li><a href="../256257/index.html">We unify the behavior of LINQ to IEnumerable and LINQ to IQueriable in terms of working with null values. ExpressionVisitor Example</a></li>
<li><a href="../256261/index.html">Manual to help you</a></li>
<li><a href="../256263/index.html">Internet in Russian schools: work on the bugs</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>