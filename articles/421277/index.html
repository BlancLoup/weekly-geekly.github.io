<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Segmentation of satellite images on the example of tree recognition</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Automatic recognition of satellite or aerial images is the most promising way to obtain information about the location of various objects on the groun...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Segmentation of satellite images on the example of tree recognition</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/29/oa/ku/29oakuyg7l3gw-gqre-xvdxr3s0.jpeg" alt="image"><br><br>  Automatic recognition of satellite or aerial images is the most promising way to obtain information about the location of various objects on the ground.  The refusal of manual segmentation of images is especially relevant when it comes to processing large areas of the earth's surface in a short time. <br><br>  Recently, I had the opportunity to apply theoretical skills and try myself in the field of machine learning on a real project of image segmentation.  The goal of the project is the recognition of forest plantations, namely tree crowns on high resolution satellite images.  Under the cut, I will share the experience and results. <br><a name="habracut"></a><br>  When it comes to image processing, then segmentation can be given the following definition - it is the finding on the image of characteristic areas that are equally described in a given feature space. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      There are luminance, contour, texture and semantic segmentation. <br><br>  Semantic (or semantic) segmentation of images consists in selecting areas on the image, each of which corresponds to a specific attribute.  In general, semantic segmentation tasks are difficult to algorithmize, therefore, for the segmentation of images, convolutional neural networks are now widely used, which show good results. <br><br><h3>  Formulation of the problem </h3><br>  The problem of binary segmentation is being solved - color images (high resolution satellite imagery) are fed to the input of the neural network, in which areas of pixels belonging to the same class - trees are selected. <br><br><h3>  Initial data </h3><br>  In my possession there was a set of tiles of satellite images of a rectangular area in which the polygon fits.  Inside it and need to look for trees.  Polygon or multipolygon represented as a GeoJSON file.  In my case, the tiles were in png format of 256 by 256 pixels in true color.  (alas, no ik) Numbering of tiles in the form of /zoom/x/y.png. <br><br>  It is guaranteed that all the tiles in the set are obtained from satellite images taken at about the same time of the year (late spring - early fall depending on the climate of a particular region) and days at a similar angle to the surface, where the presence of slight scattered clouds was allowed. <br><br><h3>  Data preparation </h3><br>  Since the area of ‚Äã‚Äãthe desired polygon may be less than this rectangular area, the first thing to do is to exclude those tiles that go beyond the boundaries of the polygon.  For this, a simple script was written that selects the necessary tiles from the GeoJSON file using the polygon.  It works as follows.  To begin with, the coordinates of all the vertices of the polygon are <a href="https://wiki.openstreetmap.org/wiki/Slippy_map_tilenames">converted</a> to tile numbers and added to an array.  There is also an offset relative to the origin.  For visual control, an image is generated, where one pixel is equal to one tile.  The polygon on the image is already filled with the offset using the PIL.  After that, the image is transferred to the array, from where the necessary tiles are selected, which fall inside the polygon. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> PIL <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Image, ImageDraw <span class="hljs-comment"><span class="hljs-comment"># . . . #             . img = Image.new("L", (x, y), 0) draw = ImageDraw.Draw(img) #    .     . points ‚Äî  . draw.polygon(points, fill=255) img.show() mask = numpy.array(img) # . . .</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/-m/mk/ai/-mmkai6lmgfie8hnipom9rgjvxk.jpeg"><br>  <i>Visual result of converting a polygon into a set of tiles</i> <br><br><h3>  Network model </h3><br>  To solve problems of image segmentation, <a href="https://github.com/mrgloom/awesome-semantic-segmentation">there are</a> a number of models of convolutional neural networks.  I decided to use <a href="https://arxiv.org/pdf/1505.04597.pdf">U-Net</a> , which has proven itself in the problems of binary image segmentation.  The U-Net architecture consists of the so-called contracting and an expansive path, which are connected by forwarding at appropriate stages, and first reduce the resolution of the image, and then increase it by first merging it with the image data and passing it through other layers convolutions.  Thus, the network serves as a kind of filter.  The compressing and unclamping blocks are presented as a set of blocks of a certain dimension.  And each block consists of basic operations: convolution, ReLu and max pooling.  There are implementations of the U-Net model on Keras, Tensorflow, Caffe and PyTorch.  I used Keras. <br><br><h3>  Creating a training set </h3><br>  Images are required for learning this Unet model.  The first thing in my head was the idea to take the data of OpenStreetMap and generate masks for training based on them.  But as it turned out in my case, the accuracy of the polygons I need leaves much to be desired.  I also needed the presence of single trees, which are not always mapped.  Therefore, such an undertaking had to be abandoned.  But it should be said, for other objects, such as roads or buildings, this approach can be <a href="https://hackathon2018.neurohive.io/projects/raspoznovanie-izobrazheniy-na-sputnikovykh-snimkakh/">effective</a> . <br><br><img src="https://habrastorage.org/webt/rc/po/v0/rcpov0qaluuosozh7isz8z-ya1y.png"><br><br>  Since the idea of ‚Äã‚Äãautomatically generating a training sample based on OSM data had to be abandoned, I decided to manually mark out a small area of ‚Äã‚Äãterrain.  For this, I used the JOSM editor, where as a substrate I used the available images of the terrain, which I placed on a local server.  Then another problem surfaced - I could not find an opportunity to turn on the display of the grid of tiles by regular JOSM tools.  Therefore, with a couple of simple lines in .htaccess on the same server from another directory, I started issuing an empty pixel-frame tile to any grid-type / z / x / y.png request and added such an improvised layer to JOSM.  Such is the bike. <br><br><img src="https://habrastorage.org/webt/xe/ir/za/xeirzah3mp-kqtbpp0vvayzxhyi.png"><br><br>  At first I marked out about 30 tiles.  With a graphics tablet and ‚Äúfast drawing mode‚Äù in JOSM, it didn't take long.  I understood that this amount is not enough for full-fledged training, but I decided to start with this.  Moreover, training on such a large amount of data will pass fairly quickly. <br><br><h3>  Training and the first result </h3><br>  The network has been trained for 15 epochs without prior augmentation of data.  The graph shows the values ‚Äã‚Äãof losses and accuracy on the test sample: <br><br><img src="https://habrastorage.org/webt/ln/h-/pk/lnh-pkjwqz-eziqh4gafem781og.png"><br><br>  The result of image recognition, which was neither in the training nor in the test sample, turned out to be quite sane: <br><br> <a href=""><img src="https://habrastorage.org/webt/ao/kr/ed/aokredzarfej2cczep8ir7facx4.png"></a> <br><br>  After a more thorough study of the results, some problems emerged.  A lot of blunders were in the shadow areas of the pictures - the network either found the trees in the shadow where they were not, or exactly the opposite.  This was expected, since there were few such examples in the training sample.  But I did not expect that some pieces of the water surface and dark metal-roofed roofs (presumably) will be recognized as trees, I did not expect.  There were also inaccuracies with lawns.  It was decided to improve the sample by adding more images with controversial areas to it, so the learning sample was almost doubled. <br><br><h3>  Data augmentation </h3><br>  Then I decided to increase the amount of data by turning the images at an arbitrary angle.  First of all, I tried the standard keras.preprocessing.image.ImageDataGenerator module.  When rotated while maintaining the scale at the edges of the images, empty areas remain, the fill of which is set by the <i>fill_mode</i> parameter.  You can simply fill these areas with color, indicating it in <i>cval</i> , but I wanted a full-fledged turn, hoping that the sample would be more complete, and implemented the generator myself.  This allowed to increase the size of more than ten times. <br><br><img src="https://habrastorage.org/webt/oa/i9/hr/oai9hrvaxhigjgwp5xatijyht5u.png"><br>  <i>fill_mode = nearest</i> <br><br>  My data generator sticks four adjacent tiles into one source, 512x512 px in size.  The angle of rotation is chosen randomly, taking into account it, the admissible intervals for x and y for the center of the resulting tile are calculated, being in which it will not go beyond the limits of the original tile.  The coordinates of the center are chosen randomly, taking into account the allowable intervals.  Of course, all these transformations are applied to a pair of tile mask.  All this is repeated for different groups of neighboring tiles.  With one group, you can get more than a dozen tiles with different parts of the area turned at different angles. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#       # image ‚Äî  , center (x, y) ‚Äî   , a ‚Äî   , width  height ‚Äî   . shape = image.shape[:2] matrix = cv2.getRotationMatrix2D( center=center, angle=a, scale=1 ) image = cv2.warpAffine( src=image, M=matrix, dsize=shape ) x = int( center[0] - width/2 ) y = int( center[1] - height/2 ) image = image[ y:y+height, x:x+width ] # </span></span></code> </pre><br><img src="https://habrastorage.org/webt/zy/mo/po/zymopoykrdt5ezki8ojru1te-di.png"><br>  <i>An example of the result of the generator</i> <br><br><h3>  Learning with more data </h3><br>  As a result, the size of the training sample was 1881 images, I also increased the number of epochs to 30: <br><br><img src="https://habrastorage.org/webt/yh/ja/k9/yhjak9qcoyqb1lr64hhbet9d8sc.png"><br><br>  After training the model on the new data volume, problems with erroneous segmentation of roofs and water were no longer detected.  It was not possible to get rid of errors in the shadows at all, but their eyes became less, as well as errors with lawns.  It should be noted that, in general, the overwhelming majority of errors consist in the fact that the network sees the trees where they are not, and not vice versa.  The achieved accuracy can be improved by using satellite images with a large number of channels and modification of the network architecture for a specific task. <br><br><img src="https://habrastorage.org/webt/er/i-/bk/eri-bkryiex4pidpmej4ybr_hyk.png"><br><br>  In general, I was satisfied with the result of the work done, and the trained network prototype was used to solve real-world problems.  For example, counting the density of forest plantations in Moscow: <br><br> <a href=""><img src="https://habrastorage.org/webt/3_/7a/dr/3_7adrfwb3ouixjq35wtn2lim7c.png"></a> </div><p>Source: <a href="https://habr.com/ru/post/421277/">https://habr.com/ru/post/421277/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../421265/index.html">Monsters after the holidays: AMD Threadripper 2990WX 32-Core and 2950X 16-Core (part 2)</a></li>
<li><a href="../421267/index.html">Semi-passive cooling of computer power supply unit</a></li>
<li><a href="../421269/index.html">The facial recognition system installed at a US airport helped catch the attacker</a></li>
<li><a href="../421271/index.html">Selection of useful materials on Azure. Part 2 - Courses</a></li>
<li><a href="../421275/index.html">What I realized after selling two startups in 12 months</a></li>
<li><a href="../421279/index.html">Additional security software for NAS</a></li>
<li><a href="../421281/index.html">Technical support 3CX responds: setting your own logo on the IP phone display</a></li>
<li><a href="../421283/index.html">The book about the "Paragraph" on Habr√©. Chapter One: The Watchman</a></li>
<li><a href="../421285/index.html">Optical trackers: ASEF and MOSSE</a></li>
<li><a href="../421287/index.html">Moon bricks from the solar furnace</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>