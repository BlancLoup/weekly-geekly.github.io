<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Performance analysis of the drive Intel Optane SSD 750GB</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Last summer, we published an article about Intel Optane SSD disk drives and invited everyone to take part in free testing . The novelty caused great i...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Performance analysis of the drive Intel Optane SSD 750GB</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/jp/qi/sy/jpqisybjwlcvos9qsznopmdc-nm.png"><br><br>  Last summer, we published <a href="https://blog.selectel.ru/intel-optane-ssd%3Futm_source%3Dblog_selectel_ru%26amp%3Butm_medium%3Dinner_referral" rel="noopener noreferrer">an article</a> about Intel Optane SSD disk drives and invited everyone to take part in <a href="https://selectel.ru/promo/intel-optane%3Futm_source%3Dblog_selectel_ru%26amp%3Butm_medium%3Dinner_referral" rel="noopener noreferrer">free testing</a> .  The novelty caused great interest: our users tried to use Optane for <a href="https://blog.selectel.ru/intel-optane-ssd-application-in-science%3Futm_source%3Dblog_selectel_ru%26amp%3Butm_medium%3Dinner_referral" rel="noopener noreferrer">scientific calculations</a> , for <a href="https://blog.selectel.ru/in-memory-database-with-intel-optane%3Futm_source%3Dblog_selectel_ru%26amp%3Butm_medium%3Dinner_referral" rel="noopener noreferrer">working with in-memory databases</a> , for projects in the field of machine learning. <br><a name="habracut"></a><br>  We ourselves have long been going to write a detailed review, but all did not reach out.  But more recently, a suitable opportunity appeared: colleagues from Intel provided us with a new <a href="https://www.intel.com/content/www/us/en/products/memory-storage/solid-state-drives/data-center-ssds/optane-dc-p4800x-series/p4800x-750gb-aic.html" rel="noopener">750 GB capacity</a> for testing.  The results of our experiments will be discussed below. <br><br><h2>  Intel Optane P4800X 750GB: General Information and Specifications </h2><br>  Intel Optane SSDs are available in a 20nm process.  It exists in two form factors: in the form of a map (HHHL (CEM3.0) - for details on what it is, see <a href="https://nvmexpress.org/new-pcie-form-factor-enables-greater-pcie-ssd-adoption" rel="nofollow noopener noreferrer">here</a> ) and U.2 15 mm. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      We have a disk in the form of a card: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/026/7be/89f/0267be89f07c01be5b9bdc3b54ebdc06.jpg" width="100%" height="100%"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/436/758/c3e/436758c3e7b2f9bc5e47f4d98d2b5c3e.jpg" width="100%" height="100%"><br><br>  It is visible in the BIOS and is detected by the system without installing any drivers and additional programs (we give an example for Ubuntu 16.04 OS): <br><br><pre><code class="bash hljs">$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 149.1G 0 disk ‚îú‚îÄsda2 8:2 0 1K 0 part ‚îú‚îÄsda5 8:5 0 148.1G 0 part ‚îÇ ‚îú‚îÄvg0-swap_1 253:1 0 4.8G 0 lvm [SWAP] ‚îÇ ‚îî‚îÄvg0-root 253:0 0 143.3G 0 lvm / ‚îî‚îÄsda1 8:1 0 976M 0 part /boot nvme0n1 259:0 0 698.7G 0 disk</code> </pre> <br>  More information can be viewed using the nvme-cli utility (it is included in the repositories of most modern Linux distributions, but in a very outdated version, so we recommend building a fresh version from the <a href="https://github.com/linux-nvme/nvme-cli" rel="nofollow noopener noreferrer">source code</a> ). <br><br>  But its main technical characteristics (taken from the <a href="https://ark.intel.com/products/97154/Intel-Optane-SSD-DC-P4800X-Series-750GB-2_5in-PCIe-x4-20nm-3D-XPoint" rel="nofollow noopener noreferrer">official site of Intel</a> ): <br><br><table><tbody><tr><td>  <b>Characteristic</b> </td><td>  <b>Value</b> </td></tr><tr><td>  Volume </td><td>  750GB </td></tr><tr><td>  Performance when performing sequential read operations, MB / s </td><td>  2500 </td></tr><tr><td>  Performance when performing sequential write operations, MB / s </td><td>  2200 </td></tr><tr><td>  Performance when performing random read operations, IOPS </td><td>  550000 </td></tr><tr><td>  Performance when performing random write operations, IOPS </td><td>  550000 </td></tr><tr><td>  Delay in performing a read operation (latency) </td><td>  10 ¬µs </td></tr><tr><td>  Delay in writing operation </td><td>  10 ¬µs </td></tr><tr><td>  Wear resistance, PBW * </td><td>  41.0 </td></tr></tbody></table><br>  <i>* PBW - short for Petabytes written.</i>  <i>This characteristic indicates the amount of information that can be written to disk during the entire life cycle.</i> <br><br>  At first glance, everything looks very impressive.  But the numbers given in the marketing materials, many (and not without reason) used to not trust.  Therefore, it will not be superfluous to check them, and also to conduct some additional experiments. <br><br>  We start with a fairly simple synthetic tests, and then conduct tests under conditions as close as possible to actual practice. <br><br><h2>  Testbed Configuration </h2><br>  Colleagues from Intel (for which many thanks to them) provided us with a server with the following technical characteristics: <br><br><ul><li>  motherboard - Intel R2208WFTZS; </li><li>  processor - Intel Xeon Gold 6154 (24.75M Cache, 3.00 GHz); </li><li>  memory - 192GB DDR4; </li><li>  Intel SSD DC S3510 (the OS was installed on this disk); </li><li>  Intel Optane ‚Ñ¢ SSD DC P4800X 750GB. </li></ul><br>  The server was running OS Ubuntu 16.04 with kernel 4.13. <br><br>  <b>Note!</b>  <b>To get good performance NVMe-drives, you need a kernel version of at least 4.10.</b>  <b>With earlier kernel versions, the results will be worse: NVMe support in them is not properly implemented.</b> <br><br>  For the tests we used the following software: <br><br><ul><li>  <a href="https://github.com/axboe/fio" rel="nofollow noopener noreferrer">fio</a> utility, which is the de facto standard for measuring disk performance; </li><li>  diagnostic tools developed by <a href="http://www.brendangregg.com/" rel="nofollow noopener noreferrer">Brendan Gregg</a> as part of the <a href="https://www.iovisor.org/" rel="nofollow noopener noreferrer">iovisor</a> project; </li><li>  <a href="" rel="nofollow noopener noreferrer">db_bench</a> utility created in Facebook and used to measure performance in the <a href="http://rocksdb.org/" rel="nofollow noopener noreferrer">rocksdb</a> data <a href="http://rocksdb.org/" rel="nofollow noopener noreferrer">warehouse</a> . </li></ul><br><h2>  Synthetic tests </h2><br>  As mentioned above, we first consider the results of synthetic tests.  We performed them using the fio utility version 3.3.31, which we collected from the <a href="https://github.com/axboe/fio" rel="nofollow noopener noreferrer">source code</a> . <br><br>  In accordance with the methodology adopted by us, the following load profiles were used in the tests: <br><br><ul><li>  random write / read in 4 KB blocks, queue depth - 1; </li><li>  random write / read in 4 KB blocks, queue depth - 16; </li><li>  random write / read in 4 M blocks, queue depth - 32; </li><li>  random write / read in 4 KB blocks, queue depth - 128. </li></ul><br>  Here is an example of the configuration file: <br><br><pre> <code class="bash hljs">[readtest] blocksize=4M filename=/dev/nvme0n1 rw=randread direct=1 buffered=0 ioengine=libaio iodepth=32 runtime=1200 [writetest] blocksize=4M filename=/dev/nvme0n1 rw=randwrite direct=1 buffered=0 ioengine=libaio iodepth=32</code> </pre><br>  Each test was performed for 20 minutes;  upon completion, we entered all the indicators of interest to us in the table (see below). <br><br>  The greatest interest for us will be represented by such a parameter as the number of I / O operations per second (IOPS).  In tests for reading and writing blocks of 4M, the size of the bandwidth is entered in the table. <br><br>  For clarity, we present the results not only for Optane, but also for other NVMe drives: this is the Intel P 4510, as well as a disk from another manufacturer - <a href="https://www.micron.com/~/media/documents/products/product-flyer/9200_ssd_product_brief.pdf" rel="noopener">Micron</a> : <br><br><table><tbody><tr><td>  <b>Disc model</b> </td><td>  <b>Disk capacity, GB</b> </td><td>  <b>randread</b> <br>  <b>4k</b> <br>  <b>iodepth</b> <br>  <b>= 128</b> </td><td>  <b>randwrite</b> <br>  <b>4k</b> <br>  <b>iodepth</b> <br>  <b>= 128</b> </td><td>  <b>randread</b> <br>  <b>4M</b> <br>  <b>iodepth</b> <br>  <b>= 32</b> </td><td>  <b>randwrite</b> <br>  <b>4M</b> <br>  <b>iodepth</b> <br>  <b>= 32</b> </td><td>  <b>randread</b> <br>  <b>4k</b> <br>  <b>iodepth</b> <br>  <b>= 1</b> </td><td>  <b>randwrite</b> <br>  <b>4k</b> <br>  <b>iodepth</b> <br>  <b>= 16</b> </td><td>  <b>randread</b> <br>  <b>4k</b> <br>  <b>iodepth</b> <br>  <b>= 1</b> </td><td>  <b>randwrite</b> <br>  <b>4k</b> <br>  <b>iodepth</b> <br>  <b>= 1</b> </td></tr><tr><td>  Intel P4800 X </td><td>  750 GB </td><td>  400k </td><td>  324k </td><td>  2663 </td><td>  2382 </td><td>  399k </td><td>  362k </td><td>  373k </td><td>  76.1k </td></tr><tr><td>  Intel P4510 </td><td>  1 TB </td><td>  335k </td><td>  179k </td><td>  2340 </td><td>  504 </td><td>  142k </td><td>  143k </td><td>  12.3k </td><td>  73.5k </td></tr><tr><td>  Micron MTFDHA <br>  X1T6MCE </td><td>  1.6 TB </td><td>  387k </td><td>  201k </td><td>  2933 </td><td>  754 </td><td>  80.6k </td><td>  146k </td><td>  8425 </td><td>  27.4k </td></tr></tbody></table><br>  As you can see, in some tests, Optane shows numbers that are several times higher than the results of similar tests for other drives. <br><br>  But in order to make more or less objective judgments about disk performance, the amount of IOPS alone is not enough.  This parameter in itself means nothing apart from the connection with the other - latency. <br><br>  <strong>Latency</strong> is the amount of time during which an I / O operation request is sent, sent by an application.  It is measured using the same fio utility.  Upon completion of all tests, it issues the following output to the console (here is a small fragment): <br><br><pre> <code class="bash hljs">Jobs: 1 (f=1): [w(1),_(11)][100.0%][r=0KiB/s,w=953MiB/s][r=0,w=244k IOPS][eta 00m:00s] writers: (groupid=0, <span class="hljs-built_in"><span class="hljs-built_in">jobs</span></span>=1): err= 0: pid=14699: Thu Dec 14 11:04:48 2017 write: IOPS=46.8k, BW=183MiB/s (192MB/s)(699GiB/3916803msec) slat (nsec): min=1159, max=12044k, avg=2379.65, stdev=3040.91 clat (usec): min=7, max=12122, avg=168.32, stdev=98.13 lat (usec): min=11, max=12126, avg=170.75, stdev=97.11 clat percentiles (usec): | 1.00th=[ 29], 5.00th=[ 30], 10.00th=[ 40], 20.00th=[ 47], | 30.00th=[ 137], 40.00th=[ 143], 50.00th=[ 151], 60.00th=[ 169], | 70.00th=[ 253], 80.00th=[ 281], 90.00th=[ 302], 95.00th=[ 326], | 99.00th=[ 363], 99.50th=[ 379], 99.90th=[ 412], 99.95th=[ 429], | 99.99th=[ 457]</code> </pre><br>  Note the following snippet: <br><br><pre> <code class="hljs swift">slat (nsec): <span class="hljs-built_in"><span class="hljs-built_in">min</span></span>=<span class="hljs-number"><span class="hljs-number">1159</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">max</span></span>=12044k, avg=<span class="hljs-number"><span class="hljs-number">2379.65</span></span>, stdev=<span class="hljs-number"><span class="hljs-number">3040.91</span></span> clat (usec): <span class="hljs-built_in"><span class="hljs-built_in">min</span></span>=<span class="hljs-number"><span class="hljs-number">7</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">max</span></span>=<span class="hljs-number"><span class="hljs-number">12122</span></span>, avg=<span class="hljs-number"><span class="hljs-number">168.32</span></span>, stdev=<span class="hljs-number"><span class="hljs-number">98.13</span></span> lat (usec): <span class="hljs-built_in"><span class="hljs-built_in">min</span></span>=<span class="hljs-number"><span class="hljs-number">11</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">max</span></span>=<span class="hljs-number"><span class="hljs-number">12126</span></span>, avg=<span class="hljs-number"><span class="hljs-number">170.75</span></span>, stdev=<span class="hljs-number"><span class="hljs-number">97.11</span></span></code> </pre><br>  These are the latency values ‚Äã‚Äãthat we obtained during the test.  Of greatest interest to us <br><br>  Slat is the time to send a request (i.e., a parameter that is related to the performance of the I / O subsystem in Linux, but not the disk), and clat is the so-called complete latency, i.e.  time of the request received from the device (this is the parameter that interests us).  How to analyze these figures is well written in <a href="https://habrahabr.ru/post/154235/" rel="nofollow noopener noreferrer">this article</a> , published five years ago, but it‚Äôs still relevant. <br><br>  <strong>Fio</strong> is a generally accepted and well-proven utility, but sometimes in real practice there are situations when you need to get more accurate information about the delay time and identify possible causes if this indicator is too high.  Tools for more accurate diagnostics are developed as part of the <a href="https://www.iovisor.org/" rel="nofollow noopener noreferrer">iovisor</a> project (see also the <a href="https://github.com/iovisor" rel="nofollow noopener noreferrer">repository on GitHub</a> . All these tools are based on the <a href="http://prototype-kernel.readthedocs.io/en/latest/bpf/" rel="nofollow noopener noreferrer">eBPF</a> mechanism <a href="http://prototype-kernel.readthedocs.io/en/latest/bpf/" rel="nofollow noopener noreferrer">(extended Berkeley Packet Filters</a> . In our tests, we tried the biosnoop utility (see the source code <a href="https://github.com/iovisor/bcc/blob/master/tools/biosnoop.py" rel="nofollow noopener noreferrer">here</a> ). It tracks everything I / O operations in the system and measures the delay time for each of them. <br><br>  This is very useful if there are problems with the performance of a disk to which a large number of read and write requests are being executed (for example, a disk contains a database for some high-load web project). <br><br>  We started with the simplest option: we ran the standard fio tests and measured the latency for each operation using biosnoop, which was run in a different terminal.  While running, biosnoop writes the following table to standard output: <br><br><pre> <code class="bash hljs">TIME(s) COMM PID DISK T SECTOR BYTES LAT(ms) 300.271456000 fio 34161 nvme0n1 W 963474808 4096 0.01 300.271473000 fio 34161 nvme0n1 W 1861294368 4096 0.01 300.271491000 fio 34161 nvme0n1 W 715773904 4096 0.01 300.271508000 fio 34161 nvme0n1 W 1330778528 4096 0.01 300.271526000 fio 34161 nvme0n1 W 162922568 4096 0.01 300.271543000 fio 34161 nvme0n1 W 1291408728 4096 0.01</code> </pre><br>  This table consists of 8 columns: <br><br><ul><li>  <strong>TIME</strong> - the time of the operation in the Unix Timestamp format; </li><li>  <strong>COMM</strong> is the name of the process that performed the operation; </li><li>  <strong>PID</strong> - PID of the process that performed the operation; </li><li>  <strong>T</strong> - type of operation (R - read, W - write); </li><li>  <strong>SECTOR</strong> - the sector where the recording was carried out; </li><li>  <strong>BYTES</strong> - the size of the recorded block; </li><li>  <strong>LAT (ms)</strong> is the delay time for the operation. </li></ul><br>  We carried out many measurements for different disks and paid attention to the following: for Optane during the whole test (and the test duration varied from 20 minutes to 4 hours) the value of the latency parameter remained unchanged and corresponded to the value stated in the table above 10 ¬µs, while while other drives have fluctuations. <br><br>  According to the results of synthetic tests, it is quite possible to assume that Optane and under high load will show good performance and most importantly - low latency.  Therefore, we decided not to stop at pure ‚Äúsynthetics‚Äù and conduct a test with real (or at least as close as possible to real) loads. <br><br>  To do this, we used the performance measurement utilities that make up RocksDB, an interesting and increasingly popular ‚Äúkey-value‚Äù repository developed on Facebook.  Below we describe in detail the tests performed and analyze their results. <br><br><h2>  Optane and RocksDB: Performance Tests </h2><br><h3>  Why RocksDB </h3><br>  In recent years, the widespread sharply increased need for fault-tolerant storage of large amounts of data.  They are used in various areas: social networks, corporate information systems, instant messengers, cloud storage and others.  Software solutions for such storages, as a rule, are built on the basis of so-called <a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree" rel="nofollow noopener noreferrer">LSM-trees</a> - for example, Big Table, HBase, Cassandra, LevelDB, Riak, MongoDB, InfluxDB.  Working with them involves serious workloads, including on the disk subsystem - see, for example, <a href="https://www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf" rel="nofollow noopener noreferrer">here</a> .  Optane with all its durability and durability could be an appropriate solution. <br><br>  <strong>RocksdDB</strong> (see also the <a href="https://github.com/facebook/rocksdb" rel="nofollow noopener noreferrer">repository on GitHub</a> ) is a ‚Äúkey-value‚Äù repository developed by Facebook and a fork of the notorious <a href="http://leveldb.org/" rel="nofollow noopener noreferrer">LevelDB</a> project.  It is used to solve a wide range of tasks: from organizing the <a href="http://myrocks.io/" rel="nofollow noopener noreferrer">storage engine for MySQL</a> to caching application data. <br><br>  We chose it for our tests, guided by the following considerations: <br><br><ul><li>  RocksDB is positioned as a storage <b>designed specifically for fast drives, including NVMe</b> ; </li><li>  RocksDB is successfully used in high-load Facebook projects; </li><li>  RocksDB includes interesting testing utilities that create a very serious load (see below for details); </li><li>  Finally, we were just curious to see how the Optane with its reliability and stability would withstand heavy loads. </li></ul><br>  All tests described below were carried out on two disks: <br><br><ul><li>  Intel Optane SSD 750 GB </li><li>  Micron MTFDHAX1T6MCE </li></ul><br><h3>  Preparation for testing: compiling RocksDB and creating a base </h3><br>  We compiled RocksDB from source code published on GitHub (here and below are examples of commands for Ubuntu 16.04): <br><br><pre> <code class="bash hljs">$ sudo apt-get install libgflags-dev libsnappy-dev zlib1g-dev libbz2-dev liblz4-dev libzstd-dev gcc g++ clang make git $ git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/facebook/rocksdb/ $ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> rocksdb $ make all</code> </pre><br>  After installation, you need to prepare for the test disk, which will be recorded data. <br>  The official documentation with RocksDB recommends using the XFS file system, which we will create on our Optane: <br><br><pre> <code class="bash hljs">$ sudo apt-get install xfsprogs $ mkfs.xfs -f /dev/nvme0n1 $ mkdir /mnt/rocksdb $ mount -t xfs /dev/nvme0n1 /mnt/rocksdb</code> </pre><br>  This preparatory work is completed, and you can proceed to create a database. <br>  RocksDB is not a DBMS in the classical sense of the word, and in order to create a database, you will need to write a small C or C ++ program.  Examples of such programs (1 and 2) are available in the official RecksDB repository in the examples directory.  The source code will need to make some changes and specify the correct path to the database.  In our case, it looks like this: <br><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> rockdb/examples $ vi simple_example.cc</code> </pre><br>  In this file you need to find the line <br><br><pre> <code class="hljs cpp"><span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::<span class="hljs-built_in"><span class="hljs-built_in">string</span></span> kDBPath =<span class="hljs-string"><span class="hljs-string">"/tmp/rocksdb_simple_example"</span></span></code> </pre><br>  And register in it the path to our database: <br><br><pre> <code class="hljs cpp"><span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::<span class="hljs-built_in"><span class="hljs-built_in">string</span></span> kDBPath =<span class="hljs-string"><span class="hljs-string">"/mnt/rocksdb/testdb1"</span></span></code> </pre><br>  After that, you need to proceed to compile: <br><br><pre> <code class="hljs ruby">$ make $ ./simple_example</code> </pre><br>  As a result of this command, a database will be created in the specified directory.  We will write to it (and read from it) the data in our tests.  We will test using the db_bench utility;  The corresponding binary file is in the RocksDB root directory. <br><br>  The testing methodology is described in detail on the <a href="https://github.com/facebook/rocksdb/wiki/Performance-Benchmarks" rel="nofollow noopener noreferrer">official wiki-page of the project</a> . <br><br>  If you carefully read the text on the link, you will see that the meaning of the test is to write one billion keys to the database (and then read the data from this database).  The total amount of all data is about 800 GB.  We can afford it: the volume of our Optane is only 750 GB.  Therefore, we have reduced the number of keys in our test by exactly half: not one billion, but 500 million.  To demonstrate the capabilities of Optane and this figure is quite enough. <br><br>  In our case, the amount of recorded data will be approximately 350 GB. <br><br>  All these data are stored in <a href="https://github.com/facebook/rocksdb/wiki/A-Tutorial-of-RocksDB-SST-formats" rel="nofollow noopener noreferrer">SST</a> format (short for <a href="https://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/" rel="noopener">Sorted String Table</a> ; see also <a href="http://mezhov.com/2013/09/sstable-lsm-tree.html" rel="noopener">this article</a> ).  At the output we will get several thousand so-called SST-files (you can read more <a href="https://github.com/facebook/rocksdb/wiki/Creating-and-Ingesting-SST-files" rel="nofollow noopener noreferrer">here</a> . <br><br>  Before running the test, it is necessary to increase the limit on the number of simultaneously open files on the system, otherwise it will not work: approximately 15-20 minutes after the test starts, we will see the message Too many open files. <br><br>  So that everything goes as it should, run the ulimit command with the n option: <br><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">ulimit</span></span> -n</code> </pre><br>  By default, the system has a limit of 1024 files.  To avoid problems, we will immediately increase it to a million: <br><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">ulimit</span></span> -n 1000000</code> </pre><br>  Please note: after a reboot, this limit is not saved and returns to its default value. <br><br>  That's all, the preparatory work is completed.  We proceed to the description of the tests and analysis of the results. <br><br><h2>  Test description </h2><br><h3>  Introductory notes </h3><br>  Based on the methodology described by the link above, we conducted the following tests: <br><br><ul><li>  bulk key loading in sequential order; </li><li>  bulk loading of keys in random order; </li><li>  random entry; </li><li>  random read. </li></ul><br>  All tests were performed using the db_bench utility, the source code of which can be found in <a href="https://github.com/facebook/rocksdb" rel="nofollow noopener noreferrer">the rocksdb repository</a> . <br><br>  The size of each key is 10 bytes, and the size of the value is 800 bytes. <br>  Consider the results of each test in more detail. <br><br><h3>  Test 1. Bulk upload keys in sequential order. </h3><br>  To run this test, we used the same parameters as indicated in the instructions for the link above.  We only changed the number of keys to be written (we already mentioned this): not 1,000,000,000, but 500,000,000. <br><br>  At the very beginning the base is empty;  it is filled during the test.  No data reading is performed during data loading. <br><br>  The db_bench command to run the test looks like this: <br><br><pre> <code class="bash hljs">bpl=10485760;mcz=2;del=300000000;levels=6;ctrig=4; delay=8; stop=12; wbn=3; \ mbc=20; mb=67108864;wbs=134217728; sync=0; r=50000000 t=1; vs=800; \ bs=4096; cs=1048576; of=500000; si=1000000; \ ./db_bench \ --benchmarks=fillseq --disable_seek_compaction=1 --mmap_read=0 \ --statistics=1 --histogram=1 --num=<span class="hljs-variable"><span class="hljs-variable">$r</span></span> --threads=<span class="hljs-variable"><span class="hljs-variable">$t</span></span> --value_size=<span class="hljs-variable"><span class="hljs-variable">$vs</span></span> \ --block_size=<span class="hljs-variable"><span class="hljs-variable">$bs</span></span> --cache_size=<span class="hljs-variable"><span class="hljs-variable">$cs</span></span> --bloom_bits=10 --cache_numshardbits=6 \ --open_files=<span class="hljs-variable"><span class="hljs-variable">$of</span></span> --verify_checksum=1 --sync=<span class="hljs-variable"><span class="hljs-variable">$sync</span></span> --disable_wal=1 \ --compression_type=none --stats_interval=<span class="hljs-variable"><span class="hljs-variable">$si</span></span> --compression_ratio=0.5 \ --write_buffer_size=<span class="hljs-variable"><span class="hljs-variable">$wbs</span></span> --target_file_size_base=<span class="hljs-variable"><span class="hljs-variable">$mb</span></span> \ --max_write_buffer_number=<span class="hljs-variable"><span class="hljs-variable">$wbn</span></span> --max_background_compactions=<span class="hljs-variable"><span class="hljs-variable">$mbc</span></span> \ --level0_file_num_compaction_trigger=<span class="hljs-variable"><span class="hljs-variable">$ctrig</span></span> \ --level0_slowdown_writes_trigger=<span class="hljs-variable"><span class="hljs-variable">$delay</span></span> \ --level0_stop_writes_trigger=<span class="hljs-variable"><span class="hljs-variable">$stop</span></span> --num_levels=<span class="hljs-variable"><span class="hljs-variable">$levels</span></span> \ --delete_obsolete_files_period_micros=<span class="hljs-variable"><span class="hljs-variable">$del</span></span> --min_level_to_compress=<span class="hljs-variable"><span class="hljs-variable">$mcz</span></span> \ --stats_per_interval=1 --max_bytes_for_level_base=<span class="hljs-variable"><span class="hljs-variable">$bpl</span></span> \ --use_existing_db=0 --db=/mnt/rocksdb/testdb</code> </pre><br>  The command contains many options that just need to be commented: they will be used in subsequent tests.  At the very beginning we set the values ‚Äã‚Äãof important parameters: <br><br><ul><li>  bpl - the maximum number of bytes per level; </li><li>  mcz - the minimum level of compression; </li><li>  del - time interval after which you want to delete obsolete files; </li><li>  levels - the number of levels; </li><li>  ctrig - the number of files after which you want to start compression; </li><li>  delay - the time after which you want to slow down the recording speed; </li><li>  stop - the time after which you want to stop recording; </li><li>  wbn - maximum number of write buffers; </li><li>  mbc is the maximum number of background compression; </li><li>  mb is the maximum number of write buffers; </li><li>  wbs - write buffer size; </li><li>  sync - enable / disable synchronization; </li><li>  r is the number of key-value pairs that will be recorded in the database; </li><li>  t is the number of threads; </li><li>  vs - the value value; </li><li>  bs - block size; </li><li>  cs - cache size; </li><li>  f - the number of open files (does not work, see the comment about this above); </li><li>  si is the frequency of statistics collection. </li></ul><br>  You can read more about the other parameters by running the command <br><br><pre> <code class="bash hljs">./db_bench --<span class="hljs-built_in"><span class="hljs-built_in">help</span></span></code> </pre><br>  Detailed descriptions of all options are also provided <a href="https://github.com/facebook/rocksdb/wiki/Benchmarking-tools" rel="nofollow noopener noreferrer">here</a> . <br><br>  What results did the test show?  The sequential load operation was completed in <b>23 minutes</b> .  The write speed was <b>536.78 MB / s</b> . <br>  For comparison: on the Micron NVMe drive, the same procedure takes a little more than <b>30 minutes</b> , and the write speed is <b>380.31 MB / s</b> . <br><br><h3>  Test 2. Bulk loading of keys in random order </h3><br>  To test random recording, the following db_bench settings were used (here‚Äôs the full listing of the command): <br><br><pre> <code class="bash hljs">bpl=10485760;mcz=2;del=300000000;levels=2;ctrig=10000000; delay=10000000; stop=10000000; wbn=30; mbc=20; \ mb=1073741824;wbs=268435456; sync=0; r=50000000; t=1; vs=800; bs=65536; cs=1048576; of=500000; si=1000000; \ ./db_bench \ --benchmarks=fillrandom --disable_seek_compaction=1 --mmap_read=0 --statistics=1 --histogram=1 \ --num=<span class="hljs-variable"><span class="hljs-variable">$r</span></span> --threads=<span class="hljs-variable"><span class="hljs-variable">$t</span></span> --value_size=<span class="hljs-variable"><span class="hljs-variable">$vs</span></span> --block_size=<span class="hljs-variable"><span class="hljs-variable">$bs</span></span> --cache_size=<span class="hljs-variable"><span class="hljs-variable">$cs</span></span> --bloom_bits=10 \ --cache_numshardbits=4 --open_files=<span class="hljs-variable"><span class="hljs-variable">$of</span></span> --verify_checksum=1 \ --sync=<span class="hljs-variable"><span class="hljs-variable">$sync</span></span> --disable_wal=1 --compression_type=zlib --stats_interval=<span class="hljs-variable"><span class="hljs-variable">$si</span></span> --compression_ratio=0.5 \ --write_buffer_size=<span class="hljs-variable"><span class="hljs-variable">$wbs</span></span> --target_file_size_base=<span class="hljs-variable"><span class="hljs-variable">$mb</span></span> --max_write_buffer_number=<span class="hljs-variable"><span class="hljs-variable">$wbn</span></span> \ --max_background_compactions=<span class="hljs-variable"><span class="hljs-variable">$mbc</span></span> --level0_file_num_compaction_trigger=<span class="hljs-variable"><span class="hljs-variable">$ctrig</span></span> \ --level0_slowdown_writes_trigger=<span class="hljs-variable"><span class="hljs-variable">$delay</span></span> --level0_stop_writes_trigger=<span class="hljs-variable"><span class="hljs-variable">$stop</span></span> --num_levels=<span class="hljs-variable"><span class="hljs-variable">$levels</span></span> \ --delete_obsolete_files_period_micros=<span class="hljs-variable"><span class="hljs-variable">$del</span></span> --min_level_to_compress=<span class="hljs-variable"><span class="hljs-variable">$mcz</span></span> \ --stats_per_interval=1 --max_bytes_for_level_base=<span class="hljs-variable"><span class="hljs-variable">$bpl</span></span> --memtablerep=vector --use_existing_db=0 \ --disable_auto_compactions=1 --allow_concurrent_memtable_write=<span class="hljs-literal"><span class="hljs-literal">false</span></span> --db=/mnt/rocksdb/testb1</code> </pre><br>  This test took us <b>1 hour and 6 minutes to complete</b> , and the write speed was 273.36 MB / s.  On the Microne, the test is performed in <b>3 hours and 30 minutes</b> , and the recording speed fluctuates: the average value is <b>49.7 MB / s</b> . <br><br><h3>  Test 3. Random Record </h3><br>  In this test, we tried to overwrite 500 million keys into the previously created database. <br><br>  Here is the full listing of the db_bench command: <br><br><pre> <code class="bash hljs">bpl=10485760;mcz=2;del=300000000;levels=6;ctrig=4; delay=8; stop=12; wbn=3; \ mbc=20; mb=67108864;wbs=134217728; sync=0; r=500000000; t=1; vs=800; \ bs=65536; cs=1048576; of=500000; si=1000000; \ ./db_bench \ --benchmarks=overwrite --disable_seek_compaction=1 --mmap_read=0 --statistics=1 \ --histogram=1 --num=<span class="hljs-variable"><span class="hljs-variable">$r</span></span> --threads=<span class="hljs-variable"><span class="hljs-variable">$t</span></span> --value_size=<span class="hljs-variable"><span class="hljs-variable">$vs</span></span> --block_size=<span class="hljs-variable"><span class="hljs-variable">$bs</span></span> \ --cache_size=<span class="hljs-variable"><span class="hljs-variable">$cs</span></span> --bloom_bits=10 --cache_numshardbits=4 --open_files=<span class="hljs-variable"><span class="hljs-variable">$of</span></span> \ --verify_checksum=1 --sync=<span class="hljs-variable"><span class="hljs-variable">$sync</span></span> --disable_wal=1 \ --compression_type=zlib --stats_interval=<span class="hljs-variable"><span class="hljs-variable">$si</span></span> --compression_ratio=0.5 \ --write_buffer_size=<span class="hljs-variable"><span class="hljs-variable">$wbs</span></span> --target_file_size_base=<span class="hljs-variable"><span class="hljs-variable">$mb</span></span> --max_write_buffer_number=<span class="hljs-variable"><span class="hljs-variable">$wbn</span></span> \ --max_background_compactions=<span class="hljs-variable"><span class="hljs-variable">$mbc</span></span> --level0_file_num_compaction_trigger=<span class="hljs-variable"><span class="hljs-variable">$ctrig</span></span> \ --level0_slowdown_writes_trigger=<span class="hljs-variable"><span class="hljs-variable">$delay</span></span> --level0_stop_writes_trigger=<span class="hljs-variable"><span class="hljs-variable">$stop</span></span> \ --num_levels=<span class="hljs-variable"><span class="hljs-variable">$levels</span></span> --delete_obsolete_files_period_micros=<span class="hljs-variable"><span class="hljs-variable">$del</span></span> \ --min_level_to_compress=<span class="hljs-variable"><span class="hljs-variable">$mcz</span></span> --stats_per_interval=1 \ --max_bytes_for_level_base=<span class="hljs-variable"><span class="hljs-variable">$bpl</span></span> --use_existing_db=/mnt/rocksdb/testdb</code> </pre><br>  In this test, a very good result was obtained: <b>2 hours 51 minutes</b> at a speed of <b>49 MB / s</b> (at the time it was reduced to <b>38 MB / s</b> ). <br><br>  On Microne, the test takes a little more - <b>3 hours and 16 minutes</b> ;  the speed is about the same, but the vibrations are more pronounced. <br><br><h3>  Test 4. Random reading </h3><br>  The meaning of this test is to randomly read 500,000,000 keys from the database.  Here is a complete listing of the db_bench command with all the options: <br><br><pre> <code class="bash hljs">bpl=10485760;mcz=2;del=300000000;levels=6;ctrig=4; delay=8; stop=12; wbn=3; \ mbc=20; mb=67108864;wbs=134217728; sync=0; r=500000000; t=1; vs=800; \ bs=4096; cs=1048576; of=500000; si=1000000; \ ./db_bench \ --benchmarks=fillseq --disable_seek_compaction=1 --mmap_read=0 \ --statistics=1 --histogram=1 --num=<span class="hljs-variable"><span class="hljs-variable">$r</span></span> --threads=<span class="hljs-variable"><span class="hljs-variable">$t</span></span> --value_size=<span class="hljs-variable"><span class="hljs-variable">$vs</span></span> \ --block_size=<span class="hljs-variable"><span class="hljs-variable">$bs</span></span> --cache_size=<span class="hljs-variable"><span class="hljs-variable">$cs</span></span> --bloom_bits=10 --cache_numshardbits=6 \ --open_files=<span class="hljs-variable"><span class="hljs-variable">$of</span></span> --verify_checksum=1 --sync=<span class="hljs-variable"><span class="hljs-variable">$sync</span></span> --disable_wal=1 \ --compression_type=none --stats_interval=<span class="hljs-variable"><span class="hljs-variable">$si</span></span> --compression_ratio=0.5 \ --write_buffer_size=<span class="hljs-variable"><span class="hljs-variable">$wbs</span></span> --target_file_size_base=<span class="hljs-variable"><span class="hljs-variable">$mb</span></span> \ --max_write_buffer_number=<span class="hljs-variable"><span class="hljs-variable">$wbn</span></span> --max_background_compactions=<span class="hljs-variable"><span class="hljs-variable">$mbc</span></span> \ --level0_file_num_compaction_trigger=<span class="hljs-variable"><span class="hljs-variable">$ctrig</span></span> \ --level0_slowdown_writes_trigger=<span class="hljs-variable"><span class="hljs-variable">$delay</span></span> \ --level0_stop_writes_trigger=<span class="hljs-variable"><span class="hljs-variable">$stop</span></span> --num_levels=<span class="hljs-variable"><span class="hljs-variable">$levels</span></span> \ --delete_obsolete_files_period_micros=<span class="hljs-variable"><span class="hljs-variable">$del</span></span> --min_level_to_compress=<span class="hljs-variable"><span class="hljs-variable">$mcz</span></span> \ --stats_per_interval=1 --max_bytes_for_level_base=<span class="hljs-variable"><span class="hljs-variable">$bpl</span></span> \ --use_existing_db=0 bpl=10485760;overlap=10;mcz=2;del=300000000;levels=6;ctrig=4; delay=8; \ stop=12; wbn=3; mbc=20; mb=67108864;wbs=134217728; sync=0; r=500000000; \ t=32; vs=800; bs=4096; cs=1048576; of=500000; si=1000000; \ ./db_bench \ --benchmarks=readrandom --disable_seek_compaction=1 --mmap_read=0 \ --statistics=1 --histogram=1 --num=<span class="hljs-variable"><span class="hljs-variable">$r</span></span> --threads=<span class="hljs-variable"><span class="hljs-variable">$t</span></span> --value_size=<span class="hljs-variable"><span class="hljs-variable">$vs</span></span> \ --block_size=<span class="hljs-variable"><span class="hljs-variable">$bs</span></span> --cache_size=<span class="hljs-variable"><span class="hljs-variable">$cs</span></span> --bloom_bits=10 --cache_numshardbits=6 \ --open_files=<span class="hljs-variable"><span class="hljs-variable">$of</span></span> --verify_checksum=1 --sync=<span class="hljs-variable"><span class="hljs-variable">$sync</span></span> --disable_wal=1 \ --compression_type=none --stats_interval=<span class="hljs-variable"><span class="hljs-variable">$si</span></span> --compression_ratio=0.5 \ --write_buffer_size=<span class="hljs-variable"><span class="hljs-variable">$wbs</span></span> --target_file_size_base=<span class="hljs-variable"><span class="hljs-variable">$mb</span></span> \ --max_write_buffer_number=<span class="hljs-variable"><span class="hljs-variable">$wbn</span></span> --max_background_compactions=<span class="hljs-variable"><span class="hljs-variable">$mbc</span></span> \ --level0_file_num_compaction_trigger=<span class="hljs-variable"><span class="hljs-variable">$ctrig</span></span> \ --level0_slowdown_writes_trigger=<span class="hljs-variable"><span class="hljs-variable">$delay</span></span> \ --level0_stop_writes_trigger=<span class="hljs-variable"><span class="hljs-variable">$stop</span></span> --num_levels=<span class="hljs-variable"><span class="hljs-variable">$levels</span></span> \ --delete_obsolete_files_period_micros=<span class="hljs-variable"><span class="hljs-variable">$del</span></span> --min_level_to_compress=<span class="hljs-variable"><span class="hljs-variable">$mcz</span></span> \ --stats_per_interval=1 --max_bytes_for_level_base=<span class="hljs-variable"><span class="hljs-variable">$bpl</span></span> \ --use_existing_db=1</code> </pre><br>     ,      :      ,    .     db_bench          . <br><br>     32 .        . <br><br>  Optane    <b>5  2 </b> ,  Microne ‚Äî  <b>6 </b> . <br><br><h2>  Conclusion </h2><br>         Intel Optane SSD  750 .   ,      .         ,      .      Intel        . <br><br>    ,  Optane          .  But it is better to see once than hear a hundred times.      Optane     .   :       ,           . <br><br>       Optane   ,       <a href="https://selectel.ru/services/dedicated/%3Futm_source%3Dblog_selectel_ru%26amp%3Butm_medium%3Dinner_referral" rel="noopener noreferrer">     -</a> . <br><br>          <a href="https://www.intel.com/content/www/us/en/software/intel-memory-drive-technology.html" rel="noopener">IMDT (Intel Memory Drive)</a> .    ,    .           . </div><p>Source: <a href="https://habr.com/ru/post/352622/">https://habr.com/ru/post/352622/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../352612/index.html">Virtual server on VPS.house: performance review</a></li>
<li><a href="../352614/index.html">How to solve 90% of NLP tasks: a step-by-step guide to natural language processing</a></li>
<li><a href="../352616/index.html">Ansible is not so simple</a></li>
<li><a href="../352618/index.html">How to switch to microservices and not break production</a></li>
<li><a href="../352620/index.html">We open the history of the Bolshoi Theater. Part one</a></li>
<li><a href="../352624/index.html">The Metrix has you ...</a></li>
<li><a href="../352626/index.html">Storage options for cryptographic keys</a></li>
<li><a href="../352628/index.html">Rumors about the cancellation of the Kotelnikov theorem are greatly exaggerated</a></li>
<li><a href="../352630/index.html">As Google Adwords experts helped me to throw away 150,000 UAH (about $ 6000) per month or why I will not ...</a></li>
<li><a href="../352632/index.html">True implementation of a neural network from scratch. Part 2. Recognition of numbers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>