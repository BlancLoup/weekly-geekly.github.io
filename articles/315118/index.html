<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The implementation of the classification of text convolutional network on keras</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Speaking, strangely enough, it will be about a text classifier using a convolutional network (vectorization of individual words is another question). ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The implementation of the classification of text convolutional network on keras</h1><div class="post__text post__text-html js-mediator-article">  Speaking, strangely enough, it will be about a text classifier using a convolutional network (vectorization of individual words is another question).  <a href="https://bitbucket.org/alex43210/pynlc">The code, test data and examples of their use</a> are on bitbucket (rested on the size limits from github and the proposal to use Git Large File Storage (LFS), until he mastered the proposed solution). <br><br><h2>  Datasets </h2><br>  Converted sets were used: <a href="http://www.daviddlewis.com/resources/testcollections/reuters21578/">reuters - 22000 records</a> , <a href="">watson-th - 530 records</a> , <a href="">and 1 more watson-th - 50 records</a> .  <i>By the way, I would not give up on typing in the comments / drugs (but better still in the comments) typing in Russian.</i> <br><br><h2>  Network device </h2><br>  Based on one implementation <a href="">of the network described here</a> .  <a href="https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras">The implementation code used on github</a> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In my case, there are word vectors at the input of the network (the gensim implementation of word2vec is used).  The network structure is shown below: <br><br><img src="https://habrastorage.org/files/e15/8e9/267/e158e9267d2a4399847a516972773ad0.png"><br>  In short: <br><br><ul><li>  The text is represented as a matrix of the form word_count x word_vector_size.  The vectors of individual words are from word2vec, about which you can read, for example, <a href="https://habrahabr.ru/post/253227/">in this post</a> .  Since I don‚Äôt know in advance what kind of text the user will slip - I take a length of 2 * N, where N is the number of vectors in the longest text of the training set.  Yes, he jabbed his fingers into the sky. </li><li>  The matrix is ‚Äã‚Äãprocessed by convolutional sections of the network (at the output we obtain the transformed signs of the word) </li><li>  Selected features are processed by a fully connected network section. </li></ul><br>  I stop the words from being prefiltered (this did not affect the reuter of the dataset, but in smaller sets it had an effect).  About this below. <br><a name="habracut"></a><br><h2>  Installing the necessary software (keras / theano, cuda) in Windows </h2><br>  Installation for linux was significantly easier.  Required: <br><br><ul><li>  python3.5 </li><li>  python header files (python-dev in debian) </li><li>  gcc </li><li>  cuda </li><li>  Python libraries are the same as in the list below. </li></ul><br>  In my case with win10 x64, the approximate sequence was as follows: <br><br><ul><li>  <a href="https://www.continuum.io/downloads">Anaconda with python3.5</a> . </li><li>  <a href="https://developer.nvidia.com/cuda-downloads">Cuda 8.0</a> .  You can also run on the CPU (then gcc is enough and the next 4 steps are not needed), but on relatively large datasets, the drop in speed should be significant (not tested). </li><li>  The path to nvcc is added to PATH (otherwise theano will not detect it). </li><li>  Visual Studio 2015 with C ++, including windows 10 kit (corecrt.h is required). </li><li>  The path to cl.exe is added to the PATH. </li><li>  The path to corecrt.h in INCLUDE (in my case - C: \ Program Files (x86) \ Windows Kits \ 10 \ Include \ 10.0.10240.0 \ ucrt). </li><li> <code>conda install mingw libpython</code> - gcc and libpython will be required when compiling the mesh. </li><li>  well, <code>pip install keras theano python-levenshtein gensim nltk</code> (maybe it was <code>pip install keras theano python-levenshtein gensim nltk</code> with replacing keras th backend from theano with tensorflow, but I haven‚Äôt checked it). </li><li>  The .theanorc flag is the following flag for gcc: <br><br><pre> <code class="hljs cs"> [<span class="hljs-meta"><span class="hljs-meta">gcc</span></span>] cxxflags = -D_hypot=hypot</code> </pre><br></li><li>  Run python and execute <br><br><pre> <code class="hljs haskell"> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> nltk nltk.download()</code> </pre><br></li></ul><br><h3>  Word processing </h3><br>  At this stage, the removal of stop words that are not included in the combination from the ‚Äúwhite list‚Äù (about it further) and the vectorization of the remaining ones occurs.  Input data for the algorithm used: <br><br><ul><li>  language - nltk is required for tokenization and return of the list of stopwords </li><li>  "White list" of combinations of words in which stopwords are used.  For example - ‚Äúon‚Äù is related to stop words, but [‚Äúturn‚Äù, ‚Äúon‚Äù] is another matter. </li><li>  word2vec vectors </li></ul><br>  Well, the algorithm (I see at least 2 possible improvements, but not mastered): <br><br><ul><li>  I split the input text into tokens ntlk.tokenize-m (conditionally - ‚ÄúHello, world!‚Äù Is converted to [‚Äúhello‚Äù, ",", "world", "!"]). </li><li>  I throw away tokens that are not in the word2vec-m dictionary. <br><br>  In fact - which are not there and select a similar distance did not work.  So far only Levenshtein distance, there is an idea to filter tokens with the smallest Levenshtein distance by distance from their vectors to vectors included in the training set. <br><br></li><li>  Select tokens: <br><br><ul><li>  which are not in the list of stopwords (reduced the error on the weather dataset, but without the next step - it spoiled the result very much on the ‚Äúcar_intents‚Äù -th). <br><br></li><li>  if the token in the list of stop words - check the entry in the text of sequences from the white list in which it is (conditionally, if ‚Äúon‚Äù is found, check the presence of sequences from the list [[‚Äúturn‚Äù, ‚Äúon‚Äù]]).  If there is one, add it.  There is something to improve - now I check (in our example) the presence of ‚Äúturn‚Äù, but it may not apply to this ‚Äúon‚Äù. </li></ul><br></li><li>  Replace selected tokens with their vectors. </li></ul><br><h2>  Code us </h2><br><div class="spoiler">  <b class="spoiler_title">Actually, the code that I evaluated the impact of changes</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> itertools <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> json <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> gensim.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Word2Vec <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pynlc.test_data <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> reuters_classes, word2vec, car_classes, weather_classes <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pynlc.text_classifier <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> TextClassifier <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pynlc.text_processor <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> TextProcessor <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mean_squared_error <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">classification_demo</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data_path, train_before, test_before, train_epochs, test_labels_path, instantiated_test_labels_path, trained_path)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(data_path, <span class="hljs-string"><span class="hljs-string">'r'</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> data_source: data = json.load(data_source) texts = [item[<span class="hljs-string"><span class="hljs-string">"text"</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> item <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> data] class_names = [item[<span class="hljs-string"><span class="hljs-string">"classes"</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> item <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> data] train_texts = texts[:train_before] train_classes = class_names[:train_before] test_texts = texts[train_before:test_before] test_classes = class_names[train_before:test_before] text_processor = TextProcessor(<span class="hljs-string"><span class="hljs-string">"english"</span></span>, [[<span class="hljs-string"><span class="hljs-string">"turn"</span></span>, <span class="hljs-string"><span class="hljs-string">"on"</span></span>], [<span class="hljs-string"><span class="hljs-string">"turn"</span></span>, <span class="hljs-string"><span class="hljs-string">"off"</span></span>]], Word2Vec.load_word2vec_format(word2vec)) classifier = TextClassifier(text_processor) classifier.train(train_texts, train_classes, train_epochs, <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) prediction = classifier.predict(test_texts) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(test_labels_path, <span class="hljs-string"><span class="hljs-string">"w"</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">"utf-8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> test_labels_output: test_labels_output_lst = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">0</span></span>, len(prediction)): test_labels_output_lst.append({ <span class="hljs-string"><span class="hljs-string">"real"</span></span>: test_classes[i], <span class="hljs-string"><span class="hljs-string">"classified"</span></span>: prediction[i] }) json.dump(test_labels_output_lst, test_labels_output) instantiated_classifier = TextClassifier(text_processor, **classifier.config) instantiated_prediction = instantiated_classifier.predict(test_texts) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(instantiated_test_labels_path, <span class="hljs-string"><span class="hljs-string">"w"</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">"utf-8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> instantiated_test_labels_output: instantiated_test_labels_output_lst = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">0</span></span>, len(instantiated_prediction)): instantiated_test_labels_output_lst.append({ <span class="hljs-string"><span class="hljs-string">"real"</span></span>: test_classes[i], <span class="hljs-string"><span class="hljs-string">"classified"</span></span>: instantiated_prediction[i] }) json.dump(instantiated_test_labels_output_lst, instantiated_test_labels_output) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(trained_path, <span class="hljs-string"><span class="hljs-string">"w"</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">"utf-8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> trained_output: json.dump(classifier.config, trained_output, ensure_ascii=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">classification_error</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(files)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> name <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> files: <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(name, <span class="hljs-string"><span class="hljs-string">"r"</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">"utf-8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> src: data = json.load(src) classes = [] real = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> row <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> data: classes.append(row[<span class="hljs-string"><span class="hljs-string">"real"</span></span>]) classified = row[<span class="hljs-string"><span class="hljs-string">"classified"</span></span>] row_classes = list(classified.keys()) row_classes.sort() real.append([classified[class_name] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> class_name <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> row_classes]) labels = [] class_names = list(set(itertools.chain(*classes))) class_names.sort() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> item_classes <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> classes: labels.append([int(class_name <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> item_classes) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> class_name <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> class_names]) real_np = numpy.array(real) mse = mean_squared_error(numpy.array(labels), real_np) print(name, mse) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> __name__ == <span class="hljs-string"><span class="hljs-string">'__main__'</span></span>: print(<span class="hljs-string"><span class="hljs-string">"Reuters:\n"</span></span>) classification_demo(reuters_classes, <span class="hljs-number"><span class="hljs-number">10000</span></span>, <span class="hljs-number"><span class="hljs-number">15000</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-string"><span class="hljs-string">"reuters_test_labels.json"</span></span>, <span class="hljs-string"><span class="hljs-string">"reuters_car_test_labels.json"</span></span>, <span class="hljs-string"><span class="hljs-string">"reuters_trained.json"</span></span>) classification_error([<span class="hljs-string"><span class="hljs-string">"reuters_test_labels.json"</span></span>, <span class="hljs-string"><span class="hljs-string">"reuters_car_test_labels.json"</span></span>]) print(<span class="hljs-string"><span class="hljs-string">"Car intents:\n"</span></span>) classification_demo(car_classes, <span class="hljs-number"><span class="hljs-number">400</span></span>, <span class="hljs-number"><span class="hljs-number">500</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-string"><span class="hljs-string">"car_test_labels.json"</span></span>, <span class="hljs-string"><span class="hljs-string">"instantiated_car_test_labels.json"</span></span>, <span class="hljs-string"><span class="hljs-string">"car_trained.json"</span></span>) classification_error([<span class="hljs-string"><span class="hljs-string">"cars_test_labels.json"</span></span>, <span class="hljs-string"><span class="hljs-string">"instantiated_cars_test_labels.json"</span></span>]) print(<span class="hljs-string"><span class="hljs-string">"Weather:\n"</span></span>) classification_demo(weather_classes, <span class="hljs-number"><span class="hljs-number">40</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-string"><span class="hljs-string">"weather_test_labels.json"</span></span>, <span class="hljs-string"><span class="hljs-string">"instantiated_weather_test_labels.json"</span></span>, <span class="hljs-string"><span class="hljs-string">"weather_trained.json"</span></span>) classification_error([<span class="hljs-string"><span class="hljs-string">"weather_test_labels.json"</span></span>, <span class="hljs-string"><span class="hljs-string">"instantiated_weather_test_labels.json"</span></span>])</code> </pre><br></div></div><br>  Here you see: <br><br><ul><li>  Data preparation: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(data_path, <span class="hljs-string"><span class="hljs-string">'r'</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> data_source: data = json.load(data_source) texts = [item[<span class="hljs-string"><span class="hljs-string">"text"</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> item <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> data] class_names = [item[<span class="hljs-string"><span class="hljs-string">"classes"</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> item <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> data] train_texts = texts[:train_before] train_classes = class_names[:train_before] test_texts = texts[train_before:test_before] test_classes = class_names[train_before:test_before]</code> </pre><br></li><li>  Creating a new classifier: <br><br><pre> <code class="python hljs">text_processor = TextProcessor(<span class="hljs-string"><span class="hljs-string">"english"</span></span>, [[<span class="hljs-string"><span class="hljs-string">"turn"</span></span>, <span class="hljs-string"><span class="hljs-string">"on"</span></span>], [<span class="hljs-string"><span class="hljs-string">"turn"</span></span>, <span class="hljs-string"><span class="hljs-string">"off"</span></span>]], Word2Vec.load_word2vec_format(word2vec)) classifier = TextClassifier(text_processor)</code> </pre><br></li><li>  His training: <br><br><pre> <code class="python hljs">classifier.train(train_texts, train_classes, train_epochs, <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre><br></li><li>  Predicting classes for a test sample and storing pairs of ‚Äúreal classes‚Äù - ‚Äúpredicted class probabilities‚Äù: <br><br><pre> <code class="python hljs">prediction = classifier.predict(test_texts) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(test_labels_path, <span class="hljs-string"><span class="hljs-string">"w"</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">"utf-8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> test_labels_output: test_labels_output_lst = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">0</span></span>, len(prediction)): test_labels_output_lst.append({ <span class="hljs-string"><span class="hljs-string">"real"</span></span>: test_classes[i], <span class="hljs-string"><span class="hljs-string">"classified"</span></span>: prediction[i] }) json.dump(test_labels_output_lst, test_labels_output)</code> </pre><br></li><li>  Creating a new instance of the classifier by configuration (dict, can be serialized into / deserialized from, for example json): <br><br><pre> <code class="python hljs">instantiated_classifier = TextClassifier(text_processor, **classifier.config)</code> </pre></li></ul><br>  Exhaust something like this: <br><br><pre> <code class="hljs vhdl">C:\Users\user\pynlc-env\lib\site-packages\gensim\utils.py:<span class="hljs-number"><span class="hljs-number">840</span></span>: UserWarning: detected Windows; aliasing chunkize <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> chunkize_serial warnings.warn(<span class="hljs-string"><span class="hljs-string">"detected Windows; aliasing chunkize to chunkize_serial"</span></span>) C:\Users\user\pynlc-env\lib\site-packages\gensim\utils.py:<span class="hljs-number"><span class="hljs-number">1015</span></span>: UserWarning: Pattern <span class="hljs-keyword"><span class="hljs-keyword">library</span></span> <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> installed, lemmatization won<span class="hljs-symbol"><span class="hljs-symbol">'t</span></span> be available. warnings.warn(<span class="hljs-string"><span class="hljs-string">"Pattern library is not installed, lemmatization won't be available."</span></span>) Using Theano backend. Using gpu device <span class="hljs-number"><span class="hljs-number">0</span></span>: GeForce GT <span class="hljs-number"><span class="hljs-number">730</span></span> (CNMeM <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> disabled, cuDNN <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> available) Reuters: Train <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> <span class="hljs-number"><span class="hljs-number">3000</span></span> samples, validate <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> <span class="hljs-number"><span class="hljs-number">7000</span></span> samples Epoch <span class="hljs-number"><span class="hljs-number">1</span></span>/<span class="hljs-number"><span class="hljs-number">10</span></span> <span class="hljs-number"><span class="hljs-number">20</span></span>/<span class="hljs-number"><span class="hljs-number">3000</span></span> [..............................] - ETA: <span class="hljs-number"><span class="hljs-number">307</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.6968</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.5376</span></span> .... <span class="hljs-number"><span class="hljs-number">3000</span></span>/<span class="hljs-number"><span class="hljs-number">3000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">640</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0018</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9996</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0019</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9996</span></span> Epoch <span class="hljs-number"><span class="hljs-number">8</span></span>/<span class="hljs-number"><span class="hljs-number">10</span></span> <span class="hljs-number"><span class="hljs-number">20</span></span>/<span class="hljs-number"><span class="hljs-number">3000</span></span> [..............................] - ETA: <span class="hljs-number"><span class="hljs-number">323</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0012</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9994</span></span> ... <span class="hljs-number"><span class="hljs-number">3000</span></span>/<span class="hljs-number"><span class="hljs-number">3000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">635</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0012</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9997</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">9.2200e-04</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9998</span></span> Epoch <span class="hljs-number"><span class="hljs-number">9</span></span>/<span class="hljs-number"><span class="hljs-number">10</span></span> <span class="hljs-number"><span class="hljs-number">20</span></span>/<span class="hljs-number"><span class="hljs-number">3000</span></span> [..............................] - ETA: <span class="hljs-number"><span class="hljs-number">315</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">3.4387e-05</span></span> - acc: <span class="hljs-number"><span class="hljs-number">1.0000</span></span> ... <span class="hljs-number"><span class="hljs-number">3000</span></span>/<span class="hljs-number"><span class="hljs-number">3000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">879</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0012</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9997</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0016</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9995</span></span> Epoch <span class="hljs-number"><span class="hljs-number">10</span></span>/<span class="hljs-number"><span class="hljs-number">10</span></span> <span class="hljs-number"><span class="hljs-number">20</span></span>/<span class="hljs-number"><span class="hljs-number">3000</span></span> [..............................] - ETA: <span class="hljs-number"><span class="hljs-number">327</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">8.0144e-04</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9997</span></span> ... <span class="hljs-number"><span class="hljs-number">3000</span></span>/<span class="hljs-number"><span class="hljs-number">3000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">655</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0012</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9997</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">7.4761e-04</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9998</span></span> reuters_test_labels.json <span class="hljs-number"><span class="hljs-number">0.000151774189194</span></span> reuters_car_test_labels.json <span class="hljs-number"><span class="hljs-number">0.000151774189194</span></span> Car intents: Train <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> <span class="hljs-number"><span class="hljs-number">280</span></span> samples, validate <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> <span class="hljs-number"><span class="hljs-number">120</span></span> samples Epoch <span class="hljs-number"><span class="hljs-number">1</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">20</span></span>/<span class="hljs-number"><span class="hljs-number">280</span></span> [=&gt;............................] - ETA: <span class="hljs-number"><span class="hljs-number">0</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.6729</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.5250</span></span> ... <span class="hljs-number"><span class="hljs-number">280</span></span>/<span class="hljs-number"><span class="hljs-number">280</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">0</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.2914</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.8980</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.2282</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9375</span></span> ... Epoch <span class="hljs-number"><span class="hljs-number">19</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">20</span></span>/<span class="hljs-number"><span class="hljs-number">280</span></span> [=&gt;............................] - ETA: <span class="hljs-number"><span class="hljs-number">0</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0552</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9857</span></span> ... <span class="hljs-number"><span class="hljs-number">280</span></span>/<span class="hljs-number"><span class="hljs-number">280</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">0</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0464</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9842</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.1647</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9494</span></span> Epoch <span class="hljs-number"><span class="hljs-number">20</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-number"><span class="hljs-number">20</span></span>/<span class="hljs-number"><span class="hljs-number">280</span></span> [=&gt;............................] - ETA: <span class="hljs-number"><span class="hljs-number">0</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0636</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9714</span></span> ... <span class="hljs-number"><span class="hljs-number">280</span></span>/<span class="hljs-number"><span class="hljs-number">280</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">0</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0447</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9849</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.1583</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9530</span></span> cars_test_labels.json <span class="hljs-number"><span class="hljs-number">0.0520754688092</span></span> instantiated_cars_test_labels.json <span class="hljs-number"><span class="hljs-number">0.0520754688092</span></span> Weather: Train <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> <span class="hljs-number"><span class="hljs-number">28</span></span> samples, validate <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> <span class="hljs-number"><span class="hljs-number">12</span></span> samples Epoch <span class="hljs-number"><span class="hljs-number">1</span></span>/<span class="hljs-number"><span class="hljs-number">30</span></span> <span class="hljs-number"><span class="hljs-number">20</span></span>/<span class="hljs-number"><span class="hljs-number">28</span></span> [====================&gt;.........] - ETA: <span class="hljs-number"><span class="hljs-number">0</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.6457</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.6000</span></span> ... Epoch <span class="hljs-number"><span class="hljs-number">29</span></span>/<span class="hljs-number"><span class="hljs-number">30</span></span> <span class="hljs-number"><span class="hljs-number">20</span></span>/<span class="hljs-number"><span class="hljs-number">28</span></span> [====================&gt;.........] - ETA: <span class="hljs-number"><span class="hljs-number">0</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0021</span></span> - acc: <span class="hljs-number"><span class="hljs-number">1.0000</span></span> ... <span class="hljs-number"><span class="hljs-number">28</span></span>/<span class="hljs-number"><span class="hljs-number">28</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">0</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0019</span></span> - acc: <span class="hljs-number"><span class="hljs-number">1.0000</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.1487</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9167</span></span> Epoch <span class="hljs-number"><span class="hljs-number">30</span></span>/<span class="hljs-number"><span class="hljs-number">30</span></span> ... <span class="hljs-number"><span class="hljs-number">28</span></span>/<span class="hljs-number"><span class="hljs-number">28</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">0</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0018</span></span> - acc: <span class="hljs-number"><span class="hljs-number">1.0000</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.1517</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9167</span></span> weather_test_labels.json <span class="hljs-number"><span class="hljs-number">0.0136964029149</span></span> instantiated_weather_test_labels.json <span class="hljs-number"><span class="hljs-number">0.0136964029149</span></span></code> </pre><br>  In the course of experiments with stopwords: <br><br><ul><li>  the error in the reuter set remained comparable regardless of deletion / preservation of stop words. <br><br></li><li>  an error in weather ‚Äî dropped from 8% while removing stop words.  The complication of the algorithm did not affect (since there are no combinations of words for which the stop word is still to be kept). <br><br></li><li>  the error in car_intent is increased to about 15% when deleting stop words (for example, the conditional ‚Äúturn on‚Äù was curtailed to ‚Äúturn‚Äù).  When adding processing "white list" - back to the previous level. </li></ul><br><h3>  Example with the launch of a pre-trained classifier </h3><br>  Actually, the TextClassifier.config property ‚Äî a dictionary that can be rendered, for example, in json and after being restored from json-a ‚Äî transfers its elements to the TextClassifier's constructor.  For example: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> json <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> gensim.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Word2Vec <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pynlc.test_data <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> word2vec <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pynlc <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> TextProcessor, TextClassifier <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> __name__ == <span class="hljs-string"><span class="hljs-string">'__main__'</span></span>: text_processor = TextProcessor(<span class="hljs-string"><span class="hljs-string">"english"</span></span>, [[<span class="hljs-string"><span class="hljs-string">"turn"</span></span>, <span class="hljs-string"><span class="hljs-string">"on"</span></span>], [<span class="hljs-string"><span class="hljs-string">"turn"</span></span>, <span class="hljs-string"><span class="hljs-string">"off"</span></span>]], Word2Vec.load_word2vec_format(word2vec)) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(<span class="hljs-string"><span class="hljs-string">"weather_trained.json"</span></span>, <span class="hljs-string"><span class="hljs-string">"r"</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">"utf-8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> classifier_data_source: classifier_data = json.load(classifier_data_source) classifier = TextClassifier(text_processor, **classifier_data) texts = [ <span class="hljs-string"><span class="hljs-string">"Will it be windy or rainy at evening?"</span></span>, <span class="hljs-string"><span class="hljs-string">"How cold it'll be today?"</span></span> ] predictions = classifier.predict(texts) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">0</span></span>, len(texts)): print(texts[i]) print(predictions[i])</code> </pre><br>  And its exhaust: <br><br><pre> <code class="hljs tex">C:<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">Users</span></span></span></span><span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">user</span></span></span></span><span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">pynlc</span></span></span></span>-env<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">lib</span></span></span></span><span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">site</span></span></span></span>-packages<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">gensim</span></span></span></span><span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">utils</span></span></span></span>.py:840: UserWarning: detected Windows; aliasing chunkize to chunkize_serial warnings.warn("detected Windows; aliasing chunkize to chunkize_serial") C:<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">Users</span></span></span></span><span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">user</span></span></span></span><span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">pynlc</span></span></span></span>-env<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">lib</span></span></span></span><span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">site</span></span></span></span>-packages<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">gensim</span></span></span></span><span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">utils</span></span></span></span>.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available. warnings.warn("Pattern library is not installed, lemmatization won't be available.") Using Theano backend. Will it be windy or rainy at evening? {'temperature': 0.039208538830280304, 'conditions': 0.9617446660995483} How cold it'll be today? {'temperature': 0.9986168146133423, 'conditions': 0.0016815820708870888}</code> </pre><br>  And yes, the config of the network trained in datedset from reuters is <a href="https://drive.google.com/file/d/0B7cY3wBgM-aBWGh3NmFjSGVHVzA/view%3Fusp%3Dsharing">here</a> .  Gigabyte grid for 19MB dataset, yes :-) </div><p>Source: <a href="https://habr.com/ru/post/315118/">https://habr.com/ru/post/315118/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../315108/index.html">Representing SAP R / 3 data in Oracle Database using the SAP Java Connector</a></li>
<li><a href="../315110/index.html">About my LIR2032 and CR2032 tester, the batteries themselves and accumulated experience</a></li>
<li><a href="../315112/index.html">Is it easy to teach people how to make websites?</a></li>
<li><a href="../315114/index.html">Replacing Algorithm Testing with Testing Introduced Effects</a></li>
<li><a href="../315116/index.html">Online course on entry-level computer networks</a></li>
<li><a href="../315120/index.html">Make it tomorrow. How not to waste time on trivia</a></li>
<li><a href="../315124/index.html">Bike Agile</a></li>
<li><a href="../315126/index.html">FAS filed a case against Microsoft</a></li>
<li><a href="../315128/index.html">Simple API gateway based on PHP and Lumen</a></li>
<li><a href="../315130/index.html">About slaves, heroes and hero slaves</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>