<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Probabilistic interpretation of classical machine learning models</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this article, I begin a series on generative models in machine learning. We will look at the classical tasks of machine learning, define what gener...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Probabilistic interpretation of classical machine learning models</h1><div class="post__text post__text-html js-mediator-article"><p>  In this article, I begin a series on generative models in machine learning.  We will look at the classical tasks of machine learning, define what generative modeling is, look at its differences from the classical problems of machine learning, look at the existing approaches to solving this problem and dive into the details of those based on deep neural network training.  But first, as an introduction, we look at the classical problems of machine learning in their probabilistic formulation. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7dc/2f7/207/7dc2f72074162747496f561e842540e3.gif"></div><a name="habracut"></a><br><h2 id="klassicheskie-zadachi-mashinnogo-obucheniya">  Classic machine learning tasks </h2><br><p>  Two classic machine learning tasks are classification and regression.  Let's look closer at each of them.  Consider the formulation of both problems and the simplest examples of their solution. </p><br><h2 id="klassifikaciya">  Classification </h2><br><p>  The classification task is the task of labeling objects.  For example, if objects are photographs, then the labels may be the content of the photographs: whether the image contains a pedestrian or not, whether the man or woman is depicted, what kind of dog is shown in the photograph.  Usually there is a set of mutually exclusive labels and a collection of objects for which these labels are known.  Having such a collection of data, it is necessary to automatically place labels on arbitrary objects of the same type that were in the original collection.  Let's formalize this definition. <br>  Suppose there are many objects <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="image">  .  These can be points on a plane, handwritten numbers, photographs, or musical works.  Suppose also that there is a finite set of labels <img src="https://habrastorage.org/getpro/habr/post_images/cde/18c/a0a/cde18ca0a2ad19f25a621fa473b8f709.svg" alt="image">  .  These tags can be numbered.  We will identify the tags and their numbers.  In this way <img src="https://habrastorage.org/getpro/habr/post_images/b64/2f3/1a9/b642f31a9b589fc5a9dd906876a8596e.svg" alt="image">  in our notation will be denoted as <img src="https://habrastorage.org/getpro/habr/post_images/f99/fa7/2c5/f99fa72c5a072ecc114f8dae179d787b.svg" alt="image">  .  If a <img src="https://habrastorage.org/getpro/habr/post_images/011/668/9a2/0116689a2417cac1799e3ea4a414b332.svg" alt="image">  then the task is called a binary classification task; if there are more than two labels, then they usually say that it is just a classification task.  Additionally, we have an input sample. <img src="https://habrastorage.org/getpro/habr/post_images/975/4e7/93d/9754e793df916c1007da1f50acc478ae.svg" alt="image">  .  These are the very marked examples on which we will be trained to put labels automatically.  Since we do not know the classes of all objects precisely, we consider that the class of an object is a random variable, which for simplicity we will also denote <img src="https://habrastorage.org/getpro/habr/post_images/97f/a10/8ab/97fa108abfcd32434b0b3973aa8c30a4.svg" alt="image">  .  For example, a photograph of a dog may be classified as a dog with a probability of 0.99 and as a cat with a probability of 0.01.  Thus, in order to classify an object, we need to know the conditional distribution of this random variable on this object. <img src="https://habrastorage.org/getpro/habr/post_images/dae/d2b/2b2/daed2b2b235a036c808d6508b9395b59.svg" alt="image">  . </p><br><p> Task of finding <img src="https://habrastorage.org/getpro/habr/post_images/dae/d2b/2b2/daed2b2b235a036c808d6508b9395b59.svg" alt="image">  with a given set of tags <img src="https://habrastorage.org/getpro/habr/post_images/cde/18c/a0a/cde18ca0a2ad19f25a621fa473b8f709.svg" alt="image">  and this set of tagged examples <img src="https://habrastorage.org/getpro/habr/post_images/975/4e7/93d/9754e793df916c1007da1f50acc478ae.svg" alt="image">  called the classification task. </p><br><h2 id="veroyatnostnaya-postanovka-zadachi-klassifikacii">  Probabilistic formulation of the classification problem </h2><br><p>  To solve this problem, it is convenient to reformulate it in a probabilistic language.  So, there are many objects <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="image">  and lots of tags <img src="https://habrastorage.org/getpro/habr/post_images/cde/18c/a0a/cde18ca0a2ad19f25a621fa473b8f709.svg" alt="image">  . <img src="https://habrastorage.org/getpro/habr/post_images/c22/2ec/f84/c222ecf840e6a4dfc00babfb87316e88.svg" alt="image">  - a random variable representing a random object from <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="image">  . <img src="https://habrastorage.org/getpro/habr/post_images/931/4c2/c68/9314c2c6858d64175608cf241778e898.svg" alt="image">  - a random variable representing a random label from <img src="https://habrastorage.org/getpro/habr/post_images/cde/18c/a0a/cde18ca0a2ad19f25a621fa473b8f709.svg" alt="image">  .  Consider a random variable <img src="https://habrastorage.org/getpro/habr/post_images/eb8/257/653/eb82576531b27d12e5ad71e59b050718.svg" alt="image">  with distribution <img src="https://habrastorage.org/getpro/habr/post_images/3d3/137/c73/3d3137c73ee11eb8089322916ad04f97.svg" alt="image">  which is the joint distribution of objects and their classes.  Then, the marked sample is samples from this distribution. <img src="https://habrastorage.org/getpro/habr/post_images/c73/ab3/d15/c73ab3d152e723ada4ac3efc7eb399fe.svg" alt="image">  .  We will assume that all samples are independently and equally distributed (iid in English literature). </p><br><p>  The classification task can now be reformulated as the task of finding <img src="https://habrastorage.org/getpro/habr/post_images/dae/d2b/2b2/daed2b2b235a036c808d6508b9395b59.svg" alt="image">  with this sample <img src="https://habrastorage.org/getpro/habr/post_images/ab8/2c9/9d0/ab82c99d0e1d844db1e45f02acb6cb1e.svg" alt="image">  . </p><br><h2 id="klassifikaciya-dvuh-normalnyh-raspredeleniy">  Classification of two normal distributions </h2><br><p>  Let's see how this works with a simple example.  Set <img src="https://habrastorage.org/getpro/habr/post_images/64c/836/843/64c836843ba7cffce318ff5388b77475.svg" alt="image">  , <img src="https://habrastorage.org/getpro/habr/post_images/011/668/9a2/0116689a2417cac1799e3ea4a414b332.svg" alt="image">  , <img src="https://habrastorage.org/getpro/habr/post_images/e5c/4bb/ca0/e5c4bbca033d2115064faee438f61a29.svg" alt="image">  , <img src="https://habrastorage.org/getpro/habr/post_images/da5/8e2/a25/da58e2a251de0a7f853d4954629ac265.svg" alt="image">  , <img src="https://habrastorage.org/getpro/habr/post_images/a42/5fd/520/a425fd520039f4685d1f8e1214e7dbe9.svg" alt="image">  .  That is, we have two gaussians, from which we are equally likely to sample the data and we need, having a dot from <img src="https://habrastorage.org/getpro/habr/post_images/1c6/007/ba8/1c6007ba8f56157a0676eb0f616069f5.svg" alt="image">  , predict from which Gaussian it was derived. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/058/abe/3eb/058abe3eb9a1965462b1db09b27068cc.png"></div><br><p>  Fig.  1. Density distribution <img src="https://habrastorage.org/getpro/habr/post_images/372/a09/624/372a096241d2652647d411e210cb8903.svg" alt="image">  and <img src="https://habrastorage.org/getpro/habr/post_images/964/979/448/964979448c098c1c47d7abb62f25645c.svg" alt="image">  . </p><br><p>  Since the domain of the Gaussian is the entire numerical line, it is obvious that these graphs intersect, which means that there are such points at which the probability density <img src="https://habrastorage.org/getpro/habr/post_images/964/979/448/964979448c098c1c47d7abb62f25645c.svg" alt="image">  and <img src="https://habrastorage.org/getpro/habr/post_images/372/a09/624/372a096241d2652647d411e210cb8903.svg" alt="image">  are equal. </p><br><p>  Find the conditional probability of classes: </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7c2/448/683/7c244868347715b23579ec4acb82be66.svg" alt="image"></div><br><p>  Those. <br></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7c9/00a/f42/7c900af4254db66d4f6cd22eb8fc2621.svg" alt="image"></div><br><p>  This is how the probability density graph will look like. <img src="https://habrastorage.org/getpro/habr/post_images/4ad/04b/f2b/4ad04bf2bb81d289aab4780070167228.svg" alt="image">  : </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/164/bc5/83a/164bc583ab7406d67d2dffe32e917071.png"></div><br><p>  Fig.  2. Density of distribution <img src="https://habrastorage.org/getpro/habr/post_images/372/a09/624/372a096241d2652647d411e210cb8903.svg" alt="image">  , <img src="https://habrastorage.org/getpro/habr/post_images/964/979/448/964979448c098c1c47d7abb62f25645c.svg" alt="image">  and <img src="https://habrastorage.org/getpro/habr/post_images/4ad/04b/f2b/4ad04bf2bb81d289aab4780070167228.svg" alt="image">  . <img src="https://habrastorage.org/getpro/habr/post_images/eed/0bf/efa/eed0bfefa5b90dc3ed215d8943648271.svg" alt="image">  where two Gaussians intersect. </p><br><p>  It can be seen that the confidence of the model in belonging to a particular class is very high (the probability is close to zero or one) is close to the Gaussian modes, and where the graphs intersect, the model can only guess by chance and give <img src="https://habrastorage.org/getpro/habr/post_images/28b/bc9/5ba/28bbc95baae1fbcda4ed266d5fc55169.svg" alt="image">  . </p><br><h2 id="metod-maksimizacii-pravdopodobiya">  Likelihood Maximization Method </h2><br><p>  Most of the practical problems can not be solved as described above, since <img src="https://habrastorage.org/getpro/habr/post_images/3d3/137/c73/3d3137c73ee11eb8089322916ad04f97.svg" alt="image">  usually not explicitly set.  Instead, there is usually a data set. <img src="https://habrastorage.org/getpro/habr/post_images/ab8/2c9/9d0/ab82c99d0e1d844db1e45f02acb6cb1e.svg" alt="image">  with some unknown joint distribution density <img src="https://habrastorage.org/getpro/habr/post_images/186/74b/e76/18674be76708883c203ce46a5abeacda.svg" alt="image">  .  In this case, <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">the maximum likelihood method is</a> used to solve the problem.  The formal definition and justification of the method can be found in your favorite book on statistics or the link above, and in this article I will describe its intuitive meaning. </p><br><p>  Likelihood maximization principle says that if there is some unknown distribution <img src="https://habrastorage.org/getpro/habr/post_images/d51/241/45c/d5124145c7c1897cbf4fcb4694eaa65a.svg" alt="image">  from which there is a set of samples <img src="https://habrastorage.org/getpro/habr/post_images/9ea/6b8/fee/9ea6b8fee4862e6547bc2483efad8290.svg" alt="image">  and some well-known parametric family of distributions <img src="https://habrastorage.org/getpro/habr/post_images/198/d24/fbf/198d24fbf91ec4e7b7d87f2415cb232a.svg" alt="image">  then in order to <img src="https://habrastorage.org/getpro/habr/post_images/ebd/9c2/387/ebd9c2387820e9e69ee6785e3f99f77d.svg" alt="image">  as close as possible <img src="https://habrastorage.org/getpro/habr/post_images/d51/241/45c/d5124145c7c1897cbf4fcb4694eaa65a.svg" alt="image">  , you need to find such a vector of parameters <img src="https://habrastorage.org/getpro/habr/post_images/84a/640/df9/84a640df942df02ae3cff21ed52146ec.svg" alt="image">  that maximizes the joint probability of data (likelihood) <img src="https://habrastorage.org/getpro/habr/post_images/be7/85a/a1f/be785aa1f44e77afb006c03bf41f3f77.svg" alt="image">  which is also called the likelihood of data.  It is proved that under reasonable conditions this estimate is a consistent and unbiased estimate of the true vector of parameters.  If samples are selected from <img src="https://habrastorage.org/getpro/habr/post_images/d51/241/45c/d5124145c7c1897cbf4fcb4694eaa65a.svg" alt="image">  , that is, iid data, then the joint distribution is decomposed into the product of the distributions: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/02c/38d/bc5/02c38dbc5dc399b6bd1a7a86dd76cbfa.svg" alt="image"></div><br><p>  Logarithm and multiplication by a constant are monotonically increasing functions and do not change the positions of the maxima, therefore the joint density can be introduced under the logarithm and multiplied by <img src="https://habrastorage.org/getpro/habr/post_images/2f8/c20/011/2f8c20011fe9073d3c901a7df9754f5b.svg" alt="image">  : </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4e1/7ce/a33/4e17cea33f4dd6a5aec3b444da371e87.svg" alt="image"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/00a/fdd/342/00afdd3421f5643c0d8313d49e9700ff.svg" alt="image"></div><br><p>  The last expression, in turn, is an unbiased and consistent estimate of the expected likelihood logarithm: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/493/8cc/95b/4938cc95b18757d579061acc151725d3.svg" alt="image"></div><br><p>  The maximization problem can be rewritten as a minimization problem: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/98e/69d/666/98e69d666a70b240f3d37181c76c3882.svg" alt="image"></div><br><p>  The latter quantity is called the cross-entropy of the distributions. <img src="https://habrastorage.org/getpro/habr/post_images/355/aad/e57/355aade571980ffd7d9caf43e92a9aa7.svg" alt="image">  and <img src="https://habrastorage.org/getpro/habr/post_images/011/a38/79c/011a3879ca7e881ec7e85bfb024dfc88.svg" alt="image">  .  That is what is customarily optimized for solving learning problems with reinforcement (supervised learning). </p><br><p>  Minimization throughout this cycle of articles will be carried out using the <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastic Gradient Descent (SGD)</a> , or rather, its expansion based on adaptive moments, using the fact that the sum of the gradients in the subsample (the so-called ‚Äúmini-match‚Äù) is an unbiased estimate of the gradient of the minimized function. </p><br><h2 id="klassifikaciya-dvuh-normalnyh-raspredeleniy-logisticheskoy-regressiey">  Classification of two normal distributions by logistic regression </h2><br><p>  Let's try to solve the same problem that was described above, using the maximum likelihood method, taking as a parametric family <img src="https://habrastorage.org/getpro/habr/post_images/9e8/971/876/9e8971876fb7ce62cefa0513e728d74d.svg" alt="image">  simplest neural network.  The resulting model is called logistic regression.  The full model code can be found <a href="https://github.com/Monnoroch/generative/tree/master/logistic_regression">here</a> , in the article only the key points are covered. </p><br><p>  First you need to generate data for training.  We need to generate a minibatch of class labels and for each label generate a point from the corresponding Gaussian: </p><br><pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">input_batch</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(dataset_params, batch_size)</span></span></span><span class="hljs-function">:</span></span> input_mean = tf.constant(dataset_params.input_mean, dtype=tf.float32) input_stddev = tf.constant(dataset_params.input_stddev,dtype=tf.float32) count = len(dataset_params.input_mean) labels = tf.contrib.distributions.Categorical(probs=[<span class="hljs-number"><span class="hljs-number">1.</span></span>/count] * count) .sample(sample_shape=[batch_size]) components = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(batch_size): components .append(tf.contrib.distributions.Normal( loc=input_mean[labels[i]], scale=input_stddev[labels[i]]) .sample(sample_shape=[<span class="hljs-number"><span class="hljs-number">1</span></span>])) samples = tf.concat(components, <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> labels, samples</code> </pre> <br><p>  We define our classifier.  It will be the simplest neural network without hidden layers: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">discriminator</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(input)</span></span></span><span class="hljs-function">:</span></span> output_size = <span class="hljs-number"><span class="hljs-number">1</span></span> param1 = tf.get_variable( <span class="hljs-string"><span class="hljs-string">"weights"</span></span>, initializer=tf.truncated_normal([output_size], stddev=<span class="hljs-number"><span class="hljs-number">0.1</span></span>) ) param2 = tf.get_variable( <span class="hljs-string"><span class="hljs-string">"biases"</span></span>, initializer=tf.constant(<span class="hljs-number"><span class="hljs-number">0.1</span></span>, shape=[output_size]) ) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> input * param1 + param2</code> </pre><br><p>  And we write the loss function - the cross-entropy between the distributions of real and predicted labels: </p><br><pre> <code class="python hljs">labels, samples = input_batch(dataset_params, training_params.batch_size) predicted_labels = discriminator(samples) loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits( labels=tf.cast(labels, tf.float32), logits=predicted_labels) )</code> </pre> <br><p>  Below are the graphs of learning of two models: basic and with L2 regularization: </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/273/0d8/71a/2730d871aac8c2c6eec55ef091c29f66.png"></div><br><p>  Fig.  3. Logistic regression learning curve. </p><br><p>  It can be seen that both models quickly converge to a good result.  The model without regularization performs better because in this problem regularization is not needed, and it slightly slows down the speed of learning.  Let's take a closer look at the learning process: </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7dc/2f7/207/7dc2f72074162747496f561e842540e3.gif"></div><br><p>  Fig.  4. The process of learning logistic regression. </p><br><p>  It can be seen that the trained dividing surface gradually converges to the analytically calculated one, and the closer it is, the slower it converges due to the increasingly weak gradient of the loss function. </p><br><h2 id="regressiya">  Regression </h2><br><p>  The regression task is the task of predicting one continuous random variable. <img src="https://habrastorage.org/getpro/habr/post_images/931/4c2/c68/9314c2c6858d64175608cf241778e898.svg" alt="image">  based on the values ‚Äã‚Äãof other random variables <img src="https://habrastorage.org/getpro/habr/post_images/c12/ad4/fca/c12ad4fcaad71a23e0dae73f7f20eede.svg" alt="image">  .  For example, the prediction of a person‚Äôs height by gender (discrete random variable) and age (continuous random variable).  In the same way as in the classification problem, we are given a marked sample. <img src="https://habrastorage.org/getpro/habr/post_images/ab8/2c9/9d0/ab82c99d0e1d844db1e45f02acb6cb1e.svg" alt="image">  .  Predicting the value of a random variable is directly impossible, because it is random and, in fact, is a function, therefore, formally, the task is written as a prediction of its conditional expected value: </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/627/c90/21e/627c9021e199aff7fd20e747cc9bf14a.svg" alt="image"></div><br><h2 id="regressiya-lineyno-zavisimyh-velichin-s-normalnym-shumom">  Regression of linearly dependent quantities with normal noise </h2><br><p>  Let's see how the regression problem is solved with a simple example.  Let there be two independent random variables <img src="https://habrastorage.org/getpro/habr/post_images/a47/71a/799/a4771a7993a06cb83b1e8cc84058cc60.svg" alt="image">  .  For example, this is tree height and normal random noise.  Then we can assume that the age of the tree is a random variable <img src="https://habrastorage.org/getpro/habr/post_images/694/443/823/69444382363149db294a89cbfefb760d.svg" alt="image">  .  In this case, by linearity of expectation and independence <img src="https://habrastorage.org/getpro/habr/post_images/1b6/3f0/a51/1b63f0a51ed07d78063e808a63725e15.svg" alt="image">  and <img src="https://habrastorage.org/getpro/habr/post_images/3fd/efa/c8c/3fdefac8cc09f7e6f21453bf7cf9a3db.svg" alt="image">  : </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fe9/61a/81a/fe961a81a05bf1f0cd2d55b0bdaffacd.svg" alt="image"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e8e/8d5/18b/e8e8d518bf14f700d0ffdab76124392c.svg" alt="image"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1ed/202/d80/1ed202d80aa47a24d0e9e4bc6b5b201a.gif"></div><br><p>  Fig.  5. The regression line of the problem is about linearly dependent values ‚Äã‚Äãwith noise. </p><br><h2 id="reshenie-zadachi-regressii-metodom-maksimalnogo-pravdopodobiya">  Solution of the regression problem with maximum likelihood method </h2><br><p>  Let's formulate the regression problem through the maximum likelihood method.  Set <img src="https://habrastorage.org/getpro/habr/post_images/3e5/d69/f98/3e5d69f98d5b9c3c874dcd703660cc50.svg" alt="image">  ).  Where <img src="https://habrastorage.org/getpro/habr/post_images/3f0/f5b/d4f/3f0f5bd4f659db43deb78e37c78105a5.svg" alt="image">  - new parameter vector.  We see that we are looking for <img src="https://habrastorage.org/getpro/habr/post_images/ea5/9f6/e75/ea59f6e751d77237450127f197362289.svg" alt="image">  - expected value <img src="https://habrastorage.org/getpro/habr/post_images/9e8/971/876/9e8971876fb7ce62cefa0513e728d74d.svg" alt="image">  i.e.  This is the correct regression task.  Then </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/102/c63/7f2/102c637f27d54821b4793645e5d87ced.svg" alt="image"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/864/eec/165/864eec1650e1a24ecd69574350d14ed0.svg" alt="image"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/be2/c7e/7a6/be2c7e7a6bfdeb4e90db9d0f82355414.svg" alt="image"></div><br><p>  A consistent and unbiased estimate of this expectation will be the average of the sample. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/59d/de8/923/59dde8923e9d289aa90c408eac11198c.svg" alt="image"></div><br><p>  Thus, to solve the regression problem, it is convenient to minimize the root-mean-square error on the training set. </p><br><h2 id="regressiya-velichiny-lineynoy-regressiey">  Regression of magnitude by linear regression </h2><br><p>  Let's try to solve the same problem as above, using the method from the previous section, taking as a parametric family <img src="https://habrastorage.org/getpro/habr/post_images/9e8/971/876/9e8971876fb7ce62cefa0513e728d74d.svg" alt="image">  the simplest possible neural network.  The resulting model is called linear regression.  The full model code can be found <a href="https://github.com/Monnoroch/generative/tree/master/linear_regression">here</a> , in the article only the key points are covered. </p><br><p>  First you need to generate data for training.  First we generate a minibatch of input variables. <img src="https://habrastorage.org/getpro/habr/post_images/a47/71a/799/a4771a7993a06cb83b1e8cc84058cc60.svg" alt="image">  after which we get a sample of the original variable <img src="https://habrastorage.org/getpro/habr/post_images/694/443/823/69444382363149db294a89cbfefb760d.svg" alt="image">  : </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">input_batch</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(dataset_params, batch_size)</span></span></span><span class="hljs-function">:</span></span> samples = tf.random_uniform([batch_size], <span class="hljs-number"><span class="hljs-number">0.</span></span>, <span class="hljs-number"><span class="hljs-number">10.</span></span>) noise = tf.random_normal([batch_size], mean=<span class="hljs-number"><span class="hljs-number">0.</span></span>, stddev=<span class="hljs-number"><span class="hljs-number">1.</span></span>) labels = (dataset_params.input_param1 * samples + dataset_params.input_param2 + noise) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> labels, samples</code> </pre><br><p>  We define our model.  It will be the simplest neural network without hidden layers: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">predicted_labels</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(input)</span></span></span><span class="hljs-function">:</span></span> output_size = <span class="hljs-number"><span class="hljs-number">1</span></span> param1 = tf.get_variable( <span class="hljs-string"><span class="hljs-string">"weights"</span></span>, initializer=tf.truncated_normal([output_size], stddev=<span class="hljs-number"><span class="hljs-number">0.1</span></span>) ) param2 = tf.get_variable( <span class="hljs-string"><span class="hljs-string">"biases"</span></span>, initializer=tf.constant(<span class="hljs-number"><span class="hljs-number">0.1</span></span>, shape=[output_size]) ) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> input * param1 + param2</code> </pre><br><p>  And we write the loss function - L2-distance between the distributions of real and predicted values: </p><br><pre> <code class="python hljs">labels, samples = input_batch(dataset_params, training_params.batch_size) predicted_labels = discriminator(samples) loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits( labels=tf.cast(labels, tf.float32), logits=predicted_labels) )</code> </pre><br><p>  Below are the graphs of learning of two models: basic and with L2 regularization: </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/79d/05e/4b7/79d05e4b7e07cf6eafd265de12d80696.png"></div><br><p>  Fig.  6. Linear regression learning curve. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/807/5ec/f76/8075ecf760503d6595d76babf4eb6fff.png"></div><br><p>  Fig.  7. Graph of the first parameter change with the learning step. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0ac/30f/450/0ac30f450db93f5a782d06e3afd9fcef.png"></div><br><p>  Fig.  8. Schedule of the second parameter change with a learning step. </p><br><p>  It can be seen that both models quickly converge to a good result.  The model without regularization performs better because in this problem regularization is not needed, and it slightly slows down the speed of learning.  Let's take a closer look at the learning process: </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c3f/b0c/79d/c3fb0c79d6f0febdf542d27fe28b31fd.gif"></div><br><p>  Fig.  9. The process of learning linear regression. </p><br><p>  It can be seen that the student expectation <img src="https://habrastorage.org/getpro/habr/post_images/a85/9ee/b50/a859eeb50bd120ae6b1091f665bcc5a6.svg" alt="image">  gradually converges to the analytically calculated one, and the closer it is, the more slowly it converges due to the increasingly weak gradient of the loss function. </p><br><h2 id="drugie-zadachi">  Other tasks </h2><br><p>  In addition to the classification and regression problems studied above, there are other tasks of the so-called learning with a teacher, mainly reduced to the mapping between points and sequences: Object-to-Sequence, Sequence-to-Sequence, Sequence-to-Object.  There is also a wide range of classic learning tasks without a teacher: clustering, filling data gaps, and, finally, explicit or implicit approximation of distributions, which is used for generative modeling.  It is about the last class of problems that will be discussed in this series of articles. </p><br><h2 id="generativnye-modeli">  Generative models </h2><br><p>  In the next chapter, we will look at what generative models are and how they differ fundamentally from the discriminative models discussed in this chapter.  We look at the simplest examples of generative models and try to train a model that generates samples from a simple data distribution. </p><br><h2 id="blagodarnosti">  Thanks </h2><br><p>  Thank you <a href="https://www.linkedin.com/in/olga-talanova-b319b761/">Olga Talanova</a> for <a href="https://www.linkedin.com/in/olga-talanova-b319b761/">reviewing</a> this article.  Thank you <a href="https://people.cs.umass.edu/~svorotni/">Sofya Vorotnikova</a> for comments, editing and checking the English version.  Thanks to <a href="https://github.com/andrewtar">Andrei Tarashkevich</a> for the help in layout. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/343800/">https://habr.com/ru/post/343800/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../343790/index.html">Configuring Authentication in SAP Netweaver AS Java (Part 3 of 3)</a></li>
<li><a href="../343792/index.html">Digital events in Moscow from December 4 to 10</a></li>
<li><a href="../343794/index.html">Cross-platform IoT: Troubleshooting</a></li>
<li><a href="../343796/index.html">How to quickly design a site using CSS Grid</a></li>
<li><a href="../343798/index.html">Basics of regular expressions in javascript</a></li>
<li><a href="../343802/index.html">Cross-platform IoT: Device Operations</a></li>
<li><a href="../343804/index.html">A bit about the .NET Framework and .NET Core [plus useful links]</a></li>
<li><a href="../343806/index.html">The digest of fresh materials from the world of the frontend for the last week ‚Ññ291 (November 27 - December 3, 2017)</a></li>
<li><a href="../343808/index.html">Where is my payment? How fraudsters earn freelancers</a></li>
<li><a href="../343810/index.html">Neural network to identify individuals embedded in the smartphone</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>