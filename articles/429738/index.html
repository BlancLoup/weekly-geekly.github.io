<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The optimal location of shards in a petabyte cluster Elasticsearch: linear programming</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="At the very heart of the Meltwater and Fairhair.ai information retrieval systems is a collection of Elasticsearch clusters with billions of articles f...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The optimal location of shards in a petabyte cluster Elasticsearch: linear programming</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/fb5/ee1/f72/fb5ee1f72a2519aae061ef6be05aa09a.png" align="left">  At the very heart of the Meltwater and <a href="https://fairhair.ai/">Fairhair.ai</a> information retrieval systems is a collection of Elasticsearch clusters with billions of articles from the media and social media. <br><br>  Index shards in clusters differ greatly in access structure, workload, and size, which raises some very interesting problems. <br><br>  In this article, we describe how linear programming (linear optimization) was applied to maximize the uniform distribution of the search and index workload across all nodes in the clusters.  This solution reduces the chance that a single node will become a bottleneck in the system.  As a result, we increased the search speed and saved on the infrastructure. <br><a name="habracut"></a><br><h1>  Prehistory </h1><br>  Fairhair.ai information retrieval systems contain about 40 billion messages from social media and editorials, processing millions of queries daily.  The platform provides customers with search results, graphics, analytics, data export for more advanced analysis. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      These massive datasets are housed in several 750-node Elasticsearch clusters with thousands of indices of more than 50,000 shards. <br><br>  For more information about our cluster, see previous articles on <a href="https://underthehood.meltwater.com/blog/2018/02/06/running-a-400%2B-node-es-cluster/">its architecture</a> and <a href="https://underthehood.meltwater.com/blog/2018/09/28/using-machine-learning-to-load-balance-elasticsearch-queries/">load balancer on machine learning</a> . <br><br><h1>  Uneven workload distribution </h1><br>  Both our data and user requests are usually tied to a date.  Most requests fall within a specific time period, for example, last week, last month, last quarter, or an arbitrary range.  To simplify indexing and queries, we use <a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/time-based.html">time-based indexing</a> , similar to <a href="https://www.elastic.co/elk-stack">the ELK stack</a> . <br><br>  Such architecture of indexes gives a number of advantages.  For example, you can perform efficient mass indexing, as well as delete whole indexes when data is aging.  It also means that the workload for a given index varies greatly over time. <br><br>  The latest indexes are exponentially more queries, compared with the old ones. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cd9/4a6/25a/cd94a625ae06e77932216d721bb5eea7.png"><br>  <i><font color="gray">Fig.</font></i>  <i><font color="gray">1. Access scheme for indexes by time.</font></i>  <i><font color="gray">The vertical axis shows the number of completed queries, the horizontal axis shows the age of the index.</font></i>  <i><font color="gray">Weekly, monthly and annual plateaus are clearly visible, followed by a long tail of a lower workload on older indices.</font></i> <br><br>  Patterns in Fig.  1 were quite predictable, as our clients are more interested in the latest information and regularly compare the current month with the past and / or this year with the last year.  The problem is that Elasticsearch does not know about this pattern and does not perform automatic optimization for the observed workload! <br><br>  The built-in algorithm for placing shards Elasticsearch takes into account only two factors: <br><br><ol><li>  <i>The number of shards</i> on each node.  The algorithm tries to evenly balance the number of shards per node throughout the cluster. </li><li>  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/disk-allocator.html">Tags</a> free disk space.  Elasticsearch considers the available disk space on a node before deciding whether to allocate new shards to this node or move segments from this node to others.  When 80% of the used disk is used, it is prohibited to place new shards on the node, the system will actively transfer shards from this node to 90%. </li></ol><br>  The fundamental assumption of the algorithm is that each segment in the cluster receives approximately the same amount of workload and that everyone has the same size.  In our case, this is very far from the truth. <br><br>  The standard load distribution quickly leads to hot spots in the cluster.  They appear and disappear randomly, because the workload changes over time. <br><br>  A hot spot is essentially a node operating near its limit of one or more system resources, such as a CPU, disk I / O, or network bandwidth.  When this happens, the node first queues requests for a while, which increases the response time to the request.  But if the overload continues for a long time, then eventually requests are rejected, and users get errors. <br><br>  Another common consequence of overload is the unsustainable pressure of the JVM garbage due to queries and indexing operations, which leads to the ‚Äúterrible hell‚Äù phenomenon of the JVM garbage collector.  In such a situation, the JVM either cannot get the memory fast enough and crashes out (out of memory), or gets stuck in an endless garbage collection cycle, freezes and stops responding to requests and pings of the cluster. <br><br>  The problem worsened when we <a href="https://underthehood.meltwater.com/blog/2018/02/06/running-a-400%2B-node-es-cluster/">refactored our architecture under AWS</a> .  Previously, we were saved by the fact that we launched up to four Elasticsearch nodes on our own powerful servers (24 cores) in our data center.  This masked the effect of the asymmetric distribution of shards: the load was largely smoothed by a relatively large number of cores on the machine. <br><br>  After refactoring, we placed only one node on less powerful machines (8 cores) - and the first tests immediately revealed major problems with ‚Äúhot spots‚Äù. <br><br>  Elasticsearch assigns shards in random order, and with more than 500 nodes in a cluster, the likelihood of too many ‚Äúhot‚Äù shards on one node has greatly increased - and such nodes quickly overflowed. <br><br>  For users, this would mean a serious deterioration in work, since overloaded nodes slowly respond, and sometimes they completely reject requests or fall.  If you bring such a system into production, users will see frequent seemingly random UI slowdowns and random timeouts. <br><br>  At the same time, there remains a large number of nodes with shards without any special load, which are actually inactive.  This leads to inefficient use of our cluster resources. <br><br>  Both problems could have been avoided if Elasticsearch more intelligently distributed shards, since the average use of system resources on all nodes is at a healthy level of 40%. <br><br><h3>  Continuous cluster change </h3><br>  During the work of more than 500 nodes, we observed one more thing: a constant change in the state of the nodes.  Shards are constantly moving back and forth along nodes under the influence of the following factors: <br><br><ul><li>  New indexes are created, and old ones are discarded. </li><li>  Disk labels work due to indexing and other changes on shards. </li><li>  Elasticsearch randomly decides that there are too few or too many shards on the node compared to the average value of the cluster. </li><li>  Hardware failures and crashes at the OS level lead to the launch of new AWS instances and their joining to the cluster.  With 500 nodes, this happens on average several times a week. </li><li>  New nodes are added almost every week due to normal data growth. </li></ul><br>  Taking all this into account, we came to the conclusion that for a comprehensive solution to all problems, a continuous and dynamic re-optimization algorithm is needed. <br><br><h3>  Solution: Shardonnay </h3><br>  After a long study of the available options, we came to the conclusion that we want: <br><br><ol><li>  Build your own solution.  We did not find good articles, code or other existing ideas that work well in our scale and for our tasks. </li><li>  Start the rebalancing process outside of Elasticsearch and use the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/1.7/cluster-reroute.html">cluster redirection APIs</a> , rather than trying to <a href="http://engineering.simplymeasured.com/dev-blog/2015/07/08/balancing-elasticsearch-cluster-by-shard-size.html">create a plugin</a> .  We wanted a fast feedback loop, and deploying a plugin on a cluster of this magnitude can take several weeks. </li><li>  Use <a href="https://en.wikipedia.org/wiki/Linear_programming">linear programming</a> to calculate optimal shard movements at any time. </li><li>  Perform optimization continuously, so that the state of the cluster gradually comes to the optimum. </li><li>  Do not move too many shards at the same time. </li></ol><br>  We noticed an interesting thing, that if you move too many shards at the same time, it is very easy to cause a <i>cascading storm to move shards</i> .  After the start of such a storm, it can continue for hours when shards uncontrollably move back and forth, causing the appearance of marks on the critical level of disk space in various places.  In turn, this leads to new movements of shards and so on. <br><br>  To understand what is happening, it is important to know that when you move an actively indexed segment, it begins to actually use much more space on the disk from which it is moving.  This is due to the way Elasticsearch saves <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-translog.html">transaction logs</a> .  We have seen cases when the index doubled as the node moved.  This means that a node that initiated shard movement due to high disk space usage will use <i>even more disk space for a</i> while until it moves a sufficient number of shards to other nodes. <br><br>  To solve this problem, we developed the <i>Shardonnay</i> service in honor of the famous Chardonnay grape variety. <br><br><h3>  Linear optimization </h3><br>  Linear optimization (or <a href="https://en.wikipedia.org/wiki/Linear_programming">linear programming</a> , LP) is a method of achieving the best result, such as maximum profit or least cost, in a mathematical model whose requirements are represented by linear relations. <br><br>  The optimization method is based on a system of linear variables, some constraints that must be met, and an objective function that determines how a successful solution looks.  The goal of linear optimization is to find the values ‚Äã‚Äãof variables that minimize the objective function while respecting the constraints. <br><br><h3>  Shard distribution as a linear optimization problem </h3><br>  Shardonnay should work continuously, and at each iteration it performs the following algorithm: <br><br><ol><li>  Using the API, Elasticsearch retrieves information about existing shards, indexes, and nodes in a cluster, as well as their current location. </li><li>  Models the cluster state as a set of binary variables of the LP.  Each combination (node, index, shard, replica) gets its own variable.  In the LP model, there are a number of carefully designed heuristics, constraints and the objective function, see below. </li><li>  Sends the LP model to a linear solver, which gives the optimal solution, taking into account constraints and the objective function.  The solution is a new assignment of shards on the nodes. </li><li>  Interprets the decision of the LP and converts it into a sequence of movements of shards. </li><li>  Instructs Elasticsearch to perform shard movements through the cluster redirection API. </li><li>  Waiting for the cluster to move the shards. </li><li>  Returns to step 1. </li></ol><br>  The main thing is to develop the right constraints and objective function.  The rest will make solver LP and Elasticsearch. <br><br>  Not surprisingly, the task turned out to be very difficult for a cluster of this size and complexity! <br><br><h3>  Restrictions </h3><br>  We base some restrictions on the model based on rules dictated by Elasticsearch itself.  For example, always stick to disk labels or prohibit replica from being placed on the same node as another replica of the same shard. <br><br>  Others are added based on experience gained over the years working with large clusters.  Here are some examples of our own limitations: <br><br><ul><li>  Do not move today's indexes, as they are the hottest and get almost constant load on reading and writing. </li><li>  Prefer the movement of smaller shards, because Elasticsearch copes with them faster. </li><li>  It is desirable to create and place future shards a few days before they become active, begin to be indexed and undergo a heavy load. </li></ul><br><br><h3>  Cost function </h3><br>  Our cost function weighs together a number of different factors.  For example, we want: <br><br><ul><li>  minimize the variance of indexing and search queries to reduce the number of ‚Äúhot spots‚Äù; </li><li>  keep the minimum dispersion of disk usage for the stable operation of the system; </li><li>  minimize the number of shard movements so that chain storms do not start, as described above. </li></ul><br><h3>  Reduction of LP variables </h3><br>  On our scale, the size of these LP models is becoming a problem.  We quickly realized that problems could not be solved in a reasonable time with more than 60 million variables.  Therefore, we applied many optimization and modeling tricks to drastically reduce the number of variables.  Among them are biased sampling, heuristics, the "divide and conquer" method, iterative relaxation and optimization. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/4db/240/fb2/4db240fb21653d48fd6a5ad69728082e.png"></a> <br>  <i><font color="gray">Fig.</font></i>  <i><font color="gray">2. The heat map shows the unbalanced load on the Elasticsearch cluster.</font></i>  <i><font color="gray">This is manifested in a large dispersion of resource use in the left part of the graph.</font></i>  <i><font color="gray">Thanks to continuous optimization, the situation is gradually stabilizing.</font></i> <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/19c/fa6/f18/19cfa6f1813be7f3a2ae9d90c58570b2.png"></a> <br>  <i><font color="gray">Fig.</font></i>  <i><font color="gray">3. The heat map shows CPU utilization on all nodes in the cluster before and after setting the hotness feature in Shardonnay.</font></i>  <i><font color="gray">You can see a significant change in CPU usage with a constant workload.</font></i> <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/dfc/288/81f/dfc28881fa593f543c14c8aa14069525.png"></a> <br>  <i><font color="gray">Fig.</font></i>  <i><font color="gray">4. The heat map shows the reading capacity of the disks during the same period as in fig.</font></i>  <i><font color="gray">3. Read operations are also more evenly distributed across the cluster.</font></i> <br><br><h1>  results </h1><br>  As a result, our solver LP finds good solutions in a few minutes, even for our huge cluster.  Thus, the system iteratively improves the state of the cluster in the direction of optimality. <br><br>  And the best part is that the dispersion of workload and disk usage converges as expected - and this near-optimal state is maintained after many intentional and unexpected changes in the cluster state since then! <br><br>  We now support a healthy workload distribution in our Elasticsearch clusters.  All thanks to linear optimization and our service, which we call <i>Chardonnay</i> with love. </div><p>Source: <a href="https://habr.com/ru/post/429738/">https://habr.com/ru/post/429738/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../429728/index.html">Modern MVot-based Kotlin architecture</a></li>
<li><a href="../429730/index.html">Large supercomputer to be built on Elbrus processors in Russia</a></li>
<li><a href="../429732/index.html">You Gonna Hate This, or a Tale of How a Good Code Should Look</a></li>
<li><a href="../429734/index.html">Dream to fly with electrotechnical bias</a></li>
<li><a href="../429736/index.html">Hogweed Sosnowski. In MO introduced fines for distribution</a></li>
<li><a href="../429744/index.html">Learn OpenGL. Lesson 6.4 - IBL. Mirror irradiance</a></li>
<li><a href="../429750/index.html">Developer Cookbook: DDD Recipes (Part 3, Application Architecture)</a></li>
<li><a href="../429754/index.html">Fatal errors in hardware integration</a></li>
<li><a href="../429756/index.html">How to configure the setting of Nuxt.js environment variables in runtime, or How to do everything differently and do not regret</a></li>
<li><a href="../429758/index.html">Why SRE is important documentation. Part 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>