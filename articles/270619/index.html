<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How we teach machine learning and data analysis in Beeline</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="After a long preparation, selection of materials and preliminary approbations of the course, on October 19 we started. Corporate intensive practical c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How we teach machine learning and data analysis in Beeline</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/182/a3a/e01/182a3ae013a04381806f020ffa63109d.png"><br><br>  After a long preparation, selection of materials and preliminary approbations of the course, on October 19 we started.  Corporate intensive practical course of data analysis from the experts of this case.  At the moment we have 6 classes, half of our course, and this is a brief overview of what we do on them. <br><br>  First of all, our task was to create a course in which we would give the maximum of practice that students could immediately apply in their daily work. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      We often saw how people who came to us for interviews, despite their good knowledge of the theory, because of lack of experience could not reproduce all the stages of solving a typical machine learning task - data preparation, feature selection / design, model selection, their correct composition, achieving high quality and correct interpretation of the results. <br><br>  Therefore, our main thing is practice.  Open notebooks IPython and immediately work with them. <br><a name="habracut"></a><br>  In the first lesson, we discussed the decision trees and forests and dismantled the feature extraction technique using the Kaggle Titanic: Machine Learning from Disaster data set as an example. <br><br>  We believe that participation in machine learning competitions is necessary in order to maintain the required level and continually improve the analyst‚Äôs expertise.  Therefore, starting from the first lesson, we launched our own car insurance payout competition (now a very popular topic in business) with the help of Kaggle Inclass.  The competition will last until the end of the course. <br><br>  In the second lesson, we analyzed Titanic data using Pandas and visualization tools.  Also tried simple classification methods on the task of predicting payments in auto insurance. <br><br>  In the third session, we considered the Kaggle ‚ÄúGreek Media Monitoring Multilabel Classification (WISE 2014)‚Äù task and the use of mixing algorithms as the main technique for most Kaggle competitions. <br><br>  In the third lesson, we also looked at the capabilities of the Scikit-Learn machine learning library, and then more detailed linear classification methods: logistic regression and linear SVM, discussed when it is better to use linear models, and when complex ones.  We talked about the importance of signs in the task of learning and on the quality metrics of classification. <br><br>  The main topic of the fourth lesson was the meta-learning stacking algorithm and its application in Otto‚Äôs Kaggle winning solution to the Classification products into correct category. <br><br>  In the fourth lesson, we got to the main problem of machine learning - the fight against retraining; we looked at how to use the learning and validation curves for this, how to cross-validate correctly, and what indicators can be optimized in this process to achieve the best generalizing ability.  They also studied methods for constructing ensembles of classifiers and regressors ‚Äî bagging, random subspaces, boosting, stacking, etc. <br><br>  And so on - each lesson is a new practical task. <br><br>  Since our main language on the course is Python, we are, of course, familiar with the main libraries of data analysis in the Python language ‚Äî NumPy, SciPy, Matplotlib, Seaborn, Pandas, and Scikit-learn.  We spend a lot of time on this, since the work of a data researcher largely consists of calling various methods of the listed modules. <br><br>  Understanding the theory on which the methods are based is also important.  Therefore, we considered the basic mathematical methods implemented in SciPy - finding eigenvalues, singular decomposition of the matrix, maximum likelihood method, optimization methods, etc. These methods were considered not just theoretically, but in conjunction with machine learning algorithms , logistic regression, spectral clustering, the method of principal components, etc. This approach is first practice, then theory, and finally, practice at a new level, with a theoretical understanding - p  marketed by many machine learning schools abroad. <br><br>  Let's take a closer look at one of the popular methods often used by participants in Kaggle competitions - stacking, which we studied in the fourth lesson. <br><br>  In its simplest form, the idea of ‚Äã‚Äãstacking is to take M basic algorithms (for example, 10), split the training set into two parts - say, A and B. First, teach all M basic algorithms into part A and make predictions for part B. Then, on the contrary, train all models on part B and make predictions for objects from part A. This way, you can get a prediction matrix whose dimensions are nx M, where n is the number of objects in the original training set, M is the number of basic algorithms.  This matrix is ‚Äã‚Äãfed to the input of another model - the model of the second level, which, in fact, is trained on learning outcomes. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/161/ea8/fab/161ea8fab25a4f95a6f2203417797e51.png" width="30%" height="30%"></div><br><br>  Often make the scheme a little more complicated.  To train models on more data than half of the training sample, the sample is divided K times into K parts.  Models are trained on the K-1 parts of the training sample, predictions are made on one part and bring the results into the matrix of predictions.  This is a K-fold stacking. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/cbe/7ee/5c6/cbe7ee5c678747618432813d768c4251.png" width="50%" height="50%"></div><br><br>  Let us briefly review the winners of the Kaggle Otto Group Product Classification Challenge.  <a href="https://www.kaggle.com/c/otto-group-product-classification-challenge">www.kaggle.com/c/otto-group-product-classification-challenge</a> <br>  The task was to correctly classify goods into one of 9 categories based on 93 attributes, the essence of which Otto does not disclose.  Predictions were estimated based on the average F1-measure.  The competition became the most popular in the history of Kaggle, perhaps because the entry threshold was low, the data was well prepared, you could quickly take and try one of the models. <br><br>  In deciding the winners of the competition, essentially the same K-fold stacking was used, only at the second level, three models were trained, not just one, and then the predictions of these three models were mixed. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/5cd/069/091/5cd069091b2b4512a5e40937aeeb556c.png" width="50%" height="50%"></div><br><br>  At the first level, 33 models were trained - various implementations of many machine learning algorithms with different sets of features, including those created. <br><br>  These 33 models were trained 5 times in 80% of the training set and made predictions for the remaining 20% ‚Äã‚Äãof the data.  The resulting predictions were collected in the matrix.  That is, 5-fold stacking.  But the difference from the classical stacking model is that the predictions of the 33 models were supplemented with 8 created features ‚Äî mostly the initial data obtained by clustering the data (and the feature itself ‚Äî labels of the received clusters) or taking into account the distances to the closest representatives of each class. <br><br>  On the learning outcomes of 33 first-level models and 8 created features, 3 second-level models were trained - XGboost, Lasagne NN neural network and AdaBoost with ExtraTrees trees.  The parameters of each algorithm were selected in the process of cross-validation.  The final formula for averaging the predictions of models of the second level was also selected in the process of cross-validation. <br><br>  In the fifth lesson, we continued to study the classification algorithms, namely, neural networks.  They also touched upon the topic of learning without a teacher - clustering, reducing dimensions and searching for outliers. <br><br>  In the sixth lesson, we plunged into Data Mining and an analysis of the consumer basket.  It is widely believed that machine learning and data mining are the same or the latter is part of the first.  We explain that this is not the case and point out the differences. <br><br>  Initially, machine learning was more aimed at predicting the type of new objects based on the analysis of the training set, and the classification problem is most often remembered when people talk about machine learning. <br><br>  The mining of data was focused on the search for ‚Äúinteresting‚Äù patterns (patterns) in the data, for example, of frequently purchased goods together.  Of course, now this edge is being erased, and those methods of machine learning, which work not as a black box, but give out and interpreted rules, can be used to look for patterns. <br><br>  So, the decision tree allows you to understand not only that this user behavior is similar to fraud, but why.  Such rules resemble association rules, however, they appeared in mining data for analyzing sales, because it is useful to know that seemingly unrelated products are bought together.  By purchasing A buy and B. <br><br>  We not only talk about all this, but also give the opportunity to analyze the real data on such purchases.  So data on sales of contextual advertising will help to understand what recommendations for the purchase of search phrases to give to those who want to promote their online store in the network. <br><br>  Another important method for mining data is searching for frequent sequences.  If the sequence &lt;laminate, fridge&gt; is quite frequent, then most likely you will buy new settlers in your store.  And such sequences are useful for predicting the next purchases. <br><br>  A separate lesson is devoted to recommender systems.  We not only teach classical algorithms for collaborative filtering, but also dispel misconceptions that SVD, the de facto standard in this area, is a panacea for all recommender tasks.  So, in the task of recommending radio stations, SVD is noticeably retrained, and the rather natural hybrid approach of collaborative filtering based on the similarity of users and dynamic profiles of users and radio stations based on tags works great. <br><br>  So, six lessons passed, what next?  Further more.  We will have an analysis of social networks, we will build a recommendation system from scratch, teach processing and comparison of texts.  Also, of course, we will have Big Data analysis using Apache Spark. <br><br>  We will also give a separate lesson to tricks and tricks on Kaggle, about which a person who is now in 4th place in the world Kaggle rating will tell. <br><br>  From November 16 we will launch the second course for those who could not get on the first one.  Details, as usual, on <a href="http://bigdata.beeline.digital/">bigdata.beeline.digital</a> . <br><br>  You can follow the chronicle of the current course on our Facebook <a href="https://www.facebook.com/events/844849848965355/845211058929234/">page</a> . </div><p>Source: <a href="https://habr.com/ru/post/270619/">https://habr.com/ru/post/270619/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../270607/index.html">Insider API: search for trends and quick navigation in the texts of social. and traditional media</a></li>
<li><a href="../270609/index.html">Parboiled Pro (Part 3)</a></li>
<li><a href="../270611/index.html">IPv6 configuration in Linux OS Debian v7.XX, Ubuntu v14.XX, CentOS v6.XX and FreeBSD v10.XX</a></li>
<li><a href="../270615/index.html">How to store a complex hierarchy of settings in Redmine projects</a></li>
<li><a href="../270617/index.html">Minify a project created in Blocs 1.5.2 using Gulp</a></li>
<li><a href="../270621/index.html">Six years go</a></li>
<li><a href="../270623/index.html">Windows Store and Malvari's Future</a></li>
<li><a href="../270625/index.html">Why upgrade to Data ONTAP Cluster Mode?</a></li>
<li><a href="../270629/index.html">Two providers of the Internet on the scheme DMVPN - dilution on different VRF on Spoke-ah</a></li>
<li><a href="../270631/index.html">Freeradius. Support for different types of user authentication simultaneously</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>