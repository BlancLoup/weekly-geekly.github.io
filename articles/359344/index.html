<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Designing software platform protected NAS</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Suppose the NAS hardware is built and the OS is installed on it, for example, as shown here . And now you have a working server with Debian, which is ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Designing software platform protected NAS</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/pf/d5/z0/pfd5z0nxyjqrzrymyoa0z29me_q.jpeg"></p><br><p>  Suppose the <a href="https://habr.com/post/353012/">NAS hardware is built</a> and the OS is installed on it, for example, as shown <a href="https://habr.com/post/351932/">here</a> .  And now you have a working server with Debian, which is loaded, connected to the network, and you have full physical access to it. </p><br><p>  Now you need to design an environment that allows you to easily and safely add, delete application services, and manage their work. </p><a name="habracut"></a><br><p>  Inspired by an <a href="https://habr.com/post/328048/">article from a certain Cloud Architect</a> , I decided to make a system in which most services work in containers. </p><br><p>  In addition, similar methods (for example, partitioning I / O spaces by container virtualization) are used in fairly important <a href="http://www.ndexpo.ru/mediafiles/u/files/materials_2017/Vladykin_NIIIS_ZPP_Sinergiya.pdf">systems of the nuclear industry</a> . </p><br><p>  It is very convenient and safe: </p><br><ul><li>  No dependency conflicts.  It is possible to keep in the system several different versions of Glibc, php, nginx and everything else, except the kernel. </li><li>  No wonder where the scattered configurations. </li><li>  Services are easy to stop, start, update, etc .. </li><li>  It is easy to add your own settings and changes to each container, no matter where they are, at least in / usr.  In the "normal OS" package system will erase such changes and it is necessary to be perverted with hooks so that they are permanent. </li><li>  Containers provide a high degree of isolation: one compromised service does not mean full access to all the others and, especially, to the system. </li></ul><br><p>  But in his version there is something that I did not like: </p><br><ul><li>  Proxmox is superfluous here.  First, for the sake of ZFS support, there is no sense in installing it, since  I install the OS with its full support, including root FS, manually.  Secondly, it is a NAS, not a virtual machine server, so the Proxmox functionality is redundant.  Thirdly, containers and virtual machines can manage OpenMediaVault, and it is inconvenient to have two entry points.  Well and, fourthly, the <strong>core of Proxmox is supported by OMV</strong> , when you connect the <a href="http://omv-extras.org/">OMV Extras</a> repositories, and you do not need to rebuild packages. </li><li>  Dependency conflicts are one of the biggest drawbacks that Proxmox brings with OpenMediaVault.  Because of this, packages have to be rebuilt, and updating is difficult. </li><li>  Installing OMV version 3, when there is a 4th version, in which much has been redone, does not make sense now. </li><li>  nginx-proxy-companion for lets-encrypt did not work for me, and I had to solve the certificate problem by another method. </li></ul><br><p>  This article offers an option that works on my NAS and so far suits me perfectly. </p><br><p>  In order to improve readability, the topic is divided into two articles: design and <a href="https://habr.com/post/415779/">implementation</a> . </p><br><h2>  Disk Organization </h2><br><div class="spoiler">  <b class="spoiler_title">Here I will repeat a little and for the sake of completeness I will briefly present the disk organization of the system.</b> <div class="spoiler_text"><h3 id="sistemnye-ssd">  System SSD </h3><br><p>  Two SSDs are reserved for the system, and the second SSD is a mirror of the first. </p><br><p>  Data structure on SSD: </p><br><ul><li> <code>part_boot</code> - partition with bootloader.  Size = 1GB. </li><li>  <code>part_system</code> - the partition with the system.  Size = 32 GB (Recommended size: 16 GB * 2). </li><li>  <code>part_slog</code> - a section with SLOG.  Size = 5 GB. <br>  SLOG can not be more than the size of RAM = 32 GB + 48 GB swap = 80 GB, 16 GB, taking into account the possible expansion of the swap). <br>  96 GB - absolute limit. <br>  The usual rule for setting the size of a SLOG is to allocate a volume of no more than that which the system can skip in 5 seconds. <br>  In the case of a board with 2 1G interfaces, SLOGsz = 1000/8 <em>2</em> 5 = 1250 MB. <br>  The size with the margin is 5 GB. </li></ul><br><p>  <code>part_system</code> and <code>part_slog</code> encrypted in XTS mode. </p><br><p>  In general, their organization is: </p><br><pre> <code class="hljs erlang-repl">SSD1: [part_boot] -&gt; [zfs_mirror] &lt;---&gt; SSD2 SSD1: [part_system] -&gt; [crypto_xts] -&gt; [zfs_mirror] &lt;---&gt; SSD2 SSD1: [part_slog] -&gt; [crypto_xts] -&gt; [zfs_zil_mirror] &lt;---&gt; SSD2</code> </pre> <br><p>  Partitions are duplicated using ZFS. </p><br><h3 id="ssd-s-keshem-l2arc">  SSD with L2ARC cache </h3><br><p>  The bottom layer is encrypted using XTS mode on a random key. </p><br><p>  Contains two sections: </p><br><ul><li>  <code>part_swap</code> - swap partition.  Size = 48 GB (max RAM * 1.5 = 32 GB * 1.5). </li><li>  <code>part_l2arc</code> - L2ARC.  Size = 196 GB (ARC size * [3..10], ARC size = 0.6 * max RAM size, ie 58 - 196 GB, besides, with deduplication disabled, you need ~ 1 GB L2ARC per 1 TB of data). </li></ul><br><p>  swap and l2arc are encrypted with a random key. </p><br><p>  A random key for the swap partition is acceptable because  the system will not use hibernation. </p><br><p>  Under L2ARC, all remaining space is allocated, its real necessity with a memory size of up to 32 GB is questionable. </p><br><p>  The size of L2ARC is required to be more precisely adjusted in the process of system operation according to the statistics of cache hits. </p><br><p>  Organization: </p><br><pre> <code class="hljs erlang-repl">SSD3: | -&gt; [part_swap] -&gt; [crypto_xts] -&gt; [system swap] | -&gt; [part_l2arc] -&gt; [crypto_xts] -&gt; [l2arc]</code> </pre> </div></div><br><h3 id="diskovaya-korzina">  Disc basket </h3><br><p>  Since  At the first stage it was planned to use 4 disks from 8 possible, all disks in the basket are included in 2 ZFS VDEV. </p><br><p>  Each disk first has an XTS encryption layer.  A physical ZFS device is organized on top of it. </p><br><p>  4 physical devices are combined into one RAIDZ1.  If you do not mind the disk space, or more devices (for example, you plan to immediately buy all the disks), it is recommended to make a RAIDZ2 and one array. </p><br><div class="spoiler">  <b class="spoiler_title">Performance Measurements</b> <div class="spoiler_text"><ul><li>  Direct copy to disk in blocks of 4 MB: 185 MB / s (dsync: 136 MB / s) </li><li>  LUKS partition with AES-NI command support: 184 MB / s (dsync: 135 MB / s) </li><li>  ZFS volume = 170 MB / s (dsync: 50 MB / s). </li><li>  LUKS on ZFS volume = 274 MB / s (dsync: 38 MB / s). </li><li>  ZFS on LUKS = 187 MB / s (dsync: 50 MB / s) </li></ul><br><p>  Hence the conclusion: in spite of the "assurances of experts", it is better to place the ZFS pool over LUKS, and not vice versa.  LUKS almost does not contribute overhead (with AES-NI).  And it is always possible to enable disk write caching manually (as well as choosing the block size, which is variable in ZFS, too). </p></div></div><br><p>  The complete scheme is as follows: </p><br><pre> <code class="hljs erlang-repl">HDD1: [crypto_xts] -&gt; [zfs_phdev] | HDD2: [crypto_xts] -&gt; [zfs_phdev] | HDD3: [crypto_xts] -&gt; [zfs_phdev] | -&gt; [RAIDZ1] -&gt; [tank0] HDD4: [crypto_xts] -&gt; [zfs_phdev] | HDD5: [crypto_xts] -&gt; [zfs_phdev] | HDD6: [crypto_xts] -&gt; [zfs_phdev] | HDD7: [crypto_xts] -&gt; [zfs_phdev] | -&gt; [RAIDZ1] -&gt; [tank1] HDD8: [crypto_xts] -&gt; [zfs_phdev] |</code> </pre> <br><h2>  File System Organization </h2><br><h3 id="struktura-pula">  Pool structure </h3><br><p>  The NAS will contain various application systems, described below. <br>  Each system adds its own directory to the fixed points of the directory structure of the pool, but the directory names for each system will be described during its design. </p><br><p>  Below is the file structure of the pool, common to all systems: </p><br><p><img src="https://habrastorage.org/webt/vc/0t/vo/vc0tvodsoqy03dvji4em8fo5dqy.png"></p><br><div class="spoiler">  <b class="spoiler_title">Chart code</b> <div class="spoiler_text"><pre> <code class="hljs scala"><span class="hljs-meta"><span class="hljs-meta">@startsalt</span></span> { {<span class="hljs-type"><span class="hljs-type">T</span></span> +/ ++ tank0 +++ docker ++++ lib ++++ services +++ apps +++ repos +++ user_data ++++ music ++++ videos ++++ pictures ++++  . ++ tank1 +++ apps +++ user_data } } <span class="hljs-meta"><span class="hljs-meta">@endsalt</span></span></code> </pre> </div></div><br><p>  In the diagram: </p><br><ul><li>  <code>tank0/docker</code> - data of services running in Docker. <br><ul><li>  <code>lib</code> - docker service files.  This is a separate file system and is required so that the docker does not clutter <code>/var</code> snapshots. </li><li>  <code>services</code> - description of containers (for example, dockercompose files) and their service files. </li></ul></li><li>  <code>tank0/apps</code> - the repository is the root in which repositories are created for user applications.  Within the application subdirectory, the application is free to locate data as it sees fit. </li><li>  <code>tank0/repos</code> - repositories.  Here user data will be stored under version control.  As I plan to use only Git, the repositories will be contained there directly.  But in general, in this directory may be subdirectories for different version control systems. </li><li>  <code>tank0/user_data</code> - the repository is the root in which repositories are created for user data: <br><ul><li>  books - the user library is stored here. </li><li>  music - music users. </li><li>  videos - video. </li><li>  pictures - various images and photos. </li><li>  User repositories (by their names). </li></ul></li><li>  <code>tank1/apps</code> - the repository is the root in which repositories are created for user applications.  Repeat structure in <code>tank0</code> . </li><li>  <code>tank1/user_data</code> - the repository is the root in which repositories are created for user data.  Repeat structure in <code>tank0</code> . </li></ul><br><p>  Storage <code>tank1</code> allocated for the future: it will be implemented in the case of expanding disk space. </p><br><h2 id="sostav-sistemnogo-po-i-vzaimodeystvie-komponentov">  The composition of the system software and the interaction of components </h2><br><p>  This is what NAS looks like in the context of systems interacting with it: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/5-/rq/v1/5-rqv1ragvekziaermc8gt7fa4e.png"></a> </p><br><div class="spoiler">  <b class="spoiler_title">Chart code</b> <div class="spoiler_text"><pre> <code class="hljs rust">@startuml ' ----------------------------------------------------- <span class="hljs-symbol"><span class="hljs-symbol">'left</span></span> to right direction scale <span class="hljs-number"><span class="hljs-number">0.72</span></span> package Internet #efefff { cloud <span class="hljs-string"><span class="hljs-string">"Let's\nEncrypt"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> le { } cloud <span class="hljs-string"><span class="hljs-string">"Cloud DNS"</span></span> #ffffff { frame <span class="hljs-string"><span class="hljs-string">"A-"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> af { artifact <span class="hljs-string"><span class="hljs-string">"system.NAS.cloudns.cc"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> d1 artifact <span class="hljs-string"><span class="hljs-string">"omv.NAS.cloudns.cc"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> d2 artifact <span class="hljs-string"><span class="hljs-string">"ldap.NAS.cloudns.cc"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> d3 artifact <span class="hljs-string"><span class="hljs-string">"ssp.NAS.cloudns.cc"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> d4 artifact <span class="hljs-string"><span class="hljs-string">"cloud.NAS.cloudns.cc"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> d5 artifact <span class="hljs-string"><span class="hljs-string">"git.NAS.cloudns.cc"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> d6 artifact <span class="hljs-string"><span class="hljs-string">"backup.NAS.cloudns.cc"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> d7 } frame <span class="hljs-string"><span class="hljs-string">"TXT-"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf { artifact <span class="hljs-string"><span class="hljs-string">"Domain:_acme-challenge.*.NAS.cloudns.cc\nTxt value:9ihDbjxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"</span></span> } } cloud <span class="hljs-string"><span class="hljs-string">" "</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> ms #ffffff } <span class="hljs-symbol"><span class="hljs-symbol">'Internet</span></span> end package LAN #efffef { node  <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> router #ffffff { component <span class="hljs-string"><span class="hljs-string">"DNS "</span></span> { artifact <span class="hljs-string"><span class="hljs-string">".*\.nas"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> ndns } component <span class="hljs-string"><span class="hljs-string">"NAT"</span></span> { artifact <span class="hljs-string"><span class="hljs-string">" 80"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> rop80 artifact <span class="hljs-string"><span class="hljs-string">" 443"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> rop443 artifact <span class="hljs-string"><span class="hljs-string">" 5022"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> rop5022 } } node NAS #ffefef { component <span class="hljs-string"><span class="hljs-string">""</span></span> { artifact <span class="hljs-string"><span class="hljs-string">" 22"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nasp22 artifact <span class="hljs-string"><span class="hljs-string">" 80"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nasp80 artifact <span class="hljs-string"><span class="hljs-string">" 443"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nasp443 } package Docker { component <span class="hljs-string"><span class="hljs-string">"letsencrypt-dns"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> led { } component <span class="hljs-string"><span class="hljs-string">"nginx-reverse-proxy"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nrp { } component <span class="hljs-string"><span class="hljs-string">"nginx-local"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> ngl { } component <span class="hljs-string"><span class="hljs-string">"LDAP    WEB GUI"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> ldaps { } component <span class="hljs-string"><span class="hljs-string">"LDAP SSP"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> ssp { } collections <span class="hljs-string"><span class="hljs-string">" \n(Backup, Cloud, DLNA, etc.)"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> services } artifact <span class="hljs-string"><span class="hljs-string">" "</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> cert_file component <span class="hljs-string"><span class="hljs-string">"WEB  OMV"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> omvwg { } component <span class="hljs-string"><span class="hljs-string">" OMV"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> omv { } component <span class="hljs-string"><span class="hljs-string">"SSH "</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sshd { } component <span class="hljs-string"><span class="hljs-string">"NUT"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nut { } } } <span class="hljs-symbol"><span class="hljs-symbol">'LAN</span></span> end node  <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> ups #ffefef actor  <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> user user &lt;- af user &lt;-&gt; router user &lt;- ms nut &lt;-- ups : USB le &lt;-- af le &lt;-- tf led .-&gt; tf : \n led &lt;-. le : \n led .&gt; cert_file nrp &lt;. cert_file af -&gt; router ndns -&gt; NAS rop80 &lt;--&gt; nasp80 rop443 &lt;--&gt; nasp443 rop5022 &lt;--&gt; nasp22 nasp22 &lt;--&gt; sshd nasp80 &lt;--&gt; nrp nasp443 &lt;--&gt; nrp nrp &lt;--&gt; ngl : \ndocker0 nrp &lt;--&gt; ldaps : \ndocker0 nrp &lt;--&gt; ssp : \ndocker0 ldaps &lt;--&gt; ssp : \ndocker0 nrp &lt;--&gt; services ngl &lt;--&gt; omvwg :   omv &lt;--&gt; omvwg :   omv .&gt; ms : \n omv ..&gt; sshd : \n WEB GUI omv ..&gt; nut : \n WEB GUI @enduml</code> </pre> </div></div><br><p>  This cluttered diagram reflects the composition of its services and most of the interactions. <br>  Further components, the appointment of services and operation algorithms will be described in more detail. </p><br><h3 id="operacionnaya-sistema">  operating system </h3><br><p>  The system, as shown above, is installed on SSDs that are included in the mirror using ZFS.  As the OS, <a href="http://www.openmediavault.org/">OpenMediaVault</a> is selected - the storage management system and WEB GUI (hereinafter - OMV). </p><br><p>  It is rather simple to install it with a package, and everything else will be pulled up according to dependencies: the kernel, additional repositories, etc. </p><br><h3 id="sistemnoe-po">  System software </h3><br><p>  The central components are: </p><br><ul><li>  <a href="https://github.com/jwilder/nginx-proxy">Nginx proxy</a> is a dispatcher with automatic certificate management (using a separate mechanism). </li><li>  <a href="https://www.docker.com/">Docker</a> - containerization system.  It provides convenient work with containers: loading images, assembling, configuring, etc ... To manage containers, the OMV plugin is used. </li></ul><br><p>  On a physical machine, only OMV, SSH, and demons that the user does not access work.  All other systems work inside the Docker containers. </p><br><h3 id="upravlenie-polzovatelyami">  user management </h3><br><p>  User authentication is done through LDAP.  This is done to manage users centrally, and most services support this mechanism, unlike, for example, <a href="https://ru.wikipedia.org/wiki/RADIUS">RADIUS servers</a> and similar, albeit more convenient, new and lightweight solutions. </p><br><p>  The LDAP server works in a container, but it can also be accessed from the host network. <br>  Services (gitlab, OMV, cloud, etc.) are configured to use an LDAP server. <br>  Users can change passwords using <a href="https://ltb-project.org/documentation/self-service-password">LDAP Self Service Password</a> . </p><br><p>  When adding a user to the system, it is first necessary to register the user using the console or the WEB-interface.  I am using <a href="http://phpldapadmin.sourceforge.net/wiki/index.php/Main_Page">PHP LDAP Admin</a> . </p><br><p>  If you want to give users rights to operate with the OS, it is possible to use <a href="https://wiki.debian.org/LDAP/PAM">PAM LDAP</a> . </p><br><h3 id="set">  Network </h3><br><h4 id="fizicheskaya-organizaciya">  Physical organization </h4><br><p>  As seen in the deployment diagram, the NAS is located behind a router on the local network.  Ideally, in order to increase network security, it would be nice to isolate it in the DMZ using a second router, but this is not necessary. </p><br><p>  The user can access the NAS from both the Internet and the local network.  Any appeal affects the router. </p><br><p>  Since  the system has several interfaces, more than one is connected to the router simultaneously (in my case two) and their <a href="https://ru.wikipedia.org/wiki/%25D0%2590%25D0%25B3%25D1%2580%25D0%25B5%25D0%25B3%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%25BA%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25BE%25D0%25B2">bonding is</a> organized. </p><br><p>  First, the interfaces in this case are not idle. <br>  Secondly, it increases reliability, and in some modes and throughput. </p><br><h4 id="logicheskaya-organizaciya">  Logical organization </h4><br><p>  As it is possible to see from the diagram, the router participates in the system operation. </p><br><p>  In the case of access from an external network, the router organizes port forwarding.  Ports 80 and 443 are used to provide access to services via HTTP and HTTPS, respectively.  Port 5022 is forwarded to NAS port 22 for SSH access.  Ideally, it is better to have some discipline for assigning port numbers: for example, ports 10001-10999 are assigned to access host services in the NAS, ports 110001-11999 are assigned to access the second home server, etc. </p><br><p>  When accessing from the Internet, you need to be able to bind several domain names to your IP.  This is implemented in different ways, but I use the option with a cloudy DNS, providing a DNS zone.  As such, <a href="https://www.cloudns.net/">ClouDNS has</a> been applied. </p><br><p>  In the case of access from the local network, the router provides a DNS server.  If you move the DNS server from the router to the NAS, the system will be completely autonomous.  But it does not make much sense, because  Without a functioning router, which serves to organize a local network (including connecting the NAS to the network) and communicating with external networks, it is impossible to use the NAS anyway. </p><br><p>  Its DNS server should be able to return a specific IP if the name falls under a regular expression. </p><br><p>  For the NAS, there are two entries: " <code>.*\.nas</code> " and " <code>.*\.NAS\.cloudns\.cc</code> ", where the NAS is registered in the ClouDNS zone. </p><br><p>  As a result, regardless of whether there is Internet, the router will redirect all calls from the local network to domains in the NAS.cloudns.cc zone on the NAS. </p><br><p>  An HTTP request, hitting the port of NAS 80 or 443, is redirected to the port of the container with nginx-reverse-proxy. </p><br><p>  It returns a signed certificate to the user for secure access over HTTPS.  Then, depending on the domain name, forwards the request to the container with the required service.  For example, a request for cloud.NAS.cloudns.cc will be redirected to the container in which the personal cloud is running. </p><br><p>  There are two types of services: </p><br><ul><li>  Services running in containers.  Their fortend shares the same network as nginx-reverse-proxy.  Here the redirection is organized directly. </li><li>  Services running on the host.  For example, the OMV WEB-interface works directly on the host, not in the container and is not available directly from the external network on port 443 (simply because this port listens to nginx-reverse-proxy). </li></ul><br><p>  In the case of services of the second type, the redirection is organized through the nginx-local container containing the domain names of the "iron" host. </p><br><p>  The process of passing the request within the NAS is shown in the diagram below.  The certificate renewal process shown on the diagram is described below. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/fe/-m/co/fe-mcovzkawnlfrnhxfh2ef_ch8.png"></a> </p><br><div class="spoiler">  <b class="spoiler_title">Chart code</b> <div class="spoiler_text"><pre> <code class="hljs haskell">@startuml actor  <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> user participant <span class="hljs-string"><span class="hljs-string">"cloudns"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> cld participant <span class="hljs-string"><span class="hljs-string">"letsencrypt-dns"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> led participant <span class="hljs-string"><span class="hljs-string">"Let's Encrypt"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> le participant <span class="hljs-string"><span class="hljs-string">"nginx-reverse-proxy"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nrp participant <span class="hljs-string"><span class="hljs-string">""</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> app participant <span class="hljs-string"><span class="hljs-string">"nginx-local"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> ngl user -&gt; cld :  <span class="hljs-type"><span class="hljs-type">IP</span></span>  <span class="hljs-type"><span class="hljs-type">Cloudns</span></span>\n[nas.nas.cloudns.cc] user &lt;- cld :  <span class="hljs-type"><span class="hljs-type">IP</span></span>\n[<span class="hljs-number"><span class="hljs-number">1.2</span></span><span class="hljs-number"><span class="hljs-number">.3</span></span><span class="hljs-number"><span class="hljs-number">.4</span></span>] user -&gt; cld :   user &lt;- cld :  group    led -&gt; le :  \n led &lt;- le :   led -&gt; cld : <span class="hljs-type"><span class="hljs-type">DNS</span></span> challenge,\n <span class="hljs-type"><span class="hljs-type">API</span></span> led &lt;- cld : \n led -&gt; le :  \n   cld &lt;- le :    cld -&gt; le :  led &lt;- le : \nc end user -&gt; nrp :   <span class="hljs-type"><span class="hljs-type">IP</span></span> [<span class="hljs-number"><span class="hljs-number">1.2</span></span><span class="hljs-number"><span class="hljs-number">.3</span></span><span class="hljs-number"><span class="hljs-number">.4</span></span>]\n  <span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">443</span></span>,\n   [nas.nas.cloudns.cc]\n   nrp -&gt; app :   \n   [nas.*]\n \n docker0 group      <span class="hljs-type"><span class="hljs-type">Docker</span></span>  app <span class="hljs-comment"><span class="hljs-comment">--&gt; ngl :    localhost ngl -&gt; "  " :  ngl &lt;- "  " :  app &lt;-- ngl :   end nrp &lt;- app :  user &lt;- nrp :     @enduml</span></span></code> </pre> </div></div><br><h5 id="sertifikat">  Certificate </h5><br><p>  Periodically, a container with letsencrypt-dns receives a certificate for a group of domains.  The certificate is <a href="https://certbot.eff.org/">obtained</a> using <a href="https://certbot.eff.org/">certbot</a> .  Before obtaining a certificate, Let's Encrypt service checks whether the domain belongs to the one who requests a certificate for it. </p><br><p>  With the case of ClouDNS, this is done using the so-called <a href="">DNS challenge</a> : </p><br><ul><li>  A random sequence is generated. </li><li>  The sequence is inserted into the TXT record. </li><li>  Let's Encrypt checks that such a sequence is indeed present. </li></ul><br><p>  Insertion into TXT can be done manually or using an API.  In order to unify access to different DNS providers, there is a library and a tool <a href="https://github.com/AnalogJ/lexicon">Lexicon</a> . </p><br><p>  Unfortunately, there is a minus for ClouDNS: its API is paid.  Considering that, I did not attach the certificate immediately, and I did not want to redo everything, I just bought access, which costs $ 42 for 2 years (and it would be strange to pinch forty bucks, with a total NAS price of more than $ 3000). </p><br><p>  If desired, it is possible to find normal services with a free API. </p><br><h4 id="setevye-servisy">  Network Services </h4><br><p>  Infrastructure services related to the network that are required for operation and maintenance: </p><br><ul><li>  Nginx-reverse-proxy is a dispatcher that provides data to a service via its URL. </li><li>  Lets-encrypt to renew certificates. </li><li>  Nginx proxying requests to the host‚Äôs network from the container network. </li><li>  SSH management is organized using OpenSSH. </li><li>  DNS server, as already mentioned, <strong>will not be</strong> part of the NAS.  In order to make the NAS completely autonomous, it certainly does not hurt.  But given the fact that NAS is not needed without a network, it is possible to implement it on a router. </li><li>  The POP3 / SMTP server shown on the diagram is external and is needed to send alerts from the NAS. </li></ul><br><p>  It is possible to use nginx-proxy-companion for obtaining certificates, but it did not work for me. </p><br><h2>  Interaction with the power source </h2><br><p>  As the core of the power management system, the <a href="https://networkupstools.org/">NUT</a> daemon was chosen, which is supported by the OMV plugin and which does not have any serious universal alternatives. </p><br><p>  Accordingly, the <a href="http://powerquality.eaton.ru/Products-services/Backup-Power-UPS/9130.aspx">Eaton</a> uninterruptible power supply was initially chosen so that problems in the Linux + NUT bundle did not arise with it. </p><br><p>  Eatons, in this case, are generally <a href="https://networkupstools.org/">very well supported</a> .  Its only serious drawback is the noise.  But it is easily corrected by replacing the fan, which was described in the "iron" article. </p><br><p>  In order for the NAS to interact properly and the UPS, you need to configure responses to the events described below. </p><br><p>  When the battery reaches its maximum service life, the following is performed: </p><br><ul><li>  Alarm in the WEB interface.  The plugin is responsible for this and the requirement is optional. </li><li>  Sending a message to e-mail every 24 hours.  Also implemented through OMV. </li></ul><br><p>  When the power supply is disconnected for more than 1 minute, it is performed: </p><br><ul><li>  Alarm in the WEB interface. </li><li>  Sending a message by e-mail. </li></ul><br><p>  When the battery drops below the critical level, it is executed: </p><br><ul><li>  Sending a message to e-mail. </li><li>  Shut down the system. </li></ul><br><h2>  Security measures from unauthorized access </h2><br><p>  Here are just general security measures regarding the NAS: </p><br><ul><li>  All users are logged in: no anonymous access.  In the future, it is possible to retreat somewhat from such a scheme.  Links to files in the cloud can be provided by third-party users. </li><li>  Implemented full disk encryption.  Including, the root file system is encrypted. </li><li>  Password protection is set on the system settings via EFI. </li><li>  System trusted boot.  It is an additional tool to control the authenticity of the bootloader. </li><li>  Two-factor authentication in the form of a token is possible, but this is not yet implemented. </li><li>  Firewall is being used. </li><li>  As an additional measure of protection against brute force, it is possible to use <a href="https://www.fail2ban.org/wiki/index.php/Main_Page">fail2ban</a> . </li><li>  The introduction of a mandatory access control system will also increase the complexity of remote hacking and may partially offset its consequences. </li></ul><br><h2>  Reliability Tools </h2><br><p>  In order to reduce the probability of system failure, are used: </p><br><ul><li>  E-mail alerts (both remote and local) about everything: power off, autopsy, SMART problems. </li><li>  SMART monitoring. </li><li>  Isolation of applications. </li><li>  Additional reservations.  For example, replication of data in cloud storage. </li></ul><br><p>  If the SMART parameters of any disk reach critical values, the following is performed: </p><br><ul><li>  Alarm in the WEB interface. </li><li>  Sending a message to e-mail. </li><li>  If possible, drive the disc out of the pool. </li></ul><br><h2>  System management </h2><br><p>  Software management is carried out through: </p><br><ul><li>  WEB interface available via HTTPS. </li><li>  Ssh.  I have a habit of working through the console, and SSH is highly desirable. </li><li>  IPMI: system management, with an unloaded OS. </li></ul><br><h3 id="web-interfeys">  WEB interface </h3><br><p>  The system provides basic services accessible via HTTPS (in the NAS.cloudns.cc zone from the Internet, or in the nas zone on the local network): </p><br><ul><li>  <a href="https://nas.nas/">https: //nas.nas</a> , <a href="https://omv.nas/">https: //omv.nas</a> - storage management system interface. </li><li>  <a href="https://ssp.nas/">https: //ssp.nas</a> - interface for changing user password. </li><li>  <a href="https://ldap.nas/">https: //ldap.nas</a> - LDAP server admin interface. </li></ul><br><p>  Additional subsystems will add their own interfaces, which are described in the same way. </p><br><h2>  Organization of application systems </h2><br><p>  Preliminary composition of systems: </p><br><ul><li>  The system works with the code. </li><li>  Backup. </li><li>  Cloud storage. </li><li>  Download content from the Internet. </li><li>  Media system. </li></ul><br><p>  Each of the systems will be described separately. </p><br><h2>  Utility Components </h2><br><h3 id="subd">  DBMS </h3><br><p>  Since  The DBMS is required for most systems, at the initial stage there was a desire to select one DBMS, based on the capabilities and requirements of the subsystems, and run in a single copy.  But in the end, it turned out that using several DBMS, depending on the implementation of the subsystem, is simpler and not particularly costly in terms of resources.  On this option, I have stopped. </p><br><h3 id="sistema-dopolnitelnogo-rezervirovaniya">  Additional reservation system </h3><br><p>  Tasks of this system: </p><br><ul><li>  Duplication of OS configuration and systems.  The configuration is backed up to the data pool. </li><li>  Saving intermediate states of both the root pool and the data pool. </li></ul><br><p>  Composition: </p><br><ul><li>  Plugin OMV backup.     . </li><li> zfs-auto-snapshot.      ,  ZFS . </li></ul><br><h2 id="chto-ne-sdelano">    </h2><br><ul><li>  ,      ,    ,  ""    .   ,  ,    .    . </li><li>        .   ,    ,     .    ,   <a href="https://fstec.ru/component/attachments/download/289"> </a> ,     <a href="https://www.microsoft.com/en-us/sdl">Microsoft SDL</a> .    ,           . </li><li>      ,        . </li><li>      ( 40 ),    ,       .   ,        IPMI,      ,  ,   NAS   ,    (   IME <a href="https://www.ixbt.com/news/2017/11/13/intel-management-engine.html">  </a> ,    <a href="https://github.com/corna/me_cleaner">   IME</a> ). </li></ul></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/359344/">https://habr.com/ru/post/359344/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../359332/index.html">Samsung will pay Apple $ 539 million for patent infringement</a></li>
<li><a href="../359334/index.html">The head of Roskomnadzor believes that around the situation with the Telegram "a lot of foam"</a></li>
<li><a href="../359338/index.html">How and why $ 10 billion in public cloud costs are unwarranted losses</a></li>
<li><a href="../359340/index.html">Issue # 23: IT training - current issues and challenges from leading companies</a></li>
<li><a href="../359342/index.html">Monday begins on Saturday, or that you can learn about life in another country from sci-hub logs</a></li>
<li><a href="../359346/index.html">A series of articles: building a secure NAS, or a home mini-server</a></li>
<li><a href="../359348/index.html">Telegram MTPROTO Proxy - everything we know about him</a></li>
<li><a href="../359350/index.html">Data storage: a brief overview of this year‚Äôs trends</a></li>
<li><a href="../359352/index.html">"Finishing line": 5G networks to be, but not before 2020</a></li>
<li><a href="../359356/index.html">Creating 3D Chess in Unity</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>