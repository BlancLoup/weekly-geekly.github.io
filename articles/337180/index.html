<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Differences Postgres Pro Enterprise and PostgreSQL</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="1. Cluster multimaster 
 The expansion of multimaster and its support in the kernel, which is only in the Postgres Pro Enterprise version, make it pos...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Differences Postgres Pro Enterprise and PostgreSQL</h1><div class="post__text post__text-html js-mediator-article"><h2>  1. Cluster multimaster </h2><br>  The expansion of <code>multimaster</code> and its support in the kernel, which is only in the <b>Postgres Pro Enterprise</b> version, make it possible to build clusters of high availability servers.  After each transaction, global integrity (data integrity across the cluster) is guaranteed, i.e.  on each node its data will be identical.  At the same time, it is easy to ensure that reading performance scales linearly with an increase in the number of nodes. <br><a name="habracut"></a><br>  In vanilla PostgreSQL, it is possible to build highly accessible clusters using streaming replication, but third-party utilities and smart scripts are required to identify failed nodes and restore the node after a crash.  <code>multimaster</code> does it on its own, works out of the box without using external utilities or services. <br><br>  Reading readscanning in <code>PostgreSQL</code> vanilla is possible during replication in hot <code>Hot-standby</code> mode, but with a significant caveat: the application must be able to separate <code>read-only</code> and <code>read-write</code> requests.  That is, to work on a vanilla cluster, an application may have to be rewritten: if possible, use separate connections to the base for read-only transactions, and distribute these connections across all nodes.  For a cluster with <code>multimaster</code> you can write to any node, so there are problems with splitting connections from the database into writers and only readers do not.  In most cases, you do not need to rewrite the application. <br><br>  To ensure fault tolerance, the application must be able to do <code>reconnect</code> - i.e.  attempt to restore the connection to the base when it is broken.  This applies to both the vanilla cluster and <code>multimaster</code> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Using logical replication in vanilla <code>PostgreSQL</code> you can implement asynchronous bidirectional replication (for example, <a href="https://www.2ndquadrant.com/en/resources/bdr/"><code>BDR</code></a> from <i>2ndQuadrant</i> ), but global integrity is not provided and there is a need to resolve conflicts, and this can be done only at the application level, based on its internal logic.  That is, these problems are shifted to application programmers.  Our <code>multimaster</code> itself provides transaction isolation (transaction repeat isolation ( <code>Repeatable Read</code> ) and Read fixed data isolation levels are now implemented. In the process of committing a transaction, all replicas will be matched, and the user application will see the same state base; he does not need to know what kind of car you run a query that initiated the transaction, we have implemented the 3-phase commit transactions (3-phase in order to achieve this and get a predictable response time in the event of a node failure. <code>commit protocol</code> ) It is.  the mechanism is more complicated than more well-known 2-phase, so let us explain its scheme. For simplicity, depict two nodes, bearing in mind that in fact similar to the node 2 generally works even number of nodes. <br><br><img src="https://habrastorage.org/web/36d/8ca/823/36d8ca8237d548969453b299703b522f.jpg"><br>  <i>Fig.</i>  <i>1. Scheme of work multimaster</i> <br><br>  The transaction commit request comes to node 1 and is written to the node's WAL.  The remaining nodes of the cluster (node ‚Äã‚Äã2 in the diagram) receive information on data changes via the logical replication protocol and, upon receiving a request to prepare a commit transaction ( <code>prepare transaction</code> ), apply the changes (without commit).  After that, they inform the node that initiated the transaction about their readiness to commit the transaction ( <code>transaction prepared</code> ).  In the case when at least one node does not respond, the transaction is rolled back.  If all nodes are positive, node 1 sends a message to the nodes that a transaction can be committed ( <code>precommit</code> transaction). <br><br>  This shows the difference from a 2-phase transaction.  This action may seem superfluous at first, but in fact this is an important phase.  In the case of a 2-phase transaction, the nodes would commit the transaction and report it to the 1st, which initiated the transaction node.  If the connection was broken at that moment, then node 1, without knowing anything about the success / failure of the transaction on node 2, would have to wait for a response until it becomes clear what it should do to preserve integrity: roll back or commit the transaction (or commit , risking integrity).  So, in the 3-phase scheme during the 2nd phase all nodes vote: whether to commit the transaction.  If the majority of nodes are ready to fix it, the arbitrator announces to all nodes that the transaction is committed.  Node 1 commits a transaction, sends <code>commit</code> on logical replication and reports a transaction commit timestamp (it is necessary for all nodes to maintain transaction isolation for reading requests. In the future, the timestamp will be replaced with a <a href="https://postgrespro.ru/blog/pgsql/130532"><code>CSN</code></a> - transaction commit identifier, <code>Commit Sequence Number</code> ).  If the nodes are in the minority, then they can neither write nor read.  Integrity violations will not occur even if the connection is broken. <br><br>  We chose the <code>multimaster</code> architecture for the future: we are engaged in developing an efficient sharding.  When the tables become distributed (that is, the data on the nodes will already be different), it will be possible to scale not only in reading but also in writing, since it will not be necessary to write in parallel all the data on all nodes in the cluster.  In addition, we develop communication tools between nodes using the <a href="https://ru.wikipedia.org/wiki/%25D0%25A3%25D0%25B4%25D0%25B0%25D0%25BB%25D1%2591%25D0%25BD%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25BF%25D1%2580%25D1%258F%25D0%25BC%25D0%25BE%25D0%25B9_%25D0%25B4%25D0%25BE%25D1%2581%25D1%2582%25D1%2583%25D0%25BF_%25D0%25BA_%25D0%25BF%25D0%25B0%25D0%25BC%25D1%258F%25D1%2582%25D0%25B8"><code>RDMA</code></a> protocol (in <code>InfiniBand</code> switches or in <code>Ethernet</code> devices, where <code>RDMA</code> supported) when the node directly communicates to the memory of other nodes.  Due to this, less time is spent on packing and unpacking network packets, and data transfer delays are small.  Since the nodes communicate intensively when synchronizing changes, this will result in a performance gain for the entire cluster. <br><br><h2>  2. 64-bit transaction counters </h2><br>  This fundamental remake of the DBMS kernel is needed only for heavily loaded systems, but for them it is not just desirable.  She is necessary.  In the <code>PostgreSQL</code> kernel, the transaction counter is 32-bit, which means it‚Äôs impossible to calculate more than 4 billion.  This leads to problems that are solved by "freezing" - a special procedure for the routine maintenance of <code>VACUUM FREEZE</code> .  However, if the meter overflows too often, the costs of this procedure are very high, and can even lead to the inability to record something in the database.  In Russia now there are not so few corporate systems, which have an overflow in 1 day, but the bases, which overflow with a weekly frequency, are no longer exotic.  At the <a href="https://www.pgcon.org/2017/">PGCon 2017</a> developer conference in Ottawa, it was said that some customers had a counter overflow in 2-3 hours.  Nowadays, people tend to add to the database the data that had previously been thrown away, with an understanding of the limited capabilities of the then technology.  In modern business it is often not known in advance what data may be needed for analytics. <br><br>  The counter overflow problem is called ( <code>transaction ID wraparound</code> ), since the transaction number space is looped back (this is clearly explained in the <a href="https://habrahabr.ru/company/postgrespro/blog/301238/">article by</a> <i>Dmitry Vasiliev</i> ).  At overflow the counter is reset and goes to the next lap. <br><br><img src="https://habrastorage.org/web/cd5/1c0/092/cd51c0092b8649d1b879c1b8a217de29.gif"><br>  <i>Figure 2. How does freezing transactions lagging behind more than half a circle.</i> <br><br>  In vanilla <code>PostgreSQL</code> (that is, with a knowingly 32-bit transaction counter), something is also being done to alleviate the problem of transaction wraparound.  For this, in version 9.6, the <code>all-frozen</code> bit was added to the <code>(visibility map)</code> format, with which whole pages are marked as frozen, therefore the planned (when many old transactions accumulate) and emergency (when approaching overflow) freezing occur much faster.  The rest of the DBMS pages work as usual.  Due to this, the overall system performance in handling overflow suffers less, but the problem is not solved in principle.  The described situation with the system shutdown is still not excluded, although its probability has decreased.  You still need to carefully monitor the settings of <code>VACUUM FREEZE</code> , so that there are no unexpected performance gains due to its operation. <br><br>  Replacing 32-bit counters with 64-bit counters pushes the overflow almost to infinity.  The need for <code>VACUUM FREEZE</code> practically eliminated (in the current version, freezing is still used to process <code>pg_clog</code> and <code>pg_multixact</code> and in an emergency case, which is described below).  But the problem is not solved in the forehead.  If a table has few fields, and especially if these fields are integer, its volume can increase significantly (after all, the transaction numbers that generated the record and the one that deleted this version of the record are stored in each record, and each number now consists of 8 bytes instead of 4).  Our developers did not just add 32 digits.  In <code>Postgres Pro Enterprise</code> top 4 bytes are not included in the record; they represent an ‚Äúera‚Äù - an offset at the data page level.  The epoch is added to the usual 32-bit transaction number in the table entries.  And the tables do not swell. <br><br>  Now, if the system tries to write an <code>XID</code> that does not fit in the range defined by the epoch for the page, then we must either increase the shift or freeze the whole page.  But it is painlessly executed in memory.  There remains a limitation in the case when the most minimal <code>XID</code> , which can still be claimed by snapshots of data, lags behind the one that we want to write to this page, by more than 2 <sup>32</sup> .  But this is unlikely.  Moreover, in the near future we will most likely overcome this limitation. <br><br>  Another problem with 32-bit counters is that handling overflows is a very complex process.  Up to version 9.5, very critical bugs were found and corrected in the corresponding code, and there are no guarantees that bugs will not appear in the next versions.  In our implementation of the 64-bit transaction counter, simple and clear logic is inherent, so working with it and developing it further will be easier than dealing with overflow. <br><br>  Data files of systems with 64-bit counters are binary incompatible with 32-bit ones, but we have handy utilities for converting data. <br><br><h2>  3. Page compression </h2><br>  In PostgreSQL, unlike most other DBMSs, there is no <code>(page level compression)</code> .  Only <code>TOAST</code> data is compressed.  If there are many records in the database with relatively small text fields, then compression could reduce the size of the database several times, which would help not only save on disks, but also improve the performance of the DBMS.  Analytical queries that read a lot of data from the disk and do not change it too often can be accelerated especially efficiently by reducing input-output operations. <br><br>  The <code>Postgres</code> community proposes using compression-enabled file systems for compression.  But this is not always convenient and possible.  Therefore, in <code>Postgres Pro Enterprise</code> we added our own implementation of paged compression.  According to the test results of various <code>Postgres Pro</code> users, the database size was reduced from 2 to 5 times. <br><br>  In our implementation, the pages are stored compressed on disk, but when they are read into the buffer, they are unpacked, so working with them in RAM is the same as usual.  Deployment of compressed data and their compression is fast and practically does not increase the processor load. <br><br>  Since the volume of the changed data may increase when the page is compressed, we cannot always return it to its original place.  We write a compressed page at the end of the file.  When sequentially writing pages that are flushed to disk, the overall system performance can be significantly increased.  This requires a file mapping logical addresses to physical, but this file is small and the costs are imperceptible. <br><br>  The size of the file itself during sequential recording will increase.  Launching on the schedule or manually garbage collection, we can periodically make the file more compact (defragment it) by moving all non-empty pages to the beginning of the file.  You can collect garbage in the background (a segment is blocked, but not the entire table), and we can set the number of background processes that collect garbage. <br><table><tbody><tr><th align="left">  Compression (algorithm) </th><th align="left">  Size (GB) </th><th align="left">  Time (sec) </th></tr><tr><td>  without compression </td><td>  15.31 </td><td>  92 </td></tr><tr><td>  snappy </td><td>  5.18 </td><td>  99 </td></tr><tr><td>  lz4 </td><td>  4.12 </td><td>  91 </td></tr><tr><td>  postgres internal lz </td><td>  3.89 </td><td>  214 </td></tr><tr><td>  lzfse </td><td>  2.80 </td><td>  1099 </td></tr><tr><td>  zlib (best speed) </td><td>  2.43 </td><td>  191 </td></tr><tr><td>  zlib (default level) </td><td>  2.37 </td><td>  284 </td></tr><tr><td>  zstd </td><td>  1.69 </td><td>  125 </td></tr></tbody></table>  <i>Comparison of compression mechanisms.</i>  <i>Test parameters: pgbench -i -s 1000</i> <br><br>  For compression, we chose a modern algorithm zstd (it was developed on <i>Facebook</i> ).  We tried various compression algorithms, and stopped at <code>zstd</code> : this is the best compromise between quality and compression speed, as the table shows. <br><br><h2>  4. Autonomous transactions </h2><br>  Technically, the essence of an autonomous transaction is that this transaction, made from the main, parent transaction, can be committed or rolled back, regardless of the parent commit / rollback.  An autonomous transaction is performed in its own context.  If you define a non-autonomous, but a normal transaction within another (nested transaction), then the internal one will always roll back if the parent rolls back.  This behavior does not always suit application developers. <br><br>  Autonomous transactions are often used where action logging or auditing is needed.  For example, a record of attempting some action to the log is required in a situation where a transaction is rolled back.  An autonomous transaction allows you to ensure that ‚Äúsensitive‚Äù actions of employees (viewing or making changes to customer accounts) always leave traces that can be used to restore the picture in an emergency situation (an example on this subject will be given below). <br><br>  In databases such as <code>Oracle</code> and <code>DB2</code> (but not <code>MS SQL</code> ), autonomous transactions are not formally defined as transactions, but as autonomous blocks within procedures, functions, triggers, and unnamed blocks.  <code>SAP HANA</code> also has autonomous transactions, but they can also be defined as transactions, not just function blocks. <br><br>  In <code>Oracle</code> , for example, autonomous transactions are defined at the beginning of a block as <code>PRAGMA AUTONOMOUS_TRANSACTION</code> .  The behavior of a procedure, function, or unnamed block is determined at the compilation stage and cannot be changed during execution. <br><br>  There are no offline transactions at all in <code>PostgreSQL</code> .  They can be simulated by launching a new connection using dblink, but this translates into overhead, affects speed and is simply inconvenient.  Recently, after the <code>pg_background</code> module <code>pg_background</code> , it was proposed to imitate autonomous transactions by starting background processes.  But this was also ineffective (we will return to the reasons below, when analyzing the test results). <br><br>  In Postgres Pro Enterprise, we implemented offline transactions in the <code></code> .  Now they can be used both as nested autonomous transactions and in functions. <br><br>  In nested autonomous transactions, you can define all available <code>PostgreSQL</code> isolation levels ‚Äî Read Committed, Repeatable Read, and Serializable ‚Äî regardless of the level of the parent transaction.  For example: <br><br> <code><font color="#a020f0">BEGIN</font> <font color="#a020f0">TRANSACTION <br> &lt;..&gt; <br> BEGIN AUTONOMOUS TRANSACTION ISOLATION LEVEL REPEATABLE READ</font> <br> &lt;..&gt; <br> <font color="#a020f0">END</font> ; <br> <font color="#a020f0">END</font> ;</code> <br> <br>  All possible combinations work and give the developer the necessary flexibility.  An autonomous transaction never sees the results of the parent's actions, because it has not yet been fixed.  The reverse depends on the level of insulation of the main.  But in their relations with the transaction, started independently, the usual rules of isolation will apply. <br><br>  The syntax is slightly different in functions: the <code><font color="#a020f0">TRANSACTION</font></code> keyword will generate an error.  A standalone block in a function is defined just like this: <br><br> <code><font color="#a020f0">CREATE</font> <font color="#a020f0">FUNCTION</font> &lt;..&gt; <font color="#a020f0">AS</font> <br> <font color="#a020f0">BEGIN</font> ; <br> &lt;..&gt; <br> <font color="#a020f0">BEGIN AUTONOMOUS</font> <br> &lt;..&gt; <br> <font color="#a020f0">END</font> ; <br> <font color="#a020f0">END</font> ; <br></code> <br>  Accordingly, the isolation level cannot be set; it is determined by the level of the parent transaction, and if it is not explicitly specified, then the default level. <br><br>  Let's give an example, which is considered one of the classic commercial DBMS in the world.  In some bank in the customer_info table stores customer data, their debts <br><br> <code><font color="#a020f0">CREATE</font> <font color="#a020f0">TABLE</font> <font color="#0000ff">customer_info</font> (acc_id <font color="#228b22">int</font> , acc_debt <font color="#228b22">int</font> ); <br> <font color="#a020f0">INSERT</font> <font color="#a020f0">INTO</font> <font color="#0000ff">customer_info</font> <font color="#a020f0">VALUES</font> (1, 1000),(2, 2000);</code> <br> <br>  Let this table be unavailable directly to a bank employee.  However, they have the opportunity to check the debts of customers using the functions available to them: <br><br> <code><font color="#a020f0">CREATE OR REPLACE FUNCTION</font> <font color="#0000ff">get_debt</font> (cust_acc_id <font color="#228b22">int</font> ) <font color="#a020f0">RETURNS</font> <font color="#228b22">int</font> <font color="#a020f0">AS</font> <br> $$ <br> <font color="#a020f0">DECLARE</font> <br> debt <font color="#228b22">int</font> ; <br> <font color="#a020f0">BEGIN</font> <br> <font color="#a020f0">PERFORM</font> log_query( <font color="#483d8b">CURRENT_USER</font> :: <font color="#228b22">text</font> , cust_acc_id, now()); <br> <font color="#a020f0">SELECT</font> acc_debt <font color="#a020f0">FROM</font> customer_info <font color="#a020f0">WHERE</font> acc_id = cust_acc_id <font color="#a020f0">INTO</font> debt; <br> <font color="#a020f0">RETURN</font> debt; <br> <font color="#a020f0">END</font> ; <br> $$ <font color="#a020f0">LANGUAGE</font> plpgsql;</code> <br> <br>  Before you look at the client data, the function records the DBMS user name, client account number and operation time in the log table: <br><br> <code><font color="#a020f0">CREATE TABLE</font> <font color="#0000ff">log_sensitive_reads</font> (bank_emp_name <font color="#228b22">text</font> , cust_acc_id <font color="#228b22">int</font> , query_time timestamptz); <br> <br> <font color="#a020f0">CREATE OR REPLACE FUNCTION</font> <font color="#0000ff">log_query</font> (bank_usr <font color="#228b22">text</font> , cust_acc_id <font color="#228b22">int</font> , query_time <font color="#228b22">timestamptz</font> ) <font color="#a020f0">RETURNS</font> void <font color="#a020f0">AS</font> <br> $$ <br> <font color="#a020f0">BEGIN</font> <br> <font color="#a020f0">INSERT INTO</font> <font color="#0000ff">log_sensitive_reads</font> <font color="#a020f0">VALUES</font> (bank_usr, cust_acc_id, query_time); <br> <font color="#a020f0">END</font> ; <br> $$ <font color="#a020f0">LANGUAGE</font> plpgsql;</code> <br> <br>  We want the employee to have the opportunity to inquire about the debts of the client, but in order not to encourage idle or malicious curiosity, we want to always see traces of his activities in the log. <br><br>  A curious employee will execute commands: <br><br> <code><font color="#a020f0">BEGIN</font> ; <br> <font color="#a020f0">SELECT</font> <font color="#0000ff">get_debt</font> (1); <br> <font color="#a020f0">ROLLBACK</font> ;</code> <br> <br>  In this case, information about its activities will be rolled back along with rollback of the entire transaction.  Since this does not suit us, we modify the logging function: <br><br> <code><font color="#a020f0">CREATE OR REPLACE FUNCTION</font> <br> <font color="#0000ff">log_query</font> (bank_usr <font color="#228b22">text</font> , cust_acc_id <font color="#228b22">int</font> , query_time <font color="#228b22">timestamptz</font> ) <font color="#a020f0">RETURNS</font> void <font color="#a020f0">AS</font> <br> $$ <br> <font color="#a020f0">BEGIN</font> <br> <font color="#a020f0">BEGIN AUTONOMOUS <br> INSERT</font> <font color="#a020f0">INTO</font> <font color="#0000ff">log_sensitive_reads</font> <font color="#a020f0">VALUES</font> (bank_usr, cust_acc_id, query_time); <br> <font color="#a020f0">END</font> ; <br> <font color="#a020f0">END</font> ; <br> $$ <font color="#a020f0">LANGUAGE</font> plpgsql;</code> <br> <br>  Now, no matter how hard an employee tries to cover his tracks, all his views of client data will be logged. <br><br>  Autonomous transactions the most convenient debugging tool.  The doubtful part of the code will have time to write a debugging message before the unsuccessful transaction is rolled back: <br><br> <code><font color="#a020f0">BEGIN AUTONOMOUS <br> INSERT</font> <font color="#a020f0">INTO</font> <font color="#0000ff">test</font> (msg) <font color="#a020f0">VALUES</font> ( <font color="#8b2252">'STILL in DO cycle. after pg_background call: '</font> ||clock_timestamp():: <font color="#228b22">text</font> ); <br> <font color="#a020f0">END</font> ;</code> <br> <br>  In conclusion about performance.  We tested our implementation of autonomous transactions against the same SQL without autonomous transactions, with a bare <code>dblink</code> , a <code>dblink</code> combination with <code>pgbouncer</code> and a connection control. <br><br>  The <code>pg_background</code> creates three functions: <code>pg_background_launch(query)</code> starts the background process <code>background worker</code> , which will execute the transferred SQL function;  <code>pg_background_result(pid)</code> gets the result from the process created by <code>pg_background_launch(query)</code> and <code>pg_background_detach(pid)</code> detaches the background process from its creator.  The code that executes the transaction is not very intuitive: <br><br> <code><font color="#a020f0">PERFORM</font> * <font color="#a020f0">FROM</font> <font color="#0000ff">pg_background_result(pg_background_launch</font> (query)) <br> <font color="#a020f0">AS</font> (result <font color="#228b22">text</font> );</code> <br> <br>  But more significantly, as expected, creating a process for each SQL is slow.  From the pg_background creation history, it is known that the fourth function <code>pg_background_run(pid, query)</code> proposed, which passes a new task to an already running process.  In this case, the time to create the process will not be spent on each SQL, but this function is not available in the current implementation. <br><br>  <i>Robert Haas</i> , who created the first version of <code>pg_background</code> , says: <br>  <i>‚ÄúI am skeptical about this approach [to simulate autonomous transactions using <code>pg_background]</code> .</i> <i> </i>  ,   <i></i>   <i>,  ,             <code>[backend]</code> ,      <code>[background_workers]</code> .             <code>[max_worker_processes]</code> ,   ,  ,  ,       , , ,    ¬ª.</i> <br><br>        :     ,    ,    ,   .     <i></i> , ,  <code>pg_background</code>   6-7  ,     <code>Postgres Pro Enterprise</code> . <br><br><img src="https://habrastorage.org/web/cf9/b7d/8b3/cf9b7d8b3a4546c0819b8d45a9d537c1.jpg"><br>  <i>Fig.</i> <i>3.     .      pgbech  <code>INSERT</code>   <code>pgbench_history</code> .        10. TPS  ¬´¬ª SQL   100.</i> <br><br>  Ps. <b><i> !</i></b> <br><br> PPS. <b>            !</b> </div><p>Source: <a href="https://habr.com/ru/post/337180/">https://habr.com/ru/post/337180/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../337170/index.html">The book "Continuous delivery. The practice of continuous updates "</a></li>
<li><a href="../337172/index.html">IT events digest for September</a></li>
<li><a href="../337174/index.html">How to protect yourself from encryptors on the perimeter of the network</a></li>
<li><a href="../337176/index.html">According to Rambler.iOS # 9</a></li>
<li><a href="../337178/index.html">How to stop worrying and start talking</a></li>
<li><a href="../337182/index.html">Greetings to Yandex developers</a></li>
<li><a href="../337184/index.html">Current data on the telephone codes of Russian cities</a></li>
<li><a href="../337186/index.html">Epidemic of the Petya blackmailer virus: what you need to know</a></li>
<li><a href="../337188/index.html">7 rules of good tone when writing Unit tests</a></li>
<li><a href="../337190/index.html">CaptureManager SDK</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>