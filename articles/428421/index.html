<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>A flexible system for testing and collecting metrics of programs on the example of LLVM test-suite</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Introduction 
 Most developers have clearly heard about quite significant open-source developments such as the LLVM system and the clang compiler. How...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>A flexible system for testing and collecting metrics of programs on the example of LLVM test-suite</h1><div class="post__text post__text-html js-mediator-article"><h2>  Introduction </h2><br>  Most developers have clearly heard about quite significant open-source developments such as the LLVM system and the clang compiler.  However, LLVM is now not only directly the system itself to create compilers, but already a large ecosystem that includes many projects to solve various problems arising in the process of any stage of compiler creation (usually each such project has its own separate repository).  Part of the infrastructure naturally includes tools for testing and benchmarking, because  when developing a compiler, its efficiency is a very important indicator.  One of such individual projects of the LLVM test infrastructure is the test suite ( <a href="https://llvm.org/docs/TestSuiteGuide.html">official documentation</a> ). <br><br><h2>  LLVM test suite </h2><br>  At first glance at the test-suite repository, it seems that this is just a set of benchmarks in C / C ++, but this is not quite so.  In addition to the source code of the programs that will measure performance, the test suite includes a flexible infrastructure for building, running, and collecting metrics.  By default, it collects the following metrics: compile time, execution time, link time, code size (by sections). <br><a name="habracut"></a><br>  Test suite is naturally useful when testing and benchmarking compilers, but it can also be used for some other research tasks where some C / C ++ code base is needed.  Those who once attempted to do something in the field of data analysis, I think, faced with the problem of lack and separation of the source data.  A test-suite, though not composed of a huge number of applications, but has a unified data collection mechanism.  It is very easy to add your own applications to the set, to collect the metrics necessary for your task.  Therefore, in my opinion, the test suite (besides the main task of testing and benchmarking) is a good option for a basic project, on the basis of which you can build your data collection for tasks where you need to analyze some features of the program code or some characteristics of the programs. <br><br><h3>  LLVM test-suite structure </h3><br><pre><code class="cmake hljs"><span class="hljs-keyword"><span class="hljs-keyword">test</span></span>-suite |----CMakeLists.txt //  CMake ,   ,  | //   .. | |---- cmake | |---- .modules //        , | //   API    | |---- litsupport //  Python,      <span class="hljs-keyword"><span class="hljs-keyword">test</span></span>-suite, | //    lit (  LLVM) | |---- tools //   :    | //     (    | // ),    .. | | //     | |---- SingleSource //   ,       | // .        . | |---- MultiSource //   ,      | //  .        | //  . | |---- MicroBenchmarks // ,   google-benchmark.   | //  ,    ,  | //       | |---- External //    ,     <span class="hljs-keyword"><span class="hljs-keyword">test</span></span>-suite,  | // ,     (  ) | // -   </code> </pre> <br>  The structure is simple and straightforward. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  Principle of operation </h3><br>  As you can see, CMake and a special lit-test format are responsible for all the work on assembling, launching and collecting metrics. <br><br>  If we consider it very abstract, it is clear that the process of benchmarking using this system looks simple and very predictable: <br><img src="https://habrastorage.org/webt/5p/ny/s8/5pnys8slvpcfavbmriofq4lymrs.jpeg"><br><br>  How does this look in more detail?  In this article, I would like to focus on exactly what role CMake plays in the entire system and what is the only file you need to write if you want to add something to this system. <br><br>  <b>1. Building test applications.</b> <br><br>  As a build system, it is already used as a standard for C / C ++ CMake programs.  CMake configures the project and generates make, ninja, etc., depending on the user's preferences.  for direct construction. <br>  However, in the test suite CMake not only generates rules for how to build applications, but also configures the tests themselves. <br><br>  After launching CMake, another files will be written to the build directory (with the .test extension) with a description of how the application should be executed and checked for correctness. <br><br>  An example of the most standard .test file <br><br><pre> <code class="cmake hljs">RUN: cd &lt;some_path_to_build_directory&gt;/MultiSource/Benchmarks/Prolangs-C/football ; &lt;some_path_to_build_directory&gt;/MultiSource/Benchmarks/Prolangs-C/football/football VERIFY: cd &lt;some_path_to_build_directory&gt;/MultiSource/Benchmarks/Prolangs-C/football ; &lt;some_path_to_build_directory&gt;/tools/fpcmp %o football.reference_output</code> </pre><br>  The file with the .test extension may contain the following sections: <br><br><ul><li>  PREPARE - describes any actions that must be done before launching the application, very similar to the Before method that exists in different unit-testing frameworks; </li><li>  RUN - describes how to start the application; </li><li>  VERIFY - describes how to check the correctness of the application; </li><li>  METRIC - describes the metrics that need to be collected in addition to the standard. </li></ul><br>  Any of these sections may be omitted. <br><br>  But since this file is automatically generated, it is in the CMake file for the benchmark that describes how to get the object files, how to assemble them into the application, and then what to do with this application. <br><br>  For a better understanding of the default behavior and how it is described, consider an example of some CMakeLists.txt <br><br><pre> <code class="cmake hljs"><span class="hljs-keyword"><span class="hljs-keyword">list</span></span>(APPEND CFLAGS -DBREAK_HANDLER -DUNICODE-pthread) <span class="hljs-comment"><span class="hljs-comment">#      (         ..     CMak,       ) list(APPEND LDFLAGS -lstdc++ -pthread) #      </span></span></code> </pre><br>  Flags can be set depending on the platform, the test-suite cmake modules include the DetectArchitecture file, which defines the target platform on which benchmarks are run, so you can simply use the data already collected.  Other data is also available: operating system, byte order, etc. <br><br><pre> <code class="cmake hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(TARGET_OS <span class="hljs-keyword"><span class="hljs-keyword">STREQUAL</span></span> <span class="hljs-string"><span class="hljs-string">"Linux"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">list</span></span>(APPEND CPPFLAGS -DC_LINUX) <span class="hljs-keyword"><span class="hljs-keyword">endif</span></span>() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">NOT</span></span> ARCH <span class="hljs-keyword"><span class="hljs-keyword">STREQUAL</span></span> <span class="hljs-string"><span class="hljs-string">"ARM"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(ENDIAN <span class="hljs-keyword"><span class="hljs-keyword">STREQUAL</span></span> <span class="hljs-string"><span class="hljs-string">"little"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">list</span></span>(APPEND CPPFLAGS -DFPU_WORDS_BIGENDIAN=<span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">endif</span></span>() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(ENDIAN <span class="hljs-keyword"><span class="hljs-keyword">STREQUAL</span></span> <span class="hljs-string"><span class="hljs-string">"big"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">list</span></span>(APPEND CPPFLAGS -DFPU_WORDS_BIGENDIAN=<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">endif</span></span>() <span class="hljs-keyword"><span class="hljs-keyword">endif</span></span>()</code> </pre><br>  In principle, in this part there should be nothing new for people who at least once saw or write a simple CMake file.  Naturally, you can use libraries, build them yourself, in general, use any means provided by CMake to describe the process of building your application. <br><br>  And then you need to ensure the generation of the .test file.  What tools does the tets-suite interface provide for this? <br><br>  There are 2 basic macros, <b>llvm_multisource</b> and <b>llvm_singlesource</b> , which are enough for most trivial cases. <br><br><ul><li>  <b>llvm_multisource is</b> used if the application consists of several files.  If you do not pass the source code files as parameters when calling this macro in your CMake, then all source code files in the current directory will be used as the base for building.  In fact, changes are currently taking place in the interface of this macro in the test suite, and the described method of transferring source files as macro parameters is the current version located in the master branch.  Previously, there was another system: source code files should have been written to the Source variable (it was still in release 7.0), and the macro did not accept any parameters.  But the basic logic of the implementation remained the same. </li><li>  <b>llvm_singlesource</b> assumes that each .c / .cpp file is a separate benchmark and for each compiles a separate executable file. </li></ul><br>  By default, both macros described above for launching a built application generate a command that simply invokes this application.  A validation check is performed by comparing with the expected output found in the file with the extension .reference_output (also with possible suffixes .reference_output.little-endian, .reference_output.big-endian). <br><br>  If this suits you, it is simply wonderful, one additional line (call llvm_multisource or llvm_singlesource) is enough for you to start the application and get the following metrics: code size (by sections), compile time, link time, execution time. <br><br>  But naturally, rarely everything is so smooth.  You may need to change one or more stages.  And this is also possible with the help of simple actions.  The only thing you need to remember is that if you override a certain stage, you need to describe all the others (even if the default algorithm suits them, which, of course, is a little upset). <br><br>  In the API, there are macros to describe the actions at each stage. <br><br>  For the preparatory stage, the <b>llvm_test_prepare</b> macro has <b>nothing</b> to write about, the commands that need to be executed are simply passed as a parameter. <br><br>  What may be needed in the launch section?  The most predictable case is that the application accepts some arguments, input files.  For this, there is the macro <b>llvm_test_run</b> , which accepts only application launch arguments (without the name of the executable file) as parameters. <br><br><pre> <code class="cmake hljs">llvm_test_run(--fixed <span class="hljs-number"><span class="hljs-number">400</span></span> --cpu <span class="hljs-number"><span class="hljs-number">1</span></span> --num <span class="hljs-number"><span class="hljs-number">200000</span></span> --seed <span class="hljs-number"><span class="hljs-number">1158818515</span></span> run.hmm)</code> </pre><br>  To change actions at the stage of verification of correctness, the <b>llvm_test_verify</b> macro is <b>used</b> , which accepts any commands as parameters.  Of course, to check the correctness it is better to use the tools included in the tools folder.  They provide quite good opportunities for comparing the generated output with the expected one (there is a separate processing for comparing real numbers with a certain error, etc.).  But you can somewhere and just check that the application has completed successfully, etc. <br><br><pre> <code class="cmake hljs">llvm_test_verify(<span class="hljs-string"><span class="hljs-string">"cat %o | grep -q 'exit 0'"</span></span>) <span class="hljs-comment"><span class="hljs-comment"># %o -   placeholder   ,   lit.          lit,    ,    .    lit (  ,   LLVM)      (   &lt;a href="https://llvm.org/docs/CommandGuide/lit.html"&gt; &lt;/a&gt;)</span></span></code> </pre><br>  And what if there is a need to collect some additional metrics?  For this, there is a macro <b>llvm_test_metric</b> . <br><br><pre> <code class="cmake hljs">llvm_test_metric(METRIC &lt; &gt; &lt;,   &gt;)</code> </pre><br>  For example, for dhrystone you can get a metric specific to it. <br><br><pre> <code class="cmake hljs">llvm_test_metric(METRIC dhry_score grep 'Dhrystones per Second' %o | awk '{print $<span class="hljs-number"><span class="hljs-number">4</span></span>}')</code> </pre><br>  Of course, if you need to collect additional metrics for all tests, this method is somewhat inconvenient.  You must either add the llvm_test_metric call to the high-level macros provided by the interface, or you can use TEST_SUITE_RUN_UNDER (CMake variable) and a specific script to collect metrics.  The variable TEST_SUITE_RUN_UNDER is quite useful, and can be used, for example, to run on simulators, etc.  In fact, a command is written to it that will accept an application with its arguments as input. <br><br>  As a result, we get some CMakeLists.txt <br><br><pre> <code class="cmake hljs"><span class="hljs-comment"><span class="hljs-comment">#       llvm_test_run(--fixed 400 --cpu 1 --num 200000 --seed 1158818515 run.hmm) llvm_test_verify("cat %o | grep -q 'exit 0'") llvm_test_metric(METRIC score grep 'Score' %o | awk '{print $4}') llvm_multisource() # llvm_multisource(my_application)   </span></span></code> </pre><br>  The integration does not require additional efforts, if the application is already assembled using CMake, then in CMakeList.txt in the test suite you can enable an already existing CMake for building and add a few simple macro calls. <br><br>  <b>2. Running tests</b> <br><br>  As a result of his work, CMake generated a special test file according to the given description.  But how is this file executed? <br><br>  lit always uses some configuration file lit.cfg, which, respectively, exists in the test suite.  This configuration file contains various settings for running tests, including the format of executable tests.  Test suite uses its own format, which is located in the litsupport folder. <br><br><pre> <code class="python hljs">config.test_format = litsupport.test.TestSuiteTest()</code> </pre> <br>  This format is described as a test class inherited from the standard lit-test and overrides the main interface method execute.  Also important components of litsupport are the class with the description of the TestPlan test plan, which stores all the commands that must be executed at different stages and knows the order of the stages.  To provide the necessary flexibility, the architecture also includes modules that should provide the mutatePlan method, within which they can modify the test plan, adding the description of collecting the necessary metrics, adding additional commands to measure the time to launch the application, etc.  Due to the similar decision the architecture well extends. <br><br><img src="https://habrastorage.org/webt/a_/s4/6y/a_s46ygow0ah-bmfzx9jkezdysg.png"><br><br>  An approximate scheme of the test-suite test (except for details in the form of TestContext classes, various lit configurations and the tests themselves, etc.) is presented below. <br><br><img src="https://habrastorage.org/webt/5b/k9/1a/5bk91asgtn8ivhsr4pehhczdgrw.png"><br><br>  Lit causes the execution of the test type specified in the configuration file.  TestSuiteTest parses the generated CMake test file, getting a description of the main stages.  Then all the found modules are called to change the current test plan, the launch is instrumented.  Then the obtained test plan is executed: executed in the order of the preparation, launch, and validation stages.  If necessary, profiling can be performed (added by one of the modules, if a variable is set during configuration that indicates the need for profiling).  The next step is to collect metrics, functions for which collection were added by standard modules in the metric_collectors field in TestPlan, and then additional metrics are collected that are described by the user in CMake. <br><br>  <b>3. Run the test suite</b> <br><br>  Running the test suite is possible in two ways: <br><br><ul><li>  Manual, i.e.  sequential invocation of commands. <pre> <code class="bash hljs">cmake -DCMAKE_CXX_COMPILER:FILEPATH=clang++ -DCMAKE_C_COMPILER:FILEPATH=clang <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-suite <span class="hljs-comment"><span class="hljs-comment">#  make #   llvm-lit . -o &lt;output&gt; #  </span></span></code> </pre></li><li>  using LNT (another system from the LLVM ecosystem that allows you to run benchmarks, save the results in the database, analyze the results in the web interface).  LNT, within its test run team, performs the same steps as in the previous paragraph. <br><pre> <code class="bash hljs">lnt runtest <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-suite --sandbox SANDBOX --cc clang --cxx clang++ --<span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-suite <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-suite</code> </pre> </li></ul><br>  The result for each test is displayed as <br><br><pre> <code class="bash hljs">PASS: <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-suite :: MultiSource/Benchmarks/Prolangs-C/football/football.test (m of n) ********** TEST <span class="hljs-string"><span class="hljs-string">'test-suite :: MultiSource/Benchmarks/Prolangs-C/football/football.test'</span></span> RESULTS ********** compile_time: 1.1120 exec_time: 0.0014 <span class="hljs-built_in"><span class="hljs-built_in">hash</span></span>: <span class="hljs-string"><span class="hljs-string">"38254c7947642d1adb9d2f1200dbddf7"</span></span> link_time: 0.0240 size: 59784 size..bss: 99800 ‚Ä¶ size..text: 37778 **********</code> </pre><br>  The results from different launches can be compared without LNT (although this framework provides great opportunities for analyzing information using different tools, but it needs a separate review), using the script included in the test suite <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-suite/utils/compare.py results_a.json results_b.json</code> </pre> <br>  An example of comparing the size of the code of the same benchmark from two launches: with the -O3 and -Os flags <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-suite/utils/compare.py -m size SANDBOX1/build/O3.json SANDBOX/build/Os.json Tests: 1 Metric: size Program O3 Os diff <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-suite...langs-C/football/football.test 59784 47496 -20.6%</code> </pre><br><h2>  Conclusion </h2><br>  The infrastructure for describing and launching benchmarks implemented in the test suite is easy to use and support, it scales well, and in principle, in my opinion, uses elegant solutions in its architecture, which, of course, makes test suite a very useful tool for developers compilers, as well as this system can be improved for use in some data analysis tasks. </div><p>Source: <a href="https://habr.com/ru/post/428421/">https://habr.com/ru/post/428421/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../428411/index.html">Radar technology: a list of languages, tools and platforms that have passed through the hands of Lamoda</a></li>
<li><a href="../428413/index.html">Cooling systems in data centers Selectel</a></li>
<li><a href="../428415/index.html">TP-Link Omada OC200 Cloud Controller Overview</a></li>
<li><a href="../428417/index.html">MatLab / Octave machine learning: examples of algorithms supported by formulas</a></li>
<li><a href="../428419/index.html">Drag and Swipe in RecyclerView. Part 2: Drag and Drop Controllers, Grid, and Custom Animations</a></li>
<li><a href="../428423/index.html">How the IBM and Red Hat deal at $ 34 billion will change the IT market: expert and analyst opinions</a></li>
<li><a href="../428429/index.html">Electronic signature GOST R 34.10 PDF documents in the LibreOffice office suite</a></li>
<li><a href="../428431/index.html">More than concentric layers</a></li>
<li><a href="../428433/index.html">Lawyers about testing when hiring in the private sector</a></li>
<li><a href="../428435/index.html">What to read in PHP in Russian?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>