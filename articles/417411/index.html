<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Evaluate the developer based on objective data.</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Unfortunately, we do not live in an ideal world, where every developer has an ideal and balanced level of performance, while focusing on tasks and thi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Evaluate the developer based on objective data.</h1><div class="post__text post__text-html js-mediator-article">  Unfortunately, we do not live in an ideal world, where every developer has an ideal and balanced level of performance, while focusing on tasks and thinking through them in and out.  Team interaction is also not always arranged in such a way that all team members work with maximum efficiency.  As with many problems in general, early diagnostics in a development team can save resources, lead nerves and create a good working atmosphere. <br><br>  In a small team, a team leader may try to judge everything on the basis of subjective feelings, but the larger the company, the more important it is to use objective data and metrics.  <strong>Alexander Kiselev</strong> ( <a href="https://habr.com/users/aleksandrkiselev/" class="user_link">AleksandrKiselev</a> ) and <strong>Sergey Semenov</strong> in their report on <a href="http://teamleadconf.ru/">TeamLead Conf</a> showed how to use the data that you have already accumulated, where to get additional data, and that all of them together can help to identify non-obvious problems.  And even, having accumulated the experience of many colleagues, they offered solutions. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/BF740kkXTvI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <strong>About speakers:</strong> Alexander Kiselev and Sergey Semenov in IT, we have more than 8 years.  Both have gone from a developer to a timlid and on to a product manager.  They are currently working on the GitLean analytical service, which automatically collects analytics from team development teams and CTOs.  The goal of this service is to enable technical managers to make their decisions based on objective data. <br><a name="habracut"></a><br><h2>  Formulation of the problem <br></h2><br>  We both worked as tmlides and often faced with the problem of uncertainty and ambiguity in our work. <br><img src="https://habrastorage.org/webt/_b/oc/cb/_boccbltl1wgqu2dckbdzevfvvk.jpeg">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      As a result, it was necessary to make decisions quite often blindly, and sometimes it was not clear whether it became better or worse.  Therefore, we looked at the existing solutions on the market, examined the methodologies for evaluating developer performance, and realized that there is no service that would satisfy our needs.  Therefore, <strong>we decided to create it ourselves</strong> . <br><br>  Today we will talk about what you can tell the data that you have already accumulated, but most likely do not use. <br><img src="https://habrastorage.org/webt/yt/0e/au/yt0eaumuz05vl_1vhmyo2rrbaji.jpeg"><br><br>  This is necessary in two main cases. <br><br>  <strong>Performance review is a</strong> rather complicated and subjective process.  It would be great to collect facts about the work of the developer automatically. <br><br>  We talked with representatives of a large German company with a large development staff.  About once a year, they stopped all development work at all for 2 weeks, and only did that the whole company conducted a performance review ‚Äî the developers wrote anonymous denunciations all the day to their colleagues, with whom they had worked for the year.  If this company had the opportunity to gather facts automatically, they would save a lot of time for themselves. <br><br>  The second aspect is <strong>monitoring the current situation in the team.</strong>  I want to quickly understand the problems that arise, and respond promptly to them. <br><br><h2>  Solutions <br></h2><br>  There may be several solutions. <br><img src="https://habrastorage.org/webt/3c/oa/aq/3coaaqtqnrpp-st-ajnjfjfz0uo.jpeg"><br><br>  First, you can <strong>never use any analytics</strong> , but only your subjective assessment.  This works if you are a team leader in a small team.  But if you are already a CTO, and you have a lot of teams, then you will not be able to use your subjective assessment, because you do not know everything.  You will have to resort to the subjective assessment of your timlids, and this is a problem, since quite often the tmlids completely differently approach the subjective assessment. <br><img src="https://habrastorage.org/webt/kq/sf/rv/kqsfrv2kw9zi6x8oxs-yy7xwf1k.jpeg"><br><br>  This is the next thing to do.  Since the subjective assessment is often not enough, you can <strong>get</strong> stuck and <strong>collect the facts by hand</strong> . <br><br>  For example, one CTO with which we spoke, somehow suspected the team that they were doing a code review too slowly, but there was nothing to show them.  Since he had only a vague feeling, he decided to collect the facts, just to watch the team for a couple of weeks.  The CTO put the time of the review of the team on the plate, and what he found in the end was just a shock.  It turned out that the 2 seigneurs had been in conflict with the code review for quite a long time, and they didn‚Äôt take it outside at all.  They sat like mice, no one shouted at anyone - the team was not aware of it at all.  The only thing they did was periodically going to the cooler, pouring some more water on themselves and running to write witty answers in code review to their enemy in pull-request. <br><br>  When CTO found out, it turned out that the problem was so old that it was impossible to do anything, and as a result I had to dismiss one of the programmers. <br><img src="https://habrastorage.org/webt/5w/tj/pj/5wtjpjcjmo7zrluzs10imywuiua.jpeg"><br><br>  <strong>Statistics for Jira</strong> - a variant that is often used.  This is a very useful tool, in which there is information about the tasks, but it is quite high-level.  It is often difficult to understand what is happening in a team specifically. <br><br>  A simple example - the developer in the previous sprint did 5 tasks, in this - 10. Is it possible to say that he began to work better?  It is impossible, because the tasks are completely different. <br><img src="https://habrastorage.org/webt/n5/7f/gr/n57fgrtcdghpsht4eqecapnxstk.jpeg"><br><br>  The last solution, which is - just roll up your sleeves and write <strong>your own script for automatic data collection</strong> .  This is the way that all CTOs more or less come to in large companies.  He is the most productive, but, naturally, the most difficult.  It is about him we will talk today. <br><br><h2>  Selected solution <br></h2><br>  So, the chosen solution is to file your own scripts for analytics collection.  The main questions are where to get the data and what to measure. <br><br><h3>  Data sources <br></h3><br>  The main data sources in which information about the work of a developer is accumulated are: <br><br><ol><li>  <strong>Git</strong> - the main entities: commits, branches and code inside them. </li><li>  <strong>Code Review Tools</strong> - Git hosting services that run code review store pull-request information that can be used. </li><li>  <strong>Task trackers</strong> - information about tasks and their life cycle. </li></ol><br>  Auxiliary data sources: <br><br><ol><li>  <strong>Messengers</strong> - there you can, for example, carry out sentiment analysis, consider the average developer response time to a request for information. </li><li>  <strong>CI services</strong> that store information about builds and releases. </li><li>  <strong>Team polls.</strong> </li></ol><br>  Since all the sources I have described above are more or less standard, and the latter is not so standard, I‚Äôll tell you a little more about it. <br><img src="https://habrastorage.org/webt/lc/fx/7q/lcfx7qdvbqptymjrw_1fumfayhc.jpeg"><br><br>  This method is shared with us by another CTO.  At the end of each iteration, he automatically sent a survey to the team, in which there were only 2 questions: <br><br><ol><li>  How do you think that what we did in this iteration was important? </li><li>  Do you think that what we are doing was interesting? </li></ol><br>  This is quite a cheap way to measure the mood in a team and, perhaps, to catch some problems with motivation. <br><br><h3>  What and how to measure <br></h3><br>  First of all, let's discuss the measurement methodology.  A good metric should answer 3 questions: <br><br><ol><li>  <strong>Is this important?</strong>  You need to measure only what signals something significant for the company. </li><li>  <strong>It became worse / better / the same?</strong>  By metric it should be crystal clear whether it has become better or worse. </li><li>  <strong>What to do?</strong>  From the metric it should be clear what to do in order to correct the situation. </li></ol><br>  In general, it is necessary to follow the principle: <br><br><blockquote>  Measure what you want and can change. </blockquote><br>  It is worth making a reservation right away that there is no universal metric, and we will not speak about the universal metric today for the following reasons: <br><br><ul><li>  A developer has <strong>many aspects of activity</strong> - he works with requirements, writes code, tests, runs code review, makes it warm - and it‚Äôs impossible to stuff all this into a single universal metric.  Therefore, it is better to focus on individual cases that can be detected. <br></li><li>  The second reason why the only metric should not be done is to easily get around one metric, because the developers are smart enough people, and they will figure out how to do it alone. </li></ul><br><img src="https://habrastorage.org/webt/vy/q-/30/vyq-30vyjgqo4zdzz69kprxnap8.jpeg"><br><br><h2>  New approach <br></h2><br>  Therefore, we formulated an approach in which we go from problems: we try to identify specific problems and select a set of metrics for them that will detect them.  A good developer is a developer with the fewest problems. <br><img src="https://habrastorage.org/webt/vf/uw/sj/vfuwsj9nasynifgojqhpqi_e6y4.jpeg"><br><br>  What is the basis of our choice of problems?  It's simple: we conducted interviews with 37 CTOs and timblids who talked about the problems that they have in the teams, and how they solve these problems. <br><br>  We prioritized the resulting huge list and collected life hacks and metrics for these problems.  All the problems we have divided into 2 groups: <br><br><ol><li>  Problems of an individual developer (the developer is responsible for these problems). <br><img src="https://habrastorage.org/webt/j2/7r/wc/j27rwcdo_9byklbwogxfvlrsz2g.jpeg"><br></li><li>  Team problems.  The team is responsible for these problems, respectively, in order to solve them, you need to work as a whole with the team and change process decisions. <br><img src="https://habrastorage.org/webt/qk/hs/vi/qkhsvia6d_svbdrqkgrpb3jdeuw.jpeg"><br></li></ol><br>  Let us consider in detail each problem, which key from the metrics to it can be selected.  Let's start with the simplest problems and slowly move along the gradient of complexity to the most difficult to measure. <br><br><h3>  Developer Issues </h3><br><h5>  <em>The developer performs a little</em> </h5><br><img src="https://habrastorage.org/webt/el/wy/p8/elwyp8z-ibmscajp_ogprvktpl8.jpeg"><br><br>  Moreover, under the "little performance" usually means that the <strong>developer does almost nothing</strong> .  Conventionally, it hangs a ticket in Jira, he somehow reports on it, but really no work is happening.  It is clear that this problem will emerge sooner or later, you will find it, but it would be cool to do it automatically. <br><br>  <strong>How can this be measured?</strong> <br><br>  The first thing that comes to mind is just to look at the <strong>number of active days</strong> with the developer.  We will call the active day the day when the developer made at least one commit.  For full-time developers, in fact, the characteristic number of active days per week is not less than 3. If less, then we begin to suspect the developer that he performs little. <br><br>  Obviously, only the number of active days is not enough.  The developer could simply write code and not commit it - he wrote, he wrote, and then one fine day he commited a bunch of code. <br><br>  Therefore, the following restriction that we impose is that the developer should also have <strong>little code</strong> .  How to determine the threshold "little code"?  We recommend putting it small enough so that anyone, at least as much as a performer developer, can easily overcome it.  For example, in our service for JS, this is around 150 lines of code, and for Clojure, 100 lines of code. <br><br>  Why such a small threshold?  The idea is that we want to separate non-cool developers from average ones, and those who do almost nothing from those who do at least some sane amount of work. <br><br>  But even if the developer has few active days and little code, this does not mean that he did not work.  He could, for example, make bug fixes that require a small amount of code.  As a result, a person seems to have done a lot of tasks, but he may have few code and active days.  That is, we take into account the <strong>number of tasks</strong> . <br><br>  The next thing to watch out for is the <strong>amount of code review</strong> that he did, because the person could not do the task and not write the code, but at the same time be completely immersed in the code review.  This happens. <br><br>  Therefore, if for all of these metrics - and only this way!  - the developer does not reach any thresholds, then you can suspect him that he performs a little. <br><br>  <strong>What to do with it?</strong> <br><br>  Firstly, if you know the legitimate reason, then you don‚Äôt need to do anything at all - for example, the developer is undergoing training or he has day off.  If you do not know the legitimate reason, then you should probably <strong>talk</strong> to someone.  If the legitimate reason does not appear, then it is necessary to monitor it further, and if this problem continues to repeat itself sometimes, then, probably, such a developer should say goodbye. <br><br>  It was the simplest and most provocative problem.  We turn to the more severe. <br><br><h5>  <em>Developer recycles</em> <br></h5><br><img src="https://habrastorage.org/webt/bg/ts/jb/bgtsjbdtsakvrekrboad2ohblfo.jpeg"><br><br>  This is also a common story.  If a person recycles, he burns out, eventually demotivating and, as a result, may leave the company.  One of the technical managers with whom we spoke, told the following story.  He worked in an American company, in which the culture of rallies was wildly developed.  As a result, all the developers, coming to work, did nothing but protest, and they wrote the code during off-hours and on weekends.  As a result, the annual turnover of developers in the company reached 30%, although in industry the rate is 6%. <br><br>  As a result, the entire technical management of 30 people was dismissed from this office.  To not bring this up, I want to detect this problem in time. <br><br>  <strong>How can this be measured?</strong> <br><br>  In fact, too, nothing complicated - let's look at the <strong>amount of code that the developer writes during off-hours.</strong>  If this amount of code is conditionally comparable or greater than what it does during working hours, then the developer explicitly recycles. <br><br>  Obviously, developers are not the only code that lives.  The frequent problem is that there is enough time for the code - the main work - and there is no longer a code review.  As a result, the code review is transferred to evenings or weekends.  This can be tracked simply by the <strong>number of comments in a pull-request</strong> after <strong>hours</strong> . <br><br>  The last explicit trigger is a <strong>large number of parallel tasks</strong> .  There is a reasonable limit of 3-4 tasks for a developer.  You can track them on git or on Jira - as you like.  It works well. <br><br>  <strong>What to do with it?</strong> <br><br>  If you find a processing developer, you should first <strong>check his calendar</strong> to see if he is not overloaded with useless meetings.  If overloaded, it is desirable to reduce them, and ideally to make a meeting day - a dedicated day, when the developer will concentrate most of his longest meetings, so that on other days he can work normally. <br><br>  If this does not work, you need to <strong>redistribute the load</strong> .  This is actually quite a difficult question - how to do it.  There are many different ways.  We will not go deep, but note the steep <a href="https://youtu.be/gB-wOd2EF2Q">report</a> on HighLoad 2017 from Anton Potapov, in which this topic was very closely considered. <br><br><h5>  <em>The developer has no focus on the release of tasks</em> <br></h5><br>  I want to understand how many such developers are in your team and how much it costs in time. <br><img src="https://habrastorage.org/webt/in/rr/ah/inrrahhv1puubdomhkd5ijj8f0o.jpeg"><br><br>  Quite a common situation that the developer takes the task, brings it to the status of in review, testing - and forgets about it.  Then she returns to the revision and hangs there is not clear how much time.  I myself had a developer in my team at one time.  I underestimated the problem for a long time, until one day I figured out the amount of time that, on average, was spent on various downtime.  As a result, it turned out that the tasks of this developer were, on average, 60% idle. <br><br>  <strong>How can this be measured?</strong> <br><br>  First, you need to measure all the downtime that depends on the developer.  This is the time to make fixes <strong>after the code review and testing</strong> .  If you have a continuous delivery, this is <strong>a waiting time for release.</strong>  For each of these times it is worthwhile to hang a reasonable restriction - of a type not more than a day. <br><br>  The reason is as follows.  When a developer comes to work in the morning, it would be cool for him to first deal with the highest priority tasks.  The highest priority tasks, if there are no bug fixes or something very important, are the tasks that are closest to release and release. <br><br>  Another cool trigger on this topic is the <strong>amount of code review that hangs on the developer, like on a reviewer.</strong>  If a person forgets about his tasks, then, most likely, he will also relate to the tasks of his colleagues. <br><br>  <strong>What to do with it?</strong> <br><br>  If you find such a developer, it clearly stands with the numbers on his hands to approach him and <strong>say</strong> : ‚ÄúLook, you have 30-40% of the time spent on downtime!‚Äù It usually works very well. In my case, for example, it had the effect that the problem is almost completely gone. If not, we must continue to <strong>monitor</strong> , periodically speak, but the main thing here is not to fall into the micromanagement, because it will be even worse. <br><br>  Therefore, whenever possible, it is worthwhile to deal with process decisions immediately.  These can be, for example, <strong>limits on the number of active tasks</strong> , or, if your budget and time allows, you can write a bot or use a service that will automatically <strong>ping the</strong> developer if the task is in a certain status for too long.  This is probably the coolest solution here. <br><br><h5>  <em>Developer doesn‚Äôt think out enough</em> <br></h5><br>  I think the symptoms you know are incomprehensible estimates of the time to complete tasks that we don‚Äôt fall into, extended periods in the end, an increase in the number of bugs in tasks ‚Äî well, nothing good. <br><img src="https://habrastorage.org/webt/lc/w9/ur/lcw9urobbr3vso7bx7znbafm63g.jpeg"><br><br>  <strong>How can this be measured?</strong> <br><br>  I think the symptoms you know are incomprehensible estimates of the time to complete tasks that we don‚Äôt fall into, extended periods in the end, an increase in the number of bugs in tasks ‚Äî well, nothing good. <br><br>  <strong>How can this be measured?</strong> <br><br>  To do this, we need to enter 2 metrics, the first of which is Churn code. <br><img src="https://habrastorage.org/webt/rv/gl/jx/rvgljxztmgo_qvbavzb-hlcce2e.jpeg"><br><br>  Churn is a measure of how much code a developer conditionally writes in vain. <br><br>  Imagine the situation.  On Monday, the developer began to do a new task, and wrote 100 lines of code.  Then came Tuesday, he wrote another 100 new lines of code in this problem.  But, unfortunately, it turned out that he removes 50 lines of code that were written on Monday, and releases the task.  As a result, 200 lines of code appeared to be created in the task, but only 150 survived to release, and 50 were written in vain.  These 50 we call Churn.  And so in this example, the developer churn was 25%. <br><br>  In our opinion, the <strong>high level of Churn</strong> is a cool trigger that the developer did not think of the task. <br><br>  There is a study of one American company in which they measured the level of Churn with 20,000 developers and concluded that a good indicator of Churn code should be in the range of 10‚Äì20%. <br><br>  But there are 2 important conditions: <br><br><ol><li>  High Churn is normal if you, for example, make a prototype or some new project.  Then it can be equal to 50-60% for several months.  There is nothing wrong with that.  Roughly speaking, Churn depends on the stage of the product - the more stable the product, the lower it should be. </li><li>  In no case should not strive for a zero level Churn - this is absolutely meaningless perfectionism.  No need to force developers to write a perfect code from scratch.  They will spend a lot of time thinking through or trying to somehow hack this story.  As a result, the delivery time will only increase. </li></ol><br><img src="https://habrastorage.org/webt/b6/uo/t0/b6uot0xpb-9fqm6hb_j_ahslyvy.jpeg"><br><br>  The following metric, which will be needed in order to clarify the fact that the developer does not think out enough tasks, is the so-called Fixed Tasks, or the <strong>number of corrected tasks</strong> .  This is an attempt to measure how many bugs a developer task introduces. <br><br>  Let's look at what kind of bug fixes the developers are doing, and the code of which tasks these bug fixes change.  If the time between the release of the task and the bug fixes is less than 3 weeks, we will assume that this bug was introduced by this task.  The more developers have fixed tasks, that is, the more they introduce bugs, the more likely it can be said about him that he is not thinking enough about tasks. <br><br>  The last trigger is the <strong>average number of returns from testing</strong> .  If, on average, a developer has more than one return from testing for a task, then, most likely, he has something wrong in terms of thinking about tasks. <br><br>  <strong>What to do with it?</strong> <br><br>  If you find yourself in such a developer, it may be worth it either for him personally or for the whole team to <strong>increase the time for planning</strong> or change his process.  You can enter process improvements, for example, enter <strong>limits on the size of the task</strong> , the maximum allowable size of the <strong>recognition</strong> , etc. <br><br>  One CTO with which we communicated had a rather cool workflow, which we like very much, we advise everyone.  If during planning it turns out that the estimate of the task‚Äôs implementation time is more than one day, or the task affects more than one component in the system, then a <strong>design document</strong> is written on it, which is separately reviewed before implementation. <br><br>  <strong>Terms for using Churn and Fixed Tasks</strong> <br><br>  In order for these metrics to be measured and monitored, you need to: <br><br><ul><li>  Specify the number of task in the commit message, without this you can not measure them.  And it is in the commit message, and not in the branch, because git does not store the history of branches. </li><li>  Do not make git-squash commit'ov tasks in one, because then Churn will be a priori equal to zero. </li><li>  It should be possible to determine the release on git.  That is, releases can be counted in merge in master, or merge in a specific branch, or at worst tag.  But the opportunity to determine should be - otherwise we can not understand when the task is over, and when to count Churn and Fixed Tasks. </li></ul><br><h3>  Team problems </h3><br><img src="https://habrastorage.org/webt/rh/0r/ty/rh0rtyyvplw4n5i3uqgrmedl8l4.jpeg"><br><br>  The first and one of the most frequent stories about which we have heard is the <strong>uneven distribution of knowledge of the code base in the team</strong> and the fact that in reality you want to see people with relatively low bus numbers.  Because it‚Äôs bad when there are irreplaceable people who can do work that no one else can do.  If such a person knocks down a bus, the development of the entire system may stop for a while. <br><br>  For example, in my practice there was a case when, due to the dismissal of a senior, an important release was postponed for 3-4 weeks.  This is a very long time, and the business is not happy. <br><br>  <strong>How can this be measured?</strong> <br><br>  The idea is very simple - it is necessary to measure in some way, how unique are the knowledge of a particular developer, and how well the developers are generally aware of the work of their neighbors. <br><img src="https://habrastorage.org/webt/un/nk/hb/unnkhb3dgba1yggcwjjcpwnrez4.jpeg"><br><br>  To do this, you can take all the developer files that he has worked with over the past 3 months, and all the team files that several developers have worked on for the same period (the blue and green circle in the diagram, respectively), and cross these two sets. <br><br>  We will call uniqueness the share of unique developer files that only he knows from all team files.  Awareness - on the contrary, the proportion of non-unique files from all command files. <br><br>  The rule is simple - if you see that you have developers with a <strong>high degree of uniqueness or low awareness</strong> - whatever you like - then most likely you have a problem with the uneven distribution of knowledge of the code base. <br><br>  <strong>How to deal with it?</strong> <br><br>  Really pretty easy.  You can simply fix the threshold of uniqueness, above which you do not want developers to appear.  Timlid or someone else must rotate tasks among developers so that none of them go beyond this threshold. <br><br>  Choosing a threshold - the question, of course, is not very simple.  It all depends on your priorities.  The higher the overlap by developers, the lower the team‚Äôs speed in the short term, respectively, because they duplicate each other‚Äôs competencies.  But the lower the risks and the higher the predictability of work in the long term. <br><br>  We recommend for companies with a staff of 30-50 people a unique threshold in the region of 50-60%.  This is quite normal, there is nothing terrible in it. <br><br>  Another life hack about what can be done with bus numbers, we learned from one company.  They simply brought a table where they listed all the components of the system, and the developers, once a quarter, put down in this table how much they are familiar with each component and how much they worked with it for this period.  As a result, they get a regularly updated <strong>knowledge table for developers</strong> . <br><img src="https://habrastorage.org/webt/a-/vo/bf/a-vobfkdyzmwrdvjitfd7ew4_nc.jpeg"><br><br>  The second most common problem that we heard about from people is <strong>bad food requirements</strong> .  Conventionally, the product manager submits to the work tasks that are poorly developed.  Then in the middle of the iteration, he resorts, changes them, and as a result, a large amount of team time is simply thrown into the trash. <br><br>  <strong>How can this be measured?</strong> <br><br>  How to catch such a product manager and explain to him how much his solutions are for his team? <br><br>  Most likely, the product manager had a hand in this problem if we see the team: <br><br><ul><li>  a high level of Churn, that is, in total for the whole team a high percentage of the code being emitted; </li><li>  on average, large tasks, both in terms of the amount of code, and simply by estimation; </li><li>  Task descriptions change after getting into status in progress or there are many comments to tasks from a product manager. </li></ul><br>  <strong>What to do with it?</strong> <br><br>  You can <strong>come up with the numbers</strong> on your hands to the product-manager and say: "Look, our team has Churn's level above the norm by so much - that's how much your constantly changing food requirements stand for us." <br><br>  Most likely, the figure ‚Äúhow much a team‚Äôs share of the emitted code costs‚Äù will be much higher than plus 1-2 days to work out product requirements.  This figure helped one of our clients very much, and this argument in fact resolved the conflict with the product manager, which lasted several months. <br><img src="https://habrastorage.org/webt/rz/jv/fu/rzjvfug-i51b9anqhcwoiieih3c.jpeg"><br><br>  <strong>Bad onboarding a new developer is</strong> more common for large companies.  In different teams, the introduction of new employees occurs with varying degrees of, let's say, efficiency. <br><br>  The team hired a junior, he was assigned to the seigneur, who was supposed to instruct him and teach him how to be a developer.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But instead of doing a deep Junior code review and explaining to him how and what needs to be fixed, the senor simply rewrote the code for him. </font><font style="vertical-align: inherit;">As a result, the junior did not learn anything, and this did not lead to anything good. </font><font style="vertical-align: inherit;">This is a prime example of bad onboarding. </font></font><br><br> <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">How can this be measured? </font></font></strong> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Most likely there is the problem of bad onboarding if we see that the newcomers to the team:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> small amount of code; </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> a small number of unique files and slowly growing; </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> low percentage of editing old code; </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> superficial code review, that is, there are few comments and the time for code review is on average less than that of the team; </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> big task. </font></font></li></ul><br> <strong>   ?</strong> <br><br> -,     ,      ,       <strong> </strong>      <strong></strong> ,     . <br><br> -, ,     ,  ,     <strong> </strong>  ,  ,      . <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">From our experience - at some point in the team one new developer came to the two timblids, almost simultaneously. One team followed this best practice, tried to rotate tasks for this developer, to carry it around the code, and the second simply gave the tasks that were. As a result, the second developer came to an acceptable level of performance 3 months later than the first, that is, onboarding took more time by 3 months. </font></font><br><img src="https://habrastorage.org/webt/1p/be/qg/1pbeqgn6dtqugshhhfila-7cefo.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The last problem we will talk about today is the </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">accumulation of technical debt.</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. We all know how bad it is. We constantly dig into the code some crutches at the request of the business, the support of the system is getting worse and worse, and in the end everything can actually go to a complete stop in the development of business tasks. I thought for a long time that this was a horror story from books, until we recently talked to a real CTO, which had so much in the company of technical debt that they stopped the development of their business by almost 100% for a whole quarter. To prevent this, we must somehow track it. </font></font><br><br> <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">How can this be measured?</font></font></strong> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> To do this, enter one metric called </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">legacy refactoring</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. This is the percentage of deleting old lines of code, and thereby reducing the complexity of the system. The complexity of the system can be measured in different ways. We recommend just the usual cyclomatic complexity, since there are a lot of libraries that do this, and for almost any language. </font></font><br><img src="https://habrastorage.org/webt/mp/ni/px/mpnipxdhwv3tehknn0v1w_ddb7a.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If we see that we have a </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">low percentage of legacy refactoring</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , that is, we pay little attention to simplifying the old code, and we already have a </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">high complexity</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> code, then this is an obvious indicator that technical debt is accumulating. </font></font><br><br> <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What to do with it?</font></font></strong> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">You can do different things with it, and you can talk a lot about how to work with it. We like the two stories that the CTO told us. Their approaches make it possible not only to work formally with technical debt, but also very cool to measure its level. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The first CTO simply taught its developers when they cut a crutch into the code, it is mandatory to start a </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ticket with the type of ‚Äúcrutch‚Äù</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in Jira </font><font style="vertical-align: inherit;">. As a result, he always has in the tracker a relatively up-to-date list of all crutches in the system, with some kind of estimation and with a priority level. He constantly watches them, setting the standard level - how many such tickets can he have, etc.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The second CTO went even further and used a more elegant approach. </font><font style="vertical-align: inherit;">When developers crush a crutch into the system, they are required to insert a </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">comment with the ‚ÄúHack‚Äù type</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> into the code where they do it </font><strong><font style="vertical-align: inherit;">. </font></strong><font style="vertical-align: inherit;">If another developer has detected some suspicious code on the code review, he said: ‚ÄúPut the word Hack here - then fix it,‚Äù for example. </font><font style="vertical-align: inherit;">As a result, he monitors the level of technical debt with a trivial grep in the repository for the word ‚ÄúHack‚Äù and always gets the current list and dynamics of how many such flaws he currently has in the system. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Actually with metrics that's all. </font><font style="vertical-align: inherit;">We tried to tell about the simplest and most understandable to measure.</font></font><br><br><h2>  Underwater rocks </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Our approach has pitfalls: </font></font><br><br><ol><li>        .  , , Churn  legacy refactoring.    ,    . <br></li><li>        . ,  ,    ,     ‚Äî  git  ,         git-.      ‚Äî     ,   . </li><li>          ,             . ,    ,        . </li><li>  ,    ,    -,     -      : git, Jira, GitHub,   .. </li></ol><br><h2>  findings </h2><br>      : <br><br><ul><li> <strong>      </strong> .   ,    . <br></li><li> <strong>    KPI</strong> .         .  ,        .     . <br></li><li> Git          ,   <strong>  </strong> ,        ,   : <br><ul><li>    ; </li><li>  ; </li><li>    : merge  master, . </li></ul></li></ul><br> <strong>   :</strong> <br><br><ul><li>  <a href="http://teamleadconf.ru/2018/abstracts/3155"></a>         . <br></li><li> <a href="https://medium.com/cto-hints"></a>        <br></li><li> Telegram : @avkiselev ( )  <a href="https://habr.com/users/sss0791/" class="user_link">sss0791</a> ( ). </li></ul><br><blockquote>  <a href="http://teamleadconf.ru/">TeamLead Conf</a>            .      ,    ,   ,            ‚Äî   . <a href="https://conf.ontico.ru/lectures/propose%3Fconference%3Dtl2018-spb"> </a>    <b> 10 </b> . <br><br>      ,   <a href="https://conf.ontico.ru/conference/join/tl2018-spb.html"> </a> ,    ,      ‚Äî          . <br></blockquote></div><p>Source: <a href="https://habr.com/ru/post/417411/">https://habr.com/ru/post/417411/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../417399/index.html">Welcome aboard: we introduce new developers to the team</a></li>
<li><a href="../417401/index.html">Finally, we choose a budget multimeter with good functionality.</a></li>
<li><a href="../417405/index.html">Autoencoders and Strong AI</a></li>
<li><a href="../417407/index.html">Optimizing the rendering of scenes from the Disney cartoon "Moana". Part 1</a></li>
<li><a href="../417409/index.html">How to tmlidu survive in a scalable scram and keep control over the quality of the code</a></li>
<li><a href="../417413/index.html">Whether the problems of Timblids differ in St. Petersburg, find out on Saint TeamLead Conf</a></li>
<li><a href="../417415/index.html">3D printing lessons. Printing parts with different layer thickness from 3Dtool</a></li>
<li><a href="../417419/index.html">[Yekaterinburg, Announcement] Alice Visiting Kontur - Hackathon to create skills for voice assistants</a></li>
<li><a href="../417421/index.html">Becoming Color Blind: An Experiment in Empathy</a></li>
<li><a href="../417423/index.html">33 ICO Marketing Tips</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>