<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How I won the Beeline BigData Contest</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Everyone has already heard about machine learning from Beeline many times and even read articles ( one , two ). Now the competition is over, and so it...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How I won the Beeline BigData Contest</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/ec2/43d/337/ec243d337315436681658acac7998fd4.jpg" width="40%" alt="image"><br><br>  Everyone has already heard about machine learning from Beeline many times and even read articles ( <a href="http://habrahabr.ru/post/269065/">one</a> , <a href="http://habrahabr.ru/post/269745/">two</a> ).  Now the competition is over, and so it happened that the first place went to me.  And although only hundredths of a percent were separated from the previous participants, I would still like to tell you what I did.  In fact - nothing incredible. <br><a name="habracut"></a><br><h4>  Data preparation </h4><br>  It is said that 80% of the time data analysts spend on the preparation of data and preliminary modifications and only 20% of them are directly analyzed.  And it is not so casual, as ‚Äúgarbage in - garbage out‚Äù.  The process of preparing the initial data can be divided into several stages, which I propose to go through. <br><br><h5>  Emission correction </h5><br>  After a careful study of the histograms, it becomes clear that quite a lot of outliers have crept into the data.  For example, if you see that 99.9% of observations of variable X are concentrated on the [0; 1] segment, and 0.01% of observations throws out over a hundred, then it is quite logical to do two things: first, introduce a new column to indicate strange events, and second, replace emissions with something sensible. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <pre><code class="python hljs">data[<span class="hljs-string"><span class="hljs-string">"x8_strange"</span></span>] = (data[<span class="hljs-string"><span class="hljs-string">"x8"</span></span>] &lt; <span class="hljs-number"><span class="hljs-number">-3.0</span></span>)*<span class="hljs-number"><span class="hljs-number">1</span></span> data.loc[data[<span class="hljs-string"><span class="hljs-string">"x8"</span></span>] &lt; <span class="hljs-number"><span class="hljs-number">-3.0</span></span> , <span class="hljs-string"><span class="hljs-string">"x8"</span></span>] = <span class="hljs-number"><span class="hljs-number">-3.0</span></span> data[<span class="hljs-string"><span class="hljs-string">"x31_strange"</span></span>] = (data[<span class="hljs-string"><span class="hljs-string">"x31"</span></span>] &lt; <span class="hljs-number"><span class="hljs-number">0.0</span></span>)*<span class="hljs-number"><span class="hljs-number">1.0</span></span> data.loc[data[<span class="hljs-string"><span class="hljs-string">"x31"</span></span>] &lt; <span class="hljs-number"><span class="hljs-number">0.0</span></span>, <span class="hljs-string"><span class="hljs-string">"x31"</span></span>] = <span class="hljs-number"><span class="hljs-number">0.0</span></span> data[<span class="hljs-string"><span class="hljs-string">"x40_zero"</span></span>] = (data[<span class="hljs-string"><span class="hljs-string">"x40"</span></span>] == <span class="hljs-number"><span class="hljs-number">0.0</span></span>)*<span class="hljs-number"><span class="hljs-number">1.0</span></span></code> </pre> <br><h5>  Normalization of distributions </h5><br>  In general, working with normal distributions is extremely pleasant, since many statistical tests are tied to the hypothesis of normality.  Modern methods of machine learning, this applies to a lesser extent, but still bring the data to a reasonable mind is important.  Especially important for methods that work with distances between points (almost all clustering algorithms, k-neighbors classifier).  In this part of the data preparation, I managed the standard approach: I log everything that is distributed more densely around zero.  Thus, for each variable, I selected a transformation that gave a more pleasant look.  Well, after that I scaled everything into a segment [0; 1] <br><br><h5>  Text variables </h5><br>  In general, text variables are a storehouse for data mining, but in the source data there were only hashes, and the names of the variables were anonymized.  Therefore, only the standard routine: replace all rare hashes with the word Rare (rare = less common 0.5%), replace all missing data with the word Missing and deploy as a binary variable (since many methods, including xgboost, are not able to categorical variables) . <br><br><pre> <code class="python hljs">data = pd.get_dummies(data, columns=[<span class="hljs-string"><span class="hljs-string">"x2"</span></span>, <span class="hljs-string"><span class="hljs-string">"x3"</span></span>, <span class="hljs-string"><span class="hljs-string">"x4"</span></span>, <span class="hljs-string"><span class="hljs-string">"x11"</span></span>, <span class="hljs-string"><span class="hljs-string">"x15"</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> col <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> data.columns[data.dtypes == <span class="hljs-string"><span class="hljs-string">"object"</span></span>]: data.loc[data[col].isnull(), col] = <span class="hljs-string"><span class="hljs-string">'Missing'</span></span> thr = <span class="hljs-number"><span class="hljs-number">0.005</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> col <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> data.columns[data.dtypes == <span class="hljs-string"><span class="hljs-string">"object"</span></span>]: d = dict(data[col].value_counts(dropna=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)/len(data)) data[col] = data[col].apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: <span class="hljs-string"><span class="hljs-string">'Rare'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> d[x] &lt;= thr <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> x) d = dict(data[<span class="hljs-string"><span class="hljs-string">'x0'</span></span>].value_counts(dropna=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)/len(data)) data = pd.get_dummies(data, columns=data.columns[data.dtypes == <span class="hljs-string"><span class="hljs-string">"object"</span></span>])</code> </pre><br><h5>  Feature engineering </h5><br>  This is what we all love data science for.  But everything is encrypted, so this item will have to be omitted.  Nearly.  After scrutinizing the graphs, I noticed that x55 + x56 + x57 + x58 + x59 + x60 = 1, which means that these are some shares.  Let's say what percentage of money a subscriber spends on SMS, calls, the Internet, etc.  This means that of particular interest are those comrades who have any of the shares of more than 90% or less than 5%.  Thus we get 12 new variables. <br><br><pre> <code class="python hljs">thr_top = <span class="hljs-number"><span class="hljs-number">0.9</span></span> thr_bottom = <span class="hljs-number"><span class="hljs-number">0.05</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> col <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> [<span class="hljs-string"><span class="hljs-string">"x55"</span></span>, <span class="hljs-string"><span class="hljs-string">"x56"</span></span>, <span class="hljs-string"><span class="hljs-string">"x57"</span></span>, <span class="hljs-string"><span class="hljs-string">"x58"</span></span>, <span class="hljs-string"><span class="hljs-string">"x59"</span></span>, <span class="hljs-string"><span class="hljs-string">"x60"</span></span>]: data[<span class="hljs-string"><span class="hljs-string">"mostly_"</span></span>+col] = (data[col] &gt;= thr_top)*<span class="hljs-number"><span class="hljs-number">1</span></span> data[<span class="hljs-string"><span class="hljs-string">"no_"</span></span>+col] = (data[col] &lt;= thr_bottom)*<span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre><br><h5>  Remove NA </h5><br>  Everything is very simple here: after all distributions are brought to a reasonable form, you can safely replace NA-patches with the mean or median (now they almost coincide).  I tried to remove from the training sample those lines where more than 60% of the variables are NA, but this did not end well. <br><br><h5>  Regression as a regressor </h5><br>  The next step is not so banal.  From the distribution of classes, I assumed that age groups are ordered, that is, 0 &lt;1 ... &lt;6 or vice versa.  And if so, then you can not classify, and build a regression.  It will work poorly, but its result can be transferred to other algorithms for learning.  Therefore, we start the usual linear regression with the loss function huber and optimize it through a stochastic gradient descent. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.linear_model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SGDRegressor sgd = SGDRegressor(loss=<span class="hljs-string"><span class="hljs-string">'huber'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">100</span></span>) sgd.fit(train, target) test = np.hstack((test, sgd.predict(test)[<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>].T)) train = np.hstack((train, sgd.predict(train)[<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>].T))</code> </pre><br><br><h5>  Clustering </h5><br>  The second interesting thought I tried: k-means data clustering.  If there is a real structure in the data (and it should be in the subscriber data), then k-means will feel it.  At first I took k = 7, then added 3 and 15 (twice as much and twice as less).  The predictions of each of these algorithms are the cluster numbers for each sample.  Since these numbers are not ordered, it is impossible to leave them with numbers, it is necessary to be binarized.  Total + 25 new variables. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.cluster <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> KMeans k15 = KMeans(n_clusters=<span class="hljs-number"><span class="hljs-number">15</span></span>, precompute_distances = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>) k15.fit(train) k7 = KMeans(n_clusters=<span class="hljs-number"><span class="hljs-number">7</span></span>, precompute_distances = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>) k7.fit(train) k3 = KMeans(n_clusters=<span class="hljs-number"><span class="hljs-number">3</span></span>, precompute_distances = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>) k3.fit(train) test = np.hstack((test, k15.predict(test)[<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>].T, k7.predict(test)[<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>].T, k3.predict(test)[<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>].T)) train = np.hstack((train, k15.predict(train)[<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>].T, k7.predict(train)[<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>].T, k3.predict(train)[<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>].T))</code> </pre><br><br><h4>  Training </h4><br>  When the data preparation was completed, the question arose which machine learning method to choose.  In principle, the answer to this question has long been <a href="https://xgboost.readthedocs.org/en/latest/">known</a> . <br><br>  In fact, besides xgboost I tried the k-neighbors method.  Despite the fact that in spaces of high dimension it is considered ineffective, I managed to achieve 75% (a small step for a person and a big step for k-neighbors), considering the distance not in the usual Euclidean space (where the variables are equal), and correcting for the importance variables as shown in the <a href="http://www.cs.haifa.ac.il/~rita/ml_course/lectures/KNN.pdf">presentation</a> . <br><br>  However, all these are toys, and really good results were not yielded by a neural network, not a logistic regression and not k-neighbors, but what was expected - xgboost.  Later, when I came to Beeline‚Äôs defense, I learned that they also achieved better results with the help of this library.  For classification tasks, it is already something of the ‚Äúgold standard‚Äù type. <br><blockquote>  "When in doubt - use xgboost" <br>  <i><a href="https://www.kaggle.com/owenzhang1">Owen Zhang</a> , top-2 on Kaggle.</i> </blockquote><br>  Before starting to really start up really and get excellent results, I decided to see how really important are all the columns that were given, and those that I created by expanding the hashes and clustering using the K-Means method.  To do this, I made a light boost (not very many trees), and built columns sorted by importance (according to xgboost). <br><br><pre> <code class="python hljs">gbm = xgb.XGBClassifier(silent=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, nthread=<span class="hljs-number"><span class="hljs-number">4</span></span>, max_depth=<span class="hljs-number"><span class="hljs-number">10</span></span>, n_estimators=<span class="hljs-number"><span class="hljs-number">800</span></span>, subsample=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, learning_rate=<span class="hljs-number"><span class="hljs-number">0.03</span></span>, seed=<span class="hljs-number"><span class="hljs-number">1337</span></span>) gbm.fit(train, target) bst = gbm.booster() imps = bst.get_fscore()</code> </pre><br><img src="https://habrastorage.org/files/334/e18/ff3/334e18ff3abe42139b64d724f122a546.png" alt="image"><br><br>  My opinion is that those columns, the importance of which is rated as ‚Äúinsignificant‚Äù (a diagram is constructed only for the most important 70 variables out of 335), contain more noise than actually useful correlations, and learning from them is to harm yourself ( ie <a href="https://en.wikipedia.org/wiki/Overfitting">retrain</a> ). <br><br>  It is also interesting to note that the first important variable is x8, and the second is the results of the SGD regression added by me.  Those who tried to participate in this competition, probably puzzled that for the variable x8, if it separates the classes so well.  On protection in Beeline, I could not resist and did not ask what it was.  It was AGE!  As I explained, the age specified in the purchase of the tariff, and the age obtained in the polls do not always match, so yes, the participants did determine the age group, including by age. <br><br><img src="https://habrastorage.org/files/683/f7e/fcb/683f7efcbe504484a9306c9ef47b7100.jpg" width="45%" alt="image"><br><br>  Through short-lived experiments, I realized that leaving 120 columns is better than leaving 70 or leaving 170 (in the first case, apparently, something useful is still being thrown away, in the second, the data is contaminated by something useless). <br>  Now it was necessary to push.  The two xgboost.XGBClassifier parameters that deserve the most attention are eta (aka learning rate) and n_estimators (number of trees).  The remaining parameters did not change the results very much (therefore, I set max_depth = 8, subsample = 0.5, and the rest parameters by default). <br><br>  There is a natural relationship between the optimal values ‚Äã‚Äãof eta and n_estimators - the lower the eta (learning rate), the more trees are needed to achieve maximum accuracy.  And here we are really confronted with the optimum, after which the addition of additional trees causes only retraining, worsening the accuracy of the test sample.  For example, for eta = 0.02, approximately 800 trees will be optimal: <br><br><img src="https://habrastorage.org/files/8b3/a5f/94d/8b3a5f94d1704ca78bb1d35e693d2e4f.jpg" width="85%" alt="image"><br><br>  At first I tried working with average eta (0.01-0.03) and saw that depending on the random state (seed) there is a noticeable variation (for example, for 0.02 the score varies from 76.7 to 77.1), and also noticed that this variation becomes smaller with decreasing eta.  It became clear that large eta are not suitable in principle (how can a model be so good, so dependent on the seed?). <br><br>  Then I chose the eta, which I can afford on my computer (I would not like to consider days).  This is eta = 0.006.  Next, it was necessary to find the optimal number of trees.  In the same way as shown above, I found that 3400 trees would be suitable for eta = 0.006.  Just in case, I tried two different seed (it was important to understand whether the fluctuations are large). <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> seed <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> [<span class="hljs-number"><span class="hljs-number">202</span></span>, <span class="hljs-number"><span class="hljs-number">203</span></span>]: gbm = xgb.XGBClassifier(silent=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, nthread=<span class="hljs-number"><span class="hljs-number">10</span></span>, max_depth=<span class="hljs-number"><span class="hljs-number">8</span></span>, n_estimators=<span class="hljs-number"><span class="hljs-number">3400</span></span>, subsample=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, learning_rate=<span class="hljs-number"><span class="hljs-number">0.006</span></span>, seed=seed) gbm.fit(trainclean, target) p = gbm.predict(testclean) filename = (<span class="hljs-string"><span class="hljs-string">"subs/sol3400x{1}x0006.csv"</span></span>.format(seed)) pd.DataFrame({<span class="hljs-string"><span class="hljs-string">'ID'</span></span> : test_id, <span class="hljs-string"><span class="hljs-string">'y'</span></span>: p}).to_csv(filename, index=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)</code> </pre><br>  Each ensemble on the usual core i7 was counted for an hour and a half.  It is an acceptable time when the competition takes one and a half months  Fluctuations on the public leaderboard were small (for seed = 202 received 77.23%, for seed = 203 77.17%).  I sent the best of them, although it is very likely that another would be just as good on a private leaderboard.  However, we will not know. <br><br>  Now a little about the contest itself.  The first thing that catches the eye of someone familiar with Kaggle is the slightly unusual rules of submission.  At Kaggle, the number of sambiches is limited (depending on the competition, but as a rule, not more than 5 per day), here it‚Äôs also sub-unlimited, which allowed some participants to send results as much as 600 times.  In addition, the final submission had to choose one, and on Kaggle it is usually allowed to choose any two, and the account on the private leaderboard is considered the best of them. <br><br>  Another unusual thing is anonymized columns.  On the one hand, it practically deprives any opportunity for feature design.  On the other hand, this is partly understandable: columns with real names would give a powerful advantage to people versed in mobile communications, and the purpose of the competition was clearly not the case. </div><p>Source: <a href="https://habr.com/ru/post/270367/">https://habr.com/ru/post/270367/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../270353/index.html">Overview of ES6 at 350 points. Part one</a></li>
<li><a href="../270359/index.html">We force the php-fpm 5.6 service running through systemd to read global environment variables</a></li>
<li><a href="../270361/index.html">Animation of transitions between two fragments</a></li>
<li><a href="../270363/index.html">Game with a list of conditions</a></li>
<li><a href="../270365/index.html">Evaluation of test coverage on the project</a></li>
<li><a href="../270369/index.html">Using websocket in Extjs applications</a></li>
<li><a href="../270371/index.html">Error Handling in Rust</a></li>
<li><a href="../270373/index.html">Impact analysis on the example of corporate data warehouse infrastructure</a></li>
<li><a href="../270377/index.html">Magento 2.0 Release Candidate Released</a></li>
<li><a href="../270379/index.html">Half a Century to "Universal Machine Languages" (1966‚Äì2016): Past, Present, Future</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>