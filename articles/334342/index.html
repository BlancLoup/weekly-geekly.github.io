<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How does neural machine translation work?</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Description of the processes of machine translation of rule-based (Rule-Based), machine translation based on phrases (Phrase-Based) and neural transla...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How does neural machine translation work?</h1><div class="post__text post__text-html js-mediator-article"><h4>  Description of the processes of machine translation of rule-based (Rule-Based), machine translation based on phrases (Phrase-Based) and neural translation </h4><br><img src="https://habrastorage.org/getpro/habr/post_images/9f7/e5d/c0d/9f7e5dc0dd7eba0274a6ee8ca85d5274.jpg" alt="image"><br><br>  In this publication of our cycle of step-by-step articles, we will explain how neural machine translation works and compare it with other methods: rule-based translation technology and frame translation technology (PBMT, the most popular subset of which is statistical machine translation - SMT). <br><br>  The research results obtained by Neural Machine Translation are surprising in terms of decoding the neural network.  It seems that the network actually ‚Äúunderstands‚Äù the sentence when it translates it.  In this article we will examine the question of the semantic approach that neural networks use for translation. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Let's start with the fact that we consider the methods of work of all three technologies at different stages of the translation process, as well as the methods used in each of the cases.  Next, we will get acquainted with some examples and compare what each of the technologies does in order to give the most correct translation. <br><a name="habracut"></a><br>  A very simple, but still useful information about the process of any type of automatic translation is the following triangle, which was formulated by the French researcher <a href="https://fr.wikipedia.org/wiki/Bernard_Vauquois">Bernard Vokua</a> (Bernard Vauquois) in 1968: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ef2/dc9/7cc/ef2dc97cc5197f56598bb304a9d58741.jpg" alt="image"><br><br>  This triangle shows the process of converting a source sentence to a target in three different ways. <br><br>  The left part of the triangle characterizes the source language, while the right - the target.  The difference in levels within a triangle represents the depth of the process of analyzing the original sentence, for example, syntactic or semantic.  Now we know that we cannot separately carry out syntactic or semantic analysis, but the theory is that we can go deep in each of the directions.  The first red arrow indicates the analysis of the sentence in the original language.  From the sentence given to us, which is simply a sequence of words, we can get an idea of ‚Äã‚Äãthe internal structure and the degree of possible depth of analysis. <br><br>  For example, at one level we can define parts of the speech of each word (noun, verb, etc.), and on the other, the interaction between them.  For example, which particular word or phrase is the subject. <br><br>  When the analysis is completed, the sentence is ‚Äúcarried over‚Äù by the second process with an equal or lesser depth of analysis to the target language.  Then the third process, called ‚Äúgeneration,‚Äù forms the actual target sentence from this interpretation, that is, creates a sequence of words in the target language.  The idea of ‚Äã‚Äãusing a triangle is that the higher (deeper) you analyze the original sentence, the easier the transfer phase goes.  Ultimately, if we could transform the source language into some kind of universal ‚Äúinterlingualism‚Äù during this analysis, we would not need to perform the transfer procedure at all.  It would take only an analyzer and a generator for each language to be translated into any other language (direct translation of a comment.) <br><br>  This general idea explains the intermediate steps when the machine translates the sentences step by step.  More importantly, this model describes the nature of the actions during translation.  Let's illustrate how this idea works for three different technologies, using the sentence ‚ÄúThe smart mouse plays violin‚Äù as an example (The sentence chosen by the authors of the publication contains a small catch, since the word Smart is in English, except for the most common meaning of smart , has another 17 meanings in the dictionary as an adjective, for example, ‚Äúagile‚Äù or ‚Äúclever‚Äù comment.) <br><br><h3>  Machine translation based on rules </h3><br>  Machine based rule translation is the oldest approach and covers a wide range of technologies.  However, all of them are usually based on the following postulates: <br><br><ul><li>  The process strictly follows the Vokua triangle, the analysis is very often overestimated, and the generation process is minimized; </li><li>  All three translation stages use a database of rules and lexical elements to which these rules apply; </li><li>  Rules and lexical elements are uniquely defined, but can be changed by a linguist. </li></ul><br>  For example, the internal representation of our proposal may be as follows: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3b6/cf6/79d/3b6cf679d33792aa758746d9edafbd4f.jpg" alt="image"><br><br>  Here we see a few simple levels of analysis: <br><br><ul><li>  Targeting parts of speech.  Each word is assigned its own "part of speech", which is a grammatical category. </li><li>  Morphological analysis: the word ‚Äúplays‚Äù is recognized as a distortion from a third person and represents the form of the verb ‚ÄúPlay‚Äù. </li><li>  Semantic analysis: some words are assigned a semantic category.  For example, ‚ÄúViolin‚Äù is a tool. </li><li>  Composite analysis: some words are grouped.  "Smart mouse" is a noun. </li><li>  Dependency analysis: words and phrases are associated with ‚Äúlinks‚Äù by which the object and subject of the main verb ‚ÄúPlays‚Äù are identified. </li></ul><br>  The transfer of such a structure will be subject to the following rules of lexical transformation: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7e6/13c/ada/7e613cada0bee0dc446540d3d4ac4dbb.jpg" alt="image"><br><br>  The application of these rules will lead to the following interpretation in the target language of the translation: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bd1/6d5/dbf/bd16d5dbfc752fa9b8c94cbcd60b8c01.jpg" alt="image"><br><br>  While the French generation rules will look like this: <br><br><ul><li>  The adjective expressed by the phrase follows the noun - with a few exceptions listed. </li><li>  The defining word is consistent in number and gender with the noun it modifies. </li><li>  The adjective is matched in number and gender with the noun it modifies. </li><li>  Verb agreed with the subject. </li></ul><br>  Ideally, this analysis will generate the following translation version: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3e7/a8e/ce7/3e7a8ece7935f58c0c83aec90a65431b.jpg" alt="image"><br><br><h3>  Machine translation based on phrases </h3><br>  Phrase-based machine translation is the simplest and most popular version of statistical machine translation.  Today, it is still the main workhorse and is used in large online translation services. <br><br>  Technically speaking, machine translation based on phrases does not follow the process formulated by Vokua.  Moreover, in the process of this type of machine translation no analysis or generation is carried out, but, more importantly, the subordinate part is not deterministic.  This means that the technology can generate several different translations of the same sentence from the same source, and the essence of the approach is to choose the best option. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3ea/668/018/3ea66801867076b83c406f1c65cab95b.jpg" alt="image"><br><br>  This translation model is based on three basic methods: <br><br><ul><li>  The use of the phrase-table, which gives the translation options and the probability of their use in this sequence in the source language. </li><li>  An order reordering table that indicates how words can be swapped when migrated from the source to the target language. </li><li>  A language model that shows the probability for each possible sequence of words in the target language. </li></ul><br>  Consequently, the following table will be built on the basis of the original sentence (this is a simplified form, in reality there would be many more options associated with each word): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/334/c5d/d0a/334c5dd0a8806717ed3fbd9a990c1838.jpg" alt="image"><br><br>  Further, this table generates thousands of possible translation sentences, for example: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/716/034/ca2/716034ca2c5824dee099187123647b44.jpg" alt="image"><br><br>  However, thanks to intelligent probability calculations and the use of more advanced search algorithms, only the most likely translation options will be considered, and the best one will be preserved as a final one. <br><br>  In this approach, the target language model is extremely important and we can get an idea of ‚Äã‚Äãthe quality of the result simply by searching the Internet: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/869/d18/879/869d18879d8cc2cf0390a5d7199eeebb.jpg" alt="image"><br><br>  Search algorithms intuitively prefer to use sequences of words that are the most likely translations of the original ones, taking into account the order reordering table.  This allows you to accurately generate the correct sequence of words in the target language. <br><br>  In this approach, there is no explicit or implicit linguistic or semantic analysis.  We were offered many options.  Some are better, others worse, but as far as we know, the main online translation services use this technology. <br><br><h3>  Neural machine translation </h3><br>  The approach to the organization of neural machine translation is fundamentally different from the previous one, and based on the Vokua triangle, it can be described as follows: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/864/815/cbc/864815cbc5921c862fe9640764f12e39.jpg" alt="image"><br><br>  Neural machine translation has the following features: <br><br><ul><li>  "Analysis" is called coding, and its result is a mysterious sequence of vectors. </li><li>  ‚ÄúTransfer‚Äù is called decoding and directly generates the target form without any generation phase.  This is not a strict limitation and there may be variations, but the underlying technology works that way. </li></ul><br>  The process itself is divided into two phases.  In the first, each word of the source sentence passes through a ‚Äúcoder‚Äù that generates what we call the ‚Äúsource context‚Äù, relying on the current word and the previous context: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/337/c2f/a8b/337c2fa8bf914908ab12e6987159b25b.jpg" alt="image"><br><br>  The sequence of source contexts (ContextS 1, ... ContextS 5) is an internal interpretation of the source sentence of the Vokua triangle and, as mentioned above, is a sequence of floating point numbers (usually 1000 floating point numbers associated with each source word).  For now, we will not discuss how the encoder performs this conversion, but I would like to note that the initial conversion of words in the ‚Äúfloat‚Äù vector is especially interesting. <br><br>  In fact, this is a technical unit, as is the case with the rule-based translation system, where each word is first compared with a dictionary, the first step of the encoder is to search for each source word within the table. <br><br>  Suppose you need to imagine different objects with variations in shape and color in two-dimensional space.  In this case, the objects that are closest to each other should be similar.  The following is an example: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/577/db3/1b1/577db31b1aae05484f279e5b7f5a0721.jpg" alt="image"><br><br>  Figures are shown on the abscissa axis and there we try to place objects of another shape that are closest in this parameter (we will need to indicate what makes the figures similar, but in the case of this example it seems intuitive).  The ordinate is a color ‚Äî green between yellow and blue (positioned because green is the result of mixing yellow and blue, approx. Lane). If our figures had different sizes, we could add this third parameter as follows: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/341/7a4/9bd/3417a49bd2daa1c4dd55736ef182841e.jpg" alt="image"><br><br>  If we add more colors or shapes, we can also increase the number of dimensions so that any point can represent different objects and the distance between them, which reflects the degree of their similarity. <br><br>  The basic idea is that it works in the case of placing words.  Instead of figures there are words, the space is much larger - for example, we use 800 dimensions, but the idea is that the words can be represented in these spaces with the same properties as the figures. <br><br>  Consequently, words with common properties and signs will be located close to each other.  For example, one can imagine that words of a certain part of speech are one dimension, words based on gender (if there is one) are another, there may be a sign of positive or negative meaning, and so on. <br><br>  We do not know exactly how these investments are formed.  In another article, we will analyze attachments in more detail, but the idea itself is as simple as the organization of figures in space. <br><br>  Let's go back to the translation process.  The second step is as follows: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/23f/cbf/0da/23fcbf0da6f6931ef596dc75bde04339.jpg" alt="image"><br><br>  At this stage, a complete sequence is formed with a focus on the ‚Äúsource context‚Äù, after which one after the other the target words are generated using: <br><br><ul><li>  ‚ÄúTarget context‚Äù formed in conjunction with the previous word and providing some information about the status of the translation process. </li><li>  The significance of the ‚Äúcontextual source‚Äù, which is a mixture of different ‚Äúsource contexts‚Äù based on a specific model called the Attention Model.  What is this we will discuss in another article.  In short, the ‚ÄúAttention Models‚Äù choose the source word to be used in translation at any stage of the process. </li><li>  Previously quoted words using word embedding to convert it into a vector that will be processed by the decoder. </li></ul><br>  The translation is completed when the decoder reaches the generation stage of the last word in the sentence. <br><br>  The whole process is undoubtedly very mysterious and we will need several publications in order to review the work of its individual parts.  The main thing to remember is that the operations of the neural machine translation process are arranged in the same sequence as in the case of machine translation based on the rules, but the nature of the operations and processing of objects is completely different.  And these differences begin with the conversion of words into vectors through their embedding in tables.  Understanding this point is enough to realize what is happening in the following examples. <br><br><h3>  Translation examples for comparison </h3><br>  Let's look at some examples of translation and discuss how and why some of the suggested options do not work for different technologies.  We chose several polysemic verbs of the English language and study their translation into French. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c24/418/f0a/c24418f0aafa5569f691f6b9547b858b.jpg" alt="image"><br><img src="https://habrastorage.org/getpro/habr/post_images/3b7/591/00b/3b759100b296b2616f38bd2bec16ec82.jpg" alt="image"><br><br>  We see that machine translation based on phrases interprets ‚Äúrender‚Äù as a meaning - with the exception of the very idiomatic variant ‚Äúrendering assistance‚Äù.  This can be easily explained.  The choice of a value depends either on checking the syntactic value of the sentence structure, or on the semantic category of the object. <br><br>  For neural machine translation, it is clear that the words ‚Äúhelp‚Äù and ‚Äúassistance‚Äù are processed correctly, which shows some superiority, as well as the obvious ability of this method to obtain syntactic data at a great distance between words, which we will take a closer look at in another publication. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6bb/4c2/07d/6bb4c207d9c30a39b1cf3c73e5b0e62d.jpg" alt="image"><br><img src="https://habrastorage.org/getpro/habr/post_images/9d1/5e7/10d/9d15e710d6b93c6c0a12417bfc4b70fe.jpg" alt="image"><br><br>  This example again shows that neural machine translation has semantic differences in two other ways (they mainly relate to the animate, denotes the word of a person or not). <br><br>  However, we note that the word "rounds" was incorrectly translated, which in this context has the meaning of the word "bullet".  We will explain this type of interpretation in another article on neural network training.  As for the rule-based translation, he recognized only the third meaning of the word ‚Äúrounds‚Äù, which is used in relation to missiles, not bullets. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f85/35a/4bf/f8535a4bf3e82ccb900dcea32e50011d.jpg" alt="image"><br><img src="https://habrastorage.org/getpro/habr/post_images/e18/b11/028/e18b11028e9328d752e9742d7afa63b5.jpg" alt="image"><br><br>  Above is another interesting example of how semantic variations of a verb in the course of neural translation interact with an object in the case of an unambiguous use of the word being proposed for translation (crime or destination). <br><br>  Other variants with the word "crime" showed the same result ... <br><br>  Translators working on the basis of words and phrases were also not mistaken, since they used the same verbs that are acceptable in both contexts. </div><p>Source: <a href="https://habr.com/ru/post/334342/">https://habr.com/ru/post/334342/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../334330/index.html">As a large courier company, the personal data of its customers handed out</a></li>
<li><a href="../334334/index.html">How ‚ÄúAktiv‚Äù organized the ‚Äúelectronic talk‚Äù</a></li>
<li><a href="../334336/index.html">20 rules of the shortest marketing texts</a></li>
<li><a href="../334338/index.html">Ideal home network or ‚Äúspiteful perfectionist to himself‚Äù</a></li>
<li><a href="../334340/index.html">Weekday Sysadmin: 17 typical situations</a></li>
<li><a href="../334344/index.html">Gamejack for Lua-developers on the Corona and Defold engines</a></li>
<li><a href="../334348/index.html">How to respect time or how to become an effective team leader.</a></li>
<li><a href="../334350/index.html">Implementing the pull-down menu NavigationDrawer using DrawerLayout using arbitrary markup</a></li>
<li><a href="../334354/index.html">Measuring the intensity of the incoming flow of events in the decay model</a></li>
<li><a href="../334356/index.html">XBRL: just about the complex - Chapter 6. Immersion in XBRL - Part 1. Getting Started</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>