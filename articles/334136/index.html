<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Random forest vs neural network: who will better cope with the task of recognizing gender in speech (part 1)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Historically, deep learning has been most successful in image processing, recognition, segmentation, and image processing. However, not the convolutio...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Random forest vs neural network: who will better cope with the task of recognizing gender in speech (part 1)</h1><div class="post__text post__text-html js-mediator-article">  Historically, deep learning has been most successful in image processing, recognition, segmentation, and image processing.  However, not the convolutional networks are united, as they say, the science of data lives. <br><br>  We tried to make a guide for solving problems related to speech processing.  The most popular and sought-after of them is probably the recognition of what they say, the analysis at the semantic level, but we turn to a simpler task - determining the gender of the speaker.  However, the toolkit in both cases is almost the same. <br><br> <a href="https://habrahabr.ru/company/neurodatalab/blog/334136/"><img src="https://habrastorage.org/web/b40/ee7/898/b40ee789846a4756b3ff6a761d32db95.png"></a> <a name="habracut"></a>  <i>/ Photo <a href="https://www.flickr.com/photos/justinlincoln/33636249661/in/photostream/">justin lincoln</a> / <a href="https://creativecommons.org/licenses/by/2.0/">CC-BY</a></i> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  What our algorithm will ‚Äúhear‚Äù </h2><br><blockquote>  <i>Characteristics of the voice that we will use</i> </blockquote><br>  First of all, you need to understand the physics of processes - to understand how a male voice differs from a female one.  About the device of the vocal tract in humans can be read in reviews or special literature, but the basic explanation ‚Äúon the fingers‚Äù is quite transparent: the vocal cords, oscillations of which produce sound waves prior to modulation by other organs of speech, in men and women have different thickness and tension, which leads to different frequency of the main tone (it is pitch, pitch).  For men, it is usually in the range of 65-260 Hz, and for women - 100-525 Hz.  In other words, the male voice most often sounds below the female. <br><br>  It would be naive to assume that sound alone can be enough.  It can be seen that for both sexes, these intervals rather overlap.  Moreover, in the process of speech, the frequency of the fundamental tone ‚Äî a variable parameter that varies, for example, during the transmission of intonation ‚Äî cannot be determined for many consonant sounds, and the algorithms for calculating it are not perfect. <br><br>  In the perception of a person, the individuality of a voice is contained not only in frequency, but also in timbre - the totality of all frequencies in a voice.  In a sense, it can be described using the spectrum, and then mathematics comes to the rescue. <br><br>  Sound is a changeable signal, which means that its spectrum from the point of view of average time is unlikely to give us something meaningful, therefore it is reasonable to consider the spectrogram ‚Äî the spectrum at each moment in time, as well as its statistics.  The audio signal is divided into overlapping 25-50 millisecond segments - frames, for each of which, using the fast Fourier transform, the spectrum is calculated, and for it, after that, moments are sought.  The centroid, entropy, dispersion, asymmetry and kurtosis coefficients are used most often - a lot of things are used when calculating random variables and time series. <br><br>  Also use chalk-frequency cepstral coefficients (MFCC).  It is worth reading about them, for example, <a href="https://habrahabr.ru/post/140828/">here</a> .  The problems they are trying to solve are two.  First, the human perception of sound is not linear in frequency and signal amplitude, therefore, some scaling (logarithmic) is required.  Secondly, the spectrum of the speech signal itself varies in frequency quite smoothly, and its description can be reduced to several numbers without any special loss in accuracy.  As a rule, 12 chalk-cepstral coefficients are used, each of which represents the logarithm of the spectral density within certain frequency bands (its width is higher, the higher the frequency). <br><br>  It is this set of features (pitch, spectrogram statistics, MFCC) that we will use for classification. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/e01/276/7d8/e012767d853d4e58967d0a56e1a5487e.jpeg" width="600"></div><br>  <i>/ Photo by <a href="https://www.flickr.com/photos/dno1967b/11831373085/">Daniel Oines</a> / <a href="https://creativecommons.org/licenses/by/2.0/">CC-BY</a></i> <br><br><h2>  We solve the classification problem </h2><br>  Machine learning begins with data.  Unfortunately, there are no open and popular databases for identifying gender, such as ImageNet for classifying images or IMDB for tonality of texts.  You can take the known base for speech recognition TIMIT, but it is paid (which imposes some restrictions on its public use), so we will use the <a href="http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html">VCTK</a> - 7 GB base in free access.  It is designed for the problem of speech synthesis, but it suits us in all respects: there is sound and data on 109 speakers.  For each of them, we take 4 random statements of 1-5 seconds in length and try to determine the gender of their author. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/a6f/c1b/050/a6fc1b05029e4ca2a7aedfb545685695.png" width="500"></div><br><br>  In a computer, sound is output as a sequence of numbers - deviations of the microphone membrane from the equilibrium position.  The sampling frequency is most often chosen from the range from 8 to 96 kHz, and for single-channel sound one of its seconds will be represented by at least 8 thousand numbers: any of them encodes the membrane deviation from the equilibrium position at each of eight thousand times per second.  For those who have heard of <a href="https://habrahabr.ru/company/Voximplant/blog/309648/">Wavenet</a> ‚Äî the neural network architecture for sound signal synthesis ‚Äî this does not seem to be a problem, but in our case this approach is redundant.  Logical action at the stage of data pre-processing is the calculation of features that can significantly reduce the number of parameters describing the sound.  Here we turned to <a href="http://audeering.com/technology/opensmile/">openSMILE</a> , a convenient package that can calculate almost everything related to sound. <br><br>  The code is written in Python, and the implementation, Random Forest, which best coped with the classification, is taken from the sklearn library.  It is also curious to see how neural networks cope with this task, but we will make a separate post about them. <br><br>  Solving the classification problem means building a function on the training data, which returns a class label using the same parameters, and does it quite accurately.  In our case, it is necessary that for a set of features for an arbitrary audio file, our classifier responds, whose speech in it is recorded, men or women. <br><br>  An audio file consists of multiple frames, usually their number is much larger than the number of training examples.  If we are trained on a set of frames, we hardly get anything worthwhile - it is reasonable to reduce the number of parameters.  In principle, it is possible to classify each frame separately, but due to outliers, the end result will also not be very encouraging.  The golden mean is to calculate the statistics of signs for all frames in an audio file. <br><br>  In addition, we need a validator algorithm for the classifier - you should make sure that it does everything correctly.  In speech processing tasks, it is considered that the generalizing ability of a model is low if it works well not for all speakers, but only for those for whom it was trained.  Otherwise, it is said that the model is so-called.  speaker free, and in itself is not bad.  To verify this fact, it is enough to divide the speakers into groups: to learn on some, and on the rest to check the accuracy. <br><br>  So we will do. <br><br>  The table with the data is stored in the data.csv file, the columns are signed in the first row, if desired, it can be displayed on the screen or viewed manually. <br><br>  We connect the necessary libraries, we read data: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> csv, os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.ensemble <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RandomForestClassifier <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> RFC <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GroupKFold <span class="hljs-comment"><span class="hljs-comment"># read data with open('data.csv', 'r')as c: r = csv.reader(c, delimiter=',') header = r.next() data = [] for row in r: data.append(row) data = np.array(data) # preprocess genders = data[:, 0].astype(int) speakers = data[:, 1].astype(int) filenames = data[:, 2] times = data[:, 3].astype(float) pitch = data[:, 4:5].astype(float) features = data[:, 4:].astype(float)</span></span></code> </pre> <br>  Now we need to organize a procedure for cross-validation of speakers.  The GroupKFold iterator built into sklearn works as follows: each point in the sample belongs to a group, in our case to one of the speakers.  The set of all speakers is divided into equal parts and each of them is successively eliminated, the classifier is trained on the remaining ones and the accuracy is remembered on the discarded ones.  The accuracy in all parts is taken as the accuracy of the classifier. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">subject_cross_validation</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(clf, x, y, subj, folds)</span></span></span><span class="hljs-function">:</span></span> gkf = GroupKFold(n_splits=folds) scores = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> train, test <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> gkf.split(x, y, groups=subj): clf.fit(x[train], y[train]) scores.append(clf.score(x[test], y[test])) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.mean(scores)</code> </pre> <br>  When everything is ready, you can set up experiments.  First, let's try to classify frames.  The attribute vector is input to the classifier, and the output label matches the label of the file from which the current frame is taken.  Let's compare the classification separately by frequency and by all signs (frequency + spectral + mfcc): <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># classify frames separately score_frames_pitch = subject_cross_validation(RFC(n_estimators=100), pitch, genders, speakers, 5) print 'Frames classification on pitch, accuracy:', score_frames_pitch score_frames_features = subject_cross_validation(RFC(n_estimators=100), features, genders, speakers, 5) print 'Frames classification on all features, accuracy:', score_frames_features</span></span></code> </pre> <br>  Expectedly, we received low accuracy - 66 and 73% of correctly classified frames.  Not much, not much better than a random classifier, which would give about 50%.  First of all, such low accuracy is related to the presence of debris in the sample - for 64% of frames it was not possible to calculate the frequency of the main tone.  There may be two reasons: the frames either did not contain speech at all (silence, sighs), or were components of consonant sounds.  And if the first can be rejected with impunity, then the second - with reservations: we believe that we can succeed in correctly separating male and female speech by sound frames. <br><br>  In fact, I want to classify not frames, but audio files as a whole.  You can calculate a variety of statistics from the temporal sequences of signs, and then classify them already: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">make_sample</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x, y, subj, names, statistics=[np.mean, np.std, np.median, np.min, np.max])</span></span></span><span class="hljs-function">:</span></span> avx = [] avy = [] avs = [] keys = np.unique(names) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> keys: idx = names == k v = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> stat <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> statistics: v += stat(x[idx], axis=<span class="hljs-number"><span class="hljs-number">0</span></span>).tolist() avx.append(v) avy.append(y[idx][<span class="hljs-number"><span class="hljs-number">0</span></span>]) avs.append(subj[idx][<span class="hljs-number"><span class="hljs-number">0</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.array(avx), np.array(avy).astype(int), np.array(avs).astype(int) <span class="hljs-comment"><span class="hljs-comment"># average features for each frame average_features, average_genders, average_speakers = make_sample(features, genders, speakers, filenames) average_pitch, average_genders, average_speakers = make_sample(pitch, genders, speakers, filenames)</span></span></code> </pre> <br>  Now each audio file is represented by a vector.  We consider the mean, variance, median and extreme values ‚Äã‚Äãof the signs and classify them: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># train models on pitch and on all features score_pitch = subject_cross_validation(RFC(n_estimators=100), average_pitch, average_genders, average_speakers, 5) print 'Utterance classification on pitch, accuracy:', score_pitch score_features = subject_cross_validation(RFC(n_estimators=100), average_features, average_genders, average_speakers, 5) print 'Utterance classification on features, accuracy:', score_features</span></span></code> </pre> <br>  97.2% is a completely different thing, everything seems to be great.  It remains to discard junk frames, recalculate statistics and enjoy the result: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># skip all frames without pitch filter_idx = pitch[:, 0] &gt; 1 filtered_average_features, filtered_average_genders, filtered_average_speakers = make_sample(features[filter_idx], genders[filter_idx], speakers[filter_idx], filenames[filter_idx]) score_filtered = subject_cross_validation(RFC(n_estimators=100), filtered_average_features, filtered_average_genders, filtered_average_speakers, 5) print 'Utterance classification an averaged features over filtered frames, accuracy:', score_filtered</span></span></code> </pre> <br>  Hooray, the bar in 98.4% achieved.  By selecting the model parameters (and choosing the model itself), you can probably increase this number, but we will not get qualitatively new knowledge. <br><br><h2>  Conclusion </h2><br>  Machine learning in speech processing is objectively difficult.  The ‚Äúhead-on‚Äù solution in most cases turns out to be far from the desired result, and often it is necessary to additionally ‚Äúscrape‚Äù 1-2% accuracy, changing something that would seem to be of little importance, but justified by physics or mathematics.  Strictly speaking, this process can be continued indefinitely, but ... <br><br>  In the next and last part of our introductory guide, we will consider in detail whether neural networks will cope with this task better, we will study different experimental settings, network architectures and related questions. <br><br>  <a href="http://neurodatalab.com/vacancies/">Worked on the</a> material: <br><br><ul><li>  Gregory Sterling, mathematician, leading expert of the <a href="http://neurodatalab.com/team/">Neurodata Lab</a> on machine learning and data analysis </li><li>  Eva Kazimirova, biologist, physiologist, expert of the Neurodata Lab in the field of acoustics, voice and speech analysis </li></ul><br>  Stay with us. <br><br><img src="https://habrastorage.org/web/c55/7b1/52c/c557b152c612425d932926c6cf10ab5f.jpg" width="300" align="left"><br><br clear="all"></div><p>Source: <a href="https://habr.com/ru/post/334136/">https://habr.com/ru/post/334136/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../334124/index.html">IP unnumbered on Debian or distribute addresses sparingly</a></li>
<li><a href="../334126/index.html">Domain Driven Design in practice</a></li>
<li><a href="../334128/index.html">Mom wants grandchildren! Or where telemedicine may be useful</a></li>
<li><a href="../334130/index.html">Uncle Bob Martin: ‚ÄúTake me to Toronto, HAL‚Äù</a></li>
<li><a href="../334134/index.html">BIT-picnic sip of summer and useful information</a></li>
<li><a href="../334138/index.html">Work with servlets for dummies. GET / POST</a></li>
<li><a href="../334140/index.html">Kubernetes success stories in production. Part 1: 4200 pods and TessMaster on eBay</a></li>
<li><a href="../334142/index.html">Quick removal of spaces from strings on ARM processors - alternative analysis</a></li>
<li><a href="../334144/index.html">Virtual machine backup and freeze / thaw scripts InterSystems Cach√©</a></li>
<li><a href="../334146/index.html">A simple family budget tracker with AWS SES, Lambda and DynamoDB (and Route53)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>