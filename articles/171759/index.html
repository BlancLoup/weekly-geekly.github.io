<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Entropy and decision trees</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Decision trees are a convenient tool in cases when it is necessary not only to classify data, but also to explain why a particular object is assigned ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Entropy and decision trees</h1><div class="post__text post__text-html js-mediator-article">  Decision trees are a convenient tool in cases when it is necessary not only to classify data, but also to explain why a particular object is assigned to a class. <br><br>  Let's first, to complete the picture, consider the nature of entropy and some of its properties.  Then, on a simple example, we will see how the use of entropy helps in creating classifiers.  Then, in general terms, we formulate an algorithm for constructing a decision tree and its features. <br><a name="habracut"></a><br><h4>  Combinatorial entropy </h4><br>  Consider a lot of multi-colored balls: <i>2</i> red, <i>5</i> green and <i>3</i> yellow.  Mix them and arrange them in a row.  We call this operation <i>permutation</i> : <br><br><img src="https://habrastorage.org/storage2/465/46c/84a/46546c84a51daf05d1e78ba93ad5bac9.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Let's count the number of different permutations, given that the balls of the same color are indistinguishable. <br><br>  If each ball had a unique color, then the number of permutations would be <i>10!</i>  , but if two balls of the same color are swapped, a new permutation will fail.  Thus, you need to exclude <i>5!</i>  permutations of green balls between themselves (as well as, <i>3!</i> yellow and <i>2!</i> red).  Therefore, in this case, the solution will be: <img src="https://habrastorage.org/storage2/716/2a4/29f/7162a429fe60a31ddd45a96b91c34c33.png"><br><br>  <i><a href="http://en.wikipedia.org/wiki/Multinomial_theorem">Multinomial coefficient</a></i> allows you to calculate the number of permutations in the general case of this problem: <img src="https://habrastorage.org/storage2/114/180/e94/114180e94f0d212e4cf2361c12a7832a.png">  ( <i>Ni</i> is the number of identical balls of each color). <br><br>  All permutations can be numbered from <i>0</i> to <i>(W - 1)</i> .  Therefore, a string of <i>log <sub>2</sub> (W)</i> bits uniquely encodes each of the permutations. <br><br>  Since the permutation consists of <i>N</i> balls, the average number of bits per permutation element can be expressed as: <img src="https://habrastorage.org/storage2/671/11d/f5c/67111df5c3f76c9b4d3039454a2539f3.png"><br><br>  This value is called <i>combinatorial entropy</i> : <br><img src="https://habrastorage.org/storage2/e7d/576/199/e7d5761997da90a8c6273cb7375a4fd3.png"><br><br>  The more homogeneous the set (balls of a single color prevail) - the less its combinatorial entropy, and vice versa - the more different elements in the set, the higher its entropy. <br><br><h4>  Shannon Entropy </h4><br>  Let's take a closer look at the expression for entropy described above: <img src="https://habrastorage.org/storage2/0db/aee/594/0dbaee594c66c3f006f439a03e91a5fc.png"><br><br>  Given the properties of logarithms, we transform the formula as follows: <img src="https://habrastorage.org/storage2/d45/12a/806/d4512a806eb37b3b851b39bcd0fe22f5.png"><br><br>  Suppose that the number of balls is large enough to use the <a href="http://ru.wikipedia.org/wiki/%25D0%25A4%25D0%25BE%25D1%2580%25D0%25BC%25D1%2583%25D0%25BB%25D0%25B0_%25D0%25A1%25D1%2582%25D0%25B8%25D1%2580%25D0%25BB%25D0%25B8%25D0%25BD%25D0%25B3%25D0%25B0">Stirling formula</a> : <img src="https://habrastorage.org/storage2/f6f/695/5cb/f6f6955cb871479409defcfe958f4407.png"><br><br>  Applying the Stirling formula, we get: <br><img src="https://habrastorage.org/storage2/a3c/8ee/d39/a3c8eed394b84cf9445c626ff744df45.png"><br>  (where <i>k</i> is the conversion factor to natural logarithms) <br><br>  Considering that <img src="https://habrastorage.org/storage2/a4f/a73/0ce/a4fa730ce84b863fec45039eb3f4c019.png">  expression can be converted: <br><img src="https://habrastorage.org/storage2/fdd/7c0/a6a/fdd7c0a6a9e37d1aed83d673d42a0c0f.png"><br><br>  Since the total number of balls is <i>N</i> , and the number of balls of the i-th color is <i>Ni</i> , then the probability that a randomly selected ball will be of this particular color is: <img src="https://habrastorage.org/storage2/20e/08e/97f/20e08e97f3347c954162c867e9777a1e.png">  .  Based on this, the formula for entropy will take the form: <br><img src="https://habrastorage.org/storage2/37f/edc/8ae/37fedc8ae95e569960e7a8c9d6e1a8a8.png"><br>  This expression is the <i>Shannon entropy</i> . <br><br>  With a more careful conclusion, it can be shown that the Shannon entropy is the limit for combinatorial entropy; therefore, its value is always somewhat greater than the value of combinatorial entropy. <br><br>  A comparison of two entropies is presented in the following figure, which is calculated for sets containing two types of objects - <i>A</i> and <i>B</i> (the total number of elements in each set is 100): <br><br><img src="https://habrastorage.org/storage2/a9a/423/f90/a9a423f90044989c06f82be521d261a6.png"><br><br><h4>  Thermodynamics </h4><br>  Similar expressions for entropy can be obtained in thermodynamics: <br><ul><li>  Based on the microscopic properties of substances: based on the postulates of statistical thermodynamics (in this case, operate on indistinguishable particles that are in different energy states). <br></li><li>  Based on the macroscopic properties of substances: by analyzing the operation of heat engines. <br></li></ul><br>  The concept of entropy plays a fundamental role in thermodynamics, appearing in the formulation of the <a href="http://ru.wikipedia.org/wiki/%25D0%2592%25D1%2582%25D0%25BE%25D1%2580%25D0%25BE%25D0%25B5_%25D0%25BD%25D0%25B0%25D1%2587%25D0%25B0%25D0%25BB%25D0%25BE_%25D1%2582%25D0%25B5%25D1%2580%25D0%25BC%25D0%25BE%25D0%25B4%25D0%25B8%25D0%25BD%25D0%25B0%25D0%25BC%25D0%25B8%25D0%25BA%25D0%25B8">Second Law of Thermodynamics</a> : <i>if an isolated macroscopic system is in a non-equilibrium state, then its most arbitrary transition to a state with a large value of entropy is most likely</i> : <br><br><img src="https://habrastorage.org/storage2/380/7c6/970/3807c6970abee0ab21f8ff2d8daf8578.png"><br><br><h4>  Maxwell's Demon </h4><br>  To emphasize the statistical nature of the Second Law of Thermodynamics in 1867, James Maxwell suggested a mental experiment: ‚ÄúImagine a vessel filled with a gas of a certain temperature, the vessel is divided by a partition with a gate that the demon opens to let the fast particles go in one direction and the slow ones in the other.  Consequently, after some time, fast particles will concentrate in one part of the vessel, and slow particles will form in another.  Thus, contrary to the second law of thermodynamics, <i>Maxwell's demon can reduce the entropy of a closed system "</i> : <br><br><img src="https://habrastorage.org/storage2/abe/e82/65b/abee8265b9bae894d7a9e90497c673a6.png"><br>  Later, Leo Szilard resolved the paradox, but this discussion is somewhat beyond the scope of this article. <br><br><h4>  Maxwell's Demon == Qualifier </h4><br>  If instead of ‚Äúfast‚Äù and ‚Äúslow‚Äù particles we consider objects belonging to different classes, then Maxwell's demon can be considered as a kind of classifier. <br><br>  The paradox formulation itself suggests the learning algorithm: <i>you need to find rules (predicates), on the basis of which you break the training data set, so that the average value of entropy decreases</i> .  The process of dividing a set of data into parts, leading to a decrease in entropy, can be viewed as the <i>production of information</i> . <br><br>  Having broken the initial data set into two parts using a certain predicate, one can calculate the entropy of each subset, and then calculate the average entropy value - if it turns out to be smaller than the entropy of the original set, then the predicate contains some generalizing information about the data. <br><br>  For example, consider the set of two-color balls, in which the color of the ball depends only on the <i>x</i> coordinate: <br>  (for practical reasons, it is convenient to use Shannon entropy for calculations) <br><br><img src="https://habrastorage.org/storage2/785/21c/7c6/78521c7c61114d0c433d76cb4f282f15.png"><br><br>  It can be seen from the figure that if the set is divided into two parts, provided that one part contains all elements with a coordinate <i>x ‚â§ 12</i> , and the other part contains all elements with <i>x&gt; 12</i> , then the average entropy will be less than the original by <i>‚àÜS</i> .  This means that this predicate summarizes some information about the data (it is easy to notice that for <i>x&gt; 12</i> - almost all the balls are yellow). <br><br>  If you use relatively simple predicates ("more", "less", "equal", etc.) - then, most likely, one rule will not be enough to create a full-fledged classifier.  But the procedure of searching for predicates can be repeated recursively for each subset.  The stopping criterion is the zero (or very small) entropy value.  The result is a tree of conditions called the <i>Decision Tree</i> : <br><br><img src="https://habrastorage.org/storage2/173/96f/27f/17396f27f81e9bb312f2f01aa1254dbe.png"><br>  The leaves of the decision tree are classes.  To classify an object using a decision tree, one must go down the tree sequentially (choosing a direction based on the values ‚Äã‚Äãof the predicates applied to the classified object).  The path from the root of the tree to the leaves can be interpreted as an explanation of why an object is assigned to a class. <br><br>  In the above example, for simplicity, all objects are characterized by only one attribute ‚Äî the <i>x</i> coordinate, but the exact same approach can be applied to objects with multiple attributes. <br><br>  Also, no restrictions are imposed on the values ‚Äã‚Äãof the attributes of the object - they can be both categorical and numerical or logical in nature.  It is only necessary to define predicates that are able to correctly handle attribute values ‚Äã‚Äã(for example, there is hardly any point in using predicates ‚Äúmore‚Äù or ‚Äúless‚Äù for attributes with logical values). <br><br><h4>  Algorithm for building a decision tree </h4><br>  In general, the algorithm for building a decision tree can be described as follows: <br>  (it seems to me that the algorithm described by "human language" is easier for perception) <br><br><pre><code class="bash hljs">s0 =      s0 == 0 :    ,             s0 != 0 :  ,                  ,       ,         </code> </pre> <br>  What does it mean <i>to look for a predicate</i> ? <br>  Alternatively, we can assume that on the basis of each element of the original set, we can construct a predicate that breaks the set into two parts.  Therefore, the algorithm can be reformulated: <br><br><pre> <code class="bash hljs">s0 =      s0 == 0 :    ,             s0 != 0 :     :      ,             ‚àÜS   ,    ‚àÜS       ,       ,         </code> </pre><br>  How can <i>‚Äúgenerate a predicate on the basis of each element of a set‚Äù</i> ? <br>  In the simplest case, you can use predicates that relate only to the value of an attribute (for example, <i>"x ‚â• 12"</i> , or <i>"color == yellow,"</i> etc.).  Therefore, the algorithm will take the form: <br><br><pre> <code class="bash hljs">s0 =      s0 == 0 :    ,             s0 != 0 :     :       :      ,             ‚àÜS   ,    ‚àÜS       ,       ,         </code> </pre><br><br>  In fact, if we consider classified objects as points in a multidimensional space, we can see that the predicates dividing the set of data into subsets are hyperplanes, and the classifier training procedure is a search for bounding volumes (in general, as for any other type of classifiers) . <br><br>  The main advantage is the resulting tree-like structure of predicates, which allows to interpret the results of the classification (although, due to its ‚Äúgreed‚Äù, the described algorithm does not always ensure the optimality of the tree as a whole). <br><br>  One of the cornerstones of the described algorithm is the stopping criterion when building a tree.  In the pseudo-codes described above, I stopped building the tree only when I reached the set, in which all elements belong to the same class ( <i>entropy == 0</i> ).  This approach allows you to fully adjust the decision tree to the training data sample, but this is not always effective from a practical point of view (the resulting tree is <i>retrained</i> ). <br><br>  One of the possible stopping criteria may be a small <i>‚àÜS</i> value.  But with this approach, nevertheless, it is impossible to give universal advice: at what values ‚Äã‚Äãof <i>‚àÜS</i> should the construction of a tree be stopped. <br><br><h4>  Random forest </h4><br>  In order not to bother with the stopping criterion when building a tree, you can do the following: <i>choose random subsets from the training data sample, and for each subset build your decision tree (in principle, it doesn't even matter which stopping criterion will be used)</i> : <br><br><img src="https://habrastorage.org/storage2/f4f/4e1/d8f/f4f4e1d8ff4da3794490dc1a69cc0033.png"><br><br>  The resulting ensemble of trees (a simplified version of <a href="http://ru.wikipedia.org/wiki/Random_forest">Random forest</a> ) can be used for classification, driving the classified object through all the trees.  Each tree seems to "vote" for the object belonging to a certain class.  Thus, on the basis of what part of the trees voted for one or another class, it can be concluded with what probability the object belongs to any class. <br><br>  This method allows to adequately handle the boundary data areas: <br><br><img src="https://habrastorage.org/storage2/f2a/f83/793/f2af8379334d9046058c9ef661b6d278.png"><br><br>  It can be seen that a single decision tree describes a region that completely contains red dots, while an ensemble of trees describes a shape that is closer to a circle. <br><br><h4>  If you want to experiment </h4><br>  I created a small application for comparing the decision tree and random forest.  Each time the application is started, a random data set is created that corresponds to a red circle on a green background, and as a result of the execution of the application, a picture is obtained, such as the one shown above. <br><br><ul><li>  You must have <a href="">Java runtime</a> installed <br></li><li>  Download the <i>dec_tree_demo.jar</i> binary <a href="https://github.com/lagodiuk/decision-tree/tree/master/bin">from here</a> <i>.</i> <br></li><li>  To start the application, type in the command line: <code>java -jar dec_tree_demo.jar out.png</code> <br></li></ul><br>  Sources <a href="https://github.com/lagodiuk/decision-tree">are</a> on github. <br><br><h4>  Instead of conclusion </h4><br>  Acceptance trees are a good alternative, in cases when it is annoying to adjust abstract weights and coefficients in other classification algorithms, or when you have to process data with mixed (categorical and numeric) attributes. <br><br><h4>  Additional Information </h4><br><ol><li>  <i>Yatsimirsky V.K.</i>  <i>Physical chemistry</i> (the concept of entropy is pretty well described here, and some philosophical aspects of this phenomenon are also considered) <br></li><li>  <a href="http://forum.compression.ru/viewtopic.php%3Ff%3D2%26t%3D3402">An interesting thread</a> about compression and entropy on <i>compression.ru</i> <br></li><li>  <a href="http://habrahabr.ru/post/116385/">One more article</a> about decision trees on Habr√© <br></li><li>  <a href="http://shop.oreilly.com/product/9780596529321.do">Toby Segaran, Programming Collective Intelligence</a> (in this book there is a chapter devoted to decision trees, and in general, if you have not read this book yet, I advise you to look there :-) <br></li><li>  Libraries such as <a href="http://en.wikipedia.org/wiki/Weka_%2528machine_learning%2529">Weka</a> and <a href="http://en.wikipedia.org/wiki/Apache_Mahout">Apache Mahout</a> contain implementation of decision trees. <br></li></ol><br><br></div><p>Source: <a href="https://habr.com/ru/post/171759/">https://habr.com/ru/post/171759/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../171745/index.html">PostgreSQL performance scaling with table partitioning</a></li>
<li><a href="../171747/index.html">CeBIT'13. The first day</a></li>
<li><a href="../171749/index.html">Caution: Intruders</a></li>
<li><a href="../171751/index.html">Differential Evolution: Genetic Function Optimization Algorithm</a></li>
<li><a href="../171757/index.html">Nginx and Websockets</a></li>
<li><a href="../171761/index.html">Space Invaders in space</a></li>
<li><a href="../171763/index.html">Google prepares rival Amazon Prime</a></li>
<li><a href="../171765/index.html">14 computer games on display at the Museum of Modern Art</a></li>
<li><a href="../171767/index.html">Google redirected Picasa Web Albums to Google+ Photos</a></li>
<li><a href="../171769/index.html">HabrAjax learned to recognize the 500th Habr page and offers to switch to copies of pages on the Web</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>