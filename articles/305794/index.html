<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Introduction to the concept of entropy and its many faces</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="As it may seem, the analysis of signals and data is a topic well enough studied and has already been spoken hundreds of times. But there are some fail...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Introduction to the concept of entropy and its many faces</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/ed0/899/0c7/ed08990c72fd408dc62ac1fd14c9bf4c.png" alt="image"><br>  As it may seem, the analysis of signals and data is a topic well enough studied and has already been spoken hundreds of times.  But there are some failures in it.  In recent years, the word "entropy" rushes all and sundry, plainly and not understanding what they are talking about.  Chaos - yes, disorder - yes, in thermodynamics it is used - it seems like yes too, with reference to signals - and then yes.  I would like at least to clarify this moment a bit and give direction to those who want to know a little more about entropy.  Let's talk about the entropy data analysis. <br><a name="habracut"></a><br>  Russian-language sources have very little literature on this subject.  And a whole idea is almost impossible to get at all.  Fortunately, my scientific leader was just a connoisseur of entropy analysis and the author of a fresh monograph [1], where everything is written "from and to".  Fortunately there was no limit, and I decided to try to convey thoughts on this subject to a wider audience, so I'll take a couple of extracts from the monograph and supplement it with my own research.  Maybe someone will come in handy. <br><br>  So let's start from the beginning.  Shannon in 1963 proposed the concept of a measure of the average informativeness of the test (the unpredictability of its outcomes), which takes into account the probability of individual outcomes (before it was Hartley, but this is omitted).  If the entropy is measured in bits, and take the base 2, then we get the formula for the <b>Shannon entropy</b> <br><img src="https://habrastorage.org/getpro/habr/post_images/c67/42d/bec/c6742dbec46f39207efd859710821438.jpg" alt="image">  where Pi is the probability of the i-th outcome. <br><br>  That is, in this case, entropy is directly related to the ‚Äúsurprise‚Äù of the occurrence of an event.  And from this follows its information content - the more predictable an event, the less informative it is.  This means that its entropy will be lower.  Although the question remains open about the relationship between the properties of information, the properties of entropy and the properties of its various estimates.  Just with the estimates we are dealing in most cases.  All that can be investigated is the informativeness of various entropy indices regarding controlled changes in the properties of processes, i.e.  in essence, their usefulness for solving specific applied problems. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The entropy of a signal described in some way (i.e. deterministic) tends to zero.  For random processes, the entropy increases the more, the higher the level of "unpredictability".  Perhaps it is precisely from such a bunch of interpretations of entropy that <b>probability-&gt; unpredictability-&gt; informativeness</b> implies the concept of ‚Äúchaos‚Äù, although it is rather vague and vague (which does not prevent its popularity).  There is also the identification of entropy and complexity of the process.  But this is again not the same thing. <br><br>  We go further. <br><br>  Entropy is a different <s>black white red</s> : <br><ul><li>  thermodynamic </li><li>  algorithmic </li><li>  informational </li><li>  differential </li><li>  topological </li></ul><br>  They all differ on the one hand, and have a common basis on the other.  Of course, each type is used to solve certain problems.  And, unfortunately, even in serious works there are errors in the interpretation of the calculation results.  And everything is connected with the fact that in practice in 90% of cases we deal with a discrete representation of a signal of continuous nature, which <u>significantly</u> affects the entropy estimate (in fact, there appears a correction factor in the formula that is usually ignored). <br><br>  In order to slightly describe the areas of application of entropy to data analysis, consider a small applied problem from the monograph [1] (which is not in digital form, and most likely will not). <br><br>  Let there be a system that switches between several states every 100 clock cycles and generates a signal x (Figure 1.5), the characteristics of which change during the transition.  But what - we do not know. <br><br>  By breaking x into realizations of 100 samples, one can construct an empirical distribution density and calculate the value of Shannon's entropy from it.  We get the values ‚Äã‚Äã‚Äúseparated‚Äù by levels (Figure 1.6). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/823/ccb/4dc/823ccb4dcb4b7cac57bf6810f6c54f70.jpg" alt="image"><br><br>  As you can see, the transitions between states are clearly observed.  But what to do if the transition time is not known to us?  As it turned out, the calculation by the sliding window can help and entropy also ‚Äúspreads‚Äù to the levels. In a real study, we used this effect to analyze the EEG signal (multi-colored pictures about it will be further). <br><br>  Now about one more entertaining property of entropy - it allows us to estimate the <b>degree of connectedness of several processes</b> .  If they have the same sources, we say that the processes are related (for example, if an earthquake is fixed in different parts of the Earth, then the main component of the signal on the sensors is common).  In such cases, correlation analysis is usually used, but it works well only to identify linear relationships.  In the case of non-linear (generated by time delays, for example), we suggest using entropy. <br><br>  Consider a model of 5 hidden variables (their entropy is shown in the figure below on the left) and 3 observables that are generated as a linear sum of hidden variables taken with time shifts according to the scheme shown below on the right.  Numbers are coefficients and time shifts (in readings). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/acf/4ae/714/acf4ae714ec165844d58ffd17577b29d.jpg" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/d30/216/9dc/d302169dcb76ce452e1fe39d1a091fc9.jpg" alt="image"><br><br>  So, the trick is that the entropy of connected processes comes closer with the strengthening of their connection.  Damn, how beautiful it is! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a0f/84b/9fa/a0f84b9faa4fa359a64ad5d06ebdb3c0.jpg" alt="image"><br><br>  Such joys make it possible to pull out almost any of the strangest and most chaotic signals (especially useful in economics and analytics) for more information.  We pulled them out of the electroencephalogram, considering the fashionable now Sample Entropy and here are some pictures. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e4d/b38/64b/e4db3864b2c2d8dca5805ba9e1ea05e1.jpg" alt="image"><br><br>  It can be seen that the entropy jumps correspond to a change in the stages of the experiment.  There are a couple of articles on this topic and the master‚Äôs is already defended, so if anyone is interested in the details, I‚Äôll be happy to share.  And so on the world by the entropy EEG have been looking for different things for a long time - the stages of anesthesia, sleep, Alzheimer's and Parkinson's disease, the effectiveness of treatment for epilepsy is considered and so on.  <b>But again, often the calculations are carried out without taking into account the correction factors, and this is sad, since the reproducibility of research is a big question (which is critical for science, so).</b> <br><br>  Summarizing, I will dwell on the universality of the entropy apparatus and its actual effectiveness, if we approach everything with regard to pitfalls.  I hope that after reading you will have a seed of respect for the great and mighty power of Entropy. <br><br>  PS If there is interest, I can talk a little more about the next time about the algorithms for calculating the entropy and why the Shannon entropy is now shifted by more recent methods. <br>  PPS Continuation about local-rank coding <a href="https://habrahabr.ru/post/310526/">see here.</a> <br><br><h4>  Literature </h4><br>  1. Tsvetkov OV  Entropy data analysis in physics, biology and technology.  SPb .: Publishing house of Saint-Petersburg Electrotechnical University "LETI", 2015. 202 p.  <a href="http://www.polytechnics.ru/shop/product-details/370-cvetkov-o-v-entropijnyj-analiz-dannyx-v-fizike-biologii-i-texnike.html">www.polytechnics.ru/shop/product-details/370-cvetkov-ov-entropijnyj-analiz-dannyx-v-fizike-biologii-i-texnike.html</a> <br>  2. Abasolo D., Hornero R., Espino P. Entropy analysis of the EEG background activity in Alzheimer's disease patients // Physiological Measurement.  2006. Vol.  27 (3).  P. 241 - 253. <a href="http://epubs.surrey.ac.uk/39603/6/Abasolo_et_al_PhysiolMeas_final_version_2006.pdf">epubs.surrey.ac.uk/39603/6/Abasolo_et_al_PhysiolMeas_final_version_2006.pdf</a> <br>  3. 28. Bruce Eugene N, Bruce C Margaret C, Vennelaganti S. Eg.  2009. Vol.  26 (4).  P. 257 - 266. <a href="http://www.ncbi.nlm.nih.gov/pubmed/19590434">www.ncbi.nlm.nih.gov/pubmed/19590434</a> <br>  4. Entropy analysis as a method for the non-hypothetical search for real (homogeneous) social groups (O. I. Shkaratan, G.A. <br>  5. Entropy and other systemic laws: Issues of managing complex systems.  Prangishvili I.V.  <a href="http://apolov-oleg.narod.ru/olderfiles/1/Prangishvili_I.V_JEntropiinye_i_dr-88665.pdf">apolov-oleg.narod.ru/olderfiles/1/Prangishvili_I.V_JEntropiinye_i_dr-88665.pdf</a> </div><p>Source: <a href="https://habr.com/ru/post/305794/">https://habr.com/ru/post/305794/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../305784/index.html">From design to development: 10 tools to help improve and optimize workflow</a></li>
<li><a href="../305786/index.html">ORM on php for MySQL, reality (part one)</a></li>
<li><a href="../305788/index.html">The recent past: a study on the problems of test automation</a></li>
<li><a href="../305790/index.html">Google page speed insights 100 out of 100 on the platform for online store</a></li>
<li><a href="../305792/index.html">The digest of interesting materials for the mobile # 162 developer (July 11-17)</a></li>
<li><a href="../305796/index.html">Learning OpenGL ES2 for Android Lesson ‚Ññ2. Creating triangles</a></li>
<li><a href="../305798/index.html">MVP on steroids: make the robot write the code for you</a></li>
<li><a href="../305800/index.html">STM32F405: flash 400kb in 10 seconds or a fast UART bootloader sharpened for USB-UART, less than 4 kilobytes in size</a></li>
<li><a href="../305802/index.html">Create a multilanguage blog using OctoberCMS</a></li>
<li><a href="../305808/index.html">PHP Digest number 88 - interesting news, materials and tools (June 13 - July 17, 2016)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>