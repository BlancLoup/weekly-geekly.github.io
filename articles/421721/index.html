<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to migrate ONTAP and not go crazy</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Migration of IT systems is not an easy task. But a particular difficulty is the situation when it is necessary not just to switch from the old hardwar...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to migrate ONTAP and not go crazy</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/a48/f5f/bc3/a48f5fbc32d7a6b98ab102d85c15ba7d.jpg"><br><br>  Migration of IT systems is not an easy task.  But a particular difficulty is the situation when it is necessary not just to switch from the old hardware to the new one, but to move to the new operating system on the existing equipment, and without migration of productive data.  One such move lasted about a year, and preparation took most of the time. <br><a name="habracut"></a><br>  The client has two sites in different cities, and each has two interconnected data storage systems.  Information from one storage system using the built-in replication is sent to the second.  Management is performed using an external backup system.  Two NetApp 3250 systems are installed in one city, the other is the main NetApp 6220 and the backup NetApp 3250. The client plans to expand this complex in the future, add disks, and upgrade controllers. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/db1/315/0de/db13150dedc3c2d454844e9fdd96797c.png"><br>  <i>Fig.</i>  <i>1 Scheme of the interaction of storage systems and IBS</i> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      And the main problem is connected with this - the end of support.  For the Data ONTAP 8.2 operating system installed on the storage system, 7-Mode major updates have not been for a couple of years, and the release of critical bug fixes will cease in 2021.  New drives and controllers are not compatible with the outdated operating system. <br><br>  The solution is to switch to the ONTAP 9.1 cluster system as the last supported for these storage controllers.  Its main advantages are: <br><br><ul><li>  Horizontal scaling by combining into a single fault-tolerant cluster, which will allow creating a unified system based on productive storage and RMS. </li><li>  Load sharing between controllers, disks, as well as data movement within the storage system without interrupting the service and stopping access to applications. </li><li>  Maintenance of hardware and software storage systems without stopping their work and interrupt service. </li><li>  The ability to create heterogeneous cluster configurations that include controllers and disks of various types, including third-party storage systems, subject to the use of licenses to virtualize their disk capacity. </li><li>  The ability to use SSD as a caching layer for aggregates. </li><li>  Optimized work of Data Compaction mechanisms (compression and deduplication). </li></ul><br>  There are 3 options for migrating from 7 mode to Cluster Mode: <br><br><ol><li>  Migrate with data replication using the Netapp 7-Mode Transition Tool (7MTT) utility and copy-based transition (CBT) mode.  To do this, you need a second storage system with disk space of a smaller volume and phased replication based on SnapMirror.  For each service, the switches are negotiated and executed at a specified time. <br><br>  At one of our customers, we have already done this procedure on the metro cluster.  Due to the large number of volumes, LUNs (&gt; 400) and long coordination of parts, downtime, etc.  migration took about 3 months without preparation. </li><li>  Migration without moving data using the Netapp 7-Mode Transition Tool (7MTT) utility and copy-free transition (CFT) mode.  This requires a second storage system with a minimum number of disks, to which, after preliminary preparation, the productive disk subsystem will be switched.  For all services one big downtime is negotiated. </li><li>  Migration with data copying by means of a host.  This is a traditional migration path between storage systems of any manufacturers. </li></ol><br>  Since the existing storage systems were still on vendor support, and there was a shortage of controller performance in the short term, the budget for the purchase of new controllers was not allocated.  In this regard, it was decided to migrate the controllers to Cluster Mode using 7MTT CFT.  One of the key requirements was the absence of noticeable interruptions in the work of the data storage system: most systems must run smoothly on weekdays.  Therefore, the main migration work on a productive storage system was scheduled for the weekend. <br><br>  The preparation phase began with the collection of information from the storage system and preliminary checks.  Specialized software NetApp 7MTT generates a list of warnings that can interfere with the migration or complete it with errors.  For example, for one of the systems, this list consisted of more than 200 items.  It was necessary to upgrade all the systems to the latest supported OS versions, to update the firmware of the controllers, disk shelves and the disks themselves.  Also, the new operating system has a different operation logic, requiring additional IP addresses and connections between the storage systems. <br><br>  The stop factor was quickly discovered: the client used a technology that was not based on replicating volumes entirely, but on qtree replication (a subsection to which restrictions on access, volume, etc.) apply. And it is impossible to migrate such SnapVault relations to the new OS .  As a result, before starting work it would be necessary to completely remove all replication copies.  So that the client after the move does not remain without backups, a backup based on the entire replication of the volumes was launched before the migration.  With SnapMirror, new backups were created next to the old ones; in the course of four weeks, a log of changes was accumulated.  And if at one of the sites there was enough space for this, then the second space was limited, it was necessary to gradually make copies of one of the volumes.  After four weeks, old relationships were removed and new ones were created.  Sufficiently long, phased process, which took in the case of one site about 1.5 months, and the second more than 3.  Additionally, I want to note that the procedure for stopping the Snapvault relationship is accompanied by the removal of the target qtree and its speed depends strongly on the number of files and to a lesser extent on its size.  For example, a qtree with 4 million files and 500GB in size was deleted within 24 hours. <br><br>  In the process there were various difficulties.  The bureaucratization of the processes of making changes on the client's systems increased the time required for the coordination of work.  Fortunately, we managed to agree on solving technical issues directly, discussing at a higher level only important, ‚Äúideological‚Äù issues, such as agreeing on a work plan and choosing specific dates for migration. <br><br>  Difficulties delivered using temporary storage.  Under the guidance of 7MTT, both storage systems were configured according to the requirements and pre-checks.  Then they turned off the old storage system and connected the disk shelves to the new one.  They checked everything again.  From the point of view of the NetApp software, the migration process is complete and everyone is dispensing <s>with champagne</s> .  But the next step was to return everything to the old client controllers.  The fact is that such a transition - from new controllers to old ones - is not officially supported.  After switching back, the OS started to make mistakes and complain about problems with the cluster.  After investigations, we managed to find out that the problem is related to the fact that the cluster suddenly switched to switched mode and did not want to go back to switchless.  It took a long time to fix the error.  The problems with the launch of the cluster were solved, without connecting the cables leading to the combat network at start-up, they raised the intracluster network on one cable, then added the second one.  By the way, it should be remembered that on old controllers and old OS versions the intracluster network can only be lifted on certain ports of a limited range of adapters, for example, on the FAS3250 this is e1a and e2a (the customer had to buy 10GB Ethernet cards). <br><br>  Additional time was laid for work on the second site, hoping to avoid at least some of the problems, but this did not help - the operating system behaved unpredictably.  The schedule moved twice.  In the first case, when we were working with the FAS3250, it was impossible to migrate the combat systems operating 24/7, due to an error in the recently changed settings of the customer‚Äôs network infrastructure (although when testing migration a week before the start of work, everything flew).  vMotion to remote storage was copied by virtuals at speeds less than 1 Mbit / s. <br><br>  During the migration process, the client partially changed the architecture.  Volumes that were delivered to their VMware vSphere infrastructure were previously issued over NFS Ethernet.  The client redid them, and they moved to the Fiber Channel.  During the migration process, it turned out that the LUN completely changed the ID and, accordingly, VMware saw new LUNs that were addressed to it with old data, and refused to connect them permanently.  As a result, thanks to the help of VMware experts, it was possible to connect these LUNs on an ongoing basis through the console, indicating that this is a snapshot of old datastores.  Then I had to restart VMware-hosts.  As a result, they were able to see the virtual machines and raise the virtual infrastructure.  And if the client continued to use NFS, then such a problem would not have occurred - the IP address and DNS name would remain the same as before. <br><br>  Work plan directly on migration days: <br><br><h4>  Friday: work with storage systems and RMS </h4><br><ol><li>  SnapVault and SnapMirror ceased all relationships, switched the temporary storage system and checked the readiness of the systems for migration.  We launched migration procedures for storage at 7MTT using the Copy Free Transition method.  Reconnect the combat disk shelves to the temporary controller. </li><li>  Migrated to 7MTT, migrated root vol swap controllers on disk shelves SHD SRK.  We installed new Ethernet adapters, launched the storage system for SRK, erased the configuration and downloaded the OS image over the network from the HTTP server.  Installed new versions of the firmware and OS (at this stage there were unexplained problems with downloading the image over the network. In the end, they downloaded directly from the laptop). </li><li>  We replaced the controllers in the cluster with old ones and connected the combat disk shelves to the storage system using the upgrading by moving storage procedure.  Restored the work of the cluster, reconfigured network interfaces (had to solve problems related to incorrect cluster operation) and installed license keys. </li></ol><br><h4>  Saturday: work with main storage </h4><br><ol><li>  We connected the temporary disk shelves to the temporary storage system and set it up again. </li><li>  Migrated important virtual machines on the storage to a remote data center using VMware vMotion. </li><li>  Migrated primary storage in 7MTT using the CFT method.  We turned off the main combat storage, connected its disk shelves to the controller and converted the OS metadata to 7MTT.  Migrated root vol swap controllers on disk shelves SHD SRK. </li><li>  We installed new Ethernet adapters and launched combat storage in diskless configuration, erased the settings, and then downloaded over the network from the HTTP server.  Install new firmware versions and OS.  We replaced the controllers in the cluster, connected the combat disk shelves to the storage system by upgrading by moving storage. </li><li>  Restored the work of the cluster, reconfigured network interfaces (fiddling with the incorrect work of the cluster due to the connected combat network interfaces).  Installed license keys. </li><li>  Restored storage connections to VMware servers, changed zoning on the SAN network, set up LUN mapping, and moved the volumes to a separate SVM to work with FC access.  LUN connected to ESXi.  Due to the fact that the LUN ID has changed, the datastores did not appear in automatic mode, we had to sequentially restart the ESXi servers and connect the LUN commands via esxcli. </li><li>  Reconfigured combat interfaces, renamed CIFS servers, and restored access to CIFS balls and NFS exports. </li></ol><br><h4>  Sunday: problem solving and software configuration </h4><br><ol><li>  Virtual machines migrated back from the storage system to the combat storage system. </li><li> Solved the problem with access to the folder in the recording mode from a Linux host.  We deployed the latest version of the monitoring software Netapp onCommand Unified Manager 7.3 and connected both storage systems to it. </li><li>  Analyzed the data on the current load on the units using the Unified Manager software, using the SSD connected the caching layer to the existing disk units (Flash / Storage Pool). </li><li>  We turned off the substitution storage system, created connections between clusters (Cluster peering, SVM peering) so that replication could be used.  Created a new SnapMirror relationship between the main storage system and the storage system of the CPM based on the existing volumes (which were used in the SnapMirror 7 mode relationship) with resynchronization of the changed data, and then converted the SnapMirror relationship into the SnapVault relationship (SnapMirror XDP). </li><li>  Both storage systems were connected to the Commvault software in the NetApp Open Replication mode with the help of Commvault technical support, it did not work out differently, although everything was done according to the instructions.  Configured sending Autosupport logs from combat storage and storage systems. </li></ol><br><h4>  Monday: polishing </h4><br><ol><li>  Verification of the work of the main productive storage system and storage system.  Solving possible problems and problems. </li><li>  Disconnection and dismantling of temporary equipment. </li></ol><br>  The migration itself took only two days.  The move was successful, all customer data is safe and sound.  The backup management system and integration with the existing IBS software was also preserved.  If the Commvault with OnCommand Unified Manager was previously used, after switching to Cluster Mode, it was decided to abandon Netapp Open Replication in order to connect Commvault directly to the storage controllers. <br><br>  The main recommendations that I can give on the results of this migration are to switch from 7 mode to Cluster Mode along with the replacement of controllers.  If you still plan to move to the second controllers, as in the case described above, then you need to plan for enough time to solve various problems that will arise when you move back to the old controllers.  Using migration without moving data using 7MTT CFT is quite a safe procedure, if you entrust it to professionals. <br><br>  Useful guides used during this migration: <br><br><ul><li>  <a href="https://docs.netapp.com/ontap-9/index.jsp">Netapp portal with ONTAP 9 OS documentation and 7MTT software</a> </li><li>  <a href="https://docs.netapp.com/platstor/topic/com.netapp.doc.hw-upgrade-controller/home.html">Netapp portal with FAS and AFF storage documentation, section on controller upgrades</a> </li><li>  <a href="https://hwu.netapp.com/">Hardware Universe portal to determine hardware compatibility for Netapp</a> </li><li>  <a href="https://documentation.commvault.com/commvault/v11/article%3Fp%3D33779.htm">Netapp Open Replication Tutorial for Commvault</a> </li></ul><br>  <i>Dmitry Kostryukov, Lead Storage Design Engineer</i> <i><br></i>  <i>Center for the design of computer systems "Jet Infosystems"</i> </div><p>Source: <a href="https://habr.com/ru/post/421721/">https://habr.com/ru/post/421721/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../421709/index.html">New (and old) units of measure in CSS that you‚Äôd hardly heard of</a></li>
<li><a href="../421711/index.html">Analysis of service requests using machine learning</a></li>
<li><a href="../421713/index.html">How to find bots on YouTube: external commentator interaction patterns</a></li>
<li><a href="../421717/index.html">Where do analysts study?</a></li>
<li><a href="../421719/index.html">Da Vinci 3D Color Printer. Photo report from XYZprinting Company presentation</a></li>
<li><a href="../421723/index.html">How to turn a computer into a radio, and other ways to extract music from computing systems</a></li>
<li><a href="../421725/index.html">Tale of how the AR SDK was searched on the hackathon</a></li>
<li><a href="../421727/index.html">Fujitsu boosts gallium nitride power output three times</a></li>
<li><a href="../421729/index.html">Timlid is a sergeant in the IT department</a></li>
<li><a href="../421731/index.html">How much software costs to build: what is the budget for developing an application</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>