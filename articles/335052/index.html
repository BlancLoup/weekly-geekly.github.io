<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>True neural network implementation from scratch in the C # programming language</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello, Habr! This article is intended for those who are roughly fumbling in the mathematical principles of the neural networks and in their essence in...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>True neural network implementation from scratch in the C # programming language</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/web/53d/edb/a9b/53dedba9b2b34fc5b71ed6506d920dbb.jpg" alt="image"></p><br><p>  Hello, Habr!  This article is intended for those who are roughly fumbling in the mathematical principles of the neural networks and in their essence in general, therefore I advise you to read this before reading.  Somehow you can understand what is happening <a href="https://habrahabr.ru/post/312450/">here</a> first, then <a href="https://habrahabr.ru/post/313216/">here</a> . </p><br><p>  Recently, I had to make a neural network for recognizing handwritten numbers (today it‚Äôs not exactly its code) as part of a school project, and, naturally, I began to understand this <del>  rubbish </del>  topic.  Having looked approximately enough about it on the Internet, I understood a little more than nothing.  But suddenly (as is usually the case), it came across a book by Simon Haykin (I don‚Äôt know why I didn‚Äôt google it before).  And then began the sweaty tasting of the materiel of neural networks, consisting of one matan. </p><a name="habracut"></a><br><p>  In fact, despite the abundance of mathematics, it is not so prohibitively complex.  Understand <del>  Satanist </del>  the average 11-grader comrade-fizmat or 1 ~ 2-course technician sharaga can do the scribbles and letters of this manual.  In addition, even if the book is rather voluminous and difficult to read, the things written in it <strong>really</strong> explain what is ‚Äúgoing on at the wheelbarrow under the hood‚Äù.  As you understand, I highly recommend (in no case do not advertise) "Neural networks. Simon Khaikin's complete course" for reading in case you have to deal with the use / writing / development of neural networks and other similar stuff.  Although there is no material in it about new-fashioned convolutional networks, no one bothers to google lectures from some charismatic employee of Yandex / Mail.ru / etc.  no one bothers. </p><br><p>  Of course, realizing the device grids, I could not just stop, because ahead of them was writing code.  In connection with his parallel lesson, which is to create games on Unity, the implementation language was <del>  lamp and nyashny </del>  Shar-pei version 7 (for it is the latest one).  It was at this moment that, being on the Internet, I realized that the number of intelligible tutorials on writing neural networks from scratch (without your frameworks) on Sharpe is infinite.  Okay.  I could use all sorts of Theano and Tensor Flow, <strong>BUT</strong> <del>  under the hood of my death-machine </del>  in my laptop there is a ‚Äúred‚Äù video card without special support from the API, through which the power of the GPU is accessed (after all, Theano / Tensor Flow / etc.). </p><br><div class="spoiler">  <b class="spoiler_title">Help shkolote prosharitsya:</b> <div class="spoiler_text"><p>  My video card is called ATI Radeon HD Mobility 4570. And if anyone knows how to use its capacity to parallelize neural network computing, please write in a comment.  Then you will help me, and perhaps this article will continue.  The proposal of other PL is not condemned. </p><br><p>  Simply, as I understand it, it is so old that it does not support it.  Maybe I'm wrong. </p></div></div><br><p>  <a href="https://habrahabr.ru/post/143129/">What</a> I <a href="https://www.youtube.com/watch%3Fv%3DvTcQxN8Odkk">saw</a> (the third is a kind of esoteric with an ugly code) can undoubtedly shock you as well, as those issued for neural networks are connected to them just like Yanix with high-quality rap.  Soon I realized that I can only rely on myself, and decided to write this article so that all users would not mislead others. </p><br><p>  Here I will not consider the network code for recognizing numbers (as mentioned earlier), because I left it on a flash drive, removing it from the laptop, but I‚Äôm lazy to look for this information carrier, and in this regard I will help you design a multilayered fully connected perceptron to solve the problem XOR and XAND (XNOR, xs how else). </p><br><p>  Before you start programming it, <del>  can </del>  it is necessary to draw on paper in order to facilitate the presentation of the structure and operation of the neuron.  My imagination resulted in the following picture.  And yes, by the way, this is a console application in Visual Studio 2017, with the .NET Framework version 4.7. </p><br><div class="spoiler">  <b class="spoiler_title">Brief infa on the grid (for those who are even talking about something)</b> <div class="spoiler_text"><p>  Multi-layer fully connected perceptron. <br>  One hidden layer. <br>  4 neurons in the hidden layer (the perceptron converged on this amount). <br>  The learning algorithm is backpropagation. <br>  The stopping criterion is overcoming the threshold value of the root-mean-square error over the epoch. (0.001) <br>  Learning rate - 0.1. <br>  The activation function is logistic sigmoid. </p></div></div><br><p><img src="https://habrastorage.org/web/419/d7f/22e/419d7f22e75c4487a880b242c1a68f06.png" alt="image"><br>  Then we need to realize that we need to write down the weight somewhere, carry out calculations, debug a little, and of course, tuples (but I don‚Äôt need a user for them).  Accordingly, using'and we have such. </p><br><div class="spoiler">  <b class="spoiler_title">Also</b> <div class="spoiler_text"><p>  The release || debug folder of this project contains the files (for each layer one by one) by the type name <em>(fieldname) _memory.xml</em> you know for what.  They are created in advance, taking into account the total number of weights of each layer.  I know that XML is not the best choice for parsing, I just had a little time to do this. </p></div></div><br><pre><code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">using</span></span> System.Xml; <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> System.Math; <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> System.Console;</code> </pre> <br><p>  We also have two types of computational neurons: hidden and weekend.  And weights can be read or written to memory.  We implement this concept with two listings. </p><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">enum</span></span> MemoryMode { GET, SET } <span class="hljs-keyword"><span class="hljs-keyword">enum</span></span> NeuronType { Hidden, Output }</code> </pre> <br><p>  Everything else will happen inside the namespace, which I will simply call: Neural Network. </p><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">namespace</span></span> <span class="hljs-title"><span class="hljs-title">NeuralNetwork</span></span> { <span class="hljs-comment"><span class="hljs-comment">//,    ,   }</span></span></code> </pre> <br><p>  First of all, it is important to understand why I depicted the neurons of the input layer in squares.  The answer is simple.  They do not calculate anything, but only capture information from the outside world, that is, they receive a signal that will be passed through the network.  As a result, the input layer has little to do with the other layers.  That is why the question is: do you have a separate class for it or not?  In fact, when processing images, video, sound, it is worth making it, only to accommodate the logic to transform and normalize this data to the form supplied to the input of the network.  That's why I still write the class InputLayer.  It contains a training sample organized by an unusual structure.  The first array in the tuple is the combination signals 1 and 0, and the second array is the pair of results of these signals after performing XOR and XAND operations (first XOR, then XAND). </p><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">InputLayer</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[], <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[])[] _trainset = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[], <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[])[]<span class="hljs-comment"><span class="hljs-comment">//-,    2  { (new double[]{ 0, 0 }, new double[]{ 0, 1 }), (new double[]{ 0, 1 }, new double[]{ 1, 0 }), (new double[]{ 1, 0 }, new double[]{ 1, 0 }), (new double[]{ 1, 1 }, new double[]{ 0, 1 }) }; // - public (double[], double[])[] Trainset { get =&gt; _trainset; }//     C# 7 }</span></span></code> </pre> <br><p>  Now we are implementing the most important thing, without which no neural network will become a terminator, namely, a neuron.  I will not use offsets, because I just don't want to.  The neuron will resemble the McCulloch-Pitts model, but have a different activation function (not a threshold), methods for calculating gradients and derivatives, its own type, and combined linear and non-linear transducers.  Naturally without the designer is not enough. </p><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">Neuron</span></span> { <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Neuron</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">double</span></span></span></span><span class="hljs-function"><span class="hljs-params">[] inputs, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">double</span></span></span></span><span class="hljs-function"><span class="hljs-params">[] weights, NeuronType type</span></span></span><span class="hljs-function">)</span></span> { _type = type; _weights = weights; _inputs = inputs; } <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> NeuronType _type; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[] _weights; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[] _inputs; <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[] Weights { <span class="hljs-keyword"><span class="hljs-keyword">get</span></span> =&gt; _weights; <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> =&gt; _weights = <span class="hljs-keyword"><span class="hljs-keyword">value</span></span>; } <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[] Inputs { <span class="hljs-keyword"><span class="hljs-keyword">get</span></span> =&gt; _inputs; <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> =&gt; _inputs = <span class="hljs-keyword"><span class="hljs-keyword">value</span></span>; } <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> Output { <span class="hljs-keyword"><span class="hljs-keyword">get</span></span> =&gt; Activator(_inputs, _weights); } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">double</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Activator</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">double</span></span></span></span><span class="hljs-function"><span class="hljs-params">[] i, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">double</span></span></span></span><span class="hljs-function"><span class="hljs-params">[] w</span></span></span><span class="hljs-function">)</span><span class="hljs-comment"><span class="hljs-function"><span class="hljs-comment">// { double sum = 0; for (int l = 0; l &lt; i.Length; ++l) sum += i[l] * w[l];// return Pow(1 + Exp(-sum), -1);// } public double Derivativator(double outsignal) =&gt; outsignal * (1 - outsignal);//            public double Gradientor(double error, double dif, double g_sum) =&gt; (_type == NeuronType.Output) ? error * dif : g_sum * dif;//g_sum -      }</span></span></span></span></code> </pre> <br><p>  Okay, we have neurons, but they need to be combined into layers for calculations.  Returning to my diagram above, I want to explain the presence of a black dotted line.  He separates the layers so as to show what they contain.  That is, one computational layer contains neurons and weights for communication with neurons of the <strong>previous</strong> layer.  Neurons are combined in an array, rather than a list, as this is less resource intensive.  Weights are organized by a matrix (two-dimensional array) of size (it is not difficult to guess) [the number of neurons in the current layer <strong>X the</strong> number of neurons in the previous layer].  Naturally, the layer initializes neurons, otherwise we say null reference.  At the same time, these layers are very similar to each other, but they have differences in logic, so the hidden and output layers must be implemented by the heirs of the same base class, which by the way turns out to be abstract. </p><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">abstract</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">Layer</span></span><span class="hljs-comment"><span class="hljs-comment">// protected       {//type          protected Layer(int non, int nopn, NeuronType nt, string type) {//   WeightInitialize numofneurons = non; numofprevneurons = nopn; Neurons = new Neuron[non]; double[,] Weights = WeightInitialize(MemoryMode.GET, type); for (int i = 0; i &lt; non; ++i) { double[] temp_weights = new double[nopn]; for (int j = 0; j &lt; nopn; ++j) temp_weights[j] = Weights[i, j]; Neurons[i] = new Neuron(null, temp_weights, nt);//  null    } } protected int numofneurons;//    protected int numofprevneurons;//    protected const double learningrate = 0.1d;//  Neuron[] _neurons; public Neuron[] Neurons { get =&gt; _neurons; set =&gt; _neurons = value; } public double[] Data//  null   ,   {//     set//(, , etc.) {//  input'     , for (int i = 0; i &lt; Neurons.Length; ++i) Neurons[i].Inputs = value; }//       } public double[,] WeightInitialize(MemoryMode mm, string type) { double[,] _weights = new double[numofneurons, numofprevneurons]; WriteLine($"{type} weights are being initialized..."); XmlDocument memory_doc = new XmlDocument(); memory_doc.Load($"{type}_memory.xml"); XmlElement memory_el = memory_doc.DocumentElement; switch (mm) { case MemoryMode.GET: for (int l = 0; l &lt; _weights.GetLength(0); ++l) for (int k = 0; k &lt; _weights.GetLength(1); ++k) _weights[l, k] = double.Parse(memory_el.ChildNodes.Item(k + _weights.GetLength(1) * l).InnerText.Replace(',', '.'), System.Globalization.CultureInfo.InvariantCulture);//parsing stuff break; case MemoryMode.SET: for (int l = 0; l &lt; Neurons.Length; ++l) for (int k = 0; k &lt; numofprevneurons; ++k) memory_el.ChildNodes.Item(k + numofprevneurons * l).InnerText = Neurons[l].Weights[k].ToString(); break; } memory_doc.Save($"{type}_memory.xml"); WriteLine($"{type} weights have been initialized..."); return _weights; } abstract public void Recognize(Network net, Layer nextLayer);//   abstract public double[] BackwardPass(double[] stuff);//  }</span></span></code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Salt abstract classes</b> <div class="spoiler_text"><p>  The Layer class is an abstract class, so you cannot create instances of it.  This means that our desire to preserve the properties of the "layer" is accomplished by inheriting the parent constructor through the keyword base and the empty constructor of the heir in one line (for all the constructor logic is defined in the base class, and it does not need to be rewritten). </p></div></div><br><p>  Now directly classes-heirs: Hidden and Output.  Immediately two classes in a single piece of code. </p><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">HiddenLayer</span></span> : <span class="hljs-title"><span class="hljs-title">Layer</span></span> { <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">HiddenLayer</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> non, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> nopn, NeuronType nt, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">string</span></span></span></span><span class="hljs-function"><span class="hljs-params"> type</span></span></span><span class="hljs-function">) : </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">base</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">non, nopn, nt, type</span></span></span><span class="hljs-function">)</span></span>{} <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">override</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Recognize</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">Network net, Layer nextLayer</span></span></span><span class="hljs-function">)</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[] hidden_out = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[Neurons.Length]; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; Neurons.Length; ++i) hidden_out[i] = Neurons[i].Output; nextLayer.Data = hidden_out; } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">override</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">double</span></span></span><span class="hljs-function">[] </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">BackwardPass</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">double</span></span></span></span><span class="hljs-function"><span class="hljs-params">[] gr_sums</span></span></span><span class="hljs-function">)</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[] gr_sum = <span class="hljs-literal"><span class="hljs-literal">null</span></span>; <span class="hljs-comment"><span class="hljs-comment">//          //    -,   //       for (int i = 0; i &lt; numofneurons; ++i) for (int n = 0; n &lt; numofprevneurons; ++n) Neurons[i].Weights[n] += learningrate * Neurons[i].Inputs[n] * Neurons[i].Gradientor(0, Neurons[i].Derivativator(Neurons[i].Output), gr_sums[i]);//  return gr_sum; } } class OutputLayer : Layer { public OutputLayer(int non, int nopn, NeuronType nt, string type) : base(non, nopn, nt, type){} public override void Recognize(Network net, Layer nextLayer) { for (int i = 0; i &lt; Neurons.Length; ++i) net.fact[i] = Neurons[i].Output; } public override double[] BackwardPass(double[] errors) { double[] gr_sum = new double[numofprevneurons]; for (int j = 0; j &lt; gr_sum.Length; ++j)//     { double sum = 0; for (int k = 0; k &lt; Neurons.Length; ++k) sum += Neurons[k].Weights[j] * Neurons[k].Gradientor(errors[k], Neurons[k].Derivativator(Neurons[k].Output), 0);//    gr_sum[j] = sum; } for (int i = 0; i &lt; numofneurons; ++i) for (int n = 0; n &lt; numofprevneurons; ++n) Neurons[i].Weights[n] += learningrate * Neurons[i].Inputs[n] * Neurons[i].Gradientor(errors[i], Neurons[i].Derivativator(Neurons[i].Output), 0);//  return gr_sum; } }</span></span></code> </pre> <br><p>  In principle, I described all the most important things in the comments.  We have all the components: training and test data, computational elements, their "conglomerates".  Now it‚Äôs time to bind everything with learning.  The learning algorithm is backpropagation, therefore the stop criterion is chosen by me, and my choice is to overcome the threshold value of the root-mean-square error of the epoch, which I chose to be 0.001.  For this goal, I wrote the Network class, which describes the state of the network, which is taken as a parameter of many methods, as you might have noticed. </p><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">Network</span></span> { <span class="hljs-comment"><span class="hljs-comment">//   InputLayer input_layer = new InputLayer(); public HiddenLayer hidden_layer = new HiddenLayer(4, 2, NeuronType.Hidden, nameof(hidden_layer)); public OutputLayer output_layer = new OutputLayer(2, 4, NeuronType.Output, nameof(output_layer)); //     public double[] fact = new double[2];//   2  //    double GetMSE(double[] errors) { double sum = 0; for (int i = 0; i &lt; errors.Length; ++i) sum += Pow(errors[i], 2); return 0.5d * sum; } //  double GetCost(double[] mses) { double sum = 0; for (int i = 0; i &lt; mses.Length; ++i) sum += mses[i]; return (sum / mses.Length); } //  static void Train(Network net)//backpropagation method { const double threshold = 0.001d;//  double[] temp_mses = new double[4];//     double temp_cost = 0;//     do { for (int i = 0; i &lt; net.input_layer.Trainset.Length; ++i) { //  net.hidden_layer.Data = net.input_layer.Trainset[i].Item1; net.hidden_layer.Recognize(null, net.output_layer); net.output_layer.Recognize(net, null); //    double[] errors = new double[net.input_layer.Trainset[i].Item2.Length]; for (int x = 0; x &lt; errors.Length; ++x) errors[x] = net.input_layer.Trainset[i].Item2[x] - net.fact[x]; temp_mses[i] = net.GetMSE(errors); //     double[] temp_gsums = net.output_layer.BackwardPass(errors); net.hidden_layer.BackwardPass(temp_gsums); } temp_cost = net.GetCost(temp_mses);//    //debugging WriteLine($"{temp_cost}"); } while (temp_cost &gt; threshold); //    "" net.hidden_layer.WeightInitialize(MemoryMode.SET, nameof(hidden_layer)); net.output_layer.WeightInitialize(MemoryMode.SET, nameof(output_layer)); } //  static void Test(Network net) { for (int i = 0; i &lt; net.input_layer.Trainset.Length; ++i) { net.hidden_layer.Data = net.input_layer.Trainset[i].Item1; net.hidden_layer.Recognize(null, net.output_layer); net.output_layer.Recognize(net, null); for (int j = 0; j &lt; net.fact.Length; ++j) WriteLine($"{net.fact[j]}"); WriteLine(); } } //  static void Main(string[] args) { Network net = new Network(); Train(net); Test(net); ReadKey();//    :) } }</span></span></code> </pre> <br><p>  The result of training. <br><img src="https://habrastorage.org/web/dc6/bfc/208/dc6bfc208a8c47d3b270aa5f71728aad.png" alt="image"></p><br><p>  Total by <del>  brain rape </del>  simple manipulations, we got the basis of a working neural network.  In order to make it do something else, it is enough to change the class InputLayer and select the network parameters for the new task.  After a while (which I don‚Äôt know specifically), I‚Äôll write a continuation of this article with a guide on creating a convolutional neural network in C # from scratch, and here I‚Äôll update it with links to the MLP reconciler for MNIST pictures ( <del>  but it is not exactly </del>  ) and article code in Python ( <del>  Exactly, but wait longer </del>  ). </p><br><p>  Behind this all, I will be glad to answer the questions in the comments, but for now, if you please, new things are waiting. <br>  UPD1: the <a href="https://habrahabr.ru/post/352632/">second part</a> <br>  PS: For those who wish to code the code. <br>  PPS: The network on the link above is the untrained nyasha-shyness. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/335052/">https://habr.com/ru/post/335052/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../335042/index.html">Look up from the bottom or Ubuntu Server for the developer of electronics. Part 2</a></li>
<li><a href="../335044/index.html">vCloud Director</a></li>
<li><a href="../335046/index.html">In the footsteps of Petya: find and exploit a software vulnerability</a></li>
<li><a href="../335048/index.html">Start learning Elixir right now! Translation of the entire series of articles is ready.</a></li>
<li><a href="../335050/index.html">Anatomy of GraphQL Queries</a></li>
<li><a href="../335056/index.html">Reliability Go in Dropbox infrastructure</a></li>
<li><a href="../335058/index.html">Puzzle game Neo Angle. Continuing development history and release in Appstore</a></li>
<li><a href="../335060/index.html">How to stand on the shoulders of a giant. Allowance for fintech startups</a></li>
<li><a href="../335064/index.html">GitLab 9.4 released: Related tasks and web monitoring applications</a></li>
<li><a href="../335066/index.html">Hackathon "Budget-Pro": the first step to winning the competition</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>