<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>KNN classifier</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="kNN stands for k Nearest Neighbor or k Nearest Neighbors - this is one of the simplest classification algorithms, also sometimes used in regression pr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>KNN classifier</h1><div class="post__text post__text-html js-mediator-article">  <b>kNN</b> stands for <i>k Nearest Neighbor</i> or <i>k Nearest Neighbors</i> - this is one of the simplest classification algorithms, also sometimes used in regression problems.  Because of its simplicity, it is a good example from which to start exploring the field of machine learning.  This article describes an example of writing the code of such a classifier in python, as well as visualization of the results. <br><a name="habracut"></a><br>  <i>The task of classification</i> in machine learning is the task of assigning an object to one of the predefined classes on the basis of its formalized features.  Each of the objects in this problem is represented as a vector in N-dimensional space, each dimension in which is a description of one of the features of the object.  Suppose we need to classify monitors: dimensions in our parameter space will be the diagonal size in inches, aspect ratio, maximum resolution, HDMI interface, cost, etc. The case of text classification is somewhat more complicated, they usually use a term-document matrix ( <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3D%25D0%2592%25D0%25B5%25D0%25BA%25D1%2582%25D0%25BE%25D1%2580%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BC%25D0%25BE%25D0%25B4%25D0%25B5%25D0%25BB%25D1%258C">described in machinelearning .ru</a> ). <br><br>  To train a classifier, you must have a set of objects for which classes are predefined.  This set is called a <i>training</i> set, its marking is done manually, with the involvement of specialists in the field of study.  For example, in the <a href="http://www.kaggle.com/c/detecting-insults-in-social-commentary">Detecting Insults in Social Commentary</a> problem for pre-assembled tests of comments by a person, the opinion is put on whether this comment is an insult to one of the participants in the discussion, the task itself is an example of a binary classification.  In the classification problem there can be more than two classes (multi-class), each of the objects can belong to more than one class (intersecting). <br><br><h4>  Algorithm </h4><br>  To classify each of the test sample objects, you must perform the following operations sequentially: <br><ul><li>  Calculate the distance to each of the objects of the training sample </li><li>  Select k objects of the training sample, the distance to which is minimal </li><li>  The class of the object being classified is the class most often found among the k nearest neighbors. </li></ul><br>  The examples below are implemented in python.  For their correct execution, besides python, you must have <i>numpy</i> , <i>pylab</i> and <i>matplotlib installed</i> .  The library initialization code is as follows: <br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> math <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pylab <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pl <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> matplotlib.colors <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ListedColormap</code> </pre> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  Initial data </h4><br>  Consider the work of the classifier by example.  To begin with, we need to generate data on which experiments will be performed: <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Train data generator def generateData (numberOfClassEl, numberOfClasses): data = [] for classNum in range(numberOfClasses): #Choose random center of 2-dimensional gaussian centerX, centerY = random.random()*5.0, random.random()*5.0 #Choose numberOfClassEl random nodes with RMS=0.5 for rowNum in range(numberOfClassEl): data.append([ [random.gauss(centerX,0.5), random.gauss(centerY,0.5)], classNum]) return data</span></span></code> </pre><br>  For simplicity, I chose a two-dimensional space in which the location of the expectation of a two-dimensional Gaussian with a standard deviation of 0.5 is chosen randomly from 0 to 5 along each axis.  A value of 0.5 is chosen so that the objects are well separable enough ( <a href="http://en.wikipedia.org/wiki/68-95-99.7_rule">three sigma rule</a> ). <br>  To look at the received sample, you need to run the following code: <br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">showData</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(nClasses, nItemsInClass)</span></span></span><span class="hljs-function">:</span></span> trainData = generateData (nItemsInClass, nClasses) classColormap = ListedColormap([<span class="hljs-string"><span class="hljs-string">'#FF0000'</span></span>, <span class="hljs-string"><span class="hljs-string">'#00FF00'</span></span>, <span class="hljs-string"><span class="hljs-string">'#FFFFFF'</span></span>]) pl.scatter([trainData[i][<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(trainData))], [trainData[i][<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(trainData))], c=[trainData[i][<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(trainData))], cmap=classColormap) pl.show() showData (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">40</span></span>)</code> </pre><br>  Here is an example of the image resulting from the execution of this code: <br><img src="https://habrastorage.org/storage2/779/cc8/d99/779cc8d9922300c27e845b0f30a078f0.png"><br><br><h4>  Getting a training and test sample </h4><br>  So, we have a set of objects, for each of which a class is defined.  Now we need to break this set into two parts: a training choice and a test sample.  To do this, use the following code: <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Separate N data elements in two parts: # test data with N*testPercent elements # train_data with N*(1.0 - testPercent) elements def splitTrainTest (data, testPercent): trainData = [] testData = [] for row in data: if random.random() &lt; testPercent: testData.append(row) else: trainData.append(row) return trainData, testData</span></span></code> </pre><br><br><h4>  Implementation of the classifier </h4><br>  Now, having a training sample, it is possible to implement the classification algorithm itself: <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Main classification procedure def classifyKNN (trainData, testData, k, numberOfClasses): #Euclidean distance between 2-dimensional point def dist (a, b): return math.sqrt((a[0] - b[0])**2 + (a[1] - b[1])**2) testLabels = [] for testPoint in testData: #Claculate distances between test point and all of the train points testDist = [ [dist(testPoint, trainData[i][0]), trainData[i][1]] for i in range(len(trainData))] #How many points of each class among nearest K stat = [0 for i in range(numberOfClasses)] for d in sorted(testDist)[0:k]: stat[d[1]] += 1 #Assign a class with the most number of occurences among K nearest neighbours testLabels.append( sorted(zip(stat, range(numberOfClasses)), reverse=True)[0][1] ) return testLabels</span></span></code> </pre><br>  To determine the distance between objects, you can use not only the Euclidean distance: the Manhattan distance, the cosine measure, the Pearson correlation criterion, etc. are also used. <br><br><h4>  Execution examples </h4><br>  Now you can evaluate how well our classifier works.  To do this, we will generate data, divide it into a training and test sample, classify the objects of the test sample and compare the real value of the class with the resulting classification: <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Calculate classification accuracy def calculateAccuracy (nClasses, nItemsInClass, k, testPercent): data = generateData (nItemsInClass, nClasses) trainData, testDataWithLabels = splitTrainTest (data, testPercent) testData = [testDataWithLabels[i][0] for i in range(len(testDataWithLabels))] testDataLabels = classifyKNN (trainData, testData, k, nClasses) print "Accuracy: ", sum([int(testDataLabels[i]==testDataWithLabels[i][1]) for i in range(len(testDataWithLabels))]) / float(len(testDataWithLabels))</span></span></code> </pre><br>  To assess the quality of the classifier's work, various algorithms and various measures are used, you can read in more detail here: <a href="http://en.wikipedia.org/wiki/Cross-validation_(statistics)">wiki</a> <br><br>  Now the most interesting thing remains: to show the classifier's work graphically.  In the pictures below, I used 3 classes, each with 40 elements, the value of k for the algorithm was equal to three. <br><br><img src="http://habrastorage.org/storage2/f6e/91d/bec/f6e91dbecf6e9b0e4a2f451941ca95ec.png"><br><br><img src="http://habrastorage.org/storage2/108/d21/433/108d21433235d5481de0569923f61611.png"><br><br>  The following code is used to display these pictures: <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Visualize classification regions def showDataOnMesh (nClasses, nItemsInClass, k): #Generate a mesh of nodes that covers all train cases def generateTestMesh (trainData): x_min = min( [trainData[i][0][0] for i in range(len(trainData))] ) - 1.0 x_max = max( [trainData[i][0][0] for i in range(len(trainData))] ) + 1.0 y_min = min( [trainData[i][0][1] for i in range(len(trainData))] ) - 1.0 y_max = max( [trainData[i][0][1] for i in range(len(trainData))] ) + 1.0 h = 0.05 testX, testY = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) return [testX, testY] trainData = generateData (nItemsInClass, nClasses) testMesh = generateTestMesh (trainData) testMeshLabels = classifyKNN (trainData, zip(testMesh[0].ravel(), testMesh[1].ravel()), k, nClasses) classColormap = ListedColormap(['#FF0000', '#00FF00', '#FFFFFF']) testColormap = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAAA']) pl.pcolormesh(testMesh[0], testMesh[1], np.asarray(testMeshLabels).reshape(testMesh[0].shape), cmap=testColormap) pl.scatter([trainData[i][0][0] for i in range(len(trainData))], [trainData[i][0][1] for i in range(len(trainData))], c=[trainData[i][1] for i in range(len(trainData))], cmap=classColormap) pl.show()</span></span></code> </pre><br><br><h4>  Conclusion </h4><br>  <b>kNN</b> is one of the simplest classification algorithms, so it often turns out to be ineffective in real-world problems.  In addition to classification accuracy, the problem of this classifier is the classification speed: if there are N objects in the training sample, M objects in the test choice, and K is the dimension of space, then the number of operations for classifying a test sample can be O (K * M * N).  Nevertheless, the kNN workflow is a good example to start exploring Machine Learning. <br><br><h4>  Bibliography </h4><br>  1. <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3D%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%25B1%25D0%25BB%25D0%25B8%25D0%25B6%25D0%25B0%25D0%25B9%25D1%2588%25D0%25B8%25D1%2585_%25D1%2581%25D0%25BE%25D1%2581%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25B9">The method of nearest neighbors on Machinelearning.ru</a> <br>  2. <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3D%25D0%2592%25D0%25B5%25D0%25BA%25D1%2582%25D0%25BE%25D1%2580%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BC%25D0%25BE%25D0%25B4%25D0%25B5%25D0%25BB%25D1%258C">Vector model on Machinelearning.ru</a> <br>  3. <a href="http://nlp.stanford.edu/IR-book/html/htmledition/k-nearest-neighbor-1.html">Book on Information Retrieval</a> <br>  4. <a href="http://scikit-learn.org/stable/modules/neighbors.html">Description of the method of the nearest neighbors in the framework of scikit-learn</a> <br>  5. <a href="http://www.ozon.ru/context/detail/id/4877842/">The book "Programming the collective mind"</a> </div><p>Source: <a href="https://habr.com/ru/post/149693/">https://habr.com/ru/post/149693/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../149684/index.html">Review of Content Designers (Content Construction Kit) for CMS Joomla</a></li>
<li><a href="../149685/index.html">Black color - taboo, myth or necessity?</a></li>
<li><a href="../149686/index.html">Forth processor on VHDL</a></li>
<li><a href="../149687/index.html">Create a Qt interface style using a table as an example.</a></li>
<li><a href="../149691/index.html">Pleasant trifle in the "Express Office" - comfortable keyboard</a></li>
<li><a href="../149694/index.html">RabbitMQ tutorial 1 - Hello World</a></li>
<li><a href="../149695/index.html">Monitoring PHP Code Performance with Pinba</a></li>
<li><a href="../149696/index.html">CERN - what is the organization for $ 900 million</a></li>
<li><a href="../149698/index.html">Radio 60-1700 MHz on RTL2832 for 20 bucks or SDR for beginners</a></li>
<li><a href="../149699/index.html">Ya.Subbotnik: BEM in the development of interfaces</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>