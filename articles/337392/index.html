<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Metacomputations and deep convolutional networks: an interview with ITMO professor</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="After AlphaGo's victory in March 2016, one of the strongest Go players in the world, Lee Cedol, spoke about deep learning methods almost everywhere. A...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Metacomputations and deep convolutional networks: an interview with ITMO professor</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/web/12a/94c/c5f/12a94cc5fa654b2f92ef3d1b3171bdd9.jpg"><br><br>  After AlphaGo's victory in March 2016, one of the strongest Go players in the world, Lee Cedol, spoke about deep learning methods almost everywhere.  And even Google did not miss the opportunity to call itself a company of machine learning and artificial intelligence. <br><br>  What is behind the term "deep learning"?  What are machine learning models and what are they written on?  We asked Alexey Potapov, a professor at the ITMO department of computer photonics and video informatics, to answer these and many other questions related to the Ministry of Defense and, in particular, with deep learning. <br><a name="habracut"></a><br> <i><img width="250" src="https://habrastorage.org/web/b11/2c5/57e/b112c557efd7424e9fc47cfb5776d23a.jpg" align="left"></i>  <i><b>Alexey Potapov</b></i> <i><br></i>  <i>Professor of the Department of Computer Photonics and Video Informatics ITMO.</i>  <i>Winner of the competition of grants for young scientific and pedagogical workers of universities in St. Petersburg.</i>  <i>The winner of the competition of grants of the President of the Russian Federation for state support of young Russian scientists - doctors of science in the field of "Information and telecommunication systems and technologies".</i> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <br>  <b>‚ÄúToday we almost everywhere hear about the boom of machine learning and especially deep learning.</b>  <b>How and in what situation can a simple IT person need it?</b> <br><br>  <b>Alexey Potapov</b> : Honestly, I don‚Äôt have an image of a ‚Äúsimple IT person‚Äù in my head, so it‚Äôs not easy for me to answer this question.  Is it possible to consider a ‚Äúsimple IT person‚Äù who can apply deep learning methods to automate their activities?  Not sure.  Probably, it will still be "difficult IT-Schnick."  Then, it‚Äôs probably not necessary for the simple to delve into anything special - except to maintain the current picture of the world and as a demotivator: sooner or later, ‚Äúrobots will take away the work‚Äù not only from McDonald‚Äôs employees, but also from IT specialists. <br><br>  <b>- And if a person has a technical education, and he decided to immerse himself in the topic - where should he start and where to go?</b> <br><br>  <b>Alexey Potapov</b> : It depends very much on how deeply and for what purpose a person decided to dive into the topic, as well as on the level of his training.  It is one thing to use existing pre-trained networks or train well-known models with implementation on high-level libraries, such as Keras.  This can be done and not the most advanced student, not really delving into the mathematics.  It is enough to study the documentation of the corresponding library and learn how to combine different layers, activation functions, etc. using examples.  - work as with ‚Äúblack boxes‚Äù. <br><br>  In general, in principle, it is better for many to begin with this - to feel deep learning in practice, to form your intuition about it.  Fortunately, now it is quite accessible without special knowledge. <br><br>  If a person does not approach this way or he wants to dig deeper, he can start with a superficial theory - with a description of the basic models (autoencoders, Boltzmann machines, convolution networks, recurrent blocks - tanh-RNN, LSTM, GRU, etc.) AS- Is.  Someone may have to start with logistic regression and perceptrons.  It is easy to read about all this on the Internet.  At this level of immersion, you can use lower-level libraries (such as Tensorflow and Theano) and implement models not from ready-made blocks, but at the level of tensor operations. <br><br>  In general, textbooks have now appeared that give a more systematic introduction to the field at a more advanced level than just a description of standard models.  For example, <a href="http://www.deeplearningbook.org/">deeplearningbook.org</a> is often mentioned, but, unfortunately, I have not read it.  Judging by the table of contents, this textbook is not so superficial and quite suitable for a somewhat in-depth study of the issue, although at the same time preference is given explicitly to those questions that the authors dealt with.  And this is not the whole area.  I also heard about good online courses.  Someone starts with them.  Unfortunately, I cannot give specific recommendations, since I myself began to study these questions before these textbooks and courses appeared. <br><br><blockquote>  For example, here are some pretty good courses: <br>  <a href="https://www.udacity.com/course/deep-learning--ud730">Introduction into Deep Learning</a> <br>  <a href="https://www.coursera.org/learn/neural-networks-deep-learning">Neural Networks and Deep Learning</a> <br>  <a href="http://cs231n.stanford.edu/">Convolutional Neural Networks for Visual Recognition</a> </blockquote><br>  <b>- In the report you are considering discriminant and generative models, can you explain with a simple example what is the difference between them and what are they for?</b> <br><br>  <b>Alexey Potapov</b> : Speaking broadly, discriminant and generative models are used by people everywhere.  Say, when the ancient people noticed a difference in the movement of planets and stars across the sky, they built a discriminant model: if a star has a backward motion, then it is a planet, and if it moves uniformly across the celestial sphere, then it is a star.  But the Kepler model is a generative model: it does not just pick out some distinguishing feature from observations, but allows you to reproduce the trajectory. <br><br>  A lot of discriminant models in the humanities, while the exact to the generative.  The discriminant model transforms the observed image into some description, often incomplete, identifies features that have a pragmatic meaning. <br><br>  Take the mushrooms.  You want to separate the poisonous mushrooms from edible.  The discriminant model will use various visual signs that do not themselves cause toxicity, but can be recognized by them, for example: the color of the cap, the color of the slice or the presence of a fringe.  It is very efficient and practical. <br><br>  The generative model in this case will, say, take the DNA of the fungus and model its formation by cell division or gene expression.  It will predict the appearance of the fungus and the substances present in them that can be used to predict toxicity. <br><br>  If you consider a mushroom, in order to use the generative model, you will have to guess its genotype, for example, from several samples of different genotypes, simulating mushrooms on the basis of them and comparing them with observation.  This is terribly inefficient if you do not add discriminant models here.  But, by defining the edibility of an unknown mushroom based on a discriminant model built on other mushrooms, you are very much at risk, whereas with a generative model there are much less chances to err.  In addition, with the generative model, you can solve not only the problem of recognition, but also, say, the derivation of a new variety of fungus with unique taste or visual properties.  Discriminant models are fundamentally incapable of this - in them the output is carried out in one direction.  The same, in general, is typical for discriminant and generative models in machine learning. <br><br>  <b>- Why did the deep learning and probabilistic approach shoot right now, and not 20-30 years ago?</b> <br><br>  <b>Alexey Potapov</b> : If you answer as simply as possible, then the point is obviously in computing resources, especially in the development of GPGPU, as well as in the availability of data for training.  All, of course, a little more complicated, and this is not the only reason, but still the main one.  Of course, it is also a matter of developing the views of researchers. <br><br>  For example, 20 years ago, it was still generally accepted that multi-layer network training is an extremely difficult optimization task, that simplified solutions will lead to extremely far from optimal results, and full-fledged optimization will be hopelessly slow.  This idea turned out to be erroneous, but then nobody knew for sure.  And few people are taken to engage in unpromising things. <br><br>  Back then, support vector machines were popular.  Many successfully dealt with them.  Now, by the way, the same with deep learning.  Everyone is engaged in it, and not in other hopelessly heavy approaches, which, however, can also fire in a decade or so.  But still the matter is largely connected with computational resources.  The same LSTM was known 20 years ago.  But if you transfer its modern use to the computers of those times, then it will immediately become unnecessary to anyone ... <br><br>  <b>Let us turn to technical issues.</b>  <b>Neuroprobability programming on Edward - how are the two paradigms combined and what does it allow to model?</b> <br><br>  <b>Alexey Potapov</b> : In Tensorflow, everything is built on tensors and operations with them and, most importantly, automatic differentiation is added, which is used to optimize a given loss function by gradient descent.  In Edward, all operations with tensors remain, but probability distributions are added on top or inside of tensors, as well as special loss functions for distributions, say, Kullback-Leibler divergence, and methods for optimizing them.  This makes it possible to simply and compactly set up and train generative deep networks, such as Generative Adversarial Networks (translation option: generative competing networks), variational autoencoders, as well as write your own similar models and make Bayesian inference over any networks. <br><br>  In the latter case, we do not just find a point estimate for the weights of the network links, but we obtain at the output a probability distribution for these weights.  This can give us information about the uncertainty with which the network performs its functions on specific input data.  In addition, we can enter any a priori information that can speed up learning and help with transfer training. <br><br>  <b>- Under what practical conditions would you definitely not recommend the use of deep learning?</b> <br><br>  <b>Alexey Potapov</b> : In fact, there are still a lot of such tasks where deep learning is not the best approach.  They can be divided into two categories - not enough computational resources and not enough data, although often these cases accompany each other.  This is often found even in image processing, where, it would seem, convolutional networks have long surpassed manual feature engineering. <br><br>  Take a robot vacuum cleaner that drives through the house, cleans it and at the same time compiles a room map.  Then the hosts come and turn on the light.  The robot must continue to successfully recognize locations with changed lighting conditions.  The on-board processor is as sickly as possible, so that it spends less energy and is cheaper.  In the cost of every penny in the account.  The room is previously unknown.  Annotated data for different lighting conditions, so that you can train the invariant features, of course, no.  Collect such data is expensive.  And making a representative sample is difficult. <br><br>  Of course, manually designed tags will not be the best.  But you can develop good signs in the face of lack of data.  And they can be made very fast ‚Äî an order of magnitude faster than a convolutional network with comparable quality.  But, of course, if you have enough data and there is no very hard limit on computing resources, then it is easier to train a network in a day, which can easily beat the signs that you developed a week, or even a month. <br><br>  There are, of course, other cases where deep learning is difficult to apply.  For example, this is complex data;  when making decisions, tasks that are combinatorial in nature.  But, as I understand it, the question was about the conditions when deep learning seems to be applicable, but it has to be abandoned. <br><br>  <b>- One of the possible classifications of learning models divides them into interpretable and uninterpreted.</b>  <b>As a rule, deep learning is attributed to the latter - is there any way, however, to look under the hood and see what she has learned?</b> <br><br>  <b>Alexey Potapov</b> : You can build a more complex metasnet that will ‚Äúunderstand‚Äù a simpler network, but this is computationally too expensive.  The question is rather what you want to know about the network.  Say, ‚ÄúBayesianization‚Äù of a network will give you information about uncertainty.  There are techniques for generating "bad" examples.  In the case of generative networks, you can visualize the learned diversity by changing one hidden variable.  That is, there are separate techniques for analysis.  But, of course, they will not show you the whole picture - if you use typical deep learning networks, and not some special compact models, then you cannot simply describe the operation of the network, since such a network is in principle complex. <br><br>  <b>Classic machine learning algorithms with a teacher (supervised), such as SVM, are trying to determine the class of an object, and without a teacher (unsupervised) - to break a set of objects into classes without labels.</b>  <b>From this point of view, autoencoders are doing something very strange: they are trying to recreate the data itself - why and why can it be necessary at all?</b> <br><br>  <b>Alexey Potapov</b> : Learning without a teacher does not necessarily come down to clustering.  This is only one possible class of models.  And in general, the division into teaching with and without a teacher is somewhat arbitrary.  You can count a class label simply as an additional categorical feature and then you will get the usual formulation of training without a teacher.  The question is rather in the objective function, what do you want to do as a result of the training.  When teaching with a teacher, as a rule, you are interested in the prediction of the very label and nothing else.  When teaching without a teacher, you do not have such a specific goal, it is much more general - you want to find patterns in the data.  Perhaps these are clusters into which images are grouped.  Perhaps a smooth small-sized manifold on which these images are located.  Thus, when learning without a teacher, you are trying to build a model of how the data is distributed.  This is usually a generative model, although not necessarily.  But how to verify the correctness of such a model?  It is necessary to compare the distribution specified by it with the real one, that is, it is necessary to recreate the data from the training sample.  Thus, autoencoders do nothing strange - they solve a completely typical learning task without a teacher.  This is exactly what all other methods of teaching without a teacher do, although not always explicitly. <br><br>  The same clustering can be interpreted as a projection of the image on the center of the cluster.  That is, each image is reconstructed as the center of the cluster closest to it, and the criterion is also the reconstruction error ‚Äî the distance from the real image to this center.  Just like in autoencoders, just another type of pattern.  Feature avtoenkoderov only how specifically they solve this problem.  Namely, they include both the generative and discriminant components of the model. <br><br>  <b>Many researchers (for example, Demis Hassabis and Andre Un) note that machine learning began with the definition of parameters (classical machine learning SVM, C4.5), switched to the construction of features (eg, CNN) and is now entering the stage of finding and building architecture.</b>  <b>How technically can such a meta-learning be implemented at all and how are the results comparable with traditional methods?</b> <br><br>  <b>Alexey Potapov</b> : If we talk about meta-training as a whole, then personally this direction is very interesting for me.  When you look at the variety of available models, it becomes obvious that none of them is fundamental, there are some heuristics in all that may not be very applicable, etc.  All these models are invented by people, which means that it is possible to automate the process of developing these models and their learning algorithms.  And just as manually constructed attributes are now easily overcome by trained attributes, so also manually constructed models and learning algorithms will hypothetically lose to meta-learning.  Now there are various separate experiments showing that this is indeed possible - in narrow experiments, meta-learning (and, more generally, AutoML) is superior to human development, although technology is not sufficiently developed for widespread use. <br><br><hr><br>  If the topics of Big Data and machine learning are close to you, just like us, we would like to draw your attention to a number of key presentations at the upcoming <a href="https://smartdataconf.ru/">SmartData</a> 2017 <a href="https://smartdataconf.ru/">conference</a> , which will be held on October 21 in St. Petersburg: <br><br><ul><li>  <a href="https://smartdataconf.ru/2017/spb/talks/3niwvfzujkukcce0kgqooi/">Deep convolutional networks for image segmentation</a> (Sergey Nikolenko, POMI RAS) <br></li><li>  <a href="https://smartdataconf.ru/2017/spb/talks/2slkmculqmyci8ysgaygsm/">From click to forecast and back: Data Science pipelines in OK</a> (Dmitry Bugaychenko, Odnoklassniki) <br></li><li>  <a href="https://smartdataconf.ru/2017/spb/talks/4zpqewdanwuuogkssqyqsw/">Automatic search for contact information on the Internet</a> (Alexander Sibiryakov, Scrapinghub) <br></li><li>  <a href="https://smartdataconf.ru/2017/spb/talks/hwncxtclswq6g68ksa8kw/">Applied machine learning in e-commerce: scenarios and architectures of pilots and combat projects</a> (Alexander Serbul, 1C-Bitrix) <br></li><li>  <a href="https://smartdataconf.ru/2017/spb/talks/h7fa9frvsekcyawoaasec/">Deep Learning: Recognizing scenes and sights on images</a> (Andrei Boyarov, Mail.ru) <br></li></ul></div><p>Source: <a href="https://habr.com/ru/post/337392/">https://habr.com/ru/post/337392/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../337378/index.html">RailsClub 2017: Interview with Richard Schneeman</a></li>
<li><a href="../337382/index.html">User behavior patterns</a></li>
<li><a href="../337386/index.html">Allure 2: New Generation Test Reports</a></li>
<li><a href="../337388/index.html">New NASH encryption algorithm</a></li>
<li><a href="../337390/index.html">Windows shortcuts: where do they lead and can they be dangerous?</a></li>
<li><a href="../337394/index.html">iOS development: quick start methods</a></li>
<li><a href="../337398/index.html">We teach the robot to cook pizza. Part 2: Neural Network Contest</a></li>
<li><a href="../337400/index.html">The practice of forming requirements in IT projects from A to Z. Part 5. Essence of the subject area and a little about strategies</a></li>
<li><a href="../337402/index.html">New course "Design of high-loaded systems" in Technopolis</a></li>
<li><a href="../337410/index.html">Satoshi Bomb</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>