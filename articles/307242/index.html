<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Pattern recognition in R using convolutional neural networks from the MXNet package</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This is a detailed instruction for pattern recognition in R using the deep convolutional neural network provided by the MXNet package. This article pr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Pattern recognition in R using convolutional neural networks from the MXNet package</h1><div class="post__text post__text-html js-mediator-article">  This is a detailed instruction for pattern recognition in R using the deep convolutional neural network provided by the <a href="https://github.com/dmlc/mxnet">MXNet</a> package.  This article provides a reproducible example of how to get 97.5% accuracy in a face recognition task on R. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6ae/0e3/943/6ae0e3943245f6da3888b8778a485c70.jpg" alt="image"></div><br><a name="habracut"></a><br><h3>  Foreword </h3><br>  I think some preface is still needed.  I am writing this <b>instruction</b> based on two considerations.  The first is to provide everyone with a fully reproducible example.  The second is to give answers to already raised questions.  Please note that this is only my way to address this problem, it is definitely not the only one, and definitely not the best. <br><br><h3>  Requirements </h3><br>  I'm going to use both <b>Python 3.x</b> (for receiving and preprocessing data) and <b>R</b> (actually, solving the problem), so it makes sense to install both.  The requirements for R packages are: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ol><li>  <a href="">MXNet</a> .  This package will provide the model that we are going to use in this article, in fact, a deep convolutional neural network.  You will not need a GPU version, the CPU will be sufficient for this task, although it may run slower.  If this happens, use the GPU version. <br><br></li><li>  <a href="https://bioconductor.org/packages/release/bioc/html/EBImage.html">EBImage</a> .  This package has many tools for working with images.  With him, working with images is a pleasure, the documentation is very clear and quite simple. </li></ol><br>  As for Python 3.x, install both <a href="http://www.numpy.org/">Numpy</a> and <a href="http://scikit-learn.org/stable/">Scikit-learn</a> .  It may also be worthwhile to install the <a href="https://www.continuum.io/downloads">Anaconda</a> distribution, which has a number of pre-installed popular packages for <b>data analysis</b> and <b>machine learning</b> . <br><br>  Once you have it all worked, you can proceed. <br><br><h3>  Data set </h3><br>  I am going to use the <a href="http://scikit-learn.org/stable/datasets/olivetti_faces.html">Olivetti face pack</a> .  This data set is a collection of 64-by-64-pixel images, in 0-256 grayscale. <br><br>  The data set contains 400 images of 40 people.  With 10 instances for each person usually use <b>uncontrolled</b> or <b>semi-</b> <b>controlled</b> algorithms, but I am going to try and use a specific <b>controlled</b> method. <br><br>  First you need to scale the image on a scale from 0 to 1. This is done automatically by the function that we are going to use to load the dataset, so you should not worry about it, but you need to know that it has already been done.  If you are going to use your own images, pre- <b>scale</b> them on a scale from 0 to 1 (or -1; 1, although the first one works better with neural networks, based on my experience).  Below is a Python script that needs to be executed to load a dataset.  Simply change the paths to your values ‚Äã‚Äãand execute from the IDE or terminal. <br><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># -*- coding: utf-8 -*- #  from sklearn.datasets import fetch_olivetti_faces import numpy as np #    Olivetti olivetti = fetch_olivetti_faces() x = olivetti.images y = olivetti.target #          print("Original x shape:", x.shape) X = x.reshape((400, 4096)) print("New x shape:", X.shape) print("y shape", y.shape) #   numpy np.savetxt("C://olivetti_X.csv", X, delimiter = ",") np.savetxt("C://olivetti_y.csv", y, delimiter = ",", fmt = '%d') print("\nDownloading and reshaping done!") ################################################################################ #  ################################################################################ # # Original x shape: (400, 64, 64) # New x shape: (400, 4096) # y shape (400,) # # Downloading and reshaping done!</span></span></code> </pre> <br>  In fact, this piece of code does the following: loads the data, changes the size of the pictures by X, and saves the numpy arrays to a .csv file. <br><br>  Array x is a tensor (tensor is a beautiful name for a multidimensional matrix) of size (400, 64, 64): this means that array x contains 400 copies of matrices 64 by 64 (read images).  If in doubt, simply output the first elements of the tensor and try to understand the data structure, taking into account what you already know.  For example, from the description of a data set, we know that we have 400 instances, each of which is an image 64 by 64 pixels.  We smooth the tensor x to a matrix of 400 by 4096 in size. That is, each matrix 64 by 64 (image) is now converted (smoothed) into a horizontal vector 4096 long. <br><br>  As for y, it is already a vertical vector of size 400. It does not need to be changed. <br><br>  Look at the resulting .csv file and make sure all conversions are clear. <br><br><h3>  Little pre-processing in R </h3><br>  Now we will use <b>EBImage</b> to resize the images to 28 by 28 pixels, and generate the training and test data sets.  You ask why I resize images.  For some reason, my computer doesn‚Äôt like 64-by-64-pixel images, and every time I launch a model with data, an error occurs.  Poorly.  But it is tolerable, since we can get good results with smaller pictures (but you can, of course, try to run from 64 to 64 pixels if you do not have such a problem).  So: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#         64  64  28  28  #    rm(list=ls()) #   EBImage require(EBImage) #   X &lt;- read.csv("olivetti_X.csv", header = F) labels &lt;- read.csv("olivetti_y.csv", header = F) #       rs_df &lt;- data.frame() #  :           for(i in 1:nrow(X)) { # Try-catch result &lt;- tryCatch({ #  (  ) img &lt;- as.numeric(X[i,]) #  64x64 ( EBImage) img &lt;- Image(img, dim=c(64, 64), colormode = "Grayscale") #     28x28  img_resized &lt;- resize(img, w = 28, h = 28) #    (    ,      !) img_matrix &lt;- img_resized@.Data #    img_vector &lt;- as.vector(t(img_matrix)) #   label &lt;- labels[i,] vec &lt;- c(label, img_vector) #   rs_df   rbind rs_df &lt;- rbind(rs_df, vec) #   print(paste("Done",i,sep = " "))}, #    (  ).     ! error = function(e){print(e)}) } #  .   - ,  - . names(rs_df) &lt;- c("label", paste("pixel", c(1:784))) #      #------------------------------------------------------------------------------- #      .   . #      set.seed(100) #  df shuffled &lt;- rs_df[sample(1:400),] #      train_28 &lt;- shuffled[1:360, ] test_28 &lt;- shuffled[361:400, ] #       write.csv(train_28, "C://train_28.csv", row.names = FALSE) write.csv(test_28, "C://test_28.csv", row.names = FALSE) # ! print("Done!")</span></span></code> </pre> <br>  This part should be clear enough if you are not sure what the output looks like, you should look at the <b>rs_df dataset</b> .  This should be a 400x785 dataset, approximately like this: <br>  label, pixel1, pixel2, ..., pixel784 <br>  0, 0.2, 0.3, ..., 0.1 <br><br><h3>  Model building </h3><br>  Now the most interesting, let's build a model.  Below is the script that was used to train and test the model.  Below are my comments and explanations to the code. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    rm(list=ls()) #  MXNet require(mxnet) #     #------------------------------------------------------------------------------- #       train &lt;- read.csv("train_28.csv") test &lt;- read.csv("test_28.csv") #       train &lt;- data.matrix(train) train_x &lt;- t(train[, -1]) train_y &lt;- train[, 1] train_array &lt;- train_x dim(train_array) &lt;- c(28, 28, 1, ncol(train_x)) test_x &lt;- t(test[, -1]) test_y &lt;- test[, 1] test_array &lt;- test_x dim(test_array) &lt;- c(28, 28, 1, ncol(test_x)) #    #------------------------------------------------------------------------------- data &lt;- mx.symbol.Variable('data') #    conv_1 &lt;- mx.symbol.Convolution(data = data, kernel = c(5, 5), num_filter = 20) tanh_1 &lt;- mx.symbol.Activation(data = conv_1, act_type = "tanh") pool_1 &lt;- mx.symbol.Pooling(data = tanh_1, pool_type = "max", kernel = c(2, 2), stride = c(2, 2)) #    conv_2 &lt;- mx.symbol.Convolution(data = pool_1, kernel = c(5, 5), num_filter = 50) tanh_2 &lt;- mx.symbol.Activation(data = conv_2, act_type = "tanh") pool_2 &lt;- mx.symbol.Pooling(data=tanh_2, pool_type = "max", kernel = c(2, 2), stride = c(2, 2)) #     flatten &lt;- mx.symbol.Flatten(data = pool_2) fc_1 &lt;- mx.symbol.FullyConnected(data = flatten, num_hidden = 500) tanh_3 &lt;- mx.symbol.Activation(data = fc_1, act_type = "tanh") #     fc_2 &lt;- mx.symbol.FullyConnected(data = tanh_3, num_hidden = 40) # .   , ..   - . NN_model &lt;- mx.symbol.SoftmaxOutput(data = fc_2) #    #------------------------------------------------------------------------------- #      mx.set.seed(100) #  .    CPU. devices &lt;- mx.cpu() #  #------------------------------------------------------------------------------- #   model &lt;- mx.model.FeedForward.create(NN_model, X = train_array, y = train_y, ctx = devices, num.round = 480, array.batch.size = 40, learning.rate = 0.01, momentum = 0.9, eval.metric = mx.metric.accuracy, epoch.end.callback = mx.callback.log.train.metric(100)) #  #------------------------------------------------------------------------------- #   predicted &lt;- predict(model, test_array) #   predicted_labels &lt;- max.col(t(predicted)) - 1 #   sum(diag(table(test[, 1], predicted_labels)))/40 ################################################################################ #  ################################################################################ # # 0.975 #</span></span></code> </pre> <br>  After loading the training and test dataset, I use the <code>data.matrix</code> function to turn each dataset into a numeric matrix.  Remember, the first column of data is the tags associated with each picture.  Make sure you remove the tags from <code>train_array</code> and <code>test_array</code> .  After separating the tags and dependent variables, you need to tell MXNet to process the data.  I do this on line 19 with the following piece of code: ‚Äúdim (train_array) &lt;- c (28, 28, 1, ncol (train_x))‚Äù for the training set and on line 24 for the test set.  Thus, we actually tell the model that the training data consists of ncol (train_x) samples (360 pictures) of size 28x28.  The number 1 indicates that the pictures are in grayscale, i.e., that they have only 1 channel.  If the pictures were in RGB, 1 would have to be replaced by 3, just as many channels would have pictures. <br><br>  Regarding the structure of the model, this is a variation of the LeNet model using the hyperbolic tangent as an activation function instead of ‚ÄúRelu‚Äù (transformed linear node), 2 convolutional layers, 2 layers of a subsample, 2 fully connected layers and a standard multi-variable logistic conclusion. <br><br>  Each <b>convolutional layer</b> uses a 5x5 core and is applied to a fixed set of filters.  Watch <a href="https://www.youtube.com/watch%3Fv%3DBFdMrDOx_CM">this wonderful video</a> to get a feel for convolutional layers.  <b>Layers of the subsample</b> use the classic ‚Äúmaximize‚Äù approach. <br><br>  My tests have shown that <b>tanh</b> works much better than sigmoid and Relu, but you can try other activation functions if you wish. <br><br>  As for the <b>hyperparameters of the</b> model, the level of training is slightly higher than usual, but it works fine as long as the number of periods is 480. The series size, equal to 40, also works well.  These hyperparameters are obtained by trial and error.  It was possible to do a search on the overlapping bands, but I did not want to over-complicate the code, so I used the proven method - trial and error. <br><br>  At the end you should get an accuracy of 0.975. <br><br><h3>  Conclusion </h3><br>  In general, this model was fairly easy to configure and run.  When running on a CPU, training takes 4-5 minutes: a little long if you want to experiment, but still acceptable for work. <br><br>  Considering the fact that we didn‚Äôt work with the data parameters at all and performed only the simplest and most common preprocessing steps, it seems to me that the results are quite good.  Of course, if we wanted to achieve higher ‚Äúreal‚Äù accuracy, we would need to do more cross-checks (which would inevitably take a long time). <br><br>  Thank you for reading, and I hope this article has helped you understand how to configure and run this particular model. <br><br>  The dataset source is the Olivetti face set created by AT &amp; T Laboratories Cambridge. </div><p>Source: <a href="https://habr.com/ru/post/307242/">https://habr.com/ru/post/307242/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../307226/index.html">New x features: Bind in UWP</a></li>
<li><a href="../307228/index.html">Sensor of absolute pressure BMP180</a></li>
<li><a href="../307232/index.html">We force FFMPEG to change HLS flows depending on the current throughput</a></li>
<li><a href="../307234/index.html">Migrating Xenserver 7 to linux raid</a></li>
<li><a href="../307236/index.html">Approximation of Pi by Mandelbrot Set</a></li>
<li><a href="../307248/index.html">Work with PGPool + ORM Yii2</a></li>
<li><a href="../307250/index.html">The intersection of the muzzles of top domains 1,000,000 by N-grams</a></li>
<li><a href="../307252/index.html">Basics of computer networks. Subject number 1. Basic network terms and network models</a></li>
<li><a href="../307256/index.html">Interface calculations (and a couple of small tips, how not to screw up, choosing a product)</a></li>
<li><a href="../307258/index.html">Pro shops and introverts: a number of implicit things</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>