<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The logic of consciousness. Part 7. Self-organization of the context space</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Earlier we said that any information has both external form and internal meaning. The external form is what we, for example, saw or heard. Meaning is ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The logic of consciousness. Part 7. Self-organization of the context space</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/cb6/aeb/52f/cb6aeb52f9ff86c0b4453c129d128103.jpg"><br>  Earlier we said that any information has both external form and internal meaning.  The external form is what we, for example, saw or heard.  Meaning is the interpretation we gave it.  Both the external form and the meaning can be descriptions composed of certain concepts. <br><br>  It was shown that if the descriptions satisfy a number of conditions, then they can be interpreted by simply replacing the concepts of the original description with other concepts, applying certain rules. <br><br>  Interpretation rules depend on the attendant circumstances in which we are trying to interpret the information.  These circumstances are called context in which information is interpreted. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The cerebral cortex consists of neural minicolumns.  We assumed that each crust minicolumn is a computational module that works with its own information context.  That is, each zone of the cortex contains millions of independent calculators of meaning, in which the same information receives its own interpretation. <br><br>  The mechanism of encoding and storing information was shown, which allows each mini-column of the cortex to have its full copy of the memory of all previous events.  The presence of its own full memory allows each minicolumn to check how its interpretation of current information is consistent with all previous experience.  Those contexts in which the interpretation turns out to be ‚Äúsimilar‚Äù to something previously familiar form a set of meanings contained in the information. <br><a name="habracut"></a><br>  In one cycle of its work, each zone of the cortex checks millions of possible hypotheses regarding how to interpret the incoming information, and selects the most meaningful of them. <br><br>  In order for the core to work this way, it is necessary to first form a context space on it.  That is, select all those "sets of circumstances" that affect the rules of interpretation. <br><br>  Our brain originated from evolution.  Its general architecture, principles of work, the system of projections, the structure of the zones of the cortex are all created by natural selection and incorporated into the genome.  But not everything is possible and it makes sense to pass through the genome.  Some knowledge must be acquired by living organisms independently after their birth.  The ideal adaptation to the environment is not to hereditarily keep the rules for all occasions, but to learn how to learn and find optimal solutions in any new circumstances. <br><br>  Contexts are the very knowledge that must be formed under the influence of the external world and its laws.  In this part we describe how contexts can be created and how subsequent self-organization can occur already within the context space. <br><br>  For each type of information, its own "tricks" work, allowing to form a context space.  We describe the two most obvious techniques. <br><br><h3>  Creating contexts with examples </h3><br>  Suppose there is a teacher who gave us some descriptions and showed how to interpret them.  At the same time, he did not just give the correct interpretation, but also explained how it was obtained, that is, what concepts were passed on during interpretation.  Thus, for each example, we became aware of the rules of interpretation.  In order to create contexts from these rules, they should be combined into groups so that, on the one hand, there are as few of these groups as possible, and on the other hand, that the rules within one group do not contradict each other. <br><br>  For example, you have sentences and their translations into another language.  At the same time there are comparisons of what words are translated.  For different sentences it may turn out that the same words will be translated differently.  The task is to find such semantic areas, they are contexts in which the translation rules will be stable and unambiguous. <br><br>  We write this formally.  Suppose that we have a memory <i>M</i> , consisting of examples of the form "description - interpretation - transformation rules". <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bf6/881/b1d/bf6881b1d957aa079772b8eb3f7943e7.png" width="171"></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/48d/6ec/bf1/48d6ecbf1f9d7ac80de8b188c780a2fd.png" width="134"></div><br>  The description and its interpretation are interconnected by the transformation rules <i>r</i> .  The rules tell how its interpretation was obtained from the original description.  In the simplest case, the transformation rules may simply be a set of rules for replacing some concepts with others. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/aea/9d6/584/aea9d6584ab919c5bf03661152d4e872.png" width="159"></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2d4/00c/87b/2d400c87b16107e6e024404d3b846a8d.png" width="155"></div><br>  That is, the transformation rules are a set of transformations ‚Äúsource concept - concept-interpretation‚Äù.  In the more general case, one concept may turn into several or several concepts that may be transformed into one, or one description from several concepts may pass into another complex description. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ff4/7c8/f73/ff47c8f73443d7d935c7397a744a94f4.png" width="151"></div><br>  We introduce the following consistency functions for the two transformation rules.  The number of matching transformations <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/981/047/ce8/981047ce8b5041d50467270d0e7d64c8.png" width="442"></div><br>  The number of contradictions <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/74b/12f/292/74b12f2929d7ff4497d6d8b5e195df9e.png" width="444"></div><br>  The number of contradictions shows how many transformations are present in which the same source information is transformed by the rules differently. <br><br>  Now we solve the clustering problem.  We divide all memories into a minimal number of classes with the condition that all memories of one class should not contradict each other with their own transformation rules.  The resulting classes will be the context space {Cont <sub>i</sub> | i = 1 N <sub>Cont</sub> }. <br><br>  For each context class, we will consider the transformation rules R as a set of all the rules for the elements included in this class. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c60/591/2a9/c605912a9d9d97d0f945a23adb7c3a39.png" width="119"></div><br>  For the required clustering, you can use the idea of ‚Äã‚ÄãEM (expectation-maximization) algorithm with the addition.  The EM algorithm implies that we first break objects into classes in any reasonable way, most often by random assignment.  For each class, we consider its portrait, which can be used to calculate the likelihood function of assignment to this class.  Then, we redistribute all the elements into classes, based on which class most likely corresponds to each of the elements. <br><br>  If any element is not plausibly attributed to any of the classes, then create a new class.  After assignment to classes, we return to the previous step, that is, we again recalculate the portraits of the classes, in accordance with those who fall into this class.  Repeat the procedure until convergence. <br><br>  In real cases, for example in our life, information does not appear all at once in one go.  It accumulates gradually as you gain experience.  At the same time, new knowledge is immediately included in the information circulation along with the old ones.  It can be assumed that our brain uses two-stage processing of new information.  At the first stage, a new experience is remembered and can be immediately used.  At the second stage, the correlation of the new experience with the old and more complex joint processing of this processing takes place. <br><br>  It can be assumed that the first stage occurs during awake and does not interfere with other information processes.  The second stage, it seems, requires the ‚Äústop‚Äù of the main activity and the transition of the brain to a special mode.  It seems that such a special regime is sleep. <br><br>  Proceeding from this, we change a little classical EM algorithm, bringing it closer to the possible push-pull scheme of the brain.  We will start with an empty set of classes.  We will use the ‚Äúwakefulness‚Äù phase to gain new experience.  We will change the portrait of each of the classes immediately after assigning to it a new element.  We will use the ‚Äúsleep‚Äù phase to rethink the accumulated experience. <br><br>  The likelihood function of assigning a memory element with transformation rules r to the context class with number j is chosen <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/844/de9/faf/844de9faf700dfa72f5adbeb00f3a738.png" width="293"></div><br>  The algorithm will look like: <br><br><ol><li>  Create an empty classset <br></li><li>  In the ‚Äúwakefulness‚Äù phase, we will consistently submit a new part of the experience. <br></li><li>  We will compare the r component of the elements and the portraits of the classes R. For each element we will choose classes with Œ¥ (r, R <sub>j</sub> ) = 0 and among them the class with the maximum œÜ (r, R <sub>j</sub> ), which corresponds to <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/97e/0fd/08e/97e0fd08eed5084d22d198b68f742b43.png" width="145"></div><br></li><li>  If there are no classes without contradictions, then we will create a new class for such an element and place it there. <br></li><li>  When adding an element to the class, we will recalculate the portrait of the class R. <br></li><li>  Upon completion of the ‚Äúwakefulness‚Äù phase, we proceed to the ‚Äúsleep‚Äù phase.  We will reconsolidate the accumulated experience.  For the experience obtained during the ‚Äúwakefulness‚Äù, and for a part of the old experience (ideally for all the old experience), we will make a reassignment to the context classes with the creation, if necessary, of the new classes.  If we change the assignment of any experience, we will change the portraits of both classes - the one from which the element has dropped out and the one to which it is now assigned. <br></li><li>  We will repeat the ‚Äúwakefulness‚Äù and ‚Äúsleep‚Äù phases, presenting a new experience and reconsolidating the old one. <br></li></ol><br><h3>  Finding translation rules for fixed contexts </h3><br>  The context creation mechanism described above is suitable for teaching, when the teacher explains the meaning of the phrases and at the same time indicates an interpretation for each of the concepts.  Another variant of creating contexts is related to the situation when the contextual transformation is known for teaching examples and there are two informational descriptions corresponding to the initial information and its interpretation.  But at the same time, it is not known which of the concepts exactly has passed into. <br><br>  Such a situation arises, for example, during the training of the primary visual cortex.  Fast, abrupt eye movements are called saccades and microsaccades.  Before and after the jump, the eye sees the same picture, but in a different context of displacement.  If a leap of a certain amplitude and direction is compared with a specific context, then the question will be, according to what rules does any visual description change in this context?  Obviously, having a sufficient set of ‚Äúsource picture - picture after offset‚Äù pairs related to identical offsets, one can construct a universal set of transformation rules. <br><br>  Another example.  Suppose you want to learn the translation into another language of a word in a specific context.  You have a set of sentences, some of which have this word.  And there are translations of all these sentences.  Pairs "sentence - translation" are pre-divided into contexts.  This means that for all translations related to the same context, this word is translated the same way.  But you do not know exactly which word in the translations corresponds to what you are looking for. <br><br>  The task of translation is solved very simply.  You need in the context in which the translation is sought to select those pairs of "sentence - translation", in which there is a search word, and see what is common in all translations.  This common will be the desired translation of the word. <br><br>  Formally, it can be written like this.  We have a memory <i>M</i> , consisting of memories of the form "description - interpretation - context." <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bf6/881/b1d/bf6881b1d957aa079772b8eb3f7943e7.png" width="171"></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/647/cea/841/647cea841a7a7adca8a99283a36528f5.png" width="165"></div><br>  The description and its interpretation are interconnected by the transformation rules R <sub>j</sub> , which are unknown to us.  But we know the context number Cont <sub>i</sub> in which these transformations are made. <br><br>  Now suppose that in the current description we have met the information fragment I <sup>orig</sup> and we have the context number j in which we want to get an interpretation of this description I <sup>trans</sup> . <br><br>  Select from memory M a subset of elements M 'such that their contextual transformation coincides with j and in the original description contains a fragment I <sup>orig</sup> , the transformation for which we want to find <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c20/32f/151/c2032f151bba8c34d3b4bc5233b81dfe.png" width="272"></div><br>  In all transformations I <sup>int</sup> <sub>i there</sub> will be a fragment of the transformation we are looking for (if the context allows such a transformation).  Our task is to define such a maximum along the length of the description, which is present in all interpretations of the elements of the set M '. <br><br>  Interestingly, the ideology of finding such a description coincides with the ideology of algorithms for quantum computing, based on the amplification of the required amplitude.  In the descriptions I <sup>int</sup> <sub>i of the</sub> set M ', all other elements, except the ones sought, are found at random.  This means that it is possible to organize the interference of descriptions so that the necessary information is amplified, and the unnecessary interfered randomly and extinguished each other. <br><br>  To do a ‚Äútrick‚Äù with an amplitude jump, it is required that the data be presented accordingly.  Let me remind you that we use a discharged binary code to encode each concept.  The description of several concepts corresponds to a binary array, resulting in the logical addition of the binary codes included in the description of the concepts. <br><br>  It turns out that if we take the binary arrays corresponding to the interpretations and execute an ‚Äúinterference‚Äù with them associated with amplifying the code we need, then the result will be a binary code of the transformation we need. <br><br>  Suppose that M 'contains N elements.  Let us match each description with an I <sup>int</sup> bit array of b bits of m, derived from the logical addition of the codes included in the description of the concepts.  Create an array of amplitudes A of dimension m <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/942/e62/8ec/942e628ec5f17f985de672ecd3bafab0.png" width="119"></div><br>  Increasing the number of examples N useful code elements will remain equal to 1 (or will be about 1 if the data contains an error), unnecessary elements will be reduced to a value equal to the probability of a random occurrence of one in the description code.  Having cut off the threshold that is guaranteed to exceed the random level (for example, 0.5), we get the desired code. <br><br><h3>  Correlated contexts </h3><br>  Usually, when determining the meaning of information, it turns out that quite a lot of high values ‚Äã‚Äãof the correspondence function occur in the context space.  This is due to two reasons.  The first reason is the presence of several meanings in the information.  The second reason is recognition in contexts close to the main one. <br><br>  Suppose that reference images of numbers are stored in memory.  For simplicity, let us assume that the images in memory are centered and brought to the same scale.  Again, for the sake of simplicity, we assume that the figures in the submitted images coincide with the reference ones, but can be in arbitrary locations.  In such a situation, the recognition of numbers in the picture is reduced to the consideration of descriptions in different contexts of displacements horizontally and vertically.  The context space can be represented as shown in the figure below.  Each context, which is indicated by a circle, corresponds to a certain offset applied to the picture in question. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fb5/fae/6ef/fb5fae6efa5aafd607a5ea00ddaf16e2.png" width="400"></div><br>  <i>The space of horizontal and vertical offset contexts (the offset is given in arbitrary units)</i> <br><br>  Let's submit an image with two letters A and B (figure below). <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/34d/9f8/38c/34d9f838c103f9d65d0528ef06f3652c.png" width="400"></div><br>  <i>Image with two letters</i> <br><br>  Each of the letters will be recognized in the context that leads it to the corresponding reference stored in memory.  In the figure below, the context most suitable for letters is highlighted in red. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f54/2bb/50f/f542bb50fccb92566ede56cc3943b4a8.png" width="400"></div><br>  <i>Context space.</i>  <i>Highlighted contexts with high match function values</i> <br><br>  But the algorithm for determining compliance can be constructed so that the correspondence, to one degree or another, will be determined not only by exact coincidence, but also by strong similarity of descriptions (such measures will be shown later).  Then a certain level of the correspondence function will be not only in the most appropriate contexts, but also in contexts close to them according to the transformation rules.  At the same time, proximity does not mean the number of matched rules, but a certain proximity of the descriptions obtained.  In the sense that two rules that translate one and the same concept into different but close concepts are two different rules, but at the same time two close transformations.  Close contexts are shown in the figure above in pink. <br><br>  After highlighting the meaning in the original image, we expect to receive a description of the form: the letter A in the context of a shift (2.1) and the letter B in the context of a shift (-2, -1).  But for this you need to leave only two main contexts, that is, to get rid of unnecessary contexts.  In this case, contexts close in meaning to local maxima, those indicated in the figure above are marked pink, are superfluous. <br><br>  In determining the meaning, we cannot take the global maximum of the correspondence functions and stop there.  In this case, we define only one of the two letters.  We can not focus only on a certain threshold.  It may happen that the level of compliance in the second local maximum will be lower than the level of contexts surrounding the first local maximum. <br><br>  In many real-world problems, contexts allow for the introduction of some reasonable proximity measures.  That is, for any context, you can specify contexts that are similar to it.  In such situations, the full definition of meaning becomes impossible without taking into account this mutual similarity. <br><br>  In the example above, we did not depict contexts as separate independent entities, but placed them on a plane in such a way that the proximity of the points representing the context became consistent with the proximity of contextual transformations.  And then we were able to describe the desired contexts as local maxima on the plane of the points representing contexts.  And superfluous contexts have become the closest surroundings of these local maxima. <br><br>  In general, you can use the same principle, that is, place points on the plane or in multidimensional space corresponding to the contexts so that their proximity best matches the proximity of the contexts.  After that, the selection of a set of meanings contained in the information is reduced to the search for local maxima in the space containing points of contexts. <br><br>  For a number of problems, the proximity of contexts can be determined analytically.  For example, for the task of visual perception, the main contexts are geometric transformations, for which the degree of their similarity can be calculated.  In artificial models, for some tasks this approach works well, but for biological systems a more universal approach based on self-organization is needed. <br><br>  Suppose that by some method we managed to form contexts.  Close contexts can be considered such contexts, whose correspondence functions respond in the same way to the same information.  Accordingly, the Pearson correlation coefficient between the context matching functions can be used as a measure of context similarity: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0e2/873/302/0e2873302451f1f9fa61249dac4bfdbc.png" width="243"></div><br>  For the entire set of contexts, it is possible to calculate the correlation matrix <i>R</i> , whose elements are the pairwise correlations of the matching functions. <br><br>  Then you can describe the following algorithm for the selection of meanings in the description: <br><br><ol><li>  In each of the contexts, the original description receives the interpretation and assessment of the correspondence between the interpretation and the memory. <br></li><li>  The global maximum of the correspondence function œÅmax and the corresponding maximum context-winner are determined. <br></li><li>  If œÅ <sub>max</sub> exceeds the cut-off threshold L <sub>0</sub> , then one of the semantic values ‚Äã‚Äãis formed, as interpreted in the winning context. <br></li><li>  The activity (the value of the correspondence functions) of all contexts, the correlation with which, on the basis of the matrix <i>R</i> , exceeds a certain threshold L <sub>1</sub> , is suppressed. <br></li><li>  The procedure is repeated from step 2 until œÅmax drops below the cut-off threshold L <sub>0</sub> . <br></li></ol><br>  As a result, we get all independent semantic interpretations and get rid of less accurate interpretations, but close to them. <br><br>  In convolutional neural networks, convolution at different coordinates is analogous to viewing images in various displacement contexts.  Using a set of kernels for convolution is similar to having different memories.  When a convolution on a nucleus in a certain place shows a high value, in neighboring simple cells responsible for the convolution on the same nucleus in neighboring coordinates, an increased value also occurs, forming a ‚Äúshadow‚Äù around the maximum value.  The reason for this ‚Äúshadow‚Äù is similar to the reason for raising the correspondence function in the neighborhood of the context with the maximum value. <br><br>  To get rid of duplicate values ‚Äã‚Äãand reduce the size of the network, use the max-pooling procedure.  After the layer of convolution, the space of the image is divided into non-intersecting areas.  In each region, the maximum value of the convolution is selected.  After that, a smaller layer is obtained, where the effect of ‚Äúshadow‚Äù values ‚Äã‚Äãis significantly weakened due to spatial coarsening. <br><br><h3>  Spatial organization </h3><br>  The correlation matrix of <i>R</i> determines the similarity of contexts.  In our assumptions, the cerebral cortex is located on the plane of a minicolumn, each of which is a processor of a particular context.  It seems quite reasonable to place minicolumns not randomly, but so that similar contexts are located as close as possible to each other. <br><br>  There are several reasons for this placement.  Firstly, it is convenient for finding local maxima in the context space.  Actually, the very concept of local maximum applies only to a set of contexts that have some kind of spatial organization. <br><br>  Secondly, it allows to ‚Äúborrow‚Äù interpretations.  It may be that the memory of a particular context does not contain an interpretation for any concept.  In this case, you can try to use the interpretation of any context that is close in meaning, which has this interpretation.  There are other very important reasons, but we'll talk about them later. <br><br>  The task of placing on a plane, on the basis of similarity, is close to the task of laying a weighted undirected graph.  In a weighted graph, edges do not only define connections between vertices, but also determine the weights of these relations, which can be interpreted, for example, as a measure of the proximity of these vertices.  The laying of a graph is the construction of such an image of the graph that best conveys the measure of proximity, given by the weights of the edges, through the distance between the vertices of the graph shown. <br><br>  To solve this problem, a spring analogy is used (Eades P., Congressus Nutnerantiunt - A heuristic for graph drawing drawing, 42, pp. 149‚Äì160. 1984.).  Connections between vertices are represented by springs.  The tension force of the springs depends on the weight of the corresponding edge and the distance between the connected vertices.  So that the peaks do not fall into one point, the repulsive force is added, which acts between all the vertices. <br><br>  For the resulting spring system, we can write the potential energy equation.  Energy minimization corresponds to finding the required graph layout.  In practice, this problem is solved either by simulating the motion of the vertices under the action of arising forces, or by solving a system of equations arising from recording the energy minimization conditions (Kamada, T., Kawai, S., Information Processing Letters, Vol. 31. - pp. 7-15. - 1989). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/60f/5a4/fb6/60f5a4fb6e37d532b4e4ff4d19c43cfa.png" width="600"></div><br> <i>    ,     </i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A specific analogue of laying a graph for the case of cellular automata is Schelling's segregation model (The Journal of Mathematical Sociology Volume 1, Issue 2, 1971. Thomas C. Schelling pages 143-186). In this model, the cells of the machine can take values ‚Äã‚Äãof different types (colors) or be empty. For non-empty cells, the satisfaction function is calculated, which depends on how much the cell environment resembles the cell itself. If the satisfaction is too low, then the value of this cell moves to a free cell. The cycle is repeated until the state of the machine is stabilized. As a result, if the parameters of the system allow it, the initial random disorder is replaced by islands consisting of values ‚Äã‚Äãof the same type (figure below). Segregation models are used, for example,to simulate the resettlement of people with different incomes, faith, race and the like.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/748/5cc/a44/7485cca447eb447b0cc3e4cb54906058.png" width="300"></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2be/130/69c/2be13069c78e454cd5ad476e3eb648aa.png" width="300"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The initial and final state of segregation in four colors</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The idea of ‚Äã‚Äãminimizing the energy of a graph and the principle of segregation of a cellular automaton can be applied with some changes to the spatial organization of contexts. </font><font style="vertical-align: inherit;">The following algorithm is possible:</font></font><br><br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> We define contexts characteristic of the incoming information. </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> We determine the matrix of mutual correlations of contexts. </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Randomly distribute contexts to the cells of a cellular automaton, the size of which allows to contain all contexts. </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Select a random cell containing the context. </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> We iterate over neighboring cells, for example, the eight nearest neighbors, as a potential place to move the context if the cell is empty, or to exchange contexts if not empty. </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> We calculate the change in the energy of the automaton in the case of each of the potential movements (exchanges). </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Make a move (exchange) that best minimizes energy. </font><font style="vertical-align: inherit;">If this is not, then we remain in place.</font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Repeat from step 4 until the state of the machine does not stabilize. </font></font><br></li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As a result, contexts are made so that similar contexts, if possible, are close to each other. </font><font style="vertical-align: inherit;">You can see how this self-organization happens on the video below.</font></font><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/i4qUm1ZVrpk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Each color point on the video corresponds to its context. Each context has several parameters that define it. Context correlation is calculated from the proximity of these parameters. In the given example, there is no creation of contexts from the initial information; it is just an illustration of the spatial organization for the case when correlations between contexts are already calculated in advance. A program illustrating self-organization by the permutation method is available </font></font><a href="http://www.aboutbrain.ru/programs/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">here</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the above example, the contexts correspond to all possible combinations of the four parameters. The first parameter is circular, two parameters are linear, the fourth takes two values. This corresponds to the contexts that can be used for image analysis. The first parameter describes the rotation, the second and third horizontal and vertical displacements, respectively, the fourth parameter indicates the information to which eye belongs.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Each parameter is associated with a color spectrum. According to the proximity of colors in the spectrum, one can judge the proximity of the parameter values. In the example, each context has four values. That is, its value for each of the parameters and, accordingly, its color in each of them. The squares show the colors of the contexts in each of the parameters. All color pictures show the same contexts, but in colors of different parameters (figure below).</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c3f/bf5/20c/c3fbf520c944fc392684cd631f667003.jpg" width="600"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The result of self-organization for contexts with four independent parameters</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The essence of spatial ordering is that the contexts in the process of moving have to find a compromise between all the parameters. In the video example, this compromise is achieved due to the fact that the linear parameters build a linear field. That is, contexts arise in such a way that they form a certain correspondence of the coordinate grid. By the way, this is how the contexts were set on the example above, when we talked about the recognition of letters.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">For the ring parameter of rotation over the entire surface, groups were formed that contain a complete set of colors. In the visual cortex, such constructions are called "turntables" or "tops." How do the "turntables" in the primary visual cortex, shown in the title picture. In more detail about it and about the fourth square with columns of an eye-dominance will be told in the following part. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Depending on how many intervals each of the parameters is split, a different number of contexts is obtained, which will certainly be close to each other. If the annular parameter is more fragmented than linear, a picture can be obtained when the contexts are lined up into one big ‚Äúspinner‚Äù (figure below). In this case, the linear parameters will form local linear fields distributed throughout the space.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f45/b50/f8d/f45b50f8db598800b2d9556798195887.png" width="600"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Spatial organization of contexts for a case with three independent parameters. The ring parameter dominates and forms a global ‚Äúspinner‚Äù; two linear parameters form local linear fields. The lower right square shows elements close to the one highlighted by a red dot.</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Regardless of what the permutation process converges to, similar contexts are mostly next to each other. The pictures below show examples of such proximity. In each figure, one of the contexts is highlighted in red, the brightness of the other contexts corresponds to the degree of their proximity to the selected context.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1ed/3da/07b/1ed3da07b9485eb85923dec0545b811f.png" width="500"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Distribution of the proximity of contexts in relation to the selected</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> When applying the described algorithm is required to take into account all the mutual correlation of contexts. The correlations themselves can be represented as connections of the cells of the automaton. Each cell is associated with all other cells. Each link is responsible for the pairwise correlation of those cells between which it passes. You can significantly reduce the number of connections if you use the </font><a href="https://en.wikipedia.org/wiki/Barnes%25E2%2580%2593Hut_simulation"><font style="vertical-align: inherit;">Barnes-Hut</font></a><font style="vertical-align: inherit;"> method</font></font><a href="https://en.wikipedia.org/wiki/Barnes%25E2%2580%2593Hut_simulation"><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(Barnes J., Hut P., A hierarchical O (N log N) force-calculation algorithm. Nature, 324 (4), December 1986). Its essence lies in replacing the influence of remote elements on the influence of quadrants, including these elements. That is, remote elements can be combined into groups and treated as one element with the average distance for the group and the average bond strength. This method works especially well for </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">calculating the mutual attraction of stars in star clusters</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0d8/bf2/c97/0d8bf2c97ee90e1ad6710dde0a07037e.png" width="600"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Spatial quadrants replacing individual stars</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> With a similarly organized map of contexts, it is possible to somewhat simplify the solution of the problem of searching for local maxima. </font><font style="vertical-align: inherit;">Now each context needs to be connected with other similar contexts located close to it, and with islands of similar contexts, taken for some distance. </font><font style="vertical-align: inherit;">The length of such connections after spatial organization will be less than before the organization, since it is this criterion that was the basis for calculating the energy of the system.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The benefits of spatial organization </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's go back to the translation example. Contests are semantic areas in which general translation rules apply. Having placed the contexts spatially, we get the neighboring groups of contexts related to approximately the same subject. Within the group, each of the individual contexts expresses the subtleties of the translation in a certain specified sense.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">How many contexts do you need? It would seem, the more the better. The more contexts available, the more details and shades of meaning can be taken into account when choosing a translation. But the flip side of the detail is the fragmentation of the interpretation experience. In order to know the translation rules for any words in a specific context, one must have experience in their translation in this particular context. Each translation example gives us a set of translation rules. If these rules are attributed to any one context that turned out to be the most successful, they will be inaccessible to other contexts.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Spatial organization and correlation links allow for any selected context to understand which contexts are close to him in meaning. This means that if there is no own translation experience in the selected context, you can use the experience of translating neighboring contexts of similar meaning, if such experience is there. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Appeal to the experience of neighbors allows you to search for a compromise between detalization of contexts and fragmentation of experience. For translation, it may look like a group of contexts related to general topics and located close to each other, jointly store the translation rules for this subject area. In addition, each separate context contains certain own clarifications that reflect the nuances of meaning.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In addition, the limited number of available contextual modules in real-world problems will require finding the best approximate solution. This can be greatly helped by the fact that the spatial context map itself largely takes into account the specifics of the information being processed.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Another benefit of spatial organization is the ability to "do several things at the same time." The conversation does not mean that we can, for example, simultaneously drive a car and talk on the phone. If different bark zones are involved for this, then there is nothing surprising in this. But everything becomes more interesting when we have to talk with someone and think about something of our own, or talk at once on two different topics with different interlocutors, or, like Caesar, write and simultaneously speak on another topic. It turns out that the same areas of the cortex are forced to work simultaneously with several information flows.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Probably, everyone noticed on himself that it was possible to combine things or thoughts only when they differed quite strongly in meaning. As soon as the meaning begins to intersect, confusion begins, or one thought completely supersedes the other. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In our model, the combination of information flows can be explained by their separation in the space of contexts. While each of the meanings is processed in its place, nothing terrible happens. The convergence of meanings and, accordingly, contexts makes the combination impossible. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In practice, with the convergence of meanings, the so-called ‚Äúbrain overload‚Äù can occur, when a person falls into a trance state, loses his own thought and ceases to control himself. The so-called ‚Äúgypsy hypnosis‚Äù is based on this effect.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If several people begin to say something different at the same time, most likely it will not make a special impression on you. </font><font style="vertical-align: inherit;">Most likely, you will concentrate on someone and you will follow his thought. </font><font style="vertical-align: inherit;">Trying to hear everyone at the same time is unlikely to give something. </font><font style="vertical-align: inherit;">You will hear fragments of phrases and most likely give up this activity.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But when you are surrounded by a crowd of gypsies, the situation changes. </font><font style="vertical-align: inherit;">You are alert and trying to control the situation. </font><font style="vertical-align: inherit;">You try to understand everything you are told. </font><font style="vertical-align: inherit;">As a rule, all Roma "beat" in one topic. </font><font style="vertical-align: inherit;">The "center" gypsy begins to talk about your future, prophesy, warn or threaten. </font><font style="vertical-align: inherit;">The rest echo her and say something consonant. </font><font style="vertical-align: inherit;">At this point, most people have a trance. </font><font style="vertical-align: inherit;">Waking up outside the gypsy environment, a person discovers the loss of a wallet, hours, and confidence in his own normality.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Adaptive calculation of correlations. </font><font style="vertical-align: inherit;">The role of synaptic plasticity</font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the process of spatial organization and at the very definition of meaning, one needs to know the mutual similarity of contexts, which can be determined through the correlation of the activity of the correspondence functions. </font><font style="vertical-align: inherit;">Correlation analysis is based on the assumption that a random process is stationary over time, that is, averaging over a set of realizations is equivalent to time averaging. </font><font style="vertical-align: inherit;">Since obtaining new experience changes the contexts themselves, and therefore their correspondence functions, it turns out that it is reasonable not to use the entire set of observations for calculating correlations, but to proceed only from their relatively recent part. </font><font style="vertical-align: inherit;">Such an approach, in particular, is used in economics for short-term adaptive forecasting of time series, when regularities may have a local temporal character.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To calculate the correlation, which takes into account mainly the last N active observations, you can use an adaptive scheme. </font><font style="vertical-align: inherit;">We introduce the cut-off threshold L </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and the threshold function:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/924/455/79a/92445579af78687d9b0a47a3a52edd96.png" width="143"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">When a new experience arises, we will change the correlation between two contexts only if at least one of them has the compliance function overcoming the threshold L </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">If both functions have crossed the threshold, then we will increase the correlation, if only one, then decrease. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">For convenience of recording, we introduce a small value:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6c9/e0a/48f/6c9e0a48f46d29cf1928fc8a6a57bad3.png" width="50"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Pearson correlation is calculated by the formula: </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/40d/8a9/6d5/40d8a96d52548c268837958678fe6966.png" width="93"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Each of the elements involved in the calculation can be adaptively recalculated when a new experience appears. </font><font style="vertical-align: inherit;">For matching functions of contexts with numbers i and j, you can write</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ab1/bc4/eae/ab1bc4eae76023c611f9897e253495c7.png" width="77"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/318/daa/4a5/318daa4a59929e5b323831881c190b8d.png" width="77"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> If at least one of the values, x or y, is non-zero, then we recalculate </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c89/dab/bf6/c89dabbf6419a430b6e005485b255935.jpg" width="384"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Such a correlation will tend to unity if contexts are always activated together and to zero when the process is random. </font><font style="vertical-align: inherit;">In accordance with this adaptive calculation, weights of the links connecting the contexts can be formed.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Synaptic plasticity </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Strengthening the strength of the connection between contexts at the time of their joint operation is very similar to Hebb's rule (Hebb DO - The Organization of Behavior. John Wiley &amp; Sons, New York, 1949). </font><font style="vertical-align: inherit;">In accordance with it, neurons, triggered together, increase the strength of the connection between them. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If a formal neuron works as a linear adder, then its output is the weighted sum of its inputs.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b18/c05/254/b18c05254ebb581d857dbc0df9b0ee30.png" width="98"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Hebb's rule for such a neuron is </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/480/3f3/476/4803f34768a4d75a01e59f274a69c04c.png" width="260"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Where </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">n</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> is a discrete time step, and Œ∑ is the learning speed parameter. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">With such training, the weights of those inputs to which the signal x </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (n) </font><font style="vertical-align: inherit;">is applied increase </font><font style="vertical-align: inherit;">, but this is done the more strongly the more active is the reaction of the neuron itself (y). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">With the direct application of the Hebbian rule, the neuron weights grow indefinitely. </font><font style="vertical-align: inherit;">This is easily avoided if you require that the total amount of weights of each neuron remain constant. </font><font style="vertical-align: inherit;">Then, instead of increasing weights, they will be redistributed. </font><font style="vertical-align: inherit;">Some weights will increase by decreasing others. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Given the general normalization of weights, the Hebbian learning rule takes the form of the Oja learning rule (Oja, 1982)</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/01e/6d9/f92/01e6d9f92d8d8833790bbb04529a0419.png" width="232"></div><br>  In this formula, x <sub>i</sub> <sup>t</sup> is the state of the inputs of the neuron, w <sub>i</sub> <sup>t</sup> are the synaptic weights of the neuron, and y <sup>t</sup> is the activity of its output, obtained by weighted summation of the inputs. <br><br>  The above-described correlation and formal neuron learning outwardly correspond to the principle of enhancing the connections of neurons that work together, but they also implement completely different mechanisms.  In the classical Hebbovs training, due to the weighted summation of the signals of the inputs and the subsequent joint normalization of the weights, a redistribution of the weights of the neuron arises in such a way that it is tuned to a specific characteristic stimulus.  In our model, nothing like this happens.  The essence of correlation weights is the description of the picture of the proximity of contexts in their spatial environment.  Weights are trained independently of each other and this has nothing to do with any characteristic stimulus.  There is also no requirement for normalization: restricting the growth of weights is a natural consequence of the limited correlation coefficients. <br><br>  For the real brain, synaptic plasticity is known.  Its essence is that the effectiveness of synaptic transmission is not constant and may vary depending on the pattern of current activity.  Moreover, the duration of these changes can vary greatly and be caused by different mechanisms. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c80/143/ac1/c80143ac1e9572af30b8c63a20a5d2ad.png" width="400"></div><br>  <i>Dynamics of changes in synaptic sensitivity.</i>  <i>(A) - Facilitation, (B) - Strengthening and depression, (C) - post-tetanic potency, (D) - long-term potency and long-term depression (Nicolls J., Martin R., Wallace B., Fuchs P., 2003)</i> <br><br>  A short volley of spikes can cause relief (facilitation) of the selection of a mediator from the corresponding presynaptic terminal.  Facilitation appears instantly, is preserved during a volley, and is substantially noticeable for another 100 milliseconds after the end of stimulation.  The same short exposure can lead to the suppression (depression) of the release of a neurotransmitter lasting several seconds.  Facilitation can go to the second phase (gain), with a duration similar to the duration of the depression. <br><br>  A long high-frequency pulse train is usually called tetanus.  The name is connected with the fact that this series precedes tetanic muscle contraction.  Receipt of tetanus at the synapse, can cause the post-tetanic potency of the release of a mediator, observed for several minutes. <br><br>  Repeated activity can cause long-term changes in synapses.  One of the reasons for these changes is an increase in calcium concentration in the postsynaptic cell.  A strong increase in concentration triggers a cascade of secondary mediators, which leads to the formation of additional receptors in the postsynaptic membrane and an overall increase in receptor sensitivity.  A weaker increase in concentration has the opposite effect - the number of receptors decreases, their sensitivity decreases.  The first condition is called long-term potency, the second - long-term depression.  The duration of such changes is from several hours to several days (Nicolls J., Martin R., Wallace B., Fuchs P., 2003). <br><br>  When Donald Hebb formulated his rule, very little was known about the plasticity of synapses.  When the first artificial neural networks were created, they used the idea of ‚Äã‚Äãthe possibility of synapses to change their weights as the key one.  It is the smooth adjustment of the scales that allowed neural networks to adapt to the incoming information and to highlight in it any common properties.  The grandmother's neuron, which I constantly refer to, is a direct consequence of the idea of ‚Äã‚Äãtuning synaptic weights to a characteristic stimulus. <br><br>  Later, when the plasticity of real synapses became better studied, it turned out that it had little in common with the rules for learning neural networks.  First, in most cases, changes in the efficiency of synaptic transmission pass without a trace after a short time.  Secondly, there is no noticeable consistency in learning various synapses, that is, nothing to remind joint normalization.  Thirdly, the efficiency of transmission changes under the action of incoming signals from the outside and it is not very clear how it depends on the reaction of the postsynaptic, that is, the signal receiving neuron.  Add to this that, besides all that, real neurons do not work as linear or threshold adders. <br><br>  The result was an interesting situation.  Neural networks work and show good results.  Many who are well versed in neural networks, but far from biology, believe that artificial neural networks are in many ways similar to the brain.  This concept of similarity is based on the history of the emergence of artificial neural networks and, accordingly, on those ideas about neurons that existed once upon a time.  Those researchers who are better at brain biology prefer to say that many ideas of artificial neural networks are inspired by the mechanisms of the real brain.  However, one should be aware of the extent of this ‚Äúinstilling‚Äù. <br><br>  My constant return to grandmother's neurons is largely due to attempts to show the difference in understanding the role of synaptic plasticity in the classical approach and in the proposed model.  In the classical model, the change in the weights of synapses is the mechanism for tuning neurons to a characteristic stimulus.  I believe that the role of synapse plasticity is quite different.  It is possible that synaptic plasticity in the neurons of the real brain is partly due to the mechanisms for setting contextual correlations. <br><br><h3>  Difference of context maps from Kohonen maps </h3><br>  Spatial organization in neural networks is usually associated with self-organizing Kohonen maps (T. Kohonen, Self-Organizing Maps). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4da/d02/e64/4dad02e64e393bb60cac1ef4cd2fc1f9.gif" width="300"></div><br><br>  Suppose we have input information given by the vector <i><b>x</b></i> .  There is a two-dimensional lattice of neurons.  Each neuron is connected with the input vector <i><b>x</b></i> , this connection is determined by the set of weights <i><b>w</b></i> <sub>j</sub> .  Initially, we initiate a network of random small weights.  By supplying an input signal, for each neuron one can determine its level of activity as a linear adder.  Take the neuron that will show the most activity, and call it the winning neuron.  Next, move its weight in the direction of the image, which he was like.  Moreover, we will do the same procedure for all its neighbors.  We will relax this shift as we move away from the winning neuron. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/605/d39/d78/605d39d786dd5dbacc84e89c0d922997.png" width="376"></div><br>  Here Œ∑ (n) is the learning rate, which decreases with time, h is the amplitude of the topological neighborhood (dependence on n assumes that it also decreases with time). <br><br>  The amplitude of the neighborhood can be selected, for example, by the Gaussian function: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/11c/114/cbe/11c114cbea52b50b57a5f742406bcf1c.png" width="222"></div><br>  Where <i>d</i> is the distance between the corrected neuron <i>j</i> and the winning neuron <i>i</i> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2f0/ba4/c74/2f0ba4c7461aee3249b722ed18e8f59c.png" width="200"></div><br>  <i>Gauss function</i> <br><br>  As you learn on such a self-organizing map, zones will be allocated corresponding to how the training images are distributed.  That is, the network itself will determine when similar pictures will meet in the input stream, and will create for them close representations on the map.  At the same time, the more the images will differ, the more separate their representations will be located apart from each other.  As a result, if we appropriately color the learning result, it will look something like the one shown in the figure below. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/07c/2f7/c76/07c2f7c7639f64480981b0f3eb8d33b7.gif" width="200"></div><br>  <i>Kohonen Card Learning Result</i> <br><br>  The result of learning a Kohonen map after coloring may turn out to be similar in appearance to the arrangement of contexts obtained by rearranging them.  This similarity should not be misleading.  It is about different things.  Kohonen maps are based on adapting the weights of neurons to the indicative descriptions given.  This, in fact, all the same "grandmother's neurons."  The supplied information "sculpts" from neurons detectors of some average "grandmothers".  In the centers of the colored areas, ‚Äúgrandmothers‚Äù are more or less similar to something, closer to the borders of the regions are ‚Äúgrandmother-mutants‚Äù.  There are hybrids of neighboring "grandmothers", "grandfathers", "cats" and "dogs". <br><br>  When you try to try Kohonen maps to the real brain, a significant problem arises.  This is the well-known dilemma of "stability-plasticity."  The new experience changes the portraits of "grandmothers", forcing all the neighbors of the "grandmother-winner" to change in her direction.  As a result, the network can change its organization, erasing previously acquired knowledge.  To stabilize the network, the learning rate has to be reduced with time.  But this leads to the "ossification" of the network and the inability to continue learning.  In our self-organization, the rearrangement of contexts does not violate their integrity.  Contexts are moved to a new location, but at the same time, all information stored in them is kept intact. <br><br>  In the next part I will talk about spatial self-organization in the real crust and try to show that much of what is observed experimentally can be explained precisely in our model. <br><br>  Alexey Redozubov <br><br><blockquote>  <a href="https://habrahabr.ru/post/308268/">The logic of consciousness.</a>  <a href="https://habrahabr.ru/post/308268/">Introduction</a> <br>  <a href="https://habrahabr.ru/post/308370/">The logic of consciousness.</a>  <a href="https://habrahabr.ru/post/308370/">Part 1. Waves in the cellular automaton</a> <br>  <a href="https://habrahabr.ru/post/308878/">The logic of consciousness.</a>  <a href="https://habrahabr.ru/post/308878/">Part 2. Dendritic waves</a> <br>  <a href="https://habrahabr.ru/post/308972/">The logic of consciousness.</a>  <a href="https://habrahabr.ru/post/308972/">Part 3. Holographic memory in a cellular automaton</a> <br>  <a href="https://habrahabr.ru/post/309366/">The logic of consciousness.</a>  <a href="https://habrahabr.ru/post/309366/">Part 4. The secret of brain memory</a> <br>  <a href="https://habrahabr.ru/post/309626/">The logic of consciousness.</a>  <a href="https://habrahabr.ru/post/309626/">Part 5. The semantic approach to the analysis of information</a> <br>  <a href="https://habrahabr.ru/post/310214/">The logic of consciousness.</a>  <a href="https://habrahabr.ru/post/310214/">Part 6. The cerebral cortex as a space for calculating meanings.</a> <br>  <a href="https://habrahabr.ru/post/310960/">The logic of consciousness.</a>  <a href="https://habrahabr.ru/post/310960/">Part 7. Self-organization of the context space</a> <br>  <a href="https://habrahabr.ru/post/312060/">The logic of consciousness.</a>  <a href="https://habrahabr.ru/post/312060/">Explanation "on the fingers"</a> <br>  <a href="https://habrahabr.ru/post/312740/">The logic of consciousness.</a>  <a href="https://habrahabr.ru/post/312740/">Part 8. Spatial maps of the cerebral cortex</a> <br>  <a href="https://habrahabr.ru/post/317712/">The logic of consciousness.</a>  <a href="https://habrahabr.ru/post/317712/">Part 9. Artificial neural networks and minicolumns of the real cortex.</a> <br>  <a href="https://habrahabr.ru/post/320866/">The logic of consciousness.</a>  <a href="https://habrahabr.ru/post/320866/">Part 10. The task of generalization</a> <br>  <a href="https://habrahabr.ru/post/321256/">The logic of consciousness.</a>  <a href="https://habrahabr.ru/post/321256/">Part 11. Natural coding of visual and sound information</a> <br>  <a href="https://habrahabr.ru/post/326334/">The logic of consciousness.</a>  <a href="https://habrahabr.ru/post/326334/">Part 12. The search for patterns.</a>  <a href="https://habrahabr.ru/post/326334/">Combinatorial space</a> <br></blockquote></div><p>Source: <a href="https://habr.com/ru/post/310960/">https://habr.com/ru/post/310960/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../310950/index.html">The driver of the computer game Street Fighter V disables the built-in protection mechanism of Windows</a></li>
<li><a href="../310952/index.html">React.js: build an isomorphic / universal application from scratch. Part 3: we add authorization and data exchange with API</a></li>
<li><a href="../310954/index.html">A few notes about MySQL</a></li>
<li><a href="../310956/index.html">Development of user interaction with mobile devices - key principles</a></li>
<li><a href="../310958/index.html">How to answer a maximum of questions with one report?</a></li>
<li><a href="../310964/index.html">The history of one feature or why hakaton programmer</a></li>
<li><a href="../310966/index.html">The digest of interesting materials for the mobile developer # 172 (September 19-25)</a></li>
<li><a href="../310968/index.html">A beginner's guide to the development of plug-ins for the graphic editor Sketch</a></li>
<li><a href="../310976/index.html">Line analysis of the license MIT</a></li>
<li><a href="../310978/index.html">Distributed computing in Elixir - classic MapReduce example</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>