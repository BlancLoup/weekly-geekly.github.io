<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The architecture of the network load balancer in Yandeks.Oblak</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, I'm Sergey Elantsev, I am developing a network load balancer in Yandex. Cloud. I used to lead the development of the L7-balancer of the Yandex por...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The architecture of the network load balancer in Yandeks.Oblak</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/webt/8p/s5/_f/8ps5_f3kanb-pmkeuze4rdd971i.png" width="430"></div><br>  Hi, I'm Sergey Elantsev, I am developing a <a href="https://cloud.yandex.ru/services/load-balancer%3Futm_source%3Dhabr%26utm_medium%3Darticle%26utm_campaign%3Darkhitektura-setevogo-balansirovshchika-nagruzki%26utm_content%3Dlink1">network load balancer</a> in Yandex. Cloud.  I used to lead the development of the L7-balancer of the Yandex portal - my colleagues joke that whatever I do, it turns out the balancer.  I will tell Habr readers how to manage the load in the cloud platform, how we see the ideal tool to achieve this goal and how we are moving towards building this tool. <a name="habracut"></a><br><br>  First, we introduce some terms: <br><br><ul><li>  VIP (Virtual IP) - balancer's IP address </li><li>  Server, Backend, Instance ‚Äî Virtual Machine Running Application </li><li>  RIP (Real IP) - server IP address </li><li>  Healthcheck - server readiness check </li><li>  Availability Zone, Availability Zone, AZ - isolated infrastructure in the data center </li><li>  Region - Association of Different AZ </li></ul><br>  Load balancers solve three main tasks: perform the balancing itself, improve the fault tolerance of the service and simplify its scaling.  Fault tolerance is provided by automatic traffic control: the balancer monitors the state of the application and excludes from the balance instances that have not passed the liveliness test.  Scaling is ensured by evenly distributing the load across the instances, as well as updating the list of instances on the fly.  If balancing is not sufficiently uniform, then some of the instans will receive a load that exceeds their limit of serviceability, and the service will become less reliable. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The load balancer is often classified by protocol level from the OSI model on which it runs.  Cloud Balancer works at TCP level, which corresponds to the fourth level, L4. <br><br>  Let us turn to the review of the architecture of the Cloud balancer.  We will gradually increase the level of detail.  We divide the balancer components into three classes.  The class config plane is responsible for user interaction and stores the target state of the system.  The Control plane stores the current state of the system and manages the systems from the data plane class, which are directly responsible for delivering traffic from clients to your instances. <br><br><h3>  Data plane </h3><br>  Traffic hits a costly device called border routers.  To increase resiliency in one data center at the same time several such devices work.  Further, the traffic gets to the balancers, which for customers announce anycast IP-address to all AZ via BGP. <br><br><img src="https://habrastorage.org/webt/pc/jl/-n/pcjl-nh1bq9egjh1d0iv3o4u9bc.jpeg"><br><br>  The traffic is transmitted via ECMP - this is a routing strategy, according to which there may be several equally good routes to the target (in our case, the destination will be the IP address) and packets can be sent on any of them.  We also support work in several availability zones according to the following scheme: we announce the address in each of the zones, the traffic gets to the closest one and does not go beyond its limits.  Further in the post, we will take a closer look at what happens to the traffic. <br><br><h3>  Config plane </h3><br>  The key component of the config plane is the API through which basic operations with balancers are performed: creating, deleting, changing the composition of instances, obtaining healthchecks results, etc. On the one hand, this is the REST API, and on the other, we very often use the framework in the Cloud gRPC, so we "translate" REST into gRPC and then use only gRPC.  Any query leads to the creation of a series of asynchronous idempotent tasks that are performed on the common pool of Yandex.Oblack workers.  Tasks are written in such a way that they can be suspended at any time, and then re-launched.  It provides scalability, repeatability and logability of operations. <br><br><img src="https://habrastorage.org/webt/yb/ru/-c/ybru-c8q6hucu-tdqfybedsjrcu.jpeg"><br><br>  As a result, the task from the API will make a request to the balance controller service controller, which is written on Go.  It can add and remove balancers, change the composition of backends and settings. <br><br><img src="https://habrastorage.org/webt/wr/v3/v7/wrv3v7dfm1_k48ztxc9wfzvvq48.jpeg"><br><br>  The service stores its status in Yandex Database - a distributed managed database, which you will soon be able to use.  In Yandex.Oblak, as we have already <a href="https://habr.com/ru/company/yandex/blog/432042/">said</a> , the dog food concept works: if we ourselves use our services, then our customers will also enjoy using them.  Yandex Database is an example of such a concept.  We store all our data in YDB, and we don‚Äôt have to think about servicing and scaling the base: these problems are solved for us, we use the base as a service. <br><br>  We return to the controller balancer.  His task is to save information about the balancer, send the task of checking the readiness of the virtual machine to the healthcheck controller. <br><br><h3>  Healthcheck controller </h3><br>  It receives requests to change the inspection rules, stores them in YDB, distributes tasks to healtcheck nodes and aggregates the results, which are then saved to the database and sent to the loadbalancer controller.  He, in turn, sends a request to change the composition of the cluster in the data plane to a loadbalancer-node, which I will discuss below. <br><br><img src="https://habrastorage.org/webt/qx/p2/ll/qxp2llhomz9slemwamgsylotd4k.jpeg" width="600"><br><br>  Let's talk more about healthchecks.  They can be divided into several classes.  Checks have different criteria for success.  TCP checks need to successfully establish a connection in a fixed time.  HTTP checks require both a successful connection and a response with a status code of 200. <br><br>  Also, checks differ in the class of action - they are active and passive.  Passive checks simply monitor what happens to the traffic, without taking any special action.  This does not work very well on L4, because it depends on the logic of the higher level protocols: on L4, there is no information about how long the operation took, and whether the connection was good or bad.  Active checks require the balancer to send requests to each server instance. <br><br>  Most load balancers perform liveliness checks themselves.  We in the Cloud decided to separate these parts of the system to increase scalability.  This approach will allow us to increase the number of balancers, while maintaining the number of healthcheck requests to the service.  Checks are performed by individual healthcheck nodes, which are shaded and replicated audit targets.  You can not do checks from one host, as it can refuse.  Then we will not get the state of the instances checked by him.  We perform checks on any of the instances from at least three healthcheck nodes.  We shuffle audit targets between nodes using consistent hashing algorithms. <br><br><img src="https://habrastorage.org/webt/6u/r4/pp/6ur4pp9sk-nqulkepwvqhvb-kdg.jpeg"><br><br>  Separation of balancing and healthcheck can lead to problems.  If healthcheck node makes requests to an instance, bypassing the balancer (which currently does not serve traffic), then a strange situation arises: the resource seems to be alive, but traffic will not reach it.  We solve this problem like this: guaranteed to get healthcheck traffic through balancers.  In other words, the scheme for moving packets with traffic from clients and healthchecks differs minimally: in both cases, the packets will go to balancers, who will deliver them to the target resources. <br><br>  The difference is that customers make VIP requests, and healthchecks access each individual RIP.  Here an interesting problem arises: we give our users the ability to create resources in gray IP networks.  Imagine that there are two different cloud owners who hid their services behind balancers.  Each of them has resources in the subnet 10.0.0.1/24, and with the same addresses.  You need to be able to somehow distinguish them, and here you have to dive into the device of the Yandex.Oblak virtual network.  It is better to find out more details in the <a href="https://www.youtube.com/watch%3Fv%3DKr6WIYPts8I%26t%3D5741s">video from the event about: cloud</a> , now it is important for us that the network is multi-layered and has tunnels in itself that can be distinguished by the subnet id. <br><br>  Healthcheck nodes access balancers using so-called quasi-IPv6 addresses.  A quasi-address is an IPv6 address within which the IPv4 address and id of the user's subnet are stitched.  The traffic hits the balancer, it extracts the IPv4 address of the resource from it, replaces IPv6 with IPv4 and sends the packet to the user's network. <br><br>  Reverse traffic goes the same way: the balancer sees that the destination is a gray network of healthcheckers, and converts IPv4 to IPv6. <br><br><h3>  VPP - the heart of the data plane </h3><br>  The balancer is implemented on the Vector Packet Processing (VPP) technology - a framework from Cisco for packet network traffic processing.  In our case, the framework runs on top of the user-space-management network device library, the Data Plane Development Kit (DPDK).  This provides high packet processing performance: much less interrupts occur in the kernel, there are no context switches between kernel space and user space. <br><br>  VPP goes even further and squeezes even more performance out of the system by combining packages into batchy.  Increased performance is due to the aggressive use of modern processor caches.  Both data caches are used (packets are processed by ‚Äúvectors‚Äù, data lie close to each other) and instruction caches: in VPP, packet processing follows a graph, in the nodes of which there are functions that perform the same task. <br><br>  For example, IP packets are processed in VPP in the following order: first, the parsing of packet headers occurs in the parse node, and then they are sent to the node, which forwards packets further according to the routing tables. <br><br>  A little hardcore.  The VPP authors do not tolerate compromises on the use of processor caches, so the typical code for processing the packet vector contains a manual vectorization: there is a processing cycle in which the situation is ‚Äúwe have four packets in a queue‚Äù, then the same for two, then for one.  Often, prefetch instructions are used that load data into caches to speed up access to them at subsequent iterations. <br><br><pre><code class="cpp hljs">n_left_from = frame-&gt;n_vectors; <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> (n_left_from &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>) { vlib_get_next_frame (vm, node, next_index, to_next, n_left_to_next); <span class="hljs-comment"><span class="hljs-comment">// ... while (n_left_from &gt;= 4 &amp;&amp; n_left_to_next &gt;= 2) { // processing multiple packets at once u32 next0 = SAMPLE_NEXT_INTERFACE_OUTPUT; u32 next1 = SAMPLE_NEXT_INTERFACE_OUTPUT; // ... /* Prefetch next iteration. */ { vlib_buffer_t *p2, *p3; p2 = vlib_get_buffer (vm, from[2]); p3 = vlib_get_buffer (vm, from[3]); vlib_prefetch_buffer_header (p2, LOAD); vlib_prefetch_buffer_header (p3, LOAD); CLIB_PREFETCH (p2-&gt;data, CLIB_CACHE_LINE_BYTES, STORE); CLIB_PREFETCH (p3-&gt;data, CLIB_CACHE_LINE_BYTES, STORE); } // actually process data /* verify speculative enqueues, maybe switch current next frame */ vlib_validate_buffer_enqueue_x2 (vm, node, next_index, to_next, n_left_to_next, bi0, bi1, next0, next1); } while (n_left_from &gt; 0 &amp;&amp; n_left_to_next &gt; 0) { // processing packets by one } // processed batch vlib_put_next_frame (vm, node, next_index, n_left_to_next); }</span></span></code> </pre> <br>  So, Healthchecks are turning to VPP over IPv6, which turns them into IPv4.  This is done by the node graph, which we call algorithmic NAT.  For reverse traffic (and conversion from IPv6 to IPv4) there is the same node of algorithmic NAT. <br><br><img src="https://habrastorage.org/webt/ug/ju/n4/ugjun48y3qurodsxpuia5lfbhn0.jpeg" width="400"><br><br>  Direct traffic from clients of the balancer goes through the nodes of the graph, which perform the balancing itself. <br><br><img src="https://habrastorage.org/webt/p4/eq/uv/p4equvplkheuowkh-ddthjgxeou.jpeg"><br><br>  The first node is sticky sessions.  It stores a hash from <a href="https://www.techopedia.com/definition/28190/5-tuple">5-tuple</a> for established sessions.  5-tuple includes the address and port of the client from which information is transmitted, the address and ports of resources available for receiving traffic, as well as the network protocol. <br><br>  The 5-tuple hash helps us perform fewer computations in a subsequent consistent hash node, and also better handle the change in the list of resources behind the balancer.  When a packet arrives at the balancer, for which there is no session, it is sent to the consistent hashing node.  Here balancing occurs with the help of consistent hashing: we select a resource from the list of available ‚Äúliving‚Äù resources.  Next, the packets are sent to the NAT node, which actually replaces the destination address and recalculates the checksums.  As you can see, we follow the rules of VPP - similar to similar, grouping similar calculations to increase the efficiency of processor caches. <br><br><h3>  Consistent hashing </h3><br>  Why did we choose it and what is it all about?  To begin, consider the previous task - selecting a resource from the list. <br><br><img src="https://habrastorage.org/webt/ab/ez/jl/abezjle6p5u8g54jthaylj5duhi.jpeg"><br><br>  In case of inconsistent hashing, the hash from the incoming packet is calculated, and the resource is selected from the list based on the remainder of dividing this hash by the amount of resources.  While the list remains unchanged, such a scheme works well: we always send packets with the same 5-tuple to the same instance.  If, for example, a resource stopped responding to healthchecks, then for a significant portion of the hashes, the choice will change.  The client will break TCP connections: a packet that previously got to instance A may start to get to instance B, which is not familiar with the session for this packet. <br><br>  Consistent hashing solves the problem described.  The easiest way to explain this concept is this: imagine that you have a ring for which you allocate resources by hash (for example, by IP: port).  The choice of resource is the rotation of the wheel at an angle, which is determined by the hash of the package. <br><br><img src="https://habrastorage.org/webt/qg/rt/yq/qgrtyq_obonzlczzouq4tcshaqq.jpeg"><br><br>  This minimizes the redistribution of traffic when the composition of resources changes.  Deleting a resource will only affect the part of the consistent hashing ring where the resource was located.  Adding a resource also changes the distribution, but we have a sticky sessions node that allows us not to switch already established sessions to new resources. <br><br>  We looked at what happens with direct traffic between the balancer and resources.  Now let's deal with the return traffic.  It follows the same pattern as verification traffic ‚Äî through algorithmic NAT, that is, through reverse NAT 44 for client traffic and through NAT 46 for healthchecks traffic.  We follow our own scheme: we unify healthchecks traffic and real users traffic. <br><br><h3>  Loadbalancer-node and component assembly </h3><br>  The composition of the balancers and resources in the VPP is reported by the local service loadbalancer-node.  It subscribes to the stream of events from loadbalancer-controller, and is able to build the difference between the current state of the VPP and the target state received from the controller.  We get a closed system: events from the API come to the balancer controller, which sets the healthcheck controller tasks to check the "liveliness" of resources.  That, in turn, sets the tasks in the healthcheck-node and aggregates the results, after which it gives them back to the balancer controller.  Loadbalancer-node subscribes to events from the controller and changes the state of the VPP.  In such a system, each service knows only what is necessary about neighboring services.  The number of links is limited, and we have the ability to independently exploit and scale different segments. <br><br><img src="https://habrastorage.org/webt/rk/a0/vi/rka0viu8dbcd7irrdx5m0quwfpw.jpeg"><br><br><h3>  What questions have been avoided </h3><br>  All our services in control plane are written in Go and have good performance in terms of scaling and reliability.  Go has many open source libraries for building distributed systems.  We actively use GRPC, all components contain an open-source implementation of service discovery - our services monitor the performance of each other, can change their composition dynamically, and we tied it with GRPC balancing.  For metrics, we also use an open source solution.  In the data plane we got decent performance and a large supply of resources: it turned out to be very difficult to assemble a stand on which to rest against the performance of the VPP, and not the iron network card. <br><br><h3>  Problems and Solutions </h3><br>  What didn't work very well?  In Go, memory management is automatic, but there are still memory leaks.  The easiest way to deal with them is to run the gorutin and not forget to complete them.  Conclusion: monitor the memory consumption of Go-programs.  Often a good indicator is the amount of gorutin.  There is a plus in this story: in Go it is easy to get data on the runtime - on memory consumption, on the number of running gorutins and on many other parameters. <br><br>  In addition, Go is probably not the best choice for functional tests.  They are rather verbose, and the standard ‚Äúrun everything in CI pack‚Äù approach is not very suitable for them.  The fact is that functional tests are more demanding of resources, with them there are real timeouts.  Because of this, tests may fail unsuccessfully, since the CPU is busy with unit tests.  Conclusion: if possible, perform "heavy" tests separately from unit tests. <br><br>  Microservice event architecture is more complicated than a monolith: logging on dozens of different machines is not very convenient.  Conclusion: if you are doing microservices, immediately think about tracing. <br><br><h3>  Our plans </h3><br>  We will launch an internal balancer, an IPv6 balancer, add support for Kubernetes scripts, continue to shuffle our services (now only healthcheck-node and healthcheck-ctrl are shaded), add new healthchecks, and also implement smart aggregation of checks.  We are considering the possibility of making our services even more independent - so that they communicate not directly with each other, but with the help of a message queue.  SQS-compatible <a href="https://cloud.yandex.ru/services/message-queue">Yandex Message Queue</a> service has recently appeared in the Cloud. <br><br>  Recently, a public release of Yandex Load Balancer took place.  Study the service <a href="https://cloud.yandex.ru/docs/load-balancer/%3Futm_source%3Dhabr%26utm_medium%3Darticle%26utm_campaign%3Darkhitektura-setevogo-balansirovshchika-nagruzki%26utm_content%3Dlink2">documentation</a> , manage balancers in the way that is convenient for you, and increase the fault tolerance of your projects! </div><p>Source: <a href="https://habr.com/ru/post/448588/">https://habr.com/ru/post/448588/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../448574/index.html">One challenge from the SEO routine: the solution in 3 steps</a></li>
<li><a href="../448576/index.html">The history of the transistor, part 2: of the thorn of war</a></li>
<li><a href="../448580/index.html">CQ CQ CQ Happy holiday, Ham radio! #WorldAmateurRadioDay</a></li>
<li><a href="../448582/index.html">Creating a tip calculator for Kotlin: how does it work?</a></li>
<li><a href="../448584/index.html">7 common mistakes when using prepositions in the English language and how to avoid them</a></li>
<li><a href="../448590/index.html">Familiar strangers or again about the use of design patterns</a></li>
<li><a href="../448594/index.html">Free antiviruses and firewalls (UTM, NGFW) from Sophos</a></li>
<li><a href="../448596/index.html">Tablet holder on a treadmill or search for free steps</a></li>
<li><a href="../448600/index.html">How to study the critical DHCP vulnerability in Windows 10 has led to the detection of two more security errors</a></li>
<li><a href="../448602/index.html">Is monitoring dead? - Long live monitoring</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>