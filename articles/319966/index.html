<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>As I parsed the entire database of Metacritic games</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Metacritic is an English-language site aggregator that collects reviews on music albums, games, movies, TV shows and DVDs. (from wikipedia). 

 Librar...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>As I parsed the entire database of Metacritic games</h1><div class="post__text post__text-html js-mediator-article">  <i><b>Metacritic</b> is an English-language site aggregator that collects reviews on music albums, games, movies, TV shows and DVDs.</i>  (from wikipedia). <br><br>  Libraries used: <b>lxml</b> , <b>asyncio</b> , <b>aiohttp</b> (lxml is a library for parsing HTML pages using Python, asyncio and aiohttp will be used for asynchrony and fast data retrieval).  We will also actively use XPath.  Who does not know what it is, an excellent <a href="http://www.w3schools.com/xml/xpath_intro.asp">tutorial</a> . <br><a name="habracut"></a><br><h3>  Getting links to all game pages </h3><br>  First you have to do some pens.  Go to <a href="http://www.metacritic.com/browse/games/genre/metascore/action/all%3Fview%3Ddetailed">www.metacritic.com/browse/games/genre/metascore/action/all?view=detailed</a> and collect all <br>  URLs from this list: <br><br><br><img src="https://habrastorage.org/getpro/habr/post_images/a5c/dad/65b/a5cdad65b9c8d57bcae65cd8be348c3f.png" alt="image">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      And save them to a .json file with the name <b>genres.json</b> .  We will use these links to parse all games from the site by genre. <br><br>  A little thought, I decided to collect all the links to games in .csv files, divided into genres.  Each file will have a name corresponding to the genre.  Go to the above link and immediately see the page of the genre Action.  We notice that there is a pagination. <br><br>  Look in the html page: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e68/e05/5f7/e68e055f7909e027a7cbd16a12bb77ee.png" alt="image"><br><br>  And we see that the sought-for element <b>a</b> , which contains the maximum number of pages, is a heir of the <b>li</b> element with a unique attribute <b>class = page last_page</b> , also note that the <b>urls of</b> all pages except the first one are &lt;page 1st url&gt; + &lt;&amp; page = page_number&gt;, and that the second page in the request parameter is number 1. <br><br>  Putting XPath to get the maximum page number: <br><br>  <i><b>// li [@ class = 'page last_page'] / a / text ()</b></i> <br><br>  Now you need to get each link to each game from this sheet. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8ab/2ff/3aa/8ab2ff3aa50b70213b622cae7cb2aff4.png" alt="image"><br><br>  We look at the layout of the sheet and learn html. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/11f/fba/8d9/11ffba8d975c5d343d0ec2c835654437.png" alt="image"><br><br>  First, we need to get the list itself <b>(ol)</b> as the root element for the search.  It has the attribute <b>class = list_products list_product_summaries,</b> which is unique for the page's html code.  Then we see that <b>li</b> has a child element <b>h3</b> with a child element <b>a</b> , in the <b>href</b> attribute of which the desired link to the game is located. <br><br>  Putting it all together: <br><br>  <i><b>// ol [@ class = 'list_products list_product_summaries'] // h3 [@ class = 'product_title'] / a / @ href</b></i> <br><br>  Fine!  Half the battle is done, now you need to programmatically organize a cycle through the pages, collecting links and saving them to files.  To speed up, parallelize the operation on all the cores of our PC. <br><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># get_games.py import csv import requests import json from multiprocessing import Pool from time import sleep from lxml import html #  . root = 'http://www.metacritic.com/' #     Metacritic   429   'Slow down' #   ,      . SLOW_DOWN = False def get_html(url): # Metacritic     User-Agent. headers = {"User-Agent": "Mozilla/5.001 (windows; U; NT4.0; en-US; rv:1.0) Gecko/25250101"} global SLOW_DOWN try: #     -    429,     #  15 . if SLOW_DOWN: sleep(15) SLOW_DOWN = False #  html     requests html = requests.get(url, headers=headers).content.decode('utf-8') #    html ,    SLOW_DOWN true. if '429 Slow down' in html: SLOW_DOWN = True print(' - - - SLOW DOWN') raise TimeoutError return html except TimeoutError: return get_html(url) def get_pages(genre): #      Games with open('Games/' + genre.split('/')[-2] + '.csv', 'w') as file: writer = csv.writer(file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL) #  url  &gt; 1 genre_page_sceleton = genre + '&amp;page=%s' def scrape(): page_content = get_html(genre) #   lxml   html . document = html.fromstring(page_content) try: #        int. lpn_text = document.xpath("//li[@class='page last_page']/a/text()" last_page_number = int(lpn_text)[0]) pages = [genre_page_sceleton % str(i) for i in range(1, last_page_number)] #     . pages += [genre] #            . for page in pages: document = html.fromstring(get_html(page)) urls_xpath = "//ol[@class='list_products list_product_summaries']//h3[@class='product_title']/a/@href" #      url  . games = [root + url for url in document.xpath(urls_xpath)] print('Page: ' + page + " - - - Games: " + str(len(games))) for game in games: writer.writerow([game]) except: #    429 .   . scrape() scrape() def main(): #        .json . dict = json.load(open('genres.json', 'r')) p = Pool(4) #  .  map        . p.map(get_pages, [dict[key] for key in dict.keys()]) print('Over') if __name__ == "__main__": main()</span></span></code> </pre> <br>  Merge all files with links into one file.  If you are under Linux, then just use cat and redirect STDOUT to a new file.  Under Windows we will write a small script and launch it in the folder with the genre files. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> listdir <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> os.path <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> isfile, join onlyfiles = [f <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> f <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> listdir(<span class="hljs-string"><span class="hljs-string">'.'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> isfile(join(mypath, f))] fout=open(<span class="hljs-string"><span class="hljs-string">"all_games.csv"</span></span>,<span class="hljs-string"><span class="hljs-string">"a"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> path <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> onlyfiles: f = open(path) f.next() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> f: fout.write(line) f.close() fout.close()</code> </pre> <br>  Now we have one big .csv file with links to all Metacritic games.  Large enough, 25 thousand records.  Plus there are duplicates, since one game can have several genres. <br><br><h3>  Extract information about all games </h3><br>  What is our plan next?  Go through each link and extract information about each game. <br>  Go, for example, to the <a href="http://www.metacritic.com/game/pc/portal-2">Portal 2</a> page. <br><br>  We will extract: <br><br><ul><li>  Name of the game </li><li>  Platforms </li><li>  Description </li><li>  Metascore </li><li>  Genre </li><li>  Date of issue </li></ul><br>  To shorten the post, I will immediately cite the xpaths that extracted this information. <br><br>  Name of the game: <br><br>  <i><b>// h1 [@ class = 'product_title'] // span [@ itemprop = 'name'] // text ()</b></i> <br><br>  We may have several platforms, so two requests will be needed: <br><br>  <i><b>// span [@ itemprop = 'device'] // text ()</b></i> <br>  <b><i>// li [@ class = 'summary_detail product_platforms'] // a // text ()</i></b> <br><br>  Our description is in Summary: <br><br>  <b><i>// span [@ itemprop = 'description'] // text ()</i></b> <br><br>  Metascore: <br><br>  <b><i>// span [@ itemprop = 'ratingValue'] // text ()</i></b> <br><br>  Genre: <br><br>  <b><i>// span [@ itemprop = 'description'] // text ()</i></b> <br><br>  Date of issue: <br><br>  <b><i>// span [@ itemprop = 'datePublished'] // text ()</i></b> <br><br>  We remember that we have 25 thousand pages and grab our heads.  What to do?  Even with a few threads will be long.  There is a way out - asynchrony and non-blocking cortices.  Here is a <a href="https://www.youtube.com/watch%3Fv%3DMCs5OvhV9S4">great video from PyCon</a> .  Async-await simplifies asynchronous programming in python 3.5.2.  <a href="https://habrahabr.ru/post/266743/">Tutorial on Habr√©</a> . <br><br>  We write the code for our parser. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sleep <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> asyncio <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> aiohttp <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ClientSession <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> lxml <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> html <span class="hljs-comment"><span class="hljs-comment">#        ,  . games_urls = list(set([line for line in open('Games/all_games.csv', 'r')])) #    . result = [] #         . total_checked = 0 async def get_one(url, session): global total_checked async with session.get(url) as response: #     . page_content = await response.read() #        . item = get_item(page_content, url) result.append(item) total_checked += 1 print('Inserted: ' + url + ' - - - Total checked: ' + str(total_checked)) async def bound_fetch(sm, url, session): try: async with sm: await get_one(url, session) except Exception as e: print(e) #     30     429. sleep(30) async def run(urls): tasks = [] #    .  . sm = asyncio.Semaphore(50) headers = {"User-Agent": "Mozilla/5.001 (windows; U; NT4.0; en-US; rv:1.0) Gecko/25250101"} #    User-Agent,      Metacritic async with ClientSession( headers=headers) as session: for url in urls: #         . task = asyncio.ensure_future(bound_fetch(sm, url, session)) tasks.append(task) #     . await asyncio.gather(*tasks) def get_item(page_content, url): #   lxml   html . document = html.fromstring(page_content) def get(xpath): item = document.xpath(xpath) if item: return item[-1] #   -      ,   None return None name = get("//h1[@class='product_title']//span[@itemprop='name']//text()") if name: name = name.replace('\n', '').strip() genre = get("//span[@itemprop='genre']//text()") date = get("//span[@itemprop='datePublished']//text()") main_platform = get("//span[@itemprop='device']//text()") if main_platform: main_platform = main_platform.replace('\n', '').strip() else: main_platform = '' other_platforms = document.xpath("//li[@class='summary_detail product_platforms']//a//text()") other_platforms = '/'.join(other_platforms) platforms = main_platform + '/' + other_platforms score = get("//span[@itemprop='ratingValue']//text()") desc = get("//span[@itemprop='description']//text()") #     . return {'url': url, 'name': name, 'genre': genre, 'date': date, 'platforms': platforms, 'score': score, 'desc': desc} def main(): #  . loop = asyncio.get_event_loop() future = asyncio.ensure_future(run(games_urls)) loop.run_until_complete(future) #  .    -  . print(result) print('Over') if __name__ == "__main__": main()</span></span></code> </pre> <br>  It turned out very well, on my computer Intel i5 6600K, 16 GB RAM, lan 10 mb / s it turned out about 10 games / sec.  You can correct the code and adapt the script to the music \ movies. </div><p>Source: <a href="https://habr.com/ru/post/319966/">https://habr.com/ru/post/319966/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../319956/index.html">CSS evolution: from CSS, SASS, BEM and CSS modules to styled-components</a></li>
<li><a href="../319958/index.html">StressCam - an application to control stress, biological age and improve well-being</a></li>
<li><a href="../319960/index.html">Why Russia has a lot of ideas, but few startups</a></li>
<li><a href="../319962/index.html">Visual C ++ Extension for Linux Development</a></li>
<li><a href="../319964/index.html">Email Video: 8 Application Tips</a></li>
<li><a href="../319968/index.html">Master Tarantool 1.6</a></li>
<li><a href="../319970/index.html">Welcome aboard: application guidance tips</a></li>
<li><a href="../319974/index.html">Soccer ball and fullerenes</a></li>
<li><a href="../319976/index.html">Sooo long whole</a></li>
<li><a href="../319978/index.html">Akso HTTP WebSocket in practice</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>