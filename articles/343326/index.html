<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>LizardFS - a brief overview of the cluster file system</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Interesting cluster file system. On one not very big project we implemented it and it works better than the popular GlusterFS or NFS solutions. Uses F...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>LizardFS - a brief overview of the cluster file system</h1><div class="post__text post__text-html js-mediator-article">  Interesting cluster file system.  On one not very big project we implemented it and it works better than the popular GlusterFS or NFS solutions.  Uses FUSE when connecting on the client side.  We thought that it would not work better than other cluster file systems, but in reality everything turned out to be very positive. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/719/3c6/719/7193c6719f8a1df5b01cb91c6d44cd8b.png" alt="image"><br><a name="habracut"></a><br><br>  <b>UPD:</b> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      After some time, we implemented LizardFS already on an average project, in fact, almost immediately after the publication of this article, we just had to look under the loads before writing the update article. <br><br>  After 3 months and even there was a reboot of the master server, for reasons beyond the FS itself, I can say that it is the best cluster FS available and less complex.  It doesn‚Äôt buggy at all, it keeps the average data volumes without problems (in our case from 1TB and small and large files), including it acts not only as a data storage for us, but almost every minute there is a constant updating of data, overwriting a large number of files. <br><br>  Generally LizardFS or MooseFS (commercial) they are intended even for petabytes of information. <br><br>  There is one nuance for those who will use it at home.  This file system is not designed to write files line by line, that is, if you write a file from php directly to disk by unloading lines from the loop into it, it will try to constantly synchronize each change, and so the file system starts to slow down wildly for obvious reasons. <br><br>  Therefore, to form a line-by-line record, simply mount the tmpfs folder somewhere, generate files into this folder line by line, then from there make the mv of the finished file into the cluster file system, or create an array of data in the RAM directly in the interceptor and merge it into the file at once.  That is, it works well with atomic operations, copying a file, moving a file, reading a file, but most importantly a complete file, and then there will be no problems.  Only with the help of for and any cycles it is not necessary to write files line by line into it. <br><br>  Its ability to set the number of replicas in folders is fine. <br><br>  I‚Äôll add a simple monitoring script to the article, a master and shadow master servers, which can be installed in cron: <br><br><pre><code class="hljs kotlin">#!/bin/bash #Variables SRV=<span class="hljs-string"><span class="hljs-string">"172.24.1.1"</span></span> #Every <span class="hljs-number"><span class="hljs-number">15</span></span> minutes repeat alert <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> have troubles test -f /tmp/$SRV.lizardfs &amp;&amp; find /tmp/$SRV.lizardfs -mmin +<span class="hljs-number"><span class="hljs-number">15</span></span> -delete #Check <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> no have previous alert flag file <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> [ ! -e /tmp/$SRV.lizardfs ]; then #Check command CHECK_LIZ=`lizardfs-probe metadataserver-status $SRV <span class="hljs-number"><span class="hljs-number">9421</span></span> | grep <span class="hljs-string"><span class="hljs-string">"running\|connected"</span></span> | awk <span class="hljs-string"><span class="hljs-string">'{print $3}'</span></span> | sed <span class="hljs-string"><span class="hljs-string">'/^$/d'</span></span> | tr -d <span class="hljs-string"><span class="hljs-string">' '</span></span>` #Send Alert <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> have troubles <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> [[ <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$CHECK_LIZ</span></span></span><span class="hljs-string">"</span></span> != <span class="hljs-string"><span class="hljs-string">"running"</span></span> &amp;&amp; <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$CHECK_LIZ</span></span></span><span class="hljs-string">"</span></span> != <span class="hljs-string"><span class="hljs-string">"connected"</span></span> ]] ; then echo <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$CHECK_LIZ</span></span></span><span class="hljs-string">"</span></span> &gt; /tmp/$SRV.lizardfs #Send Alert /home/scripts/telegram/telegram.sh --service <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$SRV</span></span></span><span class="hljs-string"> LizardFS"</span></span> --event <span class="hljs-string"><span class="hljs-string">"Status: </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$CHECK_LIZ</span></span></span><span class="hljs-string">"</span></span> ; /home/scripts/sms/msms.sh --service <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$SRV</span></span></span><span class="hljs-string"> LizardFS"</span></span> --event <span class="hljs-string"><span class="hljs-string">"Status: </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$CHECK_LIZ</span></span></span><span class="hljs-string">"</span></span> ; fi fi</code> </pre> <br>  The scripts telegram.sh and msms.sh are already your scripts. <br><br>  <b>Initial article:</b> <br><br>  <b>The advantages of LizardFS:</b> <br><br>  Very fast work in reading small files.  Many who have used GlusterFS have encountered the problem of overly slow work if a clustered file system is used for the return of these sites. <br><br>  The speed of reading large files is very good. <br><br>  The speed of writing large files is not much different from the native file systems, especially if the bottom is SSD drives and between servers in a 1-10 Gbit cluster connection. <br><br>  Operations chown / ls -la - slower than in the case of a native FS, but not so slow <br>  as in the case of GlusterFS. <br><br>  Recursive delete operations are very fast. <br><br>  Very flexible connection options in the mfsmount utility. <br><br>  Duplication of metadata is easy and simple to set up; you can simultaneously have several safety shadow master servers and metalogger servers. <br><br>  Meta data is stored in RAM. <br><br>  It's great that LizardFS may prefer to take data from a local server.  Using the example of 2 replicas to speed up reading, the LizardFS client will automatically take files from the local server, rather than pull over the network from the 2nd server. <br><br>  The ability to install any goals on the subfolders (number of replicas), that is, you can specify LizardFS / var / lizardfs / important to make 3 replicas, and / var / lizardfs / not_important without any replicas and everything within one file system.  It all depends on the number of chunk servers and the performance of the required tasks; you can even use several separate disks for each chunk server. <br><br>  It supports various modes of data replication, including EC - Erasure Coding. <br>  The read speed in this mode is not much less. <br><br>  Everything works in LXC containers, except when the client mounts the file system itself.  It is solved by mounting on the host machine and forwarding the mount point to the LXC container. <br>  You can practice. <br><br>  Automatic rebalance when adding new nodes. <br>  Pretty simple removal of dropped or unnecessary nodes. <br><br>  <b>The disadvantages of LizardFS:</b> <br><br>  The recording speed of a large stream of small files is very low.  Rsync shows it well. <br><br>  In the case of the engines of sites that generate a large number of cache files on the file system, <br>  The cache must be moved out of the cluster file system.  For engines in which this is a problem, you can mount local folders in its subfolders over the mounted LizardFS, or use symbolic links. <br><br>  Example: <br><br><pre> <code class="hljs kotlin">mount --bind /<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>/nativefs/cache /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/lizardfs/cache</code> </pre> <br>  The working master server meta data can work only 1 at a time, but at the same time a doubler or even a few in shadow mode can work.  High Availability can be realized both with the help of the paid component LizardFS, and with the help of its own scripts + uCarp / keepalived / PaceMaker and similar software.  In our case, we do not need this, rather manual control. <br><br>  Unfortunately everything is automatically, as in GlusterFS out of the box, this is not there.  The pros and flexibility outweigh the cons. <br><br>  A little demanding of RAM.  But for example on small projects where data is about 10-20GB and only 2 replicas are not critical: <br><br><pre> <code class="hljs kotlin"><span class="hljs-symbol"><span class="hljs-symbol">root@</span></span><span class="hljs-number"><span class="hljs-number">172.24</span></span>.1.1:/# df -h |grep <span class="hljs-string"><span class="hljs-string">"mfs\|Size"</span></span> Filesystem Size Used Avail Use% Mounted on mfs#mfsmaster:<span class="hljs-number"><span class="hljs-number">9421</span></span> <span class="hljs-number"><span class="hljs-number">1.8</span></span>T <span class="hljs-number"><span class="hljs-number">41</span></span>G <span class="hljs-number"><span class="hljs-number">1.8</span></span>T <span class="hljs-number"><span class="hljs-number">3</span></span>% /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/www ( <span class="hljs-number"><span class="hljs-number">2</span></span>- ) <span class="hljs-symbol"><span class="hljs-symbol">root@</span></span><span class="hljs-number"><span class="hljs-number">172.24</span></span>.1.1:/# find /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/www -type f -print0 | wc -l --files0-from=- |grep total <span class="hljs-number"><span class="hljs-number">185818</span></span> total ( ) <span class="hljs-symbol"><span class="hljs-symbol">root@</span></span><span class="hljs-number"><span class="hljs-number">172.24</span></span>.1.1:/# smem -u -t -k |grep <span class="hljs-string"><span class="hljs-string">"mfs\|RSS"</span></span> User Count Swap USS PSS RSS mfs <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">170.0</span></span>M <span class="hljs-number"><span class="hljs-number">170.9</span></span>M <span class="hljs-number"><span class="hljs-number">180.9</span></span>M (<span class="hljs-number"><span class="hljs-number">3</span></span>  - master/metalogger/chunkserver)</code> </pre><br>  On an average project I‚Äôll check the RAM consumption later, haven‚Äôt implemented it yet, but we‚Äôre already preparing. <br><br>  <b>Setup:</b> <br><br>  I will provide a brief manual on how to quickly run LizardFS on Debian 9. <br><br>  Suppose we have 4 servers with Debian 9 installed, the 8th by default does not contain any ready-made lizardfs packages. <br><br>  Set limits: <br><br>  In /etc/security/limits.conf you need to put down: <br><br><pre> <code class="hljs markdown"><span class="hljs-bullet"><span class="hljs-bullet">* </span></span>hard nofile 20000 * soft nofile 20000</code> </pre><br>  You can also register separately for root, mfs / lizardfs users.  For whom, then, the limits have already been raised, this is not required.  At a minimum, you need 10,000. <br><br>  Distribute roles: <br><br>  172.24.1.1 (master metadata / chunk) - master metadata / chunk repository <br>  172.24.1.2 (shadow metadata / chunk) - shadow master metadata / chunk repository <br>  172.24.1.3 (metalogger / chunk) - backup metadata / chunk storage <br>  172.24.1.4 (metalogger / chunk) - backup metadata / chunk storage <br><br>  Strictly speaking, a metalogger can work both on master and shadow in parallel, the more resources are cheap there.  This is actually a delayed backup of meta data, exactly the same as on the master / shadow servers.  And any metalogger, in case if the main servers have broken the meta data can be turned into a master server. <br><br>  That is, for example, this option is used by me too: <br><br>  172.24.1.1 (master metadata / metalogger / chunk) <br>  172.24.1.2 (shadow metadata / metalogger / chunk) <br><br>  We continue further in the article in the version of 4 servers. <br><br>  Items 1-3 are performed on all 4 servers. <br><br>  1. Install on all servers a full set of all services except the client if you have LXC containers.  We put all the services, because they are still disabled in Debian at startup in / etc / default / ... And if you suddenly have to turn the metalogger into a master, everything will already be installed in advance. <br><br><pre> <code class="hljs sql">apt-get -y <span class="hljs-keyword"><span class="hljs-keyword">install</span></span> lizardfs-common lizardfs-<span class="hljs-keyword"><span class="hljs-keyword">master</span></span> lizardfs-chunkserver lizardfs-metalogger lizardfs-<span class="hljs-keyword"><span class="hljs-keyword">client</span></span> cp /etc/lizardfs/mfsmaster.cfg.dist /etc/lizardfs/mfsmaster.cfg cp /etc/lizardfs/mfsmetalogger.cfg.dist /etc/lizardfs/mfsmetalogger.cfg cp /etc/lizardfs/mfsgoals.cfg.dist /etc/lizardfs/mfsgoals.cfg cp /etc/lizardfs/mfschunkserver.cfg.dist /etc/lizardfs/mfschunkserver.cfg cp /etc/lizardfs/mfshdd.cfg.dist /etc/lizardfs/mfshdd.cfg</code> </pre> <br>  Or, since the option above is given in Debian in native packages, if you install from source or build your packages, the directory will be different: <br><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">cp</span></span> /etc/mfs/mfsmaster.cfg.dist /etc/mfs/mfsmaster.cfg cp /etc/mfs/mfsmetalogger.cfg.dist /etc/mfs/mfsmetalogger.cfg cp /etc/mfs/mfsgoals.cfg.dist /etc/mfs/mfsgoals.cfg cp /etc/mfs/mfschunkserver.cfg.dist /etc/mfs/mfschunkserver.cfg cp /etc/mfs/mfshdd.cfg.dist /etc/mfs/mfshdd.cfg</code> </pre> <br>  Including the names of the utilities, depending on the packages, can also change - mfs * / lizardfs &lt;action&gt; and so on. <br><br>  In some cases, if you do not install the master package of the server, but only put metalogger and chunkserver, then by default the rights of the user and the group mfs / lizardfs on the folder / var / lib / mfs or / var / lib / lizardfs are not set. <br><br>  2. Add to / etc / hosts the default entry for LizardFS. <br><br><pre> <code class="hljs ruby">echo <span class="hljs-string"><span class="hljs-string">"172.24.1.1 mfsmaster"</span></span> <span class="hljs-meta"><span class="hljs-meta">&gt;&gt; </span></span>/etc/hosts</code> </pre> <br>  3. Choose a folder or a separate section for data storage, or even 2 sections or more.  Consider just 1 folder in the current filesystem on each server: / data / lizardfs-chunk <br><br><pre> <code class="hljs haskell"><span class="hljs-title"><span class="hljs-title">mkdir</span></span> -p /var/www (    ) mkdir -p /<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class">/lizardfs-chunk ( </span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">LizardFS</span></span></span><span class="hljs-class">      )</span></span></code> </pre><br><pre> <code class="hljs javascript">echo <span class="hljs-string"><span class="hljs-string">"/data/lizardfs-chunk"</span></span> &gt; <span class="hljs-regexp"><span class="hljs-regexp">/etc/</span></span>lizardfs/mfshdd.cfg echo <span class="hljs-string"><span class="hljs-string">"172.24.1.0/24 / rw,alldirs,maproot=0"</span></span> &gt; <span class="hljs-regexp"><span class="hljs-regexp">/etc/</span></span>lizardfs/mfsexports.cfg</code> </pre><br>  Or <br><br><pre> <code class="hljs javascript">echo <span class="hljs-string"><span class="hljs-string">"/data/lizardfs-chunk"</span></span> &gt; <span class="hljs-regexp"><span class="hljs-regexp">/etc/m</span></span>fs/mfshdd.cfg echo <span class="hljs-string"><span class="hljs-string">"172.24.1.0/24 . rw,alldirs,maproot=0"</span></span> &gt; <span class="hljs-regexp"><span class="hljs-regexp">/etc/m</span></span>fs/mfsexports.cfg echo <span class="hljs-string"><span class="hljs-string">"172.24.1.0/24 / rw,alldirs,maproot=0"</span></span> &gt; <span class="hljs-regexp"><span class="hljs-regexp">/etc/m</span></span>fs/mfsexports.cfg</code> </pre><br>  Sharing the root - meaning the entire space of the LizardFS itself, and not the root of the native FS system. <br><br>  Sharing points - meaning that it was possible to connect the file system with meta-data.  This will be required to clean up the deleted files.  Reviewed below. <br><br>  4. Specify who will be the master, and who is the shadow <br><br>  On 172.24.1.1 we specify PERSONALITY = master in the config mfsmaster.cfg <br>  At 172.24.1.2 we indicate PERSONALITY = shadow in the config mfsmaster.cfg <br><br>  5. We include the necessary services: <br><br><pre> <code class="hljs pgsql"><span class="hljs-number"><span class="hljs-number">172.24</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span>: echo "LIZARDFSMASTER_ENABLE=true" &gt; /etc/<span class="hljs-keyword"><span class="hljs-keyword">default</span></span>/lizardfs-master echo "LIZARDFSCHUNKSERVER_ENABLE=true" &gt; /etc/<span class="hljs-keyword"><span class="hljs-keyword">default</span></span>/lizardfs-chunkserver systemctl <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> lizardfs-master systemctl <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> lizardfs-chunkserver <span class="hljs-number"><span class="hljs-number">172.24</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span><span class="hljs-number"><span class="hljs-number">.2</span></span>: echo "LIZARDFSMASTER_ENABLE=true" &gt; /etc/<span class="hljs-keyword"><span class="hljs-keyword">default</span></span>/lizardfs-master echo "LIZARDFSCHUNKSERVER_ENABLE=true" &gt; /etc/<span class="hljs-keyword"><span class="hljs-keyword">default</span></span>/lizardfs-chunkserver systemctl <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> lizardfs-master systemctl <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> lizardfs-chunkserver <span class="hljs-number"><span class="hljs-number">172.24</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span><span class="hljs-number"><span class="hljs-number">.3</span></span>: echo "LIZARDFSCHUNKSERVER_ENABLE=true" &gt; /etc/<span class="hljs-keyword"><span class="hljs-keyword">default</span></span>/lizardfs-chunkserver echo "LIZARDFSMETALOGGER_ENABLE=true" &gt; /etc/<span class="hljs-keyword"><span class="hljs-keyword">default</span></span>/lizardfs-metalogger systemctl <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> lizardfs-metalogger systemctl <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> lizardfs-chunkserver <span class="hljs-number"><span class="hljs-number">172.24</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span><span class="hljs-number"><span class="hljs-number">.4</span></span>: echo "LIZARDFSCHUNKSERVER_ENABLE=true" &gt; /etc/<span class="hljs-keyword"><span class="hljs-keyword">default</span></span>/lizardfs-chunkserver echo "LIZARDFSMETALOGGER_ENABLE=true" &gt; /etc/<span class="hljs-keyword"><span class="hljs-keyword">default</span></span>/lizardfs-metalogger systemctl <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> lizardfs-metalogger systemctl <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> lizardfs-chunkserver</code> </pre> <br>  Next, we mount the client, there are different options, there are a lot of options: <br><br>  FS on chunkserver `s: <br><br>  If you have a native XFS file system, add -o mfssugidclearmode = XFS <br>  If you have collected on BSD, you must add -o mfssugidclearmode = BSD <br>  If you have collected on MAC OSX :), you need to add -o mfssugidclearmode = OSX <br><br>  It is clear that it is not recommended to mix different FS under chunkserver `s within the cluster, and if you want, you need to use -o mfssugidclearmode = ALWAYS <br><br>  The default is EXT for btrfs, ext2, ext3, ext4, hfs [+], jfs, ntfs and reiserfs. <br>  I don‚Äôt think that someone would use ntfs under Linux for chunkserver. <br><br><pre> <code class="hljs swift">mfsmount -o big_writes,nosuid,nodev,noatime,allow_other /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/www mfsmount -o big_writes,nosuid,nodev,noatime,allow_other -o cacheexpirationtime=<span class="hljs-number"><span class="hljs-number">500</span></span> -o readaheadmaxwindowsize=<span class="hljs-number"><span class="hljs-number">4096</span></span> /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/www</code> </pre> <br>  The supported options depend on the version of LizardFS, in the second version the reading speed is much higher.  mfsmount by default takes the IP master from / etc / hosts, but connects to all chunk servers in parallel.  The big_writes option is considered obsolete in new FUSE clients and is turned on by default, preferably for older systems. <br><br>  Available replica options are written in mfsgoals.cfg <br><br>  Then we indicate how many replicas of the data we need: <br><br><pre> <code class="hljs swift">mfssetgoal -r <span class="hljs-number"><span class="hljs-number">2</span></span> /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/www mfsgetgoal /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/www</code> </pre> <br>  Well, then we start using in / var / www <br><br>  Supplement about cleaning the basket LizardFS.  Deleted files can be restored by default, but they also take up space. <br><br>  We do it on any machine, you can do it at all. <br><br><pre> <code class="hljs ruby">mkdir /mnt/lizardfs-meta mfsmount /mnt/lizardfs-meta -o mfsmeta cd /mnt/lizardfs-meta/trash root@172.<span class="hljs-number"><span class="hljs-number">24.1</span></span>.<span class="hljs-number"><span class="hljs-number">1</span></span><span class="hljs-symbol"><span class="hljs-symbol">:/mnt/lizardfs-meta/trash</span></span><span class="hljs-comment"><span class="hljs-comment"># root@172.24.1.1:/mnt/lizardfs-meta/trash# ls -la total 0 drwx------ 3 root root 0 Dec 2 09:36 . dr-xr-xr-x 4 root root 0 Dec 2 09:36 .. -rw-r--r-- 0 root root 0 Dec 2 09:38 00016C88|test.txt dw------- 2 root root 0 Dec 2 09:36 undel</span></span></code> </pre><br>  The time the files stay in the default basket is 24 hours after deletion: <br><br><pre> <code class="hljs ruby">root@172.<span class="hljs-number"><span class="hljs-number">24.1</span></span>.<span class="hljs-number"><span class="hljs-number">1</span></span><span class="hljs-symbol"><span class="hljs-symbol">:/mnt/lizardfs-meta/trash</span></span><span class="hljs-comment"><span class="hljs-comment"># lizardfs rgettrashtime /var/www/ /var/www/: files with trashtime 86400 : 44220 directories with trashtime 86400 : 2266</span></span></code> </pre><br>  You can change it for immediate deletion as follows: <br><br><pre> <code class="hljs ruby">root@172.<span class="hljs-number"><span class="hljs-number">24.1</span></span>.<span class="hljs-number"><span class="hljs-number">1</span></span><span class="hljs-symbol"><span class="hljs-symbol">:/mnt/lizardfs-meta/trash</span></span><span class="hljs-comment"><span class="hljs-comment"># lizardfs rsettrashtime 0 /var/www/ /var/www/: inodes with trashtime changed: 46486 inodes with trashtime not changed: 0 inodes with permission denied: 0</span></span></code> </pre><br>  Final deletion of files: <br><br><pre> <code class="hljs ruby">root@172.<span class="hljs-number"><span class="hljs-number">24.1</span></span>.<span class="hljs-number"><span class="hljs-number">1</span></span><span class="hljs-symbol"><span class="hljs-symbol">:/mnt/lizardfs-meta/trash</span></span><span class="hljs-comment"><span class="hljs-comment"># find /mnt/lizardfs-meta/trash/ -type f -delete</span></span></code> </pre><br>  Recover all deleted files: <br><br><pre> <code class="hljs ruby">root@172.<span class="hljs-number"><span class="hljs-number">24.1</span></span>.<span class="hljs-number"><span class="hljs-number">1</span></span><span class="hljs-symbol"><span class="hljs-symbol">:/mnt/lizardfs-meta/trash</span></span><span class="hljs-comment"><span class="hljs-comment"># find /mnt/lizardfs-meta/trash/ -type f -exec mv {} /mnt/lizardfs-meta/trash/undel/ \;</span></span></code> </pre><br>  By default, after deleting files from the recycle bin, real files are deleted from chunk servers rather slowly.  How to speed it up is not very clear. <br><br>  Accordingly, it is possible to do basket management on the subdirectory separately, which gives good flexibility.  For example, for cache directories, you can put 0, for other directories from accidental deletion, you can put more than 24 hours. <br><br>  Once again, I emphasize the flexibility in replicating specific directories or files with an example: <br><br><pre> <code class="hljs javascript">root@<span class="hljs-number"><span class="hljs-number">172.24</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span>:<span class="hljs-regexp"><span class="hljs-regexp">/# mfssetgoal -r 1 /</span></span><span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/www/test/ <span class="hljs-regexp"><span class="hljs-regexp">/var/</span></span>www/test/: inodes <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> goal changed: <span class="hljs-number"><span class="hljs-number">1</span></span> inodes <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> goal not changed: <span class="hljs-number"><span class="hljs-number">0</span></span> inodes <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> permission denied: <span class="hljs-number"><span class="hljs-number">0</span></span> root@<span class="hljs-number"><span class="hljs-number">172.24</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span>:<span class="hljs-regexp"><span class="hljs-regexp">/# mfsgetgoal /</span></span><span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/www /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/www: <span class="hljs-number"><span class="hljs-number">2</span></span> root@<span class="hljs-number"><span class="hljs-number">172.24</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span>:<span class="hljs-regexp"><span class="hljs-regexp">/# mfsgetgoal /</span></span><span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/www/test /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/www/test: <span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre><br>  That is, you can specify on the directories directly how many replicas of directories and their subdirectories <br>  do.  Rebalancing is done automatically.  The type of replicas available for the mfssetgoal utility is indicated in mfsgoals.cfg <br><br>  For those who need parallel recording more than, for example, 1 gigabyte per second, we simply do not need this, but this file system can even google it, then we need to configure the goal to use erasure coding in mfsgoals.cfg and assign this type of replication using mfssetgoal to the desired folder.  In this mode, the client writes data in parallel, provided that the client has a network of 10 gigabits at a minimum.  At servers it is supposed that too not less. <br><br>  ‚Üí <a href="https://docs.lizardfs.com/adminguide/replication.html">More about replication modes</a> <br><br>  <b>Important: do not do killall -9 mfsmaster without the need.</b> <br>  By doing so, you can at least lose some of the data that is still in RAM and, as a maximum, beat the metadata file and have to either repair it with the mfsfilerepair utility or even replace it with the old one, the data will naturally be lost.  Partly depends on the wizard settings. <br><br>  I did not write this down to the minuses, since the cluster file system should be done not on the servers from the dump site.  Moreover, it is desirable that the servers had an emergency power supply. <br><br>  For large systems, it may be important to enable configuration: <br><br><pre> <code class="hljs pgsql">root@<span class="hljs-number"><span class="hljs-number">172.24</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span>:/# cat /etc/mfs/mfschunkserver.cfg |grep -B3 FACTOR ## <span class="hljs-keyword"><span class="hljs-keyword">If</span></span> enabled, chunkserver will send periodical reports <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> its I/O <span class="hljs-keyword"><span class="hljs-keyword">load</span></span> <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> master, ## which will be taken <span class="hljs-keyword"><span class="hljs-keyword">into</span></span> consideration <span class="hljs-keyword"><span class="hljs-keyword">when</span></span> picking chunkservers <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> I/O operations. ## (<span class="hljs-keyword"><span class="hljs-keyword">Default</span></span> : <span class="hljs-number"><span class="hljs-number">0</span></span>) # ENABLE_LOAD_FACTOR = <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre><br>  For quick work in the mode of connecting several clients in parallel and parallel writing and reading, do not forget about the following default parameters: <br><br><pre> <code class="hljs perl">root@172.<span class="hljs-number"><span class="hljs-number">24.1</span></span>.<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-regexp"><span class="hljs-regexp">/# cat /etc</span></span><span class="hljs-regexp"><span class="hljs-regexp">/mfs/mfschunkserver</span></span>.cfg |<span class="hljs-keyword"><span class="hljs-keyword">grep</span></span> WORKERS <span class="hljs-comment"><span class="hljs-comment"># NR_OF_NETWORK_WORKERS = 1 # NR_OF_HDD_WORKERS_PER_NETWORK_WORKER = 2 # NR_OF_NETWORK_WORKERS = 1 # NR_OF_HDD_WORKERS_PER_NETWORK_WORKER = 20</span></span></code> </pre><br>  And about the options in mfsmount: <br><br><pre> <code class="hljs pgsql">-o mfswritecachesize=&lt;N&gt; specify <span class="hljs-keyword"><span class="hljs-keyword">write</span></span> <span class="hljs-keyword"><span class="hljs-keyword">cache</span></span> size in MiB (in range: <span class="hljs-number"><span class="hljs-number">16.</span></span><span class="hljs-number"><span class="hljs-number">.2048</span></span> - <span class="hljs-keyword"><span class="hljs-keyword">default</span></span>: <span class="hljs-number"><span class="hljs-number">128</span></span>) -o mfswriteworkers=<span class="hljs-string"><span class="hljs-string">'N'</span></span> define number <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> <span class="hljs-keyword"><span class="hljs-keyword">write</span></span> workers (<span class="hljs-keyword"><span class="hljs-keyword">default</span></span>: <span class="hljs-number"><span class="hljs-number">10</span></span>) -o mfswritewindowsize=<span class="hljs-string"><span class="hljs-string">'N'</span></span> define <span class="hljs-keyword"><span class="hljs-keyword">write</span></span> <span class="hljs-keyword"><span class="hljs-keyword">window</span></span> size (<span class="hljs-keyword"><span class="hljs-keyword">in</span></span> blocks) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> <span class="hljs-keyword"><span class="hljs-keyword">each</span></span> chunk (<span class="hljs-keyword"><span class="hljs-keyword">default</span></span>: <span class="hljs-number"><span class="hljs-number">15</span></span>)</code> </pre><br>  In general, it took me one full day to deal with this clustered FS.  Yes, it is a bit more complicated than GlusterFS, but the pluses outweigh.  Quickly reading a bunch of small files is great. <br><br>  ‚Üí <a href="https://docs.lizardfs.com/">Documentation</a> </div><p>Source: <a href="https://habr.com/ru/post/343326/">https://habr.com/ru/post/343326/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../343316/index.html">What is hidden behind the term modeling</a></li>
<li><a href="../343318/index.html">How to describe layout in code</a></li>
<li><a href="../343320/index.html">"Without further ado": the shortest scientific articles</a></li>
<li><a href="../343322/index.html">Five things that online shoppers should know about in the summer</a></li>
<li><a href="../343324/index.html">Is it difficult to develop software?</a></li>
<li><a href="../343328/index.html">DDoS bypassing the Curator: simple steps for a quiet life</a></li>
<li><a href="../343332/index.html">Hamster Marketplace Token: what tasks it solves and why it is one of a kind</a></li>
<li><a href="../343334/index.html">How to frontend developer set up a database</a></li>
<li><a href="../343336/index.html">How to teach a neural network to invent the names of Russian settlements</a></li>
<li><a href="../343338/index.html">27 free services for creating visual content without a designer</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>