<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Make3D from one photo, part 1</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The project from Stanford University (now Cornell University ) " Make3D " is remarkable for the fact that it has set itself the task of restoring a th...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Make3D from one photo, part 1</h1><div class="post__text post__text-html js-mediator-article"><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/qsw6akRTMRw&amp;xid=17259,1500004,15700022,15700186,15700191,15700253,15700256,15700259&amp;usg=ALkJrhhD3kw6ew7tckRKKb0HXk3F3k6UQA" frameborder="0" allowfullscreen=""></iframe><br><br>  The project from <a href="http://en.wikipedia.org/wiki/Stanford_University"><em>Stanford University</em></a> (now <a href="http://en.wikipedia.org/wiki/Cornell_University"><em>Cornell University</em></a> ) " <a href="http://make3d.cs.cornell.edu/">Make3D</a> " is remarkable for the fact that it has set itself the task of restoring a three-dimensional model of the scene from just one photo that has not yet become typical.  Until now, in order to achieve a similar result, developers restored three-dimensional information by combining several (two or more) images of the same object.  In this case, it was demonstrated that a significant amount of information is contained in the monocular signs ( <em>monocular cues</em> ) of the image itself, which were often ignored before.  In practical implementation, it has already been possible to achieve satisfactory results for more than <em>60% of</em> arbitrary photographs provided and evaluated by third-party users of the system when conducting its tests. <br><br>  The publication consists of: Part 1, <a href="http://vikds.habrahabr.ru/blog/95559/">Part 2</a> <br>  It is published to satisfy curiosity, in order to <strike>expose the magic</strike> to make it clear how it works. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <a name="habracut"></a><br><br><h4>  Content: </h4><br>  Part One ( <em>current</em> ): <ul><li>  Brief description of the algorithm </li><li>  List of used theories and algorithms </li><li>  Ising model or where did this formula in MRF come from </li><li>  Effective image segmentation on graphs </li><li>  We want 3D Model and Location of polygons </li><li>  Watch will have wider ... </li><li>  Monocular image features </li><li>  Borders and refractions </li><li>  MRF Input and Output </li><li>  What will MRF do?  Target: min (distance error) </li></ul>  Part Two ( <a href="http://vikds.habrahabr.ru/blog/95559/">here</a> ): <ul><li>  Wish list is ready, we stuff it in the Model MRF </li><li>  1. Local features: Preliminary determination of distance by local features </li><li>  2. Connection: Connection </li><li>  3. Co-planar: Coplanarity </li><li>  4. Co-linear: Collinearity </li><li>  Briefly about learning MRF </li><li>  Briefly about the MRF Solution </li><li>  Source code and model parameters studied </li><li>  Program execution, testing </li><li>  Total </li><li>  Few files </li></ul><br><br><h4>  Brief description of the algorithm </h4><br>  The image is divided into <em>superpixels</em> , small segments of the image that are similar in texture.  Homogeneous objects and even flat surfaces, as a rule, are divided into several superpixels, but, nevertheless, superpixels lie entirely within a single object or surface, if the surface has a pronounced boundary. <br><br>  The algorithm for each super pixel tries to find out its position (depth) and orientation in three-dimensional space relative to the point from which the photo was taken, i.e.  find the surface (plane) on which it lies.  This surface can have any arrangement in space, not necessarily horizontal or vertical, as suggested in similar studies conducted earlier. <br><br>  The project uses preliminary training for the interrelationships of the totality of the set of signs of superpixels (image sections) and distances to them.  The teaching algorithm uses the <em>MRF</em> model ( <em>Markov field</em> ), taking into account the restrictions on the relative distances between adjacent superpixels, i.e.  Considering that with some probability, two neighboring sites are likely to be approximately at the same distance from the observation point, or they may even be on the same plane than belong to different objects that are far apart in space (such as the fence and the background behind it). <br><br>  After processing a segmented on superpixels image in a pre-trained <em>MRF</em> , the algorithm receives at the output the position and orientation of each super pixel.  This is enough to build a three-dimensional model of the scene, the texture of which is the photograph itself. <br><br><h6>  <em>Pic.1 Result of photo processing in Make3D</em> </h6><br><div style="text-align:center;"><img title="Pic.1 Result of photo processing in Make3D" alt="Pic.1 Result of photo processing in Make3D" src="http://img39.imageshack.us/img39/6223/pic1make3dexample.png"></div><br><br><h4>  List of used theories and algorithms </h4><br>  In the method of restoring a three-dimensional model of the scene, the following theories and algorithms are used: <br><ul><li>  <em>Image <a href="http://cgm.computergraphics.ru/content/view/147">segmentation</a></em> for splitting an image into <em>super pixels</em> </li><li>  Apply <em><a href="http://habrahabr.ru/blogs/webdev/43895/">filters</a></em> to an image to detect <em>monocular signs.</em> </li><li>  <em><a href="http://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B0%25D1%2580%25D0%25BA%25D0%25BE%25D0%25B2%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C">Markov random fields</a></em> ( <em><a href="http://en.wikipedia.org/wiki/Markov_random_field">MRF</a></em> ) for modeling <em>monocular signs</em> and the relative position of different parts of the image </li><li>  Selection of <em>contours</em> and <em>borders</em> on the image ( <em><a href="http://en.wikipedia.org/wiki/Edge_detection">edge detection</a></em> ) </li><li>  Elements of <em>computational <a href="http://ru.wikipedia.org/wiki/%25D0%2592%25D1%258B%25D1%2587%25D0%25B8%25D1%2581%25D0%25BB%25D0%25B8%25D1%2582%25D0%25B5%25D0%25BB%25D1%258C%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25B3%25D0%25B5%25D0%25BE%25D0%25BC%25D0%25B5%25D1%2582%25D1%2580%25D0%25B8%25D1%258F">geometry</a></em> , <em>linear <a href="http://ru.wikipedia.org/wiki/%25D0%259B%25D0%25B8%25D0%25BD%25D0%25B5%25D0%25B9%25D0%25BD%25D0%25BE%25D0%25B5_%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B3%25D1%2580%25D0%25B0%25D0%25BC%25D0%25BC%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5">programming</a></em> , etc. </li></ul><br><br><h4>  Ising model or where did this formula in MRF come from </h4><br>  <strong>You can quickly flip through, this is just an explanation of why the formula looks like this in <em>MRF</em> and in " <em>Make3D</em> ".</strong>  In general, historically it happened ... <br><br>  The <em><a href="http://en.wikipedia.org/wiki/Ising_model">Ising model</a></em> served as the Markovskaya prototype, so let's start with it.  This is a mathematical <em>model of statistical physics</em> , designed to describe the magnetization of the material. <br><br>  In the physical Ising model, each vertex of the crystal lattice is assigned a number, called spin, and numerical, denoted as ‚Äú <strong>+1</strong> ‚Äù or ‚Äú <strong>‚àí1</strong> ‚Äù, which determines the direction of the spin: ‚Äú <strong>up</strong> ‚Äù or ‚Äú <strong>down</strong> ‚Äù.  <em><a href="http://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25BF%25D0%25B8%25D0%25BD">Spin</a></em> is the intrinsic angular momentum of elementary particles, which has a quantum nature and is not associated with the movement of a particle as a whole. <br><br><h6>  <em>Pic.2 One-dimensional Ising model</em> </h6><br><div style="text-align:center;"><img title="Pic.2 One-dimensional Ising model" alt="Pic.2 One-dimensional Ising model" src="http://img42.imageshack.us/img42/557/pic2isingmodel.jpg"></div><br><br>  Let us consider the simplest one-dimensional case when the lattice sites are located along one straight line.  The model describes the probability distribution in space. <img src="http://img526.imageshack.us/img526/1184/form2onew.jpg" alt="œâ" title="œâ">  all possible substance configurations <img src="http://img156.imageshack.us/img156/7555/form1somews.jpg" alt="œâ = (œâ_0, œâ_1, ..., œâ_n)" title="œâ = (œâ_0, œâ_1, ..., œâ_n)">  where <img src="http://img139.imageshack.us/img139/5403/form3n.jpg" alt="n" title="n">  - the number of elementary particles, <img src="http://img689.imageshack.us/img689/5231/form4wiplusminus.jpg" alt="œâ_i = {+ 1, -1}" title="œâ_i = {+ 1, -1}">  spin of the <em>i</em> -th particle.  Such a distribution is called a random field ‚Äî it is a random function defined on a set of points of a multidimensional space. <img src="http://img195.imageshack.us/img195/1286/form5pw.jpg" alt="P (œâ)" title="P (œâ)">  = how likely it is that the substance will be in a state where all nodes have equal spins <img src="http://img156.imageshack.us/img156/7555/form1somews.jpg" alt="œâ = (œâ_0, œâ_1, ..., œâ_n)" title="œâ = (œâ_0, œâ_1, ..., œâ_n)"><br><br>  Each of <img src="http://img268.imageshack.us/img268/3958/form62n.jpg" alt="2 ^ n" title="2 ^ n">  possible locations of spins <img src="http://img526.imageshack.us/img526/1184/form2onew.jpg" alt="œâ" title="œâ">  , attributed to the energy resulting from the pairwise interaction of the spins of neighboring atoms and the external field: <br><br><h6>  <em>The total energy of the system in the Ising model:</em> </h6><br><div style="text-align:center;"><img title="Ising the system" alt="The total energy of the system in the Ising model" src="http://img153.imageshack.us/img153/2955/form7spinenergall.jpg"></div><br><br>  In the formula, the first sum is taken over all pairs of neighboring nodes, and equals the interaction energy of the spins of the nodes.  Ising simplified the model, taking into account the interaction only between neighboring nodes located in close proximity.  The exchange interaction constant <em>J</em> is a characteristic of the material under consideration and describes the interaction energy of spins.  If <em>J&gt; 0</em> , then describes the attraction energy of a ferromagnet ( <em>attractive case</em> ), when neighboring lattice sites try to line up in one direction, i.e.  <em>collinearly</em> , with values ‚Äã‚Äãof <em>J &lt;0</em> describes the repulsive energy of an antiferromagnet ( <em>repulsive case</em> ), when neighboring lattice sites try to line up in opposite directions, i.e.  <em>coplanar</em> .  The second sum describes the effect of an external magnetic field of intensity <em>H</em> , where <em>m</em> - characterizes the magnetic properties of a substance. <br><br>  In the case when <em>J&gt; 0</em> , the first sum will be minimal, in the case when all the nodes of the crystal lattice will be aligned in the collinear direction.  The second sum reaches a minimum when the direction of the spins of the nodes of the crystal lattice coincides with the direction of the external field <em>H.</em> <br><br>  We know from physics that <em>any system brought out of equilibrium tends to the state with the least energy</em> .  Therefore, sooner or later, the model will ‚Äúcalm down‚Äù and take a position in which the total energy <img src="http://img405.imageshack.us/img405/7019/form8isingfullenerg.jpg" alt="U (œâ)" title="U (œâ)">  will be equal to the smallest value ( <em>min</em> ).  This will be the most likely state of her ( <img src="http://img195.imageshack.us/img195/1286/form5pw.jpg" alt="P (œâ)" title="P (œâ)">  - <em>max</em> ). <br><br>  Probability distribution in space <img src="http://img526.imageshack.us/img526/1184/form2onew.jpg" alt="œâ" title="œâ">  all possible substance configurations in the ising model, subject to the distribution given by the so-called <em><a href="http://en.wikipedia.org/wiki/Gibbs_measure">Gibbs measure</a></em> <em><a href="http://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%2593%25D0%25B8%25D0%25B1%25D0%25B1%25D1%2581%25D0%25B0">distribution</a></em> function: <br><br><h6>  <em>Distribution in the Ising model:</em> </h6><br><div style="text-align:center;"><img title="Distribution in the Ising model" alt="Distribution in the Ising model" src="http://img697.imageshack.us/img697/5563/form9isingprob.jpg"></div><br><br>  where <em>k</em> is the Boltzmann constant, <em>T is the</em> temperature of the substance, <img src="http://img156.imageshack.us/img156/1038/form10isingnorm.jpg" alt="Z = ‚àë_œâe ^ (- 1 / kT U (œâ))" title="Z = ‚àë_œâe ^ (- 1 / kT U (œâ))">  normalization value. <br>  We do not need these values, it is important that the <em>formula has this appearance and has been working since the beginning of the 19th century</em> . <br><br>  For a more convenient form of recording the probability distribution in the model, for each <em>i</em> -th lattice node we associate its ( <em>personal</em> ) own total energy: <br><br><h6>  <em>Own energy of the Ising model node:</em> </h6><br><div style="text-align:center;"><img title="Self energy model" alt="Self energy of the Ising model node" src="http://img694.imageshack.us/img694/9553/form11isingpersonalener.jpg"></div><br><br>  Gibbs distribution function ( <em>Gibbs measure</em> ) is interesting from two positions.  The first is related to entropy, which probably led to its widespread use in statistical studies. <br><br><h6>  <em>Pic.3 Two-dimensional Ising model</em> </h6><br><div style="text-align:center;"><img title="Pic.3 Two-dimensional Ising model" alt="Pic.3 Two-dimensional Ising model" src="http://img580.imageshack.us/img580/2373/pic3ising2d.jpg"></div><br><br>  The second important property of distribution in this form, from the point of view of probability theory, is the following.  Let be <img src="http://img257.imageshack.us/img257/8840/form12nj.jpg" alt="N_j" title="N_j">  this is the set of all neighbors of <em>j</em> , i.e.  many nodes in the vicinity.  Then: <br><br><h6>  <em>Markov type property:</em> </h6><br><div style="text-align:center;"><img title="Markov type property" alt="Markov type property" src="http://img35.imageshack.us/img35/7341/form13markovprop.jpg"></div><br>  This property is called the Markov type property.  The probability that a spin is at a point <img src="http://img132.imageshack.us/img132/7797/form15wj.jpg" alt="œâ_j" title="œâ_j">  facing <em>a</em> , for given spins in all other nodes of the crystal lattice, is equal to the probability in which only spins in adjacent nodes are taken into account <img src="http://img408.imageshack.us/img408/4374/form14wkkfromn.jpg" alt="œâ_k, k ‚àà N_j" title="œâ_k, k ‚àà N_j">  .  This property is called a <em>local characteristic</em> , and the distribution with a similar property is called a <strong>random Markov field</strong> . <br><br>  <em>The Markov network</em> was presented as a generalization of the Ising model, and in it the probability distribution function <img src="http://img526.imageshack.us/img526/1184/form2onew.jpg" alt="œâ" title="œâ">  can be written in <em>multiplicative form</em> , as the product of all the proper energies of the lattice nodes: <br><br><h6>  <em>Model in multiplicative form:</em> </h6><br><div style="text-align:center;"><img title="Model in multiplicative form" alt="   " src="http://img121.imageshack.us/img121/9262/form16isingmultipl.jpg"></div><br>  In practice, the main purpose of applying a random Markov field is to detect the most likely configuration. <img src="http://img268.imageshack.us/img268/4563/form17wstar.jpg" alt="w*" title="w *">  random field <img src="http://img265.imageshack.us/img265/1883/form18wstarmax.jpg" alt="œâ: w*=argmaxw(P(w))" title="œâ: w * = argmaxw (P (w))">  . <br><br>  The main difficulties associated with the use of this method are the correct choice of parameters controlling the power of spatial interaction. <br><br>  <em>Now we have a formula, on its basis the MRF model for ‚ÄúMake3D‚Äù will be built ...</em> <br><br><br><br><br><h4>  Effective image segmentation on graphs </h4><br>  <em>A picture is worth a thousand words:</em> <br><br><h6>  <em>Segmentation example with various parameters</em> </h6><br><div style="text-align:center;"><img title="Segmentation example with various parameters" alt="    " src="http://img25.imageshack.us/img25/9208/picture14mc.jpg"></div><br><br>  These large monochrome patches are superpixels, in the terminology of the authors of ‚ÄúMake3D‚Äù.  More details are available in an accessible form in the <a href="http://habrahabr.ru/blogs/algorithm/81279/">article</a> " <em>Effective segmentation of images on graphs</em> ". <br><br><br><h4>  We want 3D Model and Location of polygons </h4><br>  We want to end up with a <em>3D model</em> .  The restored three-dimensional model is represented by a set of <em><a href="http://en.wikipedia.org/wiki/Polygon">polygons</a></em> and a <em><a href="http://en.wikipedia.org/wiki/Texture_mapping">texture</a></em> , i.e.  parts of the original photograph superimposed on these polygons.  Polygon has an arbitrary location and orientation in space. <br><br><h6>  <em>Pic.4 Superpixel orientation and orientation</em> </h6><br><div style="text-align:center;"><img title="Pic.4 Superpixel orientation and orientation" alt="Pic.4    " src="http://img229.imageshack.us/img229/8108/pic4superpixellocation.jpg"></div><br><br>  More formally, the three-dimensional location of the polygon will be determined by the plane defined by the parameters <img src="http://img97.imageshack.us/img97/8352/form19alfafromr3.jpg" alt="Œ±‚ààR^3" title="Œ±‚ààR ^ 3">  .  For any point <img src="http://img684.imageshack.us/img684/5678/form20qufromr3.jpg" alt="q‚ààR^3" title="q‚ààR ^ 3">  belonging to this plane <em>Œ±</em> , the equality <img src="http://img571.imageshack.us/img571/7990/form22alfatq1.jpg" alt="Œ±^T q=1" title="Œ± ^ T q = 1">  .  The shortest distance from the observation point, the point from which the photograph was taken, to the plane along the perpendicular lowered onto it is: <em>1 / | Œ± |</em>  .  Normal vector <img src="http://img443.imageshack.us/img443/1073/form24alfanorm.jpg" alt="Œ±=Œ±/|Œ±|" title="Œ± = Œ± / | Œ± |">  uniquely determines the orientation of the plane in space. <br><br>  If we are given a single vector <img src="http://img153.imageshack.us/img153/1046/form25rayi.jpg" alt="R_i" title="R_i">  (called ray <img src="http://img153.imageshack.us/img153/1046/form25rayi.jpg" alt="R_i" title="R_i">  ) connecting the observation point ( <em>camera center</em> ) with an arbitrary pixel <em>i</em> in the image belonging to the plane <em>Œ±</em> , the distance from the observation point to this pixel <em>i</em> is given by the formula: <img src="http://img257.imageshack.us/img257/6868/form26distrayialfa.jpg" alt="d_i=1/(R_i^T Œ±)" title="d_i = 1 / (R_i ^ T Œ±)">  .  The distance to the superpixel will also be determined. <br><br><br><br><h4>  Watch will have wider ... </h4><br>  The person is well distinguished by monocular signs of the image, helping him to restore the three-dimensional model of the scene and determine the distance to objects.  He is at one glance at the photograph is able to notice differences in the textures of the image, color gradients, differences in colors, fuzzy contours and the out-of-focus of objects in the distance. <br><br>  However, only local monocular signs are still not enough, because the blue sky and the blue object on earth have similar local monocular signs.  Therefore, when reconstructing a scene, a person has to take into account not only local features, but also the relative interrelation of various parts of this image.  Thus, a person can determine the distance to an object with not brightly pronounced signs, say, a uniformly lit large gray area, relative to other objects in the photograph.  And determine that this is a wall and even approximately see its length. <br><br>  Therefore, the <em>MRF</em> model takes into account not only local monocular signs ( <em>monocular cue</em> ), but also the relative relationship of different parts of the image: <br><ul><li>  <em>Local</em> <strong><em>features</em></strong> : local features of a super-pixel, monocular, in many respects affect its position and orientation in space. </li><li>  <em>Connections</em> ( <strong><em>connections</em></strong> ): with the exception of the borders of various objects, it is most likely that adjacent superpixels are connected at the borders and in three-dimensional space </li><li>  <em>Coplanarity</em> ( <strong><em>co-planarity</em></strong> ): if neighboring pixels have similar monocular features ( <em>feature</em> ), and do not have pronounced borders ( <em>edge</em> ) between them, then it is most likely that they lie in the same plane. </li><li>  <em>Collinearity</em> ( <strong><em>co-linearity</em></strong> ): long straight lines, most likely, will also be long straight lines in three-dimensional space </li></ul>  These properties are taken into account by the model <em>in the aggregate</em> , with the imposed condition ( <em>condition</em> ) in the form of a " <em>confidence level</em> " ( <em>confidence</em> ) on each of these properties, i.e.  as far as we can be confident in the calculated result.  The level of trust is a different value depending on the local features of the image area. <br><br>  But before turning to the MRF model ... what did the authors find so interesting in just one photo? <br><br><br><br><h4>  Image features </h4><br>  For each superpixel, the algorithm calculates a tuple of features ( <em>features</em> ) in order to identify the monocular <em>cues</em> mentioned above, as well as features for determining the <em>boundaries</em> and <em>refraction of</em> superpixels (objects in the image).  In order to make the algorithm more stable and flexible, so that it can restore the scene from an arbitrary photograph, not necessarily similar to those on which the model was trained, a relatively large number of features ( <em>features</em> ) are calculated. <br><br><h4>  Monocular image features </h4><br>  For each superpixel <em>i</em> , statistical features related to image texture, shape, and position are calculated.  In the project ‚ÄúMake3D‚Äù 17 filters are used. <img src="http://img696.imageshack.us/img696/5307/form27fnxy.jpg" alt="F_n (x,y)" title="F_n (x, y)">  where <em>n = 1, ..., 17</em> : <br><ul><li>  9 <a href="http://resource01.code.ouj.ac.jp/~motofumi/R/3D-TextureEnergy/index.html">Laws</a> masks for calculating the average image intensity ( <em>averaging</em> ), <em>spot detection</em> ( <em>spot detection</em> ) and borders ( <em>edge detection</em> ) </li><li>  6 <a href="http://www.cs.umd.edu/class/spring2002/cmsc426/Edge.html">masks</a> for detection of oriented borders ( <em>oriented edge detection</em> ), rotated by ( <em>30 ¬∞ √ó k</em> ), <em>k = 0, ..., 5</em> degrees </li><li>  2 color channels in Cb and Cr in <a href="http://en.wikipedia.org/wiki/YCbCr">YCbCr</a> format </li></ul><br><h6>  <em>Pic.5 Masks used in Make3D</em> </h6><br><div style="text-align:center;"><img title="Pic.5 Masks used in Make3D" alt="Pic.5 ,   Make3D" src="http://img706.imageshack.us/img706/655/pic5make3dmasks.png"></div><br><h6>  <em>Pic.6 Two channels of color Cb and Cr</em> </h6><br><div style="text-align:center;"><img title="Pic.6 Two channels of color Cb and Cr" alt="Pic.6    Cb  Cr" src="http://img375.imageshack.us/img375/2119/pic6channelcbcr.jpg"></div><br><br>  As a result of applying filters to the superpixel <em>i of the</em> image <em>I (x, y)</em> , a <em>34-dimensional</em> vector of features is formed, calculated as <img src="http://img411.imageshack.us/img411/182/form28pixelenergy.jpg" alt="E_i (n)=‚àë_((x,y)‚ààS_i)|I(x,y)*F_n (x,y) |^k" title="E_i (n) = ‚àë _ ((x, y) ‚àà S_i) | I (x, y) * F_n (x, y) | ^ k">  where <em>k = {2,4}</em> , which corresponds to the energy ( <em>energy</em> ) and the excess ( <em>kurtosis</em> ).  This gives <em>34</em> values ‚Äã‚Äãfor each super pixel.  Additionally, for a superpixel <em>i</em> , <em>14</em> features are calculated that relate to its position in the photograph and form, like: <br><br><h6>  <em>Pic.7 Features superpixel location</em> </h6><br><div style="text-align:center;"><img title="Pic.7 Features superpixel location" alt="Pic.7   " src="http://img683.imageshack.us/img683/5852/pic7shapefeatures.png"></div><br><br>  The " <em>Make3D</em> " project takes into account not only the features of a separate superpixel, but also takes into account the "context" in the form, the features ( <em>features</em> ) of the <em>4 largest neighboring superpixels</em> , and this operation is repeated <em>in 3 different scales</em> .  Thus, each superpixel contains in addition information and about the image around it, which leads to better results than if we confine ourselves to local features of the superpixel.  In the end, a vector is associated with each superpixel that contains <em>34 * (4 + 1) * 3 + 14 = 524</em> elements, i.e. <img src="http://img710.imageshack.us/img710/4097/form29alfafromr524.jpg" alt="x‚ààR^524" title="x‚ààR ^ 524"><br><br><h6>  <em>Pic.8 The features of each super pixel are considered in context.</em> </h6><br><div style="text-align:center;"><img title="Pic.8 The features of each super pixel are considered in context." alt="Pic.8      " src="http://img36.imageshack.us/img36/2414/pic8featurestogether.png"></div><br><h6>  <em>Pic.9 Highlighting image features at different scales.</em> </h6><br><div style="text-align:center;"><img title="Pic.9 Highlighting image features at different scales." alt="Pic.9      " src="http://img412.imageshack.us/img412/135/pic9featuresdiffscales.jpg"></div><br><h6>  <em>Pic.10 At one level, each superpixel connects to 4 max neighbors</em> </h6><br><div style="text-align:center;"><img title="Pic.10 At one level, each superpixel connects to 4 max neighbors" alt="Pic.10   ,     4 max " src="http://img62.imageshack.us/img62/2792/pic10onelevellinks.png"></div><br><br><br><h4>  Borders and refractions </h4><br>  Parameters are used to define boundaries. <img src="http://img696.imageshack.us/img696/9739/form30yhardbelong.png" alt="y_ij‚àà{0,1}" title="y_ij‚àà {0,1}">  defining the border between two super pixels (" <a href="http://www.csse.uwa.edu.au/~pk/research/matlabfns/"><em>edgel</em></a> ").  In view of the fact that the definition of boundaries is usually inaccurate, the model uses not discrete values, but more ‚Äúsoft‚Äù conditions for <img src="http://img267.imageshack.us/img267/6086/form31ysoftbelong.png" alt="y_ij‚àà[0,1]" title="y_ij‚àà [0,1]">  .  More formally, equality <img src="http://img690.imageshack.us/img690/4141/form32yij.png" alt="y_ij" title="y_ij">  = 0 means the presence of an object <em>boundary</em> ( <em>occlusion boundary</em> ) or an <em>unfold</em> between superpixels <em>i</em> and <em>j</em> , and <img src="http://img690.imageshack.us/img690/4141/form32yij.png" alt="y_ij" title="y_ij">  = 1 means no, i.e.  superpixels lie on a flat surface. <br><br><h6>  <em>Pic.11 Image detection (edge ‚Äã‚Äãdetection) - edgels</em> </h6><br><div style="text-align:center;"><img title="Pic.11 Image detection (edge ‚Äã‚Äãdetection) - edgels" alt="Pic.11    (edge detection) ‚Äì edgels" src="http://img138.imageshack.us/img138/6646/pic11edgels.png"></div><br><h6>  <em>Pic.12 Edge detection</em> </h6><br><div style="text-align:center;"><img title="Pic.12 Edge detection" alt="Pic.12 Edge detection" src="http://img295.imageshack.us/img295/7656/pic12edgedetection.jpg"></div><br>  In some cases, the presence of a large gradient or differential light, which are taken into account in the algorithms for determining the boundaries of objects ( <em>edge detection</em> ), is also a sign that these parts of the image belong to different objects.  For example, the shadow of a building dropped on a lawn can cause such a difference, and the algorithms for determining the boundaries of objects can mistakenly determine the presence of a boundary and divide the lawn into 2 objects.  Nevertheless, if we take into consideration more visual features ( <em>visual cues</em> ) of the image to find out whether there are boundaries between these objects, whether they are connected, coplanar or not, and not just gradients, then we can get better results. <br><br>  In this work, an approach was proposed for determining the boundary of objects ( <em>edge detection</em> ), using training that takes into account the lighting, color and texture of image areas.  Taking advantage of their result, the ‚ÄúMake3D‚Äù value <img src="http://img690.imageshack.us/img690/4141/form32yij.png" alt="y_ij" title="y_ij">  that determines the presence of a border between superpixels <em>i</em> and <em>j</em> is derived from the model with the distribution: <br><br><div style="text-align:center;"><img title="Distribution to calculate confidence" alt="   " src="http://img265.imageshack.us/img265/8919/form33confidprob.jpg"></div><br><ul><li><img src="http://img241.imageshack.us/img241/1110/form34elfij.jpg" alt="œµ_ij" title="œµ_ij">  features ( <em>features</em> ) a pair of superpixels <em>i</em> and <em>j</em> </li><li><img src="http://img25.imageshack.us/img25/4673/form35phi.jpg" alt="œà" title="œà">  model parameter (after training) </li></ul>  If two adjacent superpixels in the image have drastically different features ( <em>features</em> ), then the person will most likely also perceive them as two different objects.  Thus, the boundary between two super pixels with distinctly different features most likely represents the boundary of objects ( <em>occlusion boundary</em> ) or bend ( <em>unfold</em> ).  To find out how much the features differ with the two superpixels <em>i</em> and <em>j</em> in ‚ÄúMake3D‚Äù, 14 different image segmentation is carried out: for <em>2 different scales</em> and with <em>7 different sets of segmentation parameters</em> to identify texture, color, and border features. <br><br><h6>  <em>Pic.13 An example of a single image segmentation with different parameters.</em> </h6><br><div style="text-align:center;"><img title="Pic.13       " alt="Pic.13       " src="http://img717.imageshack.us/img717/5821/pic13lotssegmentation.jpg"></div><br><br><h6> <em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pic.14 y - 14-dimensional vector differences of super pixels for different segmentations.</font></font></em> </h6><br><div style="text-align:center;"><img title="Pic.14 y - 14-      " alt="Pic.14 y - 14-      " src="http://img139.imageshack.us/img139/1276/pic14segmenty14.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the 14-dimensional vector obtained after segmentation, </font></font><img src="http://img241.imageshack.us/img241/1110/form34elfij.jpg" alt="œµ_ij" title="œµ_ij"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">each element </font></font><img src="http://img718.imageshack.us/img718/120/form36eijk.png" alt="{œµ_ij_k}" title="{œµ_ij_k}"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">is a sign: did both superpixels </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> go to </font><font style="vertical-align: inherit;">the same segment when performing the </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> -th segmentation. </font><font style="vertical-align: inherit;">After all, two superpixels, which turned out to be in the same segments in all 14 cases, will most likely be coplanar or interconnected. </font><font style="vertical-align: inherit;">Conclusions about the presence of boundaries between superpixels, made on the basis of multiple segmentation, are more reliable than with a single segmentation. </font><font style="vertical-align: inherit;">This vector </font></font><img src="http://img241.imageshack.us/img241/1110/form34elfij.jpg" alt="œµ_ij" title="œµ_ij"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">is an input parameter when defining a ‚Äúsoft‚Äù border </font></font><img src="http://img267.imageshack.us/img267/6086/form31ysoftbelong.png" alt="y_ij‚àà[0,1]" title="y_ij‚àà [0,1]"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">between two superpixels </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br><br><h6> <em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pic.15 Parameter y (right) indicating the presence or absence of a border</font></font></em> </h6><br><div style="text-align:center;"><img title="Pic.15  y (),     " alt="Pic.15  y (),     " src="http://img441.imageshack.us/img441/8578/pic15boundaryexplain.png"></div><br><br><br><br><h3>  MRF Input and Output </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Now you can sum up the intermediate result: </font><font style="vertical-align: inherit;">5 types of parameters are involved in the </font><em><font style="vertical-align: inherit;">MRF</font></em></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> model </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Input are:</font></font><em><font style="vertical-align: inherit;"></font></em><font style="vertical-align: inherit;"></font><ul><li><font style="vertical-align: inherit;"></font><strong><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X</font></font></em></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> variables </font><font style="vertical-align: inherit;">related to calculated features ( </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">features</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) of an image</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Empirically calculated </font></font><strong><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Œ∏</font></font></em></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> variables </font><font style="vertical-align: inherit;">(parameters of the trained model)</font></font></li></ul><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The calculation also takes into account the conditional parameters (conditioned): </font></font><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Parameters </font></font><strong><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">œÖ</font></font></em></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> represents the ‚Äúconfidence level‚Äù ( </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">confidence</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) for the distance to the object, calculated based only on the local properties of the image section</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The </font></font><strong><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y</font></font></em></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> parameters </font><font style="vertical-align: inherit;">indicate the presence or absence of a clear boundary between the objects in the image. </font><font style="vertical-align: inherit;">These parameters are used to identify coplanarity and super-pixel connections.</font></font></li></ul><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Calculated in the </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MRF</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> parameters are:</font></font><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Parameters </font></font><strong><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Œ±</font></font></em></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , specifying the location and orientation of the planes in space, on which the polygons are located in the three-dimensional model</font></font></li></ul><h6> <em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pic.16 Model MRF combines all the features of the image, to find 3D</font></font></em> </h6><br><div style="text-align:center;"><img title="Pic.16  MRF      ,   3D" alt="Pic.16  MRF      ,   3D" src="http://img101.imageshack.us/img101/3083/pic16mrfexplain.png"></div><br><br><br><br><h4>  What will MRF do?  Target: min (distance error) </h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">When constructing a three-dimensional model of the scene, the relative error in determining the distance is the most significant. </font><font style="vertical-align: inherit;">For the true distance ( </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ground-truth depth</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) </font></font><img src="http://img25.imageshack.us/img25/3791/form38d.png" alt="d" title="d"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and estimated in the model </font></font><img src="http://img25.imageshack.us/img25/7889/form39darc.png" alt="d^" title="d^"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, the relative error is defined as</font></font><img src="http://img180.imageshack.us/img180/4176/form40darcddist.png" alt="((d^-d))/d=d^/d-1" title="((d^-d))/d=d^/d-1">  .<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Thus, the MRF model will be built in order to minimize this value. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Continued: </font></font><a href="http://vikds.habrahabr.ru/blog/95559/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">here</font></font></a> </div><p>Source: <a href="https://habr.com/ru/post/95541/">https://habr.com/ru/post/95541/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../95528/index.html">Do not rush to upgrade your HTC Hero to Android 2.1</a></li>
<li><a href="../95530/index.html">MiTM for GPRS</a></li>
<li><a href="../95535/index.html">What does the coming day prepare for us? Or start-up parties in the summer</a></li>
<li><a href="../95538/index.html">Memory structuring</a></li>
<li><a href="../95540/index.html">Google Maps offers a stroll to Edinburgh through France and Belgium</a></li>
<li><a href="../95542/index.html">Radio technology of the 21st century</a></li>
<li><a href="../95543/index.html">We found speakers and assemble iPad developers in #piter!</a></li>
<li><a href="../95544/index.html">Open beta testing of free fonts PingWi Typography (PWT) begins</a></li>
<li><a href="../95545/index.html">Overview of the practical navigator</a></li>
<li><a href="../95548/index.html">7 Ways to Hate a Developer (Quick Start Guide for Customers)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>