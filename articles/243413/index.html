<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Configuring Virtual Infrastructure: VDI Cluster Optimization</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Well, like optimization. Creative efforts to level out the heeling and loose infrastructure, which they tried to keep with all their might in a way ‚Äúd...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Configuring Virtual Infrastructure: VDI Cluster Optimization</h1><div class="post__text post__text-html js-mediator-article">  Well, like optimization.  Creative efforts to level out the heeling and loose infrastructure, which they tried to keep with all their might in a way <i>‚Äúdo not touch anything, everything can break</i> . <i>‚Äù</i>  Dangerous phrase, quickly becoming the life philosophy of an IT specialist who has stopped developing.  The source of the "fraud" IT. <br><br>  Half a year has passed since the most responsible person for the virtual infrastructure quit and left me the entire farm and operational documentation in the form of a list of service records.  During this time, a number of works were carried out to strengthen the foundation, increase the reliability and even the comfort of the structure.  The key points I want to share. <br><br><img src="https://habrastorage.org/files/198/8bc/58e/1988bc58e9dd47ce9598751eeef435de.jpg">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      So, given: <br>  VMware Enterprise Plus Virtualization Infrastructure.  Includes production, test zone and VDI.  The latter is implemented on the basis of the product fujitsu Pano Logic, which has not been updated for 2 years and, apparently, is not supported. <br>  The main cluster being upgraded is VDI, as the most extensive critical service and most dense in resource utilization.  It is implemented on the basis of full clones, because the associated clones do not understand the pano manager itself, and View does not want to buy the business either. <br><br>  As the storage system, a set of EMC arrays is used ‚Äî several CX4-240 and a pair of VNX.  And there is such a refinement as IBM SVC.  It is used for consolidation and storage virtualization (that is, luns are mounted with storages on SVC, pools are created there, and new LUNs are being created on these pools, which are sent to servers).  All storages are connected via FC SAN. <br><a name="habracut"></a><br>  Since there was no documentation, one had to learn how to live and work.  However, some seemingly innocuous changes sometimes led to unexpectedly unpleasant consequences, revealing strange settings and crutches. <br><br>  Quick navigation: <br><br>  <a href="https://habr.com/ru/post/243413/">1. Storage subsystem</a> <br>  <a href="https://habr.com/ru/post/243413/">2. Network</a> <br>  <a href="https://habr.com/ru/post/243413/">3. Computational resources</a> <br><br><a name="storage"></a><h4>  1. Storage subsystem </h4><br>  I started this direction even before my colleague left, since SAN was my main area of ‚Äã‚Äãresponsibility. <br><br><h5>  1.1 VAAI </h5><br>  The first thing that surprised me was the use of a large number of datastores of small (1TB) size.  Why - no one could explain.  An attempt to consolidate more into datastores immediately revealed a problem - too many scsi-locks, as a result, high latency and boot-storms, as a phenomenon.  Strange was that the storage system behaved as if it did not support VAAI.  But in the properties of datastores it is explicitly stated ‚ÄúHardware Acceleration: Supported‚Äù. <br><br>  Understanding came when they introduced a new host to the cluster.  A colleague remembered that before operation, "you need to enter a command - disable some kind of thing that leads to problems when working with SVC".  The ‚Äúpiece‚Äù was the <b>VMFS3.HardwareAcceleratedLocking</b> parameter set to 0. In other words, perhaps the most important function of the VAAI, Atomic Test and Set (ATS), allows you to block when changing metadata of a datastor not the entire datastore, but specific sectors on the disks. <br><br>  However, this setting had some problems with SVC.  At least with the version of the firmware that we had.  An attempt to update the firmware led to the death of two (of the three updated) nodes, support for this equipment was over, so we decided to transfer the datastores directly to the EMC arrays.  In general, it remained incomprehensible to me - why it was necessary to stretch such a curved layer of virtualization of storage on top of a clearly more advanced layer of EMC pools. <br><br>  <b>Recommendations</b> : From the point of view of performance, a smaller number of large LUNs is more profitable than a large number of small ones - there is an overhead for servicing each LUN and paralleling requests and worsening the performance of different cache levels.  (EMC recommendation to reduce the load on the SP: <i>Reduce the number of LUNs where it is possible. If you have a RAID group has multiple LUNs Replacing these with fewer, larger LUNs will also be reduced, therefore further reducing SP Utilization</i> .) <br><br>  When using storage virtualization systems, make sure that they are ‚Äúsmarter‚Äù than the lower level, or at least do not kill the repository functionality.  In our case, SVC was obviously "dumber than" EMC arrays, refused to see LUNs more than 2 TB, and made features such as auto-typing meaningless (and I suspect that Flash Cache). <br>  And of course, you should make sure that the storage hardware supports VAAI, and, moreover, this functionality is not blocked at the level of virtualization infrastructure. <br><br><h5>  1.2 Zoning and distribution to arrays </h5><br>  The second point is the strange scatter of different data categories across arrays.  BD interspersed with file servers and VDI in a chaotic manner were scattered around all the stores according to the principle ‚Äúwhere was the place‚Äù.  Not to mention the fact that some datastores were connected directly to EMC arrays, and some - through SVC. <br><br>  After long migrations and redistributions, we managed to distribute the data in the most optimal way - the most resource-consuming (productive servers and VDI) were folded onto VNXs, and the Clarions were given backups and less demanding services.  In SVC, only the directly routed to the LUN servers remained, I took all the datastores out of it.  The number of zones on the switches was reduced from ~ 120 to ~ 75, taking into account the fact that previously there were zones like "many targets - many initiators", and now no more than one initiator in the zone.  Just because the data of a certain type with which certain servers worked now lie on the same storage system, and not on three different ones. <br><br>  <b>What is the profit</b> - the extra zones created an extra load on the SAN network, the use of heterogeneous load (IO-intensive, such as database and sequential write, such as backups / file servers) on one array is detrimental to performance.  Using more than one initiator in one zone is bad practice. <br><br><h5>  1.3 Path Selection Settings </h5><br>  <i><b># esxcli storage nmp device list</b></i> on hosts showed that <br>  a) For the most part, the Round Robin policy is used to select paths to the datastores, <br>  b) For a part of datastores (first six) on the first two hosts, the path change takes place through 3 IOPS <br>  <i>Path Selection Policy Device Config: {policy = iops, iops = 3,</i> <br>  on the rest - the default value <br>  <i>Path Selection Policy Device Config: {policy = rr, iops = 1000,</i> <br>  c) On the last five hosts of the cluster, Fixed was generally used for some of the datastors (all communication with the storage system follows the same path as long as it is available). <br><br>  The choice of the Path Selection Policy is determined by the model and vendor of the storage system.  In most cases, round-robin is used for active-active configuration, in view of some load balancing.  By default, the path change occurs after 1000 iops.  However, in some cases this may lead to delays.  There is a <a href="http://kb.vmware.com/selfservice/microsites/search.do%3Flanguage%3Den_US%26cmd%3DdisplayKC%26externalId%3D2069356">kb from VMware</a> , where it is recommended to change this value to 1. There are <a href="http://www.jpaul.me/2011/10/tweaking-vmwares-round-robin-settings/">tests</a> showing that the performance of the storage subsystem is really higher in this case. <br><br>  <b>Recommendations</b> : configure multipathing according to vendor recommendations for your configurations.  And make sure that they are the same on all hosts.  Good help in this Host Profiles in VMware. <br><br><a name="network"></a><h4>  2. Network </h4><br>  In order to configure load balancing, all VDI cluster blade basket switches were stacked, organized by EtherChannel, and load balancing in the Teaming and Failover section was configured as Route Based on IP Hash.  The fact is that IP Hash works only on top of EtherChannel and only IP Hash is compatible with EtherChannel.  However, when the cluster grew to a second blade basket, whose switches did not support EtherChannel, a problem arose.  The problem manifested itself in the form of hard MAC-flapping on the switches of the second basket (according to networkers) and discarding received packets on the first (from 10 to 100 per second, according to the monitoring system). <br><br>  An important recommendation is to not change network settings for the entire cluster.  Checking on one host and making sure that everything is in order, we turned off EtherChannel on all others.  And lost access to everything except the first.  15 painful minutes, while moving away from the shock and returning the configuration back, no one, except the lucky ones located on the first server, could not work.  In the future, the settings were changed one by one, displaying it previously in the Maintenance Mode.  In parallel, I considered total man-hours of downtime, multiplying 15 by 1300 (the number of VDI) and dividing by 60. Thanks to the management for understanding ... But this was not the first shock associated with virtual desktops. <br>  By the way, I don‚Äôt know why, but until I recreated the dvSwitch, the host gave an error on every reboot: LACP Error: &lt;something that the current configuration supports IP Hash only&gt;.  Although EtherChannel has been disabled.  The new dvSwitch did not show such an error.  Moving the hosts and virtual machines to the new distributed switch burned another pack of nerve cells, but everything worked out. <br><br>  Along the way, I reconfigured the use of uplinks.  Before starting, all portgroups were configured the same way: <br><br><img src="https://habrastorage.org/files/a09/fea/46d/a09fea46d314499eba0de4413050f82c.png"><br><br>  I did this: <br><br><img src="https://habrastorage.org/files/e23/344/ae6/e23344ae660a40eba4b4272f99f0319d.png"><br><br>  As a way of organizing balancing - <b>Route based on physical NIC load</b> (the next interface in the list is selected if the current load is more than 70% loaded). <br><br>  <b>Conclusions</b> - setting up balancing and network resiliency is a creative process.  But subsequent monitoring showed that with such a configuration, a) the losses of the first blade basket packets disappeared, b) load balancing on uplinks became more uniform, c) rarely, but there were cases before that the host suddenly became inaccessible.  Over the past few months, this has not happened once.  d) Bulk vMotion (server output in Maintenance Mode, for example) does not affect VM traffic. <br><br>  The advantages of using IP Hash, compared to LB, I do not see. <br><br><a name="computing"></a><h4>  3. Computational resources </h4><br>  It immediately seemed to me that 1.5 GB of RAM for virtual desktops on Windows 7 is a mockery of users.  And most likely a negative impact on the disk subsystem due to swapping inside the OS.  But there was no excess memory.  The risk of losing fault tolerance and swapping at the virtual machine level was more negative.  The idea came from the news about disabling the Transparent Pages Sharing tool from the default settings in upcoming versions of vSphere.  More precisely, from the discussion about her on <a href="https://www.facebook.com/groups/vmug.ru/permalink/670539189733835/">Facebook</a> . <br><br>  Summary: <br><br>  The feature is disabled because there is a hypothetical security threat ( <i>under certain highly controlled conditions *</i> ). <br>  In most implementations, the technology is really almost useless since ASLR appeared and support for large memory pages.  Since to find two identical pages with the size of these in 2 MB is less likely than in 4 KB.  And for server virtualization of a 2 MB page is much more critical in terms of performance than saving memory. <br><br>  However, why not test it on a VDI cluster? <br><br>  I made the following changes to the host's Advanced Settings: <br>  Mem.AllocGuestLargePage = 0 instead of 1 - disabling large memory pages <br>  Mem.ShareScanGhz = 6 instead of 4 - increase the scanning frequency <br>  Mem.ShareScanTime = 30 instead of 60 - increase scanning speed <br><br>  To compensate for the increased processor load, I turned off vNuma chips as useless in the case of VMs that have less than 8 vCPUs.  These settings (along with the Path Selecting Policy settings) I distributed to all hosts using Host Profiles.  The result can be seen on the screenshot below. <br><br><img src="https://habrastorage.org/files/224/983/338/224983338f154d6e9db802974281dc20.png"><br><br>  Explanation of the parameters: <br>  If two VMs share 100 MB of memory, the <b>Shared</b> parameter will be 200 MB, and the <b>Shared Common</b> - 100 MB. <br>  As can be seen from the monitoring results for the month, Shared Common has grown four times, and Shared - by six.  The total memory savings amounted to almost 700 GB, that is, 600 GB more compared to the state on October 26.  This is almost a quarter of all cluster resources.  True, the average processor load has increased from 50-60% to 70-90%. <br>  On November 5, there is a slight decline, as Mem.ShareScanTime and Mem.ShareScanGhz had to be returned to default values ‚Äã‚Äãin order to reduce the processor load.  Now it is kept at 60-80%. Nevertheless, the savings still remained significant and there was an opportunity for all machines that had 1.5 GB of memory to increase its volume to 2. <br><br>  The impact of these changes in terms of the responsiveness of the disk subsystem can be estimated from the image below.  The value of Read Latency for several datastores is given.  Unfortunately, for the same period, experiments with the SCCM maintenance windows occurred, which gave night peaks with fantastic values ‚Äã‚Äãup to 80 seconds and made the average values ‚Äã‚Äãabsolutely non-indicative.  For this reason, I did not give myself the schedule - night peaks killed all the visibility of the daily load.  But it is possible to estimate the change by the minimum values ‚Äã‚Äãof the response time (the first columns). <br><br><img src="https://habrastorage.org/files/3ef/4b1/a42/3ef4b1a420cc486e8bab137ca843357c.png"><br><br>  At first, it was generally unusual to see the readings of the average Latency in the "green" zone - 10-20 ms.  It was usual that they almost never fell below 30. <br><br>  <b>Conclusion</b> : not all mutilated technologies are as useful as they are said to be.  But not all the features on which they put a cross, and which competitors, in this case VMware, bury on their speeches are so senseless.  However, you need to know them and understand when to use.  And for this you need to study the settings, parameters and properties of the product, including undocumented.  And then the management of IT infrastructure will not be shamanism.  And it will be competent and creative use of knowledge, which is the basis of professionalism. <br><br>  Thanks for attention. </div><p>Source: <a href="https://habr.com/ru/post/243413/">https://habr.com/ru/post/243413/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../243403/index.html">A brief synopsis of javascript</a></li>
<li><a href="../243405/index.html">Game hackathon, December 6-7, Moscow</a></li>
<li><a href="../243407/index.html">Odroid-W + W Docking Board LCD</a></li>
<li><a href="../243409/index.html">Secrets of setting goals for entrepreneurs</a></li>
<li><a href="../243411/index.html">Connect FB, VK, G + to Android. Light version</a></li>
<li><a href="../243417/index.html">Ethernet Weather Station 2 - Continued ...</a></li>
<li><a href="../243419/index.html">Turing test based on real search queries in Yandex</a></li>
<li><a href="../243421/index.html">Print from Google Apps Script</a></li>
<li><a href="../243423/index.html">New IBM Global Entrepreneurship Program Accelerates Cloud Development</a></li>
<li><a href="../243425/index.html">Compact RSA implementation for embedded applications</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>