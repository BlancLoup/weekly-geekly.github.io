<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>CEPH for pumping</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Something like this is the first installation of CEPH on real hardware. 

 You installed cef, but it slows down and falls why it is not clear? Then yo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>CEPH for pumping</h1><div class="post__text post__text-html js-mediator-article"> <a href="https://habrahabr.ru/company/at_consulting/blog/324374/"><img src="https://habrastorage.org/files/831/376/663/83137666375a495ea084effaaf24334f.jpg"></a> <br>  <font color="#999999"><i>Something like this is the first installation of CEPH on real hardware.</i></font> <br><br>  You installed cef, but it slows down and falls why it is not clear?  Then you came to the right place!  I'll pump your ceph. <br><a name="habracut"></a><br>  You listened to your friends and put the first disks on your car, and then you are surprised that she does not make a lap record at the local cartadrome, and you dreamed of the Nurburgring?  Right!  It does not happen, you cannot get away from physics anywhere, so before building a cluster you should calculate IOPS (at least approximately), the required volume and select disks for this.  If you are thinking of scoring a cluster of 8TB disks, then you should not expect hundreds of thousands of IOPS from it.  What are IOPS-s and their approximate value for different types of disks given <a href="https://habrahabr.ru/post/164325/">here</a> . <br><br>  Another thing worth paying attention to is the magazines. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      For those who do not know: ceph first writes the data to the log, and after some time transfers it to a slow OSD.  Immediately there are fans to save money, take one or two SSDs faster and make them a magazine for 20-25 OSD.  First, your IOPS will squeeze a little.  Secondly, it works until the first death of the SSD.  Then you have all the OSD that were on this SSD.  By setting the logs should be approached very carefully.  The recommendations on ceph.com indicate that no more than 5-6 OSD is needed per SSD, but it does not indicate what log size to specify and which filestore max sync interval parameter to select.  Unfortunately, these parameters are individual for each cluster, so you will need to select them yourself through numerous tests. <br><br>  The second thing that catches the eye, in our carriage, are two large sofas instead of seats.  You won't put two sofas instead of seats in your car in 2017? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2f7/466/087/2f746608740e375601224e979ac1da97.jpg"><br><br>  In relation to CEPH, we are now talking about pools. <br><br>  Usually they make one big pool and stop at this, but this is wrong.  One large pool leads to problems with ease of management and upgrade.  How is the upgrade related to pool size?  And here it is: it is very dangerous to upgrade a live cluster on the fly, so it makes sense to deploy a new small row, otrepletirovat data to a new pool and switch.  From this comes the recommendation: to divide the data into logical entities.  If you have an openstack, then it makes sense to start from the openstack zones.  With a large number of servers, spread them on racks and rule with crushmap rules. <br><br>  <b>Now crawled under the hood.</b> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/981/6d9/583/9816d95830466726d43248474d9c2697.jpg"><br><br>  If we are not designing a system for tens of thousands of IOPS, then the processor can be supplied with an entry level.  However, keep in mind: the more disks per server and the more they can give out IOPS, the greater the load.  Therefore, if you have an extrabudgetary configuration of 2-3 JBODs per server (which is a bad idea in and of itself), then the processor will be heavily loaded.  We, with one JBOD and 30 disks, have not yet driven the processor into the regiment.  But the "wiring" change.  No, of course, it will start on 1 gbit / s, but when the OSD drops, the rebalance begins and here the channel is loaded to the full.  Therefore, interconnect between CEPH 10 Gbit / s nodes is mandatory.  The good tone rule is to do LACP. <br><br>  "Suspension" is better strengthened.  We put the memory on the basis of the recommended 1 gigabyte RAM for 1 TB of data.  It is possible and more.  In memory CEPH quite voracious. <br><br><img src="https://habrastorage.org/files/60c/b52/e09/60cb52e09af546a59c0ee7773f2cb4d9.png"><br><br>  Now about the fast ride on what was built. <br><br>  CEPH has data integrity checks - the so-called scrub and deep scrub.  Their launch greatly affects IOPS, so it‚Äôs better to run them with a script at night, limiting the number of PGs to be checked. <br><br>  Limit the rebalance rate with the parameters osd_recovery_op_priority and osd_client_op_priority.  It is quite possible to survive during the day a slow rebalance, and at night, at the moments of minimum load, run it to the full. <br><br>  Do not fill the cluster out of place: the 85% limit is not just created.  If there is no possibility to increase space in the cluster with new servers, then use reweight-by-utilization.  Note that this utility has a default setting of 120%, and also when it starts, rebalance starts. <br><br>  The weight parameter also affects the filling, but indirectly.  Yes, CEPH has weight and reweight.  Usually weight is made equal to the volume of the disk. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fad/7e0/912/fad7e09123a9d3f1cbd47ea6809ed20d.jpg"></div><br>  <b>So, our CEPH is almost pumped up, it remains only to attach the monitor.</b>  You can monitor anything, but monitoring of the output of ceph osd perf is particularly interesting.  Most interesting is fs_apply_latency.  It is he who shows us which OSD takes a long time to respond and decreases cluster performance.  Moreover, the SMART on these OSD can be quite a good one. <br>  It looks like this. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f55/71a/b5f/f5571ab5f7cbc45e764439b48a282b38.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/476/7da/c1a/4767dac1a577c9522aca5a5f8620b60f.png" alt="image"><br><br>  <b>So filming the transformations of the old carriage is over.</b>  <b>It's time to show the backstage and disprove the myths that I heard:</b> <br><br><ol><li>  Do normally - it will be normal.  With proper design CEPH does not need a large team of administrators to support it. </li><li>  There are bugs.  Saw OOM on OSD.  Just came OOM, killed OSD, then another.  This stopped.  CEPH was updated, OOM came a couple of times, but then it stopped just as suddenly.  Otdebazhit failed. </li><li>  Mostly CEPH is very hungry for swap.  Perhaps you need to edit the swappiness settings. </li><li>  The transition from double to triple replication is sad and long, so think hard though. </li><li>  Use single-type servers, one OS in a cluster and one version of CEPH, down to the minor.  Theoretically, CEPH will start everything, but then why individually debug each piece of hardware? </li><li>  CEPH has fewer bugs than it looks.  If you think you have found a bug, then first read the documentation carefully. </li><li>  Get one test bench on the physical hardware.  Not everything can be checked in virtual machines. </li></ol><br>  <b>Separately, I want to say that you should not do:</b> <br><br><ol><li>  Do not remove disks from different nodes of a flat cluster with the words ‚Äúthis is cef, what will it be?‚Äù.  Health error will be.  Depending on the amount of data in the cluster and the level of replication. </li><li>  Double replication - the path of extremes.  First, there are more chances to lose data, and second, a slower rebalance. </li><li>  Dev releases let developers test, especially any new ones with bluestore.  Of course, it can work, but when it crashes down, how will you get the data from the incomprehensible file system? </li><li>  FIO with the parameter engine = rbd can lie.  Do not rely on it for tests. </li><li>  Do not immediately pull into the prod all the new things that you found in the documentation.  First check on the physical stand. </li></ol><br>  <b>Thanks for attention.</b> </div><p>Source: <a href="https://habr.com/ru/post/324374/">https://habr.com/ru/post/324374/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../324364/index.html">The best reports of HolyJS 2016 Moscow: Access open</a></li>
<li><a href="../324366/index.html">Tracking js errors with Metrics</a></li>
<li><a href="../324368/index.html">How to choose NGFW or what manufacturers keep back?</a></li>
<li><a href="../324370/index.html">Set method</a></li>
<li><a href="../324372/index.html">How and why static analyzers deal with false positives</a></li>
<li><a href="../324376/index.html">Exceptions in Windows x64. How it works. Part 3</a></li>
<li><a href="../324378/index.html">We bring business to the international market in 5 minutes</a></li>
<li><a href="../324380/index.html">Hard Prioritization</a></li>
<li><a href="../324382/index.html">Fine Rust state machines</a></li>
<li><a href="../324384/index.html">Intel SGX Extensions Tutorial. Part 7, revision of the enclave</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>