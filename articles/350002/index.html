<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Two models are better than one. Experience Yandex.Translate</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Once we already talked about how machine translation appeared and developed. Since then, another historical event has happened - neural networks and d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Two models are better than one. Experience Yandex.Translate</h1><div class="post__text post__text-html js-mediator-article">  Once we <a href="https://habrahabr.ru/company/yandex/blog/224445/">already talked</a> about how machine translation appeared and developed.  Since then, another historical event has happened - neural networks and deep learning have finally conquered it.  Among the tasks of natural language processing (Natural Language Processing, NLP), machine translation was one of the first to receive a strict statistical basis - as early as the early 1990s.  But in the field of deep learning, he was a relatively late participant.  In this post, we, the Yandex machine translation team, are discussing why it took so much time and what new opportunities were opened by machine translation based on neural networks. <br><br>  We will also be happy <a href="https://events.yandex.ru/events/meetings/01-march-2018/">to answer questions at</a> the Yandex ‚ÄúFrom the Inside: From Algorithms to Measurements - in Translator, Alice and Search‚Äù <a href="https://events.yandex.ru/events/meetings/01-march-2018/">meeting</a> on March 1 (you can register or ask a question in the broadcast chat). <br><br><img src="https://habrastorage.org/webt/sg/bw/lj/sgbwlj7in-94f6zx4tkgsbtimbu.jpeg">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  Phrasal machine translation </h2><br>  Just three years ago, almost all serious industrial and research machine translation systems were built using a conveyor of statistical models (‚Äúphrasal machine translation,‚Äù FMP), in which neural networks did not participate.  Phrasal machine translation for the first time made machine translation available to the mass user in the early 2000s.  With enough data and enough computing resources, the FMF allowed developers to create translation systems that basically gave an idea of ‚Äã‚Äãthe meaning of the text, but were full of grammatical and sometimes semantic errors. <br><a name="habracut"></a><br>  The conveyor of statistical models, which was used to build FMP systems, is actually quite intricate.  Based on leveled teaching sentences ‚Äî for example, translations of Russian news articles into English ‚Äî the system builds a statistical model that tries to ‚Äúexplain‚Äù each of the words of the target language with the words of the source language using ‚Äúhidden‚Äù variables known as word alignments. ).  The idea is intuitive, and the math (maximizing expectations, the EM algorithm) is <a href="http://www.aclweb.org/anthology/J93-2003">pretty good</a> . <br><br><img src="https://habrastorage.org/webt/tk/3k/to/tk3ktosktwvbldscrptk6isbhui.png"><br><br>  An iterative EM algorithm ‚Äî when at each step we make the best guess as to which source words correspond to which target words, and then these ‚Äúguesses‚Äù are used as training data for the next iteration ‚Äî quite similar to the approach we could apply to decoding menu in a foreign language.  The problem is that it is based on separate words, and word-for-word translation is often impossible. <br><br>  Word alignment models introduce various assumptions about independence in order to simplify the task of too large computational complexity (deciding which of the 2 ^ {JI} possible ways to align words in the original sentence of length I with the words in the target sentence of length J) to more acceptable sizes .  For example, a hidden Markov model can be developed to solve this problem with complexity O (I ^ 2J). <br><br>  Word-based models have never shown particularly good results.  And everything looked rather sad until a <a href="http://www.aclweb.org/anthology/N03-1017">method</a> was proposed <a href="http://www.aclweb.org/anthology/N03-1017">for constructing a translation model at the phrase level</a> , on top of the alignment of words.  After that, machine translation began to be used on the Internet - for translating websites in foreign languages ‚Äã‚Äãor simply as an inexhaustible source of inspiration for <a href="">memes</a> . <br><br>  FMP uses word alignment matrices to determine which pairs of phrases in a pair of sentences can serve as translations for each other.  The resulting table of phrases becomes the main factor in the linear model, which, together with components such as the language model, is used to generate and select potential translations. <br><img src="https://habrastorage.org/webt/er/u7/-j/eru7-jnuxktvrrmhvcgzd46weuu.png"><br>  FMP very well remembers phrases from a parallel body of texts.  Theoretically, one single example is enough for him to learn a phrasal translation, if only the words in this example are relatively well aligned.  Although this may give the impression of a fluent language, in fact, the system does not really understand what it does. <br><br>  The parameter space of the FMP system is huge, but the parameters themselves are extremely simple - ‚Äúwhat is the probability of seeing this target phrase as a translation of this source phrase?‚Äù When the model is used for translation, each source sentence is divided into phrases that the system has seen before, and these phrases are translated independently of each other. from friend.  Unfortunately, when these phrases are stitched together, inconsistencies between them turn out to be too noticeable. <br><br><img src="https://habrastorage.org/webt/og/v_/jv/ogv_jvysjmvie5qgbcq9abymcv4.png"><br><br><h2>  What do neural networks give us? </h2><br>  Researchers engaged in speech recognition or images, have long been thinking about how to encode data, because a continuous signal is difficult to imagine in the discrete world of computers.  But working on the processing of natural language is not particularly worried about this.  It seemed pretty simple - you can replace the words with integer identifiers and index the model parameters with these numbers.  This is exactly what N-gram language models and FMP systems do. <br><br><img src="https://habrastorage.org/webt/6b/rv/w7/6brvw7-dekxbe426ojpixrxl5pw.png"><br><br>  Unfortunately, in such a unitary encoding, all words are in some sense equally alike (or equally different from each other).  This does not correspond to how people perceive language, when words are defined by how they are used, how they relate to other words and how they differ from them.  This is a fundamental disadvantage of pre-neural models in the processing of natural language. <br><br>  Neural networks are not easy to cope with such discrete categorical data, and for the automatic processing of the language we had to invent a new way of representing the input data.  It is important that in this case the model itself learns the presentation of the data necessary for a specific task. <br><br>  The word word representation (word embedding) used in neural machine translation, as a rule, matches each word in the input data with a vector of several hundred real numbers.  When words are represented in such a vector space, the network can learn to model the relations between them as it would be impossible to do if the same words are presented as arbitrary discrete symbols, as in FMF systems. <br><br>  For example, the model can recognize that, since ‚Äútea‚Äù and ‚Äúcoffee‚Äù often appear in similar contexts, both of these words should be possible in the context of the new word ‚Äúspill‚Äù, which, say, in the training data only one of them occurred. <br><br>  However, the process of teaching vector representations is clearly more statistically demanding than the mechanical memorization of examples used by FMP.  In addition, it is not clear what to do with those rare input words that were not often encountered so that the network could build an acceptable vector representation for them. <br><br><h2>  No assumptions about independence </h2><br>  The transition from discrete to continuous representations allows us to model dependencies much more flexibly.  This is literally seen in the translations that are made by the machine translation system based on neural networks, which we built in Yandex (NMP). <br><br><img src="https://habrastorage.org/webt/uf/dq/0y/ufdq0y6_nvavhn5yekliopkw_du.png"><br><br>  A system based on the translation of phrases implicitly assumes that each source phrase can be translated independently of the rest of the sentence.  It sounds like a recipe for a catastrophe, but in a discrete statistical model it is inevitable, because adding to the model only one bit of context information would double the number of parameters.  This is the ‚Äúcurse of dimension‚Äù known in our business. <br><br>  The only reason why a phrases MP generally works is that a language model is used to stitch independently translated phrases.  This model is trained on large volumes of monolingual data and makes orthogonal independence assumptions, which can often compensate for phrase-level assumptions. <br><br><h2>  Decoder states and hidden states </h2><br>  In the FM system, it was necessary to make decisions at each step which source words we ‚Äútranslate‚Äù at the moment (but in fact it is often quite difficult to build such an alignment between the source and target words).  In the NMP decoder, we simply apply transformations to the vector of hidden states, generating target words step by step until the network decides to stop (or until we stop it). <br><br>  The vector of hidden states, which is updated at each step, can store information from any part of the original sentence and any part of the translation already completed by this time.  This lack of ‚Äúexplicit‚Äù assumptions about independence is perhaps the most important distinction between neural and phrasal machine translation systems.  It is precisely this that most likely explains the feeling that translations of neural systems more accurately convey the general structure and meaning of the original sentence. <br><br><h2>  Calculation of dynamic dependencies </h2><br>  However, it is naive to say that the NMP models do not make any assumptions about independence simply because they do not explicitly exclude the context, as is the case in FMF.  The most important developments in the architecture of the NMP over the past few years show that the key to quality improvement is the ability of the model to effectively transfer information between different parts of the proposal.  The most suitable mechanism for dynamically and selectively distributing information from the representations generated by the encoder for different parts of the original sentence is called ‚Äúattention‚Äù. <br><br>  Having learned to ‚Äúpay attention‚Äù to different parts of the source and target sentences in the translation process, the network tries to solve one of the fundamental problems of natural language processing: how to turn a linear sequence of words into something more structured, for example, a tree. <br><br><img src="https://habrastorage.org/webt/zd/9n/p4/zd9np4jtlkcfp9i47j-ko-sflvi.png"><br><br><img src="https://habrastorage.org/webt/pk/yg/fp/pkygfpuoa9astlqvmbrsaqr6ryg.png"><br><br><h2>  Putting it all together </h2><br>  Neural and phrasal machine translation systems are quite unlike creations.  Yandex.Translate has many years of experience in developing a FMP system.  Last year, we began exploring the possibilities of integrating into our neural machine translation service. <br><br>  Being, among other things, a search engine, we did not experience a lack of training data for both systems.  Judging by the appraisal studies, we have created the most reliable phrasal machine translation system in the world for a pair of English-Russian. <br><br>  Obviously, the latest NMP architectures could produce results no worse and even better than our phrase system.  But suddenly there were disturbing signals in the behavior of our neural system - especially when translating user requests. <br><br>  It is worth noting that Yandex.Translator strives to be able to translate everything that users can offer.  And some of them offer rather strange things.  The result is, to put it mildly, some discrepancy between the training and user data. <br><br>  Recall that our system is trained on translations retrieved from the Internet.  These are mainly high-quality translations, as a rule, without problems with spelling, punctuation, and other ‚Äútrifles‚Äù that a user who is in a hurry to receive his translation may not really care about.  In terms of topics, content and style, the texts that our users need to translate are often very different from the documents that fall into our teaching corpus. <br><br>  While the average quality of translations of the FMP system is lower than that of the NMP, the dispersion of the perceived quality of the translations of the NMP can be much higher. <br><br>  It seems that there are two explanations for this: <br><br><ol><li>  FMP remembers rare words and phrases better, so it has fewer ‚Äúgaps‚Äù in language knowledge. </li><li>  NMP pays much more attention to context, so some noise in the input signal can affect the entire translation. </li></ol><br>  FMF just learns phrases from the corpus, so when it comes to low-frequency input data, it has a distinct advantage over NMP.  On the other hand, the NWO must see the word several times in order to build an acceptable vector representation.  It also tries very hard to model the dependencies in the data, so if it encounters garbage in the input data, it can easily produce ten times more garbage in the weekend. <br><br><h2>  Examples of stewids </h2><br>  This is not exactly what we wanted to see when we were preparing to tell how much better our translations became thanks to neural networks. <br><br>  It takes a long time to earn user confidence in the product, but you can lose this trust in a matter of seconds.  Translations like the ones shown above are probably a good way to achieve precisely the loss of trust. <br><br>  Worse, not all users of our service will be able to assess the adequacy of translations, because not everyone knows the source or target language.  Our duty to them is to avoid such worst translations. <br><br><h2>  The best of both worlds </h2><br>  Our two machine translation systems - phrasal and neural - behaved quite differently on the same input data.  Each of them at the same time showed a fairly high quality of translation.  Therefore, we remembered what we were taught in the first year of data analysis and built an ensemble system.  The dissimilarity of systems often serves as the key to a successful ensemble. <br><br>  Instead of combining the two systems during decoding in some complicated way, we chose a simpler approach: choose the result from one of the two systems.  For this, we trained classifiers using <a href="https://habrahabr.ru/company/yandex/blog/333522/">CatBoost</a> . <br><br><img src="https://habrastorage.org/webt/ow/6z/cn/ow6zcnvsotbm5b1k3fl7gzaks6c.png"><br><br>  The first classifier, trained on a small, hand-marked set of stewids, revealed particularly catastrophic failures in the NMP model.  The second, trained to predict the BLEU scoring difference between the output of the two systems on a larger set of pairs of parallel sentences, was used to catch less obvious errors.  For example, such as under translation, when the neural network simply refuses to cooperate and leaves part of the original sentence untranslated. <br><br><h2>  How to cope with the mismatch of domains </h2><br><h4>  Cherry on the cake </h4><br>  Yandex users send millions of translation requests daily, while repeatedly having to translate many identical requests.  As a rule, these requests are very short, therefore, they lack context.  Unfortunately, the neural system is quite difficult to translate such short queries, since very few of the examples in which the system is trained consist of individual words or phrases.  Despite the ensemble and CatBoost, we understood that sometimes all the power of deep learning may not be enough to give our users the best result. <br><br><img src="https://habrastorage.org/webt/4l/nq/qp/4lnqqpszwnqu5xn2mwjih3q4nie.png"><br><br>  Yandex.Translate is proud of the dictionary entries it offers to users.  Yandex dictionaries are created automatically using a well-tuned machine learning pipeline optimized using user feedback and the crowdsourcing Yandex-Toloki platform. <br><br>  We decided that by including the best translations from these dictionary entries into our system as constraints, we can significantly improve the quality of machine translation for the most frequent queries. <br><br><h4>  Context adaptation </h4><br>  The last topic of today's post is that if machine translation has reached new heights thanks to a hybrid NMP, then from where can we expect further improvements? <br><br>  One of the little-studied areas in machine translation is the use of context in the broadest possible sense.  The context may include the previous sentence in the document, some information about the persons or entities mentioned in the text, or just information about where on the web page the text that we are currently translating is taken. <br><br>  By analogy with the perception of context by today's NMP systems at the sentence level (which was beyond the power of the previous generation of FMP), we are experimenting to include more and more diverse types of context in our translation system in order to better translate the text in specific situations and for various tasks. <br><br>  This idea is not too radical.  Most translators will say that the more context or background information about the translated text and its audience, the easier it will be their work.  In this sense, machine translation systems should not differ from people.  And we already have positive results showing that the work in this direction is worth the effort invested. <br><br>  For example, an experimental system that uses the previous original sentence along with the current one when translating.  It seems that the network is quite capable of learning to understand when it is necessary to ‚Äúpay attention‚Äù to the words from the previous sentence.  This works naturally for pronouns that have antecedents, thus avoiding this kind of trouble: <br><br><img src="https://habrastorage.org/webt/oz/ih/5l/ozih5lcfrzkvc4jxz2le2okdqtg.png"><br><br>  Another example is the system that we plan to launch in the near future on the Yandex Browser. <br><br>  Every day, millions of users translate pages thanks to our integration in the Yandex Browser.  Since the structure of the HTML page makes it easy to determine where the text is - in the navigation component, title, or block with content, we decided to use this information to improve our translations. <br><br>  It is clear that in many cases the best translation of the word ‚ÄúHome‚Äù is likely to be ‚ÄúHome‚Äù.  However, in the navigation bar of the website, it should certainly be translated as ‚ÄúHome‚Äù.  Similarly, ‚ÄúBack‚Äù is probably best translated in such a context as ‚ÄúBack,‚Äù not ‚ÄúBack.‚Äù <br><br><img src="https://habrastorage.org/webt/rz/oe/ac/rzoeacsgg_er2xaj3jmuyufpr1k.jpeg"><br>  By automatically marking out the training sample of parallel texts extracted from the Internet with the labels ‚Äúnavigation‚Äù, ‚Äútitle‚Äù, ‚Äúcontent‚Äù, and then setting our system to predict these marks in the translation process, we significantly improved the quality in specific types of translation. <br><br>  Of course, we could achieve only a retraining of the system on this rather specific distribution.  Therefore, in order to avoid performance degradation in more standard areas, we included in the calculation of the learning loss function the Kulbach ‚Äì Leibler distance between the predictions of our adapted model and the predictions of our common system.  In fact, this is a model fining for excessively changing predictions. <br><br><img src="https://habrastorage.org/webt/5c/_u/ad/5c_uadn3yej11u-pipreq5jyz9w.png"><br><br>  As can be seen from the graphs, the quality of the data in the domain increases significantly, and as soon as our loss function is supplemented by taking into account the Kullback-Leibler distance, an increase in quality no longer leads to a deterioration of translation for documents outside the domain. <br><br>  Very soon we will launch this system on Yandex. Browser.  Hopefully, this is only the first of a number of improvements that we will make based on a better understanding of the user context and the ‚Äúunjustified effectiveness of [recurrent] neural networks.‚Äù </div><p>Source: <a href="https://habr.com/ru/post/350002/">https://habr.com/ru/post/350002/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../349990/index.html">How to use response compression in ASP.Net Core</a></li>
<li><a href="../349992/index.html">Wi-Fi in law</a></li>
<li><a href="../349994/index.html">Fear of public speaking</a></li>
<li><a href="../349996/index.html">Binary ternary bit magic</a></li>
<li><a href="../349998/index.html">Secure home network: create an isolated segment for guests</a></li>
<li><a href="../350004/index.html">Issue # 12: IT training - current issues and challenges from leading companies</a></li>
<li><a href="../350006/index.html">"Tutor: Mathematics" to prepare for the exam and CDF - from idea to release. A story about a unique educational project</a></li>
<li><a href="../350008/index.html">Learn OpenGL. Lesson 4.7 - Advanced data handling</a></li>
<li><a href="../350010/index.html">YouTrack 2018.1 release: dependent task fields, personal localization and much more</a></li>
<li><a href="../350012/index.html">Analysis of the consensus algorithm in Tendermint</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>