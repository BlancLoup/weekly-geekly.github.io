<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Switching from Redshift to ClickHouse</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="For a long time iFunny used Redshift as a database for events that occur in backend services and mobile applications. He was chosen because at the tim...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Switching from Redshift to ClickHouse</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/s8/xo/0d/s8xo0dnodxojhff6jufnruyg660.jpeg"><br><br>  For a long time iFunny used Redshift as a database for events that occur in backend services and mobile applications.  He was chosen because at the time of implementation, by and large, there were no alternatives that are comparable in cost and convenience. <br><br>  However, everything changed after the public release of ClickHouse.  We studied it for a long time, compared the cost, estimated the approximate architecture, and then, finally, this summer we decided to see how useful it was to us.  In this article, you will learn about the problem Redshift helped us solve, and how we transferred this solution to ClickHouse. <br><a name="habracut"></a><br><h2>  Problem </h2><br>  iFunny needed a service similar to Yandex.Metrica, but only for internal consumption.  I will explain why. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      External clients write events.  These can be mobile applications, web sites or internal backend services.  It is very difficult for these clients to explain that the event reception service is now unavailable, ‚Äútry sending in 15 minutes or an hour later‚Äù.  There are many customers, they want to send events all the time and cannot wait at all. <br><br>  In contrast, there are internal services and users who are quite tolerant in this regard: they can work correctly even with an unavailable analytics service.  And most of the product metrics and A / B test results generally make sense to look only once a day, and maybe less often.  Therefore, reading requirements are quite low.  In the event of an accident or update, we can afford to be inaccessible or inconsistent in reading for several hours or even days (in a particularly neglected case). <br><br>  If we talk about numbers, then we need to take about five billion events (300 GB of compressed data) per day, while storing the data for three months in a ‚Äúhot‚Äù form available for SQL queries, and in a ‚Äúcold‚Äù one for two years or more, but so that within a few days we can turn them into ‚Äúhot‚Äù ones. <br><br>  Basically, data is a set of events ordered by time.  Event types about three hundred, each has its own set of properties.  There is still some data from third-party sources that should be synchronized with the analytics database: for example, a collection of application installations from MongoDB or the external service AppsFlyer. <br><br>  It turns out that under the database we need about 40 TB of disk, and under the ‚Äúcold‚Äù storage - about 250 TB more. <br><br><h2>  Redshift Solution </h2><br><img src="https://habrastorage.org/webt/f0/nq/dl/f0nqdl7cvriq9ygc3jlhelfdlqi.png"><br><br>  So, there are mobile clients and backend services from which you need to receive events.  The data is received by the HTTP service, it performs minimal validation, collects events on the local disk into files grouped by one minute, immediately compresses and sends them to the S3 batch.  The availability of this service depends on the availability of the servers with the application and AWS S3.  Applications do not store states, so they are easily balanced, scaled, and interchanged.  S3 is a relatively simple file storage service with a good reputation and availability, so you can rely on it. <br><br>  Next you need to somehow deliver the data to Redshift.  It's all quite simple: Redshift has a built-in S3 importer, which is the recommended way to load data.  Therefore, once every 10 minutes, a script is launched that connects to Redshift and asks it to load data using the <code>s3://events-bucket/main/year=2018/month=10/day=14/10_3*</code> prefix <code>s3://events-bucket/main/year=2018/month=10/day=14/10_3*</code> <br><br>  In order to track the status of the download task, we use <a href="https://airflow.apache.org/">Apache Airflow</a> : it allows you to repeat the operation in case of errors and have a clear execution history, which is important for a large number of such tasks.  And in case of problems, you can repeat the download for some time intervals or download the ‚Äúcold‚Äù data from S3 a year ago. <br><br>  In the same Airflow, in the same way, according to the schedule, scripts work that connect to the database and perform periodic downloads from external repositories, or build aggregations on events in the form of <code>INSERT INTO ... SELECT ...</code> <br><br>  Redshift has poor availability guarantees.  Once a week, for up to half an hour (a time window is specified in the settings) AWS can stop the cluster for updating or any other scheduled work.  In the event of a single node failure, the cluster also becomes unavailable until the host is restored.  It usually takes about 15 minutes and happens about once every six months.  In the current system, this is not a problem, it was originally designed for the fact that the base will be periodically unavailable. <br><br>  Under Redshift, 4 ds2.8xlarge instances were used (36 CPU, 16 TB HDD), which in total gives us 64 TB of disk space. <br><br>  The last point is the backup.  The backup schedule can be specified in the cluster settings, and it works fine. <br><br><h2>  Motivation for the transition to ClickHouse </h2><br>  Of course, if there were no problems, no one would have thought about the migration to ClickHouse.  However, they were. <br><br>  If you look at the ClickHouse storage scheme with the MergeTree and Redshift engine, you can see that their ideology is very similar.  Both databases are columnar, they work fine with a large number of columns and compress data on a disk very well (and in Redshift you can configure compression types for each individual column).  Even the data is stored in the same way: they are sorted by the primary key, which allows you to read only specific blocks and not to keep separate indices in memory, and this is important when working with large amounts of data. <br><br>  The essential difference, as always, is in the details. <br><br><h3>  Table for every day </h3><br>  Sorting data on disk and the actual deletion in Redshift occurs at the moment when you execute: <pre> <code class="xml hljs">VACUUM <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">tablename</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre>  In this case, the vacuum process works with all the data in this table.  If you store data for all three months in one table, this process takes an indecent amount of time, and you need to perform it at least daily, because old data is deleted and new data is added.  We had to build separate tables for each day and combine them through the View, and this is not only the difficulty in rotating and supporting this View, but also slowing down queries.  Upon request, judging by explain, all tables were scanned.  And although scanning a single table takes less than a second, with their number in 90 pieces, it turns out that any query takes at least a minute.  This is not very convenient. <br><br><h3>  Duplicates </h3><br>  The next problem is duplicates.  Anyway, when transferring data over the network, there are two options: either to lose data, or to receive duplicates.  We could not lose the message, so we simply resigned to the fact that some small percentage of events would be duplicated.  You can remove duplicates per day by creating a new table, inserting data from the old one into it, where using the window function, you deleted rows with duplicate id, deleting the old table and renaming the new one.  Since there was a view over the day tables, it was necessary not to forget about it and delete the tables for the time being renamed.  At the same time, it was also necessary to keep track of the locks, otherwise, in the event of a query that blocked the view or one of the tables, this process could take a long time. <br><br><h3>  Monitoring and maintenance </h3><br>  No Redshift request takes less than a couple of seconds.  Even if you just want to add a user or see a list of active requests, you have to wait a couple of tens of seconds.  Of course, you can suffer, and for this class of databases is acceptable, but in the end it results in a lot of wasted time. <br><br><h3>  Cost of </h3><br>  According to our calculations, deploying ClickHouse on AWS instances with exactly the same resources is exactly two times cheaper.  Of course, this is the way it should be, because using Redshift, you get a ready-made database, to which you can connect to any PostgreSQL client immediately after pressing a couple of buttons in the AWS console, and AWS does the rest for you.  However, is it worth it?  We already have the infrastructure, we seem to be able to do backups, monitoring and configuration, and we are doing this for a bunch of internal services.  Why not take up the support of ClickHouse? <br><br><h2>  Transition process </h2><br>  To begin with, we raised a small ClickHouse installation from one machine, where we began to periodically, with the help of built-in tools, load data from S3.  Thus, we were able to test our assumptions about the speed and capabilities of ClickHouse. <br><br>  After a couple of weeks of tests on a small copy of the data, it became clear that in order to replace Redshift with Clickhouse, we would have to resolve several issues: <br><br><ul><li>  on which types of instances and disks to deploy; </li><li>  Do I use replication? </li><li>  how to install, configure and run; </li><li>  how to do monitoring; </li><li>  what kind of scheme will be; </li><li>  how to deliver data from S3; </li><li>  How to rewrite all queries from standard SQL to non-standard? </li></ul><br>  <b>Instance types and disks</b> .  In the number of processors, disk and memory decided to build on the current installation of Redshift.  There were several options, including i3 instances with local NVMe disks, but decided to stop at r5.4xlarge and storage in the form of 8T ST1 EBS for each instance.  According to estimates, this should have given Redshift-comparable performance at half the cost.  At the same time, due to the use of EBS disks, we get simple backups and recovery via disk snapshots, almost like in Redshift. <br><br>  <b>Replication</b>  Since they were repelled by what is already in Redshift, they decided not to use replication.  In addition, this does not force ZooKeeper to be immediately studied, which is not yet in the infrastructure, but it‚Äôs great that now there is an opportunity to do replication on demand. <br><br>  <b>Installation</b>  This is the easiest part.  A small enough Ansible role that installs ready-made RPM packages and makes the same configuration on each host. <br><br>  <b>Monitoring</b>  To monitor all services, Prometheus is used together with Telegraf and Grafana, so they simply put Telegraf agents on ClickHouse hosts, collected a dashboard in Grafana, where the current server load was shown on the processor, memory and disks.  Through the plug-in to Grafana, current active queries on the cluster, the status of imports from S3, and other useful things were brought to this dashboard.  It turned out even better and more informative (and significantly faster) than the dashboard that the AWS console gave. <br><br>  <b>Scheme</b> .  One of our most important mistakes in Redshift was to put in separate columns only the main fields of events, and the fields that are rarely used, add <br>  in one big column properties.  On the one hand, this gave us the flexibility to change the fields at the initial stages, when there was no complete understanding of what events we were going to collect, with what properties, besides, they changed 5 times a day.  On the other hand, requests for a large column of properties took more and more time.  In ClickHouse, we decided to do it right away, so we collected all possible columns and entered the optimal type for them.  It turned out a table with about two hundred columns. <br><br>  The next task was to choose the right engine for storage and partitioning. <br>  With partitioning, they didn‚Äôt think again, but did the same as it was in Redshift, by partition for each day, but now all partitions are one table, which <br>  significantly speeds up requests and simplifies maintenance.  The storage engine took ReplacingMergeTree, as it allows you to remove duplicates from a particular partition, simply by running <a href="https://clickhouse.yandex/docs/en/query_language/misc/">OPTIMIZE ... FINAL</a> .  In addition, the daily partitioning scheme allows, in case of errors or accidents, to work only with data for a day, not a month, which is significantly faster. <br><br>  <b>Delivery of data from s3 to ClickHouse</b> .  It was one of the longest processes.  It was impossible to simply load the built-in ClickHouse tools, because the data on S3 is in JSON, each field needs to be retrieved in its own jsonpath, as we did in Redshift, and sometimes also a transformation is used: for example, the UUID of the message from the standard entry as <code>DD96C92F-3F4D-44C6-BCD3-E25EB26389E9</code> convert to bytes and put in type FixedString (16). <br><br>  I wanted to have a special service similar to what we had in Redshift as a <a href="https://docs.aws.amazon.com/en_us/redshift/latest/dg/r_COPY.html">COPY command</a> .  They did not find anything ready, so I had to do it.  How it works, you can write a separate article, but in short, this is an HTTP service, which is deployed on each host with ClickHouse.  You can contact any of them.  The request parameters specify the S3 prefix from which the files are taken, the jsonpath list for converting from JSON to the set of columns, and the set of transformations for each column.  The server to which the request came, begins to scan files on S3 and distribute the work of parsing to the other hosts.  At the same time, it is important for us that the rows that could not be imported, together with the error, were put into a separate table ClickHouse.  This greatly helps to investigate the problems and bugs in the service of receiving events and the clients that generate these events.  With the placement of the importer directly on the database hosts, we utilized those resources that are usually idle, because complex requests to them are not going around the clock.  Of course, if there are more requests, you can always bring the service of the importer to individual hosts. <br><br>  There was no big problem with importing data from external sources.  In those scripts that were, just changed the destination from Redshift to ClickHouse. <br><br>  There was an option to connect MongoDB in the form of a dictionary, and not to make daily copies.  Unfortunately, it did not fit, because the dictionary must be placed in memory, and the size of most collections in MongoDB do not allow it.  But the dictionaries are also useful to us: with their help, it is very convenient to connect GeoIP databases from MaxMind and use them in queries.  For this we use the layout ip_trie and CSV files that are provided by the service.  For example, the configuration of the geoip_asn_blocks_ipv4 dictionary looks like this: <br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dictionaries</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dictionary</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span>geoip_asn_blocks_ipv4<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">source</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">file</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">path</span></span></span><span class="hljs-tag">&gt;</span></span>GeoLite2-ASN-Blocks-IPv4.csv<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">path</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">format</span></span></span><span class="hljs-tag">&gt;</span></span>CSVWithNames<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">format</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">file</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">\</span></span></span><span class="hljs-tag">/</span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">source</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">lifetime</span></span></span><span class="hljs-tag">&gt;</span></span>300<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">lifetime</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">layout</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">ip_trie</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">layout</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">structure</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">key</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span>prefix<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span>String<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">key</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span>autonomous_system_number<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span>UInt32<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">null_value</span></span></span><span class="hljs-tag">&gt;</span></span>0<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">null_value</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span>autonomous_system_organization<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span>String<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">null_value</span></span></span><span class="hljs-tag">&gt;</span></span>?<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">null_value</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">structure</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dictionary</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dictionaries</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre><br>  This config is enough to put in <code>/etc/clickhouse-server/geoip_asn_blocks_ipv4_dictionary.xml</code> , after which you can query the dictionary to get the name of the provider at the IP address: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> dictGetString(<span class="hljs-string"><span class="hljs-string">'geoip_asn_blocks_ipv4'</span></span>, <span class="hljs-string"><span class="hljs-string">'autonomous_system_organization'</span></span>, tuple(IPv4StringToNum(<span class="hljs-string"><span class="hljs-string">'192.168.1.1'</span></span>)));</code> </pre><br>  <b>Change data schema</b> .  As mentioned above, we decided not to use replication yet, as we can now afford to become inaccessible during accidents or planned works, and a copy of the data already lies on s3 and we can transfer it to ClickHouse within a reasonable time.  If there is no replication, then ZooKeeper was not expanded, and the absence of ZooKeeper also leads to the impossibility of using the ON CLUSTER clause in DDL queries.  This problem was solved by a small python script that connects to each ClickHouse host (as long as there are only eight of them) and executes the specified SQL query. <br><br>  <b>Not full SQL support in ClickHouse</b> .  The process of transferring requests from the Redshift syntax to the ClickHouse syntax went along with the development of the importer, and it was mainly the team of analysts who worked on it.  Strangely enough, but the matter turned out to be not even in JOIN, but in window functions.  It took several days to understand how they can be done through arrays and lambda functions.  It is good that this question is quite often covered in reports about ClickHouse, of which there are a huge number, for example, <a href="https://events.yandex.ru/lib/talks/5420/">events.yandex.ru/lib/talks/5420</a> .  At that moment, the data was already written in two places at once: both in Redshift and in the new ClickHouse, so when transferring queries, we compared the results.  It was problematic to compare the speed, since we removed one large properties column, and most of the requests began to work only with the right columns, which, naturally, gave a significant increase, but those requests where the properties column did not participate, worked the same way, or slightly faster. <br><br>  The result was the following scheme: <br><br><img src="https://habrastorage.org/webt/tj/h8/ka/tjh8kagqccdbjgnswbmbm9wkloc.png"><br><br><h2>  results </h2><br>  In the bottom line, we got the following benefits: <br><br><ul><li>  One table instead of 90 </li><li>  Service requests are executed in milliseconds </li><li>  The cost has decreased by half </li><li>  Easy removal of duplicate events </li></ul><br>  There are disadvantages to which we are ready: <br><br><ul><li>  In case of an accident, you will have to repair the cluster yourself. </li><li>  Schema changes now need to be made on each host separately </li><li>  Update on the new version will have on their own </li></ul><br>  We cannot compare the speed of requests in the forehead, since the data scheme has changed significantly.  Many queries have become faster, simply because they read less data from the disk.  In an amicable way, such a change had to be made in Redshift, but it was decided to combine it with the migration to ClickHouse. <br><br>  The whole migration together with the preparation took about three months.  She went from the beginning of July to the end of September and demanded the participation of two people.  On September 27th, we turned off Redshift and since then we are working only on ClickHouse.  It turns out, already a little more than two months.  The term is small, but so far never encountered data loss or a critical bug, due to which the entire cluster would have risen.  Ahead of us are waiting for updates on new versions! </div><p>Source: <a href="https://habr.com/ru/post/433346/">https://habr.com/ru/post/433346/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../433336/index.html">Implementation of the Babylon Library</a></li>
<li><a href="../433338/index.html">Creality 3D 3D Printer Manufacturer Overview</a></li>
<li><a href="../433340/index.html">Xiaomi wireless devices in the smart home ioBroker</a></li>
<li><a href="../433342/index.html">Another simple processor on verilog</a></li>
<li><a href="../433344/index.html">Two successes of private astronautics</a></li>
<li><a href="../433348/index.html">Typed DSL in TypeScript from JSX</a></li>
<li><a href="../433350/index.html">Digital events in Moscow from December 17 to 23</a></li>
<li><a href="../433352/index.html">The digest of fresh materials from the world of the frontend for the last week ‚Ññ343 (December 10 - 16, 2018)</a></li>
<li><a href="../433354/index.html">News from the world of OpenStreetMap ‚Ññ438 (04.12.2018-10.12.2018)</a></li>
<li><a href="../433356/index.html">Malefactors learned to bypass two-factor authentication of Yahoo Mail and Gmail</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>