<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Notes with MBC Symposium: applying deep learning in brain modeling</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Attended the Stanford Symposium on the intersection of deep learning and neuroscienc√©, received a lot of pleasure. 





 I am talking about interesti...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Notes with MBC Symposium: applying deep learning in brain modeling</h1><div class="post__text post__text-html js-mediator-article"><p>  Attended the <a href="https://neuroscience.stanford.edu/events/mbc-symposium-deep-learning-fundamental-progress-brain-representations-and-semantic-learning">Stanford Symposium</a> on the intersection of deep learning and neuroscienc√©, received a lot of pleasure. </p><br><p><img src="http://visit.stanford.edu/assets/cardinal/images/seq_tour.jpg" alt="image"></p><br><p>  I am talking about interesting things - for example, a report by Dan Yamins on the use of neural networks to simulate the operation of the visual cortex. </p><a name="habracut"></a><br><p>  <em>Disclaimer: The post is written on the basis of pretty edited chat logs <a href="http://closedcircles.com/%3Finvite%3D21c629d42f7e48df8926629a18c7b575e9c33100">closedcircles.com</a> , hence the style of presentation, and clarifying questions.</em> </p><br><p>  Here is a <a href="https://talks.stanford.edu/daniel-yamins-using-behavorially-driven-computational-models-to-uncover-principles-of-coritcal-representaion/">link</a> to the full report, it's cool, but it's probably better to look after reading the post. </p><br><p>  Dan is involved in computational neuroscience, i.e.  trying computational methods to help brain research.  And there, as elsewhere, deep learning happens. </p><br><h2>  In general, the device of the visual cortex at a high level, we barely understand </h2><br><p><img src="https://habrastorage.org/files/e1f/cfb/278/e1fcfb27820f4c02880afb9c4e4c6f90.png" alt="image"></p><br><p>  When we see some kind of image, the eye triggers the activation of neurons, the activation passes through different parts of the brain, which release an increasingly higher-level representation from them. </p><br><p>  V1 is also called the primary visual cortex and it is well researched - there are neurons that drive certain filters over the image, and are activated on the lines at different angles and simple gradients. <br>  (by the way, the patterns on which neurons activate in this area are often similar to the first learned levels in CNN, which is very cool in itself) </p><br><p>  There is even success in modeling this part - they say, invent some model, see how neurons are activated on the input image, zapititit, and then this model quite predicts the activation of these neurons in new pictures. </p><br><p>  With V4 and IT (higher processing levels) this does not work. </p><br><h2>  Where does the data about biological neurons come from? </h2><br><p>  A typical experiment looks like this - a monkey is taken, electrodes are stuck in some part of the brain, which take signals from the neurons in which they fall.  Monkey show different pictures and remove the signal from the electrodes.  This is done on several hundreds of neurons - the number of neurons in the <em>entire</em> brain lobe under study is enormous, only hundreds are measured. </p><br><p>  It turns out that if we try to fit the model on the activation of biological neurons in V4 and IT, overfitting occurs - there is little data and the model does not predict anything for new pictures. </p><br><h2>  Dan companions are trying to do differently </h2><br><p>  Let's take a model and train it for some kind of <em>recognition</em> problem, so that the artificial neurons in it recognize something in these pictures. <br>  Suddenly they will predict the activation of biological neurons better? </p><br><p>  Now carefully monitor the hands. </p><br><p>  They train models (CNNs and simpler models from ordinary computer vision) to recognize objects in synthetic pictures. </p><br><p>  Pictures are as follows: </p><br><p><img src="https://habrastorage.org/files/dff/65b/faf/dff65bfaf00740d58fa0384056187269.png" alt="image"></p><br><p>  The object does not correlate with the background - maybe a plane on the background of a lake, and a head on the background of some wild forest (as I understand, in order to exclude prior learning). <br>  In total, there are 8 categories of objects in the pictures - heads, cars, airplanes, something else. </p><br><p>  And now they train models of different structure and depth to recognize the category of an object.  Among other models, there are CNNs pretrained on Imagenet, and they remove from the training dataset the categories of objects that they used in their synthetic pictures. </p><br><p>  Further, on the basis of trained CNN, they "predict" the activation of biological neurons as follows. <br>  They take some kind of training set (separate categories of objects), choose some level in CNN and neurons in some part of the brain and build a linear classifier that predicts the activation of biological neurons on the basis of artificial ones. </p><br><p>  That is, they try to approximate the activation of a biological neuron as a linear combination of activations of artificial neurons in a certain layer (after all, they cannot be exactly combined one to one, there are completely different numbers of them).  And then they check to what extent it has predictive power on the test set, where there were completely different objects. </p><br><p>  I hope to explain it turned out. </p><br><p>  <em>That is, they have as a CNN output - something like the identification of a biological neuron?</em> <br>  Not!  CNN output is a classifier of objects in pictures. <br>  CNN is training to classify images, she knows nothing about biological neurons.  Marked data for the grid is what kind of object in the picture, without knowledge of biological neurons. <br>  And then weights in CNN were recorded, and the fitthym activates biological neurons as a linear combination of artificial neuron activations. </p><br><p>  <em>and why linear combination, but not one more grid?</em> <br>  Predicting biological by artificial one wants to make it as simple as possible so that the system does not retrain and takes the main signal from neuron activations in CNN. <br>  And then on the new test images we check whether we succeeded in predicting activation in biological neurons. </p><br><h2>  So, the picture with the results! </h2><br><p><img src="https://habrastorage.org/files/638/dbd/24c/638dbd24c0d1465b8ac60b2674c36041.png" alt="image"></p><br><p>  Each point on this graph is some kind of tested model. <br>  On the X axis - how well it classifies, on the Y axis - how well it predicts biological neurons. <br>  The blue cloud is the models that either did not train at all, or were trained from scratch and there are not many very deep ones, and the red dots in the upper right are the models pretrained with imagenet and total deep learning. </p><br><p>  It can be seen that how well the model classifies strongly correlates with how well it predicts biological activations.  That is, putting a restriction that the model should also be functional - it turns out to better approximate the model of activation of biological neurons. </p><br><p>  <em>The question may be off to the side, but still.</em>  <em>But is it possible then to teach the model to classify something, using as a teacher - neurons in the brain?</em>  <em>Type showed a picture + took data from the brain and fed it to CNN?</em> <br>  This is the same thing that I talked about earlier - since you know little neurons, such a model begins to overfit and does not have predictive power. <br>  That is, training on neuron activations is not generalized, and training on the allocation of objects is a much more powerful constraint. </p><br><h2>  And now the fusion. </h2><br><p>  You can see how different levels of the neural network predict the activation of different parts of the brain: </p><br><p><img src="https://habrastorage.org/files/9b0/d63/fa9/9b0d63fa9b56437fa8146065e50c49bd.png" alt="image"></p><br><p>  It turns out that the last levels predict IT (last stage) well, but not V4 (intermediate).  And V4 is best predicted by intermediate network levels. <br>  Thus, the hierarchical representation of features in a neural network ‚Äútouches‚Äù not only at the end, but also in the middle of processing, which again suggests that there is some kind of commonality of what is happening there and there. </p><br><p>  <em>That is, they have a neural network looks about the same as biological neurons in the brain?</em> <br>  Rather, there is something similar in which stages the recognition process goes through. <br>  To say that the "architecture" is the same, of course, cannot be (of course, this cannot be considered as proof at all, etc., etc.) </p><br><h2>  The next stage - well, ok, suppose we got the opportunity to model the unknown as a working brain by some other incomprehensible like a working box.  What is the joy in it? </h2><br><p>  Further work - how can this be used to understand something new about the work of the brain. </p><br><p>  I will tell about one example, there are two more in the speech itself. <br>  Let's try to pull out not only the classification, but also some other signals from the image - the angle of inclination, size, position, etc. </p><br><p><img src="https://habrastorage.org/files/932/e6a/a57/932e6aa57fef4c86a6d4f33cd7a8c9e7.png" alt="image"></p><br><p>  It would seem that these are more "low-level" features than the class of an object, and you can expect them to be defined at lower levels of recognition in the brain - let's check it on the model. </p><br><p>  It turns out figs there! <br>  Even such "low-level" features correlate better with high-level activations, rather than low-level activations.  Then they conducted additional experiments on the living brain and saw the same thing - those lines and corners that we used to see in the patterns of the first layers are not related to the "lines" of the orientation of high-level objects. <br>  Information about the position and the relative position of objects is quite a high level. </p><br><p>  This already confirms the existing theory that the last stage (IT) - works with a high-level model, where there are objects, their location, mutual relations, etc. and so on, and converts them into something the brain needs. <br>  (other examples, if anyone is interested, about testing a hypothesis about a dedicated place in the brain for face recognition through training a virtual brain that has never seen people in their lives, and about sound recognition) </p><br><p>  <em>(Continuing amateurish questions) Did they use a model with the same number of layers in their CNN?</em>  <em>Well, that is, and if you reduce the number of layers, the similarity effects disappear?</em> <br>  No, the number of "layers" in the brain is a complicated thing - there are both end-to-end connections and feedback.  The number of layers in CNN is fundamentally less than in the brain. </p><br><p>  <em>How then is the conversation about IT and V4?</em> <br>  Well, in the brain, IT and V4 are many, many levels, and there are few levels in the neural network.  Biological neurons from IT and V4 are those into which the electrode fell.  From what "biological leuer" inside them it came out - from that it came out. </p><br><p> The following is also interesting.  If we believe Ramachandran, visual cortex is not just a feed-forward network, all layers communicate with all layers <em>in both directions</em> .  There is even <a href="https://vk.com/kolyakukushkin%3Fw%3Dwall5695154_23150">an example</a> when the visual system can be disrupted through various kinds of optical illusions. </p><br><p>  <em>That is, no one guarantees that the activation of biological neurons is somehow related to the recognition process itself, and not to the fact that the monkey‚Äôs foot begins to itch and the neurons are already reacting to the fact that it‚Äôs foot itches?</em> <br>  No, these are neurons from a region that is known to be associated with visual cortex (this part about the brain, we somehow know, it‚Äôs impossible to scratch it, it is further down the stack). </p><br><p>  Summing up, the direct direction of work is rather about using new models to study the work of the brain, but there are some indirect hints that somehow this works too well, perhaps there are general mechanisms. <br>  Hopefully converge in singularity! </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/282277/">https://habr.com/ru/post/282277/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../282261/index.html">Tinkoff Bank. Hackathon "Era Bots"</a></li>
<li><a href="../282263/index.html">Persistent OS: nothing is blocked</a></li>
<li><a href="../282265/index.html">Creating super-long flat panoramic images from video</a></li>
<li><a href="../282267/index.html">A brief history of the World Wide Web according to web developers</a></li>
<li><a href="../282275/index.html">One way to find unshielded characters with new JavaScript tools</a></li>
<li><a href="../282281/index.html">How to use JSON restrictions when working with PostgreSQL</a></li>
<li><a href="../282283/index.html">Student projects within the Google Summer of Code</a></li>
<li><a href="../282287/index.html">Separate host and user configurations in 3CX Phone System v14</a></li>
<li><a href="../282293/index.html">Use CommonJS modules in Rails with Browserify</a></li>
<li><a href="../282297/index.html">AdminVK - monitoring your own Vkontakte groups for new events using push notifications</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>