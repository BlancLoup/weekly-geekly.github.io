<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Running R functions on multiple machines</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="As we showed in ‚ÄúA little introduction to parallel programming on R‚Äù , one of the advantages of R is the ease with which you can take advantage of par...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Running R functions on multiple machines</h1><div class="post__text post__text-html js-mediator-article">  As we showed in <a href="https://habrahabr.ru/company/infopulse/blog/307708/">‚ÄúA little introduction to parallel programming on R‚Äù</a> , one of the advantages of R is the ease with which you can take advantage of parallel programming to speed up computations.  In this article we will explain how to go from running functions on multiple processors or cores to running on multiple machines (with the goal of even greater scaling and acceleration). <br><br>  R itself is not intended for parallel computing.  It does not have many parallel designs available to the user.  Fortunately, data processing tasks, for which we most often use R, are very well suited for parallel programming, and there are a number of excellent libraries that use it.  Here are three main ways to take advantage of the parallelization provided by libraries: <br><br><ul><li> <b>Connect more powerful parallel libraries, for example, Intel BLAS</b> (available under Linux, OS X and Windows as part of the <a href="https://mran.microsoft.com/download/">Microsoft R Open</a> distribution).  This will replace the already used libraries with their parallel versions, thereby obtaining acceleration (on appropriate tasks, for example, related to linear algebra in <code>lm()/glm()</code> ). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    </li><li>  <b>Bring the processing of simulation tasks from R to an external library for parallelization</b> .  This is the strategy that the following systems use: <a href="http://www.revolutionanalytics.com/sites/default/files/data-step-white-paper.pdf">rx methods from RevoScaleR (now Microsoft Open R)</a> , <a href="http://www.h2o.ai/">h2o methods from h2o.ai</a> , <a href="https://github.com/RevolutionAnalytics/RHadoop">RHadoop</a> . <br><br></li><li>  <b>Use the <code>parallel</code> utility in R to run functions on other instances of R.</b>  This strategy is from the <a href="https://habrahabr.ru/company/infopulse/blog/307708/">‚ÄúSmall introduction to parallel programming on R‚Äù</a> and a number of libraries based on <code>parallel</code> .  In fact, this is an implementation of a remote procedure call via a socket or network. </li></ul><br>  Let us consider the third approach in more detail. <br><a name="habracut"></a><br>  In fact, the third approach is a very finely detailed remote procedure call.  It depends on the transfer of code and data copies to remote processes and the subsequent return of results.  This is bad for very small tasks, but great for an acceptable number of medium or large ones.  This strategy is used in the R <code>parallel</code> library and in the <a href="https://www.python.org/">Python</a> <code>multiprocessing</code> library (although with <code>multiprocessing</code> for Python, you may need a <a href="https://wiki.python.org/moin/ParallelProcessing">number of additional libraries</a> to move from one machine to cluster computing). <br><br>  This method may seem less effective and less complex than distributed memory methods, but relying on the transfer of an object, it is very easy to spread the technique from one machine to several (‚Äúcluster computing‚Äù).  This is what we will do with the R code in this article (moving from one machine to a cluster will lead to many systems / network / security problems, and will have to be managed). <br><br>  You will need all the R code from the previous article.  It also assumes that you can configure <a href="https://en.wikipedia.org/wiki/Secure_Shell">ssh</a> , or you have someone who can assist with the configuration.  Instead of starting a parallel cluster with the ‚Äú <code>parallelCluster &lt;- parallel::makeCluster(parallel::detectCores())</code> ‚Äù <code>parallelCluster &lt;- parallel::makeCluster(parallel::detectCores())</code> , do the following. <br><br>  Collect a list of addresses of machines to which you can apply <code><a href="https://en.wikipedia.org/wiki/Secure_Shell"></a> ssh</code>  <code><a href="https://en.wikipedia.org/wiki/Secure_Shell"></a> ssh</code>  This is the hard part, depends on the operating system and may require assistance if you have not done this before.  In this example, I use IPv4 addresses, and for <a href="https://github.com/JohnMount/ec2R">Amazon EC2</a> , host names. <br><br>  In my case, the list is: <br><br><ul><li>  My car (main): ‚Äú192.168.1.235‚Äù, user ‚Äújohnmount‚Äù </li><li>  Other machine Win-Vector LLC: ‚Äú192.168.1.70‚Äù, user ‚Äújohnmount‚Äù </li></ul><br>  Please note that we do not collect passwords, assuming that the correct "authorized_keys" and key pairs are installed in the ".ssh" configurations of all these machines.  We will call the machine with which the calculation will be carried out as a whole, ‚Äúprimary‚Äù. <br><br>  You should definitely try all these addresses with ‚Äússh‚Äù in the terminal before using them in R. Also, the address of the machine selected as ‚Äúprimary‚Äù must be reachable from the working machines (i.e. you cannot use ‚Äúlocalhost‚Äù or select an unreachable machine "Primary").  Try manually ssh between the primary and other machines, and in the opposite direction, after which the settings can be used in R. <br><br>  When the system settings are over, the part on R will look like this.  Start your cluster: <br><br><pre> <code class="python hljs">primary &lt;- <span class="hljs-string"><span class="hljs-string">'192.168.1.235'</span></span> machineAddresses &lt;- list( list(host=primary,user=<span class="hljs-string"><span class="hljs-string">'johnmount'</span></span>, ncore=<span class="hljs-number"><span class="hljs-number">4</span></span>), list(host=<span class="hljs-string"><span class="hljs-string">'192.168.1.70'</span></span>,user=<span class="hljs-string"><span class="hljs-string">'johnmount'</span></span>, ncore=<span class="hljs-number"><span class="hljs-number">4</span></span>) ) spec &lt;- lapply(machineAddresses, function(machine) { rep(list(list(host=machine$host, user=machine$user)), machine$ncore) }) spec &lt;- unlist(spec,recursive=FALSE) parallelCluster &lt;- parallel::makeCluster(type=<span class="hljs-string"><span class="hljs-string">'PSOCK'</span></span>, master=primary, spec=spec) print(parallelCluster)</code> </pre> <br><pre> <code class="diff hljs">## socket cluster with 8 nodes on hosts ## '192.168.1.235', '192.168.1.70'</code> </pre><br>  That's all.  Now you can run your functions on multiple cores of multiple machines.  For the right tasks, acceleration will be <i>essential</i> .  It always makes sense to act in steps: first write a simple <a href="http://www.win-vector.com/blog/2008/02/hello-world-an-instance-rhetoric-in-computer-science/">‚Äúhello world‚Äù</a> on your cluster, then make sure that the smaller version of your calculations works locally, and only after that transfer the work to the cluster. <br><br>  There is another way to work with clusters in R. First we need a version of R with pre-installed Rmpi ‚Äã‚Äãand snow packages.  For this purpose, I propose a build R HSPCC supported by Casper Daniel Hansen.  <a href="http://www.biostat.jhsph.edu/~khansen/hpscc.html">Here are</a> the installation instructions. <br><br>  We will consider two more simple methods of parallel data processing in R. The first, using the multicore package, is limited to processors on a single node.  And although this may seem to be a serious disadvantage, in fact it can be a strong advantage, since the interaction between processes will be several orders of magnitude faster.  The second option is to use the snow package, which allows using MPI (data transfer interfaces between nodes) for computing the subcluster. <br><br><h4>  Using multiple cores inside a node </h4><br>  The multicore package is very effective in speeding up simple computations by using more than one processor core at a time.  It is very easy to use and quick to implement. <br><br>  Start the process on a cluster node with multiple processors by typing the following command: <br><br><pre> <code class="bash hljs">qrsh -l mcmc -pe <span class="hljs-built_in"><span class="hljs-built_in">local</span></span> 10-12</code> </pre> <br>  Notice, here we are requesting 10-12 processors on one node in the mcmc queue.  The number of available cores can be viewed in the NSLOTS environment variable available in R with the following command: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">as</span></span>.integer(Sys.getenv(<span class="hljs-string"><span class="hljs-string">"NSLOTS"</span></span>))</code> </pre> <br>  The next step is to run R and load the multicore library.  Finally, you can continue using the mclapply command instead of lapply in R (passing the number of kernels used as an argument to mc.cores). <br><br><h4>  Using snow between multiple nodes </h4><br>  Below is a simple example of how to raise an MPI cluster in R and how to use the cluster for simple calculations. <br><br>  First of all, I would recommend setting access without a password to the cluster nodes before continuing.  This will save some time (however, it is not absolutely necessary). <br><br>  To run an MPI cluster with 12 nodes (cores), you need to type this: <br><br><pre> <code class="bash hljs">qrsh -V -l cegs -pe orte 12 /opt/openmpi/bin/mpirun -np 12 ~hcorrada/bioconductor/Rmpi/bstRMPISNOW</code> </pre> <br>  This command should run 12 instances of R, one of which will be primary.  Notice that the starting process was in the cegs queue.  Then you can use the nodes installed with mpirun by typing in R such: <br><br><pre> <code class="python hljs">cl &lt;- getMPIcluster()</code> </pre> <br>  Then you can view and use the snow commands you need.  For example, <br><br><pre> <code class="python hljs">clusterCall(cl, function() Sys.info()[c(<span class="hljs-string"><span class="hljs-string">"nodename"</span></span>,<span class="hljs-string"><span class="hljs-string">"machine"</span></span>)])</code> </pre> <br>  will display a list of nodes in your cluster (more precisely, 11 running work nodes).  <a href="http://www.sfu.ca/~sblay/R/snow.html">Here</a> is a list of available commands. <br><br>  When calculations on the cluster are complete, close the nodes with the following command: <br><br><pre> <code class="python hljs">stopCluster(cl)</code> </pre> <br>  Warning: make sure that you have not canceled the R processes by pressing the Ctrl + C key combination.  This can cause problems with snow. <br><br><h4>  Example: comparison of sequential and parallel processing </h4><br><pre> <code class="python hljs">&gt; f.long&lt;-function(n) { + xx&lt;-rnorm(n) + log(abs(xx))+xx^<span class="hljs-number"><span class="hljs-number">2</span></span> + } <span class="hljs-comment"><span class="hljs-comment"># multicore ############ &gt; system.time(mclapply(rep(5E6,11),f.long,mc.cores=11))</span></span></code> </pre><br><pre> <code class="diff hljs"> user system elapsed 26.271 3.514 5.516</code> </pre><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># snow  MPI ############ &gt; system.time(sapply(rep(5E6,11),f.long))</span></span></code> </pre><br><pre> <code class="diff hljs"> user system elapsed 17.975 1.325 19.303</code> </pre><br><pre> <code class="python hljs">&gt; system.time(parSapply(cl,rep(<span class="hljs-number"><span class="hljs-number">5E6</span></span>,<span class="hljs-number"><span class="hljs-number">11</span></span>),f.long))</code> </pre><br><pre> <code class="diff hljs"> user system elapsed 4.224 4.113 8.338</code> </pre><br>  Please note that parallel processing with snow gives an improvement of more than 50% of the computation time.  Although it can be assumed that the improvement should be 10/11 = 91%, it is important to remember that the processors are not necessarily located on the same node, and the interaction between the nodes can be rather slow.  This interaction can be so slow that a procedure in multicore without it can give a 40% improvement in computation time. <br><br>  Accordingly, it makes sense to remember that the advantages in computation time from using more than one cluster node can largely be leveled by the time it takes for interaction between the nodes.  Of course, it depends on the performed calculations and the way they are implemented. </div><p>Source: <a href="https://habr.com/ru/post/309052/">https://habr.com/ru/post/309052/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../309040/index.html">World is not perfect</a></li>
<li><a href="../309042/index.html">CSS Containment</a></li>
<li><a href="../309044/index.html">How to get an internship</a></li>
<li><a href="../309046/index.html">Dash is a blockchain that cannot be stopped.</a></li>
<li><a href="../309048/index.html">We invite you to the second hakaton Neurohack</a></li>
<li><a href="../309054/index.html">Security Week 35: intercepting keyboard via WiFi, attacking ATMs with an EMV chip, new IoT botnet</a></li>
<li><a href="../309056/index.html">The secure notes service OneLogin Secure Notes has been compromised</a></li>
<li><a href="../309058/index.html">OK Google, what about good interfaces?</a></li>
<li><a href="../309060/index.html">The benefits of standardization</a></li>
<li><a href="../309062/index.html">Android to the State Duma will bring or mobilization of civic consciousness</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>