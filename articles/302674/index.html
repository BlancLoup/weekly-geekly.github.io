<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Machine Learning Boot Camp - how it was and how it will be</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="On June 13, ML Boot Camp started - a machine learning contest from the Mail.Ru Group. In this regard, we want to share with you our impressions of its...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Machine Learning Boot Camp - how it was and how it will be</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/1d8/9d9/c58/1d89d9c58b164c6ea12b55a44388c848.png"><br><br>  On June 13, <a href="http://mlbootcamp.ru/">ML Boot Camp</a> started - a machine learning contest from the Mail.Ru Group.  In this regard, we want to share with you our impressions of its previous launch, the success stories of the winners and tell you what's new for the participants this year. <br><a name="habracut"></a><br><h1>  How is the contest organized? </h1><br>  At the start of the championship, participants receive a condition of the problem - a verbal description of what is contained in the data serving as a training sample.  In addition to the condition, the learning sample itself becomes available, it consists of labeled examples ‚Äî vectors of descriptions of each object with a known answer.  If there are a finite number of possible answers, they can be interpreted as the number of the class to which the object belongs. <br><br>  The participants, using the methods of machine learning known to them, train the computer and use the trained system on new objects (test sample), trying to determine the answer for them.  The test sample is randomly divided into two parts: rating and final.  The overall result on the rating data is calculated by our system and published immediately, but the winner is the one who gets the best results on the final data.  They remain hidden for participants until the very end of the competition. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The assignment is given one month.  Experienced participants can improve their algorithms throughout the entire contest by updating the result.  And those who are not so deeply familiar with this area will have enough time to figure everything out and write their decision.  For beginners, an extensive introductory article is also offered, which will indicate in which direction to dig, and schematically describe a possible solution for the championship problem. <br><br><h1>  ML Boot Camp 2015 </h1><br>  In 2015, we held an open championship, which many did not know about.  The participants were mostly students of our educational projects - Technosphere and Technopark. <br><br>  Here is the condition of the task that participants received when opening a contest: <br><br>  <i>‚ÄúYou are faced with the task of classification: based on the well-known distribution of classes of training elements, also distribute test ones.</i>  <i>All data provided for this task is divided into two parts: training (x_train.csv and y_train.csv) and test (x_test.csv).</i>  <i>Each line of the x_train.csv and x_test.csv files is a description of some objects in the form of sets of binary values ‚Äã‚Äã(features), listed by comma.</i>  <i>All objects are divided into three categories (classes).</i>  <i>For objects from the training set, this partition is known and is given in the y_train.csv file.</i>  <i>The answer for this problem is a text file, each line of which corresponds to a line in the x_test.csv file and contains the class number (0, 1 or 2).</i> <br><br>  <i>Sample data:</i> <br><br><table><tbody><tr><td>  x_train.csv </td><td>  y_train.csv </td><td>  x_test.csv </td></tr><tr><td>  1,1,1,1,0,0,0,1,0,0,0,1,0,0,1,0,0,0,1,0,1,1,1,1,1,1,1, 0,1,1,0,1 </td><td>  one </td><td>  1,1,0,1,1,0,1,0,0,1,1,0,1,0,0,1,1,0,0,1,1,1,1,1,0,0, 1,1,1,1,0 </td></tr><tr><td>  1,0,1,0,0,0,1,1,0,1,0,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1, 0,0,1,0,0 </td><td>  2 </td><td>  0,1,1,0,1,0,1,0,1,0,0,0,1,0,1,0,0,1,1,0,1,0,0,1,0, 0,1,0,0,0 </td></tr><tr><td>  0,1,1,0,0,0,1,1,1,1,0,1,0,1,0,1,0,0,1,0,1,1,0,0,1,1, 1,1,0,1,0 </td><td>  one </td><td>  0,0,1,0,0,0,0,0,0,0,1,1,0,1,0,0,0,1,1,0,0,1,1,1,0,0, 1,1,1,1,1 </td></tr><tr><td>  1,0,1,1,0,1,0,1,1,0,0,0,1,0,1,1,0,1,1,1,0,0,1,0,0,1,100 0,1,1,1,0 </td><td>  0 </td><td>  1,0,1,0,1,1,0,1,0,0,1,0,0,1,0,1,0,1,1,0,0,0,1,1,0,0, 1,0,0,1,0 </td></tr></tbody></table><br>  <i>The classification accuracy will be taken as the quality criterion for solving the problem, i.e. the proportion of correctly classified objects.</i>  <i>The test sample is randomly divided into two parts in a 40/60 ratio.</i>  <i>The result of the first 40% will determine the position of participants in the rating table throughout the competition.</i>  <i>The result for the remaining 60% will be known after the end of the competition, and it is he who will determine the final placement of the participants. ‚Äù</i> <br><br>  The contestants had no idea how the faceless chains of zeros and ones were obtained, but despite this, they managed to achieve impressive results.  See for yourself in the table TOP-10 contestants. <br><br><table><tbody><tr><td>  <i>No</i> </td><td>  <i>Participant</i> </td><td>  <i>Preliminary score</i> </td><td>  <i>Final score</i> </td></tr><tr><td>  one </td><td>  Pavel Shvechikov </td><td>  0,6984126984 </td><td>  0.6785714286 </td></tr><tr><td>  2 </td><td>  Anton Gordienko </td><td>  0.6904761905 </td><td>  0.6547619048 </td></tr><tr><td>  3 </td><td>  Anastasia Videneeva </td><td>  0,6984126984 </td><td>  0.630952381 </td></tr><tr><td>  four </td><td>  Artem Kondyukov </td><td>  0.6825396825 </td><td>  0.619047619 </td></tr><tr><td>  five </td><td>  Alexander Lutsenko </td><td>  0.7301587302 </td><td>  0.619047619 </td></tr><tr><td>  6 </td><td>  Konstantin Sozykin </td><td>  0.6587301587 </td><td>  0.619047619 </td></tr><tr><td>  7 </td><td>  Vitaly Malygin </td><td>  0.4126984127 </td><td>  0.5833333333 </td></tr><tr><td>  eight </td><td>  Alexey Nesmelov </td><td>  0.4841269841 </td><td>  0.5714285714 </td></tr><tr><td>  9 </td><td>  Vladimir Vinnitsky </td><td>  0.380952381 </td><td>  0.5714285714 </td></tr><tr><td>  ten </td><td>  Viktor Shevtsov </td><td>  0.4047619048 </td><td>  0.5714285714 </td></tr></tbody></table><br>  Here, the <b>score</b> is the percentage of sequences for which the final program correctly identified the class. <br><br>  In fact, we got these chains as follows. <br><br>  <b>Class 0</b> contains sequences generated by humans.  We asked each of the 140 people who agreed to help us, to write a sequence of zeros and ones so that it looked as similar to random as possible.  However, they could not use the computer, tables and other materials.  Everyone had to simply imagine that he was throwing a coin.  If an eagle falls in his mind, then write down 1, if tails, write down 0. Of course, this coin and other devices were also forbidden to use. <br><br>  <b>Class 1</b> contains computer-generated sequences using a pseudo-random number sensor.  The probability of a zero, as well as the probability of a unit, is 50%, while there is no dependence between the different elements of the sequence. <br><br>  <b>Class 2</b> contains sequences also generated by a computer, but the sequential values ‚Äã‚Äãare different with a probability of 70%. <br><br>  Just think: even having no idea how these chains were obtained, the participants, with the help of their iron friend (computer), with a probability of almost 70% (!) Correctly determine whether this sequence was written by a person or a computer, randomly or with an algorithm .  This seems to be a sufficient reason to evaluate the power of machine learning methods, because even a 50% estimate would be good, considering that a priori chances of guessing are only 33, (3)%. <br><br>  We asked the winners to tell how they achieved such a result. <br><br><h1>  3rd place </h1><br>  <b>Anastasia Videneeva</b> <br><br>  ‚ÄúI solved the problem as follows.  First, I examined the data and found out that in the training sample of objects from different classes approximately equally, the number of zeros and ones is approximately the same in objects from each class, but in class 2 more often than in others, the next character differs from the previous one (in classes 0 and 1 there were on average about 14 such ‚Äúswitchings‚Äù in each object, in class 2 - 17).  I added the number of switchings as a new feature and trained randomForest on the resulting features.  The result was not high (without the selection of parameters, my speed was 0.438, as I recall).  Judging by the speeds of the other participants at that time, they, most likely, also used the original signs, but more carefully selected the parameters of the algorithms or used combinations of algorithms. <br><br>  I thought about a fundamentally different idea, which in this problem can "shoot".  I decided to treat the data as strings of length 30, and use <i>n-</i> grams as attributes, because they made it possible to reflect the fact that in class 2 often there is a symbol change, and other patterns in the data.  On the new signs, all algorithms made it possible to get a significantly higher result (the same randomForest without a selection of parameters received a speed of 0.706).  Then I made the selection of features using ExtraTreesClassifier (the sample is very small, I didn‚Äôt want to retrain) and classification using gradient boosting, svm and randomForest.  The parameters of all algorithms and the cut-off threshold were chosen by importance using leave-one-out cross-validation.  I did not get a significant difference between the algorithms, so in the final premise I used the more familiar randomForest.  It would be interesting to see what results can be achieved with the help of neural networks, but there was not enough time to figure them out. ‚Äù <br><br><h1>  2nd place </h1><br>  <b>Anton Gordienko</b> <br><br>  ‚ÄúBefore participating in the contest, all my machine learning experience was reduced to taking the Andrew Ng course on Coursera, and a couple of times I tried to do simple classifiers using logistic regression. <br><br>  First try: I tried to train XGBoost.  For the night I set to go through the parameters.  I got a good local quality (about 50%), but after sending the solution to the site - only 39%.  Surely I retrained, because when I sent XGBoost with default parameters I got 41%. <br><br>  Second attempt: tried to analyze the signs.  I found out that each column separately contains absolutely no information: the number of zeros and ones is the same for each class.  I decided to look for dependencies for different sets of columns.  I went through all sorts of combinations of three columns (I also tried combinations of two and four columns) and looked at how often each combination occurs in each class.  Now, many combinations in one of the classes were 1.5‚Äì2 times more often (or less) than in the other two.  But the transition to the new binary signs of ‚Äúthe presence in the sequence of each combination‚Äù resulted in nothing.  The site returned the same 41%.  The reason, most likely, is in too many signs (4060 signs at the size of the training base of 210 elements). <br><br>  Third attempt: I decided to consider as signs the number of subsequences of each type of length 5 (I also tried other lengths, but the quality turned out to be a little worse).  I trained with randomForest on random parameters and got 63% on the site.  Looking at the importance of the signs, I saw that the feature ‚Äúpresence of a subsequence 10101‚Äù stands out well.  Added another sign: "the total number of alternations in the sequence."  Slightly selected the max_features parameter and got the final 69%. ‚Äù <br><br><h1>  1st place </h1><br>  <b>Pavel Shvechikov</b> <br><br>  ‚ÄúIt was my first machine learning competition, so I tried to try everything that came to my mind and could improve the quality of the prediction.  At first, I tried to visualize the data for a long time, tried to look at the data, spread it out on the PCA, but this did not give any good results.  The data was all in a heap, and something simple, immediately noticeable to the eye was not visible.  Then I wondered if the signs are somehow related to each other, and I built a diagram of correlations between the signs in different classes.  So the diagram looked for class 0: <br><br><img src="https://habrastorage.org/files/1f7/386/2fa/1f73862fa8474f74b0847b72939ffce5.png"><br><br>  I also built a correlation diagram for the first class: <br><br><img src="https://habrastorage.org/files/76b/a6e/68d/76ba6e68d288404d8fae955588956da2.png"><br><br>  At this stage I stopped for a long time and tried to understand what the true difference was, but when I built a similar diagram for the second class, everything became somewhat clearer: <br><br><img src="https://habrastorage.org/files/08b/765/a2e/08b765a2e5404c71a7dfb4c30ff209e7.png"><br><br>  Obviously, the signs were related to each other, and most of all it was noticeable in the second grade.  After that, I remembered how one of my friends said that on kaggle, in most cases, ensembles from models were beaten, I completely forgot about the dependencies found and hit the ensemble building. <br><br>  Here I gave myself a will.  :) I tried simple voting, weighted voting, stacking, blending, each of those listed with a different number of levels and different models.  Here I tried many different interesting things, including teaching higher-level models on the predictions of the previous level with the addition of the initial binary features, I tried different types of cross-validation.  It must be admitted that the leave-one-out method of cross-validation became a real find for second-tier models due to the small amount of data, which somewhat raised the quality of the ensemble, which I carefully combed, drawing on the methods and their justification from articles and discussions. <br><br>  At some point, I quite accidentally gave some good ideas, in particular, about the fact that you can use for teaching models of the next level not predictions of previous ones, but probabilities of classes that are obtained on models of the first level, as well as a great idea that you can look at signs are like strings.  Coupled with the correlation diagrams that I received at the very beginning of the contest, it opened my eyes to what else can be done to improve the quality.  In particular, I tried to present binary signs as a random walk, tried to use the symmetry of signs, the number of shifts 0-1, the number of continuous 0 or 1, their maximum / median / average length, and also I tried to present the original signs in the form of text documents consisting from the words 0, 1, added new signs indicating the number of the sign on which a continuous sequence of identical values ‚Äã‚Äãis interrupted.  I was thinking of going further and adding TF-IDF signs for different combinations of words, but time was running out, and I decided to stop at what had already happened. <br><br>  The resulting model looked like this: <br><br><img src="https://habrastorage.org/files/64b/89c/47c/64b89c47cdec4bb39279f09d7a6d0bd7.png"><br><br>  For 90% of the data (with the added features described above) I trained seven models (presented below), for the remaining 10% of the data I received predictions of the class probability from the first level models and on these new signs I taught second level models, including svm , randomForest and a small neural network.  I pledged second-tier models with the help of majority voting. ‚Äù <br><br>  As you can see, two of the three finalists admitted that they had little experience in solving machine learning problems before the contest.  This means that the competition dates and open resources provide you with an opportunity not only to sort out and propose your solution, but also to show a decent result - there would be only desire and perseverance. <br><br>  <b>***</b> <br><br>  This year we are holding the competition again!  At this time, a new task was prepared for the participants, and the winners are expected not only with respect and respect, but also with useful prizes (iPod shuffle and 500 GB external hard drives), as well as the opportunity to join the Mail.Ru Group team!  The scope of application of machine learning algorithms is constantly expanding, now they are actively used in advertising services, antispam and social networks.  We are <a href="https://corp.mail.ru/ru/jobs/vacancy/%3Fsearch%3D%25D0%25B8%25D1%2581%25D1%2581%25D0%25BB%25D0%25B5%25D0%25B4%25D0%25BE%25D0%25B2%25D0%25B0%25D1%2582%25D0%25B5%25D0%25BB%25D1%258C">interested</a> in machine learning specialists and we want people to know what it is and how to use it.  Competitions like ML Boot Camp are a great opportunity not only to read an abstractly incomprehensible theory, but to immediately run it on tasks that are close to real ones.  This is a reason to finally engage in machine learning for those who have always been interested in this, but hesitated in indecision, and the chance for experienced users to show their high level.  <a href="http://mlbootcamp.ru/main/">Join us at ML Boot Camp!</a> <br><br>  The ML Boot Camp Championship is one of the Mail.Ru Group initiatives aimed at the development of the Russian IT industry and united by the <a href="https://it.mail.ru/">IT.Mail.Ru</a> resource.  The IT.Mail.Ru platform is designed for those who are keen on IT and are seeking to develop professionally in this area.  Also, the project combines the championships of the Russian Ai Cup, Russian Code Cup and Russian Developers Cup, educational projects Technopark in MSTU.  Bauman, Technosphere at Moscow State University.  MV Lomonosov and Tehnotrek MIPT.  In addition, using IT.Mail.Ru, you can use tests to test your knowledge of popular programming languages, learn important news from the IT world, visit or watch broadcasts from specialized events and lectures on IT topics. </div><p>Source: <a href="https://habr.com/ru/post/302674/">https://habr.com/ru/post/302674/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../302664/index.html">Angler EK exploit kit specializes in circumventing EMET security mechanisms</a></li>
<li><a href="../302666/index.html">Google Developers Agency Pro: certification for the best developers of Android applications</a></li>
<li><a href="../302668/index.html">Interview with Baruch Sadogursky: the perfect stack of technologies for enterprise development</a></li>
<li><a href="../302670/index.html">Data Warehouse Testing</a></li>
<li><a href="../302672/index.html">Analysis of the tasks of the third qualifying round of the RCC 2016</a></li>
<li><a href="../302676/index.html">Docker meetup in badoo</a></li>
<li><a href="../302678/index.html">RegionSoft CRM: a business that works for business</a></li>
<li><a href="../302680/index.html">Combining 3CX Phone System with Asterisk (FreePBX)</a></li>
<li><a href="../302682/index.html">iOS Localization: XLIFF</a></li>
<li><a href="../302684/index.html">The igrodel approach to creating modern web applications</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>