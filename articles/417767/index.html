<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Text analysis using convolutional neural networks</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Imagine that you have a paragraph of text. Is it possible to understand what kind of emotion this text carries: joy, sadness, anger? Can. Simplify you...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Text analysis using convolutional neural networks</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/2u/l3/lw/2ul3lwsbyobovjnol2g_cbvrghi.gif"><br><br>  Imagine that you have a paragraph of text.  Is it possible to understand what kind of emotion this text carries: joy, sadness, anger?  Can.  Simplify your task and we will classify the emotion as positive or negative, without clarification.  There are many ways to solve such a problem, and one of them is <b>convolutional neural networks</b> (Convolutional Neural Networks).  CNN was originally developed for image processing, but they successfully cope with the solution of problems in the field of automatic text processing.  I will introduce you to the binary analysis of the tonality of Russian-language texts using the convolutional neural network, for which the vector representations of words were formed on the basis of the trained <b>Word2Vec</b> model. <br><br>  The article is an overview, I focused on the practical component.  And at once I want to warn you that decisions made at each stage may not be optimal.  Before reading, I recommend reading the <a href="https://habr.com/company/ods/blog/353060/">introductory article</a> on the use of CNN in problems of processing natural languages, as well as to read the <a href="https://habr.com/company/ods/blog/329410/">material</a> about the methods of vector representation of words. <br><a name="habracut"></a><br><h2>  Architecture </h2><br>  The considered CNN architecture is based on approaches [1] and [2].  The approach [1], which uses the ensemble of convolutional and recurrent networks, at the largest annual competition in computational linguistics SemEval-2017 won first places [3] in five nominations in the task for analyzing the tonality of <a href="http://alt.qcri.org/semeval2017/task4/">Task 4</a> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/20a/058/4aa/20a0584aa0d0a5a6c8108af970c896fa.png"><br>  <i>Figure 1. CNN architecture [2].</i> <br><br>  The input data of CNN (Fig. 1) is a matrix with a fixed height <i>n</i> , where each row is a vector map of the token to the attribute space of dimension <i>k</i> .  Distributive semantics tools such as Word2Vec, Glove, FastText, etc. are often used to form feature spaces. <br><br>  At the first stage, the input matrix is ‚Äã‚Äãprocessed by convolution layers.  As a rule, filters have a fixed width equal to the dimension of the attribute space, and for filtering the filters only one parameter is adjusted - height <i>h</i> .  It turns out that <i>h</i> is the height of adjacent rows, considered by the filter together.  Accordingly, the dimension of the output matrix of attributes for each filter varies depending on the height of this filter <i>h</i> and the height of the original matrix <i>n</i> . <br><br>  Further, the feature map obtained at the output of each filter is processed by a sub-sampling layer with a specific compression function (in the image - 1-max pooling), i.e.  reduces the dimension of the generated feature map.  This extracts the most important information for each convolution, regardless of its position in the text.  In other words, for the used vector display, the combination of convolution layers and sub-sampling layers allows to extract the most significant <i>n-</i> programs from the text. <br><br>  After this, feature maps calculated at the output of each layer of sub-sampling are combined into one common feature vector.  It is fed to the input of the hidden fully meshed layer, and then it goes to the output layer of the neural network, where the total marks of the classes are calculated. <br><br><h2>  Data for training </h2><br>  For learning, I chose the <a href="http://study.mokoron.com/">corpus of short texts by Yulia Rubtsova</a> , formed on the basis of Russian-language messages from Twitter [4].  It contains 114,991 positive, 111,923 negative tweets, as well as a database of untagged tweets with a volume of 17,639,674 posts. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-comment"><span class="hljs-comment">#   n = ['id', 'date', 'name', 'text', 'typr', 'rep', 'rtw', 'faw', 'stcount', 'foll', 'frien', 'listcount'] data_positive = pd.read_csv('data/positive.csv', sep=';', error_bad_lines=False, names=n, usecols=['text']) data_negative = pd.read_csv('data/negative.csv', sep=';', error_bad_lines=False, names=n, usecols=['text']) #    sample_size = min(data_positive.shape[0], data_negative.shape[0]) raw_data = np.concatenate((data_positive['text'].values[:sample_size], data_negative['text'].values[:sample_size]), axis=0) labels = [1] * sample_size + [0] * sample_size</span></span></code> </pre> <br>  Before the beginning of the training, the texts underwent the preliminary processing procedure: <br><br><ul><li>  lower case; <br></li><li>  substitution of ‚Äúe‚Äù for ‚Äúe‚Äù; <br></li><li>  replacement of links to the token "URL"; <br></li><li>  replacing the user reference to the token "USER"; <br></li><li>  deletion of punctuation marks. <br></li></ul><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(text)</span></span></span><span class="hljs-function">:</span></span> text = text.lower().replace(<span class="hljs-string"><span class="hljs-string">""</span></span>, <span class="hljs-string"><span class="hljs-string">""</span></span>) text = re.sub(<span class="hljs-string"><span class="hljs-string">'((www\.[^\s]+)|(https?://[^\s]+))'</span></span>, <span class="hljs-string"><span class="hljs-string">'URL'</span></span>, text) text = re.sub(<span class="hljs-string"><span class="hljs-string">'@[^\s]+'</span></span>, <span class="hljs-string"><span class="hljs-string">'USER'</span></span>, text) text = re.sub(<span class="hljs-string"><span class="hljs-string">'[^a-zA-Z--1-9]+'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) text = re.sub(<span class="hljs-string"><span class="hljs-string">' +'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text.strip() data = [preprocess_text(t) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> raw_data]</code> </pre> <br>  Next, I broke the data set into a training and test sample in a 4: 1 ratio. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=<span class="hljs-number"><span class="hljs-number">0.2</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><h2>  Vector word mapping </h2><br>  The input data of a convolutional neural network is a matrix with a fixed height <i>n</i> , where each row is a vector mapping of a word to a feature space of dimension <i>k</i> .  To form the embedding layer of the neural network, I used the Word2Vec [5] distributive semantics utility, designed to map the semantic meaning of words into vector space.  Word2Vec finds relationships between words according to the assumption that in similar contexts there are semantically close words.  More information about Word2Vec can be found in the <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">original article</a> , as well as <a href="https://habr.com/company/ods/blog/329410/">here</a> and <a href="https://habr.com/post/249215/">here</a> .  Since tweets are characterized by author's punctuation and emoticons, defining the boundaries of sentences becomes quite a laborious task.  In this paper, I assumed that each tweet contains only one sentence. <br><br>  The database of unmarked tweets is stored in SQL format and contains more than 17.5 million records.  For convenience, I converted it to SQLite using <a href="https://github.com/dumblob/mysql2sqlite">this</a> script. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sqlite3 <span class="hljs-comment"><span class="hljs-comment">#  SQLite   conn = sqlite3.connect('mysqlite3.db') c = conn.cursor() with open('data/tweets.txt', 'w', encoding='utf-8') as f: #    for row in c.execute('SELECT ttext FROM sentiment'): if row[0]: tweet = preprocess(row[0]) #      print(tweet, file=f)</span></span></code> </pre> <br>  Then, using the Gensim library, I trained the Word2Vec-model with the following parameters: <br><br><ul><li>  <i>size = 200</i> is the dimension of the attribute space; <br></li><li>  <i>window = 5</i> - the number of words from the context that the algorithm analyzes; <br></li><li>  <i>min_count = 3</i> - the word must occur at least three times for the model to take it into account. <br></li></ul><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> logging <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> multiprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gensim <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> gensim.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Word2Vec logging.basicConfig(format=<span class="hljs-string"><span class="hljs-string">'%(asctime)s : %(levelname)s : %(message)s'</span></span>, level=logging.INFO) <span class="hljs-comment"><span class="hljs-comment">#      data = gensim.models.word2vec.LineSentence('data/tweets.txt') #   model = Word2Vec(data, size=200, window=5, min_count=3, workers=multiprocessing.cpu_count()) model.save("models/w2v/model.w2v")</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/uk/zh/h0/ukzhh0kwiptrhim7tygo1vh5vwq.png"><br>  <i>Figure 2. Visualization of clusters of similar words using t-SNE.</i> <br><br>  For a more detailed understanding of the work of Word2Vec in Fig.  <a href="https://habr.com/post/267041/">Figure</a> 2 shows the visualization of several clusters of similar words from a trained model mapped into a two-dimensional space using <a href="https://habr.com/post/267041/">the t-SNE visualization algorithm</a> . <br><br><h2>  Vector text display </h2><br><img src="https://habrastorage.org/webt/er/de/wc/erdewcunafpymiafxeqgby-8-h8.png"><br>  <i>Figure 3. The length distribution of texts.</i> <br><br>  In the next step, each text was mapped to an array of token identifiers.  I chose the dimension of the text vector <i>s = 26</i> , since, with this value, 99.71% of all texts in the formed package are fully covered (Fig. 3).  If, during the analysis, the number of words in the tweet exceeded the height of the matrix, the remaining words were discarded and not considered in the classification.  The total dimension of the matrix of the sentence was <i>s √ó d = 26 √ó 200</i> . <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.text <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Tokenizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.sequence <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pad_sequences <span class="hljs-comment"><span class="hljs-comment">#   (    ) SENTENCE_LENGTH = 26 #   NUM = 100000 def get_sequences(tokenizer, x): sequences = tokenizer.texts_to_sequences(x) return pad_sequences(sequences, maxlen=SENTENCE_LENGTH) # C    tokenizer = Tokenizer(num_words=NUM) tokenizer.fit_on_texts(x_train) #        x_train_seq = get_sequences(tokenizer, x_train) x_test_seq = get_sequences(tokenizer, x_test)</span></span></code> </pre> <br><h2>  Convolutional neural network </h2><br>  To build a neural network, I used the Keras library, which is a high-level superstructure over TensorFlow, CNTK and Theano.  Keras has excellent documentation as well as a blog that covers many of the tasks of machine learning, for example, <a href="https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html">embedding layer initialization</a> .  In our case, the embedding-layer was initiated by the weights obtained during the training Word2Vec.  To minimize changes in the embedding layer, I froze it during the first stage of training. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Input <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.embeddings <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Embedding tweet_input = Input(shape=(SENTENCE_LENGTH,), dtype=<span class="hljs-string"><span class="hljs-string">'int32'</span></span>) tweet_encoder = Embedding(NUM, DIM, input_length=SENTENCE_LENGTH, weights=[embedding_matrix], trainable=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)(tweet_input)</code> </pre> <br>  In the developed architecture, filters with heights <i>h = (2, 3, 4, 5)</i> are used, which are intended for parallel processing of digrams, trigrams, 4-grams and 5-grams, respectively.  I added 10 convolutional layers for each height of the filter to the neural network, the activation function is ReLU.  Recommendations on finding the optimal height and number of filters can be found in [2]. <br><br>  After processing by the convolution layers, the feature maps arrived at the subsampling layers, where the 1-max-pooling operation was applied to them, thereby extracting the most significant n-grams from the text.  At the next stage, merging into a common vector of signs (a merging layer) occurred, which was fed into a hidden fully connected layer with 30 neurons.  At the last stage, the final feature map was fed to the output layer of the neural network with a sigmoidal activation function. <br><br>  Since neural networks are prone to retraining, after the embedding layer and in front of the hidden fully connected layer, I added a dropout regularization with the peak ejection probability p = 0.2. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> optimizers <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense, concatenate, Activation, Dropout <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.convolutional <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Conv1D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.pooling <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GlobalMaxPooling1D branches = [] <span class="hljs-comment"><span class="hljs-comment">#  dropout- x = Dropout(0.2)(tweet_encoder) for size, filters_count in [(2, 10), (3, 10), (4, 10), (5, 10)]: for i in range(filters_count): #    branch = Conv1D(filters=1, kernel_size=size, padding='valid', activation='relu')(x) #    branch = GlobalMaxPooling1D()(branch) branches.append(branch) #    x = concatenate(branches, axis=1) #  dropout- x = Dropout(0.2)(x) x = Dense(30, activation='relu')(x) x = Dense(1)(x) output = Activation('sigmoid')(x) model = Model(inputs=[tweet_input], outputs=[output])</span></span></code> </pre> <br>  The final model configured with Adam optimization function (Adaptive Moment Estimation) and binary cross-entropy as a function of errors.  The quality of the classifier's performance was evaluated in terms of macro-averaged accuracy, completeness and f-measures. <br><br><pre> <code class="python hljs">model.compile(loss=<span class="hljs-string"><span class="hljs-string">'binary_crossentropy'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, metrics=[precision, recall, f1]) model.summary()</code> </pre> <br>  At the first stage of training, the embedding layer froze, all other layers were trained for 10 epochs: <br><br><ul><li>  The size of the group of examples used for training: 32. <br></li><li>  Validation Sample Size: 25%. <br></li></ul><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ModelCheckpoint checkpoint = ModelCheckpoint(<span class="hljs-string"><span class="hljs-string">"models/cnn/cnn-frozen-embeddings-{epoch:02d}-{val_f1:.2f}.hdf5"</span></span>, monitor=<span class="hljs-string"><span class="hljs-string">'val_f1'</span></span>, save_best_only=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'max'</span></span>, period=<span class="hljs-number"><span class="hljs-number">1</span></span>) history = model.fit(x_train_seq, y_train, batch_size=<span class="hljs-number"><span class="hljs-number">32</span></span>, epochs=<span class="hljs-number"><span class="hljs-number">10</span></span>, validation_split=<span class="hljs-number"><span class="hljs-number">0.25</span></span>, callbacks = [checkpoint])</code> </pre> <br><br><div class="spoiler">  <b class="spoiler_title">Logs</b> <div class="spoiler_text"> <code>Train on 134307 samples, validate on 44769 samples <br> Epoch 1/10 <br> 134307/134307 [==============================] - 221s 2ms/step - loss: 0.5703 - precision: 0.7006 - recall: 0.6854 - f1: 0.6839 - val_loss: 0.5014 - val_precision: 0.7538 - val_recall: 0.7493 - val_f1: 0.7452 <br> Epoch 2/10 <br> 134307/134307 [==============================] - 218s 2ms/step - loss: 0.5157 - precision: 0.7422 - recall: 0.7258 - f1: 0.7263 - val_loss: 0.4911 - val_precision: 0.7413 - val_recall: 0.7924 - val_f1: 0.7602 <br> Epoch 3/10 <br> 134307/134307 [==============================] - 213s 2ms/step - loss: 0.5023 - precision: 0.7502 - recall: 0.7337 - f1: 0.7346 - val_loss: 0.4825 - val_precision: 0.7750 - val_recall: 0.7411 - val_f1: 0.7512 <br> Epoch 4/10 <br> 134307/134307 [==============================] - 215s 2ms/step - loss: 0.4956 - precision: 0.7545 - recall: 0.7412 - f1: 0.7407 - val_loss: 0.4747 - val_precision: 0.7696 - val_recall: 0.7590 - val_f1: 0.7584 <br> Epoch 5/10 <br> 134307/134307 [==============================] - 229s 2ms/step - loss: 0.4891 - precision: 0.7587 - recall: 0.7492 - f1: 0.7473 - val_loss: 0.4781 - val_precision: 0.8014 - val_recall: 0.7004 - val_f1: 0.7409 <br> Epoch 6/10 <br> 134307/134307 [==============================] - 217s 2ms/step - loss: 0.4830 - precision: 0.7620 - recall: 0.7566 - f1: 0.7525 - val_loss: 0.4749 - val_precision: 0.7877 - val_recall: 0.7411 - val_f1: 0.7576 <br> Epoch 7/10 <br> 134307/134307 [==============================] - 219s 2ms/step - loss: 0.4802 - precision: 0.7632 - recall: 0.7568 - f1: 0.7532 - val_loss: 0.4730 - val_precision: 0.7969 - val_recall: 0.7241 - val_f1: 0.7522 <br> Epoch 8/10 <br> 134307/134307 [==============================] - 215s 2ms/step - loss: 0.4769 - precision: 0.7644 - recall: 0.7605 - f1: 0.7558 - val_loss: 0.4680 - val_precision: 0.7829 - val_recall: 0.7542 - val_f1: 0.7619 <br> Epoch 9/10 <br> 134307/134307 [==============================] - 227s 2ms/step - loss: 0.4741 - precision: 0.7657 - recall: 0.7663 - f1: 0.7598 - val_loss: 0.4672 - val_precision: 0.7695 - val_recall: 0.7784 - val_f1: 0.7682 <br> Epoch 10/10 <br> 134307/134307 [==============================] - 221s 2ms/step - loss: 0.4727 - precision: 0.7670 - recall: 0.7647 - f1: 0.7590 - val_loss: 0.4673 - val_precision: 0.7833 - val_recall: 0.7561 - val_f1: 0.7636</code> <br> </div></div><br><br>  Then I selected the model with the highest F-scores on the validation data set, i.e.  the model obtained in the eighth epoch of learning (F <sub>1</sub> = 0.7791).  The model thawed out the embedding-layer, after which it launched five more epochs of training. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> optimizers <span class="hljs-comment"><span class="hljs-comment">#    model.load_weights('models/cnn/cnn-frozen-embeddings-09-0.77.hdf5') #  embedding     model.layers[1].trainable = True #  learning rate adam = optimizers.Adam(lr=0.0001) model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[precision, recall, f1]) model.summary() checkpoint = ModelCheckpoint("models/cnn/cnn-trainable-{epoch:02d}-{val_f1:.2f}.hdf5", monitor='val_f1', save_best_only=True, mode='max', period=1) history_trainable = model.fit(x_train_seq, y_train, batch_size=32, epochs=5, validation_split=0.25, callbacks = [checkpoint])</span></span></code> </pre> <br><br><div class="spoiler">  <b class="spoiler_title">Logs</b> <div class="spoiler_text"> <code>Train on 134307 samples, validate on 44769 samples <br> Epoch 1/5 <br> 134307/134307 [==============================] - 2042s 15ms/step - loss: 0.4495 - precision: 0.7806 - recall: 0.7797 - f1: 0.7743 - val_loss: 0.4560 - val_precision: 0.7858 - val_recall: 0.7671 - val_f1: 0.7705 <br> Epoch 2/5 <br> 134307/134307 [==============================] - 2253s 17ms/step - loss: 0.4432 - precision: 0.7857 - recall: 0.7842 - f1: 0.7794 - val_loss: 0.4543 - val_precision: 0.7923 - val_recall: 0.7572 - val_f1: 0.7683 <br> Epoch 3/5 <br> 134307/134307 [==============================] - 2018s 15ms/step - loss: 0.4372 - precision: 0.7899 - recall: 0.7879 - f1: 0.7832 - val_loss: 0.4519 - val_precision: 0.7805 - val_recall: 0.7838 - val_f1: 0.7767 <br> Epoch 4/5 <br> 134307/134307 [==============================] - 1901s 14ms/step - loss: 0.4324 - precision: 0.7943 - recall: 0.7904 - f1: 0.7869 - val_loss: 0.4504 - val_precision: 0.7825 - val_recall: 0.7808 - val_f1: 0.7762 <br> Epoch 5/5 <br> 134307/134307 [==============================] - 1924s 14ms/step - loss: 0.4256 - precision: 0.7986 - recall: 0.7947 - f1: 0.7913 - val_loss: 0.4497 - val_precision: 0.7989 - val_recall: 0.7549 - val_f1: 0.7703</code> <br> </div></div><br><br>  The highest rate <i>F <sub>1</sub> = 76.80%</i> on the validation sample was achieved at the third learning epoch.  The quality of the work of the trained model on the test data was <i>F <sub>1</sub> = 78.1%</i> . <br><br>  Table 1. Quality analysis of tonality on test data. <br><table><tbody><tr><td>  Class label <br></td><td>  Accuracy <br></td><td>  Completeness <br></td><td>  F <sub>1</sub> <br></td><td>  Number of objects <br></td></tr><tr><td>  Negative <br></td><td>  0.78194 <br></td><td>  0.78243 <br></td><td>  0.78218 <br></td><td>  22457 <br></td></tr><tr><td>  Positive <br></td><td>  0.78089 <br></td><td>  0.78040 <br></td><td>  0.78064 <br></td><td>  22313 <br></td></tr><tr><td>  avg / total <br></td><td>  0.78142 <br></td><td>  0.78142 <br></td><td>  0.78142 <br></td><td>  44770 <br></td></tr></tbody></table><br><h2>  Result </h2><br>  As a baseline solution, I <a href="https://towardsdatascience.com/sentiment-analysis-of-tweets-using-multinomial-naive-bayes-1009ed24276b">trained a</a> naive Bayes classifier with a multinomial distribution model, the comparison results are presented in Table.  2 <br><br>  Table 2. Comparison of the quality analysis of tonality. <br><table><tbody><tr><td>  Classifier <br></td><td>  Precision <br></td><td>  Recall <br></td><td>  F <sub>1</sub> <br></td></tr><tr><td>  MNB <br></td><td>  0.7577 <br></td><td>  0.7564 <br></td><td>  0.7560 <br></td></tr><tr><td>  CNN <br></td><td>  <b>0.78142</b> <br></td><td>  <b>0.78142</b> <br></td><td>  <b>0.78142</b> <br></td></tr></tbody></table><br>  As you can see, the quality of the CNN classification exceeded the MNB by a few percent.  The values ‚Äã‚Äãof the metrics can be increased even more if you work on optimizing the hyperparameters and network architecture.  For example, you can change the number of learning epochs, check the effectiveness of using different vector representations of words and their combinations, choose the number of filters and their height, implement a more efficient text preprocessing (correcting typos, normalization, stemming), adjust the number of hidden fully connected layers and neurons in them . <br><br>  Source code <a href="https://github.com/sismetanin/sentiment-analysis-of-tweets-in-russian">is available on Github</a> , trained CNN and Word2Vec models can be downloaded <a href="https://yadi.sk/d/e6KJu24nijlAGA">here</a> . <br><br><h2>  Sources </h2><br><ol><li>  Cliche M. BB_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and LSTMs // Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017).  - 2017. - p. 573-580. <br></li><li>  Zhang Y., Wallace B. A Convolutional Neural Networks for Sentence Classification // ArXiv preprint arXiv: 1510.03820.  - 2015. <br></li><li>  Rosenthal S., Farra N., Nakov P. SemEval-2017 task 4: Sentiment Analysis on Twitter // Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017).  - 2017. - p. 502-518. <br></li><li>  Yu. V. Rubtsov.  Building a corpus of texts for setting the tone tone classifier // Software products and systems, 2015, ‚Ññ1 (109), ‚Äî.72-78. <br></li><li>  Mikolov, T. et al.  Representations of Words and Their Compositions // Advances in Neural Information Processing Systems.  - 2013. - p. 3111-3119. <br></li></ol></div><p>Source: <a href="https://habr.com/ru/post/417767/">https://habr.com/ru/post/417767/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../417755/index.html">Study: 80% of ICO 2017 found to be fraudulent</a></li>
<li><a href="../417757/index.html">Creating a bot to participate in the AI ‚Äã‚Äãmini cup. GPU Experience</a></li>
<li><a href="../417759/index.html">Be my rubber duck</a></li>
<li><a href="../417761/index.html">GitLab moves from Azure to the Google Cloud Platform. Moving news and maintenance dates</a></li>
<li><a href="../417763/index.html">MVIDroid: review of the new MVI library (Model-View-Intent)</a></li>
<li><a href="../417769/index.html">User Memory Design: How to Design Ages</a></li>
<li><a href="../417771/index.html">ICANN plan: the corporation proposed a new model for managing root DNS servers</a></li>
<li><a href="../417773/index.html">Homemade OpenPnP Component Installer</a></li>
<li><a href="../417775/index.html">The mechanism of commissions in Bitcoin and why be friends with miners</a></li>
<li><a href="../417777/index.html">"Reading for the weekend": 25 materials for beginners vinyl lovers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>