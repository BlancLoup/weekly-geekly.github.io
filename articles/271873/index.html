<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The practice of reducing the cost of electricity data center</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Some issues of energy saving in data centers 



 Introduction 
 One of the most important cost items at the stage of operation of the data processing...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The practice of reducing the cost of electricity data center</h1><div class="post__text post__text-html js-mediator-article">  <strong>Some issues of energy saving in data centers</strong> <br><br><img src="https://habrastorage.org/files/cbc/275/842/cbc27584241d48c0b0ca63683019cc16.jpg" alt="image"><br><br><h1>  Introduction </h1><br>  One of the most important cost items at the stage of operation of the data processing center (DPC) is the payment for consumed electricity.  This is not surprising - all data center systems, starting with IT equipment, for the sake of the normal functioning of which the data center is created, and ending with alarm systems and surveillance systems, consume electricity.  Therefore, the most reliable way to save on electricity is to sell computing and telecommunication devices, and close the data center.  Unfortunately, without a working computing and communication technology, doing business in modern conditions is unimaginable, so the option of completely eliminating the data center in this article will not be considered. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The power consumption of the data center as a single system consists of the consumption of individual systems, both information and telecommunications, and engineering infrastructure systems, including the inevitable unproductive losses.  To achieve energy savings in general, it is necessary to explore the possibilities of reducing the consumption of each subsystem. <br><a name="habracut"></a><br>  The main consumers of energy are, of course, computing and telecommunication systems.  However, for the normal functioning of computing and telecommunications equipment, work, as a rule - continuous, of a number of auxiliary, service systems is necessary.  These auxiliary systems form the engineering infrastructure of the data center. <br><br><h1>  The main consumers of energy.  Energy Consumption Structure </h1><br>  For a stable, normal operation of the most important IT equipment, it is necessary to ensure comfortable conditions: first of all, remove the heat generated by IT equipment, ensure relative humidity of air within 50 ¬± 10%, create conditions for the operation of security systems, and conditions for the work of the service personnel . <br><br><h3>  So, the main consumers: </h3><br><ol><li>  IT equipment. </li><li>  The system of maintaining climate parameters (air conditioning, humidification / dehumidification). </li><li>  Ventilation and gas removal system. </li><li>  Lighting. </li><li>  Security systems (fire extinguishing, fire alarm, drainage, access control system, video surveillance, etc.). </li></ol><br><h3>  In addition to the above, energy is consumed: </h3><br><ol><li>  The system of guaranteed power supply (maintaining the DGU in standby mode - heating, recharging batteries, automation work). </li><li>  Uninterruptible power supply system (own consumption - work of inverters, electronics, automation, etc.). </li><li>  Losses during transportation and distribution of energy (heating of cables, losses in interconnects - in switchboards, PDUs, etc.). </li></ol><br><blockquote>  Virtually all of the electricity that goes to the data center is ultimately converted to heat. <br>  <a href="http://www.apcmedia.com/salestools/NRAN-72754V/NRAN-72754V_R2_EN.pdf%3Fsdirect%3Dtrue">Electrical Efficiency Measurement for Data Centers, White Paper 154 by Neil Rasmussen, APC by Schneider Electric</a> </blockquote><br>  In general, in order to find ways to reduce energy costs, it is necessary to determine the numerical criteria for evaluating energy efficiency and analyze each of the energy-consuming systems from the point of view of reducing consumption. <br><br><h1>  Data Center Energy Efficiency Assessment Method </h1><br>  There are several methods for assessing energy efficiency in a data center, differing mainly in the degree of accounting for changes in consumption (as a function of time, computational load, etc.).  We use the simplest. <br><br><h2>  PUE / DCE </h2><br>  The simplest and most visual assessment method is to calculate the energy efficiency ratio of PUE (Power Usage Effectiveness), and its inverse DCE (Datacenter Efficiency, in fact, the efficiency).  In this case, the ‚Äúbeneficial effect‚Äù is considered to be the consumption of IT equipment, and the DCE coefficient (data center efficiency) is calculated as the ratio of the consumption of IT equipment to the total energy consumption of the data center;  PUE ratio - the ratio of total data center consumption to IT equipment consumption.  The PUE value can be in the range from 1.0 to infinity.  A PUE value close to 1.0 will indicate 100% efficiency ‚Äî all energy is used only by IT equipment.  True, this indicator is practically unattainable - in connection with the above. <br><br><img src="https://habrastorage.org/files/271/4d5/36f/2714d536fffe4eac85d802e3d40bcf97.png" alt="image"><br><br>  In practice, it is more convenient and clearer to operate with PUE.  This is confirmed by international practice. <br>  To assess the situation with us, it is useful to know the world statistics on energy efficiency.  Thus, the ‚ÄúLAN Network Solutions Journal‚Äù <a href="http://www.osp.ru/lan/2014/05/13041191/">http://www.osp.ru/lan/2014/05/13041191/</a> "&gt; <a href="http://www.osp.ru/lan/2014/05/13041191/">confirms that</a> <blockquote>  ‚ÄúThe average value of PUE dropped from 2.5 in 2007 to 1.65 at present‚Äù </blockquote>  (May 2014).  A joint study by T-Systems and Intel of 2010 ‚ÄúDataCenter 2020: hot aisle and cold aisle containment efficiencies reveal no significant differences‚Äù predicted a decrease in PUE by 2020 for new data centers from 1.8 to 1.4.  That is, today PUE can be considered typical in the range of 1.8-1.5.  I note that for popular now ‚Äúgreen‚Äù data centers, PUE values ‚Äã‚Äãwithin 1.1‚Äì1.02 are, according to experts, speculation, since they simply do not take into account their own energy sources (wind turbines, solar panels, etc.). <br><br><h1>  Energy Measurement Techniques </h1><br>  <a href="http://www.osp.ru/lan/2014/05/13041191/">According to experts</a> , a detailed measurement of PUE is quite expensive, although for a rough assessment of large efforts it is not required: ‚ÄúWith typical PUE data for PUE values ‚Äã‚Äãof 1.8‚Äì2.5, there is no need for high precision measurements ‚Äî it‚Äôs enough to measure the power at the electrical installation input and UPS outputs that feed the payload.  The minimum program is to install meters for commercial metering of electricity. ‚Äù <br><br>  In our data centers, just about this technique is used: the power consumption of the entire data center is determined using data from commercial metering devices and / or direct measurements of currents on the input lines of the input switchgear (ASU) using current tongs (unfortunately, no projects were installed in the technological control of currents and voltages; savings ...);  IT equipment consumption is determined by direct measurements of the currents at the output of the UPS by current clamps and / or by the indications of the UPS;  The total consumption of engineering infrastructure systems is determined using data from commercial metering devices and / or direct measurements of currents on the input lines of the input switchgear using current clamps, since in a normal situation the UPS / IT equipment and engineering infrastructure systems are powered from separate input lines - for the main data center ;  for backup data center data is taken from the system of commercial accounting, and from the testimony of the current load of the UPS. <br><br>  We are well aware that the measurement accuracy leaves much to be desired: neither the current clamp nor the UPS monitoring system can serve as a measuring tool.  However, for a rough assessment of them is enough.  In order to assess the reliability of the measured power consumption, the ‚Äútheoretical‚Äù consumption of IT systems and components of the engineering infrastructure is calculated using passport data of computing and telecommunications equipment.  The difference between the measured and calculated values ‚Äã‚Äãallows us to draw some interesting conclusions: about the measurement accuracy, and the accuracy of the data provided in the documentation, the level of loading of IT equipment and engineering infrastructure, and unregistered consumers (in our case, for example, data centers turned out to be unaccounted consumers of data center electricity) places of monitoring group employees connected to the main data center UPS). <br><br><h1>  Ways to reduce power consumption by system </h1><br>  Each system (subsystem) of IT and engineering infrastructure of the data center is characterized by its own capabilities and methods for reducing energy consumption.  Consider the basic systems and possible methods to reduce their power consumption. <br><br><h2>  IT equipment </h2><br>  The main consumer of energy, as has been repeatedly stated above, is IT equipment.  Methods to reduce its consumption <a href="http://ko.com.ua/cod_kak_obekt_sistemnoj_i_strukturnoj_optimizacii_42421">are given in the recommendations of</a> , for example, Eaton specialists: <br><ul><li>  Disconnect power from inactive equipment. </li><li>  Server virtualization, load balancing. </li><li>  Combining servers and data centers, using blades. </li><li>  Enhance server power management features. </li><li>  Use IT equipment with high-performance power supplies. </li></ul><br>  A detailed description of the methods and reasons for reducing the power consumption of IT equipment due to their use is left to system administrators;  However, all this is quite obvious. <br>  The application of these measures, including the replacement of IT equipment with more energy efficient ones, gives the most significant result for reducing the overall power consumption of the data center, since the contribution of IT systems to the overall consumption is the main one, and decreasing the power consumption of other critical systems decreases: the cost of cooling, own consumption of UPS, the loss of energy transportation. <br><br><h2>  Uninterruptible Power Supply </h2><br>  The own consumption of the UPS is due to the operation of the inverter-inverters that make up them, electronics, automation, and heat losses.  The efficiency of the UPS is determined by the efficiency factor, which is calculated as the ratio of the total system power, including the UPS, to the power consumed by the load.  Modern UPSs provide an efficiency of 0.75-0.98, while the best performance is achieved at the UPS load level within 0.8-0.95 of the maximum nameplate capacity.  Methods to reduce their own consumption of UPS are as follows: <br><ul><li>  The main method - the use of UPS with high efficiency. </li><li>  Maintain the UPS load level at 0.8-0.9 of the maximum (maximum efficiency, see above). </li><li>  The use of modular UPS, allowing to regulate the rated power by switching on / off individual power modules. </li><li>  Maintain optimal climatic conditions of the UPS (UPS efficiency decreases with increasing ambient temperature). </li></ul><br>  A separate item I would like to indicate the uniform distribution of the load across the phases of the supply voltage.  Modern UPSs, of course, allow 100% phase imbalance on the load, but this does not add efficiency to them. <br><br><h2>  Power Transmission Lines, Power Distribution Systems </h2><br>  Energy losses in communication lines and distribution equipment are due to the following reasons: <br><ul><li>  heat loss during energy transfer; </li><li>  leakage currents. </li></ul><br>  These losses depend on the following factors: <br><ul><li>  own electrical resistance of power transmission lines; </li><li>  length of power transmission lines; </li><li>  geometry of power transmission lines; </li><li>  insulation resistance and reliability; </li><li>  quality of connections and contacts. </li></ul><br>  Methods to minimize energy losses in transmission lines and distribution systems: <br><ul><li>  Minimization of cable lengths. </li><li>  The use of cables with conductors of materials with high electrical conductivity (low resistance), for example, copper instead of aluminum. </li><li>  Using tires instead of cables. </li><li>  Reliable insulation, minimizing leakage currents. </li><li>  Reliable connections: </li><li>  minimizing the number of compounds; </li><li>  careful drawing of mechanical connections; </li><li>  regularly checking the quality of connections, including with a pyrometer or thermal imager, to determine the increased heat generation, which may indicate a poor connection quality; </li><li>  use welding or brazing instead of mechanical (threaded) connections. </li><li>  Regular check of insulation resistance of power cables. </li><li>  The use of electrical and wiring products only high quality. </li></ul><br><br><h2>  Air conditioning systems </h2><br>  By the efficiency of the air conditioning system we mean the ratio of the cost of electricity for the operation of the system to the amount of heat removed from the cooled equipment.  The efficiency of the heat removal system from IT equipment (air conditioning, air-conditioning) is determined by many factors, including the ones listed above with respect to power transmission lines.  In addition, the overall efficiency of the data center cooling system is significantly curled by the following factors: <br><ul><li>  The efficiency of air conditioners is the ratio of cooling capacity to energy consumed. </li><li>  Heat losses in the main coolant. </li><li>  Heat loss during the delivery of cold to IT equipment. </li><li>  The level of thermal insulation of the cooled rooms (machine rooms, UPS rooms, panel rooms, etc.). </li><li>  Efficiency of distribution of cold and hot air streams in cooling rooms and near cooled equipment, elimination of cold losses. </li></ul><br>  Methods to reduce the energy consumption of the air-conditioning system are aimed at improving the efficiency of cooling and reducing heat losses (we are talking about freon systems). <br><ul><li>  The use of modern energy-efficient air conditioners.  For the "classic" domestic air conditioners to generate 1 kW of cooling capacity (more precisely, to remove 1 kW of heat) 0.33-0.4 kW of electricity is required;  for industrial precision air conditioners, this figure can now be 0.2-0.25 kW of electricity per 1 kW of cooling capacity. </li><li>  Timely maintenance of refrigeration units: contamination of heat exchangers of air conditioners can reduce the efficiency by 7-12% due to a decrease in the heat transfer coefficient;  polluted air filters reduce cooling efficiency by increasing airflow resistance by 5‚Äì8%.  For example, there have been cases of complete loss of air conditioners when the heat exchangers of the condensers were covered with a layer of poplar fluff;  heat exchange (heat removal from the heat carrier) was completely blocked, the freon mixture did not condense from the gaseous to the liquid state. </li><li>  Timely preventive maintenance: faulty components, including the motors of the fan drives, increase the power consumption of the systems and the own heat generation of the units.  ‚ÄúBroken‚Äù bearings or ‚Äúsagging‚Äù drive belts, for example, in pressure fans, due to increased friction losses, reduce the efficiency of both the fan and the unit as a whole;  in addition, as in the previous case, the heat generation of the installation increases. </li><li>  Choosing the right backup scheme for air conditioners, based on the requirements of the reliability of the system and its efficiency.  The most efficient air conditioners operate at 80-85% of the load from the maximum passport;  at high levels of load, the lifetime of the units decreases, while at lower levels, the energy efficiency of the system decreases. </li><li>  Fine tuning of the parameters of the refrigeration units. </li><li>  Proper placement of temperature sensors input and output air (relevant for complex systems of precision and ducted air conditioners). </li><li>  Minimizing the loss of cold in refrigerated rooms, thermal insulation of walls, ceilings and building structures of refrigerated rooms to prevent heat exchange through them. </li><li>  Minimizing refrigerant lines. </li><li>  Maintaining the integrity of the thermal insulation of highways and coolant pipelines (coolant). </li><li>  Minimization of the length of the path of delivery of cold air to the equipment;  Here, however, there are some peculiarities: for example, in the case of cooling IT equipment out of the raised floor, it is desirable to have as much storage space as possible - as a cold accumulator, despite the lengthening of the air supply paths. </li><li>  Providing cold air from air conditioners directly to the front panels of enclosures with installed IT equipment, using air ducts to supply cooled air, and to remove hot air. </li><li>  In the case of cooling from the raised floor - adjusting the flow of cold air to the IT equipment by installing / removing perforated plates (gratings), selecting gratings with different percentages of holes, using ‚Äúactive floor‚Äù plates (with built-in fans, temperature sensors and fan speed controllers ). </li><li>  Prevent mixing of cold and hot air streams;  in practice, the separation of data center machine halls (and other technological rooms, where appropriate) into ‚Äúcold‚Äù and ‚Äúhot‚Äù corridors, isolation of corridors, organization of isolated ‚Äúcold‚Äù and / or ‚Äúhot‚Äù rooms. </li></ul><br><br><h2>  Ventilation and gas removal system </h2><br>  Energy saving opportunities in ventilation and gas removal systems are limited.  The ventilation system in the data center is designed primarily to ensure the normal operation of staff, it does not affect the functioning of IT systems, and, conversely, can reduce the efficiency of cooling systems.  Therefore, it is recommended to start the ventilation system only for the time when the personnel work in the data center premises.  As for the gas removal system, in normal condition it should not work, and it starts only after the automatic fire extinguishing systems have triggered. <br><br><h2>  Steam humidification systems </h2><br>  Judging by experience, the continuous operation of the steam humidification system (air humidification in the data center rooms) is not always necessary.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Observations carried out in the premises of one of our data centers show, for example, that it is advisable to start the steam humidification system only in the winter, when the air humidity in the street is lowered due to frost (freezing moisture); ). </font><font style="vertical-align: inherit;">In the warm season, due to the architectural features of the premises of our data center, humidity "itself" is maintained at 40-55%. </font><font style="vertical-align: inherit;">However, before giving recommendations for a specific site, it is necessary to study the characteristics of its location, consideration of the climatic zone and long-term observations.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Systems in which the reduction of power consumption is impossible or impractical </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> In general, it is impractical to look for ways to reduce the power consumption of a number of auxiliary systems of the data center engineering infrastructure, since these systems </font></font><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> as a rule, they already have low own electricity consumption, and its reduction is a complex engineering task; </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> the power consumption of these systems is minimal within the data center, and its reduction will not have any noticeable effect on the overall situation. </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> These systems include: </font></font><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Components of the guaranteed power supply system - own consumption of diesel generator sets in standby mode. </font><font style="vertical-align: inherit;">As a rule, electric power is consumed from an external source for the operation of automation, the control system, recharging of batteries, and maintaining the temperature of process fluids to ensure quick and reliable start of a diesel generator set in the event of a break in external power supply.</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Climate parameters monitoring system. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Access Control System (ACS). </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The system of video surveillance and video registration. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Security and fire alarm system. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Automatic fire extinguishing system. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Flood protection system (drainage system). </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In general, lowering the power consumption of these systems is not possible. </font><font style="vertical-align: inherit;">In addition, the last two of these systems consume electricity only in case of emergency.</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Practical application of methods for reducing power consumption of the data center </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">During the operation of several computing sites (data centers, server and telecommunications nodes), a number of ways to reduce the cost of electricity supply, from those listed above, have been tested and introduced. </font><font style="vertical-align: inherit;">Some did not lead to a significant reduction in power consumption, others showed decent results. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">For systems and sites, you can bring the following data.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> IT equipment </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Replacing IT equipment with more energy efficient is the most costly, but also the most effective solution. An illustrative example is the process of replacing heavy-class servers, storage systems and blades at our Novosibirsk main site. For 8 months, from July 2014 to February 2015, due to the replacement of IT equipment, the power of the machine room (IT load) was reduced from 120 to 84 kW. Accordingly, energy costs for cooling equipment in the machine room, the load on the UPS system and its own consumption have decreased; the power consumed by the engineering infrastructure systems decreased from 56 to 37 kW; Total data center consumption fell from 180 to 120 kW:</font></font><br><img src="https://habrastorage.org/files/e74/96a/2bf/e7496a2bf4bc4503a1ddbe724958d279.png" alt="image"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And all this despite the fact that until July 2014, the general trend was a gradual increase in power consumption; </font><font style="vertical-align: inherit;">peak values ‚Äã‚Äãreached 160 kW of IT equipment capacity, and 228 kW of total power consumption: </font></font><br><img src="https://habrastorage.org/files/037/131/ed9/037131ed953f427993de4a8fdb1532c5.png" alt="image"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Actually, practice confirmed the thesis cited at the beginning of the article: the best way to save electricity in the data center is to turn off IT equipment.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Uninterruptible Power Supply </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">High-performance UPSs Newave ConceptPower Upgrade are used at Novosibirsk sites, in Moscow, in the main data center - APC Galaxy UPSs. All UPSs have decent characteristics, for example, the efficiency of double conversion of Newave UPSs with loads at 75-100% of the maximum is 96%; even at a load level of 25% of the maximum, the manufacturer promises an efficiency of at least 92%. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The UPSs are installed in air-conditioned premises, the climatic conditions for them are optimal. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Regulation of the UPS load level, unfortunately, is not possible due to the adopted redundancy schemes. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The question of leveling the load in phases was done from the moment the data center was put into operation: the installation of any new IT equipment ‚Äî or the movement of the existing one ‚Äî was carried out taking this factor into account.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Power Transmission Lines, Power Distribution Systems </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> In general, all measures to reduce losses in power transmission lines and switching and distribution systems were taken at the construction stage of our data centers. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Due to the adopted backup schemes and technical and operational features, part of the checks aimed at identifying losses is performed according to the situation, and not on a regular basis: insulation resistance measurement, for example, is carried out only when the power supply lines are disconnected; </font><font style="vertical-align: inherit;">the same applies to the bolting and screw connections in panel equipment. </font><font style="vertical-align: inherit;">Surveys of power and distribution boards are regularly carried out using pyrometers to identify unreliable connections. </font><font style="vertical-align: inherit;">However, the quality of the materials and equipment used and the operating conditions of cable lines and switching equipment (protection from external influences, constant temperature and humidity) guarantee the absence of significant electricity losses during transportation, at least within the territory of the data center.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Air conditioning systems </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the framework of solving the problem of optimizing and improving the efficiency of air conditioning systems, quite a lot of activities were carried out. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">During the design and construction of data centers, modern and energy-efficient models of air conditioners were selected. So, in the main data center in Novosibirsk installed precision air conditioners Stulz, in the backup data center channel conditioners Daikin; In the main data center in Moscow, HiRef precision systems are installed. The efficiency of these systems lies in the zone of 0.22-0.27 kW of consumed electric power per 1 kW of cooling capacity.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Concluded long-term contracts with service companies for regular maintenance of air conditioning systems. During operation, malfunctions that led to increased power consumption were identified and eliminated: for example, the data center in Novosibirsk, for example, replaced the fan motors twice (winding damage, increased heat generation), the bearings in the fans were replaced in three air conditioners.</font></font><br>        ;           ,       (2008-2010),           .    /                 ;      ,        . <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the first years of operation, there was an acute problem of uneven cooling of IT equipment after switching air conditioners as part of the adopted redundancy scheme: temperatures were set in different areas of the machine rooms with a significant difference, for example, the temperature on the left side of the cabinet rows was 3-5 ¬∞ C lower than nominal, on the right side 3-5¬∫ above. The problem was solved by transferring the inlet air temperature sensors from the air conditioners closer to the IT equipment. This not only ensured uniform cooling of IT equipment, but also equalized the load on air conditioners, and, ultimately, reduced energy consumption.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Much attention is paid to optimizing air flow, mainly cold, to ensure sufficient and uniform cooling of IT equipment. This is achieved by replacing perforated raised floor plates and using ‚Äúactive floor‚Äù modules (for main data centers), and direct adjustment of air flow using adjustable grilles and deflectors on air ducts (backup data center in Novosibirsk, see photo) when moving or changing the composition of the cooled equipment .</font></font><br><br><img src="https://habrastorage.org/files/138/2c0/898/1382c0898b1444bc98962e53470f2f71.jpg" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">During the design and construction of data processing centers, the possibility of separating machine rooms into ‚Äúcold‚Äù and ‚Äúhot‚Äù corridors was foreseen. </font><font style="vertical-align: inherit;">This opportunity was implemented on the Novosibirsk sites, first in the backup data center (October 2010, photo above, panels from the roofs of the cabinets to the ceiling are visible), then mainly in the data center (May 2011): </font></font><br><br><img src="https://habrastorage.org/files/df0/51a/626/df051a626c2c438d82139886903e7583.jpg" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">After the corridors were divided, the Novosibirsk data center system consumed electricity conditioning decreased by 9-17% depending on the time of year. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the backup data center, separate cold air ducts were replaced with manifolds with deflectors and adjustable grilles; </font><font style="vertical-align: inherit;">at a rough estimate, this reduced the power consumption of air conditioners by 4-7%.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Steam humidification systems </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The above recommendations are practically applied: for the warm season, the steam humidification systems of air conditioners are turned off, the water mains overlap; </font><font style="vertical-align: inherit;">continuous monitoring of the relative humidity in the data center premises is carried out according to the data of climate monitoring systems. </font><font style="vertical-align: inherit;">If necessary (the fall of the relative humidity to 30-35%), the systems are switched on for a short time. </font><font style="vertical-align: inherit;">The level of efficiency (energy savings) of this event is difficult to assess.</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The results of the measures to reduce power consumption </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The most complete, and therefore indicative data are available on the main data center in Novosibirsk. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The general reduction in power consumption due to the replacement and reduction of the IT equipment fleet was discussed above. Reducing the power consumption of the data center by 60 kW - the result is more than worthy. However, in addition to absolute indicators, relative values ‚Äã‚Äãcharacterizing overall energy efficiency are also of great importance. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As the main indicator, we use the PUE coefficient, which describes the overall energy efficiency of the data center as a single system. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The history of the struggle for kilowatts is approximately as follows:</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1. 2008‚Äì2009. The stage of final data center commissioning, "grinding"; energy efficiency was practically not thought at that time, since there were enough other various problems: ‚Äústuffing‚Äù the data center with equipment that was moved, including from other platforms, optimization of equipment placement in the hall, debugging of engineering infrastructure systems, in particular, air conditioning, etc. . And, by the way, the elimination of continuous accidents in engineering systems. Therefore, PUE in 2008 was at the level of 1.55-1.56, and in 2009 it is quite stable at the level of 1.49. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2. 2010 Special measures to reduce consumption were not carried out, PUE ranged from 1.58 in winter to 1.402 in summer, with an average annual value of 1.44.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3. In 2011, work began on improving the efficiency and reliability of the air conditioning system. The ways of supplying air to the condensers of air conditioners were expanded, the IT equipment was moved in the engine room for uniform cooling, the above-mentioned transfer of temperature sensors for incoming air, and a number of other measures. The average value of PUE for the year is 1.44. Despite the fact that by the end of the year it grew to 1.55, because of the increase in the total capacity of IT equipment in the data center, the UPS system was expanded.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4. In 2012, the corridors in the engine room were finally divided, the process of upgrading IT equipment began. Optimized air flow, to improve the cooling of heavy systems used plates "active floor". The working conditions of air conditioners condensers have been improved, the ‚Äúwindows‚Äù for the inflow and outflow of outside air have been expanded. Result: PUE average 1.4 per year. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">5. 2013. No special measures were taken to improve PUE; by the end of the year, a number of coolant leaks in the mains had been eliminated, which, it must be assumed, led to a certain decrease in PUE. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">6. 2014. The decrease in PUE is mainly due to the modernization and replacement of IT equipment, and, accordingly, the change in the redundancy scheme of air conditioners from 4 + 1 to 3 + 2. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7. 2015. The data for the first three months of the current, 2015.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In general, the dynamics is positive, the PUE ratio was reduced from an average of 1.55-1.57 at the beginning of the data center operation to 1.32 (as of March 4, 2015). </font></font><br><br><img src="https://habrastorage.org/files/bb7/959/c95/bb7959c95b0f494e8097d2c7bf1afe24.png" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The result is good, given the high complexity and cost of energy efficiency measures, and given that for many data centers, the value of PUE 1.6 is an unattainable dream ( </font></font><a href="http://alldc.ru/experts/2361.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Efficiency of the infrastructure of the data center</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ):</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúAccording to research by the Uptime Institute, a standard data center has an average value of PUE 2.5. </font><font style="vertical-align: inherit;">This means that for every "incoming" 2.5 watts only one watt comes to the server racks. </font><font style="vertical-align: inherit;">Most facilities could achieve a PUE = 1.6 using equipment more efficiently and introducing advanced working methods. ‚Äù</font></font><br></blockquote><br><h1>  Conclusion </h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Over the years of operation (2008-2015), the data center of medium size in general with elementary, generally accepted means was able to reduce the total energy consumption from an average level of 200 kW to 110-120 kW. The cost of electricity, therefore, decreased by about 40-54%, which, you see, is not the worst result. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It should be borne in mind that there are fewer opportunities to reduce PUE values; it is simply not possible to reach a value of 1.0 - for purely physical reasons.</font></font><br>   ,  ,     -, ,   .  ,        .     ,    ,       PUE       1,39-1,43. <br><br> PS    ,    ,          . <br></div><p>Source: <a href="https://habr.com/ru/post/271873/">https://habr.com/ru/post/271873/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../271859/index.html">191,000 email addresses leak from Avito resume</a></li>
<li><a href="../271863/index.html">Implementation of CRM on production: samples, errors, results</a></li>
<li><a href="../271867/index.html">Convenient viewing of MAC addresses on ports of switches huawei, linksys, dlink, extreme using expect</a></li>
<li><a href="../271869/index.html">Customize voice messages in 3CX</a></li>
<li><a href="../271871/index.html">The story of one problem with disabling Ejabberd users after update 2.1.13-> 15.07</a></li>
<li><a href="../271875/index.html">Pagination of lists in Android with RxJava. Part II</a></li>
<li><a href="../271877/index.html">RDMA: a view from the inside</a></li>
<li><a href="../271881/index.html">How to create a round Progress Button</a></li>
<li><a href="../271883/index.html">Palantir 101. What is allowed to ordinary mortals to know about the second most abrupt private company in Silicon Valley</a></li>
<li><a href="../271885/index.html">ARM today 25</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>