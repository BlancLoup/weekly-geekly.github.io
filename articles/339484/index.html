<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Traffic Sign Recognition Using CNN: Spatial Transformer Networks</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! We continue a series of materials from a graduate of our program Deep Learning, Cyril Danilyuk, on the use of convolutional neural networks ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Traffic Sign Recognition Using CNN: Spatial Transformer Networks</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr!  We continue a series of materials from a graduate of our program Deep Learning, Cyril Danilyuk, on the use of convolutional neural networks for pattern recognition - CNN (Convolutional Neural Networks). <br><br>  In the last <a href="https://habrahabr.ru/company/newprolab/blog/334618/">post,</a> we started talking about preparing data for training a convolutional network.  Now is the time to use the data and try to build on them the neural network classifier of road signs.  This is what we will do in this article, adding in addition to the classifier network a curious module - STN.  Dataset we use <a href="http://benchmark.ini.rub.de/%3Fsection%3Dgtsrb%26subsection%3Ddataset">the same as before</a> . <br><br>  Spatial Transformer Network (STN) is one of the examples of differentiable LEGO-modules, on the basis of which you can build and improve your neural network.  STN, applying a learning affine transform followed by interpolation, deprives images of spatial invariance.  Roughly speaking, the task of STN is to rotate or reduce-enlarge the original image so that the main classifier network can more easily determine the desired object.  The STN block can be placed on a convolutional neural network (CNN), working for the most part on its own, learning from gradients coming from the main network. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      All project source code is available on GitHub by <a href="https://github.com/dnkirill/stn_idsia_convnet">reference</a> .  The original of this article can be <a href="http://medium.com/towards-data-science/convnets-series-spatial-transformer-networks-cff47565ae81">viewed at Medium</a> . <br><br>  To have a basic understanding of how STN works, take a look at 2 examples below: <br><div style="text-align:center;"><img src="https://habrastorage.org/web/3e1/f67/455/3e1f674551274210a876e760158f177b.gif"></div>  <sub>Left: the original image.</sub>  <sub>Right: the same image converted by STN.</sub>  <sub>The spatial transformers recognize the most important part of the image and then scale or rotate it to focus on that part.</sub> <br><a name="habracut"></a><br><div style="text-align:center;"><img src="https://habrastorage.org/web/b79/f01/3b4/b79f013b47fa43e88f89bf64483e2a3a.gif"></div>  <sub>Another example of STN training and image conversion.</sub>  <sub>This is the first era and the first tens of batches used for training.</sub>  <sub>It can be seen how the STN recognizes the shape of the sign, to then concentrate on it.</sub> <br><br>  STN works even in difficult cases (for example, 2 characters in the image), but most importantly, STN really improves the quality of the classifier (IDSIA in my case). <br><br><h2>  Common STN device: young fighter course </h2><br>  One of the problems of convolutional neural networks is that the invariance to the input data is too low: different scale, point of shooting, background noise and <a href="http://cs231n.github.io/classification/">much more</a> .  You can, of course, say that the pooling operation, so not favored by Hinton, gives some invariance, but in fact it simply reduces the size of the feature map, which results in the loss of information. <br><br>  Unfortunately, due to the small receptive field in the standard 2x2 pooling, spatial invariance can only be achieved in deep layers close to the output layer.  Also, pooling does not provide the invariance of rotation and scale.  Kevin Zakka well explained the reason for this in his <a href="https://kevinzakka.github.io/2017/01/18/stn-part2/">post</a> . <br><br>  The main and most common way to make the model resistant to these variations is dataset augmentation, which we did in the <a href="https://habrahabr.ru/company/newprolab/blog/334618/">previous article</a> : <br><br><img src="https://habrastorage.org/web/0ef/79d/ed2/0ef79ded2efe4d01b95bb4a0bd6b6865.gif"><br>  <sub>Augmented images.</sub>  <sub>In this post we will not use augmentation.</sub> <br><br>  There is nothing wrong with this approach, but we would like to develop a smarter and more automated method for image preprocessing, which should help increase the accuracy of the classifier.  Spatial transformer network (STN) - just what we need. <br><br>  Below is another example of how STN works: <br><div style="text-align:center;"><img src="https://habrastorage.org/web/41d/85a/3fa/41d85a3fa7a34717ab1075888bf6ab86.png"></div><br>  <sub>An example from the MNIST dataset from the original article.</sub>  <sub>Cluttered MNIST (left), target, recognized STN (center), transformed image (right).</sub> <br><br>  The work of the STN module can be reduced to the following process (not including training): <br><br><img src="https://habrastorage.org/web/1b9/e2d/cf9/1b9e2dcf9be0401292b06b3925004247.jpeg"><br>  <sub>Application of STN conversion in 4 steps with a known matrix of linear transformations <em>Œ∏</em> .</sub> <br><br>  Now let's take a closer look at this process and each of its stages. <br><br><h2>  STN: conversion steps </h2><br>  <b>Step 1.</b> Determine the transformation matrix <em>Œ∏</em> , which describes the transformation itself: <br><br><img src="https://habrastorage.org/web/856/fbe/bbe/856fbebbe92745958af8aad02d0095c3.png"><br>  <sub>Affine transformation of the matrix <em>Œ∏</em> .</sub> <br><br>  Moreover, each transformation corresponds to its own matrix.  We are interested in the following 4: <br><br><ul><li>  <b>Identical transformation</b> (the output is the same image).  These are our initial <em>Œ∏</em> values.  In this case, the matrix <em>Œ∏ is</em> diagonal: <br><br><pre><code class="python hljs">theta = np.array([[<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>]])</code> </pre> </li><li>  <b>Rotation</b> (counterclockwise, 45¬∫).  <em>cos (45¬∫) = 1 / sqrt (2) ‚âà 0.7:</em> <br><br><pre> <code class="python hljs">theta = np.array([[<span class="hljs-number"><span class="hljs-number">0.7</span></span>, <span class="hljs-number"><span class="hljs-number">-0.7</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0.7</span></span>, <span class="hljs-number"><span class="hljs-number">0.7</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>]])</code> </pre> </li><li>  <b>Approaching</b> .  Approaching the center (2 times): <br><br><pre> <code class="python hljs">theta = np.array([[<span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>]])</code> </pre> </li><li>  <b>Distance</b>  Distance from the center (2 times): <br><br><pre> <code class="python hljs">theta = np.array([[<span class="hljs-number"><span class="hljs-number">2.0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">2.0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>]])</code> </pre> </li></ul><br>  <b>Step 2.</b> Instead of applying the transform directly to the source image ( <em>U</em> ), create a sampling meshgrid of the same size as <em>U.</em>  The sample grid is a set of indices <em>(x_t, y_t)</em> that cover the original image space.  The grid does not contain any information about the color of the images.  This is better explained in the code below: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Implemented in https://github.com/tensorflow/models/blob/master/transformer/spatial_transformer.py # As I mentioned, we only need height and width of the original image def _meshgrid(height, width): with tf.variable_scope('_meshgrid'): x_t = tf.matmul(tf.ones(shape=tf.stack([height, 1])), tf.transpose(tf.expand_dims(tf.linspace(-1.0, 1.0, width), 1), [1, 0])) y_t = tf.matmul(tf.expand_dims(tf.linspace(-1.0, 1.0, height), 1), tf.ones(shape=tf.stack([1, width]))) x_t_flat = tf.reshape(x_t, (1, -1)) y_t_flat = tf.reshape(y_t, (1, -1)) ones = tf.ones_like(x_t_flat) grid = tf.concat(axis=0, values=[x_t_flat, y_t_flat, ones]) return grid</span></span></code> </pre> <br>  Since this is the actual implementation in TensorFlow, in order to understand the general idea, we will translate this code into an analogue of numpy: <br><br><pre> <code class="python hljs">x_t, y_t = np.meshgrid(np.linspace(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, width), np.linspace(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, height))</code> </pre> <br>  <b>Step 3.</b> Apply a linear transformation matrix to the created sample grid to get a new set of points on the grid, each of which can be defined as the result of multiplying the matrix <em>Œ∏</em> by the coordinate vector <em>(x_t, y_t)</em> with a free member: <br><br><img src="https://habrastorage.org/web/184/f38/b69/184f38b6926d4e3282cd6b8d13875cca.png"><br><br>  <b>Step 4.</b> Obtain subsample <i>V</i> using the original feature map, transformed sample grid (see Step 3) and a differentiated interpolation function of your choice (for example, bilinear).  Interpolation is necessary because we need to translate the result of the sampling (potentially possible fractional pixel values) into whole numbers. <br><br><img src="https://habrastorage.org/web/8a6/aba/c50/8a6abac5006247f1bec36b64cd114cee.jpeg"><br>  <sub>Sampling and interpolation</sub> <br><br>  <b>The task of learning</b> .  Generally speaking, if we knew in advance the necessary <em>Œ∏</em> values ‚Äã‚Äãfor each source image, we could begin the process described above.  In fact, we would like to extract <em>Œ∏</em> from the data using machine learning.  This is quite realistic.  <b>First</b> , we need to make sure that the loss function of the road sign classifier can be minimized using backprop through the sampler.  <b>Secondly</b> , we find gradients in <em>U</em> and <em>G</em> (meshgrid): this is why the interpolation function must be differentiable or, at least, partially differentiable.  <b>Third</b> , we calculate the partial derivatives of <em>x</em> and <em>y with</em> respect to <em>Œ∏</em> .  Technical calculations can be read in the original <a href="http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf">paper.</a> <br><br>  Finally, we create <b>LocNet (a localizing network-regressor)</b> , the only task of which is to train and predict the correct <em>Œ∏</em> for the input image using the loss function, which was minimized through total backprop. <br><br>  <b>The main advantage of this approach is that we get a differentiable stand-alone module with memory (in the form of trained weights), which can be placed in any part of CNN.</b> <br><div style="text-align:center;"><img src="https://habrastorage.org/web/d27/9c7/858/d279c78582d241bd9924b93fbdb3826d.gif"></div><br>  <sub>Notice how <em>Œ∏</em> changes as STN learns to recognize the target object (road sign) in the images.</sub> <br><br>  Below is a diagram of the STN from the original article: <br><div style="text-align:center;"><img src="https://habrastorage.org/web/bb2/960/aed/bb2960aedae64cb48bacd8d09aadba9d.png"></div><br>  We reviewed all the steps involved in building an STN: the creation of LocNet, a sample mesh generator (meshgrid) and a sampler.  Now we will build and train on TensorFlow the entire classifier, which includes in its graph and STN. <br><br><h2>  Build a model in TensorFlow </h2><br>  The entire model code, of course, does not fit into the framework of a single article, but it is available as a Jupyter laptop in the <a href="https://github.com/dnkirill/stn_idsia_convnet">repository on GitHub</a> . <br><br>  In this article, I focus on some important parts of the code and the stages of learning the model. <br><br>  <b>First,</b> our ultimate goal is to learn to recognize road signs, and to achieve it we need to create some kind of classifier and train it.  We have many options: from LeNet to any other SOTA-neural network.  In the process of working on the project, inspired by the <a href="https://github.com/Moodstocks/gtsrb.torch">work of Moodstocks on STN</a> (implemented in Torch), I used the IDSIA neural network architecture, although nothing prevented me from taking something else. <br><br>  <b>At the second stage,</b> we need to identify and train the STN module, which, taking the original image as input, converts it using a sampler and the output is a new image (or minibatch, if we work in batch mode), which in turn is used by the classifier. <br>  I note that STN can be easily removed from the graph of calculations, replacing the entire module with a simple generator of batch.  In this case, we just get the usual classifier network. <br>  Here is the general scheme of operation of the obtained double neural network: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/165/71f/16d/16571f16d7164a47a58ed90a0072c857.png"></div><br>  <sub>The STN converts the original images and feeds them to the IDSIA input, which is trained using backprop and then classifies the road signs.</sub> <br><br>  Below is the part of the DAG, in which the original images are converted using STN and fed to the input of the classifier (IDSIA), which calculates the logits: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">stn_idsia_inference_type2</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(batch_x)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.name_scope(<span class="hljs-string"><span class="hljs-string">'stn_network_t2'</span></span>): <span class="hljs-comment"><span class="hljs-comment"># Unrolling the STN's LocNet -- stn_output is theta-matrix stn_output = stn_LocNet_type2(stn_convolve_pool_flatten_type2(batch_x)) # Grid generator and sampler transformed_batch_x = transformer(batch_x, stn_output, (32,32, TF_CONFIG['channels'])) with tf.name_scope('idsia_classifier'): # IDSIA uses transformed_batch_x from STN. Here we unroll the conv layers of IDSIA features, batch_act = idsia_convolve_pool_flatten(transformed_batch_x, multiscale=True) # Unrolling FC layers of IDSIA logits = idsia_fc_logits(features, multiscale=True) # Returning lots of objects. `logits` is the one that is really required for the model return logits, transformed_batch_x, batch_act</span></span></code> </pre> <br>  Now that we know the logit calculation method (STN + IDSIA network), the next step is to optimize the loss function (for which we will use cross-entropy or log loss ‚Äî the standard choice for solving the classification problem): <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">calculate_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(logits, one_hot_y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.name_scope(<span class="hljs-string"><span class="hljs-string">'Predictions'</span></span>): predictions = tf.nn.softmax(logits) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.name_scope(<span class="hljs-string"><span class="hljs-string">'Model'</span></span>): cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.name_scope(<span class="hljs-string"><span class="hljs-string">'Loss'</span></span>): loss_operation = tf.reduce_mean(cross_entropy) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> loss_operation</code> </pre> <br>  Then we need to set operations (ops) for optimization and training, which should propagate errors back to the input layers: <br><br><pre> <code class="python hljs">boundaries = [<span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">250</span></span>, <span class="hljs-number"><span class="hljs-number">500</span></span>, <span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-number"><span class="hljs-number">8000</span></span>] values = [<span class="hljs-number"><span class="hljs-number">0.02</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, <span class="hljs-number"><span class="hljs-number">0.005</span></span>, <span class="hljs-number"><span class="hljs-number">0.003</span></span>, <span class="hljs-number"><span class="hljs-number">0.001</span></span>, <span class="hljs-number"><span class="hljs-number">0.0001</span></span>] starter_learning_rate = <span class="hljs-number"><span class="hljs-number">0.02</span></span> global_step = tf.Variable(<span class="hljs-number"><span class="hljs-number">0</span></span>, trainable=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) learning_rate = tf.train.piecewise_constant(global_step, boundaries, values) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.name_scope(<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>): accuracy_operation = tf.reduce_mean(casted_corr_pred) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.name_scope(<span class="hljs-string"><span class="hljs-string">'loss_calculation'</span></span>): loss_operation = calculate_loss(logits, one_hot_y) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.name_scope(<span class="hljs-string"><span class="hljs-string">'adam_optimizer'</span></span>): optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.name_scope(<span class="hljs-string"><span class="hljs-string">'training_backprop_operation'</span></span>): training_operation = optimizer.minimize(loss_operation, global_step=global_step)</code> </pre> <br>  I initialized a network with a high value of learning rate (0.02) so that gradients could more quickly spread information to the LocNet STN, which is located in the outer layers of the entire neural network.  Otherwise, this network will learn more slowly (due to the <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">problem of the ‚Äúdisappearing‚Äù gradient</a> ).  Small initial learning rate values ‚Äã‚Äãdo not allow neural networks to bring closer small road signs in the image. <br><br>  The part of the DAG that calculates logits (network output) is added to the graph quite simply: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.name_scope(<span class="hljs-string"><span class="hljs-string">'batch_data'</span></span>): x = tf.placeholder(tf.float32, (<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>, TF_CONFIG[<span class="hljs-string"><span class="hljs-string">'channels'</span></span>]), name=<span class="hljs-string"><span class="hljs-string">"InputData"</span></span>) y = tf.placeholder(tf.int32, (<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>), name=<span class="hljs-string"><span class="hljs-string">"InputLabels"</span></span>) one_hot_y = tf.one_hot(y, n_classes, name=<span class="hljs-string"><span class="hljs-string">'InputLabelsOneHot'</span></span>) <span class="hljs-comment"><span class="hljs-comment">#### INIT with tf.name_scope('logits_and_stn_output'): logits, stn_output, batch_act = stn_idsia_inference_type2(x)</span></span></code> </pre> <br>  A piece of code above deploys the entire network - STN + IDSIA, we will discuss them in more detail below. <br><br><h2>  IDSIA: Classifier Network </h2><br>  Inspired by the work of Moodstocks and the original <a href="http://people.idsia.ch/~juergen/nn2012traffic.pdf">article</a> from the IDSIA Swiss AI Group, in which they used CNN to improve the previously achieved quality of the model, I took the general idea of ‚Äã‚Äãthe architecture of one network from the ensemble and implemented it in TensorFlow myself.  The resulting structure of the classifier is as follows: <br><br><ul><li>  Layer 1: Convolutional (batch normalization, relu, dropout).  <b>Kernel</b> : 7x7, 100 filters.  <b>At the entrance</b> : 32x32x1 (In a set of 256).  <b>At the exit</b> : 32x32x100. </li><li>  Layer 2: Max Pooling.  <b>To the entrance</b> : 32x32x100.  <b>At the exit</b> : 16x16x100. </li><li>  Layer 3: Convolutional (batch normalization, relu, dropout).  <b>Kernel</b> : 5x5, 150 filters.  <b>At the entrance</b> : 16x16x100 (in a batch of 256).  <b>At the exit</b> : 16x16x150. </li><li>  Layer 4: Max Pooling.  <b>At the entrance</b> : 16x16x150.  <b>At the exit</b> : 8x8x150. </li><li>  Layer 5: Convolutional (batch normalization, relu, dropout).  <b>Kernel</b> : 5x5, 250 filters.  <b>At the entrance</b> : 16x16x100 (in a set of 256).  <b>At the exit</b> : 16x16x150. </li><li>  Layer 6: Max Pooling.  <b>At the entrance</b> : 8x8x250.  <b>Output</b> : 4x4x250. </li><li>  Layer 7: Optional pooling for multiscale features.  Kernels: 8, 4, 2 for layers 1, 2 and 3, respectively. </li><li>  Layer 8: Extending and concatenating features into a multiscale feature vector.  <b>At the entrance</b> : 2x2x100;  2x2x150;  2x2x250.  <b>At the exit</b> : feature vector 400 + 600 + 1000 = 2000 for fully connected layers </li><li>  Layer 9: Fully-connected (batch normalization, relu, dropout).  <b>At the entrance</b> : 2000 signs (in a set of 256).  300 neurons. </li><li>  Layer 10: Logits (batch normalization).  <b>At the entrance</b> : 300 signs.  <b>At the exit</b> : logit (43 classes). </li></ul><br>  All this is illustrated below: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/c15/7cf/425/c157cf425160484796b158447120b836.png"></div><br><br>  As can be seen, the results of the activation functions of each convolutional layer are combined into one vector, which is already served by fully connected layers.  This is an example of <b>multiscale-features</b> that further improve the quality of the classifier. <br><br>  The converted STN image is fed to the input of <em>conv1</em> , as we discussed earlier. <br><br><h2>  Spatial Transformers in TensorFlow </h2><br>  Among the variety of models TensorFlow you can find the <a href="https://github.com/tensorflow/models/tree/master/transformer">implementation of STN</a> , which will be used in our network. <br><br>  Our task is to identify and train LocNet, provide the <em>transformer with the</em> correct values ‚Äã‚Äãof <em>Œ∏</em> and insert the STN module into the DAG Tensorflow.  <em>The transformer</em> generates a grid and provides transformation and interpolation. <br><br>  The LocNet configuration is shown below: <br><br>  <b>LocNet convolutional layers:</b> <br><br><ul><li>  Layer 1: Max Pooling.  <b>At the entrance</b> : 32x32x1.  <b>At the exit</b> : 16x16x1. </li><li>  Layer 2: Convolutional (relu, batch normalization).  <b>Kernel</b> : 5x5, 100 filters.  <b>Input</b> : 16x16x1 (in a set of 256).  <b>At the exit</b> : 16x16x100. </li><li>  Layer 3: Max Pooling.  <b>At the entrance</b> : 16x16x100.  <b>At the exit</b> : 8x8x100. </li><li>  Layer 4: Convolutional (batch normalization, relu).  <b>Kernel</b> : 5x5, 200 filters.  <b>At the entrance</b> : 8x8x100 (in a set of 256).  <b>At the exit</b> : 8x8x200. </li><li>  Layer 5: Max Pooling.  <b>At the entrance</b> : 8x8x200.  <b>At the exit</b> : 4x4x200. </li><li>  Layer 6: Optional pooling for multiscale features.  Kernels: 4, 2 for convolutional layers 1 and 2, respectively. </li><li>  Layer 7: Extruding and concatenating features into a vector <b>.</b>  <b>At the entrance</b> : 2x2x100;  2x2x200.  <b>At the exit</b> : vector of features with dimension 400 + 800 = 1200 for fully connected layers. </li></ul><br>  <b>Fully-connected part of LocNet:</b> <br><br><ul><li>  Layer 8: Fully-connected (batch normalization, relu, dropout).  <b>At the entrance</b> : 1200 signs (in a set of 256).  100 neurons. </li><li>  Layer 9: 2x3 matrix <em>Œ∏</em> , which defines an affine transformation.  Weights are given by zeros, the free term is a matrix similar to the unit one, with units on the main diagonal: [[1.0, 0, 0], [0, 1.0, 0]]. </li><li>  Layer 10: Transformer: Grid generator and sampler implemented in spatial_transformer.py.  This layer produces images with the same dimensions as the original (32x32x1), applying an affine transformation to them (thus, an approximate or rotated image is obtained). </li></ul><br>  The structure of convolutional layers LocNet is similar to IDSIA (although LocNet consists of 2 layers instead of 3, and in it we first do the pulling).  More curious is the structure of fully connected layers: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/4a8/329/516/4a8329516a314d9f8b96100c31e60257.png"></div><br><br><h2>  Training and Results </h2><br>  The problem with using the STN module with CNN is the need to ensure that both networks are not retrained, which makes the learning process difficult and unstable.  On the other hand, adding a small amount of augmented data (especially brightness augmentation) to a training set allows networks not to retrain.  In any case, the advantages outweigh the disadvantages: <b>even without augmentation,</b> we get good results, and STN + IDSIA outperform IDSIA without this module by 0.5-1%. <br><br>  In the process of learning the following parameters were used: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># TF Parameters TF_CONFIG = { 'epochs': 20, 'batch_size': 256, 'channels': 1 } # Omitting the model building phase (discussed earlier in this post) # ... # Train / validation datasets, see the previous post. # No augmentations. train_val_data = { 'X_train': X_tr_256, 'y_train': y_tr_256, 'X_valid': X_val_256, 'y_valid': y_val_256 } # Initializing the session and vars: sess = tf.InteractiveSession() sess.run(tf.global_variables_initializer()) # Skipping details and going to the training: for i in range(TF_CONFIG['epochs']): for batch_x, batch_y in batch_generator(train_val_data['X_train'], train_val_data['y_train'], batch_size=TF_CONFIG['batch_size']): _, loss, lr = sess.run([training_operation, loss_operation, learning_rate], feed_dict={x: batch_x, y: batch_y, dropout_conv: 1.0, dropout_loc: 0.9, dropout_fc1: 0.3}</span></span></code> </pre> <br>  After 10 epochs, we get an accuracy of 99.3% on the validation data set.  CNN is still being retrained, but do not forget that we use a double complex grid on the original dataset without its expansion by augmentation.  In truth, by adding augmentation, I managed to get an accuracy of 99.6% on the validation set after 10 iterations (although the training time significantly increased). <br><br>  Below are the model learning results ( <em>idsia_1</em> is an IDSIA network without a module, <em>idsia_stn</em> is an STN + IDSIA).  This is the accuracy of the entire network for validation. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/244/508/9c5/2445089c5824429398bc80ea1dfb3e4c.png"></div><br><br>  STN + IDSIA performs better than the IDSIA network without a module, although it takes longer to learn.  I note that the accuracy in the graph above is calculated for the batch, and not for all validation. <br><br>  Finally, here is the result of the STN transformer after training: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/7f3/c61/e5b/7f3c61e5bfb04f518f29dd648346b867.png"></div><br><br>  Well, we summarize: <br><br><ul><li>  STN is a differentiable module that can be integrated into a convolutional neural network.  Standard use case is to place it immediately after the batch generator, so that it can train the transformation matrix <em>Œ∏</em> , which minimizes the loss function of the main classifier (IDSIA in our case). </li><li>  The STN sampler applies an affine transformation to the source images (or feature map). </li><li>  STN can be considered as an alternative to image augmentation, which is a standard way to achieve spatial invariance for CNN. </li><li>  Adding one or more STN modules to CNN complicates learning, makes it unstable: now you need to make sure that both (instead of one) networks are not retrained.  As it seems to me, this is one of the reasons why STN is not so common. </li><li>  STNs trained on augmented data (especially brightness augmentation) show better quality and do not retrain too much. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/web/c30/156/dd9/c30156dd9bb14551942e09c342c44e3f.gif"></div></div><p>Source: <a href="https://habr.com/ru/post/339484/">https://habr.com/ru/post/339484/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../339474/index.html">Testing in Openshift: Openstack Integration</a></li>
<li><a href="../339476/index.html">Let them in passwords</a></li>
<li><a href="../339478/index.html">GDG community at Google Developer Days</a></li>
<li><a href="../339480/index.html">XSLT Web Vulnerability: Server Side Injection</a></li>
<li><a href="../339482/index.html">How to kill your network with Ansible</a></li>
<li><a href="../339488/index.html">How to involve users in the gaming world: contests and interactivity in social networks. Cases of the Krasnodar studio Plarium</a></li>
<li><a href="../339490/index.html">Step-by-step instructions for creating an SMS web application using the SMS API from Infobip</a></li>
<li><a href="../339492/index.html">C / C ++ code optimization</a></li>
<li><a href="../339496/index.html">Fuel for AI: a selection of open datasets for machine learning</a></li>
<li><a href="../339498/index.html">Overview of one Russian RTOS, part 7. Means of data exchange between tasks</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>