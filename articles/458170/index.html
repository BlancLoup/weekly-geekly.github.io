<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Dipping into convolutional neural networks. Part 5/10 - 18</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Full course in Russian can be found at this link . 
 The original English course is available here . 



 The release of new lectures is scheduled eve...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Dipping into convolutional neural networks. Part 5/10 - 18</h1><div class="post__text post__text-html js-mediator-article"><p>  Full course in Russian can be found at <a href="https://www.youtube.com/playlist%3Flist%3DPLfdVzZl6HHg9y9l6U5xUjqKS13rWoQPF4">this link</a> . <br>  The original English course is available <a href="https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187">here</a> . </p><br><p><img src="https://habrastorage.org/webt/1m/hz/qn/1mhzqnwa288uxjmptqhexriiahc.png"><br>  <i>The release of new lectures is scheduled every 2-3 days.</i> </p><a name="habracut"></a><br><h1>  Content </h1><br><ol><li>  Interview with Sebastian Trun </li><li>  Introduction </li><li>  Cats and cats data set </li><li>  Images of various sizes </li><li>  Color images.  Part 1 </li><li>  Color images.  Part 2 </li><li>  Convolution operation on color images </li><li>  The subsample operation on the maximum value on color images </li><li>  CoLab: cats and dogs </li><li>  Softmax and sigmoid </li><li>  Check </li><li>  Image extension </li><li>  An exception </li><li>  CoLab: dogs and cats.  Reiteration </li><li>  Other techniques to prevent retraining </li><li>  Exercise: classification of color images </li><li>  Results </li></ol><br><h1>  Softmax and Sigmoid </h1><br><p>  In the past practical CoLab, we used the following convolutional neural network architecture: </p><br><pre><code class="python hljs">model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">32</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">150</span></span>, <span class="hljs-number"><span class="hljs-number">150</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>)), tf.keras.layers.MaxPooling2D(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">64</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), tf.keras.layers.MaxPooling2D(<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>), tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">128</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), tf.keras.layers.MaxPooling2D(<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>), tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">128</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), tf.keras.layers.MaxPooling2D(<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>), tf.keras.layers.Flatten(), tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">512</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">2</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>) ])</code> </pre> <br><p>  Please note that our last layer (our classifier) ‚Äã‚Äãconsists of a fully connected layer with two output neurons and the <code>softmax</code> activation <code>softmax</code> : </p><br><pre> <code class="python hljs">tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">2</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)</code> </pre> <br><p>  Another popular approach to solving binary classification problems is the use of a classifier, which consists of a fully connected layer with 1 output neuron and the <code>sigmoid</code> activation <code>sigmoid</code> : </p><br><pre> <code class="python hljs">tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)</code> </pre> <br><p>  Both of these options will work well in the binary classification problem.  However, it is worth bearing in mind that if you decide to use the <code>sigmoid</code> activation <code>sigmoid</code> in your classifier, you will also need to change the loss function in the <code>model.compile()</code> method from <code>sparse_categorical_crossentropy</code> to <code>binary_crossentropy</code> as in the example below: </p><br><pre> <code class="python hljs">model.compile(optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'binary_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> <br><h1>  Validation </h1><br><p>  In previous sessions, we studied the accuracy of our convolutional neural networks using the <code>accuracy</code> metric on a test dataset.  When we developed the convolutional neural network to classify images from the FASHION MNIST dataset, we obtained 97% accuracy on the training data set and only 92% accuracy on the test data set.  All this happened because our model was retrained.  In other words, our convolutional neural network began to memorize the training data set.  However, we were able to learn about retraining only <em>after</em> we had conducted training and testing of the model on the available data by comparing the accuracy of work on the training data set and the test data set. </p><br><p>  To avoid this problem, we often use the data set for validation: </p><br><p><img src="https://habrastorage.org/webt/mj/fw/ea/mjfweaqi7ztkkfbmznz1nfnogjq.png"></p><br><p>  During training, our convolutional neural network "sees" only the training data set and makes decisions on how to change the values ‚Äã‚Äãof the internal parameters - weights and offsets.  After each training iteration, we check the state of the model by calculating the value of the loss function on the training data set and on the validation data set.  It is worth noting and paying special attention to the fact that the data from the validation set are not used anywhere by the model to adjust the values ‚Äã‚Äãof internal parameters.  Checking the accuracy of the model on the validation data set only tells us how well our model is working on this very data set.  Thus, the results of the model on the validation data set tell us how well our model learned how to summarize the data and apply this generalization on the new data set. </p><br><p>  The idea is that since we do not use a validation dataset when training a model, testing the model on the validation set will allow us to understand whether the model has been retrained or not. </p><br><p>  Let's take an example. </p><br><p>  In CoLab, which we performed several points above, we trained our neural network for 15 iterations. </p><br><pre> <code class="plaintext hljs">Epoch 15/15 10/10 [===] - loss: 1.0124 - acc: 0.7170 20/20 [===] - loss: 0.0528 - acc: 0.9900 - val_loss: 1.0124 - val_acc: 0.7070</code> </pre> <br><p>  If we look at the accuracy of the predictions on the training and validation data sets on the fifteenth training iteration, then we can see that we achieved high accuracy on the training data set and a significantly low figure on the validation data set - <code>0.9900</code> against <code>0.7070</code> . </p><br><p>  This is an obvious sign of retraining.  The neural network has memorized the training data set, therefore it works with incredible accuracy on the input data from it.  However, as soon as the case shifts to checking accuracy on a validation data set that the model has not ‚Äúseen‚Äù, the results are significantly reduced. </p><br><p>  One way to avoid overtraining is to closely examine the loss function graph in the training and validation datasets throughout the training iterations: </p><br><p><img src="https://habrastorage.org/webt/mo/bb/ml/mobbml4mb5sqt1ugcjjmj69ziau.png"></p><br><p>  In CoLab, we built a similar graph and got something similar to the above graph of the loss function versus the training iteration. </p><br><p>  It can be seen that after a certain training iteration, the value of the loss function on the validation data set begins to increase, while the value of the loss function on the training data set continues to decrease. </p><br><p>  At the end of the 15th training iteration, we notice that the value of the loss function on the validation data set is extremely high, and the value of the loss function on the training data set is extremely small.  Actually, this is the very indicator of the re-learning of the neural network. </p><br><p>  Carefully looking at the graph, one can understand that literally after several training iterations, our neural network begins to simply memorize training data, which means the model's ability to generalize decreases, which leads to a deterioration in accuracy on the validation data set. </p><br><p>  As you probably already understood, the validation data set allows us to determine the number of training iterations that need to be carried out in order for our convolutional neural network to be accurate and, at the same time, not to retrain. </p><br><p>  Such an approach can be extremely useful if we have a choice of several architectures of convolutional neural networks: </p><br><p><img src="https://habrastorage.org/webt/ay/5j/lq/ay5jlqtty3bwks_zdjclydfgumq.png"></p><br><p>  For example, if you decide on the number of layers in a convolutional neural network, you can create several neural network architectures and then compare their accuracy using a data set for validation. </p><br><p>  The neural network architecture, which allows you to achieve the minimum value of the loss function and will be the best to solve your problem. </p><br><p>  The next question you might have is why create a validation data set if we already have a test data set?  Can we use a test dataset for validation? </p><br><p>  The problem is that despite the fact that we do not use the validation data set in the model training process, we use the results of the work on the test data set to improve the accuracy of the model, which means that the test data set affects the weights and displacements in the neural network. </p><br><p><img src="https://habrastorage.org/webt/ys/7a/ih/ys7aihyxutwvpilqpnsaczev5eo.png"></p><br><p>  It is for this reason that we need a validation dataset, which our model has never seen before to accurately check the effectiveness of its work. </p><br><p>  We have just figured out how a validation dataset can help us avoid retraining.  In the following parts we will talk about data expansion (the so-called augmentation) and disconnections (the so-called dropout) of neurons - two popular techniques that can also help us avoid retraining. </p><br><h1>  Image extension (augmentation) </h1><br><p>  By training neural networks to define objects of a particular class, we want our neural network to find these objects regardless of their location and size in the image. </p><br><p>  For example, imagine that we want to train our neural network to recognize dogs in images: </p><br><p><img src="https://habrastorage.org/webt/0y/d2/cu/0yd2cuip0aaef2hi8auegdq6mt8.png"></p><br><p>  Thus, we want our neural network to determine the presence of a dog in the image, regardless of what size the dog is and in which part of the image it is, whether part of the dog is visible or the entire dog.  We want to make sure that our neural network can process all these options during training. </p><br><p>  If you are very lucky enough and you have a large set of training data, then we can safely say that you are lucky and your neural network is less likely to retrain.  However, which is quite often the case, we will have to work with a limited set of images (training data), which, in turn, will lead our convolutional neural network with high probability to retrain and reduce its ability to summarize and output the desired result to data that it does not "seen" earlier. </p><br><p>  This problem can be solved using the technique called "extension" (image augmentation).  The expansion of images (data) works by creating (generating) new images for learning by applying arbitrary transformations of the original set of images from the training sample. </p><br><p>  For example, we can take one of the original images from our training data set and apply several arbitrary transformations to it ‚Äî flip it by X degrees, flip horizontally and produce an arbitrary increase. </p><br><p><img src="https://habrastorage.org/webt/nd/ps/fv/ndpsfvldpaymeybswwqkmyspr40.png"></p><br><p>  By adding the generated images to our training data set, we thereby ensure that our neural network "sees" a sufficient number of different examples for learning.  As a result of such actions, our convolutional neural network will better generalize and work on the data that it has not yet seen and we will be able to avoid retraining. </p><br><p>  In the next section, we will learn what a dropout is ‚Äî another technique for preventing model retraining. </p><br><h1>  Exception (dropout) </h1><br><p>  In this part, we will learn a new technique - a dropout, which will also help us avoid retraining the model.  As we already know from the early parts, the neural network optimizes internal parameters (weights and displacements) to minimize the loss function. </p><br><p>  One of the problems that can be encountered while learning a neural network is huge values ‚Äã‚Äãin one part of the neural network and small values ‚Äã‚Äãin another part of the neural network. </p><br><p><img src="https://habrastorage.org/webt/op/bb/cf/opbbcfp4z1wl_geilpygynmvdz8.png"></p><br><p>  As a result, it turns out that neurons with greater weights play <strong>a</strong> greater role in the learning process, while neurons with smaller weights cease to be significant and less and less subject to change.  One way to avoid this is to use an arbitrary dropout of neurons. </p><br><p>  Dropout - the process of selectively turning off neurons in the learning process. </p><br><p><img src="https://habrastorage.org/webt/8j/7t/0p/8j7t0p91wpjv2qswrxugif3b2e4.png"></p><br><p>  Selective disconnection of some neurons in the learning process allows active use of other neurons in learning.  In the process of learning iterations, we arbitrarily disable some neurons. </p><br><p>  Let's look at an example.  Imagine that at the first training iteration we disable two black neurons highlighted in black: </p><br><p><img src="https://habrastorage.org/webt/rv/f7/uv/rvf7uvlolq1t38zvhi50mqw7hgg.png"></p><br><p>  The processes of direct propagation and reverse propagation occur without the use of selected two neurons. </p><br><p>  On the second training iteration, we decide not to use the following three neurons - turn them off: </p><br><p><img src="https://habrastorage.org/webt/lq/dq/k2/lqdqk2gkxaagbvcpgwsybs8i2wq.png"></p><br><p>  As in the previous case, we do not use these three neurons in the processes of direct and reverse propagation.  At the last, third learning iteration, we decide not to use these two neurons: </p><br><p><img src="https://habrastorage.org/webt/dk/jf/ac/dkjfacty06iu6r6aopxq9by5qs4.png"></p><br><p>  And in this case, in the processes of direct and reverse propagation, we do not use disconnected neurons.  And so on. </p><br><p>  By training our neural network so we can avoid retraining.  We can say that our neural network becomes more stable, because with this approach it cannot rely absolutely on all neurons to solve the problem.  Thus, other neurons begin to take a more active part in the formation of the required output value and also begin to cope with the task. </p><br><p>  In practice, this approach requires specifying the probability of excluding each of the neurons at any learning iteration.  Please note that by indicating the likelihood we may be in a situation where some neurons will shut down more often than others, and some may not be turned off at all.  However, this is not a problem, because this process is performed many times and, on average, each neuron can be turned off with the same probability. </p><br><p>  Now let's apply our theoretical knowledge in practice and refine our classifier of cats and dogs images. </p><br><h1>  CoLab: dogs and cats.  Reiteration </h1><br><p>  CoLab in English is available at <a href="https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l05c02_dogs_vs_cats_with_augmentation.ipynb">this link</a> . <br>  CoLab in Russian is available at <a href="https://colab.research.google.com/drive/1UAq-715_utvy-AbTJByStiOHr7YS0UWU">this link</a> . </p><br><h2 id="koshki-vs-sobaki-klassifikaciya-izobrazheniy-s-rasshireniem">  Cats VS Dogs: classification of images with the extension </h2><br><p>  In this tutorial we will discuss how to classify images of cats and dogs.  We will develop an image classifier using the <code>tf.keras.Sequential</code> model, and we will use <code>tf.keras.preprocessing.image.ImageDataGenerator</code> to load the data. </p><br><h3 id="idei-kotorye-budut-zatronuty-v-etoy-chasti">  Ideas to be covered in this section: </h3><br><p>  We will gain practical experience in developing a classifier and develop an intuitive understanding of the following concepts: </p><br><ol><li>  Building a data flow model ( <em>data input pipelines</em> ) using the <code>tf.keras.preprocessing.image.ImageDataGenerator</code> class (How to effectively work with data on disk interacting with the model?) </li><li>  Retraining - what is it and how to define it? </li><li>  Data extension (data augmentation) and the exception method (dropout) are key techniques in the fight against retraining in pattern recognition tasks that we will implement in our model learning process. </li></ol><br><h4 id="my-budem-sledovat-osnovnomu-podhodu-pri-razrabotke-modeley-mashinnogo-obucheniya">  We will follow the basic approach when developing machine learning models: </h4><br><ol><li>  Explore and understand data </li><li>  Configure Input Stream </li><li>  Build model </li><li>  To train a model </li><li>  Test model </li><li>  Improve Model / Repeat Process </li></ol><br><p>  <strong>Before we start ...</strong> </p><br><p>  Before running the code in the editor, we recommend that you reset all settings in <strong>Runtime -&gt; Reset all</strong> in the top menu.  Such an action will avoid problems with low memory, if you are working in parallel or working with multiple editors. </p><br><h1 id="importirovanie-paketov">  Importing packages </h1><br><p>  Let's start by importing the right packages: </p><br><ul><li>  <code>os</code> - reading files and directory structures; </li><li>  <code>numpy</code> - for some matrix operations outside TensorFlow; </li><li>  <code>matplotlib.pyplot</code> - plotting and displaying images from a test and validation data set. </li></ul><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> __future__ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> absolute_import, division, print_function, unicode_literals <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np</code> </pre> <br><p>  Import <code>TensorFlow</code> : </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.keras.preprocessing.image <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ImageDataGenerator</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> logging logger = tf.get_logger() logger.setLevel(logging.ERROR)</code> </pre> <br><h1 id="zagruzka-dannyh">  Data loading </h1><br><p>  We start the development of our classifier by loading a data set.  The dataset we use is a filtered version of the <a href="https://www.kaggle.com/c/dogs-vs-cats/data">Dogs vs Cats</a> dataset from Kaggle (after all, this dataset is provided by Microsoft Research). </p><br><p>  In the past, CoLab and I used a data set from the <a href="https://www.tensorflow.org/datasets">TensorFlow Dataset</a> module itself, which is extremely convenient for work and testing.  In this CoLab, however, we will use the <code>tf.keras.preprocessing.image.ImageDataGenerator</code> class to read data from disk.  Therefore, we first need to download the VS Cats Dogs dataset and unzip it. </p><br><pre> <code class="python hljs">_URL = <span class="hljs-string"><span class="hljs-string">'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'</span></span> zip_dir = tf.keras.utils.get_file(<span class="hljs-string"><span class="hljs-string">'cats_and_dogs_filterted.zip'</span></span>, origin=_URL, extract=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><p>  The dataset we downloaded has the following structure: </p><br><pre> <code class="plaintext hljs">cats_and_dogs_filtered |__ train |______ cats: [cat.0.jpg, cat.1.jpg, cat.2.jpg ...] |______ dogs: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...] |__ validation |______ cats: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ...] |______ dogs: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...]</code> </pre> <br><p>  To get a full list of director you can use the following command: </p><br><pre> <code class="python hljs">zip_dir_base = os.path.dirname(zip_dir) !find $zip_dir_base -type d -<span class="hljs-keyword"><span class="hljs-keyword">print</span></span></code> </pre> <br><p>  Output (when starting from CoLab): </p><br><pre> <code class="plaintext hljs">/root/.keras/datasets /root/.keras/datasets/cats_and_dogs_filtered /root/.keras/datasets/cats_and_dogs_filtered/train /root/.keras/datasets/cats_and_dogs_filtered/train/dogs /root/.keras/datasets/cats_and_dogs_filtered/train/cats /root/.keras/datasets/cats_and_dogs_filtered/validation /root/.keras/datasets/cats_and_dogs_filtered/validation/dogs /root/.keras/datasets/cats_and_dogs_filtered/validation/cats</code> </pre> <br><p>  Now, assign the variables the correct paths to directories with data sets for training and validation: </p><br><pre> <code class="python hljs">base_dir = os.path.join(os.path.dirname(zip_dir), <span class="hljs-string"><span class="hljs-string">'cats_and_dogs_filtered'</span></span>) train_dir = os.path.join(base_dir, <span class="hljs-string"><span class="hljs-string">'train'</span></span>) validation_dir = os.path.join(base_dir, <span class="hljs-string"><span class="hljs-string">'validation'</span></span>) train_cats_dir = os.path.join(train_dir, <span class="hljs-string"><span class="hljs-string">'cats'</span></span>) train_dogs_dir = os.path.join(train_dir, <span class="hljs-string"><span class="hljs-string">'dogs'</span></span>) validation_cats_dir = os.path.join(validation_dir, <span class="hljs-string"><span class="hljs-string">'cats'</span></span>) validation_dogs_dir = os.path.join(validation_dir, <span class="hljs-string"><span class="hljs-string">'dogs'</span></span>)</code> </pre> <br><h4 id="razbiraemsya-s-dannymi-i-ih-strukturoy">  We understand the data and their structure </h4><br><p>  Let's see how many images of cats and dogs we have in the test and validation data sets (directories). </p><br><pre> <code class="python hljs">num_cats_tr = len(os.listdir(train_cats_dir)) num_dogs_tr = len(os.listdir(train_dogs_dir)) num_cats_val = len(os.listdir(validation_cats_dir)) num_dogs_val = len(os.listdir(validation_dogs_dir)) total_train = num_cats_tr + num_dogs_tr total_val = num_cats_val + num_dogs_val</code> </pre> <br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">'    : '</span></span>, num_cats_tr) print(<span class="hljs-string"><span class="hljs-string">'    : '</span></span>, num_dogs_tr) print(<span class="hljs-string"><span class="hljs-string">'    : '</span></span>, num_cats_val) print(<span class="hljs-string"><span class="hljs-string">'    : '</span></span>, num_dogs_val) print(<span class="hljs-string"><span class="hljs-string">'--'</span></span>) print(<span class="hljs-string"><span class="hljs-string">'     : '</span></span>, total_train) print(<span class="hljs-string"><span class="hljs-string">'     : '</span></span>, total_val)</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">    : 1000     : 1000     : 500     : 500 --      : 2000      : 1000</code> </pre> <br><h1 id="ustanovka-parametrov-modeli">  Setting Model Parameters </h1><br><p>  For convenience, we will make setting variables that we need for further data processing and model training, in a separate declaration: </p><br><pre> <code class="python hljs">BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">100</span></span> <span class="hljs-comment"><span class="hljs-comment">#          IMG_SHAPE = 150 #       </span></span></code> </pre> <br><h1 id="rasshirenie-dannyh">  Data extension </h1><br><p>  Re-training usually occurs when there are few training examples in our data set.  One of the ways to eliminate the lack of data is to expand them to the required number of instances and the desired variability.  Data expansion is the process of generating data from existing instances by applying various transformations to the original data set.  The goal of this method is to increase the number of unique input instances that the model will never see again, which, in turn, will allow the model to better generalize the input data and show greater accuracy on the validation data set. </p><br><p>  Using <strong><code>tf.keras</code></strong> we can implement similar random transformations and the generation of new images through the <strong><code>ImageDataGenerator</code></strong> class.  It will be enough for us to pass in the form of parameters various transformations that we would like to apply to the images, while the class itself will take care of the rest during the training of the model. </p><br><p>  To begin, let's write a function that will display images obtained as a result of random transformations.  Then we take a closer look at the transformations used in the process of expanding the original data set. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plotImages</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(images_arr)</span></span></span><span class="hljs-function">:</span></span> fig, axes = plt.subplots(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">20</span></span>,<span class="hljs-number"><span class="hljs-number">20</span></span>)) axes = axes.flatten() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> img, ax <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(images_arr, axes): ax.imshow(img) plt.tight_layout() plt.show()</code> </pre> <br><h4 id="perevorachivanie-izobrazheniya-po-gorizontali">  Horizontal flip </h4><br><p>  We can start with a simple transformation - horizontal image flipping.  Let's see how this transformation will look applied to our source images.  To achieve such a transformation, you must pass the parameter <code>horizontal_flip=True</code> constructor of the <strong>ImageDataGenerator</strong> class. </p><br><pre> <code class="python hljs">image_gen = ImageDataGenerator(rescale=<span class="hljs-number"><span class="hljs-number">1.</span></span>/<span class="hljs-number"><span class="hljs-number">255</span></span>, horizontal_flip=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) train_data_gen = image_gen.flow_from_directory(batch_size=BATCH_SIZE, directory=train_dir, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, target_size=(IMG_SHAPE, IMG_SHAPE))</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">Found 2000 images belonging to 2 classes.</code> </pre> <br><p>                    .      (  )   . </p><br><pre> <code class="python hljs">augmented_images = [train_data_gen[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">5</span></span>)] plotImages(augmented_images)</code> </pre> <br><p>  (2  5 ): </p><br><p><img src="https://habrastorage.org/webt/9k/ss/zk/9ksszkjktuykiawts9xldwp8d6e.png"></p><br><h4 id="povorot-izobrazheniy">   </h4><br><p>            .        45. </p><br><pre> <code class="python hljs">image_gen = ImageDataGenerator(rescale=<span class="hljs-number"><span class="hljs-number">1.</span></span>/<span class="hljs-number"><span class="hljs-number">255</span></span>, rotation_range=<span class="hljs-number"><span class="hljs-number">45</span></span>) train_data_gen = image_gen.flow_from_directory(batch_size=BATCH_SIZE, directory=train_dir, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, target_size=(IMG_SHAPE, IMG_SHAPE))</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">Found 2000 images belonging to 2 classes.</code> </pre> <br><p>         ‚Äî          5   .      (  )   . </p><br><pre> <code class="python hljs">augmented_images = [train_data_gen[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">5</span></span>)] plotImages(augmented_images)</code> </pre> <br><p>  (2   5): </p><br><p><img src="https://habrastorage.org/webt/f4/7a/du/f47adubevummwogquqvplmzvy5k.png"></p><br><h4 id="primenenie-uvelicheniya">   </h4><br><p>             ‚Äî    50%. </p><br><pre> <code class="python hljs">image_gen = ImageDataGenerator(rescale=<span class="hljs-number"><span class="hljs-number">1.</span></span>/<span class="hljs-number"><span class="hljs-number">255</span></span>, zoom_range=<span class="hljs-number"><span class="hljs-number">0.5</span></span>) train_data_gen = image_gen.flow_from_directory(batch_size=BATCH_SIZE, directory=train_dir, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, target_size=(IMG_SHAPE, IMG_SHAPE))</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">Found 2000 images belonging to 2 classes.</code> </pre> <br><p>   ,      ‚Äî 5      .   (  )       . </p><br><pre> <code class="python hljs">augmented_images = [train_data_gen[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">5</span></span>)] plotImages(augmented_images)</code> </pre> <br><p>  (2  5 ): </p><br><p><img src="https://habrastorage.org/webt/i6/75/qx/i675qx7wsf8c4woaxoipll-j8cs.png"></p><br><h4 id="obedinyaem-vsyo-vmeste">    </h4><br><p>       ,   ,   ,          <code>ImageDataGenerator</code> . </p><br><p>       ‚Äî   ,   45 ,   ,   ,    . </p><br><pre> <code class="python hljs">image_gen_train = ImageDataGenerator( rescale=<span class="hljs-number"><span class="hljs-number">1.</span></span>/<span class="hljs-number"><span class="hljs-number">255</span></span>, rotation_range=<span class="hljs-number"><span class="hljs-number">40</span></span>, width_shift_range=<span class="hljs-number"><span class="hljs-number">0.2</span></span>, height_shift_range=<span class="hljs-number"><span class="hljs-number">0.2</span></span>, shear_range=<span class="hljs-number"><span class="hljs-number">0.2</span></span>, zoom_range=<span class="hljs-number"><span class="hljs-number">0.2</span></span>, horizontal_flip=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, fill_mode=<span class="hljs-string"><span class="hljs-string">'nearest'</span></span> ) train_data_gen = image_gen_train.flow_from_directory(batch_size=BATCH_SIZE, directory=train_dir, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, target_size=(IMG_SHAPE, IMG_SHAPE), class_mode=<span class="hljs-string"><span class="hljs-string">'binary'</span></span>)</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">Found 2000 images belonging to 2 classes.</code> </pre> <br><p>   ,           . </p><br><pre> <code class="python hljs">augmented_images = [train_data_gen[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">5</span></span>)] plotImages(augmented_images)</code> </pre> <br><p>  (2   5): </p><br><p><img src="https://habrastorage.org/webt/8e/nu/xg/8enuxgydgovr7ppmile7k3arbwg.png"></p><br><h4 id="sozdayom-validacionnyy-nabor-dannyh">     </h4><br><p>  ,       ,       ,            .        ,   ,  . </p><br><pre> <code class="python hljs">image_gen_val = ImageDataGenerator(rescale=<span class="hljs-number"><span class="hljs-number">1.</span></span>/<span class="hljs-number"><span class="hljs-number">255</span></span>) val_data_gen = image_gen_val.flow_from_directory(batch_size=BATCH_SIZE, directory=validation_dir, target_size=(IMG_SHAPE, IMG_SHAPE), class_mode=<span class="hljs-string"><span class="hljs-string">'binary'</span></span>)</code> </pre> <br><h1 id="sozdanie-modeli">   </h1><br><h3 id="opisyvaem-model">   </h3><br><p>    4           . </p><br><p>             0.5.  ,  50%          0.    . </p><br><p>        512     <code>relu</code> .        ‚Äî    ‚Äî  <code>softmax</code> . </p><br><pre> <code class="python hljs">model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">32</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, input_shape=(IMG_SHAPE, IMG_SHAPE, <span class="hljs-number"><span class="hljs-number">3</span></span>)), tf.keras.layers.MaxPooling2D(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">64</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), tf.keras.layers.MaxPooling2D(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">128</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), tf.keras.layers.MaxPooling2D(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), tf.keras.layers.Conv2D(<span class="hljs-number"><span class="hljs-number">128</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), tf.keras.layers.MaxPooling2D(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), tf.keras.layers.Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>), tf.keras.layers.Flatten(), tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">512</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), tf.keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">2</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>) ])</code> </pre> <br><h4 id="kompilirovanie-modeli">   </h4><br><p>       <code>adam</code> .      <code>sparse_categorical_crossentropy</code> .            ,    <code>accuracy</code>   <code>metrics</code> : </p><br><pre> <code class="python hljs">model.compile(optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> <br><h4 id="predstavlenie-modeli">   </h4><br><p>           <strong>summary</strong> : </p><br><pre> <code class="python hljs">model.summary()</code> </pre> <br><p>  Conclusion: </p><br><pre> <code class="plaintext hljs">Model: "sequential" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 148, 148, 32) 896 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 74, 74, 32) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 72, 72, 64) 18496 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 34, 34, 128) 73856 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 15, 15, 128) 147584 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128) 0 _________________________________________________________________ dropout (Dropout) (None, 7, 7, 128) 0 _________________________________________________________________ flatten (Flatten) (None, 6272) 0 _________________________________________________________________ dense (Dense) (None, 512) 3211776 _________________________________________________________________ dense_1 (Dense) (None, 2) 1026 ================================================================= Total params: 3,453,634 Trainable params: 3,453,634 Non-trainable params: 0 _________________________________________________________________</code> </pre> <br><h4 id="trenirovka-modeli">   </h4><br><p>    ! </p><br><p>         ( <code>ImageDataGenerator</code> )    <code>fit_generator</code>     <code>fit</code> : </p><br><pre> <code class="python hljs">EPOCHS = <span class="hljs-number"><span class="hljs-number">100</span></span> history = model.fit_generator( train_data_gen, steps_per_epoch=int(np.ceil(total_train / float(BATCH_SIZE))), epochs=EPOCHS, validation_data=val_data_gen, validation_steps=int(np.ceil(total_val / float(BATCH_SIZE))) )</code> </pre> <br><h4 id="vizualizaciya-rezultatov-trenirovki">    </h4><br><p>       : </p><br><pre> <code class="plaintext hljs">acc = history.history['acc'] val_acc = history.history['val_acc'] loss = history.history['loss'] val_loss = history.history['val_loss'] epochs_range = range(EPOCHS) plt.figure(figsize=(8,8)) plt.subplot(1, 2, 1) plt.plot(epochs_range, acc, label='  ') plt.plot(epochs_range, val_acc, label='  ') plt.legend(loc='lower right') plt.title('     ') plt.subplot(1, 2, 2) plt.plot(epochs_range, loss, label='  ') plt.plot(epochs_range, val_loss, label='  ') plt.legend(loc='upper right') plt.title('     ') plt.savefig('./foo.png') plt.show()</code> </pre> <br><p>  Conclusion: </p><br><p><img src="https://habrastorage.org/webt/bo/ea/t2/boeat2ibnf3khchazx3lslllj2e.png"></p><br><h1>      </h1><br><p>        ,    : </p><br><ol><li> <strong> </strong> :                      (   ). </li><li> <strong>  (.. augmentation)</strong> :                . </li><li> <strong> /  (.. dropout)</strong> :           (   ,      ). </li></ol><br><p>         ,     .           <a href="https://hackernoon.com/memorizing-is-not-learning-6-tricks-to-prevent-overfitting-in-machine-learning-820b091dc42"> </a> . </p><br><h1> :    </h1><br><p>  CoLab      <a href="https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l05c03_exercise_flowers_with_data_augmentation.ipynb"> </a> . <br> CoLab      <a href="https://colab.research.google.com/drive/1GX23C_LDbyhSTO6jmnQGNX4tp_BbTFkm"> </a> . </p><br><p>                .      CoLab        .   CoLab               .   CoLab       ,             . </p><br><p>               CoLab.   CoLab        ,       ,         . </p><br><p>  Enjoy! </p><br><p>ÔªøÔªø#     tf.keras </p><br><p>   CoLab     .        <code>tf.keras.Sequential</code> ,      <code>ImageDataGenerator</code> . </p><br><h1 id="importirovanie-paketov-1">   </h1><br><p>      . <code>os</code>         , <code>numpy</code>     python-  numpy-     ,  ,   <code>matplotlib.pyplot</code>         . </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> __future__ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> absolute_import, division, print_function, unicode_literals <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> glob <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> shutil <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt</code> </pre> <br><h3 id="todo-importiruem-tensorflow-i-keras-sloi"> TODO:  TensorFlow  Keras- </h3><br><p>         TensorFlow  <code>tf</code>  Keras-  ,         .  ,  <code>ImageDataGenerator</code> -  Keras         . </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  </span></span></code> </pre> <br><h1 id="zagruzka-dannyh-1">  Data loading </h1><br><p>                ‚Äî   .              . </p><br><p>    . </p><br><pre> <code class="python hljs">_URL = <span class="hljs-string"><span class="hljs-string">"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"</span></span> zip_file = tf.keras.utils.get_file(origin=_URL, fname=<span class="hljs-string"><span class="hljs-string">"flower_photos.tgz"</span></span>, extract=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) base_dir = os.path.join(os.path.dirname(zip_file), <span class="hljs-string"><span class="hljs-string">'flower_photos'</span></span>)</code> </pre> <br><p>  ,    ,  5  : </p><br><ol><li>  Roses </li><li>  </li><li>  Dandelions </li><li>  Sunflowers </li><li>  Tulips </li></ol><br><p>        : </p><br><pre> <code class="python hljs">classes = [<span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>]</code> </pre> <br><p>  ,   ,   : </p><br><pre> <code class="plaintext hljs">flower_photos |__ diasy |__ dandelion |__ roses |__ sunflowers |__ tulips</code> </pre> <br><p>                  .            .   ,    . </p><br><p>    2  <code>train</code>  <code>val</code>      5 - (    ).          ,  80%      ,   20%     .      : </p><br><pre> <code class="plaintext hljs">flower_photos |__ diasy |__ dandelion |__ roses |__ sunflowers |__ tulips |__ train |______ daisy: [1.jpg, 2.jpg, 3.jpg ....] |______ dandelion: [1.jpg, 2.jpg, 3.jpg ....] |______ roses: [1.jpg, 2.jpg, 3.jpg ....] |______ sunflowers: [1.jpg, 2.jpg, 3.jpg ....] |______ tulips: [1.jpg, 2.jpg, 3.jpg ....] |__ val |______ daisy: [507.jpg, 508.jpg, 509.jpg ....] |______ dandelion: [719.jpg, 720.jpg, 721.jpg ....] |______ roses: [514.jpg, 515.jpg, 516.jpg ....] |______ sunflowers: [560.jpg, 561.jpg, 562.jpg .....] |______ tulips: [640.jpg, 641.jpg, 642.jpg ....]</code> </pre> <br><p>       ,     ,   .             . </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> cl <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> classes: img_path = os.path.join(base_dir, cl) images = glob.glob(img_path + <span class="hljs-string"><span class="hljs-string">'/*.jpg'</span></span>) print(<span class="hljs-string"><span class="hljs-string">"{}: {} "</span></span>.format(cl, len(images))) train, val = images[:round(len(images)*<span class="hljs-number"><span class="hljs-number">0.8</span></span>)], images[round(len(images)*<span class="hljs-number"><span class="hljs-number">0.8</span></span>):] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> train: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> os.path.exists(os.path.join(base_dir, <span class="hljs-string"><span class="hljs-string">'train'</span></span>, cl)): os.makedirs(os.path.join(base_dir, <span class="hljs-string"><span class="hljs-string">'train'</span></span>, cl)) shutil.move(t, os.path.join(base_dir, <span class="hljs-string"><span class="hljs-string">'train'</span></span>, cl)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> v <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> val: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> os.path.exists(os.path.join(base_dir, <span class="hljs-string"><span class="hljs-string">'val'</span></span>, cl)): os.makedirs(os.path.join(base_dir, <span class="hljs-string"><span class="hljs-string">'val'</span></span>, cl)) shutil.move(v, os.path.join(base_dir, <span class="hljs-string"><span class="hljs-string">'val'</span></span>, cl))</code> </pre> <br><p>            : </p><br><pre> <code class="python hljs">train_dir = os.path.join(base_dir, <span class="hljs-string"><span class="hljs-string">'train'</span></span>) val_dir = os.path.join(base_dir, <span class="hljs-string"><span class="hljs-string">'val'</span></span>)</code> </pre> <br><h1 id="rasshirenie-dannyh-1">   </h1><br><p> ,  ,  ,       .       ‚Äî   (.. augmentation)     .                    .    ,    ,         ‚Äî     ,   .          . </p><br><p>  <strong>tf.keras</strong>       ,             ‚Äî <strong>ImageDataGenerator</strong> .                  . </p><br><h2 id="eksperimentiruyte-s-razlichnymi-preobrazovaniyami-izobrazheniy">      </h2><br><p>            .            ‚Äî        ()    <code>batch_size</code> ,            <code>IMG_SHAPE</code> . </p><br><h4 id="todo-ustanovite-kolichestvo-obuchayuschih-blokov-i-razmer-izobrazheniy"> TODO:        </h4><br><p>      100   <code>batch_size</code>   150   <code>IMG_SHAPE</code> : </p><br><pre> <code class="python hljs">batch_size = IMG_SHAPE =</code> </pre> <br><h4 id="todo-primenite-proizvolnyy-gorizontalnyy-perevorot-izobrazheniya"> TODO:      </h4><br><p>      <code>ImageDataGenerator</code>   ,       ,      .     <code>.flow_from_directory</code>          . ,      ,       ,          . </p><br><pre> <code class="python hljs">image_gen = train_data_gen =</code> </pre> <br><p>         5      : </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plotImages</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(images_arr)</span></span></span><span class="hljs-function">:</span></span> fig, axes = plt.subplots(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">20</span></span>,<span class="hljs-number"><span class="hljs-number">20</span></span>)) axes = axes.flatten() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> img, ax <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(images_arr, axes): ax.imshow(img) plt.tight_layout() plt.show() augmented_images = [train_data_gen[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">5</span></span>)] plotImages(augmented_images)</code> </pre> <br><h4 id="todo-primenite-proizvolnyy-perevorot-izobrazheniya"> TODO:     </h4><br><p>   ,   <code>ImageDataGenerator</code>          45 .     .flow_from_directory          . ,      ,       ,          . </p><br><pre> <code class="python hljs">image_gen = train_data_gen =</code> </pre> <br><p>         5      : </p><br><pre> <code class="python hljs">augmented_images = [train_data_gen[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">5</span></span>)] plotImages(augmented_images)</code> </pre> <br><h4 id="todo-primenite-proizvolnoe-uvelichenie-izobrazheniya"> TODO:     </h4><br><p>   ,   ImageDataGenerator          50%.     .flow_from_directory          . ,      ,       ,          . </p><br><pre> <code class="python hljs">image_gen = train_data_gen =</code> </pre> <br><p>         5      : </p><br><pre> <code class="python hljs">augmented_images = [train_data_gen[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">5</span></span>)] plotImages(augmented_images)</code> </pre> <br><h4 id="todo-obedinyaem-vse-izmeneniya"> TODO:    </h4><br><p>     ,   <code>ImageDataGenerator</code>          : </p><br><ul><li>   45  </li><li>   50% </li><li>   </li><li>     0.15 </li><li>     0.15 </li></ul><br><p>    <code>flow_from_directory</code>          . ,      ,       ,          . </p><br><pre> <code class="python hljs">image_gen_train = train_data_gen =</code> </pre> <br><p>         5      : </p><br><pre> <code class="python hljs">augmented_images = [train_data_gen[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">5</span></span>)] plotImages(augmented_images)</code> </pre> <br><h4 id="todo-sozdayte-generator-izobrazheniy-dlya-validacionnogo-nabora-dannyh"> TODO:        </h4><br><p>         . ,   ,   <code>ImageDataGenerator</code>   ,      .    <code>flow_from_directory</code>          . ,      ,           .      . </p><br><pre> <code class="python hljs">image_gen_val = val_data_gen =</code> </pre> <br><h1 id="todo-sozdayte-svyortochnuyu-neyronnuyu-set"> TODO:     </h1><br><p>       ,     3   ‚Äî        .      16 ,  ‚Äî 32 ,  ‚Äî 64 .      33.          22. </p><br><p>         <code>Flatten</code> ,      512 .           5 ,       <strong>softmax</strong> .        <strong>relu</strong> .  ,   ,       20%. </p><br><pre> <code class="python hljs">model =</code> </pre> <br><h1 id="todo-skompiliruyte-model"> TODO:   </h1><br><p>     ,        <code>adam</code>   <code>sparse_categorical_crossentropy</code>    .                  ,         <code>compile(...)</code> . </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  </span></span></code> </pre> <br><h1 id="todo-obuchite-model"> TODO:   </h1><br><p>     ,        <strong>fit_generator</strong>    <strong>fit</strong> ,    .    <strong>fit_generator</strong>       <strong>ImageDataGenerator</strong>          .    80   ,      <strong>fit_generator</strong> -. </p><br><pre> <code class="python hljs">epochs = history =</code> </pre> <br><h1 id="todo-postroyte-grafiki-tochnosti--poter-dlya-obuchayuschego-i-validacionnogo-naborov-dannyh"> TODO:    /        </h1><br><p>     ,            : </p><br><pre> <code class="python hljs">acc = val_acc = loss = val_loss = epochs_range =</code> </pre> <br><h1 id="todo-poeksperimentiruyte-s-razlichnymi-parametrami"> TODO:     </h1><br><p>              (  +  )       512 .             .      . ,            ,       ..        <strong></strong>   ,     <strong>ImageDataGenerator</strong> ‚Äî         .       ,             . </p><br><p>      ? </p><br><h1>  Results </h1><br><p>                    . </p><br><p>             RGB-  : </p><br><ul><li> <strong> </strong> :      ,                  (   ); </li><li> <strong> </strong> :      3D-; </li><li> <strong>RGB-</strong> :     3  : ,   ; </li><li> <strong></strong> :                  ().               ,          ().         ‚Äî      . </li><li> <strong>   </strong> :                       .            ,          . </li><li> <strong>  </strong> :               .               ,                . </li></ul><br><p>    : </p><br><ul><li> <strong> </strong> :                    ,      . </li><li> <strong> </strong> :                    . </li><li> <strong> ()</strong> :         . </li></ul><br><p>                       .       ,                     .                    . </p><br><p> ‚Ä¶   call-to-action ‚Äî ,     share :) <br>  <a href="https://www.youtube.com/c/%25D0%2590%25D0%25BD%25D0%25B4%25D1%2580%25D0%25B5%25D0%25B9%25D0%25A8%25D0%25BC%25D0%25B8%25D0%25B3%3Fsub_confirmation%3D1">YouTube</a> <br>  <a href="https://t.me/ashmig">Telegram</a> <br>  <a href="https://vk.com/ashmig">In contact with</a> </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/458170/">https://habr.com/ru/post/458170/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../458160/index.html">Lock with priorities in .NET</a></li>
<li><a href="../458164/index.html">Artificial Intelligence - a question will be asked for each answer.</a></li>
<li><a href="../458166/index.html">Bringing in a convenient for work type of micro-computer UKNTS Electronics MS 0511 architecture PDP-11</a></li>
<li><a href="../458168/index.html">Digest of machine learning and artificial intelligence news for June</a></li>
<li><a href="../45817/index.html">New IceIM Miranda Pack (IM for Vkontakte.ru)</a></li>
<li><a href="../458172/index.html">Coupling methods for electrical connections when tracing differential pairs on printed circuit boards</a></li>
<li><a href="../458176/index.html">Exaflops barrier will be overcome in 2021</a></li>
<li><a href="../458180/index.html">Failover Kea based DHCP server</a></li>
<li><a href="../458182/index.html">We read VKontakte via RSS</a></li>
<li><a href="../458184/index.html">Haxe and PHP: static typing, arrow functions, metaprogramming and much more</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>