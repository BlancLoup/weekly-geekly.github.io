<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Speech.framework on iOS 10</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Overview 


 The next conference is the next innovations. Judging by the mood we are waiting for the cancellation of keyboards and input devices. Appl...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Speech.framework on iOS 10</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/files/8b5/501/852/8b5501852fd34a0084c299f5116d35eb.jpg"></p><br><h2>  Overview </h2><br><p>  The next conference is the next innovations.  Judging by the mood we are waiting for the cancellation of keyboards and input devices.  Apple in iOS 10 introduced to developers the opportunity to work with speech.  My colleague <a href="https://habrahabr.ru/users/devnseven/">Geor Casapidi</a> has already described the capabilities of Siri <a href="https://habrahabr.ru/company/e-Legion/blog/303886/">in his article</a> , and I will talk about <a href="https://developer.apple.com/reference/speech">Speech.framework</a> .  The material discussed in the article is implemented in the demo application what_i_say.  At the time of writing, there is no official documentation, so we will be based on what <a href="https://developer.apple.com/videos/play/wwdc2016/509/">Henry Mason said</a> . <a name="habracut"></a></p><br><p>  Work with speech is completely tied to access to the Internet, and this is not surprising, since it uses the same Siri engine.  The developer is given the opportunity to either recognize the live speech, or already recorded in the file.  Difference in request object: </p><br><p> <code>SFSpeechURLRecognitionRequest</code> - used for recognition from a file; <br>  <code>SFSpeechAudioBufferRecognitionRequest</code> - used to recognize dictation. </p><br><p>  Consider working with real-time dictation. </p><br><p>  All work is based on the concept: </p><br><p><img src="https://habrastorage.org/files/6fe/992/cb7/6fe992cb78234aefbf11d0670bdc8151.png"></p><br><p>  But since Speech.framework does not have a special API for working with a microphone, the developer will need to use <a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAudioEngine_Class/">AVAudioEngine</a> . </p><br><h2>  Application Authorization </h2><br><p>  First of all, the developer needs to add two keys to the <code>Info.plist</code> file. </p><br><p>  <code>NSMicrophoneUsageDescription</code> - why you need access to the microphone <br>  <code>NSSpeechRecognitionUsageDescription</code> - what is access to speech recognition for <code>NSSpeechRecognitionUsageDescription</code> </p><br><p><img src="https://habrastorage.org/files/75a/d6f/56d/75ad6f56d29c43b2a94ddc85d26691a2.png"></p><br><p>  This is important, because without them there will be an inevitable crash. </p><br><p>  Next, we need to process the authorization status: </p><br><pre> <code class="hljs bash">- (void)handleAuthorizationStatus:(SFSpeechRecognizerAuthorizationStatus)s { switch (s) { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> SFSpeechRecognizerAuthorizationStatusNotDetermined: //        [self requestAuthorization]; <span class="hljs-built_in"><span class="hljs-built_in">break</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> SFSpeechRecognizerAuthorizationStatusDenied: //    [self informDelegateErrorType:(EVASpeechManagerErrorSpeechRecognitionDenied)]; <span class="hljs-built_in"><span class="hljs-built_in">break</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> SFSpeechRecognizerAuthorizationStatusRestricted: //   <span class="hljs-built_in"><span class="hljs-built_in">break</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> SFSpeechRecognizerAuthorizationStatusAuthorized: { //   } <span class="hljs-built_in"><span class="hljs-built_in">break</span></span>; } }</code> </pre> <br><p>  At the stage of acquaintance with Speech.framework, I did not find the use-case of using the status of <code>SFSpeechRecognizerAuthorizationStatusRestricted</code> .  I assume that this status was planned for limited access rights, but so far there are no additional options for Speech Recognition.  Well, wait for the official documentation. </p><br><p><img src="https://habrastorage.org/files/741/99c/470/74199c47009243ac8fbf9a99bceb1d0b.PNG"></p><br><p>  When you first start the application after installation, the recognizer will return the status of <code>SFSpeechRecognizerAuthorizationStatusNotDetermined</code> , so you need to request authorization from the user: </p><br><pre> <code class="hljs objectivec">- (<span class="hljs-keyword"><span class="hljs-keyword">void</span></span>)requestAuthorization { [SFSpeechRecognizer requestAuthorization:^(SFSpeechRecognizerAuthorizationStatus status) { [<span class="hljs-keyword"><span class="hljs-keyword">self</span></span> handleAuthorizationStatus:status]; }]; }</code> </pre> <br><p>  The API has neither methods nor notifications for tracking status changes.  The system will forcefully restart the application if the user changes the authorization status. </p><br><h2>  Customization </h2><br><p>  For successful speech recognition work, we need four objects: </p><br><pre> <code class="hljs objectivec"><span class="hljs-keyword"><span class="hljs-keyword">@property</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">nonatomic</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">strong</span></span>) SFSpeechRecognizer *recognizer; <span class="hljs-keyword"><span class="hljs-keyword">@property</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">nonatomic</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">strong</span></span>) <span class="hljs-built_in"><span class="hljs-built_in">AVAudioEngine</span></span> *audioEngine; <span class="hljs-keyword"><span class="hljs-keyword">@property</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">nonatomic</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">strong</span></span>) SFSpeechAudioBufferRecognitionRequest *request; <span class="hljs-keyword"><span class="hljs-keyword">@property</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">nonatomic</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">strong</span></span>) SFSpeechRecognitionTask *currentTask;</code> </pre> <br><p>  and implement the protocol: </p><br><pre> <code class="hljs objectivec"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">@interface</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">EVASpeechManager</span></span></span><span class="hljs-class"> () &lt;</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SFSpeechRecognitionTaskDelegate</span></span></span><span class="hljs-class">&gt;</span></span></code> </pre> <br><p>  <code>recognizer</code> - the object that will process requests. <br>  Of the important: </p><br><ul><li>  initialized with the initWithLocale: locale.  If the locale is not supported, then nil will be returned.  At the time of this writing, the following locales were supported: </li></ul><br><blockquote>  ro-RO en-IN he-IL tr-TR en-NZ sv-SE fr-BE it-CH de-CH pl-PL pt-PT uk-UA fi-FI vi-VN ar-SA zh-TW es- ES en-GB yue-CN th-TH en-ID ja-JP en-SA en-AE da-DK fr-FR sk-SK de-AT ms-MY hu-H ca ca ES ko-KR fr-CH nb -NO en-AU el-GR ru-RU zh-CN en-US en-IE nl-BE es-CO pt-BR es-US hr-HR fr-CA zh-HK es-MX it-IT id-ID nl-NL cs-CZ en-ZA es-CL en-PH en-CA en-SG de-DE. </blockquote><br><p>  The recognizer does not switch the locale automatically.  It works within the locale specified during initialization.  Those.  if the initialization was <code>en-US</code> locale, and the user speaks another language, then the recognizer will work out and return an empty answer; </p><br><ul><li>  has a <code>queue</code> property (by default <code>mainQueue</code> ), which makes it possible to process callback'and asynchronously.  This will allow you to process a large enough text without delay. </li></ul><br><p>  <code>audioEngine</code> - an object that will connect the input signal from the microphone (dictated speech) and the output signal to the buffer. </p><br><pre> <code class="hljs objectivec"> <span class="hljs-keyword"><span class="hljs-keyword">self</span></span>.audioEngine = [[<span class="hljs-built_in"><span class="hljs-built_in">AVAudioEngine</span></span> alloc] init]; <span class="hljs-built_in"><span class="hljs-built_in">AVAudioInputNode</span></span> *node = <span class="hljs-keyword"><span class="hljs-keyword">self</span></span>.audioEngine.inputNode; <span class="hljs-built_in"><span class="hljs-built_in">AVAudioFormat</span></span> *recordingFormat = [node outputFormatForBus:bus]; [node installTapOnBus:<span class="hljs-number"><span class="hljs-number">0</span></span> bufferSize:<span class="hljs-number"><span class="hljs-number">1024</span></span> format:recordingFormat block:^(<span class="hljs-built_in"><span class="hljs-built_in">AVAudioPCMBuffer</span></span> * _Nonnull buffer, <span class="hljs-built_in"><span class="hljs-built_in">AVAudioTime</span></span> * _Nonnull when) { [<span class="hljs-keyword"><span class="hljs-keyword">self</span></span>.request appendAudioPCMBuffer:buffer]; }];</code> </pre> <br><p>  <code>request</code> is a speech recognition request. <br>  From the interesting: </p><br><ul><li>  The request has a property SFSpeechRecognitionTaskHint, which can take one of four values: <br><ul><li>  SFSpeechRecognitionTaskHintUnspecified </li><li>  SFSpeechRecognitionTaskHintDictation </li><li>  SFSpeechRecognitionTaskHintSearch </li><li>  SFSpeechRecognitionTaskHintConfirmation </li></ul></li></ul><br><p>  According to the description, it can be assumed that it is planned to divide the requests also into types, i.e., ordinary dictation, dictation for information retrieval, dictation for confirmation of something.  Alas, now they are not used anywhere, except as markers.  I suppose there will be a development of this link. </p><br><p>  <code>currentTask</code> is a task object for processing a single request. <br>  Of the important: </p><br><ul><li>  <code>error</code> property.  Here you can get an error if the task was failed; </li><li>  two properties <code>finishing</code> / <code>cancelled</code> by which you can determine whether the task was completed or canceled.  When you cancel a task, the system instantly cancels further processing and recognition, but the developer in any case can get that part of the recognized one that has already been processed.  At the completion of the task, the system will return a fully processed result. </li></ul><br><h2>  Using </h2><br><h4>  Launch </h4><br><p>  After successfully setting up the speech manager, you can proceed to the work itself. <br>  You must create a query: </p><br><pre> <code class="hljs swift"><span class="hljs-keyword"><span class="hljs-keyword">self</span></span>.request = [[<span class="hljs-type"><span class="hljs-type">SFSpeechAudioBufferRecognitionRequest</span></span> alloc] <span class="hljs-keyword"><span class="hljs-keyword">init</span></span>];</code> </pre> <br><p>  and pass it to the recognizer: </p><br><blockquote>  Important!  Before <code>audioEngine</code> request to the resolver, you need to activate <code>audioEngine</code> to receive the audio stream from the microphone. </blockquote><br><pre> <code class="hljs objectivec">- (<span class="hljs-keyword"><span class="hljs-keyword">void</span></span>)performRecognize { [<span class="hljs-keyword"><span class="hljs-keyword">self</span></span>.audioEngine prepare]; <span class="hljs-built_in"><span class="hljs-built_in">NSError</span></span> *error = <span class="hljs-literal"><span class="hljs-literal">nil</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> ([<span class="hljs-keyword"><span class="hljs-keyword">self</span></span>.audioEngine startAndReturnError:&amp;error]) { <span class="hljs-keyword"><span class="hljs-keyword">self</span></span>.currentTask = [<span class="hljs-keyword"><span class="hljs-keyword">self</span></span>.recognizer recognitionTaskWithRequest:<span class="hljs-keyword"><span class="hljs-keyword">self</span></span>.request delegate:<span class="hljs-keyword"><span class="hljs-keyword">self</span></span>]; } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { [<span class="hljs-keyword"><span class="hljs-keyword">self</span></span> informDelegateError:error]; } }</code> </pre> <br><p>  By accepting the request, the system will return the task object to us.  You will need it later to stop the process. </p><br><h4>  Stop </h4><br><p>  The request object cannot be reused while it is active, otherwise the <code>‚Äúreason: 'SFSpeechAudioBufferRecognitionRequest cannot be re-used'‚Äù</code> crash.  Therefore, it is important at the end of the process to stop the task and audioEngine: </p><br><pre> <code class="hljs objectivec">- (<span class="hljs-keyword"><span class="hljs-keyword">void</span></span>)stopRecognize { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> ([<span class="hljs-keyword"><span class="hljs-keyword">self</span></span> isTaskInProgress]) { [<span class="hljs-keyword"><span class="hljs-keyword">self</span></span>.currentTask finish]; [<span class="hljs-keyword"><span class="hljs-keyword">self</span></span>.audioEngine stop]; } }</code> </pre> <br><h4>  Result processing </h4><br><p>  At the end of the task, the result will be delivered to the <code>SFSpeechRecognitionTaskDelegate</code> delegate <code>SFSpeechRecognitionTaskDelegate</code> : </p><br><p>  process the recognized text in the method: </p><br><pre> <code class="hljs erlang">- <span class="hljs-params"><span class="hljs-params">(void)</span></span>speechRecognitionTask:<span class="hljs-params"><span class="hljs-params">(SFSpeechRecognitionTask *)</span></span>task didFinishRecognition:<span class="hljs-params"><span class="hljs-params">(SFSpeechRecognitionResult *)</span></span>recognitionResult { self.buffer = recognitionResult.bestTranscription.formattedString; }</code> </pre> <br><p>  and errors in the method: </p><br><pre> <code class="hljs objectivec">- (<span class="hljs-keyword"><span class="hljs-keyword">void</span></span>)speechRecognitionTask:(SFSpeechRecognitionTask *)task didFinishSuccessfully:(<span class="hljs-built_in"><span class="hljs-built_in">BOOL</span></span>)successfully { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (!successfully) { [<span class="hljs-keyword"><span class="hljs-keyword">self</span></span> informDelegateError:task.error]; } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { [<span class="hljs-keyword"><span class="hljs-keyword">self</span></span>.delegate manager:<span class="hljs-keyword"><span class="hljs-keyword">self</span></span> didRecognizeText:[<span class="hljs-keyword"><span class="hljs-keyword">self</span></span>.buffer <span class="hljs-keyword"><span class="hljs-keyword">copy</span></span>]]; } }</code> </pre> <br><h2>  Errors </h2><br><p>  While working with the application, I encountered errors: </p><br><blockquote>  Error: SessionId=com.siri.cortex.ace.speech.session.event.SpeechSessionId@439e90ed, Message = Timeout for 30000 ms <br><br>  Error: SessionId=com.siri.cortex.ace.speech.session.event.SpeechSessionId@714a717a, Message = Empty recognition </blockquote><br><p>  the first error arrived when, after a small dictation, the speech was finished (the user stopped speaking), and the recognizer continued to work; </p><br><p>  the second error was when the recognizer did not receive any audio stream and the process was canceled by the user. </p><br><p>  Both errors were caught in the delegate method: </p><br><pre> <code class="hljs objectivec">- (<span class="hljs-keyword"><span class="hljs-keyword">void</span></span>)speechRecognitionTask:(SFSpeechRecognitionTask *)task didFinishSuccessfully:(<span class="hljs-built_in"><span class="hljs-built_in">BOOL</span></span>)successfully { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (!successfully) { <span class="hljs-comment"><span class="hljs-comment">//     task.error } }</span></span></code> </pre> <br><h2>  Conclusion </h2><br><p>  After the first acquaintance with Speech.framework, it became clear that he was still ‚Äúraw‚Äù.  Much needs to be finalized and tested.  The beginning is not bad, there is a striving towards artificial intelligence and working without means of entering information.  In combination with IoT, there are great opportunities for managing gadgets, home, car.  For now we wait, we train and we investigate further. </p><br><h2>  Links </h2><br><ul><li>  <a href="https://github.com/podaenur/what_i_say">demo application what_i_say</a> </li><li>  <a href="https://developer.apple.com/reference/speech">Speech.framework</a> </li><li>  <a href="https://developer.apple.com/videos/wwdc2016/">Apple WWDC 2016</a> </li></ul></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/304714/">https://habr.com/ru/post/304714/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../304702/index.html">Video of reports from Docker mitap</a></li>
<li><a href="../304704/index.html">The development of cloud technologies in Russia. New reality: vectors of development and major problems</a></li>
<li><a href="../304706/index.html">Prediction of the likelihood of each client‚Äôs transition to the status of a former member</a></li>
<li><a href="../304708/index.html">Web scraping updated data with Node.js and PaaS</a></li>
<li><a href="../304710/index.html">40 tutorials for creating vector illustrations</a></li>
<li><a href="../304716/index.html">How we helped a big Brazilian bank deal with the effects of the denomination</a></li>
<li><a href="../304718/index.html">Outdated CPUs or higher price? We found a compromise! 2 x E5620 / 32GB DDR3 / 6 x 240GB SSD / 1Gbps?</a></li>
<li><a href="../304720/index.html">Subtleties of selection of respondents for UX-research</a></li>
<li><a href="../304722/index.html">Development of the first game [on Unity3D]</a></li>
<li><a href="../304724/index.html">Projecting Google Material Design onto the desktop system ... (part one)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>