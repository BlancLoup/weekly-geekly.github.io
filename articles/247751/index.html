<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Introduction to machine learning using Python and Scikit-Learn</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! 



 My name is Alexander , I am engaged in machine learning and web graph analysis ( mostly theoretical ), as well as the development of Bi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Introduction to machine learning using Python and Scikit-Learn</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr! <br><br><img src="https://habrastorage.org/files/e2c/a40/b3f/e2ca40b3ffa74c5d93db82520f225fc5.png"><br><br>  My name is <a href="http://mlclass.ru/">Alexander</a> , I am engaged in machine learning and web graph analysis ( <a href="https://events.yandex.ru/lib/talks/1919/">mostly theoretical</a> ), as well as the development of Big Data products in one of the Big Three operators.  This is my first post - please do not judge strictly!) 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Recently, people who want to learn how to develop efficient algorithms and participate in machine learning competitions with the question: "Where to start?"  Some time ago, I led the development of Big Data tools for analyzing media and social networks in <a href="http://www.ac.gov.ru/">one of the institutions of the Government of the Russian Federation</a> , and I still have some material on which my team studied and which I can share.  It is assumed that the reader has a good knowledge of mathematics and machine learning (the team consisted mainly of graduates of the Moscow Institute of Physics and Technology and students of the <a href="http://shad.yandex.ru/">School of Data Analysis</a> ). <br><a name="habracut"></a><br>  In fact, it was an introduction to <b>Data Science</b> .  Recently, this science has become quite popular.  Increasingly, machine learning competitions are held (for example, <a href="http://www.kaggle.com/">Kaggle</a> , <a href="http://tunedit.org/">TudedIT</a> ), often with a considerable budget.  The purpose of this article is to give the reader a quick introduction of machine learning tools so that he can participate in competitions as soon as possible. <br><br>  The most common Data Scientist tools today are <b>R</b> and <b>Python</b> .  Each tool has its pros and cons, however, lately Python wins in all respects (this is solely the opinion of the author, who also uses both).  This came after the well-documented <b>Scikit-Learn</b> library <b>appeared</b> in which a large number of machine learning algorithms were implemented. <br><br>  Immediately, we note that in the article we will focus on machine learning algorithms.  Primary data analysis is usually best done using the <b>Pandas</b> package, which can be <b>dealt</b> with on its own.  So, let's focus on the implementation, for definiteness, assuming that at the entrance we have a matrix object-feature stored in a file with the extension <b>* .csv</b> <br><br><h3>  <b>Data loading</b> </h3><br>  First of all, the data must be loaded into RAM so that we can work with them.  The Scikit-Learn library itself uses arrays in its NumPy implementation, so we will load * .csv files using NumPy.  Download one of the datasets from the <a href="http://archive.ics.uci.edu/ml/">UCI Machine Learning Repository</a> : <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> urllib <span class="hljs-comment"><span class="hljs-comment"># url with dataset url = "http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data" # download the file raw_data = urllib.urlopen(url) # load the CSV file as a numpy matrix dataset = np.loadtxt(raw_data, delimiter=",") # separate the data from the target attributes X = dataset[:,0:7] y = dataset[:,8]</span></span></code> </pre> <br>  Further in all examples we will work with this data set, namely with the matrix object feature <b>X</b> and the values ‚Äã‚Äãof the target variable <b>y</b> . <br><br><h3>  <b>Data normalization</b> </h3><br>  It is well known to all that most gradient methods (on which, in essence, almost all machine learning algorithms are based) are highly sensitive to data scaling.  Therefore, before running the algorithms, either <b>normalization</b> or the so-called <b>standardization is</b> most often done.  Normalization involves the replacement of nominal features so that each of them lies in the range from 0 to 1. Standardization implies a data preprocessing, after which each feature has an average of 0 and variance 1. In Scikit-Learn, there are already functions ready for this: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocessing <span class="hljs-comment"><span class="hljs-comment"># normalize the data attributes normalized_X = preprocessing.normalize(X) # standardize the data attributes standardized_X = preprocessing.scale(X)</span></span></code> </pre><br><br><h3>  <b>Feature selection</b> </h3><br>  It is no secret that often the most important thing in solving a problem is the ability to correctly select and even create signs.  In English literature this is called <b>Feature Selection</b> and <b>Feature Engineering</b> .  While Future Engineering is quite a creative process and relies more on intuition and expert knowledge, for Feature Selection there are already a large number of ready-made algorithms.  "Wood" algorithms allow the calculation of the informativeness of features: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> metrics <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.ensemble <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ExtraTreesClassifier model = ExtraTreesClassifier() model.fit(X, y) <span class="hljs-comment"><span class="hljs-comment"># display the relative importance of each attribute print(model.feature_importances_)</span></span></code> </pre><br>  All other methods are in one way or another based on the effective selection of subsets of features in order to find the best subset on which the constructed model gives the best quality.  One such search algorithm is the Recursive Feature Elimination algorithm, which is also available in the Scikit-Learn library: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.feature_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RFE <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.linear_model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LogisticRegression model = LogisticRegression() <span class="hljs-comment"><span class="hljs-comment"># create the RFE model and select 3 attributes rfe = RFE(model, 3) rfe = rfe.fit(X, y) # summarize the selection of the attributes print(rfe.support_) print(rfe.ranking_)</span></span></code> </pre><br><br><h3>  <b>Algorithm construction</b> </h3><br>  As already noted, Scikit-Learn implements all the basic algorithms of machine learning.  Consider some of them. <br><br><h4>  <b>Logistic regression</b> </h4><br>  It is most often used for solving classification problems (binary), but multi-class classification is also allowed (the so-called one-vs-all method).  The advantage of this algorithm is that the output for each object we have the probability of belonging to the class <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> metrics <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.linear_model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LogisticRegression model = LogisticRegression() model.fit(X, y) print(model) <span class="hljs-comment"><span class="hljs-comment"># make predictions expected = y predicted = model.predict(X) # summarize the fit of the model print(metrics.classification_report(expected, predicted)) print(metrics.confusion_matrix(expected, predicted))</span></span></code> </pre><br><br><h4>  <b>Naive bayes</b> </h4><br>  It is also one of the most well-known machine learning algorithms, whose main task is to restore the distribution densities of the training sample.  Often this method gives good quality in the tasks of multi-class classification. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> metrics <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.naive_bayes <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GaussianNB model = GaussianNB() model.fit(X, y) print(model) <span class="hljs-comment"><span class="hljs-comment"># make predictions expected = y predicted = model.predict(X) # summarize the fit of the model print(metrics.classification_report(expected, predicted)) print(metrics.confusion_matrix(expected, predicted))</span></span></code> </pre><br><br><h4>  <b>K-nearest neighbors</b> </h4><br>  The <b>kNN</b> method <b>(k-Nearest Neighbors) is</b> often used as part of a more complex classification algorithm.  For example, its assessment can be used as a sign for an object.  And sometimes, a simple kNN on well-chosen signs gives excellent quality.  With proper parameter settings (mostly metrics), the algorithm often provides good quality in regression tasks <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> metrics <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.neighbors <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> KNeighborsClassifier <span class="hljs-comment"><span class="hljs-comment"># fit a k-nearest neighbor model to the data model = KNeighborsClassifier() model.fit(X, y) print(model) # make predictions expected = y predicted = model.predict(X) # summarize the fit of the model print(metrics.classification_report(expected, predicted)) print(metrics.confusion_matrix(expected, predicted))</span></span></code> </pre><br><br><h4>  <b>Decision trees</b> </h4><br>  <b>Classification and Regression Trees (CART)</b> are often used in tasks in which objects have categorical attributes and are used for regression and classification tasks.  Very good trees are suitable for multi-class classification. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> metrics <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.tree <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> DecisionTreeClassifier <span class="hljs-comment"><span class="hljs-comment"># fit a CART model to the data model = DecisionTreeClassifier() model.fit(X, y) print(model) # make predictions expected = y predicted = model.predict(X) # summarize the fit of the model print(metrics.classification_report(expected, predicted)) print(metrics.confusion_matrix(expected, predicted))</span></span></code> </pre><br><br><h4>  <b>Support Vector Machine</b> </h4><br>  <b>SVM (Support Vector Machines)</b> is one of the most well-known machine learning algorithms used mainly for the classification task.  As well as logistic regression, SVM allows one-vs-all multi-class classification. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> metrics <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.svm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SVC <span class="hljs-comment"><span class="hljs-comment"># fit a SVM model to the data model = SVC() model.fit(X, y) print(model) # make predictions expected = y predicted = model.predict(X) # summarize the fit of the model print(metrics.classification_report(expected, predicted)) print(metrics.confusion_matrix(expected, predicted))</span></span></code> </pre><br>  In addition to the algorithms of classification and regression, Scikit-Learn has a huge number of more complex algorithms, including clustering, as well as implemented techniques for constructing compositions of algorithms, including <b>Bagging</b> and <b>Boosting</b> . <br><br><h4>  <b>Optimization of algorithm parameters</b> </h4><br>  One of the most difficult steps in building truly effective algorithms is choosing the right parameters.  Usually, this is done easier with experience, but one way or another it is necessary to do a bust.  Fortunately, Scikit-Learn already has quite a few functions implemented for this. <br><br>  For example, let's look at the selection of the regularization parameter, in which we in turn look through several values: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.linear_model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Ridge <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.grid_search <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GridSearchCV <span class="hljs-comment"><span class="hljs-comment"># prepare a range of alpha values to test alphas = np.array([1,0.1,0.01,0.001,0.0001,0]) # create and fit a ridge regression model, testing each alpha model = Ridge() grid = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas)) grid.fit(X, y) print(grid) # summarize the results of the grid search print(grid.best_score_) print(grid.best_estimator_.alpha)</span></span></code> </pre><br>  Sometimes it is more efficient to randomly select a parameter from a given segment many times, measure the quality of the algorithm with a given parameter and thus choose the best one: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy.stats <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> uniform <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sp_rand <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.linear_model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Ridge <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.grid_search <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RandomizedSearchCV <span class="hljs-comment"><span class="hljs-comment"># prepare a uniform distribution to sample for the alpha parameter param_grid = {'alpha': sp_rand()} # create and fit a ridge regression model, testing random alpha values model = Ridge() rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100) rsearch.fit(X, y) print(rsearch) # summarize the results of the random parameter search print(rsearch.best_score_) print(rsearch.best_estimator_.alpha)</span></span></code> </pre><br>  We looked at the whole process of working with the Scikit-Learn library with the exception of outputting the results back to a file, which the reader is supposed to do as an exercise, because one of the advantages of Python (and the Scikit-Learn library itself) in comparison with R is excellent documentation.  In the following parts we will look at each section in detail, in particular, we will touch on such an important thing as <b>Feauture Engineering</b> . <br><br>  I very much hope that this material will help novice Data Scientists to start solving machine learning problems in practice as soon as possible.  In conclusion, I would like to wish success and patience to those who are just beginning to participate in machine learning competitions! </div><p>Source: <a href="https://habr.com/ru/post/247751/">https://habr.com/ru/post/247751/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../247733/index.html">Introduction to the development of web-applications on PSGI / Plack. Part 2</a></li>
<li><a href="../247735/index.html">Io.js 1.0.0 is coming</a></li>
<li><a href="../247737/index.html">Mobile traffic is being stolen from Adwords (redirecting to some recseek)</a></li>
<li><a href="../247741/index.html">Yii 2.0.2</a></li>
<li><a href="../247749/index.html">Microsoft Azure raises prices in Russia by 44%</a></li>
<li><a href="../247753/index.html">How specifically can children's education in Russia be made better (and the first practical step)?</a></li>
<li><a href="../247755/index.html">Create dynamic websites using PHP, MySQL, JavaScript, CSS, and HTML5. 3rd ed</a></li>
<li><a href="../247757/index.html">How to promote your business in 2015 with maximum efficiency? 10 marketing predictions you should know about</a></li>
<li><a href="../247759/index.html">Development vs. Testing, or where to go graduate FVT</a></li>
<li><a href="../247761/index.html">How to promote your business in 2015 with maximum efficiency? 10 marketing predictions that you should know about! (part 2)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>