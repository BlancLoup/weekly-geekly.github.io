<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The competition ‚ÄúInternet Mathematics: Yandex.Maps‚Äù - the experience of our participation and the description of the winning algorithm</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="More than a year has passed since the end of the competition " Internet Mathematics: Yandex.Maps ", but we are still being asked about the algorithm t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The competition ‚ÄúInternet Mathematics: Yandex.Maps‚Äù - the experience of our participation and the description of the winning algorithm</h1><div class="post__text post__text-html js-mediator-article">  More than a year has passed since the end of the competition " <a href="http://imat2011.yandex.ru/">Internet Mathematics: Yandex.Maps</a> ", but we are still being asked about the algorithm that brought us <a href="http://yandex.livejournal.com/140651.html">victory</a> in this competition.  Having learned that Yandex recently announced the launch of the next " <a href="http://switchdetect.yandex.ru/">Internet Mathematics</a> ", we decided to share the experience of our last year‚Äôs participation and describe our approach.  The developed algorithm was able with an accuracy of 99.44% to correctly identify unnecessary images in a series of panoramic images, for example, like this: <br><br><img src="https://habrastorage.org/storage2/0ff/eac/aef/0ffeacaef1e51cbdf4f921318e4a12dc.jpg"><br><br>  In this article, we describe the basic ideas of the algorithm and provide details for those interested in it, talk about the lessons learned and how it all was. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The source code of our solution is available on <a href="https://github.com/ilysenkov/imat2011">github</a> (C ++ using <a href="http://opencv.org/">OpenCV</a> ). <br><a name="habracut"></a><br><h4>  Description of the competitive task </h4><br>  In 2011, Yandex offered the contestants an interesting task on computer vision, so our <a href="http://itseez.com/">company</a> specializing in this area would be a sin not to take part.  The task was to search for unnecessary images in a series of panoramic images with Yandex.Maps.  In each series 5 images were given, some of which (3, 4 or all 5) were made in one place and approximately at the same time, and the rest were superfluous.  For example, images 1, 3 and 5 in the series above constitute a panorama, and images 2 and <nobr>4 are superfluous</nobr> .  A total of 6000 episodes were given and it was required to find extra shots in each of them. <br><br><h4>  From start to finish </h4><br>  Having learned about the competition from our corporate newsletter, each of us downloaded data for ourselves, stubbornly looked at the pictures for several days (there was something <nobr>to do - there were only</nobr> 6000 x 5 pictures = 30,000) and hoped to win alone.  It soon became clear that the task was actually not as simple as it seemed at first glance.  Therefore, we decided to get together, discuss ideas and hold talks on a possible joint effort.  As it turned out, we had very different approaches to solving the problem and it would be great to complement each other in the same team.  So we did, and then everything follows the standard scheme: we created a common code repository, wrote an infrastructure for testing the accuracy of algorithms, and each set about working out his own ideas.  We studied suitable articles, periodically brainstormed, tried new ideas and discarded broken ones ... So a month went by. <br><br>  And then came the final <nobr>day - according to the</nobr> terms of the competition, on the last day, Yandex gave out a new data set, on which it was necessary to drive the algorithm and get an answer in 24 hours.  Our team was facing a sleepless night at work, so the first priority was the purchase of chips and all that was needed for productive programming.  After gaining access to the final data set, we launched the best version of our algorithm, looked at a new batch of 30,000 images and until the last generated new ideas to take into account new data properties (though by 5 am some kind of nonsense was obtained, which until now then we have a reason for joking).  Everything turned out to be not in vain: in a few days we learned that our team ‚ÄúMythical Nizhny Novgorod‚Äù became the winner of the competition! <br><br>  Next, we will talk about what algorithm we have in the end. <br><br><h4>  The basic ideas of the algorithm </h4><br>  So, the task is to divide a series of images into groups: the desired panorama and superfluous images.  To find such a partition, it is necessary for any pair of images to be able to assess the plausibility of the fact that they belong to the same panorama.  To do this, we introduced two measures of similarity for a pair of images based on complementary approaches: a comparison of color histograms of images and the search for common parts in images.  We combined these two measures into one and used the obtained measure to divide images into groups (clustering).  The measure of each approach could be used separately, but the combination allows you to combine their strengths. <br><br>  <a href="http://docs.opencv.org/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.html">Histograms</a> allow you to compare the frequency of occurrence of colors on one image with frequencies on the other.  Since, according to the description of the task, the pictures in one panorama depict one scene at the same time from close camera positions, their color characteristics are usually similar to each other, and in extra pictures they are usually different.  Therefore, the distance between the histograms gives a suitable estimate of the proximity of the images.  For example, superfluous images from the series given at the beginning of the article are much darker than panoramic images, therefore dark colors will dominate in the histograms of extra images, and these histograms will be far from the panoramic image histograms. <br><br>  The second approach, the search for common parts in the images, makes it possible to determine that two images belong to the same panorama in the case when there is a sufficient intersection between them, for example, the same object is found on them.  This is achieved by using a popular approach in computer vision: the two key images are compared to the characteristic key points, after which a search is made for correspondences between them, for example, by comparing the neighborhoods of these points.  If you managed to find many similar neighborhoods, then most likely they refer to the same objects and these are two images of the same scene.  In this approach, the measure of proximity is the number of similar neighborhoods.  For example, the work of this approach is illustrated in Fig.  1 and 2. <br><br><img src="https://habrastorage.org/storage2/48c/ebb/ad6/48cebbad69313f91956ddf09d4ffd450.jpg"><br>  <i>Fig.</i>  <i>1. An example of a series in which the search for common parts gives the correct classification, since the photographs contain parts of the same building.</i>  <i>At the same time, color histograms of images 1 and 4 are very different from the others, therefore the histogram approach considers these two images as superfluous, which is in fact incorrect.</i> <br><br><img src="https://habrastorage.org/storage2/ce7/163/f97/ce7163f973852fe88dd9975cfd6710f8.jpg"><br>  <i>Fig.</i>  <i>2. Corresponding key points found by the algorithm.</i> <br><br>  These two approaches complement each other.  The histogram approach can work even when there are no intersections and common parts between images of the same panorama (see the example in Fig. 3).  However, in some cases there is an intersection between the images, but the other parts are very different in color (for example, the sun went down behind a cloud).  In this case, the histograms will be far from each other, and the search for common parts can determine that these images still belong to the same panorama even by a small overlap (see the example in Fig. 1 and 2). <br><br><img src="https://habrastorage.org/storage2/004/a63/5f9/004a635f9e49e07a3e0aaba4f3e7f5f5.jpg"><br>  <i>Fig.</i>  <i>3. An example of a series in which between images of one panorama (3, 4 and 5) there are no areas with common key points, but thanks to the color histograms, the correct classification is obtained (images 1 and <nobr>2 are redundant</nobr> ).</i> <br><br>  Experiments have shown <i>that</i> most of the correct answers can give a histogram approach alone, and the addition of the second approach only improves the classification in a few cases. <br><br>  For those interested in computer vision and details of these two approaches, the following two sections are written, and all others are invited to proceed to the <a href="https://habr.com/ru/company/intel/blog/158645/">Combination of two approaches</a> and <a href="https://habr.com/ru/company/intel/blog/158645/">Results</a> . <br><br><h4>  Details of the histogram approach </h4><br>  During the implementation, the color space <a href="http://docs.opencv.org/modules/imgproc/doc/miscellaneous_transformations.html">YCrCb</a> was used, since it explicitly highlighted the brightness component.  This allows you to take a smaller number of bins (grouping intervals) for this component and thereby increase the stability of the algorithm to changes in light and shadows. <br><br>  We also added another <nobr>one</nobr> to these three components (Y, Cr, Cb) for constructing histograms <nobr>- the ordinate of the</nobr> pixel in the image to take into account the spatial arrangement of colors.  This idea allows you to more accurately compare the colors of objects that are usually located at the same height: the road should be compared with the road, grass with grass, sky with sky.  Such splitting of the image into horizontal stripes was introduced due to the fact that in the contest data the images within one panorama are shifted relative to each other mainly horizontally. <br><br>  Thus, a four-dimensional histogram was constructed with 12 bins for brightness, 256 bins for each of the two color components and 4 bins for the pixel ordinate. <br><br>  To calculate the distance between histograms, their intersection distance was used ( <a href="http://docs.opencv.org/modules/imgproc/doc/histograms.html">Histogram Intersection Distance</a> ).  For single-resolution images, this distance actually reflects the number of similar pixels between two images. <br><br><h4>  Details of the approach based on local features </h4><br>  One of the most successful methods in computer vision is the use of unique local features in the image, such as angles or blob centers (small homogeneous areas).  This approach is applicable if the scene is rather textural, that is, it contains a large number of such local features.  The images from Yandex.Maps depict textural street scenes, so this approach is reasonable to use.  However, there are several difficulties in the competitive data set: images have a low resolution (300x300 pixels) and there are often large non-textural areas (asphalt, sky, ...).  Such low resolution results in low <a href="http://www.robots.ox.ac.uk/~vgg/research/affine/det_eval_files/vibes_ijcv2004.pdf">repeatability of the</a> detector of local features: the key point found in one of the images will often not be detected in another image of the same panorama, and we will not be able to determine that there are matches between the images.  To solve these problems, we set up such parameters of the detector of local features so that it finds a large number of key points in the image (~ 10k).  However, this leads to the fact that the features on the compared images become less unique, and false matches are often found when searching for corresponding features.  Therefore, we applied several additional filterings to select the correct matches.  We turn to the details. <br><br><ol><li> As a detector of local features, we used <a href="http://www.edwardrosten.com/work/fast.html">FAST</a> , which allows you to quickly find angles in the image.  For the stability of the algorithm to change the distance to the objects to be shot at different camera positions, it is necessary to detect local features of different scales.  To do this, we scale the images using 4 levels of the <a href="http://docs.opencv.org/doc/tutorials/imgproc/pyramids/pyramids.html">Gaussian pyramid of images</a> , launch FAST at each level and combine the sets of found key points. </li><li>  For the key points of each image, we calculate the descriptors (descriptors) of their neighborhoods.  We used the <a href="http://www.vision.ee.ethz.ch/~surf/index.html">SURF</a> descriptor, which is faster than the more well-known <a href="http://en.wikipedia.org/wiki/Scale-invariant_feature_transform">SIFT</a> , but has a comparable quality. </li><li>  Next, we find the corresponding key points in the two images, using the calculated descriptors, which are 64-dimensional vectors of real numbers.  For this purpose, for each descriptor of one image, the nearest descriptor is located in another image along the <a href="http://en.wikipedia.org/wiki/Taxicab_geometry">L1 distance</a> between the descriptor vectors.  This distance is more resistant to noise than the standard Euclidean distance.  Since we have a huge number of local features, searching the nearest descriptor by brute force would take too much time.  Therefore, we used an approximate search for the nearest descriptor using the <a href="http://www.cs.ubc.ca/~mariusm/index.php/FLANN/FLANN">FLANN</a> library integrated into OpenCV. </li><li>  Found matches between local features are filtered by the criterion of <a href="http://docs.opencv.org/modules/features2d/doc/common_interfaces_of_descriptor_matchers.html">cross-check</a> reliability, which of all matches leaves only one-to-one. </li><li>  We filter the remaining matches by the <i>y</i> coordinate: if the <i>y</i> coordinates of the points connected by a match differ by more than 100 pixels (the parameter value is chosen experimentally), then we consider such a match to be false.  This filter is based on the fact that only horizontal panoramas were found in the competitive images. </li><li>  We calculate the <a href="http://en.wikipedia.org/wiki/Homography">homology</a> matrix according to the <a href="http://ru.wikipedia.org/wiki/RANSAC">RANSAC</a> scheme using a set of one-to-one correspondences between local singularities.  According to the <a href="http://www.robots.ox.ac.uk/~vgg/hzbook/">theory</a> , between the points of the images there is a homography transformation (with an arbitrary change in the position of the camera), if their prototypes in the 3D world lie on the same plane.  For non-planar objects, we can also search for a homography transformation, if the deviation of the object points from the flat shape is much smaller than the distance from the camera to the object.  The search for homography for competitive images of panoramas is justified by the following facts: 1) the presence of planes is characteristic of objects of urban scenes;  2) objects are often sufficiently removed to be considered flat;  3) the computation of homography requires fewer iterations of the RANSAC than is necessary to find the <a href="http://en.wikipedia.org/wiki/Fundamental_matrix_(computer_vision)">fundamental matrix</a> . <br><br>  In the RANSAC homography search cycle, several heuristics were used to assess the quality of the next homography hypothesis: filter on the minimum singular number of the homography matrix, filter inlayers (matches that satisfy the current hypothesis on the homography matrix) for consistency of changes in orientations of the detected angles, replacing closely spaced inlays with one .  As a result, the homography was chosen, which gives the largest number of inlays. </li><li>  As a measure of proximity between images, the number of inlays for the found homography matrix is ‚Äã‚Äãtaken. </li></ol><br><br>  In many images, this algorithm lacked even a small intersection between images in order to find a sufficient number of corresponding key points.  So, an interesting and unexpected example of the operation of the described algorithm is shown in Fig.  four. <br><br><img src="https://habrastorage.org/storage2/af9/a0c/349/af9a0c3494f285213c274f97869913e8.jpg"><br>  <i>Fig.</i>  <i>4. An amusing situation when pictures can be combined into one panorama by the presence of the same clouds in the sky.</i>  <i>In this pair of histogram snapshots, they feel insecure because of the large shadow, but local features and cloudy weather save the situation.</i> <br><br><a name="Mixing"></a><h4>  Combining two approaches </h4><br>  The described approaches allow calculating two types of distances between images.  To take both approaches into account, we combine these distances as follows: <br><br>  <i>d</i> <sub><i>comb</i></sub> = min ( <i>c</i> * <i>d</i> <sub><i>hist</i></sub> , <i>d</i> <sub><i>feat</i></sub> ), <br><br>  Where <br>  <i>d</i> <sub><i>comb</i></sub> - combined distance <br>  <i>d</i> <sub><i>hist</i></sub> and <i>d</i> <sub><i>feat</i></sub> are the distances normalized to the interval [0, 1], calculated using the approaches on the histograms and local features, respectively; <br>  <i>c</i> - coefficient less than one, which allows to give greater priority to the results of the algorithm that uses histograms. <br><br>  To combine the two approaches, the minimum function was chosen, because if at least one of the two distances is small, then the images are really close. <br><br>  Having calculated pairwise combined distances between images of one series, we perform hierarchical clustering ( <a href="http://en.wikipedia.org/wiki/Hierarchical_clustering">Agglomerative hierarchical clustering using single-linkage</a> ) of these images.  The cluster with the maximum number of elements is recognized as the desired panorama, and the remaining images are superfluous.  Such hierarchical clustering is very natural for this task, since within one panorama there may be distant non-intersecting images that are connected by a chain of other panorama images that are close to each other, and this method allows you to combine such images into one panorama. <br><br><a name="Results"></a><h4>  results </h4><br>  So, after working hard on our algorithm, he showed a very good result on the final data set: 99.44% of images were correctly classified, which allowed to take first place in the competition.  And also get a prize in the amount of 100,000 rubles to celebrate the victory :-) <br><br><h4>  Contest Lessons </h4><br>  As a conclusion, we want to share a few lessons that we have learned or consolidated by participating in the competition.  They are definitely useful to us. <br><ul><li>  <b>Without friends I have a little bit, but with friends there are many.</b>  Participating as a team was our best decision ever.  None of us could have won the competition alone.  Together we generated ideas, were able to test many different approaches in parallel, used everyone‚Äôs knowledge and experience, supported and inspired each other and did not let anyone fall asleep on the last night before the deadline, continuing to generate delusions at 5 am.  And the victory is more pleasant to celebrate all together. </li><li>  <b>Woe from mind (or overfitting).</b>  At the beginning of the competition, Yandex provided marked data for preliminary testing of algorithms.  However, in many series it was almost impossible to find the extra images even to a person (see the example in Fig. 5). <br><br><img src="https://habrastorage.org/storage2/bdf/3a9/66f/bdf3a966fdac0fb252407fb05960b7a5.jpg"><br>  <i>Fig.</i>  <i>5. An example of a very difficult series.</i>  <i>What do you think, what images are superfluous?</i> <br><div class="spoiler">  <b class="spoiler_title">Answer</b> <div class="spoiler_text">  1 and 3 are superfluous images, and 2, 4 and 5 form one panorama.  If you look closely, then extra shots were taken at other times of the year. </div></div><br>  The problem with the unsolvable series was raised by the participants at the contest forum, and the organizers promised not to allow this in the final data set.  This meant that the final data set would have other properties than the preliminary set.  Thus, the fundamental assumption of machine learning algorithms that the training and test sample should be generated by a single probability distribution was not fulfilled.  Therefore, we decided to use the simplest algorithms with a small number of adjustable parameters, since otherwise the risk of retraining is extremely high.  As practice has shown, this was a very valuable observation, since several other participants using advanced machine learning algorithms really retrained and showed not such a good result on the final data as on the preliminary ones. </li><li>  <b>Behold the root.</b>  To test ideas, understand the behavior and problems of algorithms, and just debug, you need the correct visualization (of both the final results and intermediate steps).  This allowed us to feel the specifics of the problem and the data, to move more clearly and quickly to the right solutions. </li><li>  <b>Small, yes removed.</b>  The solution of the problem is to start with simple approaches: they are very easy and quick to implement, but the result will be immediately obtained for comparison (baseline).  Moreover, even a primitive approach can show surprisingly good results, and if you do a little bit of it for the specifics of the task, then it can become invincible.  This is what happened to us with histograms: a very simple idea that brought us the main result (local features increased the classification accuracy by less than one percent).  In this case, the calculation of all the final data by histograms took only a couple of minutes, and we waited several hours for the results of the approach on local features. </li><li>  <b>Right hook, left hook.</b>  Attacking a task is better from different angles: it is useful to develop alternate problem solving strategies and have the tools that can cope with all the discovered properties of the task.  For example, we tried approaches that take into account color, gradients and their orientations, spatial position, absence and the presence of overlaps between images.  As a result, to improve the accuracy of classification, we combined the solutions found. </li><li>  <b>Don't worry, be happy.</b>  We did not forget to enjoy the process, appreciated the opportunity to learn something new and communicate with each other.  It was a very pleasant and rewarding experience.  Winning a <nobr>competition is</nobr> , of course, great, but we often remember not the moment of the announcement of results, but many other funny and crazy moments during the time of participation in the <nobr>competition - this is</nobr> for life :-) </li></ul><br>  See you at the next contests on computer vision! <br><br><h5>  Useful materials on Habr√© </h5><br><ul><li>  Algorithm of another team: <a href="http://habrahabr.ru/post/132555/">Solving the problem ‚ÄúYandex Internet Mathematics - 2011‚Äù.</a>  <a href="http://habrahabr.ru/post/132555/">Definition of visual similarity of images</a> </li><li>  Detector and SURF Descriptor Description: <a href="http://habrahabr.ru/post/103107/">Detecting Persistent Image</a> Characteristics <a href="http://habrahabr.ru/post/103107/">: SURF Method</a> </li><li>  Detector and SIFT Descriptor Description: <a href="http://habrahabr.ru/post/106302/">Building SIFT Descriptors and Image Matching Task</a> </li><li>  Description of an alternative approach to filtering matches: <a href="http://habrahabr.ru/post/154975/">Filtering false matches between images using a dynamic matching graph</a> </li><li>  Tutorial about the standard scheme for applying local features (detector + handle + matching search + homography): <a href="http://habrahabr.ru/post/155651/">OpenCV 2.4 flat object recognition</a> </li></ul><br><br>  <i>The authors of the article:</i> <i><br></i>  <i>Maria Dimashova (Itseez, research engineer), <a href="http://habrahabr.ru/users/mdim/" class="user_link">mdim</a></i> <i><br></i>  <i>Ilya Lysenkov (Itseez, research engineer), <a href="http://habrahabr.ru/users/billys/" class="user_link">billys</a></i> </div><p>Source: <a href="https://habr.com/ru/post/158645/">https://habr.com/ru/post/158645/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../158633/index.html">Apple sells Stratocaster guitars for $ 199.95</a></li>
<li><a href="../158637/index.html">Engineering approach to the training of specialists in software engineering</a></li>
<li><a href="../158639/index.html">What each developer needs to know about character encodings and character sets</a></li>
<li><a href="../158641/index.html">Piping hot. The hottest new items from KYOCERA Document Solutions</a></li>
<li><a href="../158643/index.html">Cloud Protection: AppFuscator 2.0</a></li>
<li><a href="../158649/index.html">Review GALAXY Camera (Part II)</a></li>
<li><a href="../158651/index.html">Distribution on trackers, where it is difficult to distribute, or how to defeat a sidboxer</a></li>
<li><a href="../158653/index.html">International numbering with MTT bonus</a></li>
<li><a href="../158655/index.html">Presentation Meizu MX2, November 27, Beijing</a></li>
<li><a href="../158657/index.html">How do you prefer to listen to podcasts: with or without background music?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>