<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Meet Apache Spark</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello, dear readers! 

 We are finally starting to translate a serious book about the Spark framework: 



 Today we bring to your attention the trans...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Meet Apache Spark</h1><div class="post__text post__text-html js-mediator-article">  Hello, dear readers! <br><br>  We are finally starting to translate a serious book about the Spark framework: <br><br><img src="https://habrastorage.org/files/c27/003/7a9/c270037a98284ab8a8ffd12464e926da.jpg">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Today we bring to your attention the translation of the review article on the possibilities of Spark, which, we believe, can rightfully be called a bit stunning. <br><br><a name="habracut"></a><br><br>  I first heard about Spark at the end of 2013, when I became interested in Scala - Spark was written in this language.  Somewhat later, I began to develop, for the sake of interest, a project from the field of Data Science, dedicated to <a href="https://www.kaggle.com/c/titanic">predicting the survival of the passengers of the Titanic</a> .  It turned out to be a great way to learn about Spark programming and its concepts.  I strongly recommend getting to know all <a href="http://www.toptal.com/spark">novice Spark developers</a> . <br><br>  Today, Spark is used in many of the largest companies, such as Amazon, eBay and Yahoo!  Many organizations operate Spark in clusters of thousands of nodes.  According to the Spark FAQ, the largest of these clusters has more than 8,000 nodes.  Indeed, Spark is a technology that is worth taking note of and exploring. <br><br><img src="https://habrastorage.org/files/b1b/eb7/cce/b1beb7cce0514786ace50f0b3cb56a5c.jpg"><br><br>  This article offers an introduction to Spark, examples of usage, and sample code. <br><br>  <b>What is Apache Spark?</b>  <b>Introduction</b> <br><br>  <a href="https://spark.apache.org/">Spark</a> is an Apache project that is positioned as a tool for "lightning-fast cluster computing."  The project is being developed by a thriving free community, currently the most active of the Apache projects. <br><br>  Spark provides a fast and versatile data processing platform.  Compared to Hadoop, Spark accelerates the work of programs in memory by more than 100 times, and on disk - more than 10 times. <br><br>  In addition, Spark code is written faster, since here you will have more than 80 high-level operators.  To appreciate this, let's look at the analogue ‚ÄúHello World!‚Äù From the world of BigData: an example of word counting (Word Count).  A program written in Java for MapReduce would contain about 50 lines of code, and in Spark (Scala) we need only: <br><br><pre><code class="scala hljs">sparkContext.textFile(<span class="hljs-string"><span class="hljs-string">"hdfs://..."</span></span>) .flatMap(line =&gt; line.split(<span class="hljs-string"><span class="hljs-string">" "</span></span>)) .map(word =&gt; (word, <span class="hljs-number"><span class="hljs-number">1</span></span>)).reduceByKey(_ + _) .saveAsTextFile(<span class="hljs-string"><span class="hljs-string">"hdfs://..."</span></span>)</code> </pre> <br><br>  When studying Apache Spark, it is worth noting another important aspect: it provides a ready-made interactive shell (REPL).  With the help of REPL, you can test the result of executing each line of code without having to first program and complete the entire task.  Therefore, it is possible to write ready-made code much faster, moreover, situational analysis of data is provided. <br><br>  In addition, Spark has the following key features: <br><br><ul><li>  Currently provides APIs for Scala, Java, and Python, as well as support for other languages ‚Äã‚Äã(for example, R) </li><li>  Well integrated with the Hadoop ecosystem and data sources (HDFS, Amazon S3, Hive, HBase, Cassandra, etc.) </li><li>  Can work on clusters running Hadoop YARN or Apache Mesos, and also work offline </li></ul><br><br>  The Spark core is complemented by a set of powerful high-level libraries that seamlessly fit into it within the framework of the same application.  Currently, such libraries include SparkSQL, Spark Streaming, MLlib (for machine learning), and GraphX ‚Äã‚Äã‚Äî all of which will be discussed in detail in this article.  Other libraries and Spark extensions are also under development. <br><br><img src="https://habrastorage.org/files/805/aa7/fcf/805aa7fcf1dd484e869570e0e1b477e0.jpg"><br><br>  <b>Spark Core</b> <br>  <a href="">The Spark core</a> is a basic engine for large-scale parallel and distributed data processing.  The kernel is responsible for: <br><br><ul><li>  memory management and recovery after failures </li><li>  cluster scheduling </li><li>  interaction with storage systems </li></ul><br><br>  Spark introduces the concept of <a href="http://spark.apache.org/docs/1.2.1/programming-guide.html">RDD</a> (sustainable distributed data set) - an immutable fault-tolerant distributed collection of objects that can be processed in parallel.  RDD can contain objects of any type;  RDD is created by loading an external data set or distributing a collection from the main program (driver program).  RDD supports two types of operations: <br><ul><li>  <a href="http://spark.apache.org/docs/1.2.1/programming-guide.html">Transformations</a> are operations (eg, mapping, filtering, merging, etc.) performed on RDD;  the result of the transformation becomes the new RDD containing its result. </li><li>  <a href="http://spark.apache.org/docs/1.2.1/programming-guide.html">Actions</a> are operations (for example, reduction, counting, etc.) that return the value resulting from some calculations in RDD. </li></ul><br><br>  Transformations in Spark are carried out in the "lazy" mode - that is, the result is not calculated immediately after the transformation.  Instead, they simply ‚Äúmemorize‚Äù the operation to be performed and the data set (for example, a file) on which to perform the operation.  Calculation of transformations occurs only when an action is invoked, and its result is returned to the main program.  Thanks to this design, Spark‚Äôs efficiency is enhanced.  For example, if a large file was converted in various ways and passed to the first action, Spark will process and return the result only for the first line, and will not work on the whole file in this way. <br><br>  By default, each transformed RDD can be recalculated whenever you perform a new action on it.  However, RDDs can also be stored in memory for a long time using the storage or caching method;  in this case, Spark will keep the necessary elements on the cluster, and you will be able to request them much faster. <br><br>  <b>SparkSQL</b> <br><br>  <a href="https://spark.apache.org/sql/">SparkSQL</a> is a Spark component that supports querying data either using SQL or using the <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual">Hive Query Language</a> .  The library originated as an Apache Hive port for working on top of Spark (instead of MapReduce), and is now integrated with the Spark stack.  It not only provides support for various data sources, but also allows interweaving SQL queries with code transformations;  It turns out a very powerful tool.  The following is an example of a Hive-compatible query: <br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">// sc ‚Äì   SparkContext. val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc) sqlContext.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)") sqlContext.sql("LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src") //    HiveQL sqlContext.sql("FROM src SELECT key, value").collect().foreach(println)</span></span></code> </pre><br><br>  <b>Spark streaming</b> <br><br>  <a href="https://spark.apache.org/streaming/">Spark Streaming</a> supports real-time streaming processing;  Such data can be log files of the working web server (for example, Apache Flume and HDFS / S3), information from social networks, for example, Twitter, as well as various message queues such as Kafka.  "Under the hood" Spark Streaming receives input data streams and splits the data into packets.  They are further processed by the Spark engine, after which the final data stream is generated (also in batch form) as shown below. <br><br><img src="https://habrastorage.org/files/577/a09/ca8/577a09ca8dec428a887d6948db024a1b.jpg"><br><br>  The Spark Streaming API exactly corresponds to the Spark Core API, so programmers can easily work with both packet and stream data at the same time. <br><br>  <b>MLlib</b> <br><br>  <a href="https://spark.apache.org/mllib/">MLlib</a> is a machine learning library that provides various algorithms designed for horizontal scaling on a cluster for classification, regression, clustering, co-filtering, etc.  Some of these algorithms also work with streaming data ‚Äî for example, linear regression using the usual least squares method or k-means clustering (the list will expand soon).  <a href="http://mahout.apache.org/">Apache Mahout</a> (machine learning library for Hadoop) has already left MapReduce, now its development is being done in conjunction with Spark MLlib. <br><br>  <b>Graphx</b> <br><br>  GraphX ‚Äã‚Äãis a library for manipulating graphs and performing parallel operations with them.  The library provides a universal tool for ETL, research analysis and graph-based iterative computing.  In addition to the built-in operations for graph manipulation, a library of ordinary algorithms for working with graphs, for example, PageRank, is also provided here. <br><br>  <b>How to use Apache Spark: event detection example</b> <br><br>  Now that we‚Äôve figured out what Apache Spark is, let's think about what tasks and problems will be solved most effectively with it. <br><br>  Recently I came across <a href="http://www.ymatsuo.com/papers/www2010.pdf">an article</a> about an experiment on recording earthquakes by analyzing the flow of Twitter.  By the way, the article demonstrated that this method allows one to learn about an earthquake more quickly than from reports of the Japanese Meteorological Agency.  Although the technology described in the article is not similar to Spark, this example seems to me interesting in the context of Spark: it shows how you can work with simplified code fragments and without code-glue. <br><br>  First, it will be necessary to filter out those tweets that seem relevant to us ‚Äî for example, with the mention of ‚Äúearthquake‚Äù or ‚Äútremors‚Äù.  This can be done easily with Spark Streaming, like this: <br><br><pre> <code class="scala hljs"><span class="hljs-type"><span class="hljs-type">TwitterUtils</span></span>.createStream(...) .filter(_.getText.contains(<span class="hljs-string"><span class="hljs-string">"earthquake"</span></span>) || _.getText.contains(<span class="hljs-string"><span class="hljs-string">"shaking"</span></span>))</code> </pre><br><br>  Then we will need to perform a certain semantic analysis of tweets to determine if the jolts mentioned in them are relevant.  Probably, such tweets as ‚ÄúEarthquake!‚Äù Or ‚ÄúNow shakes‚Äù will be considered positive results, and ‚ÄúI am at a seismological conference‚Äù or ‚ÄúYesterday was shaking terribly‚Äù - negative.  The authors of the article used the support vector machine (SVM) for this purpose.  We will do the same, just implement the <a href="http://spark.apache.org/docs/1.2.1/mllib-linear-methods.html">streaming version as well</a> .  The resulting sample code from MLlib would look something like this: <br><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//    ,  ,      LIBSVM val data = MLUtils.loadLibSVMFile(sc, "sample_earthquate_tweets.txt") //     (60%)   (40%). val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L) val training = splits(0).cache() val test = splits(1) //   ,    val numIterations = 100 val model = SVMWithSGD.train(training, numIterations) //   ,    model.clearThreshold() //       val scoreAndLabels = test.map { point =&gt; val score = model.predict(point.features) (score, point.label) } //    val metrics = new BinaryClassificationMetrics(scoreAndLabels) val auROC = metrics.areaUnderROC() println("Area under ROC = " + auROC)</span></span></code> </pre><br><br>  If the percentage of correct predictions in this model suits us, we can proceed to the next stage: respond to the detected earthquake.  For this, we will need a certain number (density) of positive tweets received during a certain period of time (as shown in the article).  Please note: if tweets are accompanied by geolocation information, then we can determine the coordinates of the earthquake.  Armed with this knowledge, we can use SparkSQL and query the existing Hive table (where data about users who want to receive notifications about earthquakes are stored), retrieve their email addresses and send them personalized warnings like this: <br><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">// sc ‚Äì   SparkContext. val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc) // sendEmail ‚Äì   sqlContext.sql("FROM earthquake_warning_users SELECT firstName, lastName, city, email") .collect().foreach(sendEmail)</span></span></code> </pre><br><br>  <b>Other uses for Apache Spark</b> <br><br>  Potentially, the scope of application of Spark, of course, is far from being limited to seismology. <br>  Here is an indicative (that is, by no means exhaustive) selection of other practical situations where high-speed, diverse and bulky processing of big data is required, for which Spark is so well suited: <br><br>  In the gaming industry: processing and detection of patterns describing game events coming in a continuous stream in real time;  as a result, we can immediately react to them and make good money on it, using player retention, targeted advertising, auto-correction of the difficulty level, etc. <br><br>  In e-commerce, real-time transaction information can be transferred to a stream clustering algorithm, for example, by <a href="https://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html">k-average</a> or be subjected to joint filtering, as in the case of <a href="https://databricks.com/blog/2014/07/23/scalable-collaborative-filtering-with-spark-mllib.html">ALS</a> .  Results can even be combined with information from other unstructured data sources ‚Äî for example, customer reviews or reviews.  Gradually, this information can be used to improve the recommendations in the light of new trends. <br><br>  In the financial or security field, the Spark stack can be used to detect fraud or intruders, or to authenticate with risk analysis.  Thus, you can get first-class results by collecting huge amounts of archived logs, combining them with external data sources, for example, with information about data leaks or hacked accounts (see, for example, <a href="https://haveibeenpwned.com/">https://haveibeenpwned.com/</a> ), and also use connection / query information, focusing, for example, on geolocation by IP or on time data <br><br>  <b>Conclusion</b> <br><br>  So, Spark helps to simplify non-trivial tasks associated with high computational load, processing large amounts of data (both in real time and archived), both structured and unstructured.  Spark provides seamless integration of complex features ‚Äî for example, machine learning and algorithms for working with graphs.  Spark carries the processing of Big Data to the masses.  Try it - you will not regret! </div><p>Source: <a href="https://habr.com/ru/post/276675/">https://habr.com/ru/post/276675/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../276661/index.html">Recommendations on stream</a></li>
<li><a href="../276663/index.html">Load testing from the cloud</a></li>
<li><a href="../276665/index.html">Is Tox as safe as it is painted?</a></li>
<li><a href="../276669/index.html">Review of physics in Sonic games. Part 1: hard tiles</a></li>
<li><a href="../276673/index.html">Monsieur, your problem solving skills are not up to par, or how I failed one interview</a></li>
<li><a href="../276677/index.html">AI, BigData & HPC Digest # 4</a></li>
<li><a href="../276679/index.html">What's new Git 2.7 offers</a></li>
<li><a href="../276681/index.html">List of technical and IT conferences 2016. Part # 1</a></li>
<li><a href="../276683/index.html">Conversion design: creating online services that users truly love</a></li>
<li><a href="../276685/index.html">Photoshop, Sketch UI / UX designers toolkit: what to choose for developing interfaces?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>