<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Probabilistic and informational analysis of measurement results in Python</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="There is no more useful tool for research, than the theory confirmed by practice. 

 Why do we need information theory of measurements 
 In the previo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Probabilistic and informational analysis of measurement results in Python</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/web/c7b/c73/5d5/c7bc735d50f54c739cdb266cfcdd8d33.png"></div><br>  There is no more useful tool for research, than the theory confirmed by practice. <br><br><h3>  Why do we need information theory of measurements </h3><br>  In the previous publication [1] we considered the selection of the law of distribution of a random variable according to the statistical sampling data and only mentioned the informational approach to the analysis of measurement errors.  Therefore, we will continue the discussion of this relevant topic. <br><br>  The advantage of the informational approach to the analysis of measurement results is that the size of the entropy uncertainty interval can be found for any law of random error distribution.  This eliminates "misunderstandings" with an arbitrary choice of values ‚Äã‚Äãof confidence. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In addition, the set of probabilistic and informational characteristics of the sample can more accurately determine the nature of the distribution of random error.  This is explained by the extensive base of the numerical values ‚Äã‚Äãof such parameters as the entropy coefficient and the trajectory for various distribution laws and their superpositions. <br><a name="habracut"></a><br><h3>  On the essence of information and probabilistic analysis </h3><br>  Here the key word is uncertainty.  Measurement is considered as a process, as a result of which the initial uncertainty in information about the measured value is reduced - x.  A quantitative <b>measure of uncertainty is entropy</b> - H (x).  More often, we encounter discrete values ‚Äã‚Äãof the random variable x1, x2‚Ä¶ xn, which is caused by the widespread use of computer technology.  For such quantities, we write only one formula that explains a lot. <br><br><img src="https://habrastorage.org/web/eef/dfb/d0f/eefdfbd0f220415b8124d5fa538526ac.PNG">  (one) <br><br>  Where p_i is the probability that the random variable x has taken the value x_i.  Since 0‚â§p_i‚â§1, and while lg‚Å°p_i &lt;0, then to obtain H (x) ‚â•0, the sum in the formula is a minus sign. <br><br>  Entropy is measured in units of information.  The information units on the above formula depend on the base of the logarithm.  For the decimal logarithm, this is <b>dit</b> .  For the natural logarithm - <b>nit</b> .  For obvious reasons, the most frequently used binary logarithms are those at which entropy is measured in <b>bits</b> . <br><br>  In the process of measurement, the initial uncertainty of x decreases as our knowledge of x increases.  However, even after the measurement, the residual uncertainty H (‚àÜ) remains due to the measurement error ‚àÜ. <br><br>  The residual entropy H (‚àÜ) can be determined by the above formula, substituting the error ‚àÜ for x and, omitting the intermediate calculations, to obtain the entropy value of the error ‚àÜ for any distribution law. <br><br><img src="https://habrastorage.org/web/cf5/bbe/427/cf5bbe42793b45859c81e17b3b15afe3.PNG">  (2) <br><br><h3>  How to get the entropy value of the error according to the measurement results </h3><br>  To do this, first the sequence of discrete values ‚Äã‚Äãof the random variable of the error must be divided into intervals, followed by counting the frequencies of these values ‚Äã‚Äãfalling into each interval.  Within the interval, the probability of the occurrence of these values ‚Äã‚Äãof the error is assumed to be constant, equal to the ratio of the frequency of hitting this interval to the number of measurements taken. <br><br>  In other words, you need to reproduce the part of the procedure that the hist () plotting function from the matplotlib library performs.  Let us demonstrate this in the following simple procedure, comparing the resulting diagram with the diagram obtained with hist (). <br><br><div class="spoiler">  <b class="spoiler_title">Program for comparing data processing methods</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy.stats <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> uniform <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">diagram</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(a)</span></span></span><span class="hljs-function">:</span></span> a.sort()<span class="hljs-comment"><span class="hljs-comment">#   n=len(a)#     m= int(10+np.sqrt(n))#    d=(max(a)-min(a))/m#   x=[];y=[] for i in np.arange(0,m,1): x.append(min(a)+d*i)#      k=0 for j in a: if min(a)+d*i &lt;=j&lt;min(a)+d*(i+1): k=k+1 y.append(k)#         plt.title(" ") plt.bar(x,y, d) plt.grid(True) plt.show() plt.title("   hist()") plt.hist(a,m) plt.grid(True) plt.show() a=uniform.rvs(size=500) #     diagram(a)</span></span></code> </pre> <br></div></div><br>  Compare the resulting diagrams. <br><img src="https://habrastorage.org/webt/hm/7k/c9/hm7kc91yd7efcip4dab_v-m40gs.png"><br><img src="https://habrastorage.org/webt/im/yo/bd/imyobddzogq6g7awkngq8p6v5se.png"><br><br>  The diagrams are almost identical. A larger number by one than in the hist () function, the number of splitting intervals will only improve the result of the analysis.  Therefore, we can proceed to the second stage of obtaining the entropy error value, for this we use the relation given in [2] and obtained from relation (2). <br><br><img src="https://habrastorage.org/web/b39/4ad/854/b394ad8547e84af19b82304f9032c105.PNG">  (3) <br><br>  In the listing above, the list of x stores the values ‚Äã‚Äãof the boundaries of the intervals, and the list of y contains the frequencies of the error values ‚Äã‚Äãin these intervals.  Rewrite (3) in the Python listing line. <br><br>  h = 0.5 * d * n * 10 ** (-sum ([w * np.log10 (w) for w in y if w! = 0]) / n) <br><br>  For the final solution of our problem, we will need two more values ‚Äã‚Äãof the entropy coefficient <b>k</b> and the counter-process <b>psi</b> , we take them from [2], but we write them down for Python. <br><br>  k = h / np.std (a) <br>  mu4 = sum ([(w-np.mean (a)) ** 4 for w in a]) / n <br><br>  It remains to add the four lines to the listing, but to analyze the final result, one more important question should be considered. <br><br><h3>  How can the entropy coefficient and counter-process be used to classify the error distribution laws </h3><br>  According to probability theory, the form of the distribution law is characterized by a relative fourth moment or a counter-process.  In information theory, the form of the distribution law is determined by the value of the entropy coefficient.  Taking this into account, we will place points on the psi, k plane corresponding to the given distribution laws.  But first we obtain the values ‚Äã‚Äãof psi, k and diagrams for the five most significant laws of the distribution of errors. <br><br><div class="spoiler">  <b class="spoiler_title">Program for obtaining numerical values ‚Äã‚Äãof psi, k and plotting</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy.stats <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> logistic,norm,uniform,erlang,pareto,cauchy <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">diagram</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(a,nr)</span></span></span><span class="hljs-function">:</span></span> a.sort() n=len(a) m= int(<span class="hljs-number"><span class="hljs-number">10</span></span>+np.sqrt(n)) d=(max(a)-min(a))/m x=[];y=[] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> np.arange(<span class="hljs-number"><span class="hljs-number">0</span></span>,m,<span class="hljs-number"><span class="hljs-number">1</span></span>): x.append(min(a)+d*i) k=<span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> a: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> min(a)+d*i &lt;=j&lt;min(a)+d*(i+<span class="hljs-number"><span class="hljs-number">1</span></span>): k=k+<span class="hljs-number"><span class="hljs-number">1</span></span> y.append(k) h=<span class="hljs-number"><span class="hljs-number">0.5</span></span>*d*n*<span class="hljs-number"><span class="hljs-number">10</span></span>**(-sum([w*np.log10(w) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> w <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> y <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> w!=<span class="hljs-number"><span class="hljs-number">0</span></span>])/n) k=h/np.std (a) mu4=sum ([(w-np.mean (a))**<span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> w <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> a])/n psi=(np.std(a))**<span class="hljs-number"><span class="hljs-number">2</span></span>/np.sqrt(mu4) plt.title(<span class="hljs-string"><span class="hljs-string">"%s : k=%s; psi=%s; h=%s."</span></span>%(nr,str(round(k,<span class="hljs-number"><span class="hljs-number">3</span></span>)),str(round(psi,<span class="hljs-number"><span class="hljs-number">3</span></span>)),str(round(h,<span class="hljs-number"><span class="hljs-number">3</span></span>)))) plt.bar(x,y, d) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) plt.show() nr=<span class="hljs-string"><span class="hljs-string">" "</span></span> a=uniform.rvs( size=<span class="hljs-number"><span class="hljs-number">1000</span></span>) diagram(a,nr) nr=<span class="hljs-string"><span class="hljs-string">" "</span></span> a=logistic.rvs( size=<span class="hljs-number"><span class="hljs-number">1000</span></span>) diagram(a,nr) nr=<span class="hljs-string"><span class="hljs-string">" "</span></span> a=norm.rvs( size=<span class="hljs-number"><span class="hljs-number">1000</span></span>) diagram(a,nr) nr=<span class="hljs-string"><span class="hljs-string">"  "</span></span> a = erlang.rvs(<span class="hljs-number"><span class="hljs-number">4</span></span>,size=<span class="hljs-number"><span class="hljs-number">1000</span></span>) diagram(a,nr) nr=<span class="hljs-string"><span class="hljs-string">"  "</span></span> a = pareto.rvs(<span class="hljs-number"><span class="hljs-number">4</span></span>,size=<span class="hljs-number"><span class="hljs-number">1000</span></span>) diagram(a,nr) nr=<span class="hljs-string"><span class="hljs-string">"  "</span></span> a = cauchy.rvs(size=<span class="hljs-number"><span class="hljs-number">1000</span></span>) diagram(a,nr)</code> </pre><br></div></div><br><img src="https://habrastorage.org/webt/al/h2/vw/alh2vwnf7cjsblg87w4autgn2qa.png"><br><img src="https://habrastorage.org/webt/ab/h7/xt/abh7xtbmtp8wncaruuglgtvlusy.png"><br><img src="https://habrastorage.org/webt/j3/ca/k4/j3cak4izmk9ull0i8wrhkc_aais.png"><br><img src="https://habrastorage.org/webt/sx/c9/wp/sxc9wpzxloc7h_gy4ruc54ya7po.png"><br><img src="https://habrastorage.org/webt/1i/mz/qm/1imzqmzvs6rbfgdwabtizfmahwa.png"><br><img src="https://habrastorage.org/webt/to/vn/gz/tovngzpiztr3bq-endkpndkanq0.png"><br><br>  We have already accumulated the minimum base of entropy characteristics of the error distribution laws.  Now let's check the theory on a sample with an unknown distribution law, for example, such. <br><br>  a = [0.203, 0.154, 0.172, 0.192, 0.233, 0.181, 0.219, 0.153, 0.168, 0.132, 0.204, 0.165, 0.197, 0.205, 0.143, 0.201, 0.168, 0.147, 0.208, 0.195, 0.153, 0.193, 0.178, 0.162 , 0.157, 0.228, 0.219, 0.125, 0.101, 0.211, 0.183, 0.147, 0.145, 0.181, 0.184, 0.139, 0.198, 0.185, 0.202, 0.238, 0.167, 0.204, 0.195, 0.172, 0.196, 0.178, 0.113, 0.175, 0.194 , 0.178, 0.135, 0.178, 0.118, 0.186, 0.191] <br><br>  If anyone is interested, you can choose any and check.  This sample gives the following diagram with parameters. <br><br><img src="https://habrastorage.org/web/a30/25a/42a/a3025a42a8624a45afcb335a3d9a08c9.png"><br><br>  Now it's time to transfer the received parameters to the graph. <br><img src="https://habrastorage.org/web/567/a91/e60/567a91e601ca41a7bf056e4ba415b32c.png"><br>  From the above graph it can be seen that the distribution law for the studied sample is closest to the Erlang distribution law. <br><br><h3>  Conclusion </h3><br>  I hope that the Python implementation of the elements of the information theory of measurements considered in this publication will not be uninteresting for you. <br><br>  Thank you all for your attention! <br><br><h3>  Links </h3><br><ol><li>  <a href="https://habrahabr.ru/users//scorobey/topics">Selection of the distribution law of a random variable according to statistical sampling using Python tools.</a> </li><li>  <a href="http://www.kipiasoft.su/index.php%3Fid%3D203%26name%3Dfiles%26op%3Dview">Electrical measurements of non-electrical quantities.</a> </li></ol></div><p>Source: <a href="https://habr.com/ru/post/332066/">https://habr.com/ru/post/332066/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../332056/index.html">How to become a team leader and not explode</a></li>
<li><a href="../332058/index.html">Petya and others. ESET discloses cyber attacks on corporate networks</a></li>
<li><a href="../332060/index.html">Never write long ifs</a></li>
<li><a href="../332062/index.html">Who is Mr. Hacker?</a></li>
<li><a href="../332064/index.html">How to confuse the analyst. Part Two: What is domain modeling?</a></li>
<li><a href="../332068/index.html">Automation blocking Petya / NonPetya</a></li>
<li><a href="../332070/index.html">We answer readers' questions: what is the IBM Watson cognitive system, and how does it work?</a></li>
<li><a href="../332072/index.html">‚ÄúYou, thunderstorm, threaten, and we hold on to each other!‚Äù - tale about how I saved the ADSL modem</a></li>
<li><a href="../332074/index.html">Autoencoders in Keras, Part 6: VAE + GAN</a></li>
<li><a href="../332076/index.html">Dynamic instrumentation is not easy, but trivial *: we write yet another instrumentation for American Fuzzy Lop</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>