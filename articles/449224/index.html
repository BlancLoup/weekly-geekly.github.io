<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>About the bias of artificial intelligence</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="tl; dr: 


- Machine learning is looking for patterns in data. But artificial intelligence can be "biased" - that is, to find the wrong patterns. For ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>About the bias of artificial intelligence</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/ba/yy/oo/bayyoozdv865jlg9zqjczextnbg.png"></p><br><h2 id="tldr">  tl; dr: </h2><br><ul><li>  Machine learning is looking for patterns in data.  But artificial intelligence can be "biased" - that is, to find the wrong patterns.  For example, a skin cancer detection system from a photograph may pay particular attention to images taken at a doctor‚Äôs office.  Machine learning is not able <em>to understand</em> : its algorithms only reveal patterns in numbers, and if the data are not representative, so will the result of their processing.  And to catch such bugs can be difficult because of the mechanics of machine learning. <a name="habracut"></a></li><li>  The most obvious and frightening problem area is human diversity.  There are many reasons why data about people may lose their objectivity even at the collection stage.  But one should not think that this problem concerns only people: exactly the same difficulties arise when trying to detect a flood in a warehouse or a failed gas turbine.  Some systems may have prejudices about color, others will be biased towards Siemens sensors. </li><li>  Such problems are not new to machine learning, and are peculiar not only to him.  Wrong assumptions are made in any complex structures, and to understand why this or that decision was made is always difficult.  It is necessary to deal with this in a complex way: to create tools and processes for testing - and to form users so that they do not blindly follow the recommendations of the AI.  Machine learning really does some things much better than us - but dogs, for example, are much more effective than people in detecting drugs, which is not a reason to involve them as witnesses and to make sentences based on their testimony.  And dogs, by the way, are much smarter than any machine learning system. </li></ul><br><hr><br><p>  Machine learning today is one of the most important fundamental technological trends.  This is one of the main ways that technology will change the world around us in the next decade.  Some aspects of these changes are worrying.  For example, the potential impact of machine learning on the labor market, or its use for unethical purposes (for example, authoritarian regimes).  There is another problem that this post is devoted to: the <strong>bias of artificial intelligence</strong> . </p><br><p>  This is not an easy story. </p><br><p><img src="https://habrastorage.org/webt/ca/fy/5q/cafy5qhpw0dvjtmf7v8xcrz9voy.png"><br>  <em>AI from Google can find cats.</em>  <em>This news from 2012 was something special then.</em> </p><br><h2 id="chto-takoe-predvzyatost-ii">  What is ‚ÄúAI bias‚Äù? </h2><br><blockquote>  <em>Raw data is both an oxymoron and a bad idea;</em>  <em>data need to be well and carefully prepared.</em>  ‚ÄîGeoffrey Boker </blockquote><p>  Somewhere before 2013, in order to make a system that, say, recognizes cats in photos, you had to describe logical steps.  How to find corners on the image, recognize eyes, analyze textures for the presence of fur, count paws, and so on.  Then collect all the components - and find that it all does not really work.  Approximately like a mechanical horse - theoretically it can be done, but in practice it is too complicated to describe.  At the exit you have hundreds (or even thousands) of handwritten rules.  And not a single working model. </p><br><p>  With the advent of machine learning, we no longer use the ‚Äúmanual‚Äù rules for recognizing an object.  Instead, we take a thousand samples of "that", X, a thousand samples of "another", Y, and force the computer to build a model based on their statistical analysis.  Then we give this model some sample data, and it determines with some accuracy whether it fits one of the sets.  Machine learning generates a model based on data, and not with the help of the person who writes it.  The results are impressive, especially in the field of image and pattern recognition, and that is why the entire industry is now moving to machine learning (ML). </p><br><p>  But not everything is so simple.  In the real world, your thousands of examples of X or Y also contain A, B, J, L, O, R, and even L. They may be unevenly distributed, and some of them may occur so often that the system will pay more attention to them than on objects that interest you. </p><br><p>  What does this mean in practice?  My favorite example is when image recognition systems <a href="http://aiweirdness.com/post/171451900302/do-neural-nets-dream-of-electric-sheep">look at a grassy hill and say, "sheep</a> . <a href="http://aiweirdness.com/post/171451900302/do-neural-nets-dream-of-electric-sheep">"</a>  It is clear why: most of the photographs of the examples of the "sheep" are made in the meadows where they live, and in these images the grass takes up much more space than little white fluffy trees, and it is the grass of the system that is considered the most important. </p><br><p>  There are more serious examples.  From a recent one, one <a href="https://www.medpagetoday.com/dermatology/generaldermatology/70513">project</a> for detecting skin cancer in photographs.  It turned out that dermatologists often photograph a ruler along with skin cancer manifestations in order to fix the size of the lesions.  On examples of photos of healthy skin there are no lines.  For an AI system, such rulers (more precisely, pixels, which we define as a ‚Äúruler‚Äù) became one of the differences between sets of examples, and sometimes more important than a small skin rash.  So the system created for the recognition of skin cancer, sometimes instead recognized the ruler. </p><br><p>  The key point here is that the system does not have a semantic understanding of what it is looking at.  We look at a set of pixels and see a sheep, skin or rulers in them, and the system is just a numeric string.  She does not see three-dimensional space, does not see any objects, nor textures, nor sheep.  She simply sees patterns in the data. </p><br><p>  The difficulty of diagnosing such problems is that the neural network (the model generated by your machine learning system) consists of thousands of thousands of nodes.  There is no easy way to look at a model and see how it makes a decision.  Having such a method would mean that the process is simple enough to describe all the rules manually, without using machine learning.  People worry that machine learning has become a kind of ‚Äúblack box‚Äù.  (I will explain a little later why this comparison is still a bust.) </p><br><p>  This, in general terms, is the problem of bias in artificial intelligence or machine learning: a system for finding patterns in the data may find incorrect patterns, but you may not notice it.  This is a fundamental characteristic of technology, and this is obvious to everyone who works with it in scientific circles and in large technology companies.  But its consequences are complex, and our possible solutions to these effects are, too. </p><br><p>  Let's talk first about the consequences. </p><br><p><img src="https://habrastorage.org/webt/ty/zu/2e/tyzu2ewswsiwlpon-fb5inbwggo.png"><br>  <em>The AI ‚Äã‚Äãmay implicitly make a choice in favor of certain categories of people based on a large number of imperceptible signals.</em> </p><br><h2 id="scenarii-predvzyatosti-ii">  AI bias scenarios </h2><br><p>  The most obvious and frightening that this problem can manifest itself when it comes to human diversity.  Recently there <a href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G">was a rumor</a> that Amazon tried to build a machine learning system for the initial screening of candidates for employment.  Since there are more men among Amazon workers, the examples of ‚Äúsuccessful hiring‚Äù are also more often male, and there were more men in the list of resumes proposed by the system.  Amazon noticed this and did not release the system in production. </p><br><p>  The most important thing in this example is that the system, according to rumors, favored male candidates, despite the fact that the gender was not indicated in the resume.  <strong>The system saw other patterns in the examples of ‚Äúsuccessful recruitment‚Äù: for example, women can use special words to describe achievements, or have particular hobbies.</strong>  <strong>Of course, the system did not know what ‚Äúhockey‚Äù is, who such ‚Äúpeople‚Äù are, or what ‚Äúsuccess‚Äù is, it simply carried out a statistical analysis of the text.</strong>  But the patterns that she saw would most likely have remained unnoticed by a person, and some of them (for example, the fact that people of different sexes describe success in different ways) would probably be difficult for us to see, even looking at them. </p><br><p> Further worse.  A machine learning system that finds cancer on pale skin very well may work worse with dark skin, or vice versa.  Not necessarily because of bias, but because you probably need to build a separate model for a different skin color, choosing different characteristics.  <strong>Machine learning systems are not interchangeable even in such a narrow area as image recognition.</strong>  You need to tune the system, sometimes simply by trial and error, to notice well the features in the data of interest to you, until you reach the desired accuracy.  But you may not notice that the system in 98% of cases is accurate when working with one group and only 91% (even if it is more accurate than a human analysis) on the other. </p><br><p>  So far I have mainly used examples related to people and their characteristics.  The discussion around this problem is mainly focused on this topic.  But it is important to understand that bias towards people is only part of the problem.  We will use machine learning for a variety of things, and the sampling error will be relevant for all of them.  On the other hand, if you work with people, data bias may not be related to them. </p><br><p>  To understand this, let‚Äôs go back to the skin cancer example and consider three hypothetical possibilities for system failure. </p><br><ol><li>  Non-uniform distribution of people: an unbalanced number of photographs of skin of different tones, which leads to false-positive or false-negative results associated with pigmentation. </li><li>  The data on which the system is trained contain a frequently occurring and non-uniformly distributed characteristic that is not related to humans and does not have diagnostic value: a ruler on photos of skin cancer manifestations or grass on photos of sheep.  In this case, the result will be different if on the image the system finds the pixels of something that the human eye defines as a ‚Äúruler‚Äù. </li><li>  The data contain a third-party characteristic that a person cannot see, even if he searches for it. </li></ol><br><p>  What does it mean?  We know a priori that data can represent different groups of people in different ways, and at a minimum we can schedule a search for such exceptions.  In other words, there are a lot of social reasons to assume that the data on groups of people already contain some prejudice.  If we look at the photo with the ruler, we will see this ruler - we just ignored it before, knowing that it does not matter, and forgetting that the system does not know anything. </p><br><p>  But what if all your photos of unhealthy skin were taken in an office where incandescent bulbs are used, and healthy ones with fluorescent light?  What if, having finished taking off healthy skin, before shooting unhealthy, you updated the operating system on your phone, and Apple or Google slightly changed the noise reduction algorithm?  A person does not notice this, no matter how much he looks for such features.  And then the machine use system will immediately see and use it.  She doesn't know anything. </p><br><p>  So far we have been talking about false correlations, but it may happen that the data are accurate and the results are correct, but you do not want to use them for ethical, legal or managerial reasons.  In some jurisdictions, for example, women cannot be given a discount on insurance, although women may be safer to drive a car.  We can easily imagine a system that, when analyzing historical data, assigns a lower risk coefficient to female names.  Ok, let's remove the names from the selection.  But remember the example with Amazon: the system can determine the gender by other factors (although it doesn‚Äôt know what the floor is and what the machine is), and you won‚Äôt notice this until the controller backdating analyzes the tariffs you offer and doesn‚Äôt charge you are fine </p><br><p>  Finally, it is often implied that we will use such systems only for projects that involve people and social interactions.  This is not true.  If you are making gas turbines, you probably want to apply machine learning to telemetry transmitted by tens or hundreds of sensors on your product (audio, video, temperature, and any other sensors generate data that can be very easily adapted to create a machine learning model ).  Hypothetically, you can say: ‚ÄúHere are the data on a thousand turbines that were out of order, obtained before their breakdown, but data from a thousand turbines that did not break.  Build a model to tell the difference between them. ‚Äù  Well, now imagine that Siemens sensors are 75% bad turbines, and only 12% are good (there is no connection with failures).  The system will build a model to find turbines with Siemens sensors.  Oops! </p><br><p><img src="https://habrastorage.org/webt/yh/ie/og/yhieogd7yvobqtecoevgxe_pydk.png"><br>  Picture - Moritz Hardt, UC Berkeley </p><br><h2 id="upravlenie-predvzyatostyu-ii">  AI bias control </h2><br><p>  What can we do about it?  You can approach the issue from three sides: </p><br><ol><li>  Methodological rigor in the collection and management of data for training the system. </li><li>  Technical tools for analyzing and diagnosing model behavior. </li><li>  Training, education and caution when introducing machine learning into products. </li></ol><br><p>  In Moliere‚Äôs The Bourgeois in the Nobility, there is a joke: one man was told that literature was divided into prose and poetry, and he discovered with admiration that he had spoken prose all his life without knowing it.  Probably, statistics somehow feel today: without even noticing, they devoted their careers to artificial intelligence and sampling error.  To look for a sampling error and worry about it is not a new problem, we just need to systematically approach its solution.  As mentioned above, in some cases it is actually easier to do by studying the problems associated with data about people.  We assume a priori that we may have prejudices regarding different groups of people, but it‚Äôs hard to even imagine a prejudice about Siemens sensors. </p><br><p>  The new thing in all of this, of course, is that people are no longer engaged in statistical analysis directly.  It is carried out by machines that create large complex models that are difficult to understand.  The issue of transparency is one of the main aspects of the bias problem.  We are afraid that the system is not just biased, but that there is no way to detect its bias, and that machine learning is different from other forms of automation, which are supposed to consist of clear logical steps that can be verified. </p><br><p>  There are two problems here.  We may still be able to conduct some kind of audit of machine learning systems.  And auditing any other system is actually not at all easier. </p><br><p>  First, one of the directions of modern research in the field of machine learning is the search for methods on how to identify the important functionality of machine learning systems.  At the same time, machine learning (in its current state) is a completely new field of science, which is rapidly changing, so you should not think that impossible things today cannot soon become quite real.  The <a href="https://openai.com/">OpenAI</a> project is an interesting example. </p><br><p>  Secondly, the idea that you can test and understand the decision-making process in existing systems or organizations is good in theory, but so-so in practice.  Understanding how decisions are made in a large organization is not easy at all.  Even if there exists a formal decision-making process, it does not reflect how people actually interact, and they themselves often do not have a logical systematic approach to making their decisions.  As my colleague <a href="https://en.wikipedia.org/wiki/Vijay_S._Pande">Vijay Pande said</a> , <strong>people are black boxes too</strong> . </p><br><p>  Take a thousand people in several overlapping companies and institutions, and the problem will become even more difficult.  We know after the fact that the Space Shuttle was destined to fall apart when returning, and some people inside NASA had information that gave them reason to think that something bad could happen, but the system did not know it <em>in general</em> .  NASA even just went through a similar audit, losing the previous shuttle, and yet it lost another one - for a very similar reason.  It is easy to say that organizations and people follow clear logical rules that can be tested, understood and changed - but experience proves the opposite.  This is the ‚Äú <a href="https://ru.wikipedia.org/wiki/%25D0%2593%25D0%25BE%25D1%2581%25D0%25BF%25D0%25BB%25D0%25B0%25D0%25BD_%25D0%25A1%25D0%25A1%25D0%25A1%25D0%25A0">deception of the State Planning Committee</a> ‚Äù. </p><br><p>  <strong>I often compare machine learning with databases, especially with relational ones - a new fundamental technology that has changed the capabilities of computer science and the world around it, which has become a part of everything that we constantly use without realizing this.</strong>  Databases also have problems, and they have similar properties: the system can be built on incorrect assumptions or on bad data, but it will be difficult to notice, and people using the system will do what she says to them without asking questions.  There are a lot of old jokes about tax officials who once wrote your name wrong, and convincing them to correct the error is much more difficult than actually changing the name.  You can think about this in different ways, but it is not clear how best: how about a technical problem in SQL, or about an error in the release of Oracle, or how a bureaucratic institution fails?  How difficult is it to find an error in the process that led to the fact that the system does not have such features as correcting typos?  Could you understand this before people started complaining? </p><br><p>  Even more simply, this problem is illustrated by stories when drivers, because of outdated data in the navigator, move into rivers.  OK, maps must be constantly updated.  But how much TomTom is to blame for the fact that your car blows into the sea? </p><br><p>  I say this to the fact that yes - the bias of machine learning will create problems.  But these problems will be similar to those we have encountered in the past, and they can be noticed and solved (or not) approximately as well as we have managed in the past.  Therefore, a scenario in which AI bias will cause damage is unlikely to happen to leading researchers working in a large organization.  Most likely, some insignificant technological contractor or software vendor will write something on his knee, using open-source components, libraries and tools that are not clear to him.  And the hapless client buys the phrase ‚Äúartificial intelligence‚Äù in the product description and, without asking any questions, will distribute it to his low-paid workers, telling them to do what the AI ‚Äã‚Äãsays.  This is exactly what happened with the databases.  This is not an artificial intelligence problem, or even a software problem.  This is a human factor. </p><br><h2 id="zaklyuchenie">  Conclusion </h2><br><blockquote>  <em>Machine learning can do everything you can teach a dog - but you can never be sure what exactly you taught this dog.</em> </blockquote><p>  It often seems to me that the term ‚Äúartificial intelligence‚Äù only makes it difficult to enter conversations like this.  This term creates the false impression that we actually created it - this intelligence.  What we are on the way to HAL9000 or Skynet - to something that actually <em>understands</em> .  But no.  These are just cars, and it is much more correct to compare them, say, with a washing machine.  It is much better for a person to do laundry, but if you put dishes in it instead of laundry, she will ... wash it.  The dishes will even be clean.  But it will not be what you expected, and it will not happen because the system has some preconceptions about the dishes.  A washing machine does not know what dishes are, or what clothes are - this is just an example of automation, conceptually no different from how processes were automated before. </p><br><p>  Whatever it is about - machines, airplanes or databases - these systems will be very powerful and very limited at the same time.  They will depend entirely on how people use these systems, whether they have good or bad intentions and how they understand their work. </p><br><p>  Therefore, to say that ‚Äúartificial intelligence is mathematics, therefore it cannot have prejudices‚Äù is absolutely not true.  But it is just as wrong to say that machine learning is "subjective in nature."  Machine learning finds patterns in the data, and what patterns it finds depends on the data, and the data depends on us.  Like what we do with them.  Machine learning really does some things much better than us - but dogs, for example, are much more effective than people in detecting drugs, which is not a reason to involve them as witnesses and to make sentences based on their testimony.  And dogs, by the way, are much smarter than any machine learning system. </p><br><hr><br><p>  <strong>Translation:</strong> <a href="https://habr.com/en/users/757NF/">Diana Letskaya</a> . <br>  <strong>Editing:</strong> <a href="http://teleg.one/ponchiknews">Alexey Ivanov</a> . <br>  <strong>Community:</strong> <a href="http://teleg.one/ponchiknews">@PonchikNews</a> . </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/449224/">https://habr.com/ru/post/449224/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../449210/index.html">How HPE SimpliVity 380 for VDI will work: hard load tests</a></li>
<li><a href="../449214/index.html">KlusterKit</a></li>
<li><a href="../449216/index.html">Cheating automated surveillance cameras</a></li>
<li><a href="../44922/index.html">Image Optimization, Part 3: 4 steps to reduce file size</a></li>
<li><a href="../449220/index.html">DrumHero: As I did the first game in my life</a></li>
<li><a href="../449232/index.html">Control of solar power consumption by computer / server</a></li>
<li><a href="../449236/index.html">Ok, google: how to get captcha?</a></li>
<li><a href="../44924/index.html">Human possibilities are endless</a></li>
<li><a href="../449240/index.html">The story of one young service Daida (art by subscription)</a></li>
<li><a href="../449244/index.html">Medium - the first decentralized Internet provider in Russia</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>