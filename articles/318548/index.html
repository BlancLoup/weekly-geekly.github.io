<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Ceph in ProxMox on ZFS</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In his work (system administrator) you always have to look for things and knowledge that are unique to your region. One of these things in our office ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Ceph in ProxMox on ZFS</h1><div class="post__text post__text-html js-mediator-article"><p>  In his work (system administrator) you always have to look for things and knowledge that are unique to your region.  One of these things in our office is ProxMox, installed on the ZFS file system, which allows you to use a good raid array without using iron controllers.  One day, thinking about how we could still surprise and please our customers, we decided to plant it all on the distributed Ceph file system.  I don‚Äôt know how adequate the decision was, but I decided to make my wish come true.  And then it started ... I shoveled mountains of articles and forums, but did not find one adequate manual describing in detail what and how to do, therefore, having coped with everything, this article was born, who are interested, welcome under cat. </p><br><p><img src="https://pp.vk.me/c638017/v638017888/13fa8/Lg7SMlOaeAE.jpg" alt="image"></p><br><a name="habracut"></a><br><p>  So, in principle, everything is done in the console and we don‚Äôt really need ProxMox web-muzzle.  I did everything in test mode, so two virtual machines with four disks were raised inside a not very powerful hardware for proxmox (a sort of nested doll).  Four disks were originally due to the fact that I wanted to raise, like on the future, not the test hardware, on the ZFS10, but the goldfish did not come out for unknown reasons (in fact, it was too lazy to understand).  It turned out that ProxMox was unable to partition the ZFS10 on virtual disks, so it was decided to use a slightly different ‚Äúgeography‚Äù.  ProxMox itself was put on one of the disks, ZFS1 was raised on the other two, the third was supposedly under the Ceph magazine, but I finally forgot about it, so for now let's leave it alone.  So let's get started. </p><br><p>  There will be a small introductory: </p><br><p>  Proxmox is freshly installed in two places.  The nodes are called ceph1 and ceph2.  We make everything on both nodes the same, except for those places that I will mark.  Our network is 192.168.111.0/24.  The first node (ceph1) has the address 192.168.111.1, the second (ceph2) - 192.168.111.2.  The drives on both nodes have the following meanings: / dev / vda is the drive on which ProxMox resides, / dev / vdb and / dev / vdc are drives designed for ZFS, / dev / vdd is a drive for Ceph log. </p><br><p>  The first thing we need to do is change the paid ProxMox repository that requires a subscription to a free one: </p><br><pre><code class="hljs php">nano /etc/apt/sources.<span class="hljs-keyword"><span class="hljs-keyword">list</span></span>.d/pve-enterprise.<span class="hljs-keyword"><span class="hljs-keyword">list</span></span></code> </pre> <br><p>  There we comment on a single line and enter a new one below: </p><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">deb</span></span> http://download.proxmox.com/debian jessie pve-<span class="hljs-literal"><span class="hljs-literal">no</span></span>-subscription</code> </pre> <br><p>  Next, update our ProxMox: </p><br><pre> <code class="hljs sql">apt <span class="hljs-keyword"><span class="hljs-keyword">update</span></span> &amp;&amp; apt dist-<span class="hljs-keyword"><span class="hljs-keyword">upgrade</span></span></code> </pre> <br><p>  Install packages to work with Ceph: </p><br><pre> <code class="hljs sql">pveceph <span class="hljs-keyword"><span class="hljs-keyword">install</span></span> -<span class="hljs-keyword"><span class="hljs-keyword">version</span></span> hammer</code> </pre> <br><p>  The next step is to make a cluster of proxmoxes. </p><br><p>  On the first node we execute sequentially: </p><br><pre> <code class="hljs pgsql">pvecm <span class="hljs-keyword"><span class="hljs-keyword">create</span></span> mycluster</code> </pre> <br><p>  where mycluster is the name of our cluster. </p><br><p>  On the second node: </p><br><pre> <code class="hljs css"><span class="hljs-selector-tag"><span class="hljs-selector-tag">pvecm</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">add</span></span> 192<span class="hljs-selector-class"><span class="hljs-selector-class">.168</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.111</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.1</span></span></code> </pre> <br><p>  We agree that you need to accept the ssh key and enter the root password from the first node. </p><br><p>  Checking the whole thing with pvecm status </p><br><p>  Next, we initialize the Ceph configuration (done only on the first node, which will be ‚Äúmain‚Äù): </p><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">pveceph</span></span> init --network <span class="hljs-number"><span class="hljs-number">192.168.111.0</span></span>/<span class="hljs-number"><span class="hljs-number">24</span></span></code> </pre> <br><p>  this will create a symlink for us on /etc/ceph/ceph.conf, from which we will continue to build on. </p><br><p>  Immediately after this, we need to add an option to the [osd] section: </p><br><pre> <code class="hljs cs">[<span class="hljs-meta"><span class="hljs-meta">osd</span></span>] journal dio = <span class="hljs-literal"><span class="hljs-literal">false</span></span></code> </pre> <br><p>  This is due to the fact that ZFS does not know how to directIO. </p><br><p>  The next thing we do is prepare our ZFS pool.  To do this, you need to mark the disks in GPT: </p><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">fdisk</span></span> /dev/vdb</code> </pre> <br><p>  There we consistently press g and w (g to create a GPT table and w to accept the changes).  The same is repeated on / dev / vdc. </p><br><p>  Create a mirrored ZFS pool, we will call it as is customary in ProxMox - rpool: </p><br><pre> <code class="hljs pgsql">zpool <span class="hljs-keyword"><span class="hljs-keyword">create</span></span> rpool mirror /dev/vdb /dev/vdc</code> </pre> <br><p>  Check the zpool status -v command and get (at least should): </p><br><pre> <code class="hljs pgsql">pool: rpool state: ONLINE scan: <span class="hljs-keyword"><span class="hljs-keyword">none</span></span> requested config: <span class="hljs-type"><span class="hljs-type">NAME</span></span> STATE <span class="hljs-keyword"><span class="hljs-keyword">READ</span></span> <span class="hljs-keyword"><span class="hljs-keyword">WRITE</span></span> CKSUM rpool ONLINE <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> mirror<span class="hljs-number"><span class="hljs-number">-0</span></span> ONLINE <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> vdb ONLINE <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> vdc ONLINE <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> errors: <span class="hljs-keyword"><span class="hljs-keyword">No</span></span> known data errors</code> </pre> <br><p>  ZFS pool we have created, it's time to do the most important - ceph. </p><br><p>  Create a file system (a strange name, but it is taken from the ZFS docks) for our Ceph monitor: </p><br><pre> <code class="hljs sql">zfs <span class="hljs-keyword"><span class="hljs-keyword">create</span></span> -o mountpoint=/<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/lib/ceph/mon rpool/ceph-monfs</code> </pre> <br><p>  Let's create the monitor itself (first on the first node, then on the second): </p><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">pveceph</span></span> createmon</code> </pre> <br><p>  Then begins what it was necessary to tinker with, namely, how to make a block device for Ceph OSD (and it works with them) in ZFS and so that it also works. </p><br><p>  And everything is done simply - through zvol: </p><br><pre> <code class="hljs pgsql">zfs <span class="hljs-keyword"><span class="hljs-keyword">create</span></span> -V <span class="hljs-number"><span class="hljs-number">90</span></span>G rpool/ceph-osdfs</code> </pre> <br><p>  90G is how much we give to our Ceph to be torn apart.  So little because the server is virtual and I did not give him more than 100G. </p><br><p>  Well, let's do it myself Ceph OSD: </p><br><pre> <code class="hljs pgsql">ceph-disk <span class="hljs-keyword"><span class="hljs-keyword">prepare</span></span> <span class="hljs-comment"><span class="hljs-comment">--zap-disk --fs-type xfs --cluster ceph --cluster-uuid FSID /dev/zd0</span></span></code> </pre> <br><p>  --fs-type XFS is chosen here because XFS is the default FS from Ceph.  The FSID is our Ceph ID, which can be found in /etc/ceph/ceph.conf.  Well, / dev / zd0 is our zvol. </p><br><p>  If after that your df -h doesn't show something like this: </p><br><pre> <code class="hljs ruby">/dev/zd0p1 <span class="hljs-number"><span class="hljs-number">85</span></span>G <span class="hljs-number"><span class="hljs-number">35</span></span>M <span class="hljs-number"><span class="hljs-number">85</span></span>G <span class="hljs-number"><span class="hljs-number">1</span></span>% <span class="hljs-regexp"><span class="hljs-regexp">/var/lib</span></span><span class="hljs-regexp"><span class="hljs-regexp">/ceph/osd</span></span><span class="hljs-regexp"><span class="hljs-regexp">/ceph-0</span></span></code> </pre> <br><p>  it means something went wrong and you either need to reboot, or once again you need to create a ceph OSD. </p><br><p>  In general, we have already made our ceph on this and we can continue to drive them in the ProxMox webmoney and create the necessary RDB storage on it, but you cannot use it (actually, for what it was all started).  It is treated in a simple way (for this, all the same storage must be created) - you need to copy the ceph key from the first node to the second. </p><br><p>  Open the ProxMox storage configuration: </p><br><pre> <code class="hljs pgsql">nano /etc/pve/<span class="hljs-keyword"><span class="hljs-keyword">storage</span></span>.cfg</code> </pre> <br><p>  And enter the RBD we need there: </p><br><pre> <code class="hljs css"><span class="hljs-selector-tag"><span class="hljs-selector-tag">rbd</span></span>: <span class="hljs-selector-tag"><span class="hljs-selector-tag">test</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">monhost</span></span> 192<span class="hljs-selector-class"><span class="hljs-selector-class">.168</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.111</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.1</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:6789</span></span>;192<span class="hljs-selector-class"><span class="hljs-selector-class">.168</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.111</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.2</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:6789</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">pool</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">rbd</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">krbd</span></span> 1 <span class="hljs-selector-tag"><span class="hljs-selector-tag">username</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">admin</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">content</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">images</span></span></code> </pre> <br><p>  Here, test is the name of our repository, and IP addresses are where ceph monitors are located, that is, our proxmoxes.  The remaining options are default. </p><br><p>  Next, create a daddy for the key on the second node: </p><br><pre> <code class="hljs dos"><span class="hljs-built_in"><span class="hljs-built_in">mkdir</span></span> /etc/pve/priv/ceph</code> </pre> <br><p>  And copy the key with the first: </p><br><pre> <code class="hljs pgsql">scp ceph1:/etc/ceph/ceph.client.<span class="hljs-keyword"><span class="hljs-keyword">admin</span></span>.keyring /etc/pve/priv/ceph/test.keyring</code> </pre> <br><p>  Here ceph1 is our first node, and test is the name of the repository. </p><br><p>  At this point you can put an end - the storage is active and working, we can use all the ceph buns. </p><br><p>  Thanks for attention! </p><br><p>  In order to raise all this, use these links: </p><br><p>  ¬ª <a href="https://pve.proxmox.com/wiki/Storage:_Ceph">Https://pve.proxmox.com/wiki/Storage:_Ceph</a> <br>  ¬ª <a href="https://pve.proxmox.com/wiki/Ceph_Server">Https://pve.proxmox.com/wiki/Ceph_Server</a> <br>  ¬ª <a href="http://xgu.ru/wiki/ZFS">Http://xgu.ru/wiki/ZFS</a> <br>  ¬ª <a href="https://forum.proxmox.com/">Https://forum.proxmox.com</a> </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/318548/">https://habr.com/ru/post/318548/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../318536/index.html">"Reading for the holidays": 25 useful sources on IaaS, IT infrastructure and cloud technologies</a></li>
<li><a href="../318538/index.html">Digest: 40 materials on the topic of DDoS attacks and protection from them</a></li>
<li><a href="../318542/index.html">Intel unveiled Optane memory at CES 2017</a></li>
<li><a href="../318544/index.html">Compiling Java programs and resolving dependencies at runtime</a></li>
<li><a href="../318546/index.html">Using send for convenience, from despair and for fun</a></li>
<li><a href="../318550/index.html">Come and pick up the prototype books</a></li>
<li><a href="../318552/index.html">5 cases for lovers of the right communication</a></li>
<li><a href="../318554/index.html">Azure-IaaS-Digest number 12 (November-December)</a></li>
<li><a href="../318556/index.html">DNS servers ntp.org not available</a></li>
<li><a href="../318558/index.html">Interactive results table</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>