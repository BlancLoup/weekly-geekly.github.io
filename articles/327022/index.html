<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Forecasting financial time series with MLP in Keras</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! In this article I want to tell you about the basic pipeline in time series forecasting using neural networks, in this case, probably, with the ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Forecasting financial time series with MLP in Keras</h1><div class="post__text post__text-html js-mediator-article"><p><img src="http://imgur.com/guH2UIm.png" alt="image"><br>  Hello!  In this article I want to tell you about the basic pipeline in time series forecasting using neural networks, in this case, probably, with the most complex time series for analysis - financial data that are of a random nature, and seemingly unpredictable.  Or is it not? </p><a name="habracut"></a><br><h3 id="vstuplenie">  Introduction </h3><br><p>  I am now studying my last year at the University of Verona with a degree in Applied Mathematics, and, as a typical IT student from the CIS, I started working as a bachelor at the Kiev Polytechnic Institute, then using machine learning in various projects doing now.  At university, the topic of my research is deep learning in relation to the time series, in particular, financial. </p><br><p>  The purpose of this article is to show the process of working with time series from data processing to the construction of neural networks and validation of results.  As an example, the financial series were chosen as completely random and it is generally interesting if conventional neural network architectures can catch the necessary patterns to predict the behavior of a financial instrument. </p><br><p>  The pipeline described in this article is easily applied to any other data and to other classification algorithms.  For those who want to immediately run the code - you can download <a href="https://github.com/Rachnog/Deep-Trading/blob/master/habrahabr.ipynb">IPython Notebook</a> . </p><br><h3 id="podgotovka-dannyh">  Data preparation </h3><br><p>  For example, take the stock prices of such a modest company as Apple from 2005 to the present.  They can be downloaded from <a href="https://finance.yahoo.com/quote/AAPL/history%3Fperiod1%3D1104534000%26period2%3D1491775200%26interval%3D1d%26filter%3Dhistory%26frequency%3D1d">Yahoo Finance</a> in .csv format.  Let's load the data and see how all this beauty looks. </p><br><p>  To begin with, we import the ones we need to load the library: </p><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pylab <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd</code> </pre> <br><p>  Let's read the data and draw graphs (in .csv from Yahoo Finance, the data is loaded in the reverse order - from 2017 to 2005, so first you need to ‚Äúflip‚Äù them with [:: - 1]): </p><br><pre> <code class="python hljs">data = pd.read_csv(<span class="hljs-string"><span class="hljs-string">'./data/AAPL.csv'</span></span>)[::<span class="hljs-number"><span class="hljs-number">-1</span></span>] close_price = data.ix[:, <span class="hljs-string"><span class="hljs-string">'Adj Close'</span></span>].tolist() plt.plot(close_price) plt.show()</code> </pre> <br><p><img src="http://i.imgur.com/ICfwOrl.png" alt="image"></p><br><p>  It looks almost like a typical random process, but we will try to solve the problem of forecasting a day or more ahead.  The ‚Äúprediction‚Äù task must first be described closer to machine learning tasks.  We can predict just the movement of stock prices in the market - more or less - this will be the problem of binary classification.  On the other hand, we can predict either just the price values ‚Äã‚Äãon the next day (or a couple of days) or the price change on the next day compared to the last day, or the logarithm of this difference - that is, we want to predict the number, which is the task regression.  But in solving the regression problem, we will have to face the problems of data normalization, which we will now consider. </p><br><p>  That in the case of classification, in the case of regression, at the entrance we take some window of the time series (for example, 30 days) and try to either predict the price movement on the next day (classification), or the value of change (regression). </p><br><p>  The main problem of financial time series is that they are not even stationary at all (you can check it yourself using, say, <a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B5%25D1%2581%25D1%2582_%25D0%2594%25D0%25B8%25D0%25BA%25D0%25B8_%25E2%2580%2594_%25D0%25A4%25D1%2583%25D0%25BB%25D0%25BB%25D0%25B5%25D1%2580%25D0%25B0">the Dickey-Fuller test</a> ), that is, their characteristics are like mat.  the expectation, the variance, the average maximum and minimum values ‚Äã‚Äãin the window change with time, which means that we cannot use these values ‚Äã‚Äãfor <a href="https://stats.stackexchange.com/questions/70801/how-to-normalize-data-to-0-1-range">MinMax</a> or <a href="https://en.wikipedia.org/wiki/Standard_score">z-score normalization</a> in our windows, because if we have 30 days in our window some characteristics, but they can change the very next day or change in the middle of our window. </p><br><p>  But if you look closely at the classification problem, we are not so interested in the mat.  waiting or dispersion on the next day, we are only interested in moving up or down.  Therefore, we will risk and will normalize our 30-day windows with the help of z-score, but only them, without affecting anything from the ‚Äúfuture‚Äù: </p><br><pre> <code class="python hljs">X = [(np.array(x) - np.mean(x)) / np.std(x) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> X]</code> </pre> <br><p>  For the regression task, this will not work, because if we also subtract the average and divide by the deviation, we will have to restore this value for the price value on the next day, and there these parameters may already be completely different.  Therefore, we will try two options: train on the raw data and try to deceive the system by taking a percentage change in price on the next day ‚Äî pandas will help us with this: </p><br><pre> <code class="python hljs">close_price_diffs = close.price.pct_change()</code> </pre> <br><p><img src="http://i.imgur.com/tesXlDd.png" alt="image"></p><br><p>  It looks like this, and as we can see, these data, obtained without any manipulations with statistical characteristics, already lie in the limit from -0.5 to 0.5: </p><br><p>  To divide the training and training sample, we take the first 85% of windows in time for training and the last 15% to test the operation of the neural network. </p><br><p>  So for training our neural network, we get the following pairs X, Y: prices at the time of market closure in 30 days and [1, 0] or [0, 1] depending on whether the value for binary classification has increased or decreased;  the percentage change in prices for 30 days and the change on the next day for regression. </p><br><h3 id="arhitektura-neyronnoy-seti">  Neural network architecture </h3><br><p>  As the base model, we will use a multilayer perceptron.  If you are not familiar with the basic concepts of the work of neural networks, it is best to start <a href="http://neuralnetworksanddeeplearning.com/">here</a> . </p><br><p>  Let's take <a href="http://keras.io/">Keras</a> as a framework for implementation - it is very simple, intuitive and you can implement quite complex computational graphs on your knee, but for now we will not need it.  We implement a simple grid - an input layer with 30 neurons (the length of our window), the first hidden layer with 64 neurons, after it <a href="https://arxiv.org/abs/1502.03167">BatchNormalization</a> - it is recommended to use it for almost any multilayer networks, then the activation function ( <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU is</a> no longer considered comme il faut, so let's take something fashionable like LeakyReLU).  At the output, we place one neuron (or two for classification), which, depending on the task (classification or regression), will either have softmax at the output, or leave it non-linear to be able to predict any value. </p><br><p>  The code for classification looks like this: </p><br><pre> <code class="python hljs">model = Sequential() model.add(Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>, input_dim=<span class="hljs-number"><span class="hljs-number">30</span></span>)) model.add(BatchNormalization()) model.add(LeakyReLU()) model.add(Dense(<span class="hljs-number"><span class="hljs-number">2</span></span>)) model.add(Activation(<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>))</code> </pre> <br><p>  For a regression task, the activation parameter at the end must be 'linear'.  Next, we need to determine the error functions and the optimization algorithm.  Without going into details of the variations of gradient descent take Adam with a step length of 0.001;  The loss parameter for classification needs to be cross-entropy - 'categorical_crossentropy', and for regression - the standard error - 'mse'.  Keras also allows us to quite flexibly control the learning process, for example, good practice is to reduce the step value of the gradient descent if our results do not improve ‚Äî that is what ReduceLROnPlateau, which we added as a callback to the training model, does. </p><br><pre> <code class="python hljs">reduce_lr = ReduceLROnPlateau(monitor=<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>, factor=<span class="hljs-number"><span class="hljs-number">0.9</span></span>, patience=<span class="hljs-number"><span class="hljs-number">5</span></span>, min_lr=<span class="hljs-number"><span class="hljs-number">0.000001</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>) model.compile(optimizer=opt, loss=<span class="hljs-string"><span class="hljs-string">'categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> <br><p>  Neural network training </p><br><pre> <code class="python hljs">history = model.fit(X_train, Y_train, nb_epoch = <span class="hljs-number"><span class="hljs-number">50</span></span>, batch_size = <span class="hljs-number"><span class="hljs-number">128</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>, validation_data=(X_test, Y_test), shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, callbacks=[reduce_lr])</code> </pre> <br><p>  After the learning process is completed, it will be nice to display the graphs of the dynamics of the error and accuracy values ‚Äã‚Äãon the screen: </p><br><pre> <code class="python hljs">plt.figure() plt.plot(history.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>]) plt.plot(history.history[<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>]) plt.title(<span class="hljs-string"><span class="hljs-string">'model loss'</span></span>) plt.ylabel(<span class="hljs-string"><span class="hljs-string">'loss'</span></span>) plt.xlabel(<span class="hljs-string"><span class="hljs-string">'epoch'</span></span>) plt.legend([<span class="hljs-string"><span class="hljs-string">'train'</span></span>, <span class="hljs-string"><span class="hljs-string">'test'</span></span>], loc=<span class="hljs-string"><span class="hljs-string">'best'</span></span>) plt.show() plt.figure() plt.plot(history.history[<span class="hljs-string"><span class="hljs-string">'acc'</span></span>]) plt.plot(history.history[<span class="hljs-string"><span class="hljs-string">'val_acc'</span></span>]) plt.title(<span class="hljs-string"><span class="hljs-string">'model accuracy'</span></span>) plt.ylabel(<span class="hljs-string"><span class="hljs-string">'acc'</span></span>) plt.xlabel(<span class="hljs-string"><span class="hljs-string">'epoch'</span></span>) plt.legend([<span class="hljs-string"><span class="hljs-string">'train'</span></span>, <span class="hljs-string"><span class="hljs-string">'test'</span></span>], loc=<span class="hljs-string"><span class="hljs-string">'best'</span></span>) plt.show()</code> </pre> <br><p>  Before starting training, I want to pay attention to an important point: it is necessary to learn algorithms on such data a little longer, at least 50-100 epochs.  This is due to the fact that if you train for, say, 5-10 epochs and see 55% accuracy, it most likely will not mean that you have learned how to find patterns, if you analyze the training data, it will be seen that just 55% the windows were for one pattern (increase, for example), and the remaining 45% were for the other (decrease).  In our case, 53% of the windows are ‚Äúdowngrades‚Äù, and 47% are ‚Äúboosts‚Äù, so we will try to get an accuracy above 53%, which will say that we have learned how to find signs. </p><br><p>  Too high accuracy on raw data such as the closing price and simple algorithms will most likely talk about retraining or ‚Äúpeeking‚Äù into the future when preparing a training set. </p><br><h3 id="zadacha-klassifikacii">  Classification task </h3><br><p>  We will train our first model and look at the graphics: </p><br><p><img src="http://imgur.com/PWjOgQI.png" alt="image"><br><img src="http://imgur.com/pYGTD2B.png" alt="image"></p><br><p>  As you can see, the error is that the accuracy for the test sample all the time remains at plus or minus one value, and the error for training drops, and the accuracy increases, which tells us about retraining.  Let's try to take a deeper model with two layers: </p><br><pre> <code class="python hljs">model = Sequential() model.add(Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>, input_dim=<span class="hljs-number"><span class="hljs-number">30</span></span>)) model.add(BatchNormalization()) model.add(LeakyReLU()) model.add(Dense(<span class="hljs-number"><span class="hljs-number">16</span></span>)) model.add(BatchNormalization()) model.add(LeakyReLU()) model.add(Dense(<span class="hljs-number"><span class="hljs-number">2</span></span>)) model.add(Activation(<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>))</code> </pre> <br><p>  Here are the results of her work: </p><br><p><img src="http://imgur.com/RonBVQB.png" alt="image"><br><img src="http://imgur.com/Dl8wcsE.png" alt="image"></p><br><p>  Approximately the same picture.  When we encounter the effect of <a href="https://en.wikipedia.org/wiki/Overfitting">retraining</a> , we need to add regularization to our model.  In short, during retraining, we build a model that simply ‚Äúremembers‚Äù our training data and does not allow us to generalize knowledge to new data.  In the process of regularization, we impose certain restrictions on the weights of the neural network so that there is not a large variation in values ‚Äã‚Äãand in spite of the large number of parameters (ie, the weights of the network), to turn some of them to zero for simplification.  We will start with the most common method - adding an additional term to the error function with the <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">L2 norm</a> for the sum of weights, in Keras this is done using keras.regularizers.activity_regularizer. </p><br><pre> <code class="python hljs">model = Sequential() model.add(Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>, input_dim=<span class="hljs-number"><span class="hljs-number">30</span></span>, activity_regularizer=regularizers.l2(<span class="hljs-number"><span class="hljs-number">0.01</span></span>))) model.add(BatchNormalization()) model.add(LeakyReLU()) model.add(Dense(<span class="hljs-number"><span class="hljs-number">16</span></span>, activity_regularizer=regularizers.l2(<span class="hljs-number"><span class="hljs-number">0.01</span></span>))) model.add(BatchNormalization()) model.add(LeakyReLU()) model.add(Dense(<span class="hljs-number"><span class="hljs-number">2</span></span>)) model.add(Activation(<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>))</code> </pre> <br><p>  Such a neural net is already learning a little better in terms of the error function, but the accuracy still suffers: </p><br><p><img src="http://imgur.com/smzUrWK.png" alt="image"><br><img src="http://imgur.com/IUILCXi.png" alt="image"></p><br><p>  Such a strange effect as a decrease in error, but not a decrease in accuracy, is often found when working with data of high noise or random nature - this is because the error is considered based on the cross-entropy value, which can decrease during the time that accuracy is the neuron index with the right answer, which even when changing the error may remain wrong. </p><br><p>  Therefore, it is worth adding even more regularization to our model using the <a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">Dropout</a> technique popular in the last year - roughly speaking, this is an accidental ‚Äúneglect‚Äù of some scales in the learning process in order to avoid co-adaptation of neurons (so that they do not learn the same signs).  The code looks like this: </p><br><pre> <code class="python hljs">model = Sequential() model.add(Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>, input_dim=<span class="hljs-number"><span class="hljs-number">30</span></span>, activity_regularizer=regularizers.l2(<span class="hljs-number"><span class="hljs-number">0.01</span></span>))) model.add(BatchNormalization()) model.add(LeakyReLU()) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">16</span></span>, activity_regularizer=regularizers.l2(<span class="hljs-number"><span class="hljs-number">0.01</span></span>))) model.add(BatchNormalization()) model.add(LeakyReLU()) model.add(Dense(<span class="hljs-number"><span class="hljs-number">2</span></span>)) model.add(Activation(<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>))</code> </pre> <br><p>  As we see, between the two hidden layers we will ‚Äúdrop‚Äù the connections during training with a probability of 50% for each weight.  Dropout is usually not added between the input layer and the first hidden one, since in this case we will learn from simply noisy data, and also will not be added right before the exit.  Of course, no dropout occurs during network testing.  How to learn this grid: </p><br><p><img src="http://imgur.com/HdKoA9a.png" alt="image"><br><img src="http://imgur.com/xIg10ij.png" alt="image"></p><br><p>  As you can see, the graphics of error and accuracy are adequate, if we stop the network training a bit earlier, we can get <strong>58% accuracy in</strong> predicting the price movement, which is certainly better than random fortune telling. </p><br><p>  Another interesting and intuitive point in forecasting financial time series is that the fluctuation on the next day is of a random nature, but when we look at charts and candles, we can still notice a trend for the next 5-10 days.  Let's check whether our neuronka can cope with such a task - we predict the price movement in 5 days with the last successful architecture and for the sake of interest we will train for more periods: </p><br><p><img src="http://imgur.com/CHHgQAD.png" alt="image"><br><img src="http://imgur.com/YnfTk5R.png" alt="image"></p><br><p>  As you can see, if we stop training early enough (over time, overfitting still happens), we can get <strong>60% accuracy</strong> , which is very good. </p><br><h3 id="zadacha-regressii">  Regression task </h3><br><p>  For the regression problem, let's take our last successful architecture for classification (it has already shown that it can learn the necessary features), remove Dropout and train it on more iterations. </p><br><p>  Also in this case, we can look not only at the error value, but also visually assess the quality of forecasting with the following code: </p><br><pre> <code class="python hljs">pred = model.predict(np.array(X_test)) original = Y_test predicted = pred plt.plot(original, color=<span class="hljs-string"><span class="hljs-string">'black'</span></span>, label = <span class="hljs-string"><span class="hljs-string">'Original data'</span></span>) plt.plot(predicted, color=<span class="hljs-string"><span class="hljs-string">'blue'</span></span>, label = <span class="hljs-string"><span class="hljs-string">'Predicted data'</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'best'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'Actual and predicted'</span></span>) plt.show()</code> </pre> <br><p>  The network architecture will look like this: </p><br><pre> <code class="python hljs">model = Sequential() model.add(Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>, input_dim=<span class="hljs-number"><span class="hljs-number">30</span></span>, activity_regularizer=regularizers.l2(<span class="hljs-number"><span class="hljs-number">0.01</span></span>))) model.add(BatchNormalization()) model.add(LeakyReLU()) model.add(Dense(<span class="hljs-number"><span class="hljs-number">16</span></span>, activity_regularizer=regularizers.l2(<span class="hljs-number"><span class="hljs-number">0.01</span></span>))) model.add(BatchNormalization()) model.add(LeakyReLU()) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>)) model.add(Activation(<span class="hljs-string"><span class="hljs-string">'linear'</span></span>))</code> </pre> <br><p>  Let's see what happens if we train on the ‚Äúraw‚Äù adjustment close: </p><br><p><img src="http://imgur.com/guH2UIm.png" alt="image"></p><br><p>  It looks good from afar, but if you look closely, we will see that our neural network just lags behind its predictions, which can be considered a failure. </p><br><p>  If we train on price changes, we get the following results: </p><br><p><img src="http://imgur.com/X7xClg3.png" alt="image"></p><br><p>  Some values ‚Äã‚Äãare predicted well, in some places the trend is correctly guessed, but in general - so-so. </p><br><h3 id="obsuzhdenie">  Discussion </h3><br><p>  In principle, at first glance, the results are not impressive at all.  So it is, but we have trained the simplest type of neural network on one-dimensional data without much preprocessing.  There are a number of steps that allow to bring the accuracy to the level of 60-70%: </p><br><ul><li>  Train on high-frequency data (every hour, every five minutes) - more data - more patterns - less retraining </li><li>  Use more advanced neural network architectures that are designed to work with sequences - convolutional neural networks, recurrent neural networks. </li><li>  Use not only the closing price, but all the data from our .csv (high, low, open, close, volume) - that is, pay attention to all available information at any time. </li><li>  Optimize hyperparameters - window size, number of neurons in hidden layers, learning step - all these parameters were taken somewhat at random, using a <a href="http://scikit-learn.org/stable/modules/grid_search.html">random search</a> you can find out that maybe we need to look 45 days ago and learn a deeper grid with a smaller step. </li><li>  Use <a href="https://stats.stackexchange.com/questions/37955/how-to-design-and-implement-an-asymmetric-loss-function-for-regression">loss functions that are</a> more suitable for our task (for example, to predict price changes, we could fine the neural for the wrong sign, the usual MSE for the sign of the number is invariant) </li></ul><br><p>  Being engaged in forecasting time series, we left without attention the main goal - to use this data for trading and make sure that it will be profitable.  I would like to show this in a webinar online and apply convolutional and recurrent networks for the prediction problem plus check the profitability of strategies using these predictions.  If anyone is interested, I <a href="https://www.youtube.com/watch%3Fv%3Dc-nUAdcpiGY">wait in Hangouts on Air</a> on May 5 at 18:00 UTC. </p><br><h3 id="zaklyuchenie">  Conclusion </h3><br><p>  In this article, we used the simplest neural network architecture to predict price movements in the market.  This pipeline can be used for any time series, the main thing is to choose the right data preprocessing, determine the network architecture, evaluate the quality of the algorithm.  In our case, we managed to predict the trend with an accuracy of 60% in 5 days using the price window in the previous 30 days, which can be considered a good result.  With a quantitative prediction of price change, it turned out to be a failure, for this task it is advisable to use more serious tools and a statistical analysis of the time series.  All the code used in IPython Notebook can be taken <a href="https://github.com/Rachnog/Deep-Trading/blob/master/habrahabr.ipynb">by reference</a> . </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/327022/">https://habr.com/ru/post/327022/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../327012/index.html">We reach the level of expert! 50 shades of exam 1Z0-047 (Oracle Database SQL Certified Expert)</a></li>
<li><a href="../327014/index.html">How to automate order confirmation if you are not Yulmart (spoiler: just like Yulmart)</a></li>
<li><a href="../327016/index.html">DeclarativeCOS - Cach√© Declarative Programming</a></li>
<li><a href="../327018/index.html">Welcome to VeloCPPed Meetup in Tensor</a></li>
<li><a href="../327020/index.html">TypeScript in Slack, or how to stop worrying and start trusting the compiler</a></li>
<li><a href="../327024/index.html">List of IT companies that regularly conduct internships for students of southern Russia</a></li>
<li><a href="../327026/index.html">What programming languages ‚Äã‚Äãare popular late at night</a></li>
<li><a href="../327030/index.html">Monad transformers for practicing programmers</a></li>
<li><a href="../327038/index.html">Gorilla: fast, scalable in-memory time-series database</a></li>
<li><a href="../327042/index.html">Videos: Moscow Zabbix Meetup in the office of Badoo</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>