<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Freeform real-time speech recognition and call recording recognition</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="We already wrote about the possibility of creating scripts with speech recognition, but then the functionality of this system was somewhat limited. No...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Freeform real-time speech recognition and call recording recognition</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/files/5f7/f2e/4c3/5f7f2e4c3fc449519640b571e831927d.jpg"></div><br>  We already <a href="https://habrahabr.ru/company/Voximplant/blog/231319/">wrote</a> about the possibility of creating scripts with speech recognition, but then the functionality of this system was somewhat limited.  Not so long ago, Google has <a href="https://9to5google.com/2016/07/20/google-natural-language-speech-api-beta/">opened access</a> to a speech recognition system.  And we, of course, took advantage of this.  Many companies implement different scenarios of interaction with their customers using Voximplant.  Automation using speech recognition and search in an already recognized one allows a business to spend less effort on manual work and more on what is really important.  Next, we will describe in detail about several basic cases for which integration was done, and problems encountered in the process, and also give some examples of using the new functionality. <br><a name="habracut"></a><br><h3>  Freeform is hard </h3><br>  To begin with, freeform recognition is a very difficult technical task.  If on a limited dictionary (for example, addresses) to get recognition quality in 90 +% is real, then in the case of freeform, this is practically unattainable today.  It's one thing when a person dictates something, that is, the output is a structured text.  And a completely different thing is a telephone conversation, where, during communication, there is a million extra moments that worsen the quality of recognition: from commonplace interjections, coughing and individual speech characteristics to noise, packet loss and others that have the most different nature.  In addition, real-time recognition requires sufficiently decent computing power, and we need all this to scale well and be accessible from the cloud.  We can assure you that we have been testing a wide variety of freeform recognition solutions for a long time.  Every time something was missing, so when colleagues from Google announced their recognition, we gladly ran to test it. <br><br><h3>  Google Cloud Speech API Features </h3><br>  Currently, <a href="https://cloud.google.com/speech/">Google Cloud Speech API</a> is in open beta state.  There are a number of restrictions on the number and speed of requests that you can feed him.  There are several options for working with the API: synchronous mode, asynchronous mode and streaming.  Synchronous mode allows you to send pieces of audio data up to a minute in duration and returns a response with the recognition result to the request.  Asynchronous mode allows you to handle large files, but for this you need to upload them to Google Cloud Storage.  Streaming allows you to transfer data in parts and get the result of recognition in real time, that is, it is well suited for dictation and IVR.  For audio format - 8/16 KHz.  A number of different codecs are supported depending on the mode: ulaw, flac, amr, or just PCM.  The vendor recommends using 16 KHz and not using additional signal processing - this only degrades the quality of recognition.  Our experience has shown that the nuances are, in fact, much more.  For example, it is better not to try to recognize pieces longer than 20 seconds, if the piece is too small, then you can not get the result at the output, etc.  Many of these problems are a consequence of the beta version.  We think that they will be corrected for release. <br><br><h3>  Case ‚Ññ1: Transcribing </h3><br>  One of the most popular cases is the recognition of call recordings.  Probably no need to explain for a long time why this is such a useful feature.  Search in the text is much simpler than search in audio, so any kind of analysis will be accelerated and simplified after conversion to text.  To enable transcribing, an additional transcribe parameter must be passed to the recording function: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <pre><code class="javascript hljs"><span class="hljs-built_in"><span class="hljs-built_in">require</span></span>(Modules.ASR); <span class="hljs-comment"><span class="hljs-comment">//.. call.record({language: ASRLanguage.RUSSIAN_RU, transcribe: true, stereo: true}); //...</span></span></code> </pre> <br>  All call processing scripts in Voximplant are written in JavaScript, so everything is quite transparent here.  This code says that after the end of the recording, it will be necessary to send data to a special subsystem that will interact with the Google Speech API.  And after some time in the call history, in addition to recording, a text file will appear with the recognition results.  The result will look like this: <br><br><pre> <code class="bash hljs">Left 00:00:00 - 00:00:03 : &lt;Unrecognized&gt; Right 00:00:00 - 00:00:35 :                               Philips  1      Philips  2          Philips  3       Philips  4 Right 00:00:38 - 00:01:18 :            Philips  1      Philips  2          Philips  3       Philips  4          Philips                  Left 00:01:05 - 00:01:44 :       nanosuit  Saeco    -            .     -             Right 00:01:20 - 00:02:01 :        Philips                                 Left 00:01:49 - 00:02:33 :     x xxx xxxx Right 00:02:07 - 00:02:30 :                           125 Right 00:02:32 - 00:03:17 : 66 67  2                                         .             Left 00:02:34 - 00:03:14 :          Left 00:03:14 - 00:03:26 :      Right 00:03:17 - 00:03:25 :      Philips          </code> </pre><br>  Unfortunately, while the API does not issue timestamps during recognition, it is therefore not possible to split the time as precisely as possible when and what was said.  But even so, it is very good. <br><br><h3>  Case ‚Ññ2.  IVR </h3><br>  Keyword "automation".  Now only the lazy does not write and does not tell about how the world will change thanks to machine learning, AI and so on.  Probably, we do not agree with everything in these stories.  Especially about AI.  But the fact that automation allows us to speed up and improve a number of processes, we know very well, since we actively <a href="https://voximplant.ru/case_hoff.html">offer</a> our clients to automate the processes of interaction with their clients, for which we had to spend our time dearly on our employees.  Intelligent speech recognition IVRs will be avalanche-propagated in the near future precisely because of progress in machine learning and in speech recognition.  In the US, if you call the Department of Motor Vehicles, you will have a long and exciting conversation with their IVR, where it‚Äôs almost impossible to get to a living person.  Maximum - you can ask to call you back later.  If you're lucky somewhere next week.  We do not believe that such an extreme option is right.  Still, you need to give people the opportunity to get on a living person, if communication does not add up.  But the trend has long been clear. <br><br>  In the case of Voximplant, we have long been able to implement such scenarios.  Previously, the recognition accuracy in some cases was insufficient.  In the case of APIs from Google, you can set <b>speech_context</b> , which allows you to implement a script with a choice from a predefined list of phrases and options.  And if a person says something out of context, recognition will still work.  But if he says something out of context, then it will work with much higher accuracy.  You can use this feature in a VoxEngine script as follows: <br><br><pre> <code class="javascript hljs"><span class="hljs-built_in"><span class="hljs-built_in">require</span></span>(Modules.ASR); <span class="hljs-comment"><span class="hljs-comment">//.. mycall.say(" ", Language.RU_RUSSIAN_FEMALE); mycall.addEventListener(CallEvents.PlaybackFinished, function (e) { mycall.sendMediaTo(myasr); }); //... var myasr = VoxEngine.createASR( ASRLanguage.RUSSIAN_RU, ["", "", "", " ", " "]); myasr.addEventListener(ASREvents.Result, function (e) { if (e.confidence &gt; 0) mycall.say(" " + e.text + "  " + e.confidence, Language.RU_RUSSIAN_FEMALE); else mycall.say("   ", Language.RU_RUSSIAN_FEMALE); }); myasr.addEventListener(ASREvents.SpeechCaptured, function (e) { mycall.stopMediaTo(myasr); }); //...</span></span></code> </pre><br><h3>  Case number 3.  Streaming </h3><br>  For some technical reasons related to the work of the backend from Google, we had to decently poshamanit for the implementation of the streaming mode.  We hope in the near future this need will disappear.  So, to recognize the whole conversation in real time (or just large parts of speech) you need to modify the script: <br><br><pre> <code class="javascript hljs"><span class="hljs-built_in"><span class="hljs-built_in">require</span></span>(Modules.ASR); <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> full_result = <span class="hljs-string"><span class="hljs-string">""</span></span>, ts; <span class="hljs-comment"><span class="hljs-comment">//.. mycall.say(" ", Language.RU_RUSSIAN_FEMALE); mycall.addEventListener(CallEvents.PlaybackFinished, function (e) { mycall.sendMediaTo(myasr); }); //... //   -    freeform (     ) var myasr = VoxEngine.createASR(ASRLanguage.RUSSIAN_RU); myasr.addEventListener(ASREvents.Result, function (e) { //         full_result += e.text + " "; //   5    CaptureStarted,    ts = setTimeout(recognitionEnded, 5000); }); myasr.addEventListener(ASREvents.SpeechCaptured, function (e) { //          /*mycall.stopMediaTo(myasr);*/ }); myasr.addEventListener(ASREvents.CaptureStarted, function() { //         clearTimeout(ts); }); function recognitionEnded() { //   myasr.stop(); } //...</span></span></code> </pre><br>  I would like to note one nuance: the <b>CaptureStarted</b> event arises based on the feedback from the Google API.  There now VAD is good enough, and these events can occur not only on speech, but also on background noise.  In order to know exactly when it is time to stop recognition in streaming mode with silence, you can use our built-in VAD in addition: <br><br><pre> <code class="javascript hljs">mycall.handleMicStatus(<span class="hljs-literal"><span class="hljs-literal">true</span></span>); mycall.addEventListener(CallEvents.MicStatusChange, <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">e</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (e.active) { <span class="hljs-comment"><span class="hljs-comment">//   } else { //   } });</span></span></code> </pre><br><h3>  Demo </h3><br><img src="https://habrastorage.org/files/f74/2ab/95b/f742ab95b50744a2b3c60ffd1834a12b.png"><br>  It would be strange not to let you test this whole farm without any extra movements (although we, of course, highly recommend starting to develop using Voximplant :) The demo is available at <a href="https://demos02.voximplant.com/a011asrdemo/">https://demos02.voximplant.com/asrdemo/</a> .  You need a browser that supports WebRTC / ORTC (Chrome / Firefox / Edge) and a microphone.  The source of this wonderful demo is available on <a href="https://gist.github.com/aylarov/9f7bee706bad5eb83e11e2f7ecbe9370">Gist</a> . </div><p>Source: <a href="https://habr.com/ru/post/310684/">https://habr.com/ru/post/310684/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../310672/index.html">Spring Boot starter for Apache Ignite DIY</a></li>
<li><a href="../310674/index.html">The history of communication towers: "the country's main television tower"</a></li>
<li><a href="../310676/index.html">Browsers and app specific security mitigation. Part 1</a></li>
<li><a href="../310678/index.html">Digital Agency 2.0 - How to become a boutique agency out of the pipeline</a></li>
<li><a href="../310680/index.html">The book "Multiplayer games. Network Application Development</a></li>
<li><a href="../310686/index.html">Launch of Azure Pack Infrastructure cloud from InfoboxCloud</a></li>
<li><a href="../310688/index.html">How we did a search in elasticsearch on vulners.com</a></li>
<li><a href="../310690/index.html">Tarantool: how to save a million dollars on a database on a high-load project</a></li>
<li><a href="../310692/index.html">Review of the best reports from the conference "Around the Cloud"</a></li>
<li><a href="../310694/index.html">System scripts in php for linux, we write a screenshot</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>