<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>KDD 2018, third day, main program</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Today, finally, the main conference program has begun. Acceptance rate this year was only 8%, i.e. must perform the best of the best of the best. Appl...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>KDD 2018, third day, main program</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/rq/n-/ac/rqn-acb7bo8qrife3vxfablf0s0.jpeg"><br><br>  Today, finally, the main conference program has begun.  Acceptance rate this year was only 8%, i.e.  must perform the best of the best of the best.  Applied and research flows are clearly separated, plus several separate related activities.  Application flows look more interesting, there are reports mainly from major (Google, Amazon, Alibaba, etc.).  I will tell about those performances which I managed to attend. <br><a name="habracut"></a><br><h3>  Data for good </h3><br>  The day began with a fairly lengthy speech that the data should be useful and used for good.  A <a href="http://www.kdd.org/kdd2018/keynotes/view/jeannette-m.-wing">professor</a> at the University of California is <a href="http://www.kdd.org/kdd2018/keynotes/view/jeannette-m.-wing">speaking</a> (it is worth noting that there are a lot of women at KDD, both among students and speakers).  All this is expressed in the abbreviation FATES: <br><br><ul><li>  Faireness - no bass in model predictions, everything is gender-neutral and tolerant. </li><li>  Acountability - there must be someone or something responsible for the decisions made by the machine. </li><li>  Transparency - transparency and explanability of decisions. </li><li>  Ethic - when working with data, special emphasis should be placed on ethics and privacy. </li><li>  Safety and security - the system must be safe (do not harm) and protected (resistant to manipulative influences from the outside) </li></ul><br>  This manifesto, unfortunately, rather expresses desire and weakly correlates with reality.  A politically correct model will be only if all signs are removed from it;  the responsibility to transfer to someone specific is always very difficult;  the further the DS develops, the more difficult it is to interpret what happens inside the model;  on ethics and privacy, there were some good examples on the first day, but otherwise, the data are often quite arbitrary. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Well, it must be admitted that modern models are often not safe (the autopilot can ditch the car with the driver) and are not protected (you can pick up examples that break the work of the neural network without even knowing how the network works).  An interesting recent work by <a href="https://arxiv.org/pdf/1705.06640.pdf">DeepExplore</a> : a system for searching for vulnerabilities in neural networks generates, including images, forcing the autopilot to steer in the wrong direction. <br><br><img src="https://habrastorage.org/webt/in/of/bt/inofbt5wpune3ft4dumcuzqo7-y.png"><br><br>  The following is yet another definition of Data Science as ‚ÄúDS is the study of extracting value form data‚Äù.  In principle, quite good.  At the beginning of the speech, the speaker made a special reservation that DSs often look at the data only from the moment of analysis, whereas the full life cycle is much wider, and this, in particular, is reflected in the definition. <br><br>  Well, not without a few examples of laboratory work. <br><br>  Let us again analyze the problem of assessing the influence of many causes on the result, but not from an advertising point of view, but as a whole.  There is still not published <a href="https://arxiv.org/pdf/1805.06826.pdf">article</a> .  Consider, for example, the question of which actors to choose for a film in order to collect a good box office.  We analyze the acting lists of the highest-grossing films and try to predict the contribution of each of the actors.  But!  There are so-called <a href="https://en.wikipedia.org/wiki/Confounding">confounders</a> , which affect how effective the actor will be (for example, Stallone will do well in a trashy thriller, but not in a romantic comedy).  To choose the right, you need to find all the confounders and evaluate them, but we will never be sure that we have found everyone.  Actually, the article proposes a new approach - deconfounder.  Instead of selecting confounders, we explicitly introduce latent variables and evaluate them in unsupervised mode, and then we learn the model with their account.  All this sounds rather strange, because it looks like a simple variant of embeddings, in which the novelty is not clear. <br><br>  Some more beautiful examples were shown, how AI and so on move to their university. <br><br><h3>  E-commerce and Profiling </h3><br>  Went to the commerce application section.  In the beginning there were some very interesting reports, at the end a certain amount of porridge, but first things first. <br><br><h4>  New User Modeling and Churn prediction </h4><br>  Snapchat is an interesting <a href="http://hanj.cs.illinois.edu/pdf/kdd18_cyang.pdf">work</a> on predicting churn.  The guys use the idea, which we also ran in successfully 4 years ago: before predicting the outflow, users should be divided into clusters by type of behavior.  At the same time, the vector space for the types of actions they turned out to be quite poor, from just a few types of interactions (we, at one time, had to make a selection of signs, to go from three hundred to one and a half), but they enrich the space with additional statistics and consider it as a time series As a result, clusters are obtained not so much about what users do, as about <b>how often</b> they do it. <br><br><img src="https://habrastorage.org/webt/a4/jy/a_/a4jya_li1who2eil3h37y5b5g0c.png"><br><br>  An important observation: the network has a ‚Äúcore‚Äù of the most densely connected and active users of 1.7 million people.  At the same time, the behavior and retention of the user strongly depends on whether he can contact someone from the ‚Äúcore‚Äù. <br><br>  Then we start building a model.  Take newbies for half a month (511 thousand), simple features and ego-networks (size and density), and see if they are associated with the ‚Äúcore‚Äù, etc.  We will feed the users' behavior to LSTM and obtain the accuracy of the outflow forecast slightly higher than that of logreg (by 7-8%).  But then the interesting begins.  To take into account the specifics of individual clusters, we will train several LSTM in parallel, and on top we will attach the attention-layer.  As a result, such a scheme begins to work on clustering (which of the LSTMs received attention), and on the outflow forecast.  Gives another + 5-7% increase in quality, and logreg already looks pale.  But!  In fact, it would be fair to compare it with a segmented logreg, which was trained separately for clusters (which can be obtained in simpler ways). <br><br><img src="https://habrastorage.org/webt/l0/aq/gv/l0aqgvmzgwfsk0gxaa1yj_irlns.png"><br><br>  I asked about interpretability: after all, the outflow is often predicted not to get a forecast, but to understand what factors influence it.  The speaker was clearly prepared for this question: for this purpose, the selected clusters are used and analyzed, than those of them, where outflow projections are higher, are distinguished from others. <br><br><h4>  Universal User Representation </h4><br>  Alibaba guys <a href="https://arxiv.org/pdf/1805.10727.pdf">talk</a> about how to build user embeds.  It turns out that having many user views is bad: many are not finalized, forces are wasted.  They managed to make a universal idea and show that it works better.  Naturally on neural networks.  The architecture is fairly standard, already in one form or another has been described more than once at a conference.  Facts from user behavior are input, build embedding on them, give it all to LSTM, hang the attention layer on top, and next to it an additional grid of static features, marry multitask (in fact, several small grids for a specific task) .  We teach all this together, exit with attention and will be the embedding of the user. <br><br><img src="https://habrastorage.org/webt/4n/kd/vh/4nkdvh_wlosdow37fdxuhc9kxbo.png"><br><br>  There are several more complex additions: in addition to simple attention, they add ‚Äúin-depth‚Äù attention net, and also use a modified version of LSTM - property gated LSTM <br><br><img src="https://habrastorage.org/webt/zb/zr/j2/zbzrj2sjy17ysntxzc5w0qdpggk.png"><br><br>  The tasks that run it all on are: CTR prediction, price preference prediction, prediction prediction, learning to rank, fashion, prediction, shop prediction.  Dataset for 10 days includes 6 * 10 <sup>9</sup> examples for training. <br><br><img src="https://habrastorage.org/webt/9_/vu/sq/9_vusqa6y-moirntdx7ujnfroqk.png"><br><br>  Next was an unexpected one: they train all this on TensorFlow, on a CPU cluster of 2,000 machines with 15 cores, it takes 4 days to fill up the data for 10 days.  Therefore, further training is provided day after day (10 hours on this cluster).  I didn‚Äôt have time to ask about GPU / FPGA :(. Adding a new task is done either through full retraining, or through additional training of a shallow mesh (netwrok fine tuning). specific tasks. A / B test showed an increase of 2-3% in various indicators. <br><br><h4>  E-Tail product return prediction </h4><br>  Predict the return of goods by the user after purchase, the <a href="https://dl.acm.org/citation.cfm%3Fid%3D3219829">work</a> presents IBM.  Unfortunately, while in open access there is no text.  Returning goods is a serious problem costing $ 200 billion a year.  To build a return forecast, use a model of a hypergraph linking products and baskets, try to find the closest hypergraph in this basket, and then estimate the likelihood of return.  To prevent the return of the online store there are many opportunities, for example, to offer a discount for removing certain items from the cart. <br><br>  Immediately noted that there is a significant difference between the baskets with duplicates (for example, two identical T-shirts of different sizes) and without, therefore, we must immediately build different models for these two cases. <br><br>  The general algorithm is called HyperGo: <br><br><ul><li>  We build a hypergraph to represent purchases and returns with information from the user, product, basket. </li><li>  Next, use local graph cut based on random walk to get local information for the forecast. </li><li>  Separately, we consider baskets with doubles and no doubles. </li><li>  We use Bayesian methods to assess the impact of a single product in the basket. </li></ul><br>  Compare the quality of the forecast return with kNN on baskets, weighted by Jacquard kNN, rationing the number of duplicates, we get an increase in the result.  There was a link to GitHub on the slides, but it was not possible to find their sources, and there is no link in the article. <br><br><h4>  OpenTag: Open Attribute Value Extraction from Product Profiles </h4><br>  Quite an interesting <a href="https://arxiv.org/pdf/1806.01264">job</a> from Amazon.  Challenge: Set different facts for Alexa to make her answer questions better.  They say how complicated everything is, old systems do not know how to work with new words, often require a large number of handwritten rules and heuristics, and the results are so-so.  Solving all the problems will help, of course, the neural grids with the embednig-LSTM-attention architecture that has already become familiar, but we will make the LSTM double, and on top of it will throw a <a href="https://en.wikipedia.org/wiki/Conditional_random_field">Conditional Random Field</a> . <br><br><img src="https://habrastorage.org/webt/qj/nr/t_/qjnrt_z_5tmuja7kp_sv22ks1oc.png"><br><br>  We will solve the problem of tagging a sequence of words.  Tags will show where we begin and end the sequence of certain attributes (for example, the taste and composition of dog food), and LSTM will try to predict them.  As a bun and a curtsey in the direction of Mechanical Turk, active model training is used.  To select the examples that need to be sent to the additional markup, heuristics are used ‚Äúto take those examples where the tags are most often intersected between eras‚Äù. <br><br><h4>  Learning and Transferring IDs Representation in E-commerce </h4><br>  In their <a href="https://arxiv.org/pdf/1712.08289.pdf">work,</a> colleagues from Alibaba again return to the issue of building embeddings, this time looking not just at users, but at the ED in principle: for products, brands, categories, users, etc.  Interaction sessions are used as a source of data, and additional attributes are taken into account.  Skipgrams are used as the main algorithm. <br><br><img src="https://habrastorage.org/webt/yw/r2/sw/ywr2swhvrtq3blgkqeh7wuin6hc.png"><br><br>  The speaker has a very difficult pronunciation with a strong Chinese accent. Understanding what is happening is almost not realistic.  One of the ‚Äútweaks‚Äù of the work is in the mechanics of transferring views with a lack of information, for example, from item to user through averaging (quickly, you don‚Äôt need to learn the whole model).  With old items, you can initialize new ones (apparently, by content similarity), as well as transfer the user's view from one domain (electronics) to another (clothing). <br><br><img src="https://habrastorage.org/webt/va/lz/tp/valztpoqptx3zsju_g17b3yfkle.png"><br><br>  In general, it is not entirely clear where the novelty is here, apparently, it is necessary to dig into the details;  in addition, it is not clear how this relates to the previous story about unified user views. <br><br><h4>  Online Parameter Selection for Web-based Ranking Problems </h4><br>  Very interesting <a href="http://www.kdd.org/kdd2018/accepted-papers/view/online-parameter-selection-for-web-based-ranking-problems">work</a> from comrades from LinkedIn.  The essence of the work is to choose online the optimal parameters of the algorithm, taking into account several competing goals.  Consider the tape as an application area and try to increase the number of sessions of some types: <br><br><ul><li>  A session with some viral action (VA). </li><li>  Session with sending resume (JA). </li><li>  Session with interaction with content in the tape (EFS). </li></ul><br>  The ranking function in the algorithm is a weighted average of the predicted conversion to these three goals.  Actually, weights are those parameters that we will try to optimize online.  Initially, they formulate a business problem as ‚Äúmaximize the number of viral sessions while maintaining the other two types not below a certain level‚Äù, but then transform it a bit for ease of optimization. <br><br><img src="https://habrastorage.org/webt/h1/rq/b4/h1rqb4td_8azpmgzw2wtdevm_na.png"><br><br>  We model the data with a set of binomial distributions (whether the user converts to the desired target or not, seeing a tape with certain parameters), where the probability of success for the given parameters is the <a href="https://en.wikipedia.org/wiki/Gaussian_process">Gaussian process</a> (its own for each type of conversion).  Next we use the <a href="https://en.wikipedia.org/wiki/Thompson_sampling">Thompson semper</a> with ‚Äúinfinite- <a href="https://en.wikipedia.org/wiki/Thompson_sampling">handed</a> ‚Äù gangsters to select the optimal parameters (not online, but offline on historical data, therefore for a long time).  They give a few tips: use bold points to build the initial grid and be sure to add <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">epsilon-greedy</a> -sampling (with a chance to epsilon try a random point in space), otherwise you can overlook the global maximum. <br><br>  Simulate sampling offline once an hour (you need a lot of samples), the result is a certain distribution of optimal parameters.  Further, when a user comes in from this distribution, they take specific parameters for tape building (it is important to do this consistently with the seed from the user ID for initialization so that the user's tape does not change radically). <br><br><img src="https://habrastorage.org/webt/8p/zi/t2/8pzit2bncq4aw_kbka2inil9tnk.png"><br><br>  According to the results of the A / B experiment, we received an increase in sending resumes by 12% and likes by 3%.  Share some observations: <br><br><ul><li>  It is easier to make sampling easier than trying to add more information to the model (for example, about the day of the week / hour). </li><li>  We assume in this approach independence of goals, but it is not clear whether it is (or rather, no).  However, the approach works. </li><li>  Business must set goals and thresholds. </li><li>  It is important to exclude a person from the process and let him do something useful. </li></ul><br><h4>  Near Real-time Optimization of Activity based notification </h4><br>  Another <a href="http://www.kdd.org/kdd2018/accepted-papers/view/near-real-time-optimization-of-activity-based-notifications">job</a> from LinkedIn, this time about managing notifications.  We have people, events, delivery channels and long-term goals to increase user engagement without a significant negative in the form of complaints and formal replies from push.  The task is important and difficult, and you have to do everything correctly: the right people in the right time to send the right content through the right channel and in the right amount. <br><br><img src="https://habrastorage.org/webt/xk/01/5o/xk015orbalg1xyd2oko_qoc0zvq.png"><br><br>  The system architecture in the picture above, the essence of what is happening is approximately as follows: <br><br><ul><li>  We filter every spam at the entrance. </li><li>  The right people: the helmet to everyone who is strongly connected with the author / content, balancing the threshold for the strength of communication, manage the coverage and relevance. </li><li>  The right time: immediately send content for which time is important (events of friends), the rest can be held for less dynamic channels. </li><li>  Proper content: use logreg!  Build a prediction model of a click on a bunch of signs, separately for the case when a person is in the application and when there is not. </li><li>  The correct channel: we set different thresholds for relevance, the most strict for pushing, lower - if the user is now in the application, even lower - for the mail (there are all sorts of digests / advertisements in it). </li><li>  The correct volume: the circumcision model by volume is at the exit, it also looks at relevance, it is recommended to do it individually (good threshold heuristics - minimum score of sent objects for the last few days) </li></ul><br>  On the A / B-test received a few percent increase in the number of sessions. <br><br><h4>  Real-time Personalization using Embeddings for Search Ranking at Airbnb </h4><br>  And this was the <a href="http://www.kdd.org/kdd2018/accepted-papers/view/real-time-personalization-using-embeddings-for-search-ranking-at-airbnb">best application paper</a> from AirBnB.  Task: to optimize the issuance of similar placements and search results.  We decide through the construction of embeddings of placements and users in the same space in order to further evaluate the similarity.  It is important to remember that there is a long-term history (user preferences) and a short-term (current user / session content). <br><br>  Without further ado, we use word2vec for embedding of places on click sequences in search sessions (one session - one document).  But some modifications still do (KDD, as in any way): <br><br><ul><li>  We take the session, during which there was a reservation. </li><li>  What is ultimately booked is kept as a global context for all session elements during a w2v update. </li><li>  Negatives in training are sampled in the same city. </li></ul><br><img src="https://habrastorage.org/webt/fa/bf/jt/fabfjtillg4zteyl_ys0egmqoig.png"><br><br>  The effectiveness of this model is checked in three standard ways: <br><br><ul><li>  Check in offline: how quickly we can raise the right hotel up in the search session. </li><li>  Check with assessors: built a special tool for visualizing similar ones. </li><li>  A / B-test: spoiler, CTR has grown significantly, bookings have not increased, but now they happen before </li></ul><br>  We try to rank the results of search results not only ahead of time, but also to re-arrange (therefore real-time) when a response is received - a click on one sentence and ignore the other.  The approach is to collect the clicked and ignored places in two groups, to find in each centroid of embeddings (there is a special formula), and then in the ranking we raise similar to clicks, omit similar to skips. <br><br>  On the A / B-test, we got an increase in bookings, the approach stood the test of time: it was invented a year and a half ago and is still spinning in production. <br><br>  And if you need to look in another city?  By clicks, it is not possible to arrange in advance; there is no information about the attitude of users to places in this locality.  To circumvent this problem, we introduce ‚Äúcontent embeddings‚Äù.  At first we will get a simple discrete space of signs (cheap / expensive, in the center / on the outskirts, etc.) of the order of about 500 thousand types (for both places and people).  Next we build embeddings by type.  When training, do not forget to add a clear negative refusal (when the landlord has not confirmed the reservation). <br><br><h4>  Graph Convolution Neural Networks for Web-Scale Recommender Systems </h4><br>  <a href="https://arxiv.org/pdf/1806.01973.pdf">Work</a> from Pinterest on pin recommendations.  Consider the bipartite graph of user pins and add network features to the recommendations.  The graph is very large - 3 billion pins, 16 billion interactions, it was not possible to get the classic graph embeddings.  As a result, starting from the <a href="https://github.com/williamleif/GraphSAGE">GraphSAGE</a> system, which builds embeddings according to the local structure (we look at the embeddings of our neighbors and update our own, we update our neighbors according to the message passing model), build <a href="https://medium.com/%40Pinterest_Engineering/pinsage-a-new-graph-convolutional-neural-network-for-web-scale-recommender-systems-88795a107f48">PinSAGE</a> .  After embeddings are built, it is recommended through the search for the nearest neighbors in the embedding space. <br><br><img src="https://habrastorage.org/webt/zg/w4/54/zgw454wcs1_22halg_hdivs3gb4.png"><br><br>  Further "bloody details" begin: <br><br><ul><li>  It is advised to use max margin loss. </li><li>  Training goes together on a CPU / GPU: The CPU samples a packet of subgraphs of the same structure (so that the GPU can effectively pair in parallel) and sends the GPU.  He does not wait for the answer, immediately begins to prepare the next package. </li><li>  For vertices, with a high degree, not all neighbors are taken, but random walk is sampled. </li><li>  They use the Curriculum Learning approach: they are gradually adding more and more hard negative examples.  on which the model in past epochs was often mistaken. </li><li>  On top of Map reduce, apply a trained convolution and get embeddings for all vertices. </li></ul><br>  As a result, they clearly showed that the scheme works better than visual similarity, according to annotations and the current collaboration.  Implemented, proved to be good on the A / B-test. <br><br><h4>  Q &amp; R: A Two-Stage Approach Toward Interactive Recommendation </h4><br>  ‚ÄúAnd what will happen if we give the opportunity to the referee to ask the user what he wants?‚Äù - this was the question the guys from YouTube asked in their <a href="http://alexbeutel.com/papers/q-and-r-kdd2018.pdf">work</a> .  Questions considered only one format: ‚Äúon what topic would you like to watch the video?‚Äù.  and used the result to optimize ‚Äúonboarding‚Äù (there is, it turns out, a place in YouTube where you can set your own interests). <br><br>  Now YouTube recommendations are built using Video-RNN, which predicts video ID.  The guys decided to take the same architecture, but predict the ID of a certain topic, and also add information about the topic over the old grid (post fusion).  Below is the result (finally somewhere used <a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit">GRU</a> , and then all LSTM and LSTM). <br><br><img src="https://habrastorage.org/webt/p3/xk/kq/p3xkkqs-clle6oma9tr6yt7telc.png"><br><br>  We train this happiness on 7 days of data, we test on the 8th, we take users with enough views.  Interestingly, theme prediction works 8% better than multi-class classification by bug-of-word sequence.  In the A / B test, <a href="http://radlinski.org/papers/RadlinskiEtAl08ClickEval.pdf">interleaving is</a> + 0.7% to the viewing time on the main page, +1.23% from opening notifications. <br><br>  After that, they switched to onboarding: when using a personalized collection, the user fills the onboarding 18% more often, which ultimately results in a +4% increase in viewing time. <br><br><h3>  Graph and social nets </h3><br>  After lunch, I decided to listen to research presentations, since the topic, as they say, is close to heart. <br><br><h4>  Graph Classification using Structural Attention </h4><br>  Quite an interesting <a href="http://ryanrossi.com/pubs/KDD18-graph-attention-model.pdf">work</a> on the classification of graphs, focused on "large size" graphs.  As examples, mainly looked at different molecules, such as drugs.  Earlier in this area, <a href="https://en.wikipedia.org/wiki/Graphlets">graphlists were</a> used, but on large graphs it‚Äôs difficult, so we will train the grid. <br><br>  In essence, we do LSTM by the sequence of walks along the graph, but we add a kind of attention-layer, which, by the type of the current vertex, gives the probabilities to which types of vertices we need to move further.  This layer focuses our wandering in the right direction.  As a type I considered a chemical element, on the sidelines I threw an idea to the author to move from vertex types to some simple embeddings, and said that he would try. <br><br>  The network is being trained on the RL model: we give it a wander along the graph for a limited number of steps, if after this the classification has failed, we give a penalty.  In addition to attention, leading the walk, on top of LSTM is added self-attention, which looks at the whole history of the walk (already familiar scheme).  When crossing, we get several ‚Äúagents‚Äù who will take walks and aggregate their answers. <br><br><img src="https://habrastorage.org/webt/84/4t/fy/844tfywedlskwhu90qbq-qpbm1q.png"><br><br>  Tested on data on the structure of drugs, compared only with simple baseline with motivation - we do not see the entire graph.  Walking does not always work, they want to try <a href="https://github.com/stanfordnlp/treelstm">TreeLSTM</a> . <br><br><h4>  SpotLight: Detecting Anomalies in Streaming Graphs </h4><br>  The authors <a href="http://www.kdd.org/kdd2018/accepted-papers/view/spotlight-detecting-anomalies-in-streaming-graphs">are trying to</a> find anomalies in the interaction graphs, for example, spam ringing, port scanning, switch fallout.  Anomaly is defined as "the sudden appearance or disappearance of a large dense subgraph."  The task is complicated by the fact that anomalies must be determined on a continuous basis, having limited memory and time. <br><br>  The basic idea is to use "hashing" graphs in a certain metric space and comparing the sequence of snapshots of hashes.  If at some point the snapshot jumps somewhere far, then we have an anomaly. <br><br><img src="https://habrastorage.org/webt/r_/is/5r/r_is5rbyezcqm8jhtabf7-rljxc.png"><br><br>  The hashing algorithm impresses with its simplicity: take two random regions of the graph and calculate how many connections there are between them.  The result will be one hash dimension.  Repeat the procedure for the remaining dimensions.  To calculate the hash it is not necessary to see the entire graph, the hash can be counted on the flow of events on adding / deleting links. <br><br>  Despite the seemingly naive, the authors provide some theoretical guarantees on the separability of situations with and without anomalies.  Tested on <a href="https://www.ll.mit.edu/r-d/datasets">labeled DARPA dataset</a> .  It seems to work well, while in comparison with the baselines, they miss different types of attacks (i.e. there is a potential for an ensemble). <br><br><h4>  Adversarial Attacks on Graph Networks </h4><br>  And now the best research <a href="https://arxiv.org/pdf/1806.02371.pdf">work</a> .  Adversary is very popular on the Internet, especially in the recommendations, search, etc .: attackers are trying to raise their website / content higher.  Nowadays, networks working with graphs are becoming increasingly common in the industry - are they resistant to attacks (spoiler is not)?  As an example, consider the task of restoring a class of nodes of a graph with incomplete markup. <br><br>  In principle, an attacker has many possibilities: <br><br><ul><li>  Change attributes on "your" node. </li><li>  Change the connection of "his" node. </li><li>  Try to "throw" the connection to another node. </li><li>  The attack can be directed not only at the process of crossing, but also at the process of learning (poisoning). </li></ul><br>  They offer two approaches to cracking.  With open information about the network that needs to be hacked, they are doing a counter network, when closed, they use a genetic algorithm.             .   . <br><br>    ,        unnoticeability:  ,      ? <br><br><h4> Multi-Round Influence maximization </h4><br>   <a href="https://arxiv.org/pdf/1802.04189.pdf"></a>            .         ,   ,   . <br><br>         ,         (  ,    ). <br><br>  --    : <br><br><ul><li>     :  ,         . </li><li>     :  ,   . </li><li> - :     inffluencer-   .  ,  . </li></ul><br>       Reverse-Reachable sets:     inffluencer-,   ,   . <br><br><h4> EvoGraph: An Effective and Efficient Graph Upscaling Method for Preserving Graph Properties </h4><br>      .   ,   <a href="http://www.kdd.org/kdd2018/accepted-papers/view/evograph-an-effective-and-efficient-graph-upscaling-method-for-preserving-g"> </a> ,    . <br><br>   ,     ,      power law.   ,     , ¬´¬ª  ,         :  ¬´¬ª             . <br><br><img src="https://habrastorage.org/webt/h5/2g/0s/h52g0s6d0gnepiwjkgxw-jhlb1a.png"><br><br>         ,    ,   .    <a href="https://github.com/chan150/EvoGraph"></a> . <br><br><h3>  </h3><br>      data science  .   .    ,   .    . <br><br> <a href="https://en.wikipedia.org/wiki/All_models_are_wrong">All models are wrong, but some are useful</a> ‚Äî   data science,  ,   ,        . ¬´¬ª          ( ,    , adversarial  ..) <br><br> <a href="http://www.embo.org/news/articles/2015/the-numbers-speak-for-themselves">www.embo.org/news/articles/2015/the-numbers-speak-for-themselves</a> ‚Äî      (overfitting, selection bias,  ,     ..) <br><br>        1762    <a href="https://en.wikipedia.org/wiki/The_Equitable_Life_Assurance_Society">The Equitable Life Assurance Society</a> ,       . <br><br>      :  2011-      ,       (   ‚Äî      ¬´¬ª,         ).       UK: <br><br><ul><li>        ,      . </li><li>       ,    . </li><li>  :          . </li><li>   ¬´¬ª    ,   . </li><li>   , ,  . </li><li>        . </li></ul><br>    ¬´ ¬ª. <br><br>       . F ‚Äî Fairness,     .  ML    (   )   ,     ¬´¬ª  . </div><p>Source: <a href="https://habr.com/ru/post/420885/">https://habr.com/ru/post/420885/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../420873/index.html">Quest that no one can pass</a></li>
<li><a href="../420875/index.html">Why do I need a 3D printer</a></li>
<li><a href="../420877/index.html">YouDrive introduced registration via Telegram Passport</a></li>
<li><a href="../420881/index.html">The Paul Allen Company is preparing an orbital plane for launch from Stratolaunch</a></li>
<li><a href="../420883/index.html">The national competition "Awarding of computer games in Germany" in 2018, in which there is a place for indie</a></li>
<li><a href="../420887/index.html">Tele2 hackathon report</a></li>
<li><a href="../420889/index.html">Military mine detection technology helps robots to navigate on any roads</a></li>
<li><a href="../420891/index.html">Migration to JUnit 5 in 10 minutes Test Time Measurement with Extensions</a></li>
<li><a href="../420893/index.html">Franchise Packing A to B</a></li>
<li><a href="../420895/index.html">How I revived the device (BH-USB-560v2 JTAG emulator) via U-Boot</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>