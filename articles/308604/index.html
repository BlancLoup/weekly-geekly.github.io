<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Mathematics for artificial neural networks for beginners, part 3 - gradient descent continued</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Part 2 - Gradient Descent Start 

 In the previous part, I began to analyze the optimization algorithm called gradient descent. The previous article b...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Mathematics for artificial neural networks for beginners, part 3 - gradient descent continued</h1><div class="post__text post__text-html js-mediator-article"> <a href="https://habrahabr.ru/post/307312/">Part 2 - Gradient Descent Start</a> <br><br>  In the previous part, I began to analyze the optimization algorithm called gradient descent.  The previous article broke down on writing a variant of the algorithm called packet gradient descent. <br><br>  There is another version of the algorithm - stochastic gradient descent.  Stochastic = random. <br><a name="habracut"></a><br> <code>,    <br> <br> { <br> for i in train_set <br> { <br> <img src="https://habrastorage.org/files/315/021/f18/315021f1836946cf9809242aa70dc04e.png"> <br> } <br> } <br></code> <br>  Also recall what the batch looks like: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     <code>,    <br> { <br> <img src="https://habrastorage.org/files/278/d47/22d/278d4722d3f048ac8d24eec1fa1b38ca.png"> <br> } <br></code> <br>  The formulas are similar, but, as you can see, the packet gradient descent calculates one step, using the entire data set at once, whereas the stochastic step uses only 1 element per step.  You can cross these two options by getting a mini-batch descent, which processes, for example, 100 elements at once, but not all or one. <br><br>  These two options behave similarly, but not equally.  The batch descent really follows the direction of the fastest descent, whereas the stochastic descent, using only one element from the training sample, cannot correctly compute the gradient for the entire sample.  This distinction is easier to explain graphically.  To do this, I modify the code from the first part, in which the linear regression coefficients were calculated.  The cost function for it is as follows: <br><img src="https://habrastorage.org/files/063/7cf/4d1/0637cf4d1a5148459baf3e9e3102fc32.png"><br>  As you can see, this is a slightly elongated paraboloid.  And also his ‚Äútop view‚Äù, where the true minimum, found analytically, is marked with a cross. <br><img src="https://habrastorage.org/files/abe/693/81a/abe69381a8094a0c84ad7a1d06235cde.png"><br>  First, let's look at how the package descent behaves: <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">batch_descent</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(A, Y, speed=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.001</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> theta = np.array(INITIAL_THETA.copy(), dtype=np.float32) theta.reshape((len(theta), <span class="hljs-number"><span class="hljs-number">1</span></span>)) previous_cost = <span class="hljs-number"><span class="hljs-number">10</span></span> ** <span class="hljs-number"><span class="hljs-number">6</span></span> current_cost = cost_function(A, Y, theta) <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> np.abs(previous_cost - current_cost) &gt; EPS: previous_cost = current_cost derivatives = [<span class="hljs-number"><span class="hljs-number">0</span></span>] * len(theta) <span class="hljs-comment"><span class="hljs-comment"># --------------------------------------------- for j in range(len(theta)): summ = 0 for i in range(len(Y)): summ += (Y[i] - A[i]@theta) * A[i][j] derivatives[j] = summ #    theta[0] += speed * derivatives[0] theta[1] += speed * derivatives[1] # --------------------------------------------- current_cost = cost_function(A, Y, theta) print("Batch cost:", current_cost) plt.plot(theta[0], theta[1], 'ro') return theta</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Batch animation</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/328/6da/d40/3286dad404634807b1841b46006ea897.gif"><br></div></div><br>  Due to the fact that the paraboloid is elongated, a ‚Äúravine‚Äù passes through its ‚Äúbottom‚Äù, into which we fall.  Because of this, the last steps along this ravine are very small, but nevertheless, sooner or later the gradient descent will reach a minimum.  The descent takes place along the anti-gradient line, each step approaching the minimum. <br><br>  Now the same function, but for stochastic descent: <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">stochastic_descent</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(A, Y, speed=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> theta = np.array(INITIAL_THETA.copy(), dtype=np.float32) previous_cost = <span class="hljs-number"><span class="hljs-number">10</span></span> ** <span class="hljs-number"><span class="hljs-number">6</span></span> current_cost = cost_function(A, Y, theta) <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> np.abs(previous_cost - current_cost) &gt; EPS: previous_cost = current_cost <span class="hljs-comment"><span class="hljs-comment"># -------------------------------------- # for i in range(len(Y)): i = np.random.randint(0, len(Y)) derivatives = [0] * len(theta) for j in range(len(theta)): derivatives[j] = (Y[i] - A[i]@theta) * A[i][j] theta[0] += speed * derivatives[0] theta[1] += speed * derivatives[1] current_cost = cost_function(A, Y, theta) print("Stochastic cost:", current_cost) plt.plot(theta[0], theta[1], 'ro') # -------------------------------------- current_cost = cost_function(A, Y, theta) return theta</span></span></code> </pre><br></div></div><br>  The definition says that only one element is selected ‚Äî in this code, each subsequent element is selected at random. <br><br><div class="spoiler">  <b class="spoiler_title">Stochastic animation</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/c3d/c66/7bb/c3dc667bb1f44d3eb2132022024247a0.gif"><br></div></div><br>  As can be seen, in the case of the stochastic variant, the descent does not go along the line of the antigradient, but in general it is not clear how - every step deviates in a random direction.  It may seem that such random jerky movements to the minimum can only come by chance, but it was proved that the stochastic gradient descent converges almost certainly.  <a href="https://www.cs.rochester.edu/u/jliu/CSC-576/class-note-10.pdf">A couple of</a> <a href="http://leon.bottou.org/papers/bottou-98x">links</a> , <a href="http://jmlr.csail.mit.edu/proceedings/papers/v28/shamir13.pdf">for example</a> . <br><br><div class="spoiler">  <b class="spoiler_title">All code in its entirety</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt TOTAL = <span class="hljs-number"><span class="hljs-number">200</span></span> STEP = <span class="hljs-number"><span class="hljs-number">0.25</span></span> EPS = <span class="hljs-number"><span class="hljs-number">0.1</span></span> INITIAL_THETA = [<span class="hljs-number"><span class="hljs-number">9</span></span>, <span class="hljs-number"><span class="hljs-number">14</span></span>] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">func</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">0.2</span></span> * x + <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">generate_sample</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(total=TOTAL)</span></span></span><span class="hljs-function">:</span></span> x = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> x &lt; total * STEP: <span class="hljs-keyword"><span class="hljs-keyword">yield</span></span> func(x) + np.random.uniform(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) * np.random.uniform(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>) x += STEP <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">cost_function</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(A, Y, theta)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (Y - A@theta).T@(Y - A@theta) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">batch_descent</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(A, Y, speed=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.001</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> theta = np.array(INITIAL_THETA.copy(), dtype=np.float32) theta.reshape((len(theta), <span class="hljs-number"><span class="hljs-number">1</span></span>)) previous_cost = <span class="hljs-number"><span class="hljs-number">10</span></span> ** <span class="hljs-number"><span class="hljs-number">6</span></span> current_cost = cost_function(A, Y, theta) <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> np.abs(previous_cost - current_cost) &gt; EPS: previous_cost = current_cost derivatives = [<span class="hljs-number"><span class="hljs-number">0</span></span>] * len(theta) <span class="hljs-comment"><span class="hljs-comment"># --------------------------------------------- for j in range(len(theta)): summ = 0 for i in range(len(Y)): summ += (Y[i] - A[i]@theta) * A[i][j] derivatives[j] = summ #    theta[0] += speed * derivatives[0] theta[1] += speed * derivatives[1] # --------------------------------------------- current_cost = cost_function(A, Y, theta) print("Batch cost:", current_cost) plt.plot(theta[0], theta[1], 'ro') return theta def stochastic_descent(A, Y, speed=0.1): theta = np.array(INITIAL_THETA.copy(), dtype=np.float32) previous_cost = 10 ** 6 current_cost = cost_function(A, Y, theta) while np.abs(previous_cost - current_cost) &gt; EPS: previous_cost = current_cost # -------------------------------------- # for i in range(len(Y)): i = np.random.randint(0, len(Y)) derivatives = [0] * len(theta) for j in range(len(theta)): derivatives[j] = (Y[i] - A[i]@theta) * A[i][j] theta[0] += speed * derivatives[0] theta[1] += speed * derivatives[1] # -------------------------------------- current_cost = cost_function(A, Y, theta) print("Stochastic cost:", current_cost) plt.plot(theta[0], theta[1], 'ro') return theta X = np.arange(0, TOTAL * STEP, STEP) Y = np.array([y for y in generate_sample(TOTAL)]) #  ,     X = (X - X.min()) / (X.max() - X.min()) A = np.empty((TOTAL, 2)) A[:, 0] = 1 A[:, 1] = X theta = np.linalg.pinv(A).dot(Y) print(theta, cost_function(A, Y, theta)) import time start = time.clock() theta_stochastic = stochastic_descent(A, Y, 0.001) print("St:", time.clock() - start, theta_stochastic) start = time.clock() theta_batch = batch_descent(A, Y, 0.001) print("Btch:", time.clock() - start, theta_batch)</span></span></code> </pre><br></div></div><br>  On 200 elements, there is almost no difference in speed, however, by increasing the number of elements to 2000 (which is also very small) and adjusting the learning speed, the stochastic version beats the batch as it wants.  However, due to the stochastic nature, sometimes the method misses, oscillating near the minimum without the possibility to stop.  Something like this: <br><br><img src="https://habrastorage.org/files/31e/480/8ec/31e4808ec9294d3c90e64027f8ffa646.png"><br><br>  This fact makes pure implementation inapplicable.  To somehow call to order and reduce the "chance" you can reduce the speed of learning.  In practice, mini-batch variation is used - it differs from the stochastic one in that instead of one randomly selected element, more than one is chosen. <br><br>  Quite a lot has been written about the difference, pros and cons of these two approaches, to summarize in brief: <br><br>  - Batch descent is good for strictly convex functions, because it is confidently striving for a minimum of global or local. <br>  - Stochastic, in turn, works better on functions with a large number of local minima - every step is a chance that the next value will ‚Äúknock‚Äù out of the local pit and the final solution will be more optimal than for the package descent. <br>  - Stochastic is calculated faster - not all elements from the sample are needed at every step.  The entire sample may not get into the memory.  But more steps are required. <br>  - For stochastic, it is easy to add new elements while working (‚Äúonline‚Äù training). <br>  - In the case of mini-batch, you can also vectorize the code, which will significantly speed up its execution. <br><br>  Also, there are many modifications of gradient descent - momentum, steepest descent, averaging, Adagrad, AdaDelta, RMSProp and others.  <a href="http://sebastianruder.com/optimizing-gradient-descent/">Here</a> you can see a brief overview of some.  Often they use the gradient values ‚Äã‚Äãfrom the previous steps or automatically calculate the best speed value for a given step.  Using these methods for a simple smooth OLS function does not make much sense, but in the case of neural networks and networks with a large number of layers \ neurons, the cost function becomes quite sad and the gradient descent can get stuck in a local well without reaching an optimal solution.  For such problems are suitable methods on steroids.  Here is an example of a simple function to minimize (two-dimensional linear regression with OLS): <br><img src="https://habrastorage.org/files/aaa/aee/057/aaaaee05799d4875b8a5e5b1d6b00777.png"><br>  And an example of a non-linear function: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/db4/110/1f0/db41101f03964306eba3f3ed8ab1b98b.png"><br><br>  Gradient descent is a first order optimization method (first derivative).  There are also many second-order methods ‚Äî for them it is necessary to calculate the second derivative and build the Hessian (a rather expensive operation - <img src="https://habrastorage.org/files/959/9c6/e81/9599c6e8161a434a9fa8cde4d9212d01.png">  ).  For example, the gradient descent of the second order (the learning rate was replaced by the Hessian), BFGS, conjugate gradients, the Newton method and a huge number of <a href="https://en.wikipedia.org/wiki/Mathematical_optimization">other methods</a> .  In general, optimization is a separate and very wide layer of problems.  However, here is <a href="https://www.cs.nyu.edu/~yann/talks/lecun-20071207-nonconvex.pdf">an example (though only a presentation) of the work</a> + <a href="http://videolectures.net/eml07_lecun_wia/">video of</a> <a href="https://ru.wikipedia.org/wiki/%25D0%259B%25D0%25B5%25D0%25BA%25D1%2583%25D0%25BD,_%25D0%25AF%25D0%25BD">Jan Lekun</a> , in which he says that you can not soar and just use gradient methods.  Even considering that the 2007 presentation, many recent experiments with large ANNs used gradient methods.  <a href="">For example</a> . <br><br>  You won't get far on bare cycles - the code needs to be vectorized.  The main algorithm for vectorization is packet gradient descent, with the proviso that <img src="https://habrastorage.org/files/529/6c7/40d/5296c740d79a47d6adc88a8e9afcc1aa.png">  where k is the number of elements in the test set.  Thus, vectorization is suitable for the mini-batch method.  Like last time, I'll write out everything in the vectors that have been opened.  Note that the first matrix is ‚Äã‚Äãtransposed - <img src="https://habrastorage.org/files/508/5c7/acc/5085c7accd6045339dead77f7f72674a.png"><br><br><img src="https://habrastorage.org/files/3a7/885/252/3a7885252d8d4b0bb72ff2ea023ac4b4.png"><br><br>  For proof, let's go a couple of steps in the opposite direction: <br><br><img src="https://habrastorage.org/files/d9e/e14/750/d9ee147502e14d89a92dcbfcff66f341.png"><br><br><img src="https://habrastorage.org/files/b17/4a5/388/b174a53889cb4ea9bce155918e55fcb6.png"><br><br>  In the previous formula, in each line the index j is fixed, and i - varies from 1 to n.  Curtailed amount: <br><br><img src="https://habrastorage.org/files/8b0/f15/395/8b0f153952364d3a8d1f961b393930a1.png"><br><br>  This expression is exactly the same as in the definition of gradient descent.  Finally, the final step is the folding of vectors and matrices: <br><br><img src="https://habrastorage.org/files/f5a/8ab/8d9/f5a8ab8d938945d4a20f86c14d905e23.png"><br><br>  The cost of one such step - <img src="https://habrastorage.org/files/500/019/82c/50001982c5db4a609d4e42e0fc4f3be4.png">  where n is the number of elements, p is the number of signs.  Much better <img src="https://habrastorage.org/files/f2a/4af/9bb/f2a4af9bb7ac48ba8615afb55d3d5d61.png">  .  An expression identical to this also occurs: <br><br><img src="https://habrastorage.org/files/aa5/99e/fdb/aa599efdb0ac486a959831e8ef07a937.png"><br><br>  Notice that the predicted value and the real value were reversed, which led to a change in sign before the learning speed. <br><br>  "To run the examples you need: numpy, matplotlib. <br>  ¬ªMaterials used in the article - <a href="https://github.com/m9psy/neural_network_habr_guide">github.com/m9psy/neural_network_habr_guide</a> </div><p>Source: <a href="https://habr.com/ru/post/308604/">https://habr.com/ru/post/308604/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../308594/index.html">An example of using policy-based design in C ++ instead of copy-paste and creating OOPs of hierarchies</a></li>
<li><a href="../308596/index.html">How to use the standard settings to kill the microcontroller. Part 1</a></li>
<li><a href="../308598/index.html">Several new products in the world of data centers: will they become the norm?</a></li>
<li><a href="../308600/index.html">Russian hacker convicted of stealing $ 169 million</a></li>
<li><a href="../308602/index.html">WhatsApp is going to share data of its users with Facebook</a></li>
<li><a href="../308606/index.html">The history of the virus "The program that dismisses people"</a></li>
<li><a href="../308608/index.html">12 tools and websites for UI / UX designers</a></li>
<li><a href="../308610/index.html">Prometheus - practical use</a></li>
<li><a href="../308612/index.html">Heisenbag - New Testing Conference by JUG.ru Group</a></li>
<li><a href="../308614/index.html">Developing FreePBX Modules</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>