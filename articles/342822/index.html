<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>What happens in Kubernetes when starting the kubectl run? Part 2</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Note trans. : The second and final part of the translation of the material, entitled in the original as ‚ÄúWhat happens when ... Kubernetes edition!‚Äù ng...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>What happens in Kubernetes when starting the kubectl run? Part 2</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/-6/5h/k1/-65hk1kox7mbdg93ywwhjsk6tae.png"><br><br>  <i><b>Note</b></i>  <i><b>trans.</b></i>  <i>: The second and final part of the translation of the material, entitled in the original as ‚ÄúWhat happens when ... Kubernetes edition!‚Äù nginx.</i> <i><br><br></i>  <i>If the <a href="https://habrahabr.ru/company/flant/blog/342658/">first part</a> was devoted to the work of kubectl, kube-apiserver, etcd and initializers, now we will talk about the controllers Deployments and ReplicaSets, informers, scheduler and kubelet.</i>  <i>Let me remind you that we stopped at the moment when the request transmitted by the user (via kubectl) was authorized and executed in Kubernetes, new objects (resources) were created and stored in the database (etcd), after which they were initialized for apiserver).</i> <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  Control loops </h2><br><h3>  Deployments controller </h3><br>  By this stage, the Deployment entry exists in etcd and all initialization logic is completed.  The following steps are about setting up the topology of the resources used in Kubernetes.  If you think about it, then Deployment is really just a collection of ReplicaSets, and ReplicaSet is a collection of pods.  What happens in Kubernetes to create this hierarchy from a single HTTP request?  This is where the K8s integrated controllers take over. <br><br>  Kubernetes makes extensive use of ‚Äúcontrollers‚Äù throughout its system.  The controller is an asynchronous script that compares the current state of the Kubernetes system with the desired one.  Each controller is responsible for its small part and is started by the component <code>kube-controller-manager</code> .  Let's introduce ourselves to the first one who enters the process, the Deployment controller. <br><br>  After the record with the Deployment is saved in etcd and initialized, it becomes visible in kube-apiserver.  When a new resource appears, it is detected by the Deployment controller, whose task is to track changes among the corresponding records (Deployments).  In our case, the controller <a href="">registers a</a> special callback for creation events via an informant (for details on what it is, see below). <br><br>  This handler will be called when Deployment becomes available for the first time, and starts its work by <a href="">adding an</a> object to the internal queue.  By the time he gets to the processing of this object, the controller will <a href="">inspect the</a> Deployment and <a href="">realize</a> that there are no ReplicaSet records and pods associated with it.  He gets this information by querying kube-apiserver for label selectors <i>(for more information, see the <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">Kubernetes documentation</a> - <b>approx. Transl.</b> ).</i>  It is interesting to note that this synchronization process knows nothing about the state <i>(is state agnostic)</i> : it checks the new entries in the same way as the existing ones. <br><br>  Having learned that the necessary records do not exist, the controller starts <a href="">the scaling process</a> in order to arrive at the expected state.  This process is accomplished by rolling out (for example, creating) a ReplicaSet resource, assigning it a label selector and assigning the first revision.  PodSpec and other metadata for ReplicaSet are copied from the Deployment manifest.  Sometimes, after this, it may also be necessary to update the Deployment record (for example, if the progress deadline is set; <i>that is, the specification field of <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"><code>.spec.progressDeadlineSeconds</code></a> is <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"><code>.spec.progressDeadlineSeconds</code></a> - <b>approx. Transl.</b></i> ). <br><br>  After that, the status is <a href="">updated</a> and the same reconciliation cycle begins, comparing the Deployment with the desired, completed state.  Since the controller only knows about creating ReplicaSets, the verification phase continues with the next controller responsible for ReplicaSets. <br><br><h3>  ReplicaSets controller </h3><br>  In the previous step, the Deployments controller created the first ReplicaSet for Deployment, but we still do not have any pods.  This is where the ReplicaSets controller comes to the rescue.  His task is to follow the life cycle of ReplicaSets and dependent resources (hearths).  Like most other controllers, this is due to the event handlers for triggers of certain events. <br><br>  The event in which we are interested is creation.  When a ReplicaSet is created (as a result of the activity of the Deployments controller), the RS controller <a href="">inspects the</a> state of the new ReplicaSet and understands that there is a difference between what exists and what is required.  Therefore, he corrects the state, <a href="">rolling out the</a> hearth, which will belong to the ReplicaSet.  The process of creating them occurs neatly, corresponding to the number of bursts of ReplicaSet (inherited from the parent Deployment). <br><br>  Create operations for pods are also performed in batches, starting with <code>SlowStartInitialBatchSize</code> and <code>SlowStartInitialBatchSize</code> this value for each successful iteration of the ‚Äúslow start‚Äù operation.  This approach is designed to reduce the risk of kube-apiserver being thrown by unnecessary HTTP requests in the event of frequent errors loading podov (for example, due to quotas on resources).  If we fail, then let it happen with minimal impact on the rest of the system. <br><br>  Kubernetes implements a hierarchy of objects through references to the owner, Owner References (a field in the child resource referencing the parent ID).  This not only ensures that the garbage collector will find all child resources when deleting a resource managed by the controller (cascade deletion), but also provides an effective way for parent resources not to fight for their children (imagine a scenario in which two potential parents think they own by the same child). <br><br>  Another advantage of the Owner References architecture is that it‚Äôs stateful: if any controller needs to reboot, its idle will not affect other parts of the system, since the topology of the resources does not depend on the controller.  The isolation orientation has penetrated into the architecture of the controllers themselves: they should not work with resources that they do not possess explicitly.  On the contrary, controllers must be selective in their claims to own resources that are <i>non-interfering</i> and <i>non-sharing</i> . <br><br>  But back to the links to the owner.  Sometimes ‚Äúorphaned‚Äù resources appear in the system - this is usually due to the fact that: <br><br><ol><li>  the parent is removed, but not his children; </li><li>  garbage collection policies prohibit child removal. </li></ol><br>  When this happens, the controllers verify that the orphans have been adopted by the new parent.  Many parents can claim a child, but only one of them will succeed (the rest will receive a validation error). <br><br><h3>  Informants </h3><br>  As you can see, for the operation of controllers such as the RBAC authorizer or the Deployments controller, you need to get the status of the cluster.  Returning to the example of the RBAC authorizer, we know that when the request comes, the authenticator will save the initial view of the user's state for future use.  Later, the authorizer will use it to retrieve all roles and role bindings associated with the user from etcd.  How should controllers gain access to reading and modifying such resources?  It turns out that this is a common usage scenario and is solved in Kubernetes with the help of informers. <br><br>  The informer is a pattern that allows controllers to subscribe to events from the repository and receive a list of resources in which they are interested.  In addition to providing an easy-to-use abstraction, it also implements many basic mechanisms, such as caching (it is important because it reduces the number of connections to kube-apiserver and the need for repeated serialization on the server and controller side).  In addition, this approach allows controllers to interact with respect to thread safety <i>( <a href="https://en.wikipedia.org/wiki/Thread_safety">thread safety</a> )</i> , without fear of stepping on someone's feet. <br><br>  Read more about how informers work with controllers, read <a href="http://borismattijssen.github.io/articles/kubernetes-informers-controllers-reflectors-stores">this blog post</a> .  <i>( <b>Note</b> : The work of the informants was also described in <a href="https://habrahabr.ru/company/flant/blog/335552/">this translated article</a> from our blog.)</i> <br><br><h3>  Scheduler </h3><br>  After all the controllers have worked, we have Deployment, ReplicaSet and 3 pods stored in etcd and available in kube-apiserver.  Our pods, however, are stuck in the <code>Pending</code> state, because they have not yet been scheduled / assigned to the node.  The <i>scheduler</i> is the last controller that does this. <br><br>  The scheduler runs as a standalone component of the control plane and works like other controllers: it monitors events and tries to bring the state to the desired one.  In this case, it <a href="">selects</a> pods with an empty <code>NodeName</code> field in PodSpec and tries to find a suitable node to which it can be assigned as.  To find a suitable site, a special scheduling algorithm is used.  By default it works like this: <br><br><ol><li>  When the scheduler starts, a chain of default predicates is <a href="">registered</a> .  These predicates are essentially functions that, when they call, <a href="">filter out</a> nodes suitable for placing the hearth.  For example, if explicit requirements for CPU or RAM resources are set in the PodSpec, and the node does not meet these requirements due to lack of resources, this node will not be selected for submission (node ‚Äã‚Äãresource consumption is considered as the total capacity minus the sum of the requested resources of the containers running in this moment). </li><li>  When the appropriate nodes have been selected, <a href="">a</a> set of priority functions is <a href="">launched</a> to rank them by selecting the most appropriate ones.  For example, for the best distribution of the workload through the system, priority is given to nodes that have less than all the requested resources (since this serves as an indicator of the presence of a smaller workload).  As these functions are launched, each node is assigned a numerical rating.  The node with the highest rating is selected for planning (assignment). </li></ol><br>  When the algorithm determines a node, the scheduler <a href="">creates a</a> Binding object, the <code>Name</code> and <code>UID</code> values ‚Äã‚Äãof which correspond to the hearth, and the <code>ObjectReference</code> field contains the name of the selected node.  It is <a href="">sent</a> to the apiserver via a POST request. <br><br>  When kube-apiserver receives the Binding object, the registry deserializes it and updates the following fields in the sub-object: <a href="">sets</a> it to <code>NodeName</code> from <code>ObjectReference</code> , <a href="">adds</a> corresponding annotations <i>(annotations)</i> , <a href="">sets the</a> status to <code>PodScheduled</code> to <code>True</code> . <br><br>  When the scheduler has assigned a sub node, the kubelet located on this sub starts its work. <br><br>  <b>Note on the customization of the scheduler</b> : What is interesting is that both predicates and priority functions are expanded and can be defined with the <code>--policy-config-file</code> flag.  This gives a certain flexibility.  Administrators can also run their schedulers (controllers with arbitrary processing logic) for individual Deployments.  If the PodSpec contains a <code>schedulerName</code> , Kubernetes will transfer the scheduling of this pod to any scheduler registered with the appropriate name. <br><br><h2>  kubelet </h2><br><h3>  Synchronization </h3><br>  Okay, the main controller loop is complete, phew!  Let's recap: an HTTP request went through authentication, authorization, access control;  in etcd resources Deployment, ReplicaSet and three hearths were created;  worked out a set of initializers;  finally, each node was assigned a suitable node.  Until now, however, the state we discussed existed only in etcd.  The following steps include distributing this status across work nodes, which is the main point of the work of a distributed system like Kubernetes.  This happens through a component called kubelet.  Go! <br><br>  Kubelet is an agent that runs on every node in the Kubernetes cluster and, among other things, is responsible for ensuring the life cycle of the hearths.  Thus, it serves the entire logic of interpreting the abstraction of ‚Äúhearth‚Äù (which is essentially just the concept of Kubernetes) into its building blocks ‚Äî containers.  It also handles all the logic associated with mounting volumes, container logging, garbage collection, and many other important things. <br><br>  Kubelet is conveniently presented again as a controller.  He polls pods in kube-apiserver every 20 seconds (this is configured) <i>[about such intervals in Kubernetes was told in <a href="https://habrahabr.ru/company/flant/blog/326062/">this material of</a> our blog - <b>approx.</b></i>  <i><b>trans.</b></i>  <i>]</i> , filtering those from which the <code>NodeName</code> values ‚Äã‚Äãcorrespond to the name of the node where the kubelet is running.  After receiving the list of podov, he compares it with his internal cache, detects new additions and begins to synchronize the state, if differences exist.  Let's see how this synchronization process looks like: <br><br><ol><li>  If a sub is created (our case), kubelet <a href="">registers the</a> initial metric, which is used in Prometheus for tracking delays at the pods. </li><li>  A <code>PodStatus</code> object is <code>PodStatus</code> , representing the state of the current phase.  The hearth phase is the high level designation of the hearth position in its life cycle.  Examples: <code>Pending</code> , <code>Running</code> , <code>Succeeded</code> , <code>Failed</code> and <code>Unknown</code> .  It is not easy to determine, so let's consider what exactly is happening: <br><ul><li>  first, the <code>PodSyncHandlers</code> chain is called <code>PodSyncHandlers</code> .  Each handler checks whether it should stay on the node.  If any of them decides that there is nothing to do here, the hearth phase <a href="">changes</a> to <code>PodFailed</code> , while he himself is removed from the node.  For example, the sub must be removed from the node if the <code>activeDeadlineSeconds</code> value is <code>activeDeadlineSeconds</code> (used during Jobs); </li><li>  then the phase of the flow is determined by the state of its init and real containers.  Since containers in our case have not yet been launched, they are classified as ‚Äúwaiting‚Äù <i>( <a href="">waiting</a> )</i> .  Anyone with a container on hold <a href="">is</a> in the <code>Pending</code> phase; </li><li>  Finally, the condition for the hearth is determined by the condition of its containers.  Since none of our containers have yet been created with an executable environment for containers, the <code>PodReady</code> condition <a href="">will be set</a> to <code>False</code> . </li></ul></li><li>  After the <code>PodStatus</code> created, it will be sent to the pod state manager, which asynchronously updates the etcd entry via apiserver. </li><li>  Next, a set of admission handlers starts, checking that the security rights are valid.  In particular, the AppArmor and <code>NO_NEW_PRIVS</code> profiles <a href="">are used</a> .  Pods that were rejected at this stage will remain in the <code>Pending</code> state for an indefinite time. </li><li>  If the <code>cgroups-per-qos</code> runtime flag is <code>cgroups-per-qos</code> , the kubelet will create cgroups for the submission and apply the parameters to the resources.  This is done to enable the best implementation of Quality of Service (QoS) for hearths. </li><li>  Creates directories with data hearths.  These include <code>/var/run/kubelet/pods/&lt;podID&gt;</code> (usually <code>/var/run/kubelet/pods/&lt;podID&gt;</code> ), its volumes ( <code>&lt;podDir&gt;/volumes</code> ), and plug-ins ( <code>&lt;podDir&gt;/plugins</code> ). </li><li>  The volume manager will <a href="">connect</a> all the necessary volumes defined in <code>Spec.Volumes</code> and wait for them.  Some sites may take longer depending on the type of volumes being mounted (for example, cloud or NFS volumes). </li><li>  All secrets specified in Spec.ImagePullSecrets <a href="">will be obtained</a> from <code>Spec.ImagePullSecrets</code> so that they can be further inserted into the container. </li><li>  Then the container execution environment will launch the container (described in more detail below). </li></ol><br><h3>  CRI and pause containers </h3><br>  Now we are at the stage where the main preparatory part is completed and the container is ready for launch.  The software that performs this run is called the <i>Container Runtime</i> ‚Äî for example, <code>docker</code> or <code>rkt</code> . <br><br>  The desire to become more extensible led kubelet to the fact that since version 1.5.0 it uses a concept called CRI (Container Runtime Interface) to interact with specific executable container environments.  In short, CRI offers an abstraction between the kubelet and the concrete implementation of the executable environment.  The interaction takes place via <a href="https://github.com/google/protobuf">Protocol Buffers</a> (something like faster JSON) and <a href="https://grpc.io/">gRPC API</a> (an API type well suited for performing operations in Kubernetes).  This is a very cool idea, because when using the agreed agreement between the kubelet and the executable environment, the actual implementation details of how containers are orchestrated largely lose their significance.  Only this convention matters.  This approach allows you to add new executable environments with minimal overhead, since the basic Kubernetes code does not need to be changed. <br><br>  <i>( <b>Note</b> : For more information about the CRI interface in Kubernetes and its implementation, CRI-O we wrote in <a href="https://habrahabr.ru/company/flant/blog/340010/">this article</a> .)</i> <br><br>  Enough of lyrical digressions - let's return to the container deployment ... When it starts for the first time, kubelet <a href="">makes a</a> remote procedure call (RPC) <code>RunPodSandbox</code> .  The word ‚Äúsandbox‚Äù in its name is a CRI term that describes a set of containers, which in Kubernetes means, you guessed it, under.  This term is deliberately very broad, so as not to lose its meaning for other executable environments that can actually use non-containers (imagine an executable environment based on a hypervisor, where the sandbox is a virtual machine). <br><br>  In our case, we use Docker.  In this executable environment, creating a sandbox involves creating a ‚Äúpause‚Äù container.  The pause container acts as a parent for all other containers in the hearth, hosting many of the hearth resources that will be used by the loaded containers.  These ‚Äúresources‚Äù are Linux namespaces (IPC, network, PID).  If you are not familiar with how containers work in Linux, let's quickly refresh this information.  The Linux kernel has a concept of namespaces <i>(namespaces)</i> that allow the host operating system to take a certain set of resources (for example, CPU or memory) and assign it to processes as if they and only they consume this set of resources.  Cgroups are also important because they are the way Linux manages the allocation of resources (like the cop who controls the use of resources).  Docker uses both of these kernel features to host a process for which resources are guaranteed and isolation is provided.  More information about running Linuxx containers can be found in this wonderful b0rk publication: ‚Äú <a href="https://jvns.ca/blog/2016/10/10/what-even-is-a-container/">What even is a Container?</a>  ". <br><br>  The pause container provides a way to put all these namespaces and allows child containers to share them.  Being part of a single network namespace, containers of the same hearth can access each other through localhost.  The second role of the pause container is related to how the PID namespaces work.  In spaces of this type of name, the processes form a hierarchical tree, and the top process ‚Äúinit‚Äù takes responsibility for ‚Äúextracting‚Äù dead processes <i>( <b>note</b> deleting their records from the operating system's process table - <b>approx. Transl.</b> )</i> .  Details on how this works can be found in <a href="https://www.ianlewis.org/en/almighty-pause-container">this excellent article</a> .  After the pause-container has been created, a <i>checkpoint</i> on the disk is made for it and it starts. <br><br><h3>  CNI and network </h3><br>  Our hearth had a skeleton: a pause-container, which sheltered all the namespaces to allow interaction within the hearth.  But how does the network work and is configured? <br><br>  When kubelet sets up the network for the pod, it delegates this task to the CNI plugin.  CNI stands for Container Network Interface and works on a principle similar to the Container Runtime Interface.  , CNI ‚Äî ,          .  ,  kubelet       JSON (    <code>/etc/cni/net.d</code> ),     CNI (  <code>/opt/cni/bin</code> )  stdin.     JSON: <br><br><pre> <code class="json hljs">{ <span class="hljs-attr"><span class="hljs-attr">"cniVersion"</span></span>: <span class="hljs-string"><span class="hljs-string">"0.3.1"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"name"</span></span>: <span class="hljs-string"><span class="hljs-string">"bridge"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"type"</span></span>: <span class="hljs-string"><span class="hljs-string">"bridge"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"bridge"</span></span>: <span class="hljs-string"><span class="hljs-string">"cnio0"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"isGateway"</span></span>: <span class="hljs-literal"><span class="hljs-literal">true</span></span>, <span class="hljs-attr"><span class="hljs-attr">"ipMasq"</span></span>: <span class="hljs-literal"><span class="hljs-literal">true</span></span>, <span class="hljs-attr"><span class="hljs-attr">"ipam"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"type"</span></span>: <span class="hljs-string"><span class="hljs-string">"host-local"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"ranges"</span></span>: [ [{<span class="hljs-attr"><span class="hljs-attr">"subnet"</span></span>: <span class="hljs-string"><span class="hljs-string">"${POD_CIDR}"</span></span>}] ], <span class="hljs-attr"><span class="hljs-attr">"routes"</span></span>: [{<span class="hljs-attr"><span class="hljs-attr">"dst"</span></span>: <span class="hljs-string"><span class="hljs-string">"0.0.0.0/0"</span></span>}] } }</code> </pre> <br>     ‚Äî ,     ‚Äî     <code>CNI_ARGS</code> . <br><br>      CNI ‚Äî      <code>bridge</code> : <br><br><ol><li>      Linux-          . </li><li>    (  veth-)     pause-,       .  veth-       :     ,   ‚Äî     ,      . </li><li>   pause-  IP   .      IP-.  IP  IPAM-,   JSON-. <br><ul><li>  IPAM     :         .    IP/  , ,       .   IPAM-  <code>host-local</code>   IP-     .        ,     IP-   . </li></ul></li><li>   DNS kubelet   IP- DNS-  CNI,       <code>resolv.conf</code>  . </li></ol><br>    ,    kubelet    JSON,    . <br><br> <i>( <b>. .</b> :  CNI     <a href="https://habrahabr.ru/company/flant/blog/329830/"> </a> .)</i> <br><br><h3>    </h3><br>  ,     ,    ? ,  ,        . <br><br>            <i>(overlay networking)</i> ,        .        Flannel.      ‚Äî  L3 IPv4-   . Flannel  ,      ( ,   CNI),      .            etcd.            UDP-,      .      <a href="https://github.com/coreos/flannel"> CoreOS</a> . <br><br><h3>   </h3><br>       .   ?      . <br><br>       , kubelet      .    <a href=""></a> init-,   PodSpec,   ‚Äî .  : <br><br><ol><li> <a href=""></a>   .     ,   PodSpec. </li><li>  CRI <a href=""></a> .     <code>ContainerConfig</code> (   , , , , ,    ..)    PodSpec    protobufs   CRI.  Docker     Daemon API  payload    .        (,  ,   , ID ). </li><li>      CPU Manager ‚Äî   ,    1.8   alpha     CPU      <code>UpdateContainerResources</code>  CRI. </li><li>    <a href=""></a> . </li><li>   -     <i>(post-start)</i> ,  <a href=""></a> .     <code>Exec</code> (    )  <code>HTTP</code> ( HTTP-  endpoint ).     ,     ,       <code>Running</code> . </li></ol><br><h2>  Results </h2><br>  Okay.  Is done.  The end. <br><br>        3 ,       . ,      kubelet       CRI. <br><br><h2>  PS from translator </h2><br>  Read also in our blog: <br><br><ul><li> ¬´ <a href="https://habrahabr.ru/company/flant/blog/342658/"><b>   Kubernetes   kubectl run?  1</b></a> ¬ª; </li><li>  " <a href="https://habrahabr.ru/company/flant/blog/331188/">Our experience with Kubernetes in small projects</a> " <i>(video of the report, which includes an introduction to the technical device Kubernetes);</i> </li><li>  ‚Äú <a href="https://habrahabr.ru/company/flant/blog/335552/">How does the Kubernetes scheduler actually work?</a>  "; </li><li>  " <a href="https://habrahabr.ru/company/flant/blog/341760/">Infrastructure with Kubernetes as an affordable service</a> "; </li><li> ¬´ <a href="https://habrahabr.ru/company/flant/blog/340010/">CRI-O ‚Äî  Docker     Kubernetes</a> ¬ª; </li><li>  ‚Äú <a href="https://habrahabr.ru/company/flant/blog/329830/">Container Networking Interface (CNI) - network interface and standard for Linux containers</a> ‚Äù; </li><li> ¬´ <a href="https://habrahabr.ru/company/flant/blog/338230/">Kubernetes 1.8:   </a> ¬ª. </li></ul></div><p>Source: <a href="https://habr.com/ru/post/342822/">https://habr.com/ru/post/342822/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../342810/index.html">Cutting hippo. Again about design and refactoring from Sandy Metz</a></li>
<li><a href="../342812/index.html">Selection of useful materials: Microservices on .NET Core</a></li>
<li><a href="../342814/index.html">Key Distribution Protocols on Symmetric Ciphers</a></li>
<li><a href="../342818/index.html">Use DevOps to turn IT into a strategic weapon.</a></li>
<li><a href="../342820/index.html">How we built the software-defined data center in the drawer</a></li>
<li><a href="../342824/index.html">Log4j2 log aggregation by ELK</a></li>
<li><a href="../342826/index.html">Linux operating systems for different tasks</a></li>
<li><a href="../342830/index.html">How the leaders of the storage industry gathered at Prostor in Skolkovo (photo report)</a></li>
<li><a href="../342832/index.html">Avito iOS Meetup: Winter Edition</a></li>
<li><a href="../342834/index.html">PKCS # 11 Cryptographic Tokens: Managing and Accessing Token Objects (Continued)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>