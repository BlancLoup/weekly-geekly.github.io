<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Thematic modeling with BigARTM</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Introduction 
 Drew attention to the translation of the publication titled "Thematic modeling of repositories on GitHub" [1]. The publication has a lo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Thematic modeling with BigARTM</h1><div class="post__text post__text-html js-mediator-article"><h3>  Introduction </h3><br>  Drew attention to the translation of the publication titled "Thematic modeling of repositories on GitHub" [1].  The publication has a lot of theoretical data and very well describes topics, concepts, the use of natural languages ‚Äã‚Äãand many other applications of the BigARTM model. <br><br>  However, a regular user without knowledge in the field of thematic modeling for practical use is sufficiently knowledge of the interface and a clear sequence of actions when preparing text source data. This publication is dedicated to developing software for preparing text data and choosing the development environment. <br><a name="habracut"></a><br><h3>  Installing BigARTM on Windows and preparing source data </h3><br>  The installation of BigARTM is well described in the video of the presentation [2], so I will not dwell on it, I‚Äôll note that the programs listed in the documentation are designed for a specific version and may not work on the downloaded version.  The article uses version_v 0.8.1. <br><br>  The BigARTM program works only on Python 2.7.  Therefore, to create a single software package, all auxiliary programs and examples are written in Python 2.7, which led to some complication of the code. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Text data for thematic modeling should be processed in accordance with the following steps [4]. <br><br><ol><li>  Lemmatization or stemming; </li><li>  Delete stop words and words that are too rare; </li><li>  Selection of terms and phrases. </li></ol><br>  Consider how you can implement these requirements in Python. <br><br><h3>  What is better to apply: lemmatization or stemming? </h3><br>  The answer to this question will be obtained from the following listing, in which the first paragraph of the text from the article [5] is used as an example.  Hereinafter, parts of the listing and the result of their work will be presented as they are displayed in the jupyter notebook environment format. <br><br><div class="spoiler">  <b class="spoiler_title">Lemma listing with lemmatize</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># In[1]: #!/usr/bin/env python # coding: utf-8 # In[2]: text=u'        \         \          \         ' # In[3]: import time start = time.time() import pymystem3 mystem = pymystem3 . Mystem ( ) z=text.split() lem="" for i in range(0,len(z)): lem =lem + " "+ mystem. lemmatize (z[i])[0] stop = time.time() print u",  lemmatize- %f   %i  "%(stop - start,len(z))</span></span></code> </pre> <br></div></div><br>  Result robots listing on lemmatization. <br><blockquote>  <i>for practice, very often there is a problem for solving which method of optimization is used in ordinary life with multiple choices, for example, a gift for the new year, we intuitively solve the problem of minimal expenditure while setting quality of purchase</i> </blockquote><br>  Time taken lemmatize - 56.763000 to process 33 words <br><br><div class="spoiler">  <b class="spoiler_title">Listing stemming with stemmer NLTK</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#In [4]: start = time.time() import nltk from nltk.stem import SnowballStemmer stemmer = SnowballStemmer('russian') stem=[stemmer.stem(w) for w in text.split()] stem= ' '.join(stem) stop = time.time() print u",  stemmer NLTK- %f   %i  "%(stop - start,len(z))</span></span></code> </pre><br></div></div><br>  Result robots listing: <br><blockquote>  <i>There are often problems in practice, for which the optimization method in ordinary life is solved with a multitude of choices, for example, for the new year, we intuitively solve the problems of minimal costs while setting the quality of purchases</i> </blockquote><br>  Time spent stemmer NLTK- 0.627000 for processing 33 words <br><br><div class="spoiler">  <b class="spoiler_title">Listing stemming with Stemmer module</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#In [5]: start = time.time() from Stemmer import Stemmer stemmer = Stemmer('russian') text = ' '.join( stemmer.stemWords( text.split() ) ) stop = time.time() print u",  Stemmer- %f   %i "%(stop - start,len(z))</span></span></code> </pre><br></div></div><br>  Result robots listing: <br><blockquote>  <i>There are often problems in practice, for which the optimization method in ordinary life is solved with a multitude of choices, for example, for the new year, we intuitively solve the problems of minimal costs while setting the quality of purchases</i> </blockquote><br>  Time spent by Stemmer- 0.093000 on processing 33 words <br><br>  <b>Conclusion</b> <br><br>  When time for data preparation for thematic modeling is not critical, lemmatization should be applied using the pymystem3 and mystem modules, otherwise stemming should be applied using the Stemmer module. <br><br><h3>  Where can I get a list of stop words for their subsequent removal? </h3><br>  Stop words by definition are words that do not carry a semantic load.  The list of such words should be made taking into account the specifics of the text, but there should be a basis.  The base can be obtained with the brown case. <br><br><div class="spoiler">  <b class="spoiler_title">Listing of getting stop-words</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#In [6]: import nltk from nltk.corpus import brown stop_words= nltk.corpus.stopwords.words('russian') stop_word=" " for i in stop_words: stop_word= stop_word+" "+i print stop_word</span></span></code> </pre><br></div></div><br>  Result robots listing: <br><blockquote>  <i>and it‚Äôs not about me since I‚Äôm since it‚Äôs all so it‚Äôs but you‚Äôll have it, I‚Äôve never had it from me now because of it, if suddenly or not it was him before you again so you after all there then nothing she can they are there where you need it for we you what they were so that without it, if something like that too, then who will be this one for this one almost my order for her now to be where everyone can ever be at last two on the other, even after over the more the one through these of us about all of them to</i>  <i>akaya isn‚Äôt much three this mine however well it‚Äôs my own before this sometimes a little bit better that it‚Äôsn‚Äôt always like them all between</i> </blockquote><br>  You can also get a list of stop words in the network service [6] for a given text. <br><br>  <b>Conclusion</b> <br><br>  It is rational to first use the basis of stop words, for example, from the brown case, and after analyzing the processing results, modify or supplement the list of stop words. <br><br><h3>  How to distinguish terms from the text and ngram? </h3><br>  In the publication [7] for thematic modeling using the program BigARTM recommend ‚ÄúAfter lemmatization, n-grams can be collected from the collection.  Bigrams can be added to the main dictionary by dividing words with a special character that is not in your data: <br><br><ul><li>  Russian_post; </li><li>  Ukrainian_rod; </li><li>  send_cause </li><li>  Russian_bolnitsa. </li></ul><br>  Here is a listing for selecting from the text bigrams, trigrams, fourgrams, fivegrams. <br>  The listing is adapted to Python 2.7.10 and is configured to highlight bigrams, trigrams, fourgrams, fivegrams from text. The "_" is used as a special character. <br><br><div class="spoiler">  <b class="spoiler_title">Listing of getting bigrams, trigrams, fourgrams, fivegrams</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#In [6]: #!/usr/bin/env python # -*- coding: utf-8 -* from __future__ import unicode_literals import nltk from nltk import word_tokenize from nltk.util import ngrams from collections import Counter text = "          \           \           " #In [7]: token = nltk.word_tokenize(text) bigrams = ngrams(token,2) trigrams = ngrams(token,3) fourgrams = ngrams(token,4) fivegrams = ngrams(token,5) #In [8]: for k1, k2 in Counter(bigrams): print (k1+"_"+k2) #In [9]: for k1, k2,k3 in Counter(trigrams): print (k1+"_"+k2+"_"+k3) #In [10]: for k1, k2,k3,k4 in Counter(fourgrams): print (k1+"_"+k2+"_"+k3+"_"+k4) #In [11]: for k1, k2,k3,k4,k5 in Counter(fivegrams): print (k1+"_"+k2+"_"+k3+"_"+k4+"_"+k5)</span></span></code> </pre><br></div></div><br>  Result robots listing.  To shorten, I quote only one value from each ngram. <br><br>  bigrams - new_year <br>  trigrams - given_quality_purchases <br>  fourgrams - which_is used_of_optimization <br>  fivegrams- costs_in_already_qualified_qualities_ <br><br>  <b>Conclusion</b> <br><br>  This program can be used to highlight consistently repeated in the text NGram considering each one word. <br><br><h3>  What should a program for the preparation of textual data for thematic modeling contain? </h3><br>  More often copies of documents are placed one by one in a separate text file.  At the same time, the initial data for thematic modeling is the so-called ‚Äúbag of words‚Äù, in which words related to a particular document begin with a new line after the tag - | text. <br><br>  It should be noted that even with the complete fulfillment of the above requirements, there is a high probability that the most frequently used words do not reflect the content of the document. <br>  Such words can be removed from a copy of the original document.  It is necessary to control the distribution of words in the documents. <br><br>  To speed up the simulation, after each word the colon indicates the frequency of its use in this document. <br><br>  The initial data for testing the program were 10 articles from Wikipedia.  The titles of the articles are as follows. <br><br><ol><li>  Geography </li><li>  Maths </li><li>  Biology </li><li>  Astronomy </li><li>  Physics </li><li>  Chemistry </li><li>  Botany </li><li>  Story </li><li>  Physiology </li><li>  Computer science </li></ol><br><div class="spoiler">  <b class="spoiler_title">Listing of getting ready for modeling text</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#In [12]: #!/usr/bin/env python # -*- coding: utf-8 -*- import matplotlib.pyplot as plt import codecs import os import nltk import numpy as np from nltk.corpus import brown stop_words= nltk.corpus.stopwords.words('russian') import pymystem3 mystem = pymystem3.Mystem() path='Texts_habrahabr' f=open('habrahabr.txt','a') x=[];y=[]; s=[] for i in range(1,len(os.listdir(path))+1): #      i filename=path+'/'+str(i)+".txt" text=" " with codecs.open(filename, encoding = 'UTF-8') as file_object:#     i-  for line in file_object: if len(line)!=0: text=text+" "+line word=nltk.word_tokenize(text)#   i-  word_ws=[w.lower() for w in word if w.isalpha() ]#    word_w=[w for w in word_ws if w not in stop_words ]#  lem = mystem . lemmatize ((" ").join(word_w))#  i -  lema=[w for w in lem if w.isalpha() and len(w)&gt;1] freq=nltk.FreqDist(lema)#    i -    z=[]#      z=[(key+":"+str(val)) for key,val in freq.items() if val&gt;1] #    :   f.write("|text" +" "+(" ").join(z).encode('utf-8')+'\n')#       |text c=[];d=[] for key,val in freq.items():#        i -  if val&gt;1: c.append(val); d.append(key) a=[];b=[] for k in np.arange(0,len(c),1):#      i -  ind=c.index(max(c)); a.append(c[ind]) b.append(d[ind]); del c[ind]; del d[ind] x.append(i)#   y.append(len(a))#     a=a[0:10];b=b[0:10]# TOP-10   a   b  i -  y_pos = np.arange(1,len(a)+1,1)# TOP-10  performance =a plt.barh(y_pos, a) plt.yticks(y_pos, b) plt.xlabel(u' ') plt.title(u'    ‚Ññ %i'%i, size=12) plt.grid(True) plt.show() plt.title(u'   ', size=12) plt.xlabel(u' ', size=12) plt.ylabel(u' ', size=12) plt.bar(x,y, 1) plt.grid(True) plt.show() f.close()</span></span></code> </pre><br></div></div><br>  The result is a listing of generating robots of auxiliary diagrams. To shorten, I quote only one diagram for TOP-10 words from one document and one diagram for the distribution of words over documents. <br><br><img src="https://habrastorage.org/web/dda/a13/b25/ddaa13b25681495193f21e80a4bddb8d.PNG"><br><br><img src="https://habrastorage.org/web/2b2/09a/c17/2b209ac170874f48afc490970d8eeb6d.PNG"><br><br>  As a result of the program, we received ten diagrams which selected 10 words by frequency of use. In addition, the program builds a diagram of the distribution of the number of words over documents.  This is convenient for preliminary analysis of the source data. With a large number of documents, frequency diagrams can be saved in a separate folder. <br><br>  The result of the robots of the listing on the generation of the ‚Äúbag of words‚Äù. To reduce the results, I cite the data from the created text file habrahabr.txt only according to the first document. <br><blockquote>  | text of the earth: 3 range: 2 country: 4 perekipely: 2 people: 2 tradition: 2 structure: 2 appearance: 2 some: 2 name: 2 first: 4 create: 2 find: 2 Greek: 3 to have: 4 form: 2 ii: 2 inhabited: 4 contain: 3 river: 4 east: 2 sea: 6 place: 2 eratosthenes: 3 information: 2 view: 3 herodot: 3 sense: 4 cartography: 2 famous: 2 whole: 2 imagine: 2 quite a few: 2 science: 4 modern: 2 achievement: 2 period: 2 ball: 3 definition: 2 assumption: 2 to lay: 2 presentation: 7 to compose: 3 to depict: 2 straitometer: 3 term: 2 round: 7 to use: 2 coast: 2 south : 2 coordinate: 2 land: 16 to dedicate: 2 reach: 2 map: 7  discipline: 2 meridian: 2 disk: 2 Aristotle: 4 due: 2 description: 6 separate: 2 geographical: 12 it: 2 surround: 3 anaximander: 2 name: 8 that: 2 author: 2 composition: 3 ancient: 8 late: 4 experience: 2 ptolemy: 2 geography: 10 time: 3 work: 2 also: 6 detour: 3 your: 2 approach: 2 circle: 2 wash: 3 inland: 2 Greeks: 2 china: 2 century: 6 her: 2 ocean : 3 north: 2 side: 2 era: 3 inner: 2 flat: 2 red: 2 arrianin: 2 which: 8 other: 2 to use: 3 this: 5 base: 3 live: 2 </blockquote><br>  A single textual modality indicated at the beginning of each document as | text was used.  After each word, the number of its use in the text is entered through the colon.  The latter speeds up both the process of creating a batch and filling out the dictionary. <br><br><h3>  How can I simplify the work with BigARTM to create and analyze a topic? </h3><br>  To do this, you first need to prepare text documents and analyze them using prepositional software solutions, and secondly, use the development environment jupyter notebook. <br><br><img src="https://habrastorage.org/web/133/467/979/133467979d964f95ab68d322555432d0.PNG"><br><br>  The notebooks directory contains all the necessary folders and files for the program. <br>  Parts of the program code are debugged in separate files and after debugging are collected in a common file. <br><br>  The proposed preparation of text documents allows for thematic modeling on a simplified version of BigARTM without regularizers and filters. <br><br><div class="spoiler">  <b class="spoiler_title">Listing to create batch</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#In [1]: #!/usr/bin/env python # -*- coding: utf-8 -* import artm #     batch batch_vectorizer = artm.BatchVectorizer(data_path='habrahabr.txt',#   " " data_format='vowpal_wabbit',#   target_folder='habrahabr', #     batch batch_size=10)#     batch</span></span></code> </pre> <br></div></div><br><br>  From the file habrahabr.txt, the program in the habrahab folder creates one batch from ten documents, the number of which is given in the variable batch_size = 10.  If the data does not change and the frequency matrix has already been created, then the above part of the program can be skipped. <br><br><div class="spoiler">  <b class="spoiler_title">Listing to populate the dictionary and create a model</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#In [2]: batch_vectorizer = artm.BatchVectorizer(data_path='habrahabr',data_format='batches') dictionary = artm.Dictionary(data_path='habrahabr')#     model = artm.ARTM(num_topics=10, num_document_passes=10,#10    dictionary=dictionary, scores=[artm.TopTokensScore(name='top_tokens_score')]) model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=10)#10    top_tokens = model.score_tracker['top_tokens_score']</span></span></code> </pre><br></div></div><br>  The program BigARTM after downloading data to the dictionary generates 10 topics (by the number of documents), the number of which is given in the variable num_topics = 10. The number of passes through the document and the collection are specified in the variables num_document_passes = 10, num_collection_passes = 10. <br><br><div class="spoiler">  <b class="spoiler_title">Listing to create and analyze topics</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#In [3]: for topic_name in model.topic_names: print (topic_name) for (token, weight) in zip(top_tokens.last_tokens[topic_name], top_tokens.last_weights[topic_name]): print token, '-', round(weight,3)</span></span></code> </pre><br></div></div><br>  Result robots program BigARTM: <br><blockquote>  topic_0 <br>  plant - 0.088 <br>  botany - 0.032 <br>  century - 0.022 <br>  peace - 0.022 <br>  Linney - 0.022 <br>  year - 0.019 <br>  which is 0.019 <br>  development - 0.019 <br>  Aristotle - 0.019 <br>  nature - 0.019 <br>  topic_1 <br>  astronomy - 0.064 <br>  heavenly - 0.051 <br>  body - 0.046 <br>  task - 0.022 <br>  movement - 0.018 <br>  to study - 0.016 <br>  method - 0.015 <br>  star - 0.015 <br>  system - 0.015 <br>  which is 0.014 <br>  topic_2 <br>  land - 0.049 <br>  geographical - 0.037 <br>  geography - 0.031 <br>  ancient - 0.025 <br>  which is 0.025 <br>  name - 0.025 <br>  performance - 0.022 <br>  round - 0.022 <br>  card - 0.022 <br>  also - 0.019 <br>  topic_3 <br>  physics - 0.037 <br>  physical - 0.036 <br>  phenomenon - 0.027 <br>  theory - 0.022 <br>  which is 0.022 <br>  law - 0.022 <br>  total - 0.019 <br>  new - 0.017 <br>  base - 0.017 <br>  science - 0.017 <br>  topic_4 <br>  to study - 0.071 <br>  total - 0.068 <br>  section - 0.065 <br>  theoretical - 0.062 <br>  substance - 0.047 <br>  visible - 0.047 <br>  physical - 0.044 <br>  movement - 0.035 <br>  hypothesis - 0.034 <br>  pattern - 0.031 <br>  topic_5 <br>  physiology - 0.069 <br>  thyroid - 0.037 <br>  people - 0.034 <br>  organism - 0.032 <br>  Lats - 0.03 <br>  artery - 0.025 <br>  iron - 0.023 <br>  cell - 0.021 <br>  study - 0.021 <br>  livelihoods - 0.018 <br>  topic_6 <br>  mathematics - 0.038 <br>  cell - 0.022 <br>  science - 0.021 <br>  organism - 0.02 <br>  total - 0.02 <br>  which is 0.018 <br>  mathematical - 0.017 <br>  live - 0.017 <br>  object - 0.016 <br>  gene - 0.015 <br>  topic_7 <br>  history - 0.079 <br>  historical - 0.041 <br>  word - 0.033 <br>  event - 0.03 <br>  science - 0.023 <br>  which is 0.023 <br>  source - 0.018 <br>  historiography - 0.018 <br>  research - 0.015 <br>  philosophy - 0.015 <br>  topic_8 <br>  term - 0.055 <br>  informatics - 0.05 <br>  scientific - 0.031 <br>  language - 0.029 <br>  year - 0.029 <br>  science - 0.024 <br>  Information - 0.022 <br>  computational - 0.017 <br>  name - 0.017 <br>  science - 0.014 <br>  topic_9 <br>  century - 0.022 <br>  which is 0.022 <br>  science - 0.019 <br>  chemical - 0.019 <br>  substance - 0.019 <br>  chemistry - 0.019 <br>  also - 0.017 <br>  development - 0.017 <br>  time - 0.017 <br>  item - 0.017 </blockquote><br>  The results obtained generally correspond to the topics and the simulation result can be considered satisfactory.  If necessary, regularizers and filters can be added to the program. <br><br><h3>  Conclusions on the results of work </h3><br>  We considered all stages of the preparation of text documents for thematic modeling.  For specific examples, a simple comparative analysis of modules for lemmatization and stemming was carried out.  We considered the possibility of using NLTK to get a list of stop words and search for phrases for the Russian language.  We consider the listings written in Python 2.7.10 and adapted for the Russian language, which allows them to be integrated into a single progam complex.  An example of thematic modeling in the jupyter-notebook environment, which provides additional opportunities for working with BigARTM, is analyzed. <br><br><div class="spoiler">  <b class="spoiler_title">Used links</b> <div class="spoiler_text">  1. <a href="https://habrahabr.ru/post/312596">Thematic modeling of repositories on GitHub</a> . <br>  2. <a href="https://www.coursera.org/learn/unsupervised-%2520learning/lecture/qmsFm/ustanovka-bigartm-v-windows">Lecture 49 - Install BigARTM in Windows</a> <br>  3. <a href="https://github.com/bigartm/bigartm/releases">bigartm / bigartm</a> <br>  4. <a href="http://docplayer.ru/40376022-Osnovy-obrabotki-tekstov.html">Basics of word processing.</a> <br>  5. <a href="https://habrahabr.ru/post/330648/">Solving linear programming problems using Python.</a> <br>  6. We are <a href="https://glvrd.ru/">testing a new verification algorithm.</a>  <a href="https://glvrd.ru/">Questions and suggestions.</a> <br>  7. <a href="http://www.machinelearning.ru/wiki/images/f/f5/MelLain_Python_API_slides.pdf">Using the library for thematic modeling BigARTM.</a> <br></div></div></div><p>Source: <a href="https://habr.com/ru/post/334668/">https://habr.com/ru/post/334668/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../334658/index.html">How to try to make the user comfortable and bungle something in the process.</a></li>
<li><a href="../334660/index.html">Test documentation. Turn tables into trees</a></li>
<li><a href="../334662/index.html">Zimbra - we build communications in the company</a></li>
<li><a href="../334664/index.html">About supporting C # language features in Visual Studio and CodeRush for Roslyn</a></li>
<li><a href="../334666/index.html">Generation of documents. Problems and Solutions</a></li>
<li><a href="../334670/index.html">Math pack for Android - "Micro-Math" - now open source</a></li>
<li><a href="../334672/index.html">JetBrains MPS for those interested # 3</a></li>
<li><a href="../334674/index.html">How not to distract yourself from work. Tips and slak bot</a></li>
<li><a href="../334676/index.html">Leaflet 1.xx vs Openlayers 4.xx Part 2. How maps are drawn</a></li>
<li><a href="../334678/index.html">Writing a bot for Slack in Python</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>