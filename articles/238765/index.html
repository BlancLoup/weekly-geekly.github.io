<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The best publications of social networks</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello. In my free time I do social projects. My friends and I have a sufficient number of ‚Äúpublic‚Äù in different social networks, which allows us to co...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The best publications of social networks</h1><div class="post__text post__text-html js-mediator-article">  Hello.  In my free time I do social projects.  My friends and I have a sufficient number of ‚Äúpublic‚Äù in different social networks, which allows us to conduct various experiments.  There is an acute problem of finding relevant content and news that can be published.  In this regard, the idea came to write a service that will collect posts from the most popular pages and issue them on the specified filter.  For the initial test, I chose the social network VKontakte and Twitter. <br><a name="habracut"></a><br><h4>  Technology </h4><br>  First of all, it was necessary to determine the data warehouse (by the way, now the number of saved records is more than 2 million) and this figure will melt every day.  The requirements were: very frequent insertion of large amounts of data and quick sampling among them. <br><br>  I have already heard about nosql databases and wanted to try them.  I will not describe in the article a comparison of the databases that I conducted (mysql vs sqlite vs mongodb). <br>  I chose <a href="http://ru.wikipedia.org/wiki/Memcached">memcached</a> as caching, later I will explain why and in what cases. <br>  As a data collector, a python daemon was written that simultaneously updates all groups from the database. <br><br><h4>  MongoDB and demon </h4><br>  First of all, I wrote a prototype of the collector of publications from groups.  Saw several problems: <br><ul><li>  Storage capacity </li><li>  API restrictions </li></ul><br>  One publication with all the metadata takes about 5-6KB of data, and in the average group about 20,000-30,000 records, it turns out about 175MB of data per group, and there are a lot of these groups.  Therefore, we had to set a task in filtering uninteresting and advertising publications. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      I didn‚Äôt have to invent too much, I have only 2 ‚Äútables‚Äù: <b>groups</b> and <b>posts</b> , the first one keeps the records of groups that need to be parsed and updated, and the second is the scope of all publications of all groups.  Now it seems to me that this is an unnecessary and even bad decision.  It would be best to create a table for each group, so it will be easier to select and sort records, although the speed even with 2 million is not lost.  But this approach should simplify the overall sample for all groups. <br><br><h4>  API </h4><br>  In cases when you need server processing of some data from the social network of VKontakte, a standalone application is created that can issue a token for any action.  For such cases, I have saved a note with the following address: <br><br> <code><a href="https"></a> oauth.vk.com/authorize?client_id=APP_ID&amp;redirect_uri=https://oauth.vk.com/blank.html&amp;response_type=token&amp;scope=groups,offline,photos,friends,wall <br></code> <br><br>  Instead of <b>APP_ID,</b> insert the identifier of your standalone application.  The generated token allows you to access the specified actions at any time. <br><br>  Algorithm parser this: <br>  We take the group id, in the cycle we get all the publications, at each iteration we filter the ‚Äúbad‚Äù posts, save it to the database. <br>  The main problem is speed.  API vkontakte allows you to perform 3 requests per second.  1 request allows you to get a total of 100 publications - 300 publications per second. <br>  In the case of the parser, this is not so bad: you can ‚Äúmerge‚Äù the group in one minute, but there will be problems with the update.  The more groups - the longer the update will take place and, accordingly, the issue will not be updated so quickly. <br><br>  The solution was to use the execute method, which allows you to collect requests for api in a heap and execute at once.  Thus, in one request I am doing 5 iterations and I get 500 publications - 1500 per second, which gives the group "discharge" in ~ 13 seconds. <br><br>  Here is the file with the code that is transferred to execute: <br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">var</span></span> groupId = -|replace_group_id|; <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> startOffset = |replace_start_offset|; <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> it = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> offset = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> walls = []; <span class="hljs-keyword"><span class="hljs-keyword">while</span></span>(it &lt; <span class="hljs-number"><span class="hljs-number">5</span></span>) { <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> count = <span class="hljs-number"><span class="hljs-number">100</span></span>; offset = startOffset + it * count; walls = walls + [API.wall.get({<span class="hljs-string"><span class="hljs-string">"owner_id"</span></span>: groupId, <span class="hljs-string"><span class="hljs-string">"count"</span></span> : count, <span class="hljs-string"><span class="hljs-string">"offset"</span></span> : offset})]; it = it + <span class="hljs-number"><span class="hljs-number">1</span></span>; } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> { <span class="hljs-string"><span class="hljs-string">"offset"</span></span> : offset, <span class="hljs-string"><span class="hljs-string">"walls"</span></span> : walls };</code> </pre><br><br>  The code is read into memory, replacement of tokens <b>replace_group_id</b> and <b>replace_start_offset is done</b> .  As a result, I get an array of publications, the format of which you can see on the VK API official page <a href="">vk.com/dev/wall.get</a> <br><br>  The next stage is the filter.  I took different groups, looked through the publications and came up with possible screening options.  First of all I decided to delete all publications with links to external pages.  Almost always it is advertising. <br><br><pre> <code class="python hljs">urls1 = re.findall(<span class="hljs-string"><span class="hljs-string">'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'</span></span>, text) urls2 = re.findall(<span class="hljs-string"><span class="hljs-string">ur"[-a-zA-Z0-9@:%._\+~#=]{2,256}\.[az]{2,6}\b([-a-zA-Z0-9@:%_\+.~#?&amp;//=]*)"</span></span>, text) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> urls1 <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> urls2: <span class="hljs-comment"><span class="hljs-comment">#   </span></span></code> </pre><br><br>  Then I decided to completely eliminate repost - this is 99% advertising.  Few people will just repost someone else's page.  Check for repost is very simple: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> item[<span class="hljs-string"><span class="hljs-string">'post_type'</span></span>] == <span class="hljs-string"><span class="hljs-string">'copy'</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">False</span></span></code> </pre> <br><br>  item is another element from the walls collection that the execute method returned. <br><br>  I also noticed that a lot of ancient publications are empty, they have no attachments and the text is empty.  For the filter, it is sufficient to prepend that item ['attachments'] and item ['text'] are empty. <br><br>  And the last filter that I just brought out with time: <br><pre> <code class="python hljs">yearAgo = datetime.datetime.now() - datetime.timedelta(days=<span class="hljs-number"><span class="hljs-number">200</span></span>) createTime = datetime.datetime.fromtimestamp(int(item[<span class="hljs-string"><span class="hljs-string">'date'</span></span>])) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> createTime &lt;= yearAgo <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> attachments <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> len(text) &lt; <span class="hljs-number"><span class="hljs-number">75</span></span>: <span class="hljs-comment"><span class="hljs-comment">#   </span></span></code> </pre> <br><br>  As in the previous paragraph, many old publications were with text (the description of the picture in the attachment), but the pictures themselves are no longer preserved. <br><br>  The next step was to clean up failed publications that simply ‚Äúdid not log in‚Äù: <br><pre> <code class="javascript hljs">db.posts.aggregate( { <span class="hljs-attr"><span class="hljs-attr">$match</span></span> : { <span class="hljs-attr"><span class="hljs-attr">gid</span></span> : GROUP_ID } }, { <span class="hljs-attr"><span class="hljs-attr">$group</span></span> : { <span class="hljs-attr"><span class="hljs-attr">_id</span></span> : <span class="hljs-string"><span class="hljs-string">"$gid"</span></span>, <span class="hljs-attr"><span class="hljs-attr">average</span></span> : {<span class="hljs-attr"><span class="hljs-attr">$avg</span></span> : <span class="hljs-string"><span class="hljs-string">"$likes"</span></span>} } } )</code> </pre><br><br>  This method is performed on the posts table, which has a likes field (the number of likes of the post).  It returns the average of likes for this group. <br>  Now you can simply delete all posts older than 3 days that have less than average likes: <br><pre> <code class="javascript hljs">db.posts.remove( { <span class="hljs-string"><span class="hljs-string">'gid'</span></span> : groupId, <span class="hljs-string"><span class="hljs-string">'created'</span></span> : { <span class="hljs-string"><span class="hljs-string">'$lt'</span></span> : removeTime }, <span class="hljs-string"><span class="hljs-string">'likes'</span></span>: { <span class="hljs-string"><span class="hljs-string">'$lt'</span></span> : avg } } )</code> </pre> <br><br><pre> <code class="python hljs">removeTime = datetime.datetime.now() - datetime.timedelta(days=<span class="hljs-number"><span class="hljs-number">3</span></span>) avg =   ,    ( ).</code> </pre><br><br>  I add the resulting and filtered publication to the database, this is where the parsing ends.  I made the difference between parsing and updating groups only in one point: the update is called exactly 1 time for a group, i.e.  I receive only 500 last records (5 on 100 through execute).  In general, this is quite enough, given that VKontakte imposed a limit on the number of publications: 200 per day. <br><br><h5>  Front-end </h5><br>  I will not paint in great detail, javascript + jquery + isotope + inview + mustache. <br><ul><li>  Isotope is used for modern tile output. </li><li>  Inview makes it easy to respond to events that hit the viewport of a specific element.  (in my case, I memorize viewed publications, and highlight new ones in a special color). </li><li>  Mustache allows you to build dom-objects on the template. </li></ul><br><br><h5>  Filter publications by group </h5><br>  A simple php script was written to output data by groups. <br>  This is an auxiliary function that, by the type of time filter, created an object that can be used directly in the request. <br><pre> <code class="php hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">filterToTime</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">($timeFilter)</span></span></span><span class="hljs-function"> </span></span>{ $mongotime = <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> ($timeFilter == <span class="hljs-string"><span class="hljs-string">'year'</span></span>) $mongotime = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Mongodate(strtotime(<span class="hljs-string"><span class="hljs-string">"-1 year"</span></span>, time())); <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> ($timeFilter == <span class="hljs-string"><span class="hljs-string">'month'</span></span>) $mongotime = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Mongodate(strtotime(<span class="hljs-string"><span class="hljs-string">"-1 month"</span></span>, time())); <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> ($timeFilter == <span class="hljs-string"><span class="hljs-string">'week'</span></span>) $mongotime = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Mongodate(strtotime(<span class="hljs-string"><span class="hljs-string">"-1 week"</span></span>, time())); <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> ($timeFilter == <span class="hljs-string"><span class="hljs-string">'day'</span></span>) $mongotime = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Mongodate(strtotime(<span class="hljs-string"><span class="hljs-string">"midnight"</span></span>)); <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> ($timeFilter == <span class="hljs-string"><span class="hljs-string">'hour'</span></span>) $mongotime = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Mongodate(strtotime(<span class="hljs-string"><span class="hljs-string">"-1 hour"</span></span>)); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> $mongotime; }</code> </pre><br><br>  And the following code already receives 15 best posts for the month: <br><pre> <code class="php hljs">$groupId = <span class="hljs-number"><span class="hljs-number">42</span></span>; <span class="hljs-comment"><span class="hljs-comment">// - id  $mongotime = filterToTime('week'); $offset = 1; //   $findCondition = array('gid' =&gt; $groupId, 'created' =&gt; array('$gt' =&gt; $mongotime)); $mongoHandle-&gt;posts-&gt;find($findCondition)-&gt;limit(15)-&gt;skip($offset * $numPosts);</span></span></code> </pre><br><br><h5>  Page index logic </h5><br>  It is interesting to watch group statistics, but it is much more interesting to build a general rating of absolutely all groups and their publications.  If you think, the task is very difficult: <br>  We can build a rating of only 3 factors: the number of likes, reposts and subscribers.  The more subscribers - the more likes and reposts, but this does not guarantee the quality of the content. <br><br>  Most million-plus groups often publish any garbage that has been surfing the Internet for several years, and among the million subscribers are constantly those who will repost and like. <br>  It is easy to build a rating based on bare numbers, but the result cannot be called a rating of publications in terms of their quality and uniqueness. <br>  There were ideas to derive the quality factor of each group: build a time scale, watch user activity for each time interval, and so on. <br>  Unfortunately, I did not come up with an adequate solution.  If you have any ideas, I will be glad to hear. <br><br>  The first thing I understood was the realization that the contents of the index page need to be calculated and cached for all users, because this is a very slow operation.  This is where memcached comes to the rescue.  For the simplest logic, the following algorithm was chosen: <br><ol><li>  Cycle through all groups </li><li>  We take all the publications of the i-th group and choose 2 of the best ones for a specified period of time. </li></ol><br><br>  As a result, there will be no more than 2 publications from one group.  Of course, this is not the most correct result, but in practice it shows good statistics and the relevance of the content. <br><br>  This is how the code of the stream looks, which generates an index page once every 15 minutes: <br><br><pre> <code class="python hljs"> <span class="hljs-comment"><span class="hljs-comment"># timeDelta -     (hour, day, week, year, alltime) # filterType - likes, reposts, comments # deep - 0, 1, ... () def _get(self, timeDelta, filterTime, filterType='likes', deep = 0): groupList = groups.find({}, {'_id' : 0}) allPosts = [] allGroups = [] for group in groupList: allGroups.append(group) postList = db['posts'].find({'gid' : group['id'], 'created' : {'$gt' : timeDelta}}) \ .sort(filterType, -1).skip(deep * 2).limit(2) for post in postList: allPosts.append(post) result = { 'posts' : allPosts[:50], 'groups' : allGroups } #     timestamp  mongotime,    json dthandler = lambda obj: (time.mktime(obj.timetuple()) if isinstance(obj, datetime.datetime) or isinstance(obj, datetime.date) else None) jsonResult = json.dumps(result, default=dthandler) key = 'index_' +filterTime+ '_' +filterType+ '_' + str(deep) print 'Setting key: ', print key self.memcacheHandle.set(key, jsonResult)</span></span></code> </pre><br><br>  I will describe the filters that affect the issue: <br>  Time: hour, day, week, month, year, all the time <br>  Type: likes, reposts, comments <br><br>  Objects were generated for all points of time. <br><pre> <code class="python hljs"> hourAgo = datetime.datetime.now() - datetime.timedelta(hours=<span class="hljs-number"><span class="hljs-number">3</span></span>) midnight = datetime.datetime.now().replace(hour=<span class="hljs-number"><span class="hljs-number">0</span></span>, minute=<span class="hljs-number"><span class="hljs-number">0</span></span>, second=<span class="hljs-number"><span class="hljs-number">0</span></span>, microsecond=<span class="hljs-number"><span class="hljs-number">0</span></span>) weekAgo = datetime.datetime.now() - datetime.timedelta(weeks=<span class="hljs-number"><span class="hljs-number">1</span></span>) monthAgo = datetime.datetime.now() + dateutil.relativedelta.relativedelta(months=<span class="hljs-number"><span class="hljs-number">-1</span></span>) yearAgo = datetime.datetime.now() + dateutil.relativedelta.relativedelta(years=<span class="hljs-number"><span class="hljs-number">-1</span></span>) alltimeAgo = datetime.datetime.now() + dateutil.relativedelta.relativedelta(years=<span class="hljs-number"><span class="hljs-number">-10</span></span>)</code> </pre><br><br>  All of them are in turn passed to the <b>_get</b> function along with various filter variations by type (likes, reposts, comments).  To all this, you need to generate 5 pages for each variation of filters.  As a result, the following keys are put into memcached: <br><br><blockquote>  Setting key: index_hour_likes_0 <br>  Setting key: index_hour_reposts_0 <br>  Setting key: index_hour_comments_0 <br>  Setting key: index_hour_common_0 <br>  Setting key: index_hour_likes_1 <br>  Setting key: index_hour_reposts_1 <br>  Setting key: index_hour_comments_1 <br>  Setting key: index_hour_common_1 <br>  Setting key: index_hour_likes_2 <br>  Setting key: index_hour_reposts_2 <br>  Setting key: index_hour_comments_2 <br>  Setting key: index_hour_common_2 <br>  Setting key: index_hour_likes_3 <br>  Setting key: index_hour_reposts_3 <br>  Setting key: index_hour_comments_3 <br>  Setting key: index_hour_common_3 <br>  Setting key: index_hour_likes_4 <br>  Setting key: index_hour_reposts_4 <br>  Setting key: index_hour_comments_4 <br>  Setting key: index_hour_common_4 <br>  Setting key: index_day_likes_0 <br>  Setting key: index_day_reposts_0 <br>  Setting key: index_day_comments_0 <br>  Setting key: index_day_common_0 <br>  Setting key: index_day_likes_1 <br>  Setting key: index_day_reposts_1 <br>  Setting key: index_day_comments_1 <br>  Setting key: index_day_common_1 <br>  Setting key: index_day_likes_2 <br>  Setting key: index_day_reposts_2 <br>  Setting key: index_day_comments_2 <br>  Setting key: index_day_common_2 <br>  Setting key: index_day_likes_3 <br>  Setting key: index_day_reposts_3 <br>  ... </blockquote><br><br>  And on the client side, only the necessary key is generated and the json string is pulled out of memcached. <br><br><h5>  Twitter </h5><br>  The next interesting task was to generate popular tweets across the CIS countries.  The task is also not easy, I would like to receive relevant and not ‚Äútrash‚Äù information.  I was very surprised at the limitations of Twitter: it would not be so easy to take and merge all the tweets of certain users.  The API greatly limits the number of requests, so you can‚Äôt do it the way it does it: make a list of popular accounts and constantly parse their tweets. <br><br>  A day later, a solution came: we create an account on Twitter, subscribe to all the important people whose topics of publications are of interest to us.  The trick is that in almost 80% of cases, one of these people retweets some popular tweet.  Those.  we do not need to have a list of all accounts in the database, just dial a database of 500-600 active people who are constantly in trend and retweet real interesting and popular tweets. <br>  In the Twitter API, there is a method that allows you to receive a user's tape, which includes tweets of those to whom we are following and their reposts.  All we need now is to read our tape to the maximum once every 10 minutes and save the tweets, filters and everything else we do in the same way as in the case of VKontakte. <br><br>  So, another thread was written inside the daemon, which once in 10 minutes ran such code: <br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.twitter = Twython(APP_KEY, APP_SECRET, TOKEN, TOKEN_SECRET) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">logic</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> lastTweetId = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(<span class="hljs-number"><span class="hljs-number">15</span></span>): <span class="hljs-comment"><span class="hljs-comment">#     self.getLimits() tweetList = [] if i == 0: tweetList = self.twitter.get_home_timeline(count=200) else: tweetList = self.twitter.get_home_timeline(count=200, max_id=lastTweetId) if len(tweetList) &lt;= 1: print '1 tweet, breaking' # ,   API    break # ... lastTweetId = tweetList[len(tweetList)-1]['id']</span></span></code> </pre><br><br>  Well, then the usual and boring code: we have tweetList, loop and process each tweet.  List of fields in the official documentation.  The only thing I want to emphasize: <br><br><pre> <code class="python hljs"> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> tweet <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tweetList: localData = <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-string"><span class="hljs-string">'retweeted_status'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tweet: localData = tweet[<span class="hljs-string"><span class="hljs-string">'retweeted_status'</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: localData = tweet</code> </pre><br><br>  In the case of retweet, we need to save not the tweet of one of our subscribers, but the original one.  If the current record is retweet, then it contains inside the key 'retweeted_status' exactly the same tweet object, only the original one. <br><br><h5>  The final </h5><br>  There are problems with website design and layout (I myself have never been a web programmer), but I hope someone will find useful information that I have described.  I myself have been working with social services for a long time.  networks and their API and know a lot of tricks.  If someone has any questions, I will be happy to help. <br><br>  Well, a few pictures: <br><br><h6>  Index page: </h6><br><img src="//habrastorage.org/files/7ee/364/a7f/7ee364a7f2b746de9ffbadce9527fdef.png"><br><br><h6>  A page of one of the groups that I constantly monitor: </h6><br><img src="//habrastorage.org/files/3ce/d43/0c7/3ced430c7cf246d190a75304962e0ae0.png"><br><br><h6>  Twitter per day: </h6><br><img src="//habrastorage.org/files/b36/4b1/a07/b364b1a070e848038d6991fd5e007537.png"><br><br>  Thanks for attention. <br> <a href="http://88.198.106.150/"><img src="//habrastorage.org/files/d77/123/359/d77123359c2d4dff965540da9ff059bc.png"></a>  - <a href="http://88.198.106.150/">88.198.106.150</a> </div><p>Source: <a href="https://habr.com/ru/post/238765/">https://habr.com/ru/post/238765/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../238755/index.html">Some interesting and useful things for web developer # 30</a></li>
<li><a href="../238757/index.html">Market Watch for Apple Watch - Pebble Update, Pebble Alliance and Jawbone</a></li>
<li><a href="../238759/index.html">Finishing the genome: fast, high quality, inexpensive</a></li>
<li><a href="../238761/index.html">Lumia SensorCore SDK: new opportunities for developing mobile applications. Part 1: Overview</a></li>
<li><a href="../238763/index.html">Microsoft introduced Windows 10</a></li>
<li><a href="../238771/index.html">‚ÄúBleed the mind. Inexpensive. Is there any sense of a mask for lucid dreams?</a></li>
<li><a href="../238775/index.html">Windows 10 Technical Preview is available for download.</a></li>
<li><a href="../238777/index.html">Epson Contest Reminder for Android Developers</a></li>
<li><a href="../238779/index.html">IDE Atom from GitHub. Published roadmap to version 1.0</a></li>
<li><a href="../238783/index.html">What will happen if you mix nuts, Arduino, OpenCV and Delphi. Part 2</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>