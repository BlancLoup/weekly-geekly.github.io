<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>About color blindness and color perception modeling</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="As it turned out in the discussion of the previous post about color blindness, the model of vision with a lack of red and green cones was not too accu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>About color blindness and color perception modeling</h1><div class="post__text post__text-html js-mediator-article">  As it turned out in the discussion of the <a href="http://ymik.habrahabr.ru/blog/58803/">previous post</a> about color blindness, the model of vision with a lack of red and green cones was not too accurate. <br><br>  Over the past three weeks, I have three times rewritten the algorithm that simulates the impairment of color vision, each time checking the results on real deatanope (lack of green cones) and protanopes (lack of red cones).  And now I am ready to share with you the results of my research.  :) <br><br>  And so, the purpose of modeling was actually not even a picture-result, but the creation of an algorithm that allows you to say how much, say, red color is perceived more than blue.  Along the way, it was found that some <a href="http://www.vischeck.com/daltonize/">services</a> that simulate the loss of color vision actually simply offset the ab coefficients in the <a href="http://ru.wikipedia.org/wiki/Lab">CIE Lab model</a> and checking for real color blind people showed that the result they give is far from positive :) 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Because of this, we had to <a href="http://ymik.habrahabr.ru/blog/57681/">return</a> to the theory and carry out detailed work on modeling the work of the retina in the perception of colors. <br><br><a name="habracut"></a><blockquote><h3>  Insert the first.  How color is perceived </h3><br><br>  The sensation of color <a href="http://ymik.habrahabr.ru/blog/58803/">depends</a> on the level of the signal entering the brain from the <b>S</b> , <b>M</b> and <b>L</b> cones (S, M and L are, respectively, ‚Äúgreen‚Äù, ‚Äúred‚Äù and ‚Äúblue (purple)‚Äù cones). <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/585/632/168/5856321688692855d029ae77911ef561.gif" width="424" height="403"><br><br>  <b>Yellow color</b> (hereinafter - <b>YY</b> ) is perceived as the average level of M and L cones <br>  <b>Green is</b> perceived as the signal strength from S cones minus the power of perception of yellow <br>  <b>Red is</b> perceived as green as a signal from M cones minus YY <br><br>  With blue, cyan and violet same flowers trouble.  Let's start with the fact that the maximum perception of L cones falls on purple: <br><img src="http://www.ngi-central.org/ngi_documentation/html/chunk/images/srgb_spectrum.png"><br><img src="http://upload.wikimedia.org/wikipedia/commons/6/65/Cone-response.svg"><br>  For this reason, the actual blue color is perceived by man as the difference between the signals from <b>L</b> cones and the power of perception of yellow. <br><br>  But cyan and magenta are perceived as the average between the perception of blue-green (for azure) and blue-red (for magenta), where blue is <b>L-YY</b> , and green and red, in turn, are <b>S-YY</b> and <b>M -Yy</b> <br><br>  Thus, the reference color that determines all color perception is yellow, which is confirmed in <a href="http://elementy.ru/news/164929">various experiments</a> : <br>  <i>‚ÄúVolunteers were asked to independently adjust the color of the light flux, getting pure yellow light, devoid of red and green impurities.</i> <i><br></i>  <i>As expected, each subject picked up for the ‚Äúperfect yellow‚Äù light with almost the same wavelength.</i>  <i>The surprise was that the number of cones capable of distinguishing yellow from, say, red, was very different for all participants in the experiment, and in polar cases, the spread was forty times.</i>  <i>That is, the perception of color obviously depends not so much on the receptors, as on the brain processing the signals received by the receptor.</i> <i><br><img src="https://habrastorage.org/getpro/geektimes/post_images/9d3/88e/d8d/9d388ed8d926b7d291736cf5fc09878a.jpg"><br></i>  <i>These pictures clearly show the variation in the proportion of cones responsible for the perception of different colors.</i>  <i>Cones that are responsible for the perception of shades of red predominate in the right picture (images of a living retina of two different people on the left and right). ‚Äù</i> <br></blockquote><br><br>  The confusion in determining color adds two more factors: <br><br>  <b>The first factor.</b>  The ability of the brain to adjust its sense of yellow in order to get the correct colors (this is what we know as the ‚Äúwhite point‚Äù or ‚Äúwhite balance‚Äù) <br><br>  <b>The second factor.</b>  As you know, the image on the monitors is obtained by radiating three types of light beams of red, blue and green light.  But the trouble is, the characteristics of these beams from monitor to monitor can vary significantly: <br><br><img src="http://www.ngi-central.org/ngi_documentation/html/chunk/images/468px-Computer_color_spectrum.png"><br><br>  The width and intensity of the color beams do not match the width of the cone perception zone, and the resulting image is calibrated for people with normal color vision, so it‚Äôs damn difficult to predict how a picture on a cheap monitor with a small color gamut will be perceived by a color blind.  One can even say with confidence that in most cases, in a photo shown by a cheap monitor, the color blind will see more colors than in real life.  :) <br><br><blockquote><h3>  Box two.  How color is coded </h3><br>  In order to somehow resolve the situation with color perception, as early as 1931, the French committee CIE had invented a color scheme based on the mixing of three primary colors - red, blue and green.  The scheme was called XYZ (http://ru.wikipedia.org/wiki/XYZ) and as a result of experiments with three searchlights and 200 people with normal color perception, some average graphs were obtained that are valid for a white point of around 4300K.  Later, in 1964, a similar series of experiments and graphs (called the ‚Äústandard observer color model‚Äù) were carried out.  At the moment they have the following standard view: <br><img src="http://www.ngi-central.org/ngi_documentation/html/chunk/images/CIE1931_XYZCMF.png"><img src="http://www.ngi-central.org/ngi_documentation/html/chunk/images/476px-CIExy1931.png"><img src="https://habrastorage.org/getpro/geektimes/post_images/03b/45d/0ad/03b45d0ad490b62a4df8fe3d03c32168.jpg"><br>  Putting the values ‚Äã‚Äãof RG and B on three axes (corresponding to X, Y and Z in the title of the scheme), a certain color diagram was obtained, including in theory all the colors perceived by human vision.  It is on the basis of the CIE XYZ scheme that modern CIE Lab and Hunter Lab color schemes were subsequently created. <br>  The CIE XYZ model has known disadvantages.  First of all, you need to take into account the fact that the scheme is valid only near the white point of 4300K.  In addition, when lighting is lowered or increased, the color coverage of a person decreases, and disproportionately: the colors close to azure disappear first, then the red-violet ones are shifted either to red or to blue, etc. <br>  And finally, the standard model of the observer differs from modern data on the light sensitivity of cones (see the graph above). <br></blockquote><br>  To simulate color perception, I took modern data on the cone's photosensitivity.  And that's what I did. <br><br><h3>  Basic color sensitivity: </h3><br> <a href="http://fotki.yandex.ru/users/ymi/view/136276/"><img src="https://habrastorage.org/getpro/geektimes/post_images/34a/52b/3c5/34a52b3c57b7e6dca4230934700753b3.png" width="500" height="188"></a> <br><br><h3>  The perception of color depending on the type of cones: </h3><br> <a href="http://fotki.yandex.ru/users/ymi/view/136269/"><img src="https://habrastorage.org/getpro/geektimes/post_images/0f1/4b5/b76/0f14b5b7697c0ddc7875df86c8302df8.png" width="500" height="188"></a> <br>  (for perception of Magenta, the red cones are shifted to the minimum yellow level) <br>  The bar below shows the level of lightness, the height of the column above the bar - the power of color perception.  Thus, red is perceived a little more strongly than green, while the peak of perception falls on red-orange.  The minimum power of perception is somewhere in the azure region. <br><br><h3>  Modeling </h3><br><br>  <b>What happens in the case of protanopia?</b>  The size of the signal from the red cones decreases, while the white point remains in place, but the relative white level decreases in the red region (the model is given in 10% increments): <br><img src="https://habrastorage.org/getpro/geektimes/post_images/215/8d8/4f3/2158d84f3161d5d3a2ebc055cdee28d0.gif" width="320" height="120"><img src="https://habrastorage.org/getpro/geektimes/post_images/a6f/917/366/a6f917366d8601deed7c7b0aa47c3dcb.gif" width="320" height="120"><br><br>  <b>The case of deitanopia (reduced perception of green):</b> <br><img src="https://habrastorage.org/getpro/geektimes/post_images/433/fc3/844/433fc3844ef1f224c4bd05abe2909808.gif" width="320" height="120"><img src="https://habrastorage.org/getpro/geektimes/post_images/104/82b/e0f/10482be0f2bb5f93b596a1fe8861ba92.gif" width="320" height="120"><br><br>  <b>And the most difficult case to model is tritanopia.</b>  The difficulty lies in shifting red to get magenta: <br><img src="https://habrastorage.org/getpro/geektimes/post_images/017/b7f/ec8/017b7fec8f90e6845eabc4db17ea5c5c.gif" width="320" height="120"><img src="https://habrastorage.org/getpro/geektimes/post_images/bda/42e/975/bda42e9756515907f19c902c22c9b527.gif" width="320" height="120"><br><br><h3>  Real life </h3><br><br>  Naturally, all these are beautiful pictures, but how does this fit in with the real world?  The models were tested on two protanopes (converged on coefficients for the reduction of sensitivity of M cones to 34% and 52%), one deitanope (converged on a coefficient of reduction of sensitivity of S by 43%) and two tritanopes (78% and 92%). <br><br>  Testing of models was made on the basis of an assessment of two color targets: <br>  <a href="http://fotki.yandex.ru/users/ymi/view/136273%3Fpage%3D0">linear</a> and <a href="http://fotki.yandex.ru/users/ymi/view/136274%3Fpage%3D0">round</a> <br><br>  Satisfactory was considered the result of almost complete compliance and discrepancies only in the area of ‚Äã‚Äãmazhenta (immediately after the red color).  A piece with magenta - alas - I haven‚Äôt managed to overcome it so far: (It‚Äôs difficult to model :) <br><br><h3>  And the pictures? </h3><br><br>  I prepared several pictures obtained as a result of modeling.  In the simulation, a loss coefficient of 60% was used.  In order not to overload the post with pictures, I quote one set, the rest you can see <a href="http://fotki.yandex.ru/users/ymi/album/65684%3Fp%3D1">here</a> (the original, tritanopia, deitanopia and protanopia are presented sequentially): <br> <a href="http://fotki.yandex.ru/users/ymi/view/136314/"><img src="https://habrastorage.org/getpro/geektimes/post_images/5f2/775/bf6/5f2775bf6984534d321c23f89f8ee508.png" width="378" height="500"></a> <a href="http://fotki.yandex.ru/users/ymi/view/136315/"><img src="https://habrastorage.org/getpro/geektimes/post_images/a2c/810/109/a2c810109492a3e71127d9cfe9f83458.png" width="378" height="500"></a> <br> <a href="http://fotki.yandex.ru/users/ymi/view/136316/"><img src="https://habrastorage.org/getpro/geektimes/post_images/91e/547/776/91e547776bd73352fbbc18225f7a5f3a.png" width="378" height="500"></a> <a href="http://fotki.yandex.ru/users/ymi/view/136317/"><img src="https://habrastorage.org/getpro/geektimes/post_images/377/b67/d0e/377b67d0e5278d86f4bfe09d5f37fc48.png" width="378" height="500"></a> <br><br>  <sup>The article used illustrations from <a href="http://en.wikipedia.org/wiki/Color_blindness">Wikipedia</a> and <a href="http://www.ngi-central.org/ngi_documentation/html/chunk/ch04s12.html">NGI Book</a> , examples for tests were found <a href="http://www.photoforum.ru/photo/429975/index.ru.html%3Flang%3Den">here</a> and <a href="http://loradom.mylivepage.ru/image/index">here</a> .</sup> <br><br>  And in the next post I will talk about the results that I could get with the help of the model (the test will be more about the design than about the modeling) </div><p>Source: <a href="https://habr.com/ru/post/60362/">https://habr.com/ru/post/60362/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../60353/index.html">Pet Shop Boys (QR Code Video Remix)</a></li>
<li><a href="../60355/index.html">Installing Windows 7 on the EEE-PC 1000HE</a></li>
<li><a href="../60358/index.html">We program Windows 7: Taskbar. Part 4 - Custom OverlayIcon</a></li>
<li><a href="../60359/index.html">We program Windows 7: Taskbar. Part 5 - CustomWindowsManager</a></li>
<li><a href="../60360/index.html">We program Windows 7: Taskbar. Part 6 - AppId</a></li>
<li><a href="../60363/index.html">Basics of digital signal processing for the smallest</a></li>
<li><a href="../60365/index.html">Modern warfare 2</a></li>
<li><a href="../60368/index.html">Disable touchpad scrolling after viewing fullscreen video in Firefox</a></li>
<li><a href="../60369/index.html">Regexp and Python: extracting tokens from text</a></li>
<li><a href="../60370/index.html">Experiment: How it works ... maps.google.com + maps.live.com</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>