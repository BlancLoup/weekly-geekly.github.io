<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Spotify: Google Cloud event subsystem migration (part 2)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the first article, we talked about how the old message delivery system worked and the lessons we learned from its work. In this (second) article we...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Spotify: Google Cloud event subsystem migration (part 2)</h1><div class="post__text post__text-html js-mediator-article">  In the <a href="https://habrahabr.ru/company/google/blog/319772/">first article,</a> we talked about how the old message delivery system worked and the lessons we learned from its work.  In this (second) article we will talk about the architecture of the new system and why we chose <a href="https://cloud.google.com/pubsub/overview">Google Cloud Pub / Sub</a> as the transport mechanism for all events. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f61/6c8/044/f616c8044efa48d9b93a8f5b575f74fc.png" alt="image"><a name="habracut"></a><br><br><h2>  How to create a new event delivery system for Spotify </h2><br>  Our experience in operating and maintaining the old event delivery system gave us a lot of background information to create a new and improved one.  Our current platform was built on an even older system that works with hourly logs.  This design created difficulties, for example, with the distribution and confirmation of end-of-file markers on each machine issuing events.  In addition, the current implementation could enter into a state of failure from which it could not automatically exit.  The existence of a piece of software that requires manual intervention in the event of some failures and that runs on each machine that ‚Äúgenerates‚Äù logs results in significant operating costs.  In the new system, we wanted to simplify the work of machines with logs, processing events with a smaller number of computers located closer to the network for further processing. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Here the missing part is the event delivery system or a queue that implements reliable event transport and storing undelivered messages in the queue.  Using such a system, we could speed up and significantly speed up the transfer of data by producers, receive confirmations with low latency and shift the responsibility for recording events in HDFS to the rest of the system. <br><br>  Another change we planned was that each event type had its own channel, or topic, and the events were converted to a more structured format in the early stages of the process.  More work on Producers means less time to convert data to Extract, Transform, Load (ETL) in later stages.  Dividing events into topics is a key requirement for building an effective real-time system. <br><br>  Since the message delivery should work all the time, we created a new system so that it works in parallel with the current one.  The interfaces on both the Producer and Consumers corresponded to the current system, and we could check the performance and correctness of the new operation before switching to it. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b7a/ebc/fee/b7aebcfeefb5c88e61e465aac931c8ef.png" alt="image"><br><br>  The four main components of the new system are the File Agent (File Tailer), the Event Delivery Service, the Reliable Persistent Queue, and the ETL service. <br><br>  In this design, the File Agent has a much narrower remit than the Producer in our old system.  It generates log files from new events and sends them to the Event Delivery Service.  As soon as he receives confirmation that events have been received, this is where his work ends.  There is no more complicated processing of end-of-file markers or making sure that the data has reached the end point in HDFS. <br><br>  The event delivery system receives them from the Agent, translates them into the final structured format and sends them to the Queue.  The service is built as a RESTful microservice using the <a href="http://spotify.github.io/apollo/">Apollo</a> framework and deployed using the <a href="https://github.com/spotify/helios">Helios</a> orchestrator, which is a general scheme for Spotify.  This allows you to decouple customers from a certain uniform technology, and also allows you to switch to any other basic technology without interrupting service. <br><br>  The queue is the core of our system and in this form is important for scaling in accordance with the growth of data flow.  To cope with Hadoop downtime, it must reliably save messages for several days. <br><br>  The ETL service should reliably prevent duplication and export data from the Queue to hourly assemblies in HDFS.  Before he opens such a package to the downstream users, he must with a high degree of probability make sure that all the data for the package has been received. <br><br>  In the figure above, you can see the block that says "Service uses the API directly."  We have been feeling for some time that syslog is not an ideal API for the Event Producer.  When the new system comes into operation, and the old one completely disappears from the scene, it will be logical to abandon the syslog and start working with libraries that can directly communicate with the Event Delivery Service. <br><br><h2>  Choosing a Secure Delivery Queue </h2><br><h3>  Kafka 0.8 </h3><br>  Creating a Secure Delivery Queue that reliably handles huge amounts of Spotify events is a daunting task.  Our goal was to use existing tools for the hardest work.  Since event delivery is the basis of our infrastructure, we would like to make the system convenient and stable.  The first choice was Kafka 0.8. <br><br>  There are many reports that Kafka 0.8 is successfully used in large companies around the world and Kafka 0.8 is a significantly improved version compared to the one we use now.  In particular, in it the improved Kafka brokers provide reliable permanent storage.  The Mirror Maker project introduced mirroring between data centers, and Camus can be used to export Avro-structured events into hourly assemblies. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f14/b09/356/f14b09356c7251f5a9118cbb684d7fc7.png" alt="image"><br><br>  To make sure that event delivery can work correctly on Kafka 0.8, we deployed the test system, which is shown in the figure above.  Embed a simple Kafka Producer in the Event Delivery Service turned out to be easy.  To ensure that the system works correctly from beginning to end - from Event Delivery Service to HDFS - we have implemented a variety of integration tests in our continuous integration and delivery processes. <br><br>  Unfortunately, as soon as this system began processing real traffic, it began to fall apart.  The only component that turned out to be stable was Camus (but since we didn‚Äôt get too much traffic through the system, we still don‚Äôt know how Camus would behave under load). <br><br>  Mirror Maker gave us the most headache.  We assumed that it would reliably mirror data between data centers, but this turned out to be just not the case.  It works <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-3%2B-%2BMirror%2BMaker%2BEnhancement">only in ideal conditions (more precisely, the best effort basis)</a> .  As soon as a problem occurs in the target cluster, the Mirror Maker simply loses data, although it also informs the source cluster that the data was successfully mirrored (note that this must be fixed in Kafka 0.9). <br><br>  Mirror Makers sometimes became confused about who was the leader in the cluster.  The leader sometimes forgot that he was the leader, while the rest of the Mirror Makers from the cluster could happily try to follow him.  When this happened, the mirroring between the data centers stopped. <br><br>  Kafka Producer also has serious stability issues.  If one or more brokers were removed from the cluster, or even simply restarted, with a certain probability, the Producer entered a state from which he could not leave himself.  In this state, he could not produce any events.  The only way out was a complete restart of the service. <br><br>  Without even addressing the resolution of these issues, we realized that it would take a lot of energy to bring the system into working condition.  We will need to define a deployment strategy for Kafka Brokers and Mirror Makers, model the required capacity and plan all system components, and also set performance metrics for <a href="https://spotify.github.io/heroic/">the Spotify monitoring system</a> . <br><br>  We were at a crossroads.  Should we make a significant investment and try to make Kafka work for us?  Or should you try something else? <br><br><h3>  Google Cloud Pub / Sub </h3><br>  While we were fighting Kafka, other members of the Spotify team began experimenting with Google Cloud.  We were especially interested in <a href="https://cloud.google.com/pubsub/overview">Cloud Pub / Sub</a> .  It seemed that Cloud Pub / Sub will be able to satisfy our need for a reliable queue: it can store undelivered data for <a href="https://cloud.google.com/pubsub/quotas">7 days</a> , provides reliability through confirmations at the application level, and has at-least-once delivery semantics. <br><br>  In addition to meeting our basic needs, Cloud Pub / Sub has additional benefits: <br><br><ol><li>  <b>Accessibility</b> - as a global service, Pub / Sub is available in all <a href="https://cloud.google.com/compute/docs/zones">areas of the Google Cloud</a> .  Data transfer between our data centers will not go through our normal Internet service provider, but will be used by Google‚Äôs core network. </li><li>  <b>A simple REST API</b> - if we didn't like the client library that Google provides, then we could easily write our own. </li><li>  <b>Operational responsibility lay with someone else</b> - there is no need to create a resource miscalculation model or deployment strategy, set up monitoring and warnings. </li></ol><br>  Sounds great on paper ... but too good to be true?  The solutions that we created on Apache Kafka, although they were not perfect, still served us well.  We had a lot of experience in dealing with various failures, access to hardware and source codes, and - theoretically - we could find the source of any problem.  The transition to a managed service meant that we had to entrust the operations of another organization.  At the same time, Cloud Pub / Sub was advertised as a beta version - we did not know about any organization other than Google, which would use it on the scale we needed. <br><br>  With all this in mind, we decided that we need a detailed test plan to be absolutely sure that if we switch to Cloud Pub / Sub, it will meet all our requirements. <br><br><h4>  Producer Test Load </h4><br>  The first item in our plan was testing Cloud Pub / Sub to see if it could handle our workload.  Currently, our workload reaches 700K events per second at a peak.  Considering future growth and possible disaster recovery scenarios, we stopped at a test load of 2M events per second.  In order to completely finish Pub / Sub, we decided to publish all this amount of traffic in one data center, so that all these requests would fall into Pub / Sub machines in one zone.  We made the assumption that Google planned the zones as independent domains, and that each zone could handle equal amounts of traffic.  In theory, if we could shove 2M messages into one zone, we could also send a number of zones * 2M messages in all zones.  Our hope was that the system would be able to handle this traffic both on the manufacturer‚Äôs side and on the consumer‚Äôs side for a long time without degrading the service. <br><br>  At the very beginning, we stumbled upon a stumbling block: the <a href="https://developers.google.com/api-client-library/java/apis/pubsub/v1">Cloud Pub / Sub Java client</a> did not work well enough.  This client, like many other Google Cloud API clients, is automatically generated from API specifications.  This is good if you want clients to support a wide range of languages, but not too much if you want to get a high-speed client for one language. <br><br>  Fortunately, Pub / Sub has a REST API, so it was easy for us to write our own <a href="https://github.com/spotify/async-google-pubsub-client">library</a> .  We created a new client, thinking first of all about his speed.  To use resources more efficiently, we used asynchronous Java.  We also added queues and batch processing to the client.  (This is not the first time we had to roll up our sleeves and rewrite the Google Cloud API client - in another <a href="https://github.com/spotify/async-datastore-client">project</a> we developed a high-speed client for the Datastore API.) <br><br>  With the new client, we were ready to start loading Pub / Sub in an adult way.  We used a simple generator to send bogus traffic from the Event Service to Pub / Sub.  Formed traffic was redirected through two Pub / Sub topics in a ratio of 7: 3.  To generate 2M messages per second, we launched Event Service on 29 machines. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bda/b9b/136/bdab9b136da477ea50c4f8758508c76e.png" alt="image"><br>  <i>Number of successful requests per second to Pub / Sub from all data centers</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5fc/512/8db/5fc5128dbe5004c70beb7f43392715ad.png" alt="image"><br>  <i>Number of unsuccessful requests per second to Pub / Sub from all data centers</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/586/18d/044/58618d044e4d439da6d39c8b1523e284.png" alt="image"><br>  <i>Inbound and outbound network traffic from Event Service machines in bps</i> <br><br>  Pub / Sub passed the test with honor.  We published 2M messages without any quality problems and almost did not receive server errors from the Pub / Sub backend.  Enabling batch processing and compression on Event Service machines resulted in approximately 1 Gpbs of traffic to Pub / Sub. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4cd/99a/a40/4cd99aa400a0054494f4ab8f22a604bd.png" alt="image"><br>  <i>Google Cloud Monitoring graph for total published messages in Pub / Sub</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1cb/380/4d4/1cb3804d4c758ba42829bc1611694411.png" alt="image"><br>  <i>Google Cloud Monitoring graph for the number of posted posts in Pub / Sub topics</i> <br><br>  A useful side effect of our test is that we were able to compare our internal metrics with the metrics provided by Google.  As shown in Charts 3 and 6, they ideally match. <br><br><h4>  Consumer Stability Test </h4><br>  Our second important test was devoted to consumption.  For 5 days, we measured end-to-end delays under heavy loads.  At the time of the test, we published, on average, about 800K messages per second.  To simulate real loads, the speed of publication changed throughout the day.  To ensure that we can use several topics at the same time, all data was published for two topics in a ratio of 7: 3. <br><br>  Slightly surprised by the behavior of Cloud Pub / Sub is that it‚Äôs necessary to create subscriptions before saving messages - while no subscriptions exist, no data is saved.  Each subscription stores data independently, and there are no restrictions on how many subscriptions a consumer may have.  Consumers are coordinated on the server side, and the server is responsible for sufficient message allocation for all consumers requesting data.  This is very different from Kafka, where data is stored in the created topic and the number of consumers in the topic is limited by the number of sections in the topic. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/95b/dcf/668/95bdcf6687bc90c4c24fbe759a8831ea.png" alt="image"><br><br>  In our test, we created a subscription, an hour later we began to consume data.  We consumed them in batches of 1000 messages.  Since we did not try to reach the consumption limit, we just wanted to slightly exceed the current peak level.  It took 8 hours.  After we reached it, Consumers continued to work at the same level that corresponded to the speed of publication. <br><br>  The average end-to-end delay, which we measured during the test period ‚Äî including the restoration of the backlog ‚Äî was around 20 seconds.  We did not observe message losses during the entire test period. <br><br><h2>  Decision </h2><br>  On these tests, we made sure that Cloud Pub / Sub is the right choice for us.  Delays were small and constant, and the only limitation in capacity we encountered was the established quota.  In short, choosing Cloud Pub / Sub instead of Kafka 0.8 for our new message delivery platform was the obvious choice. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f61/6c8/044/f616c8044efa48d9b93a8f5b575f74fc.png?w=730&amp;h=409" alt="image"><br><br><h2>  Next step </h2><br>  After the events are securely stored in Pub / Sub, it‚Äôs time to export them to HDFS.  To take full advantage of Google Cloud, we decided to try Dataflow. <br><br>  In the <a href="https://habrahabr.ru/company/google/blog/323908/">last article</a> of this series, we will talk about how Dataflow was used for our purposes.  Stay with us! </div><p>Source: <a href="https://habr.com/ru/post/320192/">https://habr.com/ru/post/320192/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../320180/index.html">JDK 9 development has completed the ‚ÄúFeature Complete‚Äù phase</a></li>
<li><a href="../320184/index.html">Automatic visualization of python code using flowcharts</a></li>
<li><a href="../320186/index.html">Selection of useful slides from Julia Evans</a></li>
<li><a href="../320188/index.html">Hey, tv, are you the smartest?</a></li>
<li><a href="../320190/index.html">Virtual Client Computing in 2017: Tools and Tips for Successful Implementation</a></li>
<li><a href="../320194/index.html">3 days left before the conference ‚ÄúRegulation of cryptocurrency in Russia: interim results‚Äù</a></li>
<li><a href="../320196/index.html">Simple Wolfram Language http server</a></li>
<li><a href="../320198/index.html">Welcome to Moscow CocoaHeads January 27</a></li>
<li><a href="../320200/index.html">Pygest # 1. Releases, articles, interesting projects from the world of Python [January 01, 2017 - January 15, 2017]</a></li>
<li><a href="../320202/index.html">Wikimart and the ‚Äútheory of the big fool‚Äù in e-commerce</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>