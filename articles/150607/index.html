<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Recommended systems: LDA</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Last time I talked about the Bayes theorem and gave a simple example - the naive Bayes classifier. This time we will move on to a more complex topic, ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Recommended systems: LDA</h1><div class="post__text post__text-html js-mediator-article">  Last time I talked about the Bayes theorem and gave a simple example - the naive Bayes classifier.  This time we will move on to a more complex topic, which develops and continues the work of naive Bayes: we will learn how to highlight topics using the LDA (latent Dirichlet allocation) model, and also apply this to recommender systems. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/269/679/cf4/269679cf4c51992c1c6c308a6892f6c5.png"><br><a name="habracut"></a><br>  The LDA model solves the classical text analysis problem: to create a probabilistic model of a large collection of texts (for example, for information retrieval or classification).  We already know the naive approach: the hidden variable is the topic, and the words are obtained with a fixed topic independently of the discrete distribution.  Similarly, approaches based on clustering work.  Let's complicate the model a bit. <br><br>  Obviously, one document may have several topics;  approaches that cluster documents on topics do not take this into account.  LDA is a hierarchical Bayesian model consisting of two levels: <br><ul><li>  at the first level - a mixture, the components of which correspond to the "themes"; </li><li>  at the second level, a multinomial variable with the Dirichlet a priori distribution, which specifies the ‚Äúdistribution of themes‚Äù in the document. </li></ul><br>  Here is the graph of the model (picture taken from <a href="http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Wikipedia</a> ): <br><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Latent_Dirichlet_allocation.svg/500px-Latent_Dirichlet_allocation.svg.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Complex models are often easiest to understand this way - let's see how the model will generate a new document: <br><ul><li>  choose the length of the document <i>N</i> (this is not drawn on the graph - it is not that part of the model); </li><li>  choose vector <img src="http://habr.habrastorage.org/post_images/fbf/5b0/617/fbf5b061708fff69d170c3e2e37645bc.png">  - the vector of "severity" of each topic in this document </li><li>  for each of the <i>N</i> words <i>w</i> : <br><ul><li>  choose a topic <img src="http://habr.habrastorage.org/post_images/af2/163/d1e/af2163d1ecb7e905400d31b46e144a79.png">  by distribution <img src="http://habr.habrastorage.org/post_images/92f/665/7da/92f6657da9a33ba894fb8655730c8a24.png">  ; </li><li>  choose a word <img src="http://habr.habrastorage.org/post_images/3a3/1f3/4bf/3a31f34bf1e79e7297672d298bdb8200.png">  with probabilities given in Œ≤. </li></ul></li></ul><br><br>  For simplicity, we fix the number of those <i>k</i> and we will assume that Œ≤ is just a set of parameters <img src="http://mathurl.com/98qdu8r.png">  that need to be assessed, and we will not worry about the distribution on N. The joint distribution then looks like this: <br><img src="https://habrastorage.org/getpro/habr/post_images/f06/5a0/f1d/f065a0f1dd521cdcbbaa4783af1e3fab.png"><br>  Unlike conventional clustering with a priori Dirichlet distribution or ordinary naive Bayes, we don‚Äôt select a cluster here once, and then add words from this cluster, and for each word, first select the theme for the Œ∏ distribution, and then we throw the word on this topic . <br><br>  At the exit after learning the LDA model, we obtain the Œ∏ vectors, showing how the topics are distributed in each document, and the Œ≤ distributions, which words are more likely to be in certain topics.  Thus, from the LDA results it is easy to get for each document a list of topics in it, and for each topic a list of words characteristic for it, i.e.  an actual topic description.  Note: all this learning without a teacher (unsupervised learning), it is not necessary to mark out the texts from the texts! <br><br>  As for how it all works, I confess honestly - I don‚Äôt think that in a brief Habrapost you can easily tell how the conclusion was organized in the initial work about the LDA;  it was based on the search for variational approximations to the distribution of the model (we simplify the structure of the model, breaking the bonds, but adding free variables for which we optimize).  However, much simpler approaches were soon developed, based on Gibbs sampling;  Maybe someday I will return to this topic when I talk about sampling, but now they are too narrow for this field.  Let me just leave here the link to <a href="http://mallet.cs.umass.edu/">MALLET</a> - the most popular and, as far as I can tell, the best ready LDA implementation.  MALLET'a you enough for almost all occasions, except that if you want to select topics from the entire Internet entirely - on the cluster MALLET, it seems, does not know how to work. <br><br>  And I will tell you how LDA can be applied in the recommender system;  This method is particularly well suited for situations where ‚Äúratings‚Äù are not set on a long scale of stars, but simply binary ‚Äúexpressions of approval‚Äù, <i>like</i> in Surfingbird.  The idea is quite simple: let's <i>consider the user as a document consisting of the products he likes</i> .  At the same time, products become ‚Äúwords‚Äù for LDA, users - ‚Äúdocuments‚Äù, and as a result, ‚Äútopics‚Äù of user preferences are highlighted.  In addition, you can evaluate which products are most typical for a particular topic ‚Äî that is, select the product group that is most relevant for the corresponding user group, and also enter from distributions on the topics of distance both on users and on products. <br><br>  We applied this analysis to the <a href="http://surfingbird.ru/">Surfingbird.ru</a> database.  and got a lot of interesting topics - it turned out that in almost all cases groups of sites are really distinguished, united by one theme and quite similar to each other.  In the pictures below, I drew statistics of words that are often found on the pages of some of the topics obtained with the help of LDA (while LDA itself did not work with the text of the pages, but only with user preferences!);  I cut out the links to the pages themselves, just in case. <br><br><table><tbody><tr><td> <a href="http://www.flickr.com/photos/86242626%40N03/7899216000/" title="topic-wildlife by snikolenko, on Flickr"><img src="https://habrastorage.org/getpro/habr/post_images/929/b94/6ed/929b946ed90bdbd0b269169db69991a5.jpg" width="240" height="148" alt="topic wildlife"></a> </td><td> <a href="http://www.flickr.com/photos/86242626%40N03/7899216130/" title="topic-simpson by snikolenko, on Flickr"><img src="http://farm9.staticflickr.com/8035/7899216130_e8435d0324_m.jpg" width="240" height="150" alt="topic-simpson"></a> </td><td> <a href="http://www.flickr.com/photos/86242626%40N03/7899216376/" title="topic-movie by snikolenko, on Flickr"><img src="https://habrastorage.org/getpro/habr/post_images/316/c94/eb1/316c94eb175139a619debd44827003b1.jpg" width="240" height="164" alt="topic-movie"></a> </td></tr><tr><td> <a href="http://www.flickr.com/photos/86242626%40N03/7899216482/" title="topic-exercise by snikolenko, on Flickr"><img src="https://habrastorage.org/getpro/habr/post_images/679/262/129/679262129504a6e772329c4a60624c5f.jpg" width="240" height="137" alt="topic-exercise"></a> </td><td> <a href="http://www.flickr.com/photos/86242626%40N03/7899216602/" title="topic-comp by snikolenko, on Flickr"><img src="https://habrastorage.org/getpro/habr/post_images/224/ceb/6a4/224ceb6a45377ec8649208d789c3279c.jpg" width="240" height="146" alt="topic-comp"></a> </td><td> <a href="http://www.flickr.com/photos/86242626%40N03/7899216248/" title="topic-sex by snikolenko, on Flickr"><img src="https://habrastorage.org/getpro/habr/post_images/d8e/0b9/f3e/d8e0b9f3e8bc09982471d48cc4bdab8b.jpg" width="240" height="163" alt="topic-sex"></a> </td></tr></tbody></table></div><p>Source: <a href="https://habr.com/ru/post/150607/">https://habr.com/ru/post/150607/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../150601/index.html">Study schedules online. The history of the startup Schoodle</a></li>
<li><a href="../150602/index.html">The strange logic of the programmers of Sberbank is dedicated to</a></li>
<li><a href="../150603/index.html">Introduction to frameworks (Part 2)</a></li>
<li><a href="../150604/index.html">John's Phone - the world's simplest mobile phone.</a></li>
<li><a href="../150606/index.html">Share your impressions on Yandex.Maps</a></li>
<li><a href="../150608/index.html">We return to the distribution or how to do the impossible</a></li>
<li><a href="../150609/index.html">Hard disk in the printer / MFP - convenience or excess?</a></li>
<li><a href="../150610/index.html">Woven - an interactive vest for games and not only</a></li>
<li><a href="../150611/index.html">Google Online School is changing face!</a></li>
<li><a href="../150612/index.html">From you - applications, from us - new Hubs on Windows and Visual Studio 2012!</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>