<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to understand Tensorflow and not die, but even teach a car something</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habrazhiteli. Today's post will be about how not to get lost in the wilds of the variety of options for using TensorFlow for machine learning and ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to understand Tensorflow and not die, but even teach a car something</h1><div class="post__text post__text-html js-mediator-article"><p>  Hi, Habrazhiteli.  Today's post will be about how not to get lost in the wilds of the variety of options for using TensorFlow for machine learning and achieve your goal.  The article is designed to ensure that the reader knows the basics of the principles of machine learning, but has not yet tried to do it with his own hands.  As a result, we get a working demo on Android, which recognizes something with fairly high accuracy.  But first things first. </p><br><p><img src="https://habrastorage.org/webt/rs/7r/_f/rs7r_f7v6dywnklpaok4htwntsq.jpeg"></p><a name="habracut"></a><br><p>  After looking at the latest materials - it was decided to start using <a href="https://www.tensorflow.org/">Tensorflow</a> , which is now gaining high momentum, and it seems that there are enough articles in English and Russian not to dig in all of this and be able to figure out what's what. </p><br><p>  After spending two weeks studying articles and numerous ekzamply at the office.  site, I realized that I did not understand.  TOO much information and options on how Tensorflow can be used.  My head is already plump on how much they offer different solutions and what to do with them, in relation to my task. </p><br><p><img src="https://habrastorage.org/webt/bd/2z/jy/bd2zjyct-gx0xbz9nfbwwya5aw8.png"></p><br><p>  Then I decided to try everything, from the simplest and almost ready-made options (in which I was required to register a dependency in the gradle and add a couple of lines of code) to more complex ones (in which I would have to create and train graph models myself and learn how to use them in mobile application). </p><br><p>  In the end, I had to use a complex version, which will be described in more detail below.  In the meantime, I have compiled for you a list of simpler options that are no less effective, just everyone is suited to their goal. </p><br><h3 id="1--ml-kithttpsfirebasegooglecomdocsml-kit">  1. <a href="https://firebase.google.com/docs/ml-kit/">ML KIT</a> </h3><br><p><img src="https://habrastorage.org/webt/al/of/8w/alof8wunrnv66f66xwv2rrlbrn0.png"></p><br><p>  The easiest solution to use is to use a couple of lines of code: </p><br><ul><li>  Text recognition (text, Latin characters) </li><li>  Face detection (faces, emotions) </li><li>  Barcode scanning (barcode, qr-code) </li><li>  Image labeling (limited number of object types in the image) </li><li>  Landmark recognition </li></ul><br><p>  With this solution, it is also a little more difficult to use your own TensorFlow Lite model, but converting to this format has caused difficulties, so this item has not been tried. </p><br><p>  As the creators of this creation write, it is possible to solve most of the problems using these developments.  But if this does not apply to your task - you will have to use custom models. </p><br><h3 id="2--custom-visionhttpswwwcustomvisionai">  2. <a href="https://www.customvision.ai/">Custom Vision</a> </h3><br><p><img src="https://habrastorage.org/webt/p9/c_/2u/p9c_2ujglvyu8mbffhrmoqpzav0.png"></p><br><p>  Very handy tool for creating and training your custom models using images. <br>  From the Pros - there is a free version that allows you to keep one project. <br>  From the Minuses - the free version limits the number of "incoming" images in 3000 pcs.  To try and make an average network of accuracy - quite enough.  For more accurate tasks, you need more. <br>  All that is required from the user is to add images with a mark (for example, image1 is "racoon", image2 - "sun"), train and export the graph for further use. </p><br><p><img src="https://habrastorage.org/webt/co/lk/nw/colknw0ljunbtzcixxdrde6qwtm.png"></p><br><p>  Caring Microsoft even offers its own <a href="https://github.com/Azure-Samples/cognitive-services-android-customvision-sample">sample</a> , with which you can try out your resulting graph. <br>  For those who are already "in the subject" - the graph is generated already in the Frozen state, i.e.  You don‚Äôt need to do anything else with it. <br>  This solution is good when you have a large sample and (attention) MANY different classes when learning.  Since  otherwise, there will be many false definitions in practice.  For example, you have trained on raccoons and suns, and if there is a person at the entrance, then he may be equally likely to be defined by such a system as one or the other.  Although in fact - Nothing else. </p><br><h3 id="3--sozdanie-modeli-vruchnuyu">  3. Creating a model manually </h3><br><p><img src="https://habrastorage.org/webt/m_/ku/r_/m_kur_ks0vdyiqoiw7h5pvbwoey.jpeg"></p><br><p>  When you need to fine-tune the model yourself for image recognition, more complex manipulations with the input sample of images come into play. <br>  For example, we do not want to have restrictions on the size of the input sample (as in the preceding paragraph), or we want to train the model more precisely by adjusting the number of epoch and other learning parameters ourselves. <br>  In this approach, there are several examples from Tensorflow, which describe the procedure and the final result. <br>  Here are a few such examples: </p><br><ul><li>  Cool codebot <a href="https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/index.html%3Findex%3D..%252F..%252Findex">Tensorflow for Poets</a> . <br></li></ul><br><br><p>  It provides an example of how to create a color type classifier based on an open ImageNet image database ‚Äî prepare images, and then train the model.  Also mentioned a bit is how you can work with a rather interesting tool - TensorBoard.  Of its simplest functions, it clearly demonstrates the structure of your finished model, as well as the learning process in many ways. </p><br><ul><li><p>  Kodlab <a href="https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/index.html%3Findex%3D..%252F..%252Findex">Tensorflow for Poets 2</a> - continued work with the color classifier.  Shows how if there are graph files and its labels (which were obtained in the previous Kodlab), you can run the application on android.  One of the Kodlab points is the conversion from the "usual" graph format ".pb" into the Tensorflow lite format (which provides for some file optimizations to reduce the total size of the graph file, because mobile devices are demanding for this). </p><br></li><li><p>  Recognition of handwritten characters <a href="https://github.com/nex3z/tflite-mnist-android">MNIST</a> . <br></p><br><img src="https://habrastorage.org/webt/bz/ah/mx/bzahmxc0xozicgssbkzfqbgi1qw.gif"></li></ul><br><br><p>  The turnip contains the original model (which is already prepared for this task), instructions on how to train it, convert it, and how to launch the project for Android at the end to check how it all works. </p><br><p>  Based on these examples, you can figure out how to work with custom models in Tensorflow and try to either make your own, or take one of the already pre-trained models that are assembled on a githaba: <br>  <a href="https://github.com/tensorflow/models/tree/master/official">Tensorflow models</a> </p><br><p>  Speaking of "pre-trained" models.  Interesting nuances when using these: </p><br><ul><li> Their structure is already prepared for a specific task. </li><li>  They are already trained on large sample sizes. <br>  Therefore, if your sample is not full enough, you can take a pre-trained model that is close in scope to your task.  Using such a model, adding your own learning rules, you will get a better result, rather than trying to train a model from scratch. </li></ul><br><h3 id="4--object-detection-api---cozdanie-modeli-vruchnuyu">  4. Object Detection API + model creation manually </h3><br><p>  However, all the previous points did not give the desired result.  From the very beginning it was difficult to understand what needs to be done and with the help of which approach.  Then a cool article was found on the <a href="https://habr.com/company/nixsolutions/blog/422353/">Object Detection API</a> , which tells how you can find several categories on one image, as well as several instances of one category.  In the process of working on this sample, the original articles and video tutorials on recognizing custom objects turned out to be more convenient (links will be at the end). </p><br><p>  But the work could not have been completed without an <a href="https://towardsdatascience.com/detecting-pikachu-on-android-using-tensorflow-object-detection-15464c7a60cd">article on the recognition of Pikachu</a> - because a very important nuance was mentioned there, which for some reason is not mentioned anywhere in a single guide or example.  And without it, all the work done would be wasted. </p><br><p>  So, now at last about what still had to do and what happened at the exit. </p><br><ol><li>  First, the flour installation Tensorflow.  Who will not be able to install it, or use the standard creation scripts, model training - just have patience and google.  Almost every problem has already been written to issues on a githheb or on stackoverflow. <br></li></ol><br>  According to the instructions for object recognition, we need to prepare an input sample before learning the model.  These articles describe in detail how to do this with the help of a convenient tool - labelImg.  The only difficulty here is to do a very long and scrupulous work on identifying the boundaries of the objects we need.  In this case, stamp-stamps on the images of documents. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/webt/ge/hh/x_/gehhx_5fqfezu1sbh5tvoofss20.png"><br>  With the help of ready-made scripts, the next step is to export the data from step 2 first to csv files, then to TFRecords - the input data format Tensorflow.  There should be no difficulties. <br>  Choosing a pre-trained model, based on which we will pre-train the graph, as well as the training itself.  Here there can appear the most huge number of unknown errors, the cause of which is unidentified (or crookedly set) packages necessary for work.  But you will succeed, do not despair, the result is worth it. <br><br><img src="https://habrastorage.org/webt/9y/qw/1b/9yqw1boyubfcrrf5jcaylkjjtyo.jpeg"><br>  Export the resulting file after learning to the 'pb' format.  Simply select the last 'ckpt' file and export it. <br>  Run an example of work on Android. <br>  Downloading the official object recognition sample from the Tensorflow - <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android">TF Detect</a> github.  We insert there our model and file with labels.  But.  Nothing will work. <br><br><img src="https://habrastorage.org/webt/kj/3k/o4/kj3ko4d3ywoap8ff6oknuwova7c.gif"><br><br><p>  It was here that the biggest plug-in in all the work arose, oddly enough - well, Tensorflow samples didn‚Äôt want to work at all.  Everything fell.  Only the mighty Pikachu with his article managed to help bring everything to work. <br>  In the labels.txt file, the first line must be the inscription "???", because  By default, in Object Detection API, object id numbers do not begin with 0 as usual, but from 1. Due to the fact that the null class is reserved - and you need to specify magic questions.  Those.  Your tag file will look something like this: </p><br><pre><code class="hljs">??? stamp</code> </pre> <br><p>  And then - run the sample and see the recognition of objects and the level of confidience with which it was obtained. </p><br><p><img src="https://habrastorage.org/webt/ly/kr/dm/lykrdma-x9h8epuqsuah3gkr3bk.png"><img src="https://habrastorage.org/webt/ne/lm/7v/nelm7v8rpjiuhzhevlptp0dc-fa.png"><img src="https://habrastorage.org/webt/9t/ci/4r/9tci4rxzhixufdjhb5ecpdof0ik.png"></p><br><p>  Thus, the result was a simple application that, when you hover the camera, recognizes the borders of the stamp on the document and points them along with the accuracy of recognition. <br>  And if we exclude the time that was spent on searching for the right approach and trying to launch it, in general, the work turned out to be quite fast and not really difficult.  You just need to know the nuances before proceeding to work. </p><br><p>  Already as an additional section (here you can already close the article, if you are tired of the information), I would like to write a couple of life hacks that helped in working with all this. </p><br><ul><li><p>  quite often tensorflow scripts did not work because they were run from the wrong directories.  And on different PCs it was different: someone needed to run from the " <code>tensroflowmodels/models/research</code> " directory, and someone - to a deeper level - from " <code>tensroflowmodels/models/research/object-detection</code> " </p><br></li><li><p>  Remember that for each open terminal, you need to re-export the path with the command </p><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">export</span></span> PYTHONPATH=/  /tensroflowmodels/models/research/slim:<span class="hljs-variable"><span class="hljs-variable">$PYTHONPATH</span></span></code> </pre> <br></li><li><p>  if you are not using your graph and want to find out information about it (for example, " <code>input_node_name</code> ", which is required later in the work), execute two commands from the root folder: </p><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">bazel</span></span> build tensorflow/tools/graph_transforms:summarize_graph bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=<span class="hljs-string"><span class="hljs-string">"/  /frozen_inference_graph.pb"</span></span></code> </pre> <br><p>  where " <code>/  /frozen_inference_graph.pb</code> " is the path to the column about which you want to find out information </p><br></li><li><p>  To view information about the graph, you can use Tensorboard </p><br><pre> <code class="hljs pgsql">python import_pb_to_tensorboard.py <span class="hljs-comment"><span class="hljs-comment">--model_dir=output/frozen_inference_graph.pb --log_dir=training</span></span></code> </pre> <br><p>  where you need to specify the path to the graph ( <code>model_dir</code> ) and the path to the files that were obtained in the learning process ( <code>log_dir</code> ).  Then just open localhost in the browser and watch what interests you. </p><br></li></ul><br><p>  And the last part - on working with python scripts in the instructions for the Object Detection API - for you prepared a little cheat sheet below with commands and hints. </p><br><div class="spoiler">  <b class="spoiler_title">Crib</b> <div class="spoiler_text"><p>  Export from labelimg to csv (from the object_detection directory) </p><br><pre> <code class="hljs mel"><span class="hljs-keyword"><span class="hljs-keyword">python</span></span> xml_to_csv.py</code> </pre> <br><p>  Further, all the steps that are listed below should be performed from the same Tensorflow folder (" <code>tensroflowmodels/models/research/object-detection</code> " or one level higher - depending on how you do it) - that is, all images of the input sample, TFRecords and other files before the start of the work should be copied inside this directory. </p><br><p>  Export from csv to tfrecord </p><br><pre> <code class="hljs kotlin">python generate_tfrecord.py --csv_input=<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>/train_labels.csv --output_path=<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>/train.record python generate_tfrecord.py --csv_input=<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>/test_labels.csv --output_path=<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>/test.record</code> </pre> <br><p>  * Do not forget to change in the file itself (generate_tfrecord.py) the lines 'train' and 'test' in the paths, as well as <br>  the name of the recognized classes in the <code>class_text_to_int</code> function (which should be duplicated in the <code>pbtxt</code> file that you create before learning the graph). </p><br><p>  Training </p><br><pre> <code class="hljs pgsql">python legacy/train.py ‚Äîlogtostderr <span class="hljs-comment"><span class="hljs-comment">--train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_coco.config</span></span></code> </pre> <br><p>  ** Before training, do not forget to check the file " <code>training/object-detection.pbtxt</code> " - all recognized classes and the file " <code>training/ssd_mobilenet_v1_coco.config</code> " should be listed there - you need to change the parameter " <code>num_classes</code> " to the number of your classes. </p><br><p>  Export model to pb </p><br><pre> <code class="hljs tex">python export_inference_graph.py <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--input_type=image_tensor <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--pipeline_config_path=training/pipeline.config <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--trained_checkpoint_prefix=training/model.ckpt-110 <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--output_directory=output</code> </pre> </div></div><br><p>  Thank you for your interest in this topic! </p><br><p>  Links </p><br><ol><li>  <a href="https://becominghuman.ai/tensorflow-object-detection-api-tutorial-training-and-evaluating-custom-object-detector-ed2594afcf73">Original article on object recognition</a> </li><li>  <a href="https://www.youtube.com/watch%3Fv%3DK_mFnvzyLvc%26list%3DPLQVvvaa0QuDcNK5GeCQnxYnSSaar2tpku%26index%3D3">Cycle video to article on the recognition of objects in English</a> </li><li>  <a href="https://pythonprogramming.net/creating-tfrecord-files-tensorflow-object-detection-api-tutorial/">A set of scripts that were used in the original article.</a> </li></ol></div><p>Source: <a href="https://habr.com/ru/post/427449/">https://habr.com/ru/post/427449/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../427437/index.html">Configuring linked servers: ms sql server and teradata</a></li>
<li><a href="../427439/index.html">The whole truth about the RTOS. Article # 16. Signals</a></li>
<li><a href="../427441/index.html">Convergence with Kubernetes</a></li>
<li><a href="../427443/index.html">Vivisection success</a></li>
<li><a href="../427447/index.html">GNU Arm Embedded Toolchain appeared in PVS-Studio</a></li>
<li><a href="../427451/index.html">Connect phpStorm tasks to Bitrix24</a></li>
<li><a href="../427453/index.html">How I did the transfer of sound on the Raspberry Pi</a></li>
<li><a href="../427455/index.html">Satellite provider OneWeb will try to get frequencies in Russia</a></li>
<li><a href="../427457/index.html">"The third wave" of AI and systems for state security</a></li>
<li><a href="../427459/index.html">LED lamps Diall from the store Castorama</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>