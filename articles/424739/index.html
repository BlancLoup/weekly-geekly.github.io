<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Event Registration with Kafka</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! 

 We uncorked the last reserves of the book " Apache Kafka. Stream processing and data analysis " and sent it to a reprint. Moreover, we re...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Event Registration with Kafka</h1><div class="post__text post__text-html js-mediator-article"> Hi, Habr! <br><br>  We uncorked the last reserves of the book " <a href="https://www.piter.com/product_by_id/112863410">Apache Kafka. Stream processing and data analysis</a> " and sent it to a reprint.  Moreover, we received a contract for the book " <a href="https://www.manning.com/books/kafka-streams-in-action">Kafka Streams in Action</a> " and proceed to translate it literally next week. <br><br><img src="https://habrastorage.org/webt/re/29/51/re2951jsut-yre1r79xmmt4ibdy.jpeg">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      To show an interesting use case for the Kafka Streams library, we decided to translate an article about the Event Sourcing paradigm in Kafka from the very Adam Worsky, whose <a href="https://habr.com/company/piter/blog/423317/">article</a> on the Scala language was published with us two weeks ago.  All the more interesting is that Adam Worsky‚Äôs opinion is not indisputable: <a href="https://medium.com/serialized-io/apache-kafka-is-not-for-event-sourcing-81735c3cf5c">here</a> , for example, it is argued that this paradigm is decisive for Kafka.  All the more memorable, we hope, we get the impression of the article. <br><br>  The term ‚ÄúEvent Sourcing‚Äù is translated as ‚ÄúEvent Registration‚Äù in both our Martin <a href="https://www.piter.com/product_by_id/99338891">Pure Architecture</a> edition and in this article.  If someone is impressed by the translation of "pumping events" - let me know please. <br><a name="habracut"></a><br>  Creating a system in which event sourcing is provided for, we sooner or later encounter the problem of persistence - and here we have a couple of options.  First, there is an <a href="https://eventstore.org/">EventStore</a> , a mature implementation, battle-hardened.  Alternatively, you can use <a href="https://doc.akka.io/docs/akka/snapshot/persistence.html%3Flanguage%3Dscala">akka-persistence</a> to take full advantage of the scalability of <a href="https://doc.akka.io/docs/akka/snapshot/persistence.html%3Flanguage%3Dscala">Cassandra</a> , as well as rely on the performance of the actor model.  Another option is a good old <a href="https://softwaremill.com/entry-level-event-sourcing/">relational database</a> , where the <code>CRUD</code> approach is combined using events, and the maximum benefit is squeezed out of transactions. <br><br>  In addition to these (and, perhaps, many other) opportunities that have emerged due to several recently implemented things, today it has become very easy to organize the registration of events on top of <a href="https://kafka.apache.org/">Kafka</a> .  Let's look at how. <br><br>  <b>What is event registration?</b> <br><br>  There are a number of <a href="https://martinfowler.com/eaaDev/EventSourcing.html">excellent</a> <a href="https://eventstore.org/docs/event-sourcing-basics/">introductory</a> <a href="https://docs.microsoft.com/en-us/azure/architecture/patterns/event-sourcing">articles</a> on this subject, so I will limit myself to a brief introduction.  When registering events, we save not the ‚Äúcurrent‚Äù state of the entities used in our system, but the stream of events related to these entities.  Each <i>event</i> is a <b>fact</b> describing a change of state (already!) <b>Occurred</b> with the object.  As you know, the facts are not discussed and <b>unchanged</b> . <br><br>  When we have a stream of such events, the actual state of the entity can be found out by winding up all the events related to it;  However, please note that the opposite is impossible - keeping only the ‚Äúactual‚Äù state, we discard a lot of valuable chronological information. <br><br>  Event logging can peacefully <b>coexist</b> with more traditional state storage methods.  As a rule, the system processes a number of entity types (for example: users, orders, goods, ...) and it is quite possible that event registration will be expedient only for some of these categories.  It is important to note that here we are not faced with the choice of ‚Äúall or nothing‚Äù;  it's just about the additional ability to manage the state in our application. <br><br>  <b>Storing events in Kafka</b> <br><br>  The first problem that needs to be solved: how to store events in Kafka?  There are three possible strategies: <br><br><ul><li>  Store all events for all types of entities in a <b>single topic</b> (with multiple segments) </li><li>  According to topic-to-each-type-entity, i.e., we put in a separate topic all the events related to the user, to a separate topic - everything related to the product, etc. </li><li>  According to topic-to-entity, i.e., on a separate topic for each specific user and each item of goods </li></ul><br>  The third strategy (on-topic-on-essence) is practically impracticable.  If, when each new user appeared in the system, he would have to start a separate topic, soon the number of topics would become unlimited.  Any aggregation in this case would be very difficult, for example, it would be difficult to index all users in a search engine;  not only that at the same time would have to consume a huge number of topics - so also not all of them would be known in advance. <br><br>  Therefore, it remains to choose between 1 and 2. Both options have their advantages and disadvantages.  Having a single topic, it's easier to get a <b>global view</b> of all events.  On the other hand, highlighting the topic for each type of entity, you can scale and segment the flow of each entity separately.  The choice of one of two strategies depends on the specific use case. <br><br>  In addition, you can implement both strategies at once, if you have additional storage space: to produce topics by the type of entities from one comprehensive topic. <br><br><img src="https://habrastorage.org/webt/1i/lg/v4/1ilgv4fs1_uoaw6uximo7fy9e7k.png"><br><br>  In the rest of the article, we will work with only one type of entity and a single topic, although the stated material is easy to extrapolate and apply to work with many topics or types of entities. <br><br>  (READ: as <a href="https://twitter.com/huntchr/status/970964561498054656">Chris Hunt</a> noted, there is <a href="https://www.confluent.io/blog/put-several-event-types-kafka-topic/">an excellent article by</a> <a href="https://www.piter.com/collection/all/product/vysokonagruzhennye-prilozheniya-programmirovanie-masshtabirovanie-podderzhka">Martin Kleppman</a> , which discusses in detail how to distribute events across topics and segments). <br><br>  <b>Simplest storage operations in the event logging paradigm</b> <br><br>  The simplest operation that is logical to expect from a repository that supports event registration is to read the ‚Äúcurrent‚Äù (minimized) state of a particular entity.  As a rule, each entity has one or another <code>id</code> .  Accordingly, knowing this <code>id</code> , our storage system must return the current state of the object. <br><br>  The event log will serve us as the ultimate truth: the current state can always be derived from the stream of events associated with a particular entity.  For this, the database engine will require a pure function (without side effects), which accepts the event and the initial state and returns the changed state: <code>Event = &amp;gt State =&amp;gt State</code> .  With such a function and the <b>values ‚Äã‚Äãof the initial state, the</b> current state is a <b>convolution</b> of the event flow (the state change function must be <b>clean</b> so that it can be repeatedly applied to the same events.) <br><br>  The simplified implementation of the ‚Äúread the current state‚Äù operation in Kafka collects a stream from <b>all</b> events from the topic, filters them, leaving only the events with the given <code>id</code> and collapses with the help of the specified function.  If there are a lot of events (and over time the number of events only grows), this operation can become slow and consume a lot of resources.  Even if its result will be cached in memory and stored on the service node, this information will still have to be periodically recreated, for example, due to node failures or due to crowding out the cache data. <br><br><img src="https://habrastorage.org/webt/r5/te/aa/r5teaa64otzjedcvs0g1snt9lj8.png"><br><br>  Therefore, we need a more rational way.  This is where kafka-streams and state stores will come in handy.  Kafka-streams applications run on a whole cluster of nodes that consume certain topics together.  Each node is assigned a number of segments of consumed topics, just as is the case with the conventional Kafka consumer account.  However, kafka-streams provides higher-level operations on data, with which it is much easier to create derived streams. <br><br>  One of these operations in <a href="https://kafka.apache.org/documentation/streams/">kafka-streams</a> is a convolution of the stream in the local storage.  Each local storage contains data only from those segments that are consumed by a given node.  Out of the box, two local storage implementations are available: <i>in RAM</i> and based on <i>RocksDB</i> . <br><br>  Returning to the event registration topic, we note that it is possible to minimize the flow of events in <b>the state store</b> , keeping the ‚Äúcurrent state‚Äù of each entity in the local node from the segments assigned to the node.  If we use the RocksDB-based state storage implementation, it depends only on the amount of disk space how many entities we can track on a single node. <br><br>  Here is what the convolution of events in the local storage looks like when using the Java API (serde means "serializer / deserializer"): <br><br><pre> <code class="java hljs">KStreamBuilder builder = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> KStreamBuilder(); builder.stream(keySerde, valueSerde, <span class="hljs-string"><span class="hljs-string">"my_entity_events"</span></span>) .groupByKey(keySerde, valueSerde) <span class="hljs-comment"><span class="hljs-comment">//  :     .reduce((currentState, event) -&gt; ..., "my_entity_store"); .toStream(); //     return builder;</span></span></code> </pre> <br>  A complete example with <a href="https://github.com/confluentinc/kafka-streams-examples/tree/4.0.0-post/src/main/java/io/confluent/examples/streams/microservices">microservice-based order processing</a> is available on the Confluent website. <br><br>  (READ: as noted by <a href="https://twitter.com/bsideup/status/970717670881538048">Sergey Egorov</a> and <a href="https://twitter.com/iNikem/status/970880555922444288">Nikita Salnikov</a> on Twitter, for the system with event registration, you will probably need to change the default data storage settings in Kafka so that no limits can be applied either in time or in size, as well as optional , enable data compression.) <br><br>  <b>View current status</b> <br><br>  We have created a state repository, where the actual states of all the entities coming from the segments assigned to the node are located, but how now to request this repository?  If the request is local (that is, it originates from the same node on which the storage is located), then everything is quite simple: <br><br><pre> <code class="java hljs">streams .store(<span class="hljs-string"><span class="hljs-string">"my_entity_store"</span></span>, QueryableStoreTypes.keyValueStore()); .get(entityId);</code> </pre> <br>  But what if we want to request data located on another node?  And how to figure out what kind of node?  Here we have another opportunity that has recently appeared in Kafka: <b>interactive requests</b> .  With their help, you can access the Kafka metadata and find out which node processes the topic segment with the given <code>id</code> (in this case, the tool is used implicitly for the topic segmentation): <br><br><pre> <code class="java hljs">metadataService .streamsMetadataForStoreAndKey(<span class="hljs-string"><span class="hljs-string">"my_entity_store"</span></span>, entityId, keySerde)</code> </pre> <br>  Next, you need to somehow redirect the request to the correct node.  Please note: the specific way in which inter-node communication is implemented and processed ‚Äî be it REST, akka-remote, or any other ‚Äî is not the responsibility of kafka-streams.  Kafka simply provides access to the state store and provides information on which node the state store for the given <code>id</code> . <br><br>  <b>Recovery after failure</b> <br><br>  State repositories look pretty, but what happens if a node fails?  Recreating a local state repository for a given segment can also be a costly operation.  It can for a long time provoke increased latency or loss of requests, since kafka-streams will need rebalancing (after adding or deleting a node). <br><br>  That is why, by default, long-term state stores are logged: that is, all changes made to the store are additionally recorded in the changelog-topic.  This topic is compressed (after all, for each <code>id</code> we are only interested in the last entry, without a history of changes, since the history is stored in the events themselves) - therefore, it is as small as possible.  That is why recreating a repository on another node can occur much faster. <br><br>  However, with rebalancing in this case, delays are still possible.  To reduce them even more, kafka-streams provides the ability to keep multiple <b>backup replicas</b> ( <code>num.standby.replicas</code> ) for each repository.  These replicas apply all updates retrieved from topics with change logs as they arrive, and are ready to switch to the main state storage for a given segment as soon as the current main storage fails. <br><br>  <b>Consistency</b> <br><br>  With the default settings, Kafka provides at least one-time delivery.  That is, in case of node failure, some messages may be delivered several times.  For example, it is possible that a specific event will be applied twice to the state store if the system crashes after the state store has recorded changes in the state store, but before the offset has been made for that particular event.  Perhaps this will not cause any difficulties: our state update function ( <code>Event = &amp;gt State =&amp;gt State</code> ) can handle such situations quite normally.  However, it may not cope: in such a case, you can use the guarantees of <a href="https://softwaremill.com/what-kafka-exactly-once-really-means/">strictly single delivery</a> in Kafka.  Such guarantees apply only when reading and writing Kafka topics, but this is exactly what we are doing here: <a href="https://www.confluent.io/blog/enabling-exactly-kafka-streams/">in the background, all entries in Kafka topics come down to updating the change log for the state store</a> and performing offsets.  All this can be done <b>in the form of transactions</b> . <br><br>  Therefore, if our state update function requires this, we can enable the semantics of processing ‚Äúone-time delivery‚Äù streams using a single configuration option: <code>processing.guarantee</code> .  Because of this, productivity drops, but nothing comes for nothing. <br><br>  <b>Hearing of events</b> <br><br>  Now that we‚Äôve covered the basics ‚Äî querying the ‚Äúcurrent state‚Äù and updating it for each entity ‚Äî what about triggering <b>side effects</b> ?  At some point it will become necessary, for example, for: <br><br><ul><li>  Sending notification emails </li><li>  Indexing entities in a search engine </li><li>  Call external services via REST (or SOAP, CORBA, etc.) </li></ul><br>  All these tasks are blocking to one degree or another and are associated with I / O operations (this is natural for side effects), so perhaps not the best idea to perform them within the state updating logic: as a result, the frequency of failures in the main loop may increase events, and in terms of performance there will be a bottleneck. <br><br>  Moreover, the function with the state update logic (E <code>Event = &amp;gt State =&amp;gt State</code> ) can be run multiple times (in case of failures or restarts), and more often we want to minimize the number of cases in which side effects for a particular event are run repeatedly. <br><br>  Fortunately, since we are working with Kafka tops, we have a fair amount of flexibility.  At the flow stage where the state storage is updated, events can be emitted in an unmodified (or, if necessary, in a modified) form, and the resulting stream / topic (in Kafka, these concepts are equivalent) can be consumed as you like.  Moreover, it can be consumed either before or after the state update stage.  Finally, we can also control how we will trigger side effects: at least once or at most once.  The first option is provided if you perform the offset of the consumed topic-event only after all the side effects have been successfully completed.  Conversely, with a maximum of one-time launch, we perform offsets before triggering side effects. <br><br>  There are several options for starting side effects, they depend on the specific practical situation.  First of all, it is possible to determine the Kafka-streams stage, where side effects for each event are triggered as part of the stream processing function. <br>  It is quite simple to set up such a mechanism, but this decision is inflexible when it comes to retrying, managing displacements and competing displacements at once for many events.  In such more complex cases, it is more appropriate to determine processing using, say, a <a href="https://github.com/akka/alpakka-kafka">reactive-kafka</a> or other mechanism that consumes Kafka topics "directly." <br><br>  It is also possible that one event will <b>trigger other events</b> ‚Äî for example, an order event may trigger the ‚Äúpreparation for dispatch‚Äù and ‚Äúclient notification‚Äù events.  This can also be implemented at the kafka-streams stage. <br><br>  Finally, if we wanted to save events or certain data extracted from events in a database or a search engine, say, in ElasticSearch or PostgreSQL, we could use the <a href="https://www.confluent.io/hub/">Kafka Connect</a> connector, which will process all the details associated with the consumption of topics for us. <br><br>  <b>Creating views and projections</b> <br><br>  Normally, system requirements are not limited to requesting and processing only single entity streams.  Also should be supported aggregation, a combination of multiple streams of events.  Such combined streams are often referred to as <b>projections</b> , and when minimized they can be used to create <b>representations of data</b> .  Is it possible to implement them with Kafka? <br><br><img src="https://habrastorage.org/webt/yc/r2/jt/ycr2jtvibrdg7wy0lhin1ehwu1y.png"><br><br>  Again - yes!  Remember that basically we are dealing simply with the Kafka topic, where our events are stored;  Consequently, we have all the power of Kafka raw consumers / producers, kafka-streams combinator and even <a href="https://www.confluent.io/product/ksql/">KSQL</a> - all this will be useful to us for defining projections.  For example, using kafka-streams, you can filter the stream, display, group by key, aggregate in temporal or session windows, etc.  either at the code level, or using an SQL-like KSQL. <br><br>  Such streams can be stored and provided for a long time for requests using state stores and interactive requests, just as we did with separate entity streams. <br><br>  <b>What's next</b> <br><br>  To prevent the infinite growth of the flow of events as the system develops, such a compression option, such as saving <b>snapshots of the</b> ‚Äúcurrent state‚Äù, can be useful.  Thus, we can limit ourselves to storing only a few recent snapshots and the events that occurred after they were created. <br><br>  Although, Kafka does not have direct support for snapshots (and in some other systems operating on the principle of event registration, it does exist), you can definitely add this kind of functionality by using some of the mechanisms mentioned above, such as threads, consumers, state stores, etc. d. <br><br>  <b>Summary</b> <br><br>  Although initially Kafka was not designed with an eye on the event registration paradigm, in fact it is a stream processing engine with support for <b>topic replication</b> , segmentation, <b>state storage</b> and <b>streaming APIs</b> , and is very flexible at the same time.  Therefore, on top of Kafka, you can easily implement an event recording system.  Moreover, since, against the background of everything that is happening, we will always have a Kafka topic, we will gain additional flexibility, as we can work with either high-level streaming APIs or low-level consumers. </div><p>Source: <a href="https://habr.com/ru/post/424739/">https://habr.com/ru/post/424739/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../424729/index.html">Why did the compiler turn my loop into an infinite condition?</a></li>
<li><a href="../424731/index.html">Hot tech support history, or Why does AutoCAD remove proxies?</a></li>
<li><a href="../424733/index.html">Blue pill (blue tablet) STM32F103 as a PLC</a></li>
<li><a href="../424735/index.html">How does it work, and does conversational psychotherapy work at all</a></li>
<li><a href="../424737/index.html">42nd Protocol of life, the universe and all of this: "parting words"</a></li>
<li><a href="../424741/index.html">Guys, let's live together or on the "Password" field when registering</a></li>
<li><a href="../424745/index.html">The activity of the "GosSOPKI" has increased</a></li>
<li><a href="../424747/index.html">The place where the sound lives</a></li>
<li><a href="../424751/index.html">How does the Unified Biometric System work?</a></li>
<li><a href="../424753/index.html">What's new in YouTrack 2018.3</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>