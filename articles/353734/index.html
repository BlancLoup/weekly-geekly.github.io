<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Concepts of distributed architecture, which I met when building a large payment system</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I joined Uber two years ago as a mobile developer with some backend development experience. Here I was engaged in the development of functional paymen...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Concepts of distributed architecture, which I met when building a large payment system</h1><div class="post__text post__text-html js-mediator-article">  I joined Uber two years ago as a mobile developer with some backend development experience.  Here I was engaged in the development of functional payments in the application - and along the way I <a href="https://eng.uber.com/new-rider-app/">rewrote the application itself</a> .  After that, I <a href="http://blog.pragmaticengineer.com/things-ive-learned-transitioning-from-engineer-to-engineering-manager/">moved to the management of developers</a> and headed the team itself.  Thanks to this, I was able to get much closer acquainted with the backend, since my team is responsible for many of our backend systems that allow making payments. <br><br>  <b>Before my work at Uber, I had no experience with distributed systems.</b>  I received a traditional education in Computer Science, after which I worked in full-stack development for a dozen years.  Therefore, even though I could draw various diagrams and talk about <i>tradeoffs</i> in the systems, by that time I didn‚Äôt understand and understood concepts of distribution, such as <i>consistency</i> , <i>availability</i> or idempotency. . <br><br>  In this post, I‚Äôm going to talk about several concepts that I needed to learn and put into practice when building a large-scale, highly available distributed payment system that today works in Uber.  This is a system with a load of up to several thousand requests per second, in which the critical functionality of payments should work correctly, even in those cases when individual parts of the system stop working. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Is this a complete list?  Most likely no.  However, if I personally knew about these concepts earlier, it would make my life much easier. <br><br>  So, let's get down to our immersion in the SLA, consistency, durability of data, message integrity, idempotency, and some other things that I needed to learn in my new job. <br><a name="habracut"></a><br><h2>  SLA </h2><br>  In large systems that process millions of events a day, some things are simply by definition not required to go according to plan.  That is why, before diving into system planning, the most important step is to take a decision on what the ‚Äúhealthy‚Äù system means to us.  The degree of "health" should be something that can <i>actually</i> be measured.  <a href="https://en.wikipedia.org/wiki/Service-level_agreement">SLA</a> ( <i>service level agreements</i> ) are the generally accepted way to measure the ‚Äúhealth‚Äù of a system.  Here are some of the most common types of SLA I've encountered in practice: <br><br><ul><li>  <b>Availability</b> : the percentage of time that the service is operational.  Let there be a temptation to achieve 100% accessibility, the achievement of this result can be a truly difficult task, and, moreover, very expensive.  Even large and critical systems like a network of VISA, Gmail or Internet service providers do not have 100% availability - over the years they accumulate seconds, minutes or hours spent in downtime.  For many systems, availability at four nines (99.99%, or roughly <a href="https://uptime.is/">50 minutes of downtime per year</a> ) is considered high availability.  In order to get to this level, you have to pretty sweat. </li><li>  <b>Accuracy</b> : Is data loss acceptable or inaccurate?  If so, what percentage is acceptable?  For the payment system I worked on, this indicator should have been 100%, since it was impossible to lose data. </li><li>  <b>Capacity</b> : what load should the system withstand?  This indicator is usually expressed in requests per second. </li><li>  <b>Latency</b> : how long should the system respond?  How much time should be served 95% and 99% of requests?  In such systems, usually many of the queries are ‚Äúnoise‚Äù, so <a href="https://www.quora.com/What-is-p99-latency">p95 and p99 delays are</a> more practical in the real world. </li></ul><br>  <b><i>Why is SLA needed when creating a large payment system?</i></b>  We are creating a new system that replaces the existing one.  To make sure that we are doing everything right and that our new system will be ‚Äúbetter‚Äù than its predecessor, we used the SLA to determine our expectations from it.  Accessibility was one of the most important requirements.  Once we had a goal, we needed to deal with the trade-offs in architecture in order to achieve these indicators. <br><br><h2>  Horizontal and vertical scaling </h2><br>  As the business that uses our newly created system grows, the load on it will only increase.  At some point, the existing installation will not be able to withstand a further increase in load, and we will need to increase the allowable load.  The two generally accepted scaling strategies are vertical or horizontal scaling. <br><br>  Horizontal scaling is to add more machines (or nodes) to the system to increase throughput ( <i>capacity</i> ).  Horizontal scaling is the most popular way of scaling distributed systems. <br><br>  Vertical scaling is essentially ‚Äúbuying a bigger / stronger machine‚Äù - a (virtual) machine with a larger number of cores, better computing power and more memory.  In the case of distributed systems, vertical scaling is usually less popular because it can be more expensive than horizontal scaling.  However, some well-known large sites, such as Stack Overflow, <a href="https://www.slideshare.net/InfoQ/scaling-stack-overflow-keeping-it-vertical-by-obsessing-over-performance">successfully scaled vertically</a> to match the load. <br><br>  <b><i>Why does scaling strategy make sense when you create a large payment system?</i></b>  We decided at an early stage that we would build a system that would scale horizontally.  Despite the fact that vertically scaling is permissible to use in some cases, our payment system has already reached the projected load by that time and we are pessimistic about the assumption that the only super-expensive mainframe can withstand this load today, not to mention the future .  In addition, there were several people in our team who worked for major payment service providers and had a negative experience trying to scale vertically even on the most powerful machines that could be bought with money in those years. <br><br><h2>  Consistency </h2><br>  The availability of any of the systems is important.  Distributed systems are often built from machines whose individual availability is lower than the availability of the entire system.  Let our goal is to build a system with 99.999% availability (downtime is approximately 5 minutes / year).  We use machines / nodes that have an average availability of 99.9% (they are in downtime for about 8 hours / year).  The direct way to achieve the availability indicator we need is to add several more such machines / nodes to the cluster.  Even if some of the nodes are ‚Äúdown‚Äù, others will remain in service and the overall availability of the system will be higher than the availability of its individual components. <br><br>  Consistency is a key issue in high-availability systems.  The system is consistent if all nodes see and return the same data at the same time.  Unlike our previous model, when we added more nodes to achieve greater availability, making sure that the system remains consistent is far from trivial.  To make sure that each node contains the same information, they must send messages to each other in order to be constantly synchronized.  However, messages sent by them to each other may not be delivered - they may be lost and some of the nodes may be inaccessible. <br><br>  Consistency is a concept that took me the most time to realize before I understood it and appreciated it.  There are <a href="https://en.wikipedia.org/wiki/Consistency_model">several types of consistency</a> , the most widely used in distributed systems is <i>strong consistency</i> , <i>weak consistency</i> and <a href="http://sergeiturukin.com/2017/06/29/eventual-consistency.html">eventual</a> <i>consistency</i> .  You can read a useful practical analysis of the advantages and disadvantages of each of the models <a href="https://hackernoon.com/eventual-vs-strong-consistency-in-distributed-databases-282fdad37cf7">in this article</a> .  Usually, the weaker the required level of consistency, the faster the system can work - but the more likely it is that it will not return the most recent data set. <br><br>  <b><i>Why consistency should be considered when creating a large payment system?</i></b>  Data in the system must be consistent.  But how agreed?  For some parts of the system, only strongly consistent data will do.  For example, we need to keep in a strongly coordinated form the information that the payment was initiated.  For other parts of the system that are not so important, consistency can ultimately be considered a reasonable compromise. <br><br>  It illustrates well this listing of recent transactions: they can be implemented using eventual <i>consistency</i> ‚Äî that is, the last transaction may appear in some parts of the system only some time later, but due to this the list request will return a result with less delay or will require less resources to perform. <br><br><h2>  Data durability </h2><br>  <a href="https://en.wikipedia.org/wiki/Durability_(database_systems)">Durability</a> means that as soon as data is successfully added to the data warehouse, it will be available to us in the future.  This will be true even if the nodes of the system go offline, they will fail or the data nodes will be damaged. <br><br>  Different distributed databases have different levels of data longevity.  Some of them support <i>data durability</i> at the machine / node level, others do it at the cluster level, and some do not provide this functionality out of the box.  Some form of replication is usually used to increase longevity ‚Äî if data is stored on several nodes and one of the nodes stops working, the data will still be available.  <a href="https://drivescale.com/2017/03/whatever-happened-durability/">Here is a good article</a> explaining why achieving longevity in distributed systems can be a serious challenge. <br><img src="https://habrastorage.org/getpro/habr/post_images/7e6/870/cbc/7e6870cbc95716accd7421321f911bd2.png" alt="image"><br>  <b><i>Why does data longevity matter when building a payment system?</i></b>  If the data is critical (for example, these are payments), then we cannot afford to lose them in many of the parts of our system.  The distributed data warehouses that we built had to maintain data longevity at the cluster level ‚Äî so even if the instances crash, completed transactions will persist.  Nowadays, most distributed data storage services ‚Äî like Cassandra, MongoDB, HDFS, or Dynamodb ‚Äî all maintain longevity at various levels and can all be configured to provide cluster-level longevity. <br><br><h2>  Preservation of messages (message persistence) and durability (durability) </h2><br>  Nodes in distributed systems perform calculations, store data, and send messages to each other.  A key characteristic of sending messages is how reliably these messages will arrive.  For critical systems, there is often a requirement that none of the messages be lost. <br><br>  In the case of distributed systems, <i>messaging is</i> usually performed using some distributed message service ‚Äî RabbitMQ, Kafka, or others.  These message brokers can support (or are configured to support) various levels of message delivery reliability. <br><br>  Preservation of the message means that when a failure occurs on the node processing the message, the message will still be available for processing after the problem is resolved.  Message longevity is typically used at the <a href="https://en.wikipedia.org/wiki/Message_queue">message queue</a> level.  With a durable message queue, if the queue (or node) goes offline when the message is sent, it will still receive the message when it returns to the online one.  A good detailed article on this issue <a href="https://developers.redhat.com/blog/2016/08/10/persistence-vs-durability-in-messaging/">is available at the link</a> . <br><img src="https://habrastorage.org/getpro/habr/post_images/25d/505/736/25d505736984bd712b4cca05c1ec4050.png" alt="image"><br>  <i><b>Why the safety and durability of messages are important in the construction of large payment systems?</b></i>  We had messages that we couldn‚Äôt afford to lose ‚Äî for example, a message that a person initiated payment for a trip payment.  This meant that the messaging system that we had to use had to work without loss: every message had to be delivered once.  However, creating a system that delivers each message <i>exactly</i> once than <i>at least</i> once - these are tasks that differ significantly in their difficulty.  We decided to implement a messaging system that delivers at least once, and chose a <i>messaging bus</i> , on top of which we decided to build it (we chose Kafka, creating a lossless cluster, which was required in our case). <br><br><h2>  Idempotency </h2><br>  In the case of distributed systems, anything may go wrong - connections may fall off in the middle or requests may time out.  Customers will repeat these requests frequently.  The idempotent system ensures that no matter what happens, and no matter how many times a specific query is executed, the actual execution of this query occurs only once.  A good example is making a payment.  If the client creates a request for payment, the request is successful, but if the client is in time out, then the client can repeat the same request.  In the case of an idempotent system, money will not be written off twice from the person making the payment;  but for a non-idemonet system this is quite possible. <br><br>  Designing idempotent distributed systems requires some kind of distributed locking strategy.  This is where the concepts we discussed earlier come into play.  Let's say we intend to implement idempotency with optimistic locking to avoid parallel updates.  In order for us to resort to optimistic blocking, the system must be strictly consistent - so that during the execution of the operation we can check whether another operation was started using some form of versioning. <br><br>  There are many ways to achieve idempotency, and each particular choice will depend on the limitations of the system and the type of operation performed.  Designing idempotent approaches is a worthy challenge for a developer ‚Äî just look at <a href="https://www.bennadel.com/blog/3390-considering-strategies-for-idempotency-without-distributed-locking-with-ben-darfler.htm">Ben Nadel's</a> posts <a href="https://www.bennadel.com/blog/3390-considering-strategies-for-idempotency-without-distributed-locking-with-ben-darfler.htm">, in which he talks about the different strategies he used</a> , which include both distributed locks and database <i>constraints</i> .  When you design a distributed system, idempotency can easily be one of the parts that you have missed from your attention.  In our practice, we have come across cases in which my team ‚Äúburned out‚Äù because it was not convinced that there was a correct idempotency for some key operations. <br><br>  <b><i>Why does idempotency matter when building a large payment system?</i></b>  Most importantly: to avoid double debits and double refunds.  Given that our messaging system has a ‚Äúat least once, without loss‚Äù type of delivery, we must assume that all messages can be delivered several times and the systems must guarantee idempotency.  We made decisions to handle this using versioning and optimistic locking, where our systems implement idempotent behavior using strictly consistent storage as their data source. <br><br><h2>  Sharding and Quorum </h2><br>  Distributed systems often need to store much more data than a single node can afford.  So how do we save the data set on the right number of machines?  The most popular technique for this is <a href="https://en.wikipedia.org/wiki/Shard_(database_architecture)">sharding</a> .  The data is horizontally partitioned using some kind of hash assigned to the partition.  Let many distributed databases today implement sharding in their ‚Äúunder the hood‚Äù, it is in itself an interesting topic that should be studied - especially <a href="https://medium.com/%40jeeyoungk/how-sharding-works-b4dec46b3f6">resharing</a> .  In 2010, Foursquare had a 17-hour downtime due to hitting a regional sharding event, after which the company shared an <a href="http://highscalability.com/blog/2010/10/15/troubles-with-sharding-what-can-we-learn-from-the-foursquare.html">interesting post-mortem</a> that sheds light on the root of the problem. <br><br>  Many distributed systems have data or calculations that replicate across multiple nodes.  In order to make sure that operations are performed in a consistent manner, a voting approach is defined, in which, in order to recognize the operation as successful, it is necessary that a certain number of nodes get the same result.  This process is called quorum. <br><br>  <b><i>Why quorum and sharding make sense when building a large payment system in Uber?</i></b>  Both of these concepts are simple and are used almost everywhere.  I met them when we set up replication in Cassandra.  Cassandra (and other distributed systems) <a href="https://docs.datastax.com/en/archived/cassandra/3.x/cassandra/dml/dmlConfigConsistency.html">uses quorum</a> and local quorum to ensure consistency between clusters. <br><br><h2>  Actor model </h2><br>  The familiar vocabulary we use to describe programming practices ‚Äî things like variables, interfaces, method calls ‚Äî involves systems from one machine.  When we talk about distributed systems, we have to use other approaches.  A common way to describe such systems is the <a href="https://en.wikipedia.org/wiki/Actor_model">model of actors</a> , within which the code is seen in terms of communication.  This model is popular due to the fact that it coincides with the mental model of how we envision, for example, the interaction of people in an organization.  Another equally popular way of describing distributed systems is CSP, <a href="https://en.wikipedia.org/wiki/Communicating_sequential_processes">interacting sequential processes</a> . <br><br>  The model of actors is based on the actors who send each other messages and react to them.  Each actor can do a limited set of things ‚Äî create other actors, send messages to others, or decide what to do with the next message.  With the help of a few simple rules, we can fairly well describe complex distributed systems that can restore themselves after the actor falls.  If you are not familiar with this approach, then I recommend you an article <a href="https://www.brianstorti.com/the-actor-model/">Model of actors for 10 minutes</a> by <a href="https://twitter.com/brianstorti">Brian Storty</a> .  For many languages, there are <a href="https://en.wikipedia.org/wiki/Actor_model">libraries or frameworks that implement the model of actors</a> .  For example, in Uber, we use <a href="https://doc.akka.io/docs/akka/2.4/intro/what-is-akka.html">Akka</a> for some of our systems. <br><br>  <b><i>Why does it make sense to apply the model of actors in a large payment system?</i></b>  Many engineers have participated in the development of our system, most of whom have already had experience working on distributed systems.  We decided to follow the standard distributed model instead of engaging in ‚Äúbikes‚Äù and inventing our own distributed principles. <br><br><h2>  Reactive architecture </h2><br>  When building large distributed systems, the goal is usually to make them fault-tolerant, resilient, and scalable.  Whether it is a payment system or some other high-load system, the patterns for achieving the desired can be the same.  Those who deal with such systems regularly discover and disseminate the best practices of their construction - and the reactive architecture is a similar popular and widely used pattern. <br><br>  To get acquainted with the reactive architecture, I suggest reading the <a href="https://www.reactivemanifesto.org/">Reactive Manifest</a> ( <a href="https://habrahabr.ru/post/195562/">in Russian</a> ) and watching the 12-minute video <a href="https://www.lightbend.com/blog/understand-reactive-architecture-design-and-programming-in-less-than-12-minutes">here</a> . <br><br>  <b><i>Why does it make sense to use a reactive architecture if you create a large payment system?</i></b>  <a href="https://akka.io/">Akka</a> , the library we used to create most of our new payment system, is heavily influenced by reactive architecture.  Many of our engineers involved in building this system were already familiar with the best practices of reactive programming.  Following the reactive principles - creating a responsive, fault-tolerant, flexible <i>message-driven</i> system based on messages, we arrived at this conclusion in a natural way.  The ability to have a model on which you can rely and with which you can assess the progress of development and its direction, has proved extremely useful, and I will rely on it in the future when creating new systems. <br><br><h2>  Conclusion </h2><br>  I was lucky to take part in the restructuring of a large-scale, distributed and critical system: the one that allows you to work with payments in Uber.  Working in this environment, I became acquainted with many distributed concepts that I had not used before.  I collected them here in the hope that others will find my story useful for starting the study of distributed systems or learn something new for themselves. <br><br>  This post was devoted exclusively to the planning and architecture of such systems.  There are many different subtleties that are worth telling about building, deploying and migrating high load systems, and also about their reliable operation;  I am going to raise all these topics in subsequent posts. </div><p>Source: <a href="https://habr.com/ru/post/353734/">https://habr.com/ru/post/353734/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../353724/index.html">Why it is not necessary to always obtain consent for the processing of personal data within the framework of the GDPR</a></li>
<li><a href="../353726/index.html">The device of special effects for games under NES. Part 1</a></li>
<li><a href="../353728/index.html">How to increase the number of friends in the company</a></li>
<li><a href="../353730/index.html">Mikrotik RoS, useful things</a></li>
<li><a href="../353732/index.html">Crypto Trading Automation with Django and Celery</a></li>
<li><a href="../353736/index.html">Digest news from the world of PostgreSQL. Issue number 5</a></li>
<li><a href="../353738/index.html">Secure SOCKS5 proxy for Telegram for 1 Euro and 10 minutes</a></li>
<li><a href="../353740/index.html">Global lighting using voxel cone tracing</a></li>
<li><a href="../353744/index.html">Red Hat is heading for a hybrid cloud with Enterprise Linux 7.5: what does it mean</a></li>
<li><a href="../353746/index.html">How Sberbank Online applications work: Workflow API and frameworks</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>