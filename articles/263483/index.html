<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Improving the convenience of working with Android applications: gesture recognition and more</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The user turned the phone when received a call? Remove the sound. The device is raised as if they want to take a photo? Turn on, if for us it is not d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Improving the convenience of working with Android applications: gesture recognition and more</h1><div class="post__text post__text-html js-mediator-article">  The user turned the phone when received a call?  Remove the sound.  The device is raised as if they want to take a photo?  Turn on, if for us it is not done in the old manner, the camera.  How?  Sensors to help us. <br><br> <a href="http://habrahabr.ru/company/intel/blog/263483/"><img src="https://habrastorage.org/getpro/habr/post_images/2aa/583/06e/2aa58306ecdd1e5d20f8a63bad007dd0.jpg" alt="image"></a> <br><a name="habracut"></a><br><h2>  <font color="#0071c5">Overview</font> </h2><br>  In mobile devices, there are usually quite a few sensors.  Among them you can find an accelerometer, gyroscope, magnetometer, barometer, light sensor and others. <br><br>  Owners of smartphones and tablets physically interact with them: move, shake, tilt.  The information that comes from different sensors is well suited for recognizing actions taking place in the physical world.  When the action is recognized, you can respond to it.  Using sensor readings when developing applications allows you to equip programs with truly user-friendly features that will not leave users indifferent. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      With the release of the Intel Context Sensing SDK for Android * v1.6.7, application creators have the opportunity to work with several new types of context-sensitive data.  That is, data based on information about the environment and the user's actions.  Among them is the position of the device in space (position), lifting it to the ear, as at the beginning of a conversation (ear touch), fast moving and returning to its original position (flick).  Do not confuse the move gesture in space with the eponymous flick, related to the work with the touch display.  Also, the new library supports the recognition by the device of drawing various characters (glyphs) in the air. <br><br>  From this material you will learn how to extract from the sensor readings valuable information about what is happening with the device.  In addition, we will look at examples using the Context Sensing SDK in detecting movements, device shakes, and character recognition. <br><br>  However, Context Sensing SDK, even if limited to questions of the spatial position of the device, can be much more.  For example, to determine what kind of physical activity a user is busy with.  Is he walking, resting, riding a bike?  Or maybe running, traveling by car or by train?  Having such information about what is happening, you can take the interaction of the user and his mobile device on which your application is installed to a whole new level. <br><br><h2>  <font color="#0071c5">Preliminary Information</font> </h2><br>  A common question that arises when working with sensors is to connect them to the application processor (AP) at the hardware level.  Below you can see three ways to connect.  Namely, it is a direct connection, the use of a dedicated sensor hub (discrete sensor hub) and a built-in processor hub (integrated sensor hub, ISH). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a06/244/6b5/a062446b5f41b2afbba9cf786a0c2fda.png"><br>  <em><font color="#999999">Comparison of different approaches to pairing sensors with a processor application.</font></em> <br><br>  If the sensors are attached directly to the AP, this is called a direct connection.  However, there is one problem.  It lies in the fact that the sensors consume processor resources to perform measurements. <br><br>  The next more advanced method is to use a dedicated sensor hub.  As a result, sensors can operate continuously, without overloading the processor.  Even if the processor goes into sleep mode ( <a href="http://en.wikipedia.org/wiki/Advanced_Configuration_and_Power_Interface">S3</a> ), the sensor hub can wake it up using the interrupt mechanism. <br><br>  The next step in the development of interaction between processors and sensors is to use an integrated hub.  This, among other benefits, leads to a decrease in the number of discrete components used and to a decrease in the cost of the device. <br>  A sensor hub is essentially a microchip that serves to organize the interfacing of multiple devices (Multipoint Control Unit, MCU).  For it, you can write programs in C / C ++ languages ‚Äã‚Äãand upload the compiled code to the MCU. <br><br>  In 2015, Intel launches the CherryTrail-T platform, designed for tablets, and the SkyLake platform, designed for two-in-one devices.  These solutions use sensor hubs.  You can learn more about integrated hubs by following the <a href="http://ishfdk.iil.intel.com/download">link</a> . <br><br>  Below the sensor coordinate system.  In particular, it shows an accelerometer, which is able to measure acceleration along the X, Y and Z axes, and a gyroscope that tracks the device‚Äôs position in space, in particular, turns around the same axes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/982/c0a/eec/982c0aeec8fd34dbc26ffa9d59c2f31d.png"><br>  <em><font color="#999999">The coordinate systems of the accelerometer and gyroscope.</font></em> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b9f/aa1/2cc/b9faa12cc998be8fd2766e8c81c3b430.png"><br>  <em><font color="#999999">Acceleroration values ‚Äã‚Äãalong the accelerometer axes at different positions of the device at rest.</font></em>  <em><a href="http://cache.freescale.com/files/sensors/doc/app_note/AN4317.pdf">Read in PDF</a> <font color="#999999">, without registration and SMS.</font></em> <br><br>  The table shows the new events caused by the physical movements of devices included in the Android OS Lollipop. <br><br>  <b>New events supported by Android Lollipop</b> <br><table><tbody><tr><td>  <b>Title</b> <br></td><td>  <b>Description</b> <br></td></tr><tr><td width="330">  SENSOR_STRING_TYPE_PICK_UP_GESTURE <br></td><td>  Called when the device is taken in hand, no matter where it was before (on the table, in your pocket, in your bag) <br></td></tr><tr><td>  SENSOR_STRING_TYPE_GLANCE_GESTURE <br></td><td>  It allows, based on a specific movement, to turn on the screen for a short time so that the user can look at it. <br></td></tr><tr><td>  SENSOR_STRING_TYPE_WAKE_GESTURE <br></td><td>  Allows you to unlock a device based on the specific movement of this device in space. <br></td></tr><tr><td>  SENSOR_STRING_TYPE_TILT_DETECTOR <br></td><td>  The corresponding event is generated each time the device is tilted. <br></td></tr></tbody></table>  The definition of these events can be found in the Android Lollipop source folder: /hardware/libhardware/include/hardware/sensor.h. <br><br><h2>  <font color="#0071c5">Gesture recognition process</font> </h2><br>  The process of gesture recognition can be divided into the following stages: preliminary processing of initial data (preprocessing), selection of characteristic features (feature extraction) and comparison with templates (template matching). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3cb/190/201/3cb1902016d99eca72b5aeeae88fbd9e.png"><br>  <em><font color="#999999">The process of recognizing gestures.</font></em> <br><br>  Consider the stages of the gesture recognition process in more detail. <br><br><h2>  <font color="#0071c5">Preliminary processing of source data</font> </h2><br>  Pre-processing begins after raw data has been obtained from the sensor.  Below you can see a graphic representation of the data received from the gyroscope after the device was once quickly tilted to the right and returned to its former state (flick gesture).  The following is a graph for a similar gesture, but already constructed from the data obtained from the accelerometer. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/89d/f48/06b/89df4806b11300da690e28f11c4d97ce.png"><br>  <em><font color="#999999">Data obtained from the gyroscope (single tilt of the device to the right and a quick return to its original position, RIGHT FLICK ONCE).</font></em> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/922/d39/4e2/922d394e2c4f68755c778074e77ba1c6.png"><br>  <em><font color="#999999">The data obtained from the accelerometer (a single tilt of the device to the right and a quick return to its original position, RIGHT FLICK ONCE).</font></em> <br><br>  You can create a program that will send data received from the sensor on an Android device over the network, then write a script in Python * designed to work on a PC.  This allows you to receive, for example, from a smartphone, changing sensor data and build graphs. <br><br>  So, the following is involved in this step: <br><br><ul><li>  A computer running a Python script that receives data from sensors. </li><li>  An application that runs on a device under test (DUT-run application).  It collects information from sensors and sends it over the network. </li><li>  Android test bridge (android debug bridge, ADB), configured to send data to the device and receive it.  When you configure it, use the command: adb forward tcp: port tcp: port </li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/6c8/6b5/1a7/6c86b51a7cbeeaf3bf2af8cf8309d291.png"><br>  <em><font color="#999999">Dynamic display of data received from sensors.</font></em> <br><br>  At this stage, we remove the unusual values ‚Äã‚Äãof the signals, and, as is often done, we use a filter to suppress interference.  The graph below shows the sensor data obtained after the device was rotated 90 ¬∞, and then returned to its original position. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c30/950/3e0/c309503e0894c86ca010c3d59c0be96a.png"><br>  <em><font color="#999999">Elimination of gyro and noise.</font></em> <br><br><h2>  <font color="#0071c5">Selection of characteristic features</font> </h2><br>  Noise may be present in the signal that the sensor produces, this can affect the recognition results.  For example, characteristics such as FAR (False Acceptance Rate, False Pass Rate) and FRR (False Rejection Rate, False Rejection Rate) indicate the level of occurrence of failures in the recognition of signals.  Combining data from different sensors, we can improve the accuracy of event recognition.  Combining sensor data (sensor fusion; <a href="http://www.codeproject.com/Articles/729759/Android-Sensor-Fusion-Tutorial">once a</a> useful link and <a href="http://en.wikipedia.org/wiki/Sensor_fusion">two</a> ) has found application in many mobile devices.  Below is an example of using an accelerometer, a magnetometer and a gyroscope to obtain information about the orientation of the device in space.  Usually, in the process of extracting the characteristic features of signals, the FFT (Fast Fourier Transform, Fast Fourier Transform) method and zero-crossing analysis are used.  Accelerometer and magnetometer are exposed to electromagnetic radiation.  Usually these sensors need calibration. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/62b/ed3/26d/62bed326d9efe4100214ac0cdb646e38.png"><br>  <em><font color="#999999">Retrieving device orientation in space using a combination of sensor data.</font></em> <br><br>  Characteristic features of the signal include minimum and maximum values, peaks and valleys.  After receiving this information, proceed to the next step. <br><br><h2>  <font color="#0071c5">Pattern matching</font> </h2><br>  After analyzing the graph data from the accelerometer, you can find the following: <br><br><ul><li>  A typical tilt of the device to the right with returning to the initial position gives a graph with two troughs and one peak. </li><li>  The same gesture, but performed twice, contains three cavities and two peaks. </li></ul><br>  From this it follows that it is realistic to design a very simple gesture recognition system based on a finite state machine.  In comparison with the gesture recognition approach based on <a href="http://en.wikipedia.org/wiki/Hidden_Markov_model">hidden Markov models</a> (HMM), this approach is more reliable and has higher accuracy. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/568/bd3/f91/568bd3f91c7eaf76c9c8bff02e5cb09c.png"><br>  <em><font color="#999999">Charts of data from the accelerometer and gyroscope, obtained by a single or double tilt of the device with a return to its original position.</font></em> <br><br><h2>  <font color="#0071c5">Examples: Intel¬Æ Context Sensing SDK in action</font> </h2><br>  <a href="https://software.intel.com/sites/default/files/managed/01/37/Context-Sensing-SDK_ReleaseNotes_v1.6.7.pdf">The Intel Context Sensing SDK</a> uses information from sensors and acts as a data provider for context-oriented services.  Below you can see a diagram of the architecture of the system, on which the traditional application and the context-oriented application are presented. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bd9/331/3cd/bd93313cd8f51964a85f418346911709.png"><br>  <em><font color="#999999">Comparison of Intel Context Sensing SDK and traditional Android architecture.</font></em> <br><br>  Currently, the SDK supports character recognition, which you can draw with a device in the air (glyph), tilt gestures with a return to its original state (flick), and a touch gesture with an ear device (ear_touch).  The demonstration of these functions is implemented in the example ContextSensingApiFlowSample, which is designed to work with Android-devices. <br><br>  In order to experience this and other examples of using the Intel Context Sensing SDK, you need to download the Context Sensing SDK that comes with the Intel Integrated Native Developer Experience ( <a href="https://software.intel.com/en-us/context-sensing-sdk">Intel INDE</a> ).  After downloading and installing the package, provided you use the standard paths, everything you need can be found at C: \ Intel \ INDE \ context_sdk_1.6.7.x.  In particular, there is a intel-context-sensing-1.6.7.x.jar JAVA library for connecting to Android projects, and a Samples folder containing the code for Android demo applications. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/70b/6f4/ea2/70b6f4ea2fd03f065520dec426c2e9c0.png"><br>  <em><font color="#999999">Tilt gesture support in the Intel Context SDK.</font></em> <br><br>  The Intel Context Sensing SDK supports tilt gesture recognition with a return to its original state in four directions.  Namely - tilts left, right, up and down. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3de/b8c/573/3deb8c573b3f63b9c168c9e9322ee060.png"><br>  <em><font color="#999999">Ear Touch gesture support in the Intel Context SDK.</font></em> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7ea/486/0e4/7ea4860e45be27a513a534b253161dd7.png"><br>  <em><font color="#999999">Support for drawing characters in the air in the Intel Context SDK.</font></em> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/70a/78d/9a6/70a78d9a6bfb31f169af338ec3b83730.png"><br>  <em><font color="#999999">The ContextSensingApiFlowSample sample application using the Intel Context SDK.</font></em> <br><br>  Now let's take a look at the example application PhysicalActivitySensingSample.  As the name implies, it allows you to use the capabilities of the Intel Context SDK to recognize the user's physical activity (Activity Recognition).  The data from the sensors are analyzed, after which the system issues a forecast, indicating the probabilities for various types of activity as a percentage. <br>  Implemented an example in the form of an Eclipse project.  It can be imported into Android Studio, and in order for the code to work, you need to add the intel-context-sensing-1.6.7.x.jar library to the project and connect it to build.gradle: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c45/3c3/3ca/c453c33cad5993d00d195bcda2f9b006.png"><br>  <em><font color="#999999">Preparing the PhysicalActivitySensingSample project for launch in Android Studio.</font></em> <br><br>  The project contains two main elements.  The first is an Activity that implements the user interface.  The second is a class inherited from Application, containing several auxiliary inner classes.  Its main task is to support the work with the object of the com.intel.context.Sensing class, which we access by pressing buttons, and which provides information about the user's physical activity. <br>  The test took place on the ASUS Fonepad 8 tablet, which is based on the Intel Atom Z3530 CPU.  After starting the application, we need two buttons.  First you need to click the Start Daemon button, then - Enable Sensing (respectively, after completing the experiments - Disable Sensing and Stop Daemon).  Now you can expect a message with information about what, according to the program, we are busy. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/25a/1c7/481/25a1c74814219c1dd825cd0b24063370.png"><br>  <em><font color="#999999">Analysis of the user's physical activity.</font></em> <br><br>  During the experiments, the tablet was taken for a walk.  As can be seen from the message, the program suggested that the user, with a probability of 87%, walks on foot (the WALKING parameter).  If the device lies motionless, the system decides that the user is resting, setting a very high rate for the SEDENTARY parameter (100% with no movement at all).  If you shake and rotate the device randomly, you can usually see a fairly high indicator of the RANDOM parameter. <br><br><h2>  <font color="#0071c5">Results</font> </h2><br>  Sensors are widely used in modern computing systems.  Motion recognition implemented on their base, applied in mobile devices, can become a valuable competitive advantage of applications that attracts users.  Working with sensors is a very important feature that can significantly improve the usability of mobile devices and applications.  The recently released Intel Context Sensing SDK v1.6.7 allows you to reproach and simplify the creation of applications that use sensor data.  This is good for developers and for those who use their applications. <br><br><h2>  <font color="#0071c5">For home reading</font> </h2><br>  ¬ª <a href="https://source.android.com/devices/sensors/sensor-stack.html">Sensor stack</a> <br>  ¬ª <a href="https://graphics.ethz.ch/teaching/former/scivis_07/Notes/Slides/07-featureExtraction.pdf">Feature Extraction</a> </div><p>Source: <a href="https://habr.com/ru/post/263483/">https://habr.com/ru/post/263483/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../263473/index.html">SibirCTF 2015: how it was</a></li>
<li><a href="../263475/index.html">How I taught English and programming at the same time</a></li>
<li><a href="../263477/index.html">Summer series of webinars on new features RAD Studio XE8</a></li>
<li><a href="../263479/index.html">The rich, diverse, free web that I loved so much, for the years I spent in an Iranian prison, just died. Why no one will stop it?</a></li>
<li><a href="../263481/index.html">Azure RMS. Service features</a></li>
<li><a href="../263485/index.html">Cloudy, database on demand possible</a></li>
<li><a href="../263487/index.html">Vim in full: Depla</a></li>
<li><a href="../263489/index.html">Developer Path (Part 0x01)</a></li>
<li><a href="../263491/index.html">Analyzing large amounts of data with Apache Spark</a></li>
<li><a href="../263493/index.html">Implementation of the Hough Transform algorithm (Hough Transform) (+ work visualization)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>