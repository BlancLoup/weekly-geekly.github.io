<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Set up a Kubernetes HA cluster on bare metal with GlusterFS & MetalLB. Part 2/3</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Part 1/3 here 


 Hello and welcome back! This is the second part of an article about setting up a Kubernetes cluster on bare metal. Previously, we se...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Set up a Kubernetes HA cluster on bare metal with GlusterFS & MetalLB. Part 2/3</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/oa/xl/av/oaxlavwz_atdglepw3r_vn6hmxm.jpeg"></p><br><p>  <strong>Part 1/3</strong> <a href="https://habr.com/ru/company/southbridge/blog/439562/"><strong>here</strong></a> </p><br><p>  Hello and welcome back!  This is the second part of an article about setting up a Kubernetes cluster on bare metal.  Previously, we set up the Kubernetes ON-cluster using external etcd, master-lead schemes and load balancing.  Well, now it's time to set up an additional environment and utilities to make the cluster more useful and as close as possible to its working state. </p><br><p>  In this part of the article, we will focus on configuring the internal load balancer of cluster services ‚Äî this will be MetalLB.  We will also install and configure distributed file storage between our working nodes.  We will use GlusterFS for persistent volumes that are available in Kubernetes. <br>  After performing all the actions, the scheme of our cluster will look like this: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/_v/yp/pe/_vyppenp91uzmkowqv1qcyomnrc.jpeg"></a> </p><a name="habracut"></a><br><h3 id="1-nastroyka-metallb-v-kachestve-vnutrennego-balansirovschika-nagruzki">  1. Set up MetalLB as an internal load balancer. </h3><br><p>  A few words about MetalLB, directly from the document page: </p><br><blockquote>  MetalLB is a load balancer implementation for Kubernetes clusters on bare metal with standard routing protocols. <br><br>  <a href="https://kubernetes.io/">Kubernetes</a> does not offer an implementation of network load balancers ( <a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/">services like LoadBalancer</a> ) for bare metal.  All the options for implementing Network LB that Kubernetes ships with are the linking code, it refers to various IaaS platforms (GCP, AWS, Azure, etc.).  If you are not working on a platform supported by IaaS (GCP, AWS, Azure, etc.), the LoadBalancer will remain in the ‚Äúwaiting‚Äù state for an indefinite period when created. <br><br>  The operators of the BM servers have two less efficient tools for entering user traffic into their clusters, the NodePort and externalIPs services.  Both of these options have significant production flaws, which makes BM clusters second-class citizens in the Kubernetes ecosystem. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      MetalLB seeks to correct this imbalance by offering a Network LB implementation that integrates with standard network equipment, so external services on BM clusters also ‚Äújust work‚Äù at maximum speeds. </blockquote><p>  Thus, using this tool, we start services in the Kubernetes cluster using a load balancer, for which many thanks to the MetalLB team.  The setup process is really simple and straightforward. </p><br><p>  Earlier in the example, we chose the subnet 192.168.0.0/24 for the needs of our cluster.  Now take some of this subnet for the future load balancer. </p><br><p>  We log into the machine system with the <strong>kubectl</strong> utility <strong>configured</strong> and run: </p><br><pre><code class="plaintext hljs">control# kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.7.3/manifests/metallb.yaml</code> </pre> <br><p>  This will deploy MetalLB in a cluster, in the <code>metallb-system</code> .  Ensure that all MetalLB components are functioning normally: </p><br><pre> <code class="plaintext hljs">control# kubectl get pod --namespace=metallb-system NAME READY STATUS RESTARTS AGE controller-7cc9c87cfb-ctg7p 1/1 Running 0 5d3h speaker-82qb5 1/1 Running 0 5d3h speaker-h5jw7 1/1 Running 0 5d3h speaker-r2fcg 1/1 Running 0 5d3h</code> </pre> <br><p>  Now configure MetalLB using configmap.  In this example, we use the Layer 2 setting. For information about other setting options, refer to the MetalLB documentation. </p><br><p>  Create a file <strong>metallb-config.yaml</strong> in any directory within the selected IP range of the subnet of our cluster: </p><br><pre> <code class="plaintext hljs">control# vi metallb-config.yaml apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: default protocol: layer2 addresses: - 92.168.0.240-192.168.0.250</code> </pre> <br><p>  And apply this setting: </p><br><pre> <code class="plaintext hljs">control# kubectl apply -f metallb-config.yaml</code> </pre> <br><p>  If necessary, check and change the configmap later: </p><br><pre> <code class="plaintext hljs">control# kubectl describe configmaps -n metallb-system control# kubectl edit configmap config -n metallb-system</code> </pre> <br><p>  We now have our own customized local load balancer.  Let's check how it works, on the example of the Nginx service. </p><br><pre> <code class="plaintext hljs">control# vi nginx-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 control# vi nginx-service.yaml apiVersion: v1 kind: Service metadata: name: nginx spec: type: LoadBalancer selector: app: nginx ports: - port: 80 name: http</code> </pre> <br><p>  Then create a test deployment and Nginx service: </p><br><pre> <code class="plaintext hljs">control# kubectl apply -f nginx-deployment.yaml control# kubectl apply -f nginx-service.yaml</code> </pre> <br><p>  And now - check the result: </p><br><pre> <code class="plaintext hljs">control# kubectl get po NAME READY STATUS RESTARTS AGE nginx-deployment-6574bd76c-fxgxr 1/1 Running 0 19s nginx-deployment-6574bd76c-rp857 1/1 Running 0 19s nginx-deployment-6574bd76c-wgt9n 1/1 Running 0 19s control# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx LoadBalancer 10.100.226.110 192.168.0.240 80:31604/TCP 107s</code> </pre> <br><p>  Created 3 pod Nginx, as we indicated in the Deploy earlier.  The Nginx service will send traffic to all these hearths using a circular balancing scheme.  And you can also see the external IP received from our load balancer MetalLB. </p><br><p>  Now try to minimize to the IP address 192.168.0.240, and you will see the Nginx page index.html.  Do not forget to remove the test deployment and the Nginx service. </p><br><pre> <code class="plaintext hljs">control# kubectl delete svc nginx service "nginx" deleted control# kubectl delete deployment nginx-deployment deployment.extensions "nginx-deployment" deleted</code> </pre> <br><p>  Well, on this with MetalLB everything, we go ahead - let's configure GlusterFS for the Kubernetes volumes. </p><br><h3 id="2-nastroyka-glusterfs-s-heketi-na-rabochih-nodah">  2. Configure GlusterFS with Heketi on the working nodes. </h3><br><p>  In fact, the Kubernetes cluster cannot be used without volumes inside it.  As you know, pods are ephemeral, i.e.  They can be created and deleted at any time.  All data inside them will be lost.  Thus, in a real cluster, you will need distributed storage to ensure the exchange of settings and data between the nodes and applications within it. </p><br><p>  In Kubernetes volumes are available in various versions, choose the appropriate ones.  In this example, I will demonstrate how GlusterFS storage is created for any internal applications, it is like permanent volumes.  Earlier, I used the ‚Äúsystem‚Äù installation of GlusterFS on all Kubernetes work nodes for this, and then I just created volumes of the hostPath type in the GlusterFS directories. </p><br><p>  Now we have a new handy tool <a href="https://github.com/heketi/heketi"><strong>Heketi</strong></a> . </p><br><p>  A few words from the Heketi documentation: </p><br><blockquote>  RESTful-based volume management infrastructure for GlusterFS. <br><br>  Heketi offers a RESTful management interface that you can use to manage the life cycle of GlusterFS volumes.  Thanks to Heketi, cloud services, such as OpenStack Manila, Kubernetes, and OpenShift, can dynamically provide GlusterFS volumes with any type of reliability supported.  Heketi automatically determines the location of the blocks in the cluster, providing the location of the blocks and their replicas in different areas of failure.  Heketi also supports any number of GlusterFS clusters, allowing cloud services to offer network storage files, not limited to a single GlusterFS cluster. </blockquote><p>  It sounds good, and, in addition, this tool will bring our VM cluster closer to the large cloud clusters Kubernetes.  In the end, you can create <strong>PersistentVolumeClaims</strong> , which will be formed automatically, and much more. </p><br><p>  You can take additional system hard drives to configure GlusterFS or just create several dummy block devices.  In this example, I will use the second method. </p><br><p>  Create dummy block devices on all three working nodes: </p><br><pre> <code class="plaintext hljs">worker1-3# dd if=/dev/zero of=/home/gluster/image bs=1M count=10000</code> </pre> <br><p>  You will receive a file of about 10 GB.  Then use <strong>losttup</strong> - to add it to these nodes as a loopback device: </p><br><pre> <code class="plaintext hljs">worker1-3# losetup /dev/loop0 /home/gluster/image</code> </pre> <br><blockquote>  <em>Please note: if you already have some loopback device 0, then you will need to select any other number.</em> </blockquote><p>  I took the time and found out why Heketi does not want to work properly.  Therefore, to prevent any problems in future configurations, first make sure that we load the <strong>dm_thin_pool</strong> kernel <strong>module</strong> and install the <strong>glusterfs-client</strong> package on all working nodes. </p><br><pre> <code class="plaintext hljs">worker1-3# modprobe dm_thin_pool worker1-3# apt-get update &amp;&amp; apt-get -y install glusterfs-client</code> </pre> <br><p>  Ok, now you need to have the file <strong>/ home / gluster / image</strong> and the device <strong>/ dev / loop0</strong> on all the working nodes.  Remember to create a systemd service that will automatically launch <strong>losetup</strong> and <strong>modprobe</strong> each time these servers are loaded. </p><br><pre> <code class="plaintext hljs">worker1-3# vi /etc/systemd/system/loop_gluster.service [Unit] Description=Create the loopback device for GlusterFS DefaultDependencies=false Before=local-fs.target After=systemd-udev-settle.service Requires=systemd-udev-settle.service [Service] Type=oneshot ExecStart=/bin/bash -c "modprobe dm_thin_pool &amp;&amp; [ -b /dev/loop0 ] || losetup /dev/loop0 /home/gluster/image" [Install] WantedBy=local-fs.target</code> </pre> <br><p>  And turn it on: </p><br><pre> <code class="plaintext hljs">worker1-3# systemctl enable /etc/systemd/system/loop_gluster.service Created symlink /etc/systemd/system/local-fs.target.wants/loop_gluster.service ‚Üí /etc/systemd/system/loop_gluster.service.</code> </pre> <br><p>  The preparatory work is completed, and we are ready to deploy GlusterFS and Heketi to our cluster.  For this, I will use this cool <a href="">guide</a> .  Most of the commands run from an external control computer, and very small commands run from any master node inside the cluster. </p><br><p>  First, copy the repository and create DaemonSet GlusterFS: </p><br><pre> <code class="plaintext hljs">control# git clone https://github.com/heketi/heketi control# cd heketi/extras/kubernetes control# kubectl create -f glusterfs-daemonset.json</code> </pre> <br><p>  Now let's mark our three working nodes for GlusterFS;  after marking them, GlusterFS will be created: </p><br><pre> <code class="plaintext hljs">control# kubectl label node worker1 storagenode=glusterfs control# kubectl label node worker2 storagenode=glusterfs control# kubectl label node worker3 storagenode=glusterfs control# kubectl get pod NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 1m6s glusterfs-hzdll 1/1 Running 0 1m9s glusterfs-p8r59 1/1 Running 0 2m1s</code> </pre> <br><p>  Now create a Heketi service account: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f heketi-service-account.json</code> </pre> <br><p>  We will ensure that this service account has the ability to manage gluster bogs.  To do this, create a cluster function that is required for our newly created service account: </p><br><pre> <code class="plaintext hljs">control# kubectl create clusterrolebinding heketi-gluster-admin --clusterrole=edit --serviceaccount=default:heketi-service-account</code> </pre> <br><p>  Now let's create a secret key, Kubernetes, which blocks the configuration of our Heketi instance: </p><br><pre> <code class="plaintext hljs">control# kubectl create secret generic heketi-config-secret --from-file=./heketi.json</code> </pre> <br><p>  Create the first source for Heketi, which we use for the first configuration operations and subsequently delete: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f heketi-bootstrap.json service "deploy-heketi" created deployment "deploy-heketi" created control# kubectl get pod NAME READY STATUS RESTARTS AGE deploy-heketi-1211581626-2jotm 1/1 Running 0 2m glusterfs-5dtdj 1/1 Running 0 6m6s glusterfs-hzdll 1/1 Running 0 6m9s glusterfs-p8r59 1/1 Running 0 7m1s</code> </pre> <br><p>  After creating and running the Bootstrap Heketi service, we will need to go to one of our master nodes, we will run several commands there, because our external managing node is not inside our cluster, so we cannot access the working sub-networks and the internal network of the cluster. </p><br><p>  First let's load the heketi-client utility and copy it to the system bin folder: </p><br><pre> <code class="plaintext hljs">master1# wget https://github.com/heketi/heketi/releases/download/v8.0.0/heketi-client-v8.0.0.linux.amd64.tar.gz master1# tar -xzvf ./heketi-client-v8.0.0.linux.amd64.tar.gz master1# cp ./heketi-client/bin/heketi-cli /usr/local/bin/ master1# heketi-cli heketi-cli v8.0.0</code> </pre> <br><p>  Now we‚Äôll find the IP address of the heketi and export it as a system variable: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf describe pod deploy-heketi-1211581626-2jotm For me this pod have a 10.42.0.1 ip master1# curl http://10.42.0.1:57598/hello Handling connection for 57598 Hello from Heketi master1# export HEKETI_CLI_SERVER=http://10.42.0.1:57598</code> </pre> <br><p>  Now let's give Heketi information about the GlusterFS cluster that he should manage.  We provide it through a topology file.  Topology is a JSON manifest with a list of all nodes, disks, and clusters used by GlusterFS. </p><br><blockquote>  NOTE.  Make sure that <code>hostnames/manage</code> points to the exact name, as in the <code>kubectl get node</code> section, and that <code>hostnames/storage</code> is the IP address of the storage nodes. </blockquote><br><pre> <code class="plaintext hljs">master1:~/heketi-client# vi topology.json { "clusters": [ { "nodes": [ { "node": { "hostnames": { "manage": [ "worker1" ], "storage": [ "192.168.0.7" ] }, "zone": 1 }, "devices": [ "/dev/loop0" ] }, { "node": { "hostnames": { "manage": [ "worker2" ], "storage": [ "192.168.0.8" ] }, "zone": 1 }, "devices": [ "/dev/loop0" ] }, { "node": { "hostnames": { "manage": [ "worker3" ], "storage": [ "192.168.0.9" ] }, "zone": 1 }, "devices": [ "/dev/loop0" ] } ] } ] }</code> </pre> <br><p>  Then download this file: </p><br><pre> <code class="plaintext hljs">master1:~/heketi-client# heketi-cli topology load --json=topology.json Creating cluster ... ID: e83467d0074414e3f59d3350a93901ef Allowing file volumes on cluster. Allowing block volumes on cluster. Creating node worker1 ... ID: eea131d392b579a688a1c7e5a85e139c Adding device /dev/loop0 ... OK Creating node worker2 ... ID: 300ad5ff2e9476c3ba4ff69260afb234 Adding device /dev/loop0 ... OK Creating node worker3 ... ID: 94ca798385c1099c531c8ba3fcc9f061 Adding device /dev/loop0 ... OK</code> </pre> <br><p>  Next we use Heketi to provide volumes for storing the database.  The team name is a bit strange, but everything is in order.  Also create a heketi repository: </p><br><pre> <code class="plaintext hljs">master1:~/heketi-client# heketi-cli setup-openshift-heketi-storage master1:~/heketi-client# kubectl --kubeconfig /etc/kubernetes/admin.conf create -f heketi-storage.json secret/heketi-storage-secret created endpoints/heketi-storage-endpoints created service/heketi-storage-endpoints created job.batch/heketi-storage-copy-job created</code> </pre> <br><p>  These are all commands that need to be run from the master node.  Let's go back to the managing node and continue from there;  First of all, make sure that the last running command was successfully completed: </p><br><pre> <code class="plaintext hljs">control# kubectl get pod NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 39h glusterfs-hzdll 1/1 Running 0 39h glusterfs-p8r59 1/1 Running 0 39h heketi-storage-copy-job-txkql 0/1 Completed 0 69s</code> </pre> <br><p>  And the heketi-storage-copy-job task is completed. </p><br><blockquote>  If there is currently no <strong>glusterfs-client</strong> package installed on your work nodes, then there is an error. </blockquote><p>  It's time to remove the Hetsti Bootstrap installation file and do a little cleanup: </p><br><pre> <code class="plaintext hljs">control# kubectl delete all,service,jobs,deployment,secret --selector="deploy-heketi"</code> </pre> <br><p>  In the last step, we need to create a long-term instance of Heketi: </p><br><pre> <code class="plaintext hljs">control# cd ./heketi/extras/kubernetes control:~/heketi/extras/kubernetes# kubectl create -f heketi-deployment.json secret/heketi-db-backup created service/heketi created deployment.extensions/heketi created control# kubectl get pod NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 39h glusterfs-hzdll 1/1 Running 0 39h glusterfs-p8r59 1/1 Running 0 39h heketi-b8c5f6554-knp7t 1/1 Running 0 22m</code> </pre> <br><p>  If there is currently no glusterfs-client package installed on your work nodes, then there is an error.  And we are almost done, now the Heketi database is stored in the GlusterFS volume and is not reset every time the Heketi pod restarts. </p><br><p>  To start using a GlusterFS cluster with dynamic resource allocation, we need to create a StorageClass. </p><br><p>  First, let's find the endpoint of the Gluster storage, which will be passed to the StorageClass as a parameter (heketi-storage-endpoints): </p><br><pre> <code class="plaintext hljs">control# kubectl get endpoints NAME ENDPOINTS AGE heketi 10.42.0.2:8080 2d16h ....... ... ..</code> </pre> <br><p>  Now create several files: </p><br><pre> <code class="plaintext hljs">control# vi storage-class.yml apiVersion: storage.k8s.io/v1beta1 kind: StorageClass metadata: name: slow provisioner: kubernetes.io/glusterfs parameters: resturl: "http://10.42.0.2:8080" control# vi test-pvc.yml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: gluster1 annotations: volume.beta.kubernetes.io/storage-class: "slow" spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi</code> </pre> <br><p>  Use these files to create a class and pvc: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f storage-class.yaml storageclass "slow" created control# kubectl get storageclass NAME PROVISIONER AGE slow kubernetes.io/glusterfs 2d8h control# kubectl create -f test-pvc.yaml persistentvolumeclaim "gluster1" created control# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE gluster1 Bound pvc-27f733cd-1c77-11e9-bb07-7efe6b0e6fa5 1Gi RWO slow 2d8h</code> </pre> <br><p>  We can also view the PV volume: </p><br><pre> <code class="plaintext hljs">control# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-27f733cd-1c77-11e9-bb07-7efe6b0e6fa5 1Gi RWO Delete Bound default/gluster1 slow 2d8h</code> </pre> <br><p>  Now we have a dynamically created GlusterFS volume associated with <strong>PersistentVolumeClaim</strong> , and we can use this statement in any subject. </p><br><p>  Create a simple one under Nginx and test it: </p><br><pre> <code class="plaintext hljs">control# vi nginx-test.yml apiVersion: v1 kind: Pod metadata: name: nginx-pod1 labels: name: nginx-pod1 spec: containers: - name: nginx-pod1 image: gcr.io/google_containers/nginx-slim:0.8 ports: - name: web containerPort: 80 volumeMounts: - name: gluster-vol1 mountPath: /usr/share/nginx/html volumes: - name: gluster-vol1 persistentVolumeClaim: claimName: gluster1 control# kubectl create -f nginx-test.yaml pod "nginx-pod1" created</code> </pre> <br><p>  Browse under (wait a few minutes, you may need to upload an image if it does not already exist): </p><br><pre> <code class="plaintext hljs">control# kubectl get pods NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 4d10h glusterfs-hzdll 1/1 Running 0 4d10h glusterfs-p8r59 1/1 Running 0 4d10h heketi-b8c5f6554-knp7t 1/1 Running 0 2d18h nginx-pod1 1/1 Running 0 47h</code> </pre> <br><p>  Now go into the container and create the file index.html: </p><br><pre> <code class="plaintext hljs">control# kubectl exec -ti nginx-pod1 /bin/sh # cd /usr/share/nginx/html # echo 'Hello there from GlusterFS pod !!!' &gt; index.html # ls index.html # exit</code> </pre> <br><p>  It will be necessary to find the internal IP address of the hearth and curl into it from any master node: </p><br><pre> <code class="plaintext hljs">master1# curl 10.40.0.1 Hello there from GlusterFS pod !!!</code> </pre> <br><p>  In doing so, we are just testing our new permanent volume. </p><br><blockquote>  Some useful commands for checking the new GlusterFS cluster are: <code>heketi-cli cluster list</code> and <code>heketi-cli volume list</code> .  They can be run on your computer if <strong>heketi-cli is installed</strong> .  In this example, this is the <strong>master1</strong> node. </blockquote><br><pre> <code class="plaintext hljs">master1# heketi-cli cluster list Clusters: Id:e83467d0074414e3f59d3350a93901ef [file][block] master1# heketi-cli volume list Id:6fdb7fef361c82154a94736c8f9aa53e Cluster:e83467d0074414e3f59d3350a93901ef Name:vol_6fdb7fef361c82154a94736c8f9aa53e Id:c6b69bd991b960f314f679afa4ad9644 Cluster:e83467d0074414e3f59d3350a93901ef Name:heketidbstorage</code> </pre> <br><p>  At this stage, we have successfully configured an internal load balancer with file storage, and our cluster is now closer to its operational state. </p><br><p>  In the next part of the article we will focus on creating a cluster monitoring system, and also launch a test project in it to use all the resources we have configured. </p><br><p>  Stay in touch, and all the best! </p></div><p>Source: <a href="https://habr.com/ru/post/443110/">https://habr.com/ru/post/443110/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../443098/index.html">The data on the disk will be recorded using magnets and lasers.</a></li>
<li><a href="../443100/index.html">Calculate bugs in the calculator Windows</a></li>
<li><a href="../443102/index.html">Behavior change as a product: why does Marie Kondo raise a $ 40M round with Sequoia Capital?</a></li>
<li><a href="../443104/index.html">We calculate symbolic expressions with fuzzy triangular numbers in python</a></li>
<li><a href="../443106/index.html">Announced USB4: what's known about the standard</a></li>
<li><a href="../443112/index.html">Are you sure you can trust your VPN?</a></li>
<li><a href="../443120/index.html">The State Duma will continue the fight against the illegal sale of SIM cards</a></li>
<li><a href="../443122/index.html">809 million email addresses for the Verifications.io service leak due to publicly open MongoDB</a></li>
<li><a href="../443124/index.html">React.lazy? But what if you don't have a component?</a></li>
<li><a href="../443128/index.html">kW, kW * h and kW / h</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>