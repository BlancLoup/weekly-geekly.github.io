<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>We create a neural network InceptionV3 for image recognition</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! Under the cat we will talk about the implementation of the InceptionV3 convolutional neural network architecture using the Keras framework. ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>We create a neural network InceptionV3 for image recognition</h1><div class="post__text post__text-html js-mediator-article"> <a href="https://habrahabr.ru/post/321834/"><img src="https://habrastorage.org/files/b60/891/4ad/b608914ad4c4421c9734fb6e685c4aa4.png"></a> <br><br>  Hi, Habr!  Under the cat we will talk about the implementation of the InceptionV3 convolutional neural network architecture using the <a href="https://keras.io/">Keras</a> framework.  I decided to write an article after getting acquainted with the tutorial " <a href="https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html">Building powerful classification models using a small amount of data</a> ."  With the approval of the author of the tutorial, I slightly changed the content of my article.  Unlike the <a href="http://www.robots.ox.ac.uk/~vgg/practicals/cnn/">VGG16</a> neural network proposed by the author, we will train the Google deep neural network <a href="https://habrahabr.ru/post/302242/">Inception V3</a> , which is already preinstalled in Keras. <br><br>  You will learn: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ol><li>  Import the Inception V3 neural network from the Keras library; </li><li>  Adjust the network: load weights, change the upper part of the model ( <abbr title="Fully connected layers">fc-layers</abbr> ), thus fitting the model to the binary classification; </li><li>  To fine-tune the lower convolutional layer of the neural network; </li><li>  Apply data augmentation with ImageDataGenerator; </li><li>  Teach the network in parts to save resources and time; </li><li>  Evaluate the work model. </li></ol><br>  When writing an article, I set myself the task of presenting the most practical material that will reveal some interesting features of the Keras framework. <br><a name="habracut"></a><br>  Recently, tutorials devoted to the creation and application of neural networks have increasingly appeared.  It is with great pleasure that I observe an interesting trend: new posts are becoming more and more understandable for non-specialists in the field of programming.  <a href="https://habrahabr.ru/post/317712/">Some authors</a> even try to introduce readers to this topic, using the most natural language possible.  There are also excellent articles (eg. <a href="https://habrahabr.ru/company/yandex/blog/307260/">1</a> , <a href="https://habrahabr.ru/post/271563/">2</a> , <a href="https://habrahabr.ru/post/309508/">3</a> ) that combine a reasonable amount of theory and practice, which allows you to quickly understand the necessary minimum and begin to create something of your own. <br><br>  So for the cause! <br><br><div class="spoiler">  <b class="spoiler_title">First of all, a little about libraries:</b> <div class="spoiler_text">  I recommend installing the <a href="https://www.continuum.io/downloads">Anaconda</a> platform.  I used Python 2.7.  For work it is convenient to use <a href="http://jupyter.org/">Jupyter notebook</a> .  It is already pre-installed on Anaconda.  We will also need to install the <a href="https://keras.io/">Keras</a> framework.  I used <a href="http://deeplearning.net/software/theano/">Theano</a> as a backend.  You can use <a href="https://www.tensorflow.org/">Tensorflow</a> because Keras supports both of them.  Installing CUDA for Theano on Windows is described <a href="https://lepisma.github.io/articles/2015/07/30/up-with-theano-and-cuda/">here</a> . </div></div><br><h2>  1. Data: </h2><br>  In our example, we will use images from a machine learning competition called " <a href="https://www.kaggle.com/c/dogs-vs-cats">Dogs vs Cats</a> " on <a href="https://www.kaggle.com/">kaggle.com</a> .  Data will be available after registration.  The set includes 25000 images: 12,500 cats and 12,500 dogs.  Class 1 corresponds to dogs, class 0 to cats.  After downloading the archives, place <b>1000 images of each class</b> in directories as follows: <br><br><pre><code class="bash hljs">data/ train/ dogs/ dog001.jpg dog002.jpg ... cats/ cat001.jpg cat002.jpg ... validation/ dogs/ dog1000.jpg dog1001.jpg ... cats/ cat1000.jpg cat1001.jpg ...</code> </pre> <br>  <i>Nothing prevents you from using the entire data set.</i>  <i>I, as well as the author of the original article, decided to use a limited sample to test the network performance with a small set of images.</i> <br><br><h4>  We have three problems: </h4><br><ol><li>  Limited amount of data; </li><li>  Limited system resources (for me, for example, Intel Core i5-4440 3.10GHz, 8 GB of RAM, NVIDIA GeForce GTX 745); </li><li>  Limited time: we want to train the model for less than a day. </li></ol><br><h4>  With a limited image size, the likelihood of <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25B5%25D1%2580%25D0%25B5%25D0%25BE%25D0%25B1%25D1%2583%25D1%2587%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">retraining</a> is high.  To combat this you need: </h4><br><ol><li>  Install a large dropout.  In our case, it will be 0.5; </li><li>  We will use data augmentation.  This technique will allow us to increase the number of images through various transformations (in our case there will be changes in scale, shifts, horizontal reflection). </li><li>  For our experiment, we take a deep network. </li></ol><br>  The last point should alarm us, because the deep neural network is demanding of resources.  I couldn‚Äôt train even VGG16 on my video card, let alone such a gigantic one as Inception.  Nevertheless, the solution is: <br><br><ol><li>  Initially, we use a model that has been trained on a large number of images from the <a href="http://www.image-net.org/">imagenet</a> database.  Fortunately, the set of images included cats and dogs; </li><li>  We will teach the model in parts: </li><li>  First, run the augmented images through the bottom of the network (only through Inception) and save them as numpy arrays; </li><li>  Using the obtained numpy arrays, we train the upper fully connected layer; </li><li>  Then, we will merge the upper and lower layers of the model and fine-tune the new model, but at the same time we will block all layers except the last one from learning from Inception. </li></ol><br>  The only solution to the problem with time is that I see the use of parallel computing.  You need a video card with <a href="https://ru.wikipedia.org/wiki/CUDA">CUDA</a> support for this.  <a href="https://lepisma.github.io/articles/2015/07/30/up-with-theano-and-cuda/">I hope that installing CUDA for Python will not cause you much difficulty</a> . <br><br>  Import libraries: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.image <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ImageDataGenerator <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential, Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.inception_v3 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> InceptionV3 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ModelCheckpoint <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.optimizers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SGD <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> backend <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> K K.set_image_dim_ordering(<span class="hljs-string"><span class="hljs-string">'th'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> h5py <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt</code> </pre> <br><h1>  2. Create an InceptionV3 model, load images into it, save them: </h1><br><div class="spoiler">  <b class="spoiler_title">Big picture with the scheme of our actions</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/ae6/99c/205/ae699c205522458ebf56b38a447ff203.png"><br></div></div><br>  The <a href="https://keras.io/applications/">Keras library</a> has several prepared neural networks. <br><br><div class="spoiler">  <b class="spoiler_title">Model Argument List</b> <div class="spoiler_text">  <i>include_top: to</i> include or not to include the upper part of the network, which is a fully connected layer with 1000 outputs. <br>  We don't need it, so we set: include_top = False; <br><br>  <i>weights</i> : load / do not load trained weights.  If None, the weights are initialized randomly.  If imagenet, weights loaded with <a href="http://www.image-net.org/">ImageNet</a> data will be loaded. <br>  In our model, weights are needed, so weights = "imagenet"; <br><br>  <i>input_tensor</i> : this argument is convenient if we use the Input layer for our model. <br>  We will not touch it. <br><br>  <i>input_shape</i> : in this argument we set the size of our image.  It is indicated if the upper layer is detached (include_top = False).  If we loaded the model with the top layer, one hundred image size should be only (3, 299, 299). <br>  We removed the top layer and want to analyze smaller images (3, 150, 150).  Therefore, show: input_shape = () </div></div><br><h4>  Create our model: </h4><br><pre> <code class="python hljs">inc_model=InceptionV3(include_top=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, weights=<span class="hljs-string"><span class="hljs-string">'imagenet'</span></span>, input_shape=((<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">150</span></span>, <span class="hljs-number"><span class="hljs-number">150</span></span>)))</code> </pre> <br>  Now we will augment the data.  For this Keras provides the so-called ImageDataGenerator.  They will take images directly from the folders and carry out all the necessary transformations on them. <br><br>  Pictures of each class must be in separate folders.  In order for us not to load RAM with images, we transform them right before uploading to the network.  For this we use the .flow_from_directory method.  Create separate generators for training and test images: <br><br><pre> <code class="python hljs">bottleneck_datagen = ImageDataGenerator(rescale=<span class="hljs-number"><span class="hljs-number">1.</span></span>/<span class="hljs-number"><span class="hljs-number">255</span></span>) <span class="hljs-comment"><span class="hljs-comment">#,  train_generator = bottleneck_datagen.flow_from_directory('data/img_train/', target_size=(150, 150), batch_size=32, class_mode=None, shuffle=False) validation_generator = bottleneck_datagen.flow_from_directory('data/img_val/', target_size=(150, 150), batch_size=32, class_mode=None, shuffle=False)</span></span></code> </pre> <br>  I want to highlight an important point.  We specified <i>shuffle = False</i> .  That is, images from different classes will not be mixed.  First, there will be images from the first folder, and when they all end, from the second.  Why it is needed, see later. <br><br>  Run the augmented images through the trained Inception and save the output as numpy arrays: <br><br><pre> <code class="python hljs">bottleneck_features_train = inc_model.predict_generator(train_generator, <span class="hljs-number"><span class="hljs-number">2000</span></span>) np.save(open(<span class="hljs-string"><span class="hljs-string">'bottleneck_features/bn_features_train.npy'</span></span>, <span class="hljs-string"><span class="hljs-string">'wb'</span></span>), bottleneck_features_train) bottleneck_features_validation = inc_model.predict_generator(validation_generator, <span class="hljs-number"><span class="hljs-number">2000</span></span>) np.save(open(<span class="hljs-string"><span class="hljs-string">'bottleneck_features/bn_features_validation.npy'</span></span>, <span class="hljs-string"><span class="hljs-string">'wb'</span></span>), bottleneck_features_validation)</code> </pre> <br>  The process will take some time. <br><br><h2>  3. Create the upper part of the model, load the data into it, save it: </h2><br><div class="spoiler">  <b class="spoiler_title">Scheme</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/b15/4ff/ad8/b154ffad830641cc9244028940a42369.png"><br></div></div><br>  In the original post, the author used one network layer with 256 neurons, however I will use two layers with 64 neurons each and a Dropout layer with a value of 0.5.  I was forced to make this change due to the fact that when I trained the finished model (which we will do in the next step), my computer would hang and reboot. <br><br><h4>  Load the arrays: </h4><br><pre> <code class="python hljs">train_data = np.load(open(<span class="hljs-string"><span class="hljs-string">'bottleneck_features_and_weights/bn_features_train.npy'</span></span>, <span class="hljs-string"><span class="hljs-string">'rb'</span></span>)) train_labels = np.array([<span class="hljs-number"><span class="hljs-number">0</span></span>] * <span class="hljs-number"><span class="hljs-number">1000</span></span> + [<span class="hljs-number"><span class="hljs-number">1</span></span>] * <span class="hljs-number"><span class="hljs-number">1000</span></span>) validation_data = np.load(open(<span class="hljs-string"><span class="hljs-string">'bottleneck_features_and_weights/bn_features_validation.npy'</span></span>, <span class="hljs-string"><span class="hljs-string">'rb'</span></span>)) validation_labels = np.array([<span class="hljs-number"><span class="hljs-number">0</span></span>] * <span class="hljs-number"><span class="hljs-number">1000</span></span> + [<span class="hljs-number"><span class="hljs-number">1</span></span>] * <span class="hljs-number"><span class="hljs-number">1000</span></span>)</code> </pre> <br>  Please note, earlier we specified <i>shuffle = False</i> .  And now we can easily specify the <i>labels</i> .  Since in each class we have 2000 images and all the images were received in turn, we will have 1000 zeros and 1000 units for training and for test samples. <br><br><h4>  Create a model of the FFN network, compile it: </h4><br><pre> <code class="python hljs">fc_model = Sequential() fc_model.add(Flatten(input_shape=train_data.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>:])) fc_model.add(Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, name=<span class="hljs-string"><span class="hljs-string">'dense_one'</span></span>)) fc_model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>, name=<span class="hljs-string"><span class="hljs-string">'dropout_one'</span></span>)) fc_model.add(Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, name=<span class="hljs-string"><span class="hljs-string">'dense_two'</span></span>)) fc_model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>, name=<span class="hljs-string"><span class="hljs-string">'dropout_two'</span></span>)) fc_model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>, name=<span class="hljs-string"><span class="hljs-string">'output'</span></span>)) fc_model.compile(optimizer=<span class="hljs-string"><span class="hljs-string">'rmsprop'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'binary_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> <br><h4>  Let's load our arrays into it: </h4><br><pre> <code class="python hljs">fc_model.fit(train_data, train_labels, nb_epoch=<span class="hljs-number"><span class="hljs-number">50</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">32</span></span>, validation_data=(validation_data, validation_labels)) fc_model.save_weights(<span class="hljs-string"><span class="hljs-string">'bottleneck_features_and_weights/fc_inception_cats_dogs_250.hdf5'</span></span>) <span class="hljs-comment"><span class="hljs-comment">#  </span></span></code> </pre> <br>  Now we load data not from folders, so we use the usual <i>fit</i> method. <br><br><div class="spoiler">  <b class="spoiler_title">The learning process will be indecently fast.</b>  <b class="spoiler_title">Each epoch took 1 sec from me:</b> <div class="spoiler_text"><pre> <code class="python hljs">Train on <span class="hljs-number"><span class="hljs-number">2000</span></span> samples, validate on <span class="hljs-number"><span class="hljs-number">2000</span></span> samples Epoch <span class="hljs-number"><span class="hljs-number">1</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">2000</span></span>/<span class="hljs-number"><span class="hljs-number">2000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">1</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">2.4588</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.8025</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.7950</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9375</span></span> Epoch <span class="hljs-number"><span class="hljs-number">2</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">2000</span></span>/<span class="hljs-number"><span class="hljs-number">2000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">1</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">1.3332</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.8870</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.9330</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9160</span></span> ‚Ä¶ Epoch <span class="hljs-number"><span class="hljs-number">48</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">2000</span></span>/<span class="hljs-number"><span class="hljs-number">2000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">1</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.1096</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9880</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.5496</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9595</span></span> Epoch <span class="hljs-number"><span class="hljs-number">49</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">2000</span></span>/<span class="hljs-number"><span class="hljs-number">2000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">1</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.1100</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9875</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.5600</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9560</span></span> Epoch <span class="hljs-number"><span class="hljs-number">50</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">2000</span></span>/<span class="hljs-number"><span class="hljs-number">2000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">1</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0850</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9895</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.5674</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9565</span></span></code> </pre> <br></div></div><br>  We estimate the accuracy of the model: <br><br><pre> <code class="python hljs">fc_model.evaluate(validation_data, validation_labels)</code> </pre> <br>  [0.56735104312408047, 0.95650000000000002] <br><br>  Our model is doing its job well.  But it takes only numpy arrays.  It does not suit us.  In order to get a full-fledged model that accepts images as input, we will connect our two models and train them again. <br><br><h2>  4. Create the final model, load the augmented data into it, save the weights: </h2><br><div class="spoiler">  <b class="spoiler_title">Scheme</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/fe7/e33/312/fe7e333120db4e6b904960ca7288d82f.png"><br></div></div><br><pre> <code class="python hljs">weights_filename=<span class="hljs-string"><span class="hljs-string">'bottleneck_features_and_weights/fc_inception_cats_dogs_250.hdf5'</span></span> x = Flatten()(inc_model.output) x = Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, name=<span class="hljs-string"><span class="hljs-string">'dense_one'</span></span>)(x) x = Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>, name=<span class="hljs-string"><span class="hljs-string">'dropout_one'</span></span>)(x) x = Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, name=<span class="hljs-string"><span class="hljs-string">'dense_two'</span></span>)(x) x = Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>, name=<span class="hljs-string"><span class="hljs-string">'dropout_two'</span></span>)(x) top_model=Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>, name=<span class="hljs-string"><span class="hljs-string">'output'</span></span>)(x) model = Model(input=inc_model.input, output=top_model)</code> </pre> <br>  Load weights into it: <br><br><pre> <code class="python hljs">weights_filename=<span class="hljs-string"><span class="hljs-string">'bottleneck_features_and_weights/fc_inception_cats_dogs_250.hdf5'</span></span> model.load_weights(weights_filename, by_name=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br>  <i>To be honest, I did not notice any difference between the effectiveness of training the model with or without loading weights.</i>  <i>But I left this section because it describes how to load weights into certain layers by name (by_name = True).</i> <br><br><h4>  Block from 1 to 205 layers Inception: </h4><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> layer <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> inc_model.layers[:<span class="hljs-number"><span class="hljs-number">205</span></span>]: layer.trainable = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span></code> </pre> <br><h4>  Compile the model: </h4><br><pre> <code class="python hljs">model.compile(loss=<span class="hljs-string"><span class="hljs-string">'binary_crossentropy'</span></span>, optimizer=SGD(lr=<span class="hljs-number"><span class="hljs-number">1e-4</span></span>, momentum=<span class="hljs-number"><span class="hljs-number">0.9</span></span>), <span class="hljs-comment"><span class="hljs-comment">#optimizer='rmsprop', metrics=['accuracy'])</span></span></code> </pre> <br>  Notice that the first time we trained fully connected layers from .npy arrays, we used the <abbr title="Root Mean Square Propagation">RMSprop</abbr> optimizer.  Now, to tune the model, we use the Stochastic Gradient Descent.  This is done in order to prevent too pronounced updates to the already trained weights. <br><br>  Let us make sure that in the learning process only the weights with the highest accuracy on the test sample are saved: <br><br><pre> <code class="python hljs">filepath=<span class="hljs-string"><span class="hljs-string">"new_model_weights/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5"</span></span> checkpoint = ModelCheckpoint(filepath, monitor=<span class="hljs-string"><span class="hljs-string">'val_acc'</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>, save_best_only=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'max'</span></span>) callbacks_list = [checkpoint]</code> </pre> <br>  Create new image generators for learning the full model.  We will transform only the training sample.  Test will not touch. <br><br><pre> <code class="python hljs">train_datagen = ImageDataGenerator( rescale=<span class="hljs-number"><span class="hljs-number">1.</span></span>/<span class="hljs-number"><span class="hljs-number">255</span></span>, shear_range=<span class="hljs-number"><span class="hljs-number">0.2</span></span>, zoom_range=<span class="hljs-number"><span class="hljs-number">0.2</span></span>, horizontal_flip=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) test_datagen = ImageDataGenerator(rescale=<span class="hljs-number"><span class="hljs-number">1.</span></span>/<span class="hljs-number"><span class="hljs-number">255</span></span>) train_generator = train_datagen.flow_from_directory( <span class="hljs-string"><span class="hljs-string">'data/img_train/'</span></span>, target_size=(<span class="hljs-number"><span class="hljs-number">150</span></span>, <span class="hljs-number"><span class="hljs-number">150</span></span>), batch_size=<span class="hljs-number"><span class="hljs-number">32</span></span>, class_mode=<span class="hljs-string"><span class="hljs-string">'binary'</span></span>) validation_generator = test_datagen.flow_from_directory( <span class="hljs-string"><span class="hljs-string">'data/img_val/'</span></span>, target_size=(<span class="hljs-number"><span class="hljs-number">150</span></span>, <span class="hljs-number"><span class="hljs-number">150</span></span>), batch_size=<span class="hljs-number"><span class="hljs-number">32</span></span>, class_mode=<span class="hljs-string"><span class="hljs-string">'binary'</span></span>) pred_generator=test_datagen.flow_from_directory(<span class="hljs-string"><span class="hljs-string">'data/img_val/'</span></span>, target_size=(<span class="hljs-number"><span class="hljs-number">150</span></span>,<span class="hljs-number"><span class="hljs-number">150</span></span>), batch_size=<span class="hljs-number"><span class="hljs-number">100</span></span>, class_mode=<span class="hljs-string"><span class="hljs-string">'binary'</span></span>)</code> </pre> <br>  we use <i>pred_generator</i> later to demonstrate how the model works. <br><br><h4>  Load the images into the model: </h4><br><pre> <code class="python hljs">model.fit_generator( train_generator, samples_per_epoch=<span class="hljs-number"><span class="hljs-number">2000</span></span>, nb_epoch=<span class="hljs-number"><span class="hljs-number">200</span></span>, validation_data=validation_generator, nb_val_samples=<span class="hljs-number"><span class="hljs-number">2000</span></span>, callbacks=callbacks_list)</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">We hear the noise of the cooler and wait ...</b> <div class="spoiler_text"><pre> <code class="python hljs">Epoch <span class="hljs-number"><span class="hljs-number">1</span></span>/<span class="hljs-number"><span class="hljs-number">200</span></span> <span class="hljs-number"><span class="hljs-number">1984</span></span>/<span class="hljs-number"><span class="hljs-number">2000</span></span> [============================&gt;.] - ETA: <span class="hljs-number"><span class="hljs-number">0</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">1.0814</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.5640</span></span>Epoch <span class="hljs-number"><span class="hljs-number">00000</span></span>: val_acc improved <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> -inf to <span class="hljs-number"><span class="hljs-number">0.71750</span></span>, saving model to new_model_weights/weights-improvement<span class="hljs-number"><span class="hljs-number">-00</span></span><span class="hljs-number"><span class="hljs-number">-0.72</span></span>.hdf5 <span class="hljs-number"><span class="hljs-number">2000</span></span>/<span class="hljs-number"><span class="hljs-number">2000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">224</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">1.0814</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.5640</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.6016</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.7175</span></span> Epoch <span class="hljs-number"><span class="hljs-number">2</span></span>/<span class="hljs-number"><span class="hljs-number">200</span></span> <span class="hljs-number"><span class="hljs-number">1984</span></span>/<span class="hljs-number"><span class="hljs-number">2000</span></span> [============================&gt;.] - ETA: <span class="hljs-number"><span class="hljs-number">0</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.8523</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.6240</span></span>Epoch <span class="hljs-number"><span class="hljs-number">00001</span></span>: val_acc improved <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> <span class="hljs-number"><span class="hljs-number">0.71750</span></span> to <span class="hljs-number"><span class="hljs-number">0.77200</span></span>, saving model to new_model_weights/weights-improvement<span class="hljs-number"><span class="hljs-number">-01</span></span><span class="hljs-number"><span class="hljs-number">-0.77</span></span>.hdf5 <span class="hljs-number"><span class="hljs-number">2000</span></span>/<span class="hljs-number"><span class="hljs-number">2000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">215</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.8511</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.6240</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.5403</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.7720</span></span> ‚Ä¶ Epoch <span class="hljs-number"><span class="hljs-number">199</span></span>/<span class="hljs-number"><span class="hljs-number">200</span></span> <span class="hljs-number"><span class="hljs-number">1968</span></span>/<span class="hljs-number"><span class="hljs-number">2000</span></span> [============================&gt;.] - ETA: <span class="hljs-number"><span class="hljs-number">1</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.1439</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9385</span></span>Epoch <span class="hljs-number"><span class="hljs-number">00008</span></span>: val_acc improved <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> <span class="hljs-number"><span class="hljs-number">0.90650</span></span> to <span class="hljs-number"><span class="hljs-number">0.91500</span></span>, saving model to new_model_weights/weights-improvement<span class="hljs-number"><span class="hljs-number">-08</span></span><span class="hljs-number"><span class="hljs-number">-0.92</span></span>.hdf5 <span class="hljs-number"><span class="hljs-number">2000</span></span>/<span class="hljs-number"><span class="hljs-number">2000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">207</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.1438</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9385</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.2786</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9150</span></span> Epoch <span class="hljs-number"><span class="hljs-number">200</span></span>/<span class="hljs-number"><span class="hljs-number">200</span></span> <span class="hljs-number"><span class="hljs-number">1968</span></span>/<span class="hljs-number"><span class="hljs-number">2000</span></span> [============================&gt;.] - ETA: <span class="hljs-number"><span class="hljs-number">1</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.1444</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9350</span></span>Epoch <span class="hljs-number"><span class="hljs-number">00009</span></span>: val_acc did <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> improve <span class="hljs-number"><span class="hljs-number">2000</span></span>/<span class="hljs-number"><span class="hljs-number">2000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">206</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.1438</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9355</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.3898</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.8940</span></span></code> </pre> <br></div></div><br>  It took me 210-220 seconds for each epoch.  200 learning epochs took about 12 hours. <br><br><h2>  5. We estimate the accuracy of the model. </h2><br><pre> <code class="python hljs">model.evaluate_generator(pred_generator, val_samples=<span class="hljs-number"><span class="hljs-number">100</span></span>)</code> </pre> <br>  [0.2364250123500824, 0.9100000262260437] <br><br>  This is <i>where the pred_generator</i> came in <i>handy</i> .  Please note, <i>val_samples</i> must match the <i>batch_size</i> value in the generator! <br><br>  Accuracy 91.7%.  Given the limitations of the sample, I dare to say that this is a good accuracy. <br><br><h4>  Illustrate the work of the model </h4><br>  Just look at the% correct answers and the magnitude of the error we are not interested.  Let's see how many correct and incorrect answers were given by the model for each class: <br><br><pre> <code class="python hljs">imgs,labels=pred_generator.next() array_imgs=np.transpose(np.asarray([img_to_array(img) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> img <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> imgs]),(<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>)) predictions=model.predict(imgs) rounded_pred=np.asarray([round(i) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> predictions])</code> </pre> <br>  <i>pred_generator.next () is a</i> handy thing.  It loads images into a variable and assigns labels. <br><br>  The number of images of each class will be different with each generation: <br><br><pre> <code class="python hljs">pd.value_counts(labels) <span class="hljs-number"><span class="hljs-number">0.0</span></span> <span class="hljs-number"><span class="hljs-number">51</span></span> <span class="hljs-number"><span class="hljs-number">1.0</span></span> <span class="hljs-number"><span class="hljs-number">49</span></span> dtype: int64</code> </pre> <br>  How many images of each class model predicted correctly? <br><br>  pd.crosstab (labels, rounded_pred) <br><table><tbody><tr><th>  Col_0 </th><th>  0.0 </th><th>  1.0 </th></tr><tr><td>  Row_0 </td><td></td><td></td></tr><tr><td>  <b>0.0</b> </td><td>  47 </td><td>  four </td></tr><tr><td>  <b>1.0</b> </td><td>  eight </td><td>  41 </td></tr></tbody></table><br>  For the model, 100 random images were uploaded: 51 images of cats and 49 dogs.  Of the 51 cats, 47 recognized the model correctly. Of the 50 dogs, 41 were correctly recognized. The overall accuracy of the model on this narrow sample was 88%. <br><br><h4>  Let's see which photos were recognized incorrectly: </h4><br><pre> <code class="python hljs">wrong=[im <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> im <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(array_imgs, rounded_pred, labels, predictions) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> im[<span class="hljs-number"><span class="hljs-number">1</span></span>]!=im[<span class="hljs-number"><span class="hljs-number">2</span></span>]] plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">12</span></span>,<span class="hljs-number"><span class="hljs-number">12</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> ind, val <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(wrong[:<span class="hljs-number"><span class="hljs-number">100</span></span>]): plt.subplots_adjust(left=<span class="hljs-number"><span class="hljs-number">0</span></span>, right=<span class="hljs-number"><span class="hljs-number">1</span></span>, bottom=<span class="hljs-number"><span class="hljs-number">0</span></span>, top=<span class="hljs-number"><span class="hljs-number">1</span></span>, wspace = <span class="hljs-number"><span class="hljs-number">0.2</span></span>, hspace = <span class="hljs-number"><span class="hljs-number">0.2</span></span>) plt.subplot(<span class="hljs-number"><span class="hljs-number">5</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>,ind+<span class="hljs-number"><span class="hljs-number">1</span></span>) im=val[<span class="hljs-number"><span class="hljs-number">0</span></span>] plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) plt.text(<span class="hljs-number"><span class="hljs-number">120</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, round(val[<span class="hljs-number"><span class="hljs-number">3</span></span>], <span class="hljs-number"><span class="hljs-number">2</span></span>), fontsize=<span class="hljs-number"><span class="hljs-number">11</span></span>, color=<span class="hljs-string"><span class="hljs-string">'red'</span></span>) plt.text(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, val[<span class="hljs-number"><span class="hljs-number">2</span></span>], fontsize=<span class="hljs-number"><span class="hljs-number">11</span></span>, color=<span class="hljs-string"><span class="hljs-string">'blue'</span></span>) plt.imshow(np.transpose(im,(<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>)))</code> </pre> <br><img src="https://habrastorage.org/files/f41/452/916/f4145291607d4d5daafddc68e211bcda.png"><br>  Blue numbers are a true class of images.  Red numbers are predicted by the model (if the red number is less than 0.5, the model considers that in the photo is a cat, if more than 0.5, then the dog).  The more the number approaches zero, the more confident the network is that the cat is in front of it.  Interestingly, many bugs with dog images contain small breeds or puppies. <br><br><h4>  Let's look at the first 20 images that the model predicted correctly: </h4><br><pre> <code class="python hljs">right=[im <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> im <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(array_imgs, rounded_pred, labels, predictions) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> im[<span class="hljs-number"><span class="hljs-number">1</span></span>]==im[<span class="hljs-number"><span class="hljs-number">2</span></span>]] plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">12</span></span>,<span class="hljs-number"><span class="hljs-number">12</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> ind, val <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(right[:<span class="hljs-number"><span class="hljs-number">20</span></span>]): plt.subplots_adjust(left=<span class="hljs-number"><span class="hljs-number">0</span></span>, right=<span class="hljs-number"><span class="hljs-number">1</span></span>, bottom=<span class="hljs-number"><span class="hljs-number">0</span></span>, top=<span class="hljs-number"><span class="hljs-number">1</span></span>, wspace = <span class="hljs-number"><span class="hljs-number">0.2</span></span>, hspace = <span class="hljs-number"><span class="hljs-number">0.2</span></span>) plt.subplot(<span class="hljs-number"><span class="hljs-number">5</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>,ind+<span class="hljs-number"><span class="hljs-number">1</span></span>) im=val[<span class="hljs-number"><span class="hljs-number">0</span></span>] plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) plt.text(<span class="hljs-number"><span class="hljs-number">120</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, round(val[<span class="hljs-number"><span class="hljs-number">3</span></span>], <span class="hljs-number"><span class="hljs-number">2</span></span>), fontsize=<span class="hljs-number"><span class="hljs-number">11</span></span>, color=<span class="hljs-string"><span class="hljs-string">'red'</span></span>) plt.text(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, val[<span class="hljs-number"><span class="hljs-number">2</span></span>], fontsize=<span class="hljs-number"><span class="hljs-number">11</span></span>, color=<span class="hljs-string"><span class="hljs-string">'blue'</span></span>) plt.imshow(np.transpose(im,(<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>)))</code> </pre> <br><img src="https://habrastorage.org/files/347/02c/344/34702c3443154ca3a2bb1e42b7f385dd.png"><br>  It can be seen that the model decently copes with the task of image recognition in relatively small samples. <br><br>  I hope the post was helpful to you.  I am pleased to hear your questions or suggestions. <br><br>  <a href="https://github.com/MaxTitkov/Keras_InceptionV3_Binary_classification/tree/master">Github Project</a> </div><p>Source: <a href="https://habr.com/ru/post/321834/">https://habr.com/ru/post/321834/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../321822/index.html">Web animation: where, why and why</a></li>
<li><a href="../321824/index.html">The refactoring of the J.Money payment process - the awakening of force</a></li>
<li><a href="../321826/index.html">Getting to know the team of Java stack courses on Hexlet</a></li>
<li><a href="../321828/index.html">Mobile platform. How not to be afraid of ReactNative</a></li>
<li><a href="../321832/index.html">Budget option of transition from the working group to the domain</a></li>
<li><a href="../321836/index.html">Collecting VK posts for Samsung Gear</a></li>
<li><a href="../321838/index.html">How we built cloud infrastructure in Azure</a></li>
<li><a href="../321840/index.html">Notification of problems with your site through Telegrams and other features of Hosttracker</a></li>
<li><a href="../321842/index.html">Routing layer in iOS applications</a></li>
<li><a href="../321844/index.html">Overview of the 5 most popular JavaScript frameworks and libraries 2017</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>