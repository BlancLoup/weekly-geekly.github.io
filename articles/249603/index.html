<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Create a standalone drone on Intel Edison</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="We continue to talk about how to independently make an autonomous flying machine. Last time we talked about the element base, mechanics and control, w...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Create a standalone drone on Intel Edison</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/ce3/767/f9c/ce3767f9c2744d14969003d60563fc64.jpg"><br>  We continue to talk about how to independently make an autonomous flying machine.  <a href="http://habrahabr.ru/company/intel/blog/230299/">Last time</a> we talked about the element base, mechanics and control, we also equipped our device with mind chats based on the OpenCV library.  Time to move on - we need to teach our gadget more subtle and confused things, in other words - to increase its intelligence. <br>  In the second article of the cycle, our colleague from Intel, Paul Guermonprez, offers to change the platform and see what the drone based on the Intel Edison computer can achieve and most importantly how to do it.  Well, at the very end of the post proposal for those who set about trying to translate everything said in practice.  We assure you that under certain conditions <b>it is quite possible to get a hardware platform for experiments from Intel for free!</b> <br><a name="habracut"></a><br><h4>  Maps and measurements </h4>  Civilian drones achieve good results in recording high-definition video and in stabilizing the flight.  You can even plan automatic flight routes on your tablet.  But there is still a serious problem: how to prevent the collision of small drones, in flight, with people and with buildings?  Large drones are equipped with radars and transceivers.  They fly in controlled areas, away from obstacles.  They are remotely controlled by pilots using cameras.  But small civilian drones should be close to the ground, where there are many obstacles.  But pilots can not always get a video image or can be distracted from flight control.  Therefore, it is necessary to develop collision avoidance systems. <br><ol><li>  You can use single beam sonar.  Simple drones are often equipped with vertical sonar to maintain a stable altitude at a short distance from the ground.  Sonar data is extremely easy to interpret: you get the distance in millimeters from the sensor to the obstacle.  Similar systems exist in large aircraft, but directed horizontally: they serve to detect obstacles ahead.  But this information alone is not enough for the flight.  It is not enough to know whether there is any obstacle right in front or not.  It is like extending an arm in the dark. </li><li>  You can use an advanced sensor.  A sonar beam gives information about one distance, like 1 pixel.  When using the radar horizontal view you get full information: 1 distance from all directions.  The development of a one-dimensional sensor is a two-dimensional sensor.  In this family of sensors, you may know Microsoft Kinect.  Intel has Intel RealSense.  From this sensor you get a two-dimensional distance matrix.  Each pixel is a distance.  It looks like a black and white bitmap, where the black pixels are closely spaced objects, and the white dots are objects far away.  Therefore, it is very easy to work with such data: if in a certain direction you see a group of dark pixels, it means that there is an object there.  The range of such systems is limited: sensors the size of a webcam have a range of 2-3 meters, larger sensors - up to 5 meters.  Therefore, such sensors can be useful on drones for detecting obstacles at a short distance (and when moving at a low speed), but they will not allow detecting obstacles at a distance of 100 m. Samples of such drones were shown at CES: <iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/Gj-5RNdUz3I%3Ffeature%3Doembed&amp;xid=17259,15700023,15700186,15700190,15700253&amp;usg=ALkJrhhAScBDiYyRP9GN5pwlCp7gHBcTlQ" frameborder="0" allowfullscreen=""></iframe>  . </li></ol><br><br>  So, single-beam sonar is ideal for measuring the distance to the earth at low altitude.  Sensors such as RealSense are good for short range.  But what if you need to see and analyze the volume objects ahead?  You need computer vision and artificial intelligence! <br><br>  To detect three-dimensional objects, you can either obtain a three-dimensional image using two images from two adjacent webcams (stereoscopic vision), or use successive images taken while moving. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  Our autonomous drone project </h4>  In our project we use the camera <a href="http://us.creative.com/p/web-cameras/creative-senz3d">Intel-Creative Senz3D</a> .  This device includes a two-dimensional distance sensor (with a resolution of 320 x 240), an ordinary webcam, two microphones and accelerometers.  We use a distance sensor to detect objects at short range and a webcam for long range. <br><br><img src="https://habrastorage.org/files/236/b46/360/236b463601ad4997887607fef2fd2d26.png"><br><br>  In this article, we consider mainly long-range computer vision, rather than two-dimensional information about the depth of short-range.  Therefore, the same code can be run with a cheap five-dollar webcam with Linux instead of the powerful multipurpose Senz3D.  But if you need and computer vision, and data on the depth, the camera Senz3D is just perfect. <br><br>  We chose Intel Edison as a built-in drone platform.  In the <a href="http://habrahabr.ru/company/intel/blog/230299/">previous project,</a> we used a full-featured smartphone with Android as a built-in computing platform, but Edison is smaller and cheaper. <br>  Edison is not so much a processor as a full-featured Linux computer: a dual-core Intel Atom processor, RAM, a WiFi module, Bluetooth, and much more.  It is only necessary to select an expansion card for it, relying on the requirements for input-output, and connect it.  In this case, we need to connect the Senz3D USB camera interface, so we use a large expansion card.  But there are tiny boards with only the components you need, and you can easily create the necessary board yourself.  This is just an expansion card, not a motherboard. <br><br><img src="https://habrastorage.org/files/06c/5ba/cee/06c5bacee20e48af9cc0f786abb6ea05.jpg"><br><br><h4>  Installation </h4>  <b>OS</b>  We unpacked Edison, updated the firmware, and saved Yocto Linux OS.  Other Linux flavors are available with many already compiled packages, but we need to install simple software with as few dependencies as possible, so Yocto is fine with us. <br><br>  <b>Software.</b>  We set up WiFi and get access to the board via ssh.  We will edit the source code files and compile them directly on the board via ssh.  You can also compile on a PC with Linux and transfer the binaries.  If you compile code for the 32-bit i686 architecture, it will work on the Edison platform.  Just install the gcc tool chain and your favorite source editor. <br><br>  <b>Camera.</b>  The first difficulty is using a PerC camera with Linux.  This camera was designed for use with Windows.  Fortunately, the company that released the sensor itself provides a driver for Linux: <a href="http://www.softkinetic.com/products/depthsensecameras.aspx">SoftKinetic DepthSense 325</a> . <br>  This is a binary driver, but a version compiled for Intel processors is available.  Using Intel Edison, you can compile in place using gcc or Intel Compiler, but you can also get binaries compiled for Intel and deploy them without modification.  In such situations, an important advantage is compatibility with Intel.  After solving several problems with dependencies and layout, our driver is ready and running.  Now we can get the image from the camera. <br><br>  <b>Sensor data</b>  Depth data is very simple, easy to get from the sensor and easy to analyze.  This is a depth information matrix, its resolution is 320 x 240. The depth is encoded as grayscale.  Black pixels are close, white ones are far away.  The video component of the sensor is a regular webcam.  From the point of view of the developer, the sensor gives us 2 webcams: one is black and white, it returns depth data, the other is color, it gives a normal video image. <br>  We will use video to detect obstacles in the distance.  Only light is needed, and the distance is not limited.  Depth information will be used to detect obstacles at a very close distance from drones, no more than 2‚Äì3 m. <br><br>  <b>Security Notice.</b>  The photos show that we work in the laboratory and simulate the flight of the drone.  The proposed algorithm is still far from ready, so you should not launch the drone in flight, especially when there are people nearby.  First of all, in some countries it is absolutely rightly prohibited.  But more importantly, it is simply dangerous!  The many videos taken by drones that you see on the Internet (taken both indoors and outdoors) are in fact very dangerous.  We also send drones to fly on the street, but under completely different conditions in order to comply with all the requirements of French law and avoid accidents. <br><br><h4>  Code </h4>  So, we have two sensors.  One returns depth data.  You do not need our help to process this data: black is close, white is far.  The sensor is very accurate (accuracy of 1‚Äì3 mm) and works with extremely low latency.  As we said above, this is all fine, but the range is too small. <br>  The second sensor is a regular webcam.  And here we have a new difficulty.  How to get extensive information from a webcam?  In our case, the drone flies along a relatively straight path.  Therefore, you can analyze two consecutive images to detect differences between them. <br>  We get all the important points on each image and try to correlate the difference between the positions of these points to get vectors.  In the photo result, when the drone is in a chair and slowly moving through the laboratory. <br><br><img src="https://habrastorage.org/files/7da/78a/347/7da78a347094410f99ae29d1c0db11e9.jpg"><br><br>  Small vectors are green, long vectors are red.  This means that if between two consecutive snapshots the position of a point changes quickly, then it is red, and if slowly, it is green.  It is clear that the errors still remain, but the result is already quite acceptable. <br>  It's like in Star Trek.  Remember how the starry sky ‚Äústretches‚Äù there when the ship moves faster than the speed of light?  Stars that are close, turn into long white vectors.  Farther stars form short vectors. <br><br>  Then we filter the vectors.  If the vectors (any: both large and small) are on the side, there is no risk of collision.  In the test photo, we find two black suitcases on the side, but we fly right between the masses.  There is no risk. <br>  If the vector is directly in front of you, it means that a collision is possible.  A short vector means you still have time.  A long vector means that there is no time. <br><br><img src="https://habrastorage.org/files/ca1/ac3/134/ca1ac3134bb74f6e81fff5082c8914fa.jpg"><br><br>  Example.  In the previous photo, the suitcases were close, but on the side, that was safe.  The objects in the background were ahead, but far away, also safe.  In the second shot, the objects are already directly in front of us, with large vectors: this is already dangerous. <br><br>  Download project materials <a href="">from here</a> . <br><br><h4>  results </h4>  On the set of equipment described above, we demonstrated 4 theses. <br><ul><li>  Intel Edison platform has enough power to process data from complex two-dimensional sensors and webcams with USB interface.  You can even implement support for computer vision on the board itself, and it is convenient to work with it, since it is a full-fledged computer with Linux.  Yes, you can achieve higher performance per watt if you use a dedicated image processor, but the development of the simplest software for it will take several weeks or even months.  Prototyping using Intel Edison is easy. </li><li>  Analyzing data from a single webcam provides detection of volumetric objects at a basic level, with a frequency of 10‚Äì20 times per second on the Intel Edison platform without optimization.  This is sufficient for detecting three-dimensional objects at a great distance and for a corresponding adjustment of the trajectory. </li><li>  The same combination of hardware and software can support a two-dimensional volume sensor for working with ultra-low latency and obtaining an accurate three-dimensional map.  With Intel Edison and Senz3D, you can solve the problem of avoiding collisions at low speed / short distance and at high speed / long distance. </li><li> The proposed solution is low cost, low weight and low electricity consumption.  This is a practical solution for small consumer drones and professional drones. </li></ul><br><h4>  <b>It's time to act!</b> </h4>  Now how to get a free experiment kit from Intel.  The company continues the academic program <a href="http://habrahabr.ru/company/intel/blog/200336/">Intel Do-It-Yourself Challenge</a> and now we have 10 sets that we will be happy to share with you - subject to certain conditions.  Namely: <br><ul><li>  You are a student or employee of a Russian university; </li><li>  Together with your colleagues, you have enough knowledge to start a project; </li><li>  Your research team is led by a professor or university lecturer; </li><li>  You understand the complexity of the task and are willing to devote enough time to work on it; </li><li>  Your project is not related to the military theme. </li></ul><br>  In this case, you need to formulate your idea, briefly talk about your creative team and send the resulting text in English or French <a href="">in the name of Paul Guermonprez</a> . <br><br><h4>  Reference materials and resources </h4><br><ul><li>  <a href="http://intel-software-academic-program.com/pages/courses">Intel Software Academic Program</a> (project description and source codes) </li><li>  <a href="http://www.intel.com/content/www/us/en/architecture-and-technology/realsense-overview.html">Intel Real Sense</a> </li><li>  <a href="http://www.softkinetic.com/products/depthsensecameras.aspx">SoftKinetic DS325</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/249603/">https://habr.com/ru/post/249603/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../249579/index.html">Unconventional React Overview</a></li>
<li><a href="../249581/index.html">Difficulties in monetizing stolen data</a></li>
<li><a href="../249589/index.html">XSS on sites using the Instagram API</a></li>
<li><a href="../249591/index.html">Microsoft is changing good Windows Store apps for certificates</a></li>
<li><a href="../249597/index.html">(Kiev) On February 5, a video stream of the seminar ‚ÄúENS 15.1: Advanced Switch Operation, Configuration and Management‚Äù will be held</a></li>
<li><a href="../249605/index.html">Streaming audio in iOS using Yandex.Disk as an example</a></li>
<li><a href="../249609/index.html">Professional development in information security</a></li>
<li><a href="../249611/index.html">Free school of iOS developers in Petersburg</a></li>
<li><a href="../249613/index.html">Reverse Engineering KR580VM80A / i8080 completed</a></li>
<li><a href="../249619/index.html">Year of struggle for web development without bugs</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>