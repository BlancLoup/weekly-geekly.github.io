<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The day Dodo IS stopped. Asynchronous script</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello, Habr! Each SRE in our team once dreamed of sleeping peacefully at night. Dreams come true. In this article, I will talk about this and how we a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The day Dodo IS stopped. Asynchronous script</h1><div class="post__text post__text-html js-mediator-article">  Hello, Habr!  Each SRE in our team once dreamed of sleeping peacefully at night.  Dreams come true.  In this article, I will talk about this and how we achieve the performance and stability of our Dodo IS system. <br><br><img src="https://habrastorage.org/webt/wk/2o/t6/wk2ot6razkmzgly1s69fdwz5quq.png"><a name="habracut"></a><br><blockquote>  <b>A series of articles about the collapse of the Dodo IS * system</b> : <br><br>  1. The <a href="https://habr.com/ru/company/dodopizzaio/blog/440676/">day when Dodo IS stopped.</a>  <a href="https://habr.com/ru/company/dodopizzaio/blog/440676/">Synchronous script.</a> <br>  2. The day Dodo IS stopped.  Asynchronous script. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      * <i>Materials were written based on <a href="https://www.youtube.com/watch%3Fv%3DXNuAJmOXgYw%26fbclid%3DIwAR29kXTLGh6kdUhGlIXqze90C7kd8WNKZLVLS6LTJghZ-X9-D1qXjQPzHxM">my performance at DotNext 2018 in Moscow</a></i> . </blockquote>  In a previous article, we looked at blocking code issues in the Preemptive Multitasking paradigm.  It was supposed that it was necessary to rewrite the blocking code on async / await.  So we did.  Now let's talk about what problems arose when we did this. <br><br><h2>  We introduce the term Concurrency </h2><br>  Before you get to async, you must enter the term Concurrency. <br><blockquote>  In queuing theory, <b>Concurrency</b> is the number of clients that are currently inside the system.  Concurrency is sometimes confused with Parallelism, but it's actually two different things. </blockquote>  For those new to Concurrency for the first time, <a href="https://blog.golang.org/concurrency-is-not-parallelism">I recommend Rob Pike's video</a> .  Concurrency is when we deal with many things at the same time, and Parallelism is when we do many things at the same time. <br><br>  In computers, not many things happen in parallel.  One such thing is computing on multiple processors.  The degree of parallelism is limited by the number of CPU threads. <br><br>  In fact, Threads is part of the concept of Preemptive Multitasking, one way to model Concurrency in a program when we rely on the operating system in the Concurrency question.  This model remains useful as long as we understand that we are dealing specifically with the Concurrency model, and not with concurrency. <br><br>  Async / await is syntactic sugar for State Machine, another useful Concurrency model that can run in a single-threaded environment.  In essence, this is Cooperative Multitasking - the model itself does not take parallelism into account at all.  In combination with Multithreading, we get one model on top of another, and life is greatly complicated. <br><br><h2>  Comparison of the two models </h2><br><h4>  How it worked in the Preemptive Multitasking model </h4><br>  Let's say we have 20 Threads and 20 requests in processing per second.  The picture shows a peak - 200 requests in the system at the same time.  How could this happen: <br><br><ul><li>  requests could be grouped if 200 clients pressed the button at the same time; </li><li>  the garbage collector could stop requests for several tens of milliseconds; </li><li>  requests could be delayed in any queue if the proxy supports the queue. </li></ul><br>  There are many reasons why requests for a short period of time have accumulated and come in a single bundle.  In any case, nothing terrible happened, they stood in the Thread Pool queue and slowly completed.  There are no more peaks, everything goes on, as if nothing had happened. <br><br>  Suppose that the smart Thread Pool algorithm (and there are machine learning elements there) decided that there is no reason to increase the number of Threads so far.  The Connection Pool in MySql is also 20, because Threads = 20.  Accordingly, we need only 20 connections to SQL. <br><br><img src="https://habrastorage.org/webt/gm/ch/pz/gmchpzxpvegoyljdauranwzgn7k.png"><br><br>  In this case, the Concurrency level of the server from the point of view of the external system = 200. The server has already received these requests, but has not yet completed it.  However, for an application running in the Multithreading paradigm, the number of simultaneous requests is limited by the current size of Thread Pool = 20. So, we are dealing with the degree of Concurrency = 20. <br><br><h4>  How everything now works in the async model </h4><br><img width="33%" height="33%" src="https://habrastorage.org/webt/dg/yz/pz/dgyzpzj-nl9rawn5ctxdfr3gxgm.png"><br><br>  Let's see what happens in an application running async / await with the same load and distribution of requests.  There is no queue before creating a Task, and the request is immediately processed.  Of course, Thread from ThreadPool is used for a short time, and the first part of the request, before contacting the database, is executed immediately.  Because Thread quickly returns to Thread Pool, we don‚Äôt need a lot of Threads to process.  In this diagram we do not display Thread Pool at all, it is transparent. <br><br><img src="https://habrastorage.org/webt/bm/6h/to/bm6hto7o6gxnrlg9ruzhfsxfet0.png"><br><br>  What will this mean for our application?  The external picture is the same - the level of Concurrency = 200. At the same time, the situation inside has changed.  Previously, requests were "crowded" in the ThreadPool queue, now the degree of application Concurrency is also 200, because we have no restrictions on the part of TaskScheduler.  Hooray!  We have achieved the goal of async - the application "cope" with almost any degree of Concurrency! <br><br><h4>  Consequences: nonlinear degradation of the system </h4><br>  The application has become transparent in terms of Concurrency, so now Concurrency is projected onto the database.  Now we need a connection pool of the same size = 200. The database is the CPU, memory, network, storage.  This is the same service with its problems, like any other.  The more requests we try to execute at the same time, the slower they run. <br><br>  With a full load on the database, at best, Response Time degrades linearly: you gave twice as many queries, it began to work twice as slow.  In practice, due to query competition, overhead will necessarily occur, and it may turn out that the system will degrade non-linearly. <br><br><h4>  Why it happens? </h4><br>  Reasons for the second order: <br><br><ul><li>  Now the database needs to be simultaneously stored in the memory of data structures to serve more requests; </li><li>  Now the database needs to serve larger collections (which is algorithmically disadvantageous). </li></ul><br>  First order reason: <br><br><ul><li>  contention, which was discussed a bit <a href="https://habr.com/ru/company/dodopizzaio/blog/440676/">in the previous article</a> . </li></ul><br>  In the end, async fights against limited resources and ... wins!  The database does not stand up and starts to slow down.  From this, the server further increases Concurrency, and the system can no longer get out of this situation with honor. <br><br><h2>  Server Sudden Death Syndrome </h2><br>  Sometimes an interesting situation occurs.  We have a server.  He works for himself like that, everything is in order.  There are enough resources, even with a margin.  Then we suddenly get a message from clients that the server is slowing down.  We look at the chart and see that there was some surge in customer activity, but now everything is normal.  Thinking of a DOS attack or coincidence.  Now everything seems to be fine.  Only now the server continues to stupid, and it gets tougher until timeouts begin to pour in.  After some time, another server that uses the same database also begins to bend.  Common situation? <br><br><h4>  Why did the system die? </h4><br>  You can try to explain this by the fact that at some point the server received a peak number of requests and ‚Äúbroke‚Äù.  But we do know that the load was reduced, and the server after that didn‚Äôt get better for a very long time, until the load completely disappeared. <br><br>  The rhetorical question: was the server supposed to break due to excessive load?  Do they do that? <br><br><h4>  We simulate a server crash situation </h4><br>  Here we will not analyze graphs from a real production system.  At the time of the server crash, we often cannot get such a schedule.  The server runs out of CPU resource, and as a result, it cannot write logs, give metrics.  On the diagrams at the time of the disaster, a break in all graphs is often observed. <br><br>  SREs should be able to produce monitoring systems that are less prone to this effect.  Systems that in any situation provide at least some information, and at the same time, are able to analyze post-mortem systems using fragmentary information.  For educational purposes, we use a slightly different approach in this article. <br><br>  Let's try to create a model that mathematically works just like a server under load.  Next, we will study the characteristics of the server.  We discard the nonlinearity of real servers and simulate a situation where linear deceleration occurs when the load grows above nominal.  Twice as many requests as needed - we serve twice as slow. <br><br>  This approach will allow: <br><br><ul><li>  consider what will happen at best; </li><li>  take accurate metrics. </li></ul><br>  Scheduled Navigation: <br><br><ul><li>  blue - the number of requests to the server; </li><li>  green - server responses; </li><li>  yellow - timeouts; </li><li>  dark gray - requests that wereted on server resources because the client did not wait for a timeout response.  Sometimes a client may report this to the server by disconnecting, but in general, such a luxury may not be technically feasible, for example, if the server does CPU-bound work without cooperation with the client. </li></ul><br><br><img src="https://habrastorage.org/webt/8d/r8/lr/8dr8lr7gizm-ovozc0laylwadaa.png"><br><br>  Why did the client‚Äôs request graph (blue in the diagram) turn out to be so?  Usually the schedule of orders in our pizzerias smoothly grows in the morning and decreases in the evening.  But we observe three peaks against the background of the usual uniform curve.  This form of the graph was not chosen for the model by chance, but rather.  The model was born during the investigation of a real incident with the server of the pizzeria contact center in Russia during the World Cup. <br><br><h2>  Case "World Cup" </h2><br>  We sat and waited for more orders.  Prepared for the Championship, now the servers will be able to pass a strength test. <br><br>  The first peak - football fans go to watch the championship, they are hungry and buy pizza.  During the first half they are busy and cannot order.  But people who are indifferent to football can, so on the chart everything goes on as usual. <br><br>  And then the first half ends, and the second peak comes.  Fans became nervous, hungry and made three times more orders than in the first peak.  Pizza is bought at a terrible rate.  Then the second half begins, and again not to pizza. <br><br>  Meanwhile, the contact center server begins to slowly bend and serve requests more and more slowly.  The system component, in this case, the Call Center web server, is destabilized. <br><br>  The third peak will come when the match is over.  Fans and the system awaits a penalty. <br><br><h4>  We analyze the reasons for the server crash </h4><br>  What happened  The server could hold 100 conditional requests.  We understand that it is designed for this power and will not stand it anymore.  A peak arrives, which in itself is not so big.  But the gray area of ‚Äã‚ÄãConcurrency is much higher. <br><br>  The model is designed so that Concurrency is numerically equal to the number of orders per second, so visually on the graph it should be of the same scale.  However, it is much higher because it accumulates. <br><br>  We see a shadow from the graph here - these are requests that began to return to the client, executed (shown by the first red arrow).  The time scale here is conditional to see the time offset.  The second peak has already knocked out our server.  He crashed and began to process four times less requests than usual. <br><br><img src="https://habrastorage.org/webt/n1/92/qw/n192qwxtwdrfatt_a-lb8y8eire.png"><br><br>  In the second half of the graph, it is clear that some requests were still executed at first, but then yellow spots appeared - the requests stopped fulfilling completely. <br><br><img src="https://habrastorage.org/webt/pi/la/nv/pilanvzebdl_vl3k3hno9vwezga.png"><br><br>  Once again the whole schedule.  It can be seen that Concurrency is going wild.  A huge mountain appears. <br><br><img src="https://habrastorage.org/webt/ci/l6/kb/cil6kblebzhjokmuvzkwolmngvo.png"><br><br>  Usually we analyzed completely different metrics: how slowly the request was completed, how many requests per second.  We don‚Äôt even look at Concurrency, we didn‚Äôt even think about this metric.  But in vain, because it is precisely this quantity that best shows the moment of server failure. <br><br>  But where did such a huge mountain come from?  The biggest peak load has already passed! <br><br><h2>  Little Law </h2><br>  Little's law governs Concurrency. <br><br>  <i>L (number of customers within the system) = Œª (speed of their stay) ‚àó W (time they spend inside the system)</i> <br><br>  This is an average.  However, our situation is developing dramatically, the average does not suit us.  We will differentiate this equation, then integrate.  To do this, look at the book of John Little, who invented this formula, and see the integral there. <br><br><img src="https://habrastorage.org/webt/sx/f5/dz/sxf5dzgwc9l7low8fild5cpsrf0.png"><br><br>  We have the number of entries in the system and the number of those who leave the system.  The request arrives and leaves when everything is complete.  Below is a region of the growth graph corresponding to the linear growth of Concurrency. <br><br><img src="https://habrastorage.org/webt/ax/rv/du/axrvdu3vyx1iw9afieohv5pe-cs.png"><br><br>  There are few green requests.  These are the ones that are actually being implemented.  The blue ones are those that come.  Between times, we have the usual number of requests, the situation is stable.  But Concurrency is still growing.  The server can no longer cope with this situation itself.  This means that he will fall soon. <br><br>  But why is concurrency increasing?  We look at the integral of the constant.  Nothing changes in our system, but the integral looks like a linear function that grows only up. <br><br><h2>  Will we play? </h2><br>  The explanation with integrals is complicated if you do not remember mathematics.  Here I propose to warm up and play the game. <br><br><h4>  Game number 1 </h4><br>  <b>Prerequisites</b> : The server receives requests, each requires three processing periods on the CPU.  The CPU resource is divided evenly between all tasks.  This is similar to how CPU resources are consumed during Preemptive Multitasking.  The number in the cell means the amount of work left after this measure.  For each conditional step, a new request arrives. <br><br>  Imagine that you received a request.  Only 3 units of work, at the end of the first processing period 2 units remain. <br><br>  In the second period, another request is layered, now both CPUs are busy.  They did one unit of work for the first two queries.  It remains to complete 1 and 2 units for the first and second request, respectively. <br><br>  Now the third request has arrived, and the fun begins.  It would seem that the first request should have been completed, but in this period three requests already share the CPU resource, so the degree of completion for all three requests is now fractional at the end of the third processing period: <br><br><img src="https://habrastorage.org/webt/k-/tq/mv/k-tqmvjsqabbv0zgwt_vmkfkhy4.png"><br><br>  Further more interesting!  The fourth request is added, and now the degree of Concurrency is already 4, since all four requests required a resource in this period.  Meanwhile, the first request by the end of the fourth period has already been completed, it does not go to the next period, and it has 0 work left for the CPU. <br><br>  Since the first request has already been completed, let‚Äôs summarize for him: it ran a third longer than we expected.  It was assumed that the length of each task horizontally ideally = 3, by the amount of work.  We mark it with orange, as a sign that we are not completely satisfied with the result. <br><br><img src="https://habrastorage.org/webt/ap/9n/uy/ap9nuyfqun_gd1ggeidqdlomxuy.png"><br><br>  The fifth request arrives.  The degree of Concurrency is still 4, but we see that in the fifth column the remaining work is more in total.  This happens because the fourth column left more work to do than the third. <br><br>  We continue another three periods.  Waiting for answers. <br>  - Server, hello! <br>  - ... <br><br><img src="https://habrastorage.org/webt/tv/2m/8r/tv2m8r8selzumgub75zkvs78deu.png"><br><br>  ‚ÄúYour call is very important to us ...‚Äù <br><br><img src="https://habrastorage.org/webt/ud/k9/rk/udk9rk7ynqduovmhyymp7shqe0q.png"><br><br>  Well, finally came the answer to the second request.  The response time is twice as long as expected. <br><br><img src="https://habrastorage.org/webt/tt/7m/iv/tt7mivq-stfincjhlwxm7wqznsq.png"><br><br>  The degree of Concurrency has tripled already, and nothing portends that the situation will change for the better.  I did not draw further, because the response time to the third request will no longer fit into the picture. <br><br><blockquote>  Our server has entered an undesirable state, from which it will never exit on its own.  <b>Game over</b> </blockquote><br><h2>  What is the GameOver state of the server characterized by? </h2><br>  Requests are accumulated in memory indefinitely.  Sooner or later, memory will simply end.  In addition, with an increase in scale, the CPU overhead for servicing various data structures increases.  For example, the connection pool should now track timeouts for more connections, the garbage collector should now recheck more objects on the heap, and so on. <br><br>  Exploring all the possible consequences of the accumulation of active objects is not the goal of this article, but even a simple accumulation of data in RAM is already enough to fill up the server.  In addition, we have already seen that the client server projects its Concurrency problems onto the database server, and other servers that it uses as a client. <br><br>  The most interesting: now even if you submit a lower load to the server, it still will not recover.  All requests will end with a timeout, and the server will consume all available resources. <br><br>  And what did we actually expect ?!  After all, we knowingly gave the server an amount of work that it could not handle. <br><br>  When dealing with distributed system architecture, it‚Äôs useful to think about how ordinary people solve such problems.  Take, for example, a nightclub.  It will stop functioning if too many people enter it.  The bouncer copes with the problem simply: it looks how many people are inside.  One came out - the other launches.  A new guest will come and appreciate the size of the queue.  If the line is long, he will go home.  What if you apply this algorithm to the server? <br><br><img src="https://habrastorage.org/webt/wg/tn/p3/wgtnp3n6qq57dqzfi-ac9s0rj1u.png"><br><br>  Let's play again. <br><br><h4>  Game number 2 </h4><br>  <b>Prerequisites</b> : Again we have two CPUs, the same tasks of 3 units, arriving each period, but now we will set the bouncer, and the tasks will be smart - if they see that the queue is 2, then they go home right away. <br><br><img src="https://habrastorage.org/webt/b4/cs/gt/b4csgtkcmi3hw1qog4fko6aus2o.png"><br><br><img src="https://habrastorage.org/webt/uw/gi/-v/uwgi-vopihzere8fs_c5f5yctfo.png"><br><br>  The third request came.  In this period, he stands in line.  He has the number 3 at the end of the period.  There are no fractional numbers in the residuals, because two CPUs perform two tasks, one for a period. <br><br>  Although we have three requests stacked, the degree of Concurrency inside the system = 2. The third is in the queue and does not count. <br><br><img src="https://habrastorage.org/webt/nm/x4/yw/nmx4ywd6xdqyte5b6vkwgco3hdq.png"><br><br>  The fourth came - the same picture, although more work has already been accumulated. <br><br><img src="https://habrastorage.org/webt/uq/jb/_5/uqjb_5b8whjwjzzetwqwrzjusfy.png"><br>  ... <br>  ... <br><br>  In the sixth period, the third request was completed with a third lag, and the degree of Concurrency is already = 4. <br><br><img src="https://habrastorage.org/webt/xs/i1/sf/xsi1sf60jniqqbamvlko-bmd_xa.png"><br><br>  The degree of concurrency has doubled.  She can‚Äôt grow anymore, because we have set a clear ban on this.  With maximum speed, only the first two requests were completed - those who came to the club first, while there was enough space for everyone. <br><br>  The yellow requests were in the system longer, but they stood in line and did not drag out the CPU resource.  Therefore, those inside were quietly entertained.  This could continue even until a man came and said that he would not stand in line, but rather he would go home.  This is a failed request: <br><br><img src="https://habrastorage.org/webt/tf/qf/qr/tfqfqrsfqvhxphy3wzrdqf_cpvk.png"><br><br>  The situation can be repeated endlessly, while the query execution time remains at the same level - exactly twice as long as we would like. <br><br><img src="https://habrastorage.org/webt/xh/l2/8-/xhl28-uoe_jao9hjppy5zc30in8.png"><br><br>  We see that a simple restriction on the Concurrency level eliminates the server viability problem. <br><br><h4>  How To Increase Server Viability Through Concurrency Level Limit </h4><br>  The simplest ‚Äúbouncer‚Äù you can write yourself.  Below is the code using the semaphore.  There is no limit to the length of the line outside.  The code is for illustration purposes only, no need to copy it. <br><br><pre><code class="swift hljs">const int <span class="hljs-type"><span class="hljs-type">MaxConcurrency</span></span> = <span class="hljs-number"><span class="hljs-number">100</span></span>; <span class="hljs-type"><span class="hljs-type">SemaphoreSlim</span></span> bulkhead = new <span class="hljs-type"><span class="hljs-type">SemaphoreSlim</span></span>(<span class="hljs-type"><span class="hljs-type">MaxConcurrency</span></span>, <span class="hljs-type"><span class="hljs-type">MaxConcurrency</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> async <span class="hljs-type"><span class="hljs-type">Task</span></span> <span class="hljs-type"><span class="hljs-type">ProcessRequest</span></span>() { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (!await bulkhead.<span class="hljs-type"><span class="hljs-type">WaitAsync</span></span>()) { <span class="hljs-keyword"><span class="hljs-keyword">throw</span></span> new <span class="hljs-type"><span class="hljs-type">OperationCanceledException</span></span>(); } <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> { await <span class="hljs-type"><span class="hljs-type">ProcessRequestInternal</span></span>(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span>; } finally { bulkhead.<span class="hljs-type"><span class="hljs-type">Release</span></span>(); } }</code> </pre> <br>  To create a limited queue, you need two semaphores.  For this, <a href="https://github.com/App-vNext/Polly">the Polly library</a> , which Microsoft recommends, is suitable.  Pay attention to the Bulkhead pattern.  Literally translated as "bulkhead" - a structural element that allows the ship not to sink.  To be honest, I think the term ‚Äúbouncer‚Äù is better suited.  Importantly, this pattern allows the server to survive in hopeless situations. <br><br>  First, we squeeze out everything that is possible on the load bench from the server until we determine how many requests it can hold.  For example, we determined that it is 100. We put bulkhead. <br><br>  Then the server will skip only the required number of requests, the rest will be queued.  It would be wise to choose a slightly lower number so that there is a margin.  I have no ready-made recommendation on this subject, because there is a strong dependence on the context and the specific situation. <br><br><ol><li>  If the server behavior stably depends on the load in terms of resources, then this number may approach the limit. </li><li>  If the medium is subject to load fluctuations, a more conservative number should be chosen, taking into account the size of these fluctuations.  Such fluctuations can occur for various reasons, for example, the performance environment with GC is characterized by small peaks of load on the CPU. </li><li>  If the server performs periodic tasks on a schedule, this should also be considered.  You can even develop an adaptive bulkhead that will calculate how many queries can be sent simultaneously without degrading the server (but this is already beyond the scope of this study). </li></ol><br><h2>  Query Experiments </h2><br>  Take a look at this post-mortem last, we won‚Äôt see this again. <br><img src="https://habrastorage.org/webt/n1/qn/1i/n1qn1irfrgm-fezifzonark7j94.png"><br>  All this gray heap unambiguously correlates with server crash.  Gray is death for the server.  Let's just cut it off and see what happens.  It seems that a certain number of requests will go home, simply will not be fulfilled.  But how much? <br><br><h4>  100 inside, 100 outside </h4><br><img src="https://habrastorage.org/webt/u0/6c/m5/u06cm5odrt-dxltnomhwatmhwze.png"><br>  It turned out that our server began to live very well and fun.  He constantly plows at maximum power.  Of course, when a peak occurs, it kicks him out, but not for long. <br><br>  Inspired by success, we will try to make sure that he is not bounced at all.  Let's try to increase the length of the queue. <br><br><h4>  100 inside, 500 outside </h4><br><img src="https://habrastorage.org/webt/sk/5i/gi/sk5igi9cfgyjajp8lujawwunace.png"><br><br>  It got better, but the tail grew.  These are the requests that are executed for a long time later. <br><br><h4>  100 inside, 1000 outside </h4><br>  Since something has become better, let's try to bring it to the point of absurdity.  Let‚Äôs resolve the queue length 10 times longer than we can serve simultaneously: <br><br><img src="https://habrastorage.org/webt/2g/qd/s6/2gqds68piszleawyid_yk_rtdhg.png"><br><br>  If we talk about the metaphor of the club and the bouncers, this situation is hardly possible - no one wants to wait at the entrance for longer than spending time in the club.  We will also not pretend that this is a normal situation for our system. <br><br>  It‚Äôs better not to serve the client at all than to torment him on the site or in the mobile application by loading each screen for 30 seconds and spoiling the company's reputation.  It‚Äôs better to immediately honestly tell a small part of customers that now we can‚Äôt serve them.  Otherwise, we will serve all customers several times slower, because the graph shows that the situation persists for quite some time. <br><br>  There is one more risk - other system components may not be designed for such server behavior, and, as we already know, Concurrency is projected onto clients. <br><br>  Therefore, we return to the first option ‚Äú100 per 100‚Äù and think about how to scale our capacities. <br><br><h4>  Winner - 100 inside, 100 outside </h4><br><img src="https://habrastorage.org/webt/z5/cg/pv/z5cgpvn4qs9abj5ifewfelapa6g.png"><br><br>  ¬Ø \ _ („ÉÑ) _ / ¬Ø <br><br>  With these parameters, the greatest degradation in runtime is exactly 2 times the ‚Äúface value‚Äù.  At the same time, it is 100% degradation in query execution time. <br><br>  If your client is sensitive to runtime (and this is usually true both with human clients and server clients), then you can think about further reducing the queue length.  In this case, we can take some percentage of the internal Concurrency, and we will know for sure that the service does not degrade in response time by more than this percentage on average. <br><br>  In fact, we are not trying to create a queue, we are trying to protect ourselves from load fluctuations.  Here, just as in the case of determining the first parameter of the bulkhead (quantity inside), it is useful to determine what fluctuations in load the client may cause.  So we will know in which cases, roughly speaking, we will miss the profit from potential service. <br><br>  It is even more important to determine what Latency fluctuations can withstand other components of the system interacting with the server.  So we will know that we are really squeezing the maximum out of the existing system without the danger of losing service completely. <br><br><h2>  Diagnosis and treatment </h2><br>  We are treating Uncontrolled Concurrency with Bulkhead Isolation. <br>  This method, like the others discussed in this series of articles, is conveniently implemented by <a href="https://github.com/App-vNext/Polly">the Polly library</a> . <br><br>  The advantage of the method is that it will be extremely difficult to destabilize an individual component of the system as such.  The system acquires very predictable behavior in terms of time to complete successful requests and much higher chances of successful complete requests. <br><br>  However, we do not solve all the problems.  For example, the problem of insufficient server power.  In this situation, you must obviously decide to ‚Äúdrop the ballast‚Äù in the event of a jump in load, which we assessed as excessive. <br><br>  Further measures that our study does not address may include, for example, dynamic scaling. </div><p>Source: <a href="https://habr.com/ru/post/461081/">https://habr.com/ru/post/461081/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../46107/index.html">Booting MS Windows from a USB flash drive</a></li>
<li><a href="../461071/index.html">Optimization of database queries on the example of B2B service for builders</a></li>
<li><a href="../461073/index.html">We connect online maps to the navigator on the smartphone. Part 3 - OverpassTurbo</a></li>
<li><a href="../461075/index.html">Business analytics. IT objects, components, tools</a></li>
<li><a href="../461077/index.html">How are pentesters cooked? Entrance Testing for Digital Security Interns</a></li>
<li><a href="../461083/index.html">Writing software with the functionality of client-server utilities Windows, part 02</a></li>
<li><a href="../461085/index.html">Switching the language in the Android application</a></li>
<li><a href="../461087/index.html">Generating dungeons and caves for my game</a></li>
<li><a href="../46109/index.html">Call me back on ... iTouch!</a></li>
<li><a href="../461091/index.html">LED Camelion Lamps</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>