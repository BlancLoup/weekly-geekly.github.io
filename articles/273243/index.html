<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>7 errors of the ETL developer</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Data warehousing projects have long been part of the IT infrastructure of most large enterprises. ETL processes are part of these projects, but develo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>7 errors of the ETL developer</h1><div class="post__text post__text-html js-mediator-article">  Data warehousing projects have long been part of the IT infrastructure of most large enterprises.  ETL processes are part of these projects, but developers sometimes make the same mistakes when designing and maintaining these processes.  Some of these errors are described in this post. <br><a name="habracut"></a><br>  I would like to immediately narrow the scope of the discussion and agree on terminology: <br><br><ul><li>  Data storage (Datawarehouse, DWH) implies traditional SQL DWH (Oracle Database, MS SQL Server, etc.); <br></li><li>  When simulating DWH, the concepts of single version of truth and historical truth are usually implied; <br></li><li>  ETL-process (Extraction-Transformation-Loading) means the process of loading data from one or several source systems (source systems) into DWH. <br></li><li>  DWH was not created yesterday, and currently several development teams are independently working on it with their projects. <br></li></ul><br>  The term ETL is often interpreted differently, due to the ‚Äúsimple‚Äù interpretation of its abbreviation.  In fact, ETL tasks are only a subset of Data Movement tasks.  In Kimball, in his book ‚ÄúThe Data Warehouse ETL Toolkit‚Äù there are 3 operations that must be performed by the ETL process: <br><ol><li>  Download data in the most convenient form for analytical applications; <br></li><li>  In the process of loading data, enrich them with additional information; <br></li><li>  Record and document lineage (origin) of data. <br></li></ol><br>  The first point is fairly obvious, so I‚Äôll skip it.  The second point says that the data should not just be reloaded from one place to another, but also enriched in the process, for example, with new calculated attributes, technical attributes (download session id, download date, source system, etc.).  The third one says that for any recording it should be possible to track from where and when this record appeared in DWH, when and by what process it changed. <br><br>  In general, the essence of most errors of an ETL developer can be explained by ignoring the life rule from this picture. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/789/82c/c5b/78982cc5b616bf2e24d5d52670c6a39a.png"><br><br>  Further examples will be used for DWH based on Oracle 11g.  So let's get started. <br><br><h1>  1. Using the system date (or a similar function) in business logic </h1><br>  One of the easiest and most common mistakes, especially for inexperienced developers.  Suppose there is a business rule: during the ‚Äúnight window for loading‚Äù, unload orders that were closed for that day (across the close_date field).  The result is sometimes something like this sql statement: <br><br>  <i>insert &lt;....&gt; into target_table</i> <i><br></i>  <i>select &lt;....&gt; from orders</i> <i><br></i>  <i>where close_date&gt; = sysdate () - 1</i> <br><br>  Even if we forget that sysdate () can contain not only a date, but also a time, then we have problems with this script when the regular work of the ETL process is disrupted for quite obvious reasons (the original system will upgrade to the new version, lost connection with the original system, because of the new ETL process, a place in the temporary tablespace, etc. has ended.  Those.  at the moment when our ETL process needs to be restarted for some reason or suspended for a while and then restarted.  Something interesting can also happen if for some reason this process is launched twice a day. <br><br>  The solution to this error is usually simple: to parameterize the call to this process, and if necessary, then use sysdate () as the default value with the possibility of overriding.  Although using the datetime type field to process the delta from the point of view of HD maintenance is not very optimal, and instead it‚Äôs better to use the delta on some discrete field (for example, the integer load session id or something like that) <br><br><h1>  2. Data profiling was not done before starting development. </h1><br>  Even the most documented and developed by all rules and methods of the original system usually contains incorrect or inconsistent data, despite the numerous assurances of its developers or support teams.  And relying on assurances of correctness on the other side of the barricades is usually fraught with problems at the end of development.  Any data source (table, file, xml, json, etc.) must be checked against the logical model DWH.  There are various tools for data profiling, both tools that are built into ETL and are independent of them.  I will list the most popular checks: <br><br><h2>  Check # 1: Uniqueness of identifiers and natural keys of source data </h2><br>  The difference between an identifier and a natural key is that an identifier is usually some kind of surrogate value that technically identifies a string, and a natural key is a value or a combination of values ‚Äã‚Äãthat have business meaning. <br><br>  Order_details table: <br><table><tbody><tr><td>  order_details_id <br></td><td>  document_position <br></td><td>  order_id <br></td></tr><tr><td>  35346346 <br></td><td>  ten <br></td><td>  1224114 <br></td></tr><tr><td>  35346365 <br></td><td>  20 <br></td><td>  1224114 <br></td></tr><tr><td>  ... <br></td><td>  ... <br></td><td>  ... <br></td></tr><tr><td>  35345464 <br></td><td>  ten <br></td><td>  1224438 <br></td></tr></tbody></table><br>  In this example, order_details_id is an identifier, and the combination document_position + order_id is a natural key. <br><br>  Example: I participated in a project on loading data into DWH from a distributed (instance-based) system, in which the accounting of network infrastructure objects was kept.  The developers of this system in the blue eye assured that the id of this object is unique and even showed in the initial system a unique index on the table in confirmation of their words.  The catch did not come to light right away: it turns out that the uniqueness of these ids existed only within one instance of the system, and when we tried to load all the data from all instances, we got a problem with uniqueness.  As a result, we had to change the data model and expand the natural key of the ‚Äúnetwork object‚Äù entity with an additional ‚Äúinstance‚Äù field to ensure uniqueness. <br><br><h2>  Check # 2: Data Types </h2><br>  If the field is called Order_nr, then it does not necessarily contain only numeric values ‚Äã‚Äã- there may well be alphanumeric sequences.  It is also always worth checking the length of the fields.  This problem is usually characteristic of file data sources - database tables are usually well typed. <br><br><h2>  Check # 3: Referential Integrity (FK Check) </h2><br>  The fact that the developer shows the ER-diagrams of his original system, shows on his DEV-environment the existing FK between the tables, and in general his mother swears that he has everything under control, is not a reason not to check the existence of "dangling" records.  Since  he may not be aware that in the productive environment the DBA has already disabled this check to improve performance (of course, agreeing with the developer manager, i.e. no one is to blame).  Also problems with referential integrity are very common for file data sources.  Also, do not forget about the use of the script <a href="http://www.kimballgroup.com/2006/04/design-tip-78-late-arriving-dimension-rows/">late-arriving-data</a> (for example, if the data <a href="http://www.kimballgroup.com/2006/04/design-tip-78-late-arriving-dimension-rows/">arrives is</a> agreed today, it is far from a fact that it will be so in six months). <br><br><h2>  Check # 4: NULL values </h2><br>  The main problem with NULL values ‚Äã‚Äãis that NULL &lt;&gt; NULL, so any queries with a join across a field that may contain NULL will return unpredictable results.  Therefore, all important fields should be wrapped with the nvl () construct.  There is a separate holivar about loading NULL in non-key fields or replacing it with some default values.  I‚Äôm closer to the idea of ‚Äã‚Äãa universal replacement of NULLs for a more standardized approach to using DWH, but I don‚Äôt undertake to insist that you always have to do this. <br><br><h2>  Check # 5: Dates </h2><br>  Checking fields with dates are usually the most complicated, because  in addition to standard checks, it is necessary to take into account that not all dates that are acceptable from the point of view of the database are such from the point of view of DWH: the date ‚Äú21-07-1007‚Äù is hardly acceptable for the date of the conclusion of the contract for the provision of cellular services.  When modeling DWH, there are usually so-called.  dates of ‚Äúbeginning of time‚Äù and ‚Äúend of time‚Äù (other names are possible), and any date not falling in this time range should be replaced with some default value. <br><br>  Special mention should be made of the use of data types like varchar (8) for storing dates (in a format like '20151201'), because  The number of checks here should be even greater. <br><br><h1>  3. Removing duplicates via GROUP BY or DISTINCT </h1><br>  Despite the fact that all data that comes from a source is usually loaded into DWH, there are scenarios when deliberately duplicated data arrives.  But the uniqueness of the natural key requires only one record of duplicates.  There are two wrong ways to remove duplicates: <br><br><h2>  Wrong way # 1: GROUP BY </h2><br>  Suppose we are loading customer addresses and we know that theoretically several records with address information can come for one customer (they are usually complete duplicates due to problems, for example, with synchronization).  Yielding to the desire to solve the problem "in the forehead," the developer can write such a query: <br><br>  <i>insert into customer_address</i> <i><br></i>  <i>select customer_id, max (street_name), max (house_nr)</i> <i><br></i>  <i>from source_table</i> <i><br></i>  <i>group by customer_id</i> <br><br>  The problems will begin if two really different records for one client come to the input (for example, there was an operator input error that he fixed, but both versions of the record were added to the data source): <br><table><tbody><tr><td>  customer_id <br></td><td>  street_name <br></td><td>  house_nr <br></td></tr><tr><td>  1321 <br></td><td>  Moskovskaya str <br></td><td>  127 <br></td></tr><tr><td>  1321 <br></td><td>  Pushkinskaya str <br></td><td>  34 <br></td></tr></tbody></table><br>  A query can return this result (depending on the locale): <br><table><tbody><tr><td>  customer_id <br></td><td>  street_name <br></td><td>  house_nr <br></td></tr><tr><td>  1321 <br></td><td>  Pushkinskaya str <br></td><td>  127 <br></td></tr></tbody></table><br>  There was no such record in the source data, and DWH users may have a reasonable question, what is it all about?  In fact, the 3rd requirement for the ETL process was violated here: an entry was loaded into DWH that could not be tracked to the source system, in other words, which is not there.  And this is a clear ETL developer‚Äôs error. <br><br><h2>  Wrong way # 2: DISTINCT </h2><br>  The second solution to the forehead option in the scenario described above is to use DISTINCT to remove duplicate records. <br><br>  <i>insert into customer_address</i> <i><br></i>  <i>select distinct customer_id, street_name, house_nr</i> <i><br></i>  <i>from source_table</i> <br><br>  In this case, a pair of duplicate records with different attributes will be identified earlier, because instead of one there will be two records, and the uniqueness of the natural key will be violated and the ETL process will fall with an error. <br><br><h2>  One of the right ways </h2><br>  How should solve the problem of having two entries with the same natural key, but different attributes?  Obviously, if no changes are made to the data data model, then only one correct one should be selected from all the records.  You need to choose it according to a predetermined criterion: if the information is rather critical, then you can implement various Data Quality scenarios, if not, then take the last downloaded one as a valid record. <br><br>  <i>insert into customer_address</i> <i><br></i>  <i>select customer_id, street_name, house_nr from (</i> <i><br></i>  <i>select customer_id, street_name, house_nr,</i> <i><br></i>  <i>row_number () over (partition by customer_id order by change_datetime desc) row_num</i> <i><br></i>  <i>from source_table)</i> <i><br></i>  <i>where row_num = 1</i> <br><br>  In general, it should be remembered that any entry in DWH should be able to be tracked to the source (s) of data depending on the business rule and not create ‚Äúunexplainable‚Äù entries. <br><br><h1>  4. Using "static" scripts from source systems </h1><br>  Very often, business logic for DWH entities comes from developers or analysts of source systems in the form of SQL scripts.  And this is a big help for an ETL developer, but, as they say, ‚Äúfear the Daians who bring gifts‚Äù: as a rule, these scripts capture some conditionally ‚Äústatic‚Äù state of the original system at some point in time, and the ETL developer usually tracks data dynamics. and downloading only changes ("delta").  What should alarm in these "static" SQL scripts?  Here are some of: <br><br><ul><li>  aggregate functions (SUM, AVG, COUNT, etc.) <br></li><li>  IN and EXISTS operators <br></li><li>  window functions (OVER (PARTITION BY ...)) <br></li></ul><br>  An example of such a script: <br><br>  <i>insert order_id into orders_from_calls</i> <i><br></i>  <i>select order_id from orders</i> <i><br></i>  <i>where order_id IN (select order_id from calls where order_id &lt;&gt; -1)</i> <i><br></i>  <i>and changed_date&gt; $ last_loaded_date</i> <br><br>  It seems to be all logical: load all orders to our <i>order_from_calls</i> table that are referenced in the call table and for which the last modified date is more than the last download date.  Now imagine that the update of the table <i>calls</i> in DWH did not occur (for example, it is loaded from another source system and the connection with it is broken for some reason), and this request did not load some <i>id</i> orders.  After that, the table <i>calls</i> was reloaded correctly, and there these missed order <i>IDs</i> appeared, but we will not load them into the <i>order_from_calls</i> table, since  nothing has changed in the <i>orders</i> table and the new launches of this query will not give anything.  Therefore, in this case, the delta should be monitored not only by the <i>orders</i> table, but also by the <i>calls</i> table. <br><br><h1>  5. Development on a small amount of data for development </h1><br>  As a rule, ETL-developer for development on the DEV-environment unloads a small part of the data from the productive system, on which it is proposed to develop and debug the work of the ETL-processes.  Unfortunately, solutions developed on such a small amount of data usually lead to various problems on a productive system, such as insufficient performance, lack of space for intermediate tables (for example, the developer decided to beautifully separate business logic steps on a set of intermediate tables, sequentially overloading from one to another one - but in the productive data system it turned out to be too much, and the tablespace for temporary tables suddenly ended). <br><br>  Unfortunately, this error ETL-developer can not always solve on their own, due to various regulations and instructions, lack of budget for a full-fledged DEV-environment with the same amount of data as on the production, etc.  Therefore, it is worth considering as a project risk. <br><br>  One way out is to split the project stages into smaller ones and make releases more often in order to identify such problems not at the end of the project, but at least in the middle. <br><br><h1>  6. Improper use of technical and business dates </h1><br>  There are 2 types of dates in DWH: business dates and technical dates.  The difference is in their origin: a business date is the date that came from a data source or was created according to business rules;  technical date is the date that was generated by the ETL process or DWH itself.  And very often they are used incorrectly: <br><br><h2>  # 1 Business dates are used as technical dates. </h2><br>  If an entity is historized as SCD2 (Slowly Changing Dimension type 2) and the data source contains the fields "_from" and "_to", which the ETL developer is offered to use as validity ranges of the data, then it should have just reinforced concrete guarantees that all validity ranges for each natural key there will be: 1) non-overlapping, 2) there will be no gaps between ranges, 3) the union of these date ranges will coincide with the date range ‚Äúfrom the beginning of time‚Äù to the ‚Äúend of time‚Äù set for your DWH (for example, it can be a pair of dates "01/01/1000  and "31.12.9999" or "11.11.1111" and "09.09.9999").  As a rule, developers of source systems do not bother much, and if the rule of "non-overlapping date ranges" is usually followed, then problems usually arise with the 2nd and 3rd paragraph.  In any case, the general recommendation is not to use business dates for SCD2, but to generate your technical dates. <br><br><h2>  # 2 Technical dates are used as business dates. </h2><br>  Very often, data sources do not supply fields to track any key dates: for example, the document has only the closing status, but not the timestamp when this event occurred, and the solution is to use the technical dates "_from" and "_to", which were generated by the ETL process.  However, this solution works before the first ETL process failure (for example, stopping the ETL processes for a couple of days): the failure occurred on Monday, the recovery occurred on Wednesday, and since  The original system worked quite well all this time, all created documents will be loaded as created on Wednesday.  In general, the ‚Äúhistorical truth‚Äù scenario cannot be implemented if the data source does not supply all the dates needed by the users and can only be emulated (using technical dates), but in this case this scenario should be spoken and described in the documentation so that in a year users were not surprised at the zero number of closed documents on Monday and Tuesday, as well as triple their number on Wednesday. <br><br><h1>  7. ‚ÄúMechanical‚Äù implementation </h1><br>  This is one of the most difficult to identify errors and, in truth, it is not an ETL developer‚Äôs error, but rather the DWH architect.  But the same team is working on the project, and it is necessary to help out colleagues too. <br><br>  Sometimes it happens that the target entity in DWH was incorrectly modeled on the basis of discrepancies in terminology for the developer of the source system and the architect.  The developer of the source system thinks with the categories of its source system, the DWH architect also needs to think through various integration schemes, how to connect multiple objects from heterogeneous source systems in a single DWH. <br><br>  I will describe with the example of the ‚Äúcustomer‚Äù entity as one of the typical problems for this kind: in the data source there is a ‚Äúcustomer‚Äù table, which has a unique natural key, the referential integrity is in order.  Based on this table, the ‚Äúcustomer‚Äù entity was created in DWH.  Based on the name, it is logical to assume that one record in this table should correspond to one client, but in fact it turned out that in fact the same real client could have several records with the same attributes, but different natural keys.  And this would lead to an unpleasant collision for DWH users who used this entity, for example, to count the total number of company customers.  As a result, it was decided to divide this entity into two: ‚Äúcustomer_record‚Äù and ‚Äúcustomer‚Äù, connected through the FK by the relation M: 1. <br><br>  And if the ETL developer had ‚Äúmechanically‚Äù implemented everything according to the specification, then he would certainly not be guilty, but he had the opportunity to notice this, since  in any case, compared with the architect, he works relatively speaking ‚Äúon the ground‚Äù, unlike the architect who is ‚Äúhovering in the clouds‚Äù. <br><br>  In general, some symptoms of a ‚Äúmechanical‚Äù implementation can be mentioned: <br><br><ul><li>  "Inherit" table names from source system <br></li><li>  Copy business logic from existing data streams to new ones. <br></li><li>  Use Joba "pilot" project mainly <br></li></ul><br>  What should be done to minimize the risks of "mechanical" implementation: <br><ul><li>  Carefully analyze business rules with any joins that could potentially ‚Äúcut off‚Äù some of the data (left outer join is usually preferable to inner join) <br></li><li>  On the other hand, check for ‚Äúdoubtful‚Äù joins, which can ‚Äúpropagate‚Äù the data due to incorrect or incomplete conditions. <br></li></ul><br>  Summarizing this point: you should always understand what exactly you load into DWH and whether the name matches the content, and also to load no more and no less data than is required. <br><br><h1>  Conclusion </h1><br>  Of course, this list is not complete, but I hope that this article can bring some order in the heads, which are already confused with deadlines, milestones, releases and bugfixes. </div><p>Source: <a href="https://habr.com/ru/post/273243/">https://habr.com/ru/post/273243/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../273225/index.html">Responsibility of the vendor. Who is responsible for the accident?</a></li>
<li><a href="../273231/index.html">Fast algorithm for graph isomorphism problem published</a></li>
<li><a href="../273233/index.html">Intel GPA and Android gaming performance improvement</a></li>
<li><a href="../273237/index.html">Robot seller. Automating your sales team with SaaS</a></li>
<li><a href="../273241/index.html">We study the graph-oriented DBMS Neo4j on the example of the lexical database Wordnet</a></li>
<li><a href="../273245/index.html">Announcement of the JPoint 2016 Java Conference</a></li>
<li><a href="../273247/index.html">We are testing the Russian E-Class server platform from T-Platforms</a></li>
<li><a href="../273249/index.html">How to get to the president's cottage at five o'clock in the morning</a></li>
<li><a href="../273251/index.html">DaData.ru finds and destroys the same people.</a></li>
<li><a href="../273253/index.html">Parsing formulas in 50 lines in Python</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>