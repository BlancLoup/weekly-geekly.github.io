<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Probabilistic programming - the key to artificial intelligence?</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Some water 
 More than a year and a half ago, there was a news that "DARPA intends to make a revolution in machine learning . " Of course, DARPA just ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Probabilistic programming - the key to artificial intelligence?</h1><div class="post__text post__text-html js-mediator-article"><h4>  Some water </h4><br>  More than a year and a half ago, there was a news that <a href="http://habrahabr.ru/post/174145/">"DARPA intends to make a revolution in machine learning</a> . <a href="http://habrahabr.ru/post/174145/">"</a>  Of course, DARPA just allocated money for a research program related to probabilistic programming.  The very same probabilistic programming exists and develops without DARPA for quite a long time, and research has been conducted both in leading universities, such as MIT, and in large corporations, such as Microsoft.  And it‚Äôs not for nothing that DARPA, Microsoft, MIT, etc.  Pay close attention to this area, because it is really promising for machine learning, and, perhaps, for artificial intelligence in general.  It is said that probabilistic programming for machine learning will play the same role as high-level languages ‚Äã‚Äãfor ordinary programming.  We would bring another parallel - with the role of the Prologue, which he played for the good old AI.  Here only in RuNet on this topic you can still find only single references, and then mostly containing only descriptions of general principles.  Perhaps this is due to the fact that the potential of probabilistic programming has just begun to unfold and it has not become the main trend.  However, what are probabilistic languages ‚Äã‚Äãcapable of or will be capable of? <br><a name="habracut"></a><br>  Two main classes of probabilistic programming languages ‚Äã‚Äãcan be distinguished: these are languages ‚Äã‚Äãthat allow the generation of generative models only in the form of Bayesian networks (or other graphical probabilistic models), or Turing-complete languages. <br><br>  A typical representative of the first is Infer.NET, developed by Microsoft.  In it, due to the use of Bayesian networks as generative models, it is possible to apply known for them effective methods of derivation.  Naturally, the use of a well-known class of models with well-known inference methods does not lead to the possibility of solving some fundamentally new tasks (and even such generative models as deep learning networks based on limited Boltzmann machines are not representable in such languages), but it gives quite practical tool.  As the developers say, using this tool, it is possible to implement a non-trivial probabilistic model in a couple of hours, such as the full Bayesian version of the main component analysis, which will only take a couple dozen lines of code and for which a separate implementation of an efficient output procedure in ordinary language would require a significantly larger amount knowledge and several weeks of work.  Thus, due to probabilistic programming, the use of graphical models becomes much simpler and more accessible. <br><br>  However, Turing-complete probabilistic languages ‚Äã‚Äãhave much greater potential.  They allow you to go beyond the class of tasks that existing methods of machine learning are already able to solve.  Naturally, in such languages ‚Äã‚Äãa problem arises of the efficiency of inference, which is still far from being solved, which leads to poor scalability for real-world problems.  However, this direction is being actively developed, and there are a number of works showing how to achieve effective inference for interesting practical problems in general probabilistic languages.  It is hoped that in the near future these solutions will be available for use in specific languages.  In addition, Turing-complete probabilistic languages ‚Äã‚Äãare already very useful in research related to cognitive modeling and general artificial intelligence.  For these reasons, we consider the basic principles of probabilistic programming using the example of Turing-complete languages, of which we chose Church (Church), which is an extension of Lisp (more specifically, its dialect - Scheme).  The convenience of this language (at least for the purposes of initial acquaintance with it) lies in the existence for it of a web-implementation (web-church) with which one can experiment without installing additional software. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  So, to the point </h4><br>  A program in a probabilistic language may, at first glance, be no different from a program in an ordinary language.  That is what is done in Church.  As in a regular Lisp, variables can be defined in this language, functions can be performed deterministic calculations.  For example, the following program sets a function of one argument, which calculates the factorial by the recursive formula n! = N * (n ‚Äì 1) !, and calls this function for n = 10 <br><br><pre><code class="lisp hljs">(<span class="hljs-name"><span class="hljs-name">define</span></span> (<span class="hljs-name"><span class="hljs-name">fn</span></span>) (<span class="hljs-name"><span class="hljs-name">if</span></span> (<span class="hljs-name"><span class="hljs-name">=</span></span> n <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-number"><span class="hljs-number">1</span></span> (<span class="hljs-name"><span class="hljs-name">*</span></span> n (<span class="hljs-name"><span class="hljs-name">f</span></span> (‚Äì n <span class="hljs-number"><span class="hljs-number">1</span></span>))))) (<span class="hljs-name"><span class="hljs-name">f</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span>)</code> </pre> <br><br>  Also in this language there may be calls to (pseudo) random functions.  For example, if you make a call (flip 0.3) with probability 0.3, #t will be returned, and with probability 0.7 - #f.  Such a function is simply implemented in Lisp as <pre> <code class="lisp hljs">(<span class="hljs-name"><span class="hljs-name">define</span></span> (<span class="hljs-name"><span class="hljs-name">flip</span></span> p) (<span class="hljs-name"><span class="hljs-name">&lt;</span></span> (<span class="hljs-name"><span class="hljs-name">random</span></span>) p))</code> </pre>  Church, like other probabilistic languages, includes many built-in functions that return random values ‚Äã‚Äãin accordance with a particular distribution.  For example, (gaussian x0 s) returns a real random variable distributed over a Gaussian with given parameters.  As other implemented probability distributions, there are usually uniform, multinomial, Dirichlet, beta, gamma.  All these distributions are not so difficult to implement manually in ordinary language, and there is not yet a fundamental difference between Church and Lisp. <br><br>  However, besides the usual semantics, the program on Church has probabilistic semantics, within which it is assumed that a program containing random function calls does not just generate some specific values ‚Äã‚Äãof random variables when it starts, but specifies the probability distribution over them.  So, (gaussian x0 s) is not just a function that returns some specific value of a random variable distributed over a Gaussian, but precisely the Gaussian distribution itself. <br><br>  But how to get these probability distributions specified by the program?  Imagine, for example, a program <pre> <code class="lisp hljs">(<span class="hljs-name"><span class="hljs-name">if</span></span> (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.4</span></span>) (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.1</span></span>) (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.6</span></span>))</code> </pre>  That is, with a probability of 0.4, the value of this expression is P (#t) = 0.1 and P (#f) = 0.9, and with a probability of 0.6, P (#t) = 0.6 and P (#f) = 0.4.  Where does the final distribution defined by this expression come from: P (#t) = 0.4 and P (#f) = 0.6?  This probabilistic semantics is often implemented through the sampling process: we can simply run a program many times and build a sample of the results of its execution.  Such a procedure, of course, is also easy to implement in ordinary language (and, indeed, even Simula-67 was regularly used in this way for modeling stochastic processes in this way). <br><br>  However, modern probabilistic languages ‚Äã‚Äãgo further and add to the sampling process a condition imposed on the results of the program.  This idea leads to the simplest sampling with failures, which in Church is implemented by the rejection-query function.  This function accepts a probabilistic program as an input (as a set of define), the penultimate expression in which calculates the return value, and the last expression is a condition (predicate) that must be true during execution.  Consider the program <br><br><pre> <code class="lisp hljs">(<span class="hljs-name"><span class="hljs-name">rejection-query</span></span> (<span class="hljs-name"><span class="hljs-name">define</span></span> A (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.4</span></span>)) (<span class="hljs-name"><span class="hljs-name">define</span></span> B (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.6</span></span>)) B (<span class="hljs-name"><span class="hljs-name">or</span></span> AB))</code> </pre> <br><br>  A rejection-query executes the program submitted to it until the last condition is fulfilled - here (or AB) - and returns (once) the value of the penultimate expression - here B. You can use the repeat function to get a sample of values.  Church also has built-in functions for plotting histograms.  Consider a slightly advanced program: <br><br><pre> <code class="lisp hljs">(<span class="hljs-name"><span class="hljs-name">define</span></span> (<span class="hljs-name"><span class="hljs-name">get-sample</span></span>) (<span class="hljs-name"><span class="hljs-name">rejection-query</span></span> (<span class="hljs-name"><span class="hljs-name">define</span></span> A (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.4</span></span>)) (<span class="hljs-name"><span class="hljs-name">define</span></span> B (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.6</span></span>)) B (<span class="hljs-name"><span class="hljs-name">or</span></span> AB))) (<span class="hljs-name"><span class="hljs-name">hist</span></span> (<span class="hljs-name"><span class="hljs-name">repeat</span></span> <span class="hljs-number"><span class="hljs-number">1000</span></span> get-sample))</code> </pre> <br><br>  When starting, we will get the following result: #f - 21%, #t - 79% (the numbers from launch to launch may vary slightly).  This result means that the value of B is equal to #t with a probability slightly lower than 0.8.  Where did this probability come from, if in program B is a binary random variable for which P (#t) = 0.6?  Obviously, the point is to impose a condition: (or AB).  In the process of sampling, we take only such values ‚Äã‚Äãof B, which is either A or B itself. In fact, we consider the a posteriori probability P (B | A + B).  One could use the Bayes rule to calculate this probability manually: <br><br><pre>         P (B | A + B) = P (A + B | B) P (B) / P (A + B) = 
         = (P (A | B) + P (B | B) ‚ÄìP (A | B) P (B | B)) P (B) / (P (A) + P (B) ‚ÄìP (A) P (B)) =
         = (P (A) + 1 ‚Äì P (A)) P (B) / (P (A) + P (B) ‚ÄìP (A) P (B)) = 0.6 / (0.4 + 0.6‚Äì0.4 * 0.6 ) = 0.789.
</pre><br>  However, for such an elementary program, manual application of the Bayes rule takes some time, and for non-trivial programs, analytically calculating the values ‚Äã‚Äãmay not be possible at all. <br><br>  So, sampling allows us to calculate a posteriori probabilities of random variables of interest when applying certain conditions.  It replaces the Bayes Rule, widely used in machine learning for model selection or prediction fulfillment.  At the same time, recording a program in a probabilistic language for many people may be much clearer than applying the Bayesian rule.  Of course, notch sampling itself can be very simply implemented in a conventional programming language, but probabilistic languages ‚Äã‚Äãare not limited to this. <br>  In Church, in particular, implemented another function for sampling - enumeration-query.  Run the program <br><br><pre> <code class="lisp hljs">(<span class="hljs-name"><span class="hljs-name">enumeration-query</span></span> (<span class="hljs-name"><span class="hljs-name">define</span></span> A (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.4</span></span>)) (<span class="hljs-name"><span class="hljs-name">define</span></span> B (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.6</span></span>)) B (<span class="hljs-name"><span class="hljs-name">or</span></span> AB))</code> </pre> <br><br>  At the output we get: ((#t #f) (0.7894736842105263 0.2105263157894737)).  Here, the exact values ‚Äã‚Äãare derived (of course, with a discount on the final bit grid) of the probabilities P (B | A + B).  The enumeration-query no longer simply runs the program many times, but it analyzes the ways to execute it and iterates through all possible values ‚Äã‚Äãof random variables, taking into account their probabilities.  Of course, such a ‚Äúsampling‚Äù will work only when the set of possible combinations of values ‚Äã‚Äãof random variables is not too large. <br><br>  There is a more advanced replacement for the notch-based sampling based on MCMC (Monte Carlo Markov Chains), namely the Metropolis Hastings algorithm, hence the name of the procedure - mh-query.  This query procedure immediately generates a specified number of samples (and also receives one additional parameter at the input, the lag).  This procedure is also non-trivial in implementation, so the use of a finished probabilistic language (rather than the own implementation of simple sampling procedures in ordinary language) makes sense. <br><br>  However, the main thing that probabilistic programming gives is the style of thinking. <br><br><h4>  From basics to application </h4><br>  Different developers find different uses for probabilistic programming.  Many use it directly to solve machine learning problems.  The authors of the Church language, Noah D. Goodman and Joshua B. Tenenbaum, in their web book Probabilistic Models of Cognition show the use of probabilistic programming for cognitive modeling.  It is also known how it is convenient to represent the solution of planning problems in terms of inference in probabilistic languages.  It also turns out to be applicable for knowledge representation and output over them, as well as for computer perception tasks (including image recognition).  All these applications are still more or less fragmented, but the presence of a common framework for all of them suggests that probabilistic programming can become a ‚Äútheory of great unification‚Äù for AI.  Let's look at the simplest examples of possible use. <br><br>  One of the most classic examples of the use of expert systems is medical diagnostics.  In particular, the MYCIN system was built on a system of rules of the form: <br><br>  Rule 52: <br>  If <br><ol><li>  THE SITE OF THE CULTURE IS BLOOD </li><li>  THE GRAM OF THE ORGANISM IS NEG </li><li>  THE MORPHOLOGY OF THE ORGANISM IS ROD </li><li>  THE BURN OF THE PATIENT IS SERIOUS </li></ol><br>  Then there is weakly suggestive evidence (0.4) that <br><ol><li>  THE IDENTITY OF THE ORGANISM IS PSEUDOMONAS </li></ol><br><br>  Obviously, rules of this type are well described in a language like Church.  Moreover, there is no need to also implement the inference procedure - it is enough to simply write down the rule system.  Let's give an example from the mentioned book ‚ÄúProbabilistic Models of Cognition‚Äù: <br><br><pre> <code class="lisp hljs">(<span class="hljs-name"><span class="hljs-name">define</span></span> samples (<span class="hljs-name"><span class="hljs-name">mh-query</span></span> <span class="hljs-number"><span class="hljs-number">1000</span></span> <span class="hljs-number"><span class="hljs-number">100</span></span> (<span class="hljs-name"><span class="hljs-name">define</span></span> lung-cancer (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.01</span></span>)) (<span class="hljs-name"><span class="hljs-name">define</span></span> TB (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.005</span></span>)) (<span class="hljs-name"><span class="hljs-name">define</span></span> cold (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.2</span></span>)) (<span class="hljs-name"><span class="hljs-name">define</span></span> stomach-flu (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.1</span></span>)) (<span class="hljs-name"><span class="hljs-name">define</span></span> other (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.1</span></span>)) (<span class="hljs-name"><span class="hljs-name">define</span></span> cough (<span class="hljs-name"><span class="hljs-name">or</span></span> (<span class="hljs-name"><span class="hljs-name">and</span></span> cold (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.5</span></span>)) (<span class="hljs-name"><span class="hljs-name">and</span></span> lung-cancer (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.3</span></span>)) (<span class="hljs-name"><span class="hljs-name">and</span></span> TB (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.7</span></span>)) (<span class="hljs-name"><span class="hljs-name">and</span></span> other (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.01</span></span>)))) (<span class="hljs-name"><span class="hljs-name">define</span></span> fever (<span class="hljs-name"><span class="hljs-name">or</span></span> (<span class="hljs-name"><span class="hljs-name">and</span></span> cold (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.3</span></span>)) (<span class="hljs-name"><span class="hljs-name">and</span></span> stomach-flu (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.5</span></span>)) (<span class="hljs-name"><span class="hljs-name">and</span></span> TB (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.2</span></span>)) (<span class="hljs-name"><span class="hljs-name">and</span></span> other (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.01</span></span>)))) (<span class="hljs-name"><span class="hljs-name">define</span></span> chest-pain (<span class="hljs-name"><span class="hljs-name">or</span></span> (<span class="hljs-name"><span class="hljs-name">and</span></span> lung-cancer (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.4</span></span>)) (<span class="hljs-name"><span class="hljs-name">and</span></span> TB (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.5</span></span>)) (<span class="hljs-name"><span class="hljs-name">and</span></span> other( <span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.01</span></span>)))) (<span class="hljs-name"><span class="hljs-name">define</span></span> shortness-of-breath (<span class="hljs-name"><span class="hljs-name">or</span></span> (<span class="hljs-name"><span class="hljs-name">and</span></span> lung-cancer (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.4</span></span>)) (<span class="hljs-name"><span class="hljs-name">and</span></span> TB (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.5</span></span>)) (<span class="hljs-name"><span class="hljs-name">and</span></span> other (<span class="hljs-name"><span class="hljs-name">flip</span></span> <span class="hljs-number"><span class="hljs-number">0.01</span></span>)))) (<span class="hljs-name"><span class="hljs-name">list</span></span> lung-cancer TB) (<span class="hljs-name"><span class="hljs-name">and</span></span> cough fever chest-pain shortness-of-breath))) (<span class="hljs-name"><span class="hljs-name">hist</span></span> samples <span class="hljs-string"><span class="hljs-string">"Joint inferences for lung cancer and TB"</span></span>)</code> </pre> <br><br>  In this program, a priori probabilities of the occurrence of lung cancer, tuberculosis, colds, etc. in the patient are determined.  Further, the probabilities of observing coughing, fever, chest pain, and shortness of breath for certain diseases are determined.  The return value is a pair of boolean values, whether the patient has cancer and / or tuberculosis.  And finally, the condition is a combination of the observed symptoms (that is, sampling is performed under the condition that the value of all variables is cough fever chest-pain shortness-of-breath - #t). <br><br>  The result of the program will be as follows: (#f #f) - 4%, (#f #t) - 58%, (#t #f) - 37%, (#t #t) - 1%. <br>  It is easy to make samples to be a function in which a list of symptoms is submitted, which later on in mh-query is used for sampling, which will make it possible to make diagnoses for different patients.  Of course, this example is greatly simplified, but it can be seen that in the style of probabilistic programming, it is possible to represent knowledge and draw a conclusion over them. <br><br>  Naturally, you can solve the problem of machine learning.  Their difference will be only in that the unknown parameters will be the parameters of the model itself, and the condition for sampling will be the generation of the training sample by this model.  For example, in the program presented above, we would replace the numbers in the strings of the form (define lung-cancer (flip 0.01)) with variables that would be given as random, for example (define p-lung-cancer (uniform 0 1)) , and then for each patient from the training set, the value of lung-cancer would already be determined with the probability of p-lung-cancer. <br><br>  Let us consider this possibility using a simple example of estimating the parameters of a polynomial by a set of points.  In the next program, the calc-poly function calculates the value of a polynomial with parameters ws at the point x.  The generate function applies calc-poly to each value from the given xs list and returns a list of the corresponding ordinates.  Noisy-equals procedure?  ‚ÄúApproximately‚Äù compares two given values ‚Äã‚Äã(if these values ‚Äã‚Äãare equal, then the function returns #t with a probability of 1; if they are not equal, then the more they differ, the less likely it is to return #t). <br><br><pre> <code class="lisp hljs">(<span class="hljs-name"><span class="hljs-name">define</span></span> (<span class="hljs-name"><span class="hljs-name">calc-poly</span></span> x ws) (<span class="hljs-name"><span class="hljs-name">if</span></span> (<span class="hljs-name"><span class="hljs-name">null</span></span>? ws) <span class="hljs-number"><span class="hljs-number">0</span></span> (<span class="hljs-name"><span class="hljs-name">+</span></span> (<span class="hljs-name"><span class="hljs-name">car</span></span> ws) (<span class="hljs-name"><span class="hljs-name">*</span></span> x (<span class="hljs-name"><span class="hljs-name">calc-poly</span></span> x (<span class="hljs-name"><span class="hljs-name">cdr</span></span> ws)))))) (<span class="hljs-name"><span class="hljs-name">define</span></span> (<span class="hljs-name"><span class="hljs-name">generate</span></span> xs ws) (<span class="hljs-name"><span class="hljs-name">map</span></span> (<span class="hljs-name"><span class="hljs-name">lambda</span></span> (<span class="hljs-name"><span class="hljs-name">x</span></span>) (<span class="hljs-name"><span class="hljs-name">calc-poly</span></span> x ws)) xs)) (<span class="hljs-name"><span class="hljs-name">define</span></span> (<span class="hljs-name"><span class="hljs-name">noisy-equals</span></span>? xy) (<span class="hljs-name"><span class="hljs-name">flip</span></span> (<span class="hljs-name"><span class="hljs-name">exp</span></span> (<span class="hljs-name"><span class="hljs-name">*</span></span> <span class="hljs-number"><span class="hljs-number">-3</span></span> (<span class="hljs-name"><span class="hljs-name">expt</span></span> (<span class="hljs-name"><span class="hljs-name">-</span></span> xy) <span class="hljs-number"><span class="hljs-number">2</span></span>))))) (<span class="hljs-name"><span class="hljs-name">define</span></span> (<span class="hljs-name"><span class="hljs-name">samples</span></span> xs ys) (<span class="hljs-name"><span class="hljs-name">mh-query</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">100</span></span> (<span class="hljs-name"><span class="hljs-name">define</span></span> n-coef <span class="hljs-number"><span class="hljs-number">4</span></span>) (<span class="hljs-name"><span class="hljs-name">define</span></span> ws (<span class="hljs-name"><span class="hljs-name">repeat</span></span> n-coef (<span class="hljs-name"><span class="hljs-name">lambda</span></span> () (<span class="hljs-name"><span class="hljs-name">gaussian</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">3</span></span>)))) ws (<span class="hljs-name"><span class="hljs-name">all</span></span> (<span class="hljs-name"><span class="hljs-name">map</span></span> noisy-equals? (<span class="hljs-name"><span class="hljs-name">generate</span></span> xs ws) ys)))) (<span class="hljs-name"><span class="hljs-name">samples</span></span> '(<span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span>) '(<span class="hljs-number"><span class="hljs-number">0.01</span></span> <span class="hljs-number"><span class="hljs-number">1.95</span></span> <span class="hljs-number"><span class="hljs-number">6.03</span></span> <span class="hljs-number"><span class="hljs-number">12.01</span></span> <span class="hljs-number"><span class="hljs-number">20.00</span></span>))</code> </pre> <br><br>  Inside the mh-query call, the n-coef parameter determines the number of coefficients in the polynomial (that is, its degree plus one);  ws is a list of random variables generated in accordance with the normal distribution.  The return value is a list of parameters of the polynomial.  The condition for sampling is the ‚Äúapproximate‚Äù equality of all given values ‚Äã‚Äãof ys to all ordinates generated by a polynomial given ws.  Here we request only one implementation that passes by the condition (since it is not very convenient to build a histogram for the parameter vector).  The result of this query may be, for example, a list (2.69 1.36 0.53-00), which sets the polynomial 2.69 + 1.36x + 0.53x ^ 2‚Äì0.10x ^ 3. <br><br>  In general, the conclusion on models with real parameters is not the strongest side of the Church language (but this should not be considered a global disadvantage of probabilistic programming in general).  However, in this example, mh-query works somehow.  To verify this, instead of defining the values ‚Äã‚Äãof the parameters in the query, you can ask to return the prediction at some point.  Rewrite the last code snippet like this: <br><br><pre> <code class="lisp hljs">(<span class="hljs-name"><span class="hljs-name">define</span></span> (<span class="hljs-name"><span class="hljs-name">samples</span></span> xs ys) (<span class="hljs-name"><span class="hljs-name">mh-query</span></span> <span class="hljs-number"><span class="hljs-number">100</span></span> <span class="hljs-number"><span class="hljs-number">100</span></span> (<span class="hljs-name"><span class="hljs-name">define</span></span> n-coef <span class="hljs-number"><span class="hljs-number">4</span></span>) (<span class="hljs-name"><span class="hljs-name">define</span></span> ws (<span class="hljs-name"><span class="hljs-name">repeat</span></span> n-coef (<span class="hljs-name"><span class="hljs-name">lambda</span></span> () (<span class="hljs-name"><span class="hljs-name">gaussian</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">3</span></span>)))) (<span class="hljs-name"><span class="hljs-name">calc-poly</span></span> <span class="hljs-number"><span class="hljs-number">5</span></span> ws) (<span class="hljs-name"><span class="hljs-name">all</span></span> (<span class="hljs-name"><span class="hljs-name">map</span></span> noisy-equals? (<span class="hljs-name"><span class="hljs-name">generate</span></span> xs ws) ys)))) (<span class="hljs-name"><span class="hljs-name">hist</span></span> (<span class="hljs-name"><span class="hljs-name">samples</span></span> '(<span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span>) '(<span class="hljs-number"><span class="hljs-number">0.01</span></span> <span class="hljs-number"><span class="hljs-number">1.95</span></span> <span class="hljs-number"><span class="hljs-number">6.03</span></span> <span class="hljs-number"><span class="hljs-number">12.01</span></span> <span class="hljs-number"><span class="hljs-number">20.00</span></span>)))</code> </pre> <br><br>  That is, we request the most probable (given the available data) value at the point x = 5.  For different launches, the maximum of the histogram, unfortunately, will fall on slightly different values ‚Äã‚Äã(the MCMC method, theoretically, guarantees a convergence to the true distribution, but only in the limit), but usually these values ‚Äã‚Äãwill be quite intelligible.  It is worth noting that here we were ‚Äúfree‚Äù (replacing one line) with a complete Bayesian prediction: instead of choosing the best model and prediction just by it, we got the a posteriori distribution of values ‚Äã‚Äãat x = 5, averaged over many models taking into account their own probabilities . <br>  But that's not all.  Again, replacing one line - (define n-coef 4) -&gt; (define n-coef (random-integer 5)) we can make an automatic choice between models with a different number of parameters.  Moreover, the sampling of the n-coef value shows (although not very stable) that the most likely value is n-coef = 3 (that is, a parabola, which is embedded in a given set of points).  With this modification, the prediction becomes more stable.  In other words, there is no retraining effect!  Why aren't polynomials of a higher degree chosen, because they can more precisely pass to given points?  The fact is that when sampling, ‚Äúguessing‚Äù suitable values ‚Äã‚Äãof parameters of a polynomial of a smaller degree is simpler than a polynomial of a higher degree; therefore, the probability to generate such parameters that pass the test is higher for a polynomial of the second degree than for a third.  At the same time, a polynomial of the first degree will give large deviations, for which the probability of triggering noisy-equals?  will be greatly reduced. <br><br>  Let's look at another application, which in the framework of probabilistic programming may seem unexpected.  This is a solution to "deductive" tasks.  Let's take the factorial calculation function given at the beginning, but instead of calling it with a fixed value, we will assume that the argument is a random variable, but the factorial value itself is subject to a restriction: <br><br><pre> <code class="lisp hljs">(<span class="hljs-name"><span class="hljs-name">define</span></span> (<span class="hljs-name"><span class="hljs-name">fn</span></span>) (<span class="hljs-name"><span class="hljs-name">if</span></span> (<span class="hljs-name"><span class="hljs-name">=</span></span> n <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-number"><span class="hljs-number">1</span></span> (<span class="hljs-name"><span class="hljs-name">*</span></span> n (<span class="hljs-name"><span class="hljs-name">f</span></span> (<span class="hljs-name"><span class="hljs-name">-</span></span> n <span class="hljs-number"><span class="hljs-number">1</span></span>))))) (<span class="hljs-name"><span class="hljs-name">enumeration-query</span></span> (<span class="hljs-name"><span class="hljs-name">define</span></span> n (<span class="hljs-name"><span class="hljs-name">random-integer</span></span> <span class="hljs-number"><span class="hljs-number">20</span></span>)) n (<span class="hljs-name"><span class="hljs-name">equal</span></span>? (<span class="hljs-name"><span class="hljs-name">fn</span></span>) <span class="hljs-number"><span class="hljs-number">120</span></span>))</code> </pre> <br><br>  As a response, we will see n = 5 with a probability of 1. If instead of 120 we specify 100, the program will not loop (unlike the rejection-query or mh-query case, which can be considered a disadvantage), but simply return the empty set .  You can put as a condition and not a strict equality, but some other restriction. <br><br>  In the same way, you can solve more complex problems.  Suppose we want to solve the problem of the sum of subsets: it is necessary to find such a subset from a given set of numbers, the sum of which is equal to a given number (usually 0 is taken as this number and it is required that the subset is not empty; but to get rid of checking for non-trivial solution, we take a nonzero sum).  It would seem, where does probabilistic programming?  But random variables are simply unknown quantities (for which a priori probabilities are given).  In any problems we need to find something unknown, including in the problem of the sum of subsets.  Let's look at the following elementary program (it could even be simplified by writing summ through fold). <br><br><pre> <code class="lisp hljs">(<span class="hljs-name"><span class="hljs-name">define</span></span> (<span class="hljs-name"><span class="hljs-name">solution</span></span> xs v) (<span class="hljs-name"><span class="hljs-name">rejection-query</span></span> (<span class="hljs-name"><span class="hljs-name">define</span></span> ws (<span class="hljs-name"><span class="hljs-name">repeat</span></span> (<span class="hljs-name"><span class="hljs-name">length</span></span> xs) flip)) (<span class="hljs-name"><span class="hljs-name">define</span></span> (<span class="hljs-name"><span class="hljs-name">summ</span></span> xs ws) (<span class="hljs-name"><span class="hljs-name">if</span></span> (<span class="hljs-name"><span class="hljs-name">null</span></span>? xs) <span class="hljs-number"><span class="hljs-number">0</span></span> (<span class="hljs-name"><span class="hljs-name">+</span></span> (<span class="hljs-name"><span class="hljs-name">if</span></span> (<span class="hljs-name"><span class="hljs-name">car</span></span> ws) (<span class="hljs-name"><span class="hljs-name">car</span></span> xs) <span class="hljs-number"><span class="hljs-number">0</span></span>) (<span class="hljs-name"><span class="hljs-name">summ</span></span> (<span class="hljs-name"><span class="hljs-name">cdr</span></span> xs) (<span class="hljs-name"><span class="hljs-name">cdr</span></span> ws))))) ws (<span class="hljs-name"><span class="hljs-name">equal</span></span>? (<span class="hljs-name"><span class="hljs-name">summ</span></span> xs ws) v))) (<span class="hljs-name"><span class="hljs-name">solution</span></span> '(<span class="hljs-number"><span class="hljs-number">-1</span></span> <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">7</span></span> <span class="hljs-number"><span class="hljs-number">5</span></span> <span class="hljs-number"><span class="hljs-number">-9</span></span> <span class="hljs-number"><span class="hljs-number">-1</span></span>) <span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><br>  Here ws is a list of random boolean values.  The summ procedure calculates the sum of the elements of the xs list for which the corresponding elements of the ws list are true.  Further, the values ‚Äã‚Äãof ws are requested, for which the condition of equality of the obtained sum to the given number v is satisfied.  By running this program, you can get this result: (#f #t #t #f #t #f), which, of course, is correct (since 3 + 7-9 = 1). <br><br>  Naturally, Church does not make a miracle and he will not cope with it if the dimension of this task is increased.  However, it cannot but surprise that such different AI tasks can be at least posed (and partially solved) using the same language.  Well, the problem of effective withdrawal as it was, and remains.  In probabilistic languages, it at least stands out in its purest form. </div><p>Source: <a href="https://habr.com/ru/post/242993/">https://habr.com/ru/post/242993/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../242979/index.html">How to make profitable design development sites (part 4)</a></li>
<li><a href="../242983/index.html">Advantages of a new backup method for virtual machines over classic schemes</a></li>
<li><a href="../242987/index.html">Wikipedia. Forbidden to donate from Russia?</a></li>
<li><a href="../242989/index.html">Run GWT Super Dev Mode for remote server</a></li>
<li><a href="../242991/index.html">High Frequency Trading Neighborhood - Part I</a></li>
<li><a href="../242995/index.html">Efficient Landing Page design or how not to remove ‚ÄúBad Cinema‚Äù</a></li>
<li><a href="../242997/index.html">Critical vulnerability in Microsoft SChannel</a></li>
<li><a href="../242999/index.html">Postgres. Sample N random entries</a></li>
<li><a href="../243001/index.html">Memo: How startups provide data protection in the cloud</a></li>
<li><a href="../243005/index.html">Monospaced fonts with programmer ligatures</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>