<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The history of the struggle for IOPS in the self-assembly SAN</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! 

 One of my projects uses something a bit like a private cloud. These are several servers for storing data and several diskless ones responsib...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The history of the struggle for IOPS in the self-assembly SAN</h1><div class="post__text post__text-html js-mediator-article">  Hello! <br><br>  One of my projects uses something a bit like a private cloud.  These are several servers for storing data and several diskless ones responsible for virtualization.  The other day I seem to have finally put an end to the question of squeezing the maximum performance of the disk subsystem of this solution.  It was quite interesting, and even at some moments, quite unexpectedly.  Therefore, I want to share my story with the habrasoobshchestvu, which began in the distant 2008th year, even before the appearance of the ‚ÄúFirst Cloud Provider of Russia‚Äù and the action on sending free water meters. <br><br><h5>  Architecture </h5><br>  Virtual hard disks are exported via a separate gigabit network using <a href="http://en.wikipedia.org/wiki/ATA_over_Ethernet">AoE protocol</a> .  In short, this is the brainchild of Coraid, which offered to transfer ATA commands over the network directly.  The protocol specification takes only ten pages!  The main feature is the absence of TCP / IP.  When transferring data, a minimal overhead projector is obtained, but as a payment for simplicity, it is impossible to route. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Why such a choice?  If you omit the reprint of official sources - including  and banal lowcost. <br><br>  Accordingly, in the storages we used ordinary SATA drives with 7200 rpm.  Their flaw is known to everyone - low IOPS. <br><br><a name="habracut"></a><br><br><h5>  RAID10 </h5><br>  The very first, popular and obvious way to solve the problem of random access speed.  They took mdadm into their hands, drove a couple of corresponding commands into the console, raised LVM on top (we are going to distribute block devices for virtual machines as a result) and launched several naive tests. <br><br><pre><code class="bash hljs">root@storage:~<span class="hljs-comment"><span class="hljs-comment"># hdparm -tT /dev/md127 /dev/md127: Timing cached reads: 9636 MB in 2.00 seconds = 4820.51 MB/sec Timing buffered disk reads: 1544 MB in 3.03 seconds = 509.52 MB/sec</span></span></code> </pre> <br><br>  To be honest, it was scary to check IOPS, there were no solutions to the problem other than switching to SCSI or writing your own crutches. <br><br><h5>  Network and MTU </h5><br>  Although the network was also gigabit, from diskless servers, the read speed did not reach the expected ~ 100MiB / sec.  Naturally, the drivers for the network cards were to blame (hello, Debian).  Using fresh drivers from the manufacturer‚Äôs site seems to have partially eliminated the problem ... <br><br>  In all AoE speed optimization manuals, the first item indicates the maximum MTU setting.  At that moment it was 4200. Now it seems ridiculous, but compared to the standard 1500, the linear read speed really reached ~ 120MiB / sec, cool!  And even with a slight load on the disk subsystem by all virtual servers, local caches rectified the situation and within each virtual machine the linear read speed was kept at least 50MiB / sec.  In fact, pretty good!  Over time, we changed the network cards, the switch - and raised the MTU to the maximum 9K. <br><br><h5>  MySQL has not come yet </h5><br>  Yes, one of the 24/7 projects was jerking MySQL, both for writing and reading.  It looked something like this: <br><pre> <code class="bash hljs">Total DISK READ: 506.61 K/s | Total DISK WRITE: 0.00 B/s TID PRIO USER DISK READ DISK WRITE SWAPIN IO&gt; COMMAND 30312 be/4 mysql 247.41 K/s 11.78 K/s 0.00 % 11.10 % mysqld 30308 be/4 mysql 113.89 K/s 19.64 K/s 0.00 % 7.30 % mysqld 30306 be/4 mysql 23.56 K/s 23.56 K/s 0.00 % 5.36 % mysqld 30420 be/4 mysql 62.83 K/s 11.78 K/s 0.00 % 5.03 % mysqld 30322 be/4 mysql 23.56 K/s 23.56 K/s 0.00 % 2.58 % mysqld 30445 be/4 mysql 19.64 K/s 19.64 K/s 0.00 % 1.75 % mysqld 30183 be/4 mysql 7.85 K/s 7.85 K/s 0.00 % 1.15 % mysqld 30417 be/4 mysql 7.85 K/s 3.93 K/s 0.00 % 0.36 % mysqld</code> </pre><br>  Harmless?  As if not so.  A huge stream of small requests, 70% io wait on a virtual server, 20% load on each of the hard drives (if you believe atop) on the storage, and such a dull picture on the other virtual machines: <br><pre> <code class="bash hljs">root@mail:~<span class="hljs-comment"><span class="hljs-comment"># hdparm -tT /dev/xvda /dev/xvda: Timing cached reads: 10436 MB in 1.99 seconds = 5239.07 MB/sec Timing buffered disk reads: 46 MB in 3.07 seconds = 14.99 MB/sec</span></span></code> </pre><br>  And it is still fast!  Often, the linear reading speed was no more than 1-2 MiB / sec. <br><br>  I think everyone already guessed what we ran into.  Low IOPS SATA drives, even though RAID10. <br><br><h5>  Flashcache </h5><br>  How did these guys appear on time!  This salvation, this is the same!  Life is getting better, we will be saved! <br>  Urgent purchase of Intel's SSD, including the module and flashcache utilities in the live image of the storage servers, setting the write-back cache and fire in the eyes.  Yeah, all the zeros counters.  Well, the LVM + Flashcache features are easy to googling, the problem is quickly resolved. <br><br>  On a virtual server with MySQL, loadavg dropped from 20 to 10. Linear reading on the rest of the virtual servers increased to stable 15-20 MiB / sec.  Do not be fooled! <br><br>  After some time, I collected the following statistics: <br><pre> <code class="bash hljs">root@storage:~<span class="hljs-comment"><span class="hljs-comment"># dmsetup status cachedev 0 2930294784 flashcache stats: reads(85485411), writes(379006540) read hits(12699803), read hit percent(14) write hits(11805678) write hit percent(3) dirty write hits(4984319) dirty write hit percent(1) replacement(144261), write replacement(111410) write invalidates(2928039), read invalidates(8099007) pending enqueues(2688311), pending inval(1374832) metadata dirties(11227058), metadata cleans(11238715) metadata batch(3317915) metadata ssd writes(19147858) cleanings(11238715) fallow cleanings(6258765) no room(27) front merge(1919923) back merge(1058070) disk reads(72786438), disk writes(374046436) ssd reads(23938518) ssd writes(42752696) uncached reads(65392976), uncached writes(362807723), uncached IO requeue(13388) uncached sequential reads(0), uncached sequential writes(0) pid_adds(0), pid_dels(0), pid_drops(0) pid_expiry(0)</span></span></code> </pre><br>  read hit percent: 13, write hit percent: 3. A huge amount of uncached reads / writes.  It turns out that flashcache worked, but not at all.  There were a couple of dozen virtual machines, the total amount of virtual disks did not exceed a terabyte, disk activity was small.  Those.  such a low percentage of hitting the cache is not due to the activity of neighbors. <br><br><h5>  Insight! </h5><br>  Looking at this for the hundredth time: <br><pre> <code class="bash hljs">root@storage:~<span class="hljs-comment"><span class="hljs-comment"># dmsetup table cachedev 0 2930294784 flashcache conf: ssd dev (/dev/sda), disk dev (/dev/md2) cache mode(WRITE_BACK) capacity(57018M), associativity(512), data block size(4K) metadata block size(4096b) skip sequential thresh(0K) total blocks(14596608), cached blocks(3642185), cache percent(24) dirty blocks(36601), dirty percent(0) nr_queued(0) Size Hist: 512:117531108 1024:61124866 1536:83563623 2048:89738119 2560:43968876 3072:51713913 3584:83726471 4096:41667452</span></span></code> </pre><br>  I decided to open my favorite <s>Excel</s> LibreOffice Calc: <br><img src="https://habrastorage.org/storage2/c12/c61/0e4/c12c610e4da5a443d6fd814aff1c71f5.png"><br>  The diagram is plotted according to the last line, the histogram of the distribution of requests by block sizes. <br>  We all know that <br>  Hard drives usually operate in blocks of 512 bytes.  Exactly like AoE.  The Linux kernel is 4096 bytes each.  Data block size in flascache is also 4096. <br><br>  Summing up the number of requests with block sizes different from 4096, then it can be seen that the received number suspiciously coincides with the number uncached reads + uncached writes from the flashcache statistics.  Only 4K blocks are cached!  Remember that MTU was originally 4200?  If we subtract the size of the AoE packet header from this, we get the size of the data block at 3584. This means that any request to the disk subsystem will be split into at least 2 AoE packets: 3584 bytes and 512 bytes.  What exactly was clearly visible on the original diagram, which I saw.  Even on the diagram from the article, the predominance of 512 byte packets is noticeable.  And the MTU in 9K recommended at every corner also has a similar problem: the size of the data block is 8704 bytes, these are 2 blocks of 4K and one of 512 bytes (which is exactly what is observed in the diagram from the article).  Opanki!  The decision, I think, is obvious to everyone. <br><br><h5>  MTU 8700 </h5><br><img src="https://habrastorage.org/storage2/9b4/663/1e6/9b46631e69c2bebd85ee1151ea61c14a.png"><br><br>  The diagram was made several days after updating the configuration on one of the diskless nodes.  After updating the MTU on the rest - the situation will become even better.  And loadavg on a virtual server with MySQL dropped to 3! <br><br><h5>  Conclusion </h5><br>  Not being system administrators with 20 years of experience, we solved problems using the ‚Äústandard‚Äù and most popular approaches known to the community at the appropriate time.  But in the real world there is always a place for imperfections, crutches and assumptions.  To which we, in fact, ran into. <br>  Here is such a story. </div><p>Source: <a href="https://habr.com/ru/post/158159/">https://habr.com/ru/post/158159/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../158143/index.html">Evaluation of information security in the activities of organizations</a></li>
<li><a href="../158145/index.html">Google launched a strange site Niantic Project</a></li>
<li><a href="../158151/index.html">We collect ProGit documentation, under Windows</a></li>
<li><a href="../158155/index.html">Inline editing in Django</a></li>
<li><a href="../158157/index.html">Mobile client server development</a></li>
<li><a href="../158161/index.html">Theory of radio waves: educational program</a></li>
<li><a href="../158165/index.html">Text Mining Framework (Java)</a></li>
<li><a href="../158167/index.html">jQuery File Upload. Upload and add images to the database</a></li>
<li><a href="../158169/index.html">Running Gadgets: From Simple to Very Simple</a></li>
<li><a href="../158171/index.html">We make from the old ‚Äústupid‚Äù TV - new and ‚Äúsmart‚Äù and that ‚Äúbeautiful‚Äù was</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>