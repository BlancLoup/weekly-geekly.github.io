<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>T2F: project to convert text to face drawing with in-depth training</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Project code is available in the repository. 

 Introduction 
 When I read descriptions of the appearance of the characters in the books, I always won...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>T2F: project to convert text to face drawing with in-depth training</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/5ae/703/0df/5ae7030df8270466b01b81aad0ace49f.jpg"><br><br>  <i>Project code is available in the <a href="https://github.com/akanimax/T2F">repository.</a></i> <br><br><h2>  Introduction </h2><br>  When I read descriptions of the appearance of the characters in the books, I always wondered what they would look like in life.  It is quite possible to imagine a person as a whole, but the description of the most conspicuous details is a difficult task, and the results vary from person to person.  Many times I couldn‚Äôt imagine anything but a very blurred face for the character until the very end of the piece.  Only when a book is turned into a film does the blurry face fill with details.  For example, I could never imagine how exactly Rachel‚Äôs face in the book ‚ÄúThe <a href="https://www.youtube.com/watch%3Fv%3Dy5yk-HGqKmM">Girl on the Train</a> ‚Äù looks like.  But when the movie came out, I was able to match Emily Blunt‚Äôs face with Rachel‚Äôs character.  Surely, the people involved in the selection of actors, takes a long time to correctly portray the characters in the script. <br><a name="habracut"></a><br>  This problem inspired and motivated me to find a solution.  After that, I began to study the literature on depth learning in search of something similar.  Fortunately, there were quite a few studies on the synthesis of images from text.  Here are some of those that I based on: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  <a href="https://arxiv.org/abs/1605.05396">arxiv.org/abs/1605.05396</a> ‚ÄúGenerative Adversarial Text to Image Synthesis‚Äù </li><li>  <a href="https://arxiv.org/abs/1612.03242">arxiv.org/abs/1612.03242</a> ‚ÄúStackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks‚Äù </li><li>  <a href="https://arxiv.org/abs/1710.10916">arxiv.org/abs/1710.10916</a> ‚ÄúStackGAN ++: Realistic Image Synthesis with Stacked Generative Adversarial Networks‚Äù </li></ul><br>  [ <i>projects use generative and adversary networks, GSS (Generative adversarial network, GAN) / approx.</i>  <i>trans.</i>  ] <br><br>  After reviewing the literature, I chose an architecture that was simplified compared to StackGAN ++, and is doing quite well with my problem.  In the following sections, I will explain how I solved this problem, and I will share preliminary results.  I will also describe some of the details of programming and training, for which I spent a lot of time. <br><br><h2>  Data analysis </h2><br>  Undoubtedly, the most important aspect of the work is the data used to train the model.  As Professor Andrew Un spoke in his deeplearning.ai courses: ‚ÄúIn the case of machine learning, it is not the one who has the best algorithm that achieves success, but the one who has the best data‚Äù.  So began my search for a dataset for people with good, rich, and varied text descriptions.  I ran across different data sets - either they were just faces, or faces with names, or faces with descriptions of eye color and face shape.  But there were none that I needed.  My last option was to use <a href="https://github.com/akanimax/natural-language-summary-generation-from-structured-data">an early project</a> - the generation of a description of structural data in natural language.  But such an option would add extra noise to an already fairly noisy data set. <br><br>  Time passed, and at some point a new project <a href="https://arxiv.org/pdf/1803.03827.pdf">Face2Text appeared</a> .  It was a collection of a database of detailed text descriptions of persons.  Thanks to the authors of the project for the provided data set. <br><br>  The data set contained textual descriptions of 400 randomly selected images from the LFW database (marked-up faces).  Descriptions have been cleared to eliminate ambiguous and minor characteristics.  Some descriptions contained not only information about individuals, but also some conclusions drawn from images - for example, ‚Äúthe person in the photo is probably a criminal‚Äù.  All these factors, as well as the small size of the data set, led to the fact that my project so far only demonstrates the proof of the performance of the architecture.  Subsequently, this model can be scaled to a larger and more diverse data set. <br><br><h2>  Architecture </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/c70/ea1/6de/c70ea16de2bbfb674e618fa556cfc9ff.jpg"><br><br>  The T2F project architecture combines two stackGAN architectures for encoding text with a conditional increment, and ProGAN ( <a href="https://arxiv.org/pdf/1710.10196.pdf">progressive GSS growth</a> ) for synthesizing face images.  The original stackgan ++ architecture used several GSS with different spatial resolutions, and I decided that this was too serious an approach for any match distribution task.  But ProGAN uses only one GSS, which is progressively trained on more and more detailed resolutions.  I decided to combine these two approaches. <br><br>  Explanation of the data flow through is: text descriptions are encoded into the final vector using LSTM (Embedding) (psy_t) network embedding (see diagram).  Then the embedding is transmitted through the conditional addition block (Conditioning Augmentation) (one linear layer) to get the text part of the eigenvector (using the VAE repair technique) for the GSS as an input.  The second part of the eigenvector is random Gaussian noise.  The resulting eigenvector is fed to the GSS generator, and the embedding is fed to the last layer of the discriminator for conditional distribution of compliance.  The training of GSS processes proceeds in the same way as in the article on ProGAN - in layers, with an increase in spatial resolution.  A new layer is introduced using the fade-in technique to avoid the destruction of previous learning outcomes. <br><br><h2>  Implementation and other details </h2><br>  The application was written in python using the PyTorch framework.  I used to work with tensorflow and keras packages, but now I wanted to try PyTorch.  I liked using the python debugger for working with the Network architecture - all thanks to the early execution strategy.  In tensorflow recently also included the eager execution mode.  However, I do not want to judge which framework is better, I just want to emphasize that the code for this project was written using PyTorch. <br><br>  Quite a few parts of the project seem reusable to me, especially ProGAN.  Therefore, I wrote separate code for them as an <a href="https://github.com/akanimax/pro_gan_pytorch">extension to</a> the PyTorch module, and it can be used on other data sets.  It is only necessary to specify the depth and size of the GSS features.  GSS can be trained progressively for any data set. <br><br><h2>  Workout details </h2><br>  I have trained quite a few versions of the network using different hyperparameters.  Work details are as follows: <br><br><ol><li>  The discriminator does not have batch-norm or layer-norm operations, so the loss of a WGAN-GP can grow explosively.  I used a drift penalty with a lambda of 0.001. </li><li>  To control one's own diversity obtained from the coded text, it is necessary to use the Kullback ‚Äì Leibler distance in the losses of the Generator. </li><li>  To make the resulting images better match the incoming textual distribution, it is better to use the WGAN variant of the corresponding (Matching-Aware) discriminator. </li><li>  The fade-in time for the upper level must exceed the fade-in time for the lower ones.  I used 85% as a fade-in value when training. </li><li>  I found that higher resolution examples (32 x 32 and 64 x 64) produce more background noise than lower resolution examples.  I think this is due to lack of data. </li><li>  During a progressive workout, it is better to spend more time on smaller resolutions, and to reduce the time you work with larger resolutions. </li></ol><br>  The video shows the Timelapse Generator.  The video is collected from images with different spatial resolution, obtained during the GSS training session. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/NO_l87rPDb8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h2>  Conclusion </h2><br>  According to preliminary results, it is possible to judge that the T2F project is efficient, and has interesting applications.  Suppose it can be used to compile identikits.  Or for cases when it is necessary to spur the imagination.  I will continue to work on scaling this project on data sets such as Flicker8K, Coco captions, and so on. <br><br>  Progressive GSS growth is a phenomenal technology for faster and more stable GSS training.  It can be combined with various modern technologies mentioned in other articles.  GSS can be used in different areas of MO. </div><p>Source: <a href="https://habr.com/ru/post/420709/">https://habr.com/ru/post/420709/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../420699/index.html">In Egypt, imposed fines for visiting blocked sites</a></li>
<li><a href="../420701/index.html">VSBI invites you to a game design lecture night on August 29</a></li>
<li><a href="../420703/index.html">Summary of the book "Negotiations without defeat. Harvard Method</a></li>
<li><a href="../420705/index.html">8 deep ideas from the book ‚ÄúThe Mentor Tribe‚Äù by Tim Ferris</a></li>
<li><a href="../420707/index.html">JITX startup uses AI to automate the development of complex printed circuit boards</a></li>
<li><a href="../420711/index.html">Moscow Data Science Major: announcement and registration</a></li>
<li><a href="../420713/index.html">How Chuck Hull Invented 3D Printing</a></li>
<li><a href="../420715/index.html">Hard truth about the burden of learning</a></li>
<li><a href="../420725/index.html">As I taught AI to play Tetris for NES. Part 1: game code analysis</a></li>
<li><a href="../420729/index.html">Open webinar "Naive Bayes Classifier"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>