<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Correction of typos, side view</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="We will talk about the use of fashionable ‚ÄúWord embedding‚Äù not quite as intended - namely, to correct typos (strictly speaking, mistakes, too, but we ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Correction of typos, side view</h1><div class="post__text post__text-html js-mediator-article">  We will talk about the use of fashionable ‚ÄúWord embedding‚Äù not quite as intended - namely, to correct typos (strictly speaking, mistakes, too, but we assume that people are literate and sealed).  On Habr√© was a fairly close <a href="https://habr.com/post/249215/">article</a> , but there will be a little about something else. <br><br><img src="https://habrastorage.org/webt/fa/ae/0l/faae0ld7woqjj3pzipxowkjk5bo.jpeg"><br>  <em>Visualization of the Word2Vec model obtained by the student.</em>  <em>She studied at the "Lord of the Rings."</em>  <em>Obviously something in the black dialect.</em> <br><a name="habracut"></a><br><h2>  Formulation of the problem </h2><br>  <b>Given</b> : Dictionary (many words). <br>  <b>Required</b> : For an input word with a typo (which may not be in the dictionary), find the nearest words from the dictionary based on a predefined metric.  These words are found and are options for correcting the input word. <br><br><h2>  Theoretical educational program </h2><br>  For our task, we need Word Embedding, or a vector representation of words (the Russian-speaking community still has doubts about the translation of the term), and again there is a wonderful <a href="https://habr.com/company/ods/blog/329410/">article</a> in Habr√©, which tells about it well.  In order not to repeat, but also not to chase the reader on the links - a brief excursion into the topic. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <strong>Word embedding</strong> is a vector representation of words, that is, one word is associated with a vector of fixed dimension, for example, for the conditional word ‚Äúhome‚Äù - <code>[0.1, 0.3, ..., 0.7]</code> .  Important note: usually the dimension of the vector is much smaller than the dimension of the dictionary, otherwise it degenerates into <a href="https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/">one-hot encoding</a> . <br><br>  The number of vector elements depends on many conditions: the chosen method, the requirements of the problem, the weather in Krasnoyarsk and much more.  In current <abbr title="State of the Art">SotA</abbr> implementations, this is 100-500 values ‚Äã‚Äã(usually 300). <br><br>  Vectors are obtained in the process of learning, for this, some text or texts are fed to the algorithm (from works of art to the wikipedia dump), and a vector is calculated for each word iteratively. <br><br>  There are various methods for obtaining these vectors, the most popular now are <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Word2Vec</a> , <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a> , <a href="https://arxiv.org/abs/1506.02761">WordRank</a> and <a href="https://fasttext.cc/">FastText</a> . <br><br>  In short, the idea is based on: words whose contexts are similar - most likely have similar meanings.  This implies how the typos are corrected in the first <a href="https://habr.com/post/249215/">example</a> given.  That is, we have two sentences: ‚Äúto find tours with preferences‚Äù and ‚Äúto find tours with preferences‚Äù, the contexts are similar, therefore, the words ‚Äúadventures‚Äù and ‚Äúadventures‚Äù are similar in meaning (this is a rough approximation, but the meaning is ). <br><br>  Such an approach to correcting typos, besides the obvious merits, has one important drawback - all variants of errors that we can correct should be in the text in which we learn.  That is, we cannot get a vector for a word that we have never met before. <br><br>  All (known to the author) modern methods for obtaining word vectors, except FastText, operate on words as indivisible entities (the word is replaced by an integer index, and then the index is processed).  In FastText (if even more precisely, in its <a href="https://arxiv.org/abs/1607.04606">supplement</a> ) an interesting sentence was added: let's consider these vectors not for whole words, but for n-grams of characters.  For example, the word "table" (with the added characters of the beginning and end of a word, like "&lt;table&gt;") is converted to the following list: 3-grams: &lt;St, hundred, tol, ol&gt;;  4 grams: &lt;one hundred, table, tol&gt;.  The authors propose to use n-grams from 3 to 6 inclusive, we will not argue with them.  Then the resultant vector of the word is equal to the sum of the vectors of the n-grams composing it: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>V</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>u</mi><msub><mi>m</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>g</mi><mtext>&amp;#xA0;</mtext><mi>i</mi><mi>n</mi><mi>G</mi></mrow></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>z</mi><mi>g</mi></msub></mrow><mo>,</mo></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="17.124ex" height="2.66ex" viewBox="0 -780.1 7372.6 1145.2" role="img" focusable="false" style="vertical-align: -0.848ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/singularis/blog/358664/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhGfte0eEaITlD-fNrQtQx7-VOObQ#MJMATHI-56" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/singularis/blog/358664/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhGfte0eEaITlD-fNrQtQx7-VOObQ#MJMAIN-3D" x="1047" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/singularis/blog/358664/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhGfte0eEaITlD-fNrQtQx7-VOObQ#MJMATHI-73" x="2353" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/singularis/blog/358664/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhGfte0eEaITlD-fNrQtQx7-VOObQ#MJMATHI-75" x="2823" y="0"></use><g transform="translate(3395,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/singularis/blog/358664/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhGfte0eEaITlD-fNrQtQx7-VOObQ#MJMATHI-6D" x="0" y="0"></use><g transform="translate(878,-155)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/singularis/blog/358664/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhGfte0eEaITlD-fNrQtQx7-VOObQ#MJMATHI-67" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/singularis/blog/358664/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhGfte0eEaITlD-fNrQtQx7-VOObQ#MJMATHI-69" x="834" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/singularis/blog/358664/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhGfte0eEaITlD-fNrQtQx7-VOObQ#MJMATHI-6E" x="1179" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/singularis/blog/358664/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhGfte0eEaITlD-fNrQtQx7-VOObQ#MJMATHI-47" x="1780" y="0"></use></g></g><g transform="translate(6188,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/singularis/blog/358664/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhGfte0eEaITlD-fNrQtQx7-VOObQ#MJMATHI-7A" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/singularis/blog/358664/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhGfte0eEaITlD-fNrQtQx7-VOObQ#MJMATHI-67" x="658" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/singularis/blog/358664/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhGfte0eEaITlD-fNrQtQx7-VOObQ#MJMAIN-2C" x="7094" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>V</mi><mo>=</mo><mtext>&nbsp;</mtext><mi>s</mi><mi>u</mi><msub><mi>m</mi><mrow class="MJX-TeXAtom-ORD"><mi>g</mi><mtext>&nbsp;</mtext><mi>i</mi><mi>n</mi><mi>G</mi></mrow></msub><mrow class="MJX-TeXAtom-ORD"><msub><mi>z</mi><mi>g</mi></msub></mrow><mo>,</mo></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> V = \ sum_ {g \ in G} {z_g}, </script></p><br>  Where <br><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.827ex" height="2.057ex" viewBox="0 -780.1 786.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/singularis/blog/358664/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhGfte0eEaITlD-fNrQtQx7-VOObQ#MJMATHI-47" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi></math></span></span><script type="math/tex" id="MathJax-Element-2"> G </script>  - the set of all n-gram words, <br><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>z</mi><mi>g</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.103ex" height="2.057ex" viewBox="0 -520.7 905.3 885.9" role="img" focusable="false" style="vertical-align: -0.848ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/singularis/blog/358664/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhGfte0eEaITlD-fNrQtQx7-VOObQ#MJMATHI-7A" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/singularis/blog/358664/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhGfte0eEaITlD-fNrQtQx7-VOObQ#MJMATHI-67" x="658" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>z</mi><mi>g</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-3"> z_g </script>  - the vector of the corresponding n-gram, <br><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.787ex" height="2.057ex" viewBox="0 -780.1 769.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/singularis/blog/358664/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhGfte0eEaITlD-fNrQtQx7-VOObQ#MJMATHI-56" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>V</mi></math></span></span><script type="math/tex" id="MathJax-Element-4"> V </script>  - word vector. <br><br>  What important changes does this approach hold for us? <br><br>  First, the authors introduced this in order to work better in languages ‚Äã‚Äãwith a rich morphology (which is Russian).  And indeed, morphological changes now have less effect on the distance between words; here is a tablet for different languages ‚Äã‚Äãfrom the same <a href="https://arxiv.org/abs/1607.04606">article</a> as proofs: <br><table><thead><tr><th>  Tongue </th><th>  Dataset </th><th>  Sg </th><th>  CBOW </th><th>  sisg- </th><th>  sisg </th></tr></thead><tbody><tr><td>  Ar </td><td>  WS353 </td><td>  51 </td><td>  52 </td><td>  54 </td><td>  <b>55</b> </td></tr><tr><td>  De </td><td>  Gur350 </td><td>  61 </td><td>  62 </td><td>  64 </td><td>  <b>70</b> </td></tr><tr><td>  De </td><td>  Gur65 </td><td>  78 </td><td>  78 </td><td>  <b>81</b> </td><td>  <b>81</b> </td></tr><tr><td>  De </td><td>  ZG222 </td><td>  35 </td><td>  38 </td><td>  41 </td><td>  <b>44</b> </td></tr><tr><td>  En </td><td>  Rw </td><td>  43 </td><td>  43 </td><td>  46 </td><td>  <b>47</b> </td></tr><tr><td>  En </td><td>  WS353 </td><td>  72 </td><td>  <b>73</b> </td><td>  71 </td><td>  71 </td></tr><tr><td>  Es </td><td>  WS353 </td><td>  57 </td><td>  58 </td><td>  58 </td><td>  <b>59</b> </td></tr><tr><td>  Fr </td><td>  RG65 </td><td>  70 </td><td>  69 </td><td>  <b>75</b> </td><td>  <b>75</b> </td></tr><tr><td>  Ro </td><td>  WS353 </td><td>  48 </td><td>  52 </td><td>  51 </td><td>  <b>54</b> </td></tr><tr><td>  Ru </td><td>  HJ </td><td>  59 </td><td>  60 </td><td>  60 </td><td>  <b>66</b> </td></tr></tbody></table><br>  <i>Correlation between expert evaluation and method evaluation.</i>  <i>SG and CBOW - skip-gram and continious bag of words, respectively, are Word2Vec variants, sisg- when unknown words are replaced with a zero vector, and sisg - when unknown words are replaced by the sum of their n-grams.</i> <br><br>  Secondly, it is a little step back, in Word2Vec we departed from the literal representation of the word, trying to bring together words like "king" and "king", "mother" and "son", now we return to the "letter affinity", which for The semantic task may <a href="https://rare-technologies.com/wordrank-embedding-crowned-is-most-similar-to-king-not-word2vecs-canute/">not be very good</a> , but for our version (I remind you - correcting typos), this is what you need. <br><br>  This is the theoretical part, let's move on to practice. <br><br><h2>  Practical training ground </h2><br>  We introduce some preconditions: <br><br><ol><li>  For the tests, we take a relatively small text, namely, some artistic work, for example, ‚ÄúThe Quiet Don‚Äù by Sholokhov.  Why is that?  It will be easier to repeat to the interested reader and, being aware of the context of the work, we will be able to explain the behavior of our method.  In general, vector representations of words are trained on large bodies of language, such as Wikipedia dumps. </li><li>  Normalize the words before learning, that is, we bring them into normal form (for example, for nouns this is the only number and nominative case).  This is a strong simplification in order not to tinker with the endings and increase the frequency of occurrence of words for more adequate vectors (for this purpose, it is primarily taught on large bodies to get as many uses as possible for each word). </li></ol><br><h3>  Implementation </h3><br>  The test code is quite simple (thanks, <a href="https://radimrehurek.com/gensim/">gensim</a> ), the whole script is <a href="https://gist.github.com/Kwentar/a957d29f7370f896b691c82ff9ebe7d2">here</a> , directly learning the model in one line: <br><br><pre> <code class="hljs mel">model = gensim.models.FastText(sentences, <span class="hljs-keyword"><span class="hljs-keyword">size</span></span>=<span class="hljs-number"><span class="hljs-number">300</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">window</span></span>=<span class="hljs-number"><span class="hljs-number">3</span></span>, min_count=<span class="hljs-number"><span class="hljs-number">2</span></span>, sg=<span class="hljs-number"><span class="hljs-number">1</span></span>, iter=<span class="hljs-number"><span class="hljs-number">35</span></span>)</code> </pre><br><p>  Small explanations: <br>  <em>sentences</em> is a list of lists, each element is a sentence, each element of a sentence is a word; <br>  <em>size</em> - the size of the output vectors; <br>  <em>window</em> - <em>window</em> size, the words within the window we consider the context for the word in the center; <br>  <em>min_count</em> ‚Äî consider only words that occur at least 2 times; <br>  <em>sg</em> - use skip-gram option, not CBOW; <br>  <em>iter</em> is the number of iterations. </p><br><br><p>  In addition, there are two parameters that are left by default, their value was discussed above, but you can play with them: <em>min_n</em> and <em>max_n</em> - lower and upper thresholds, which n-grams to take (default is 3 to 6 characters) </p><br><br><h3>  Measure of similarity </h3><br><p>  As a metric, we take the measure of similarity between the <a href="https://en.wikipedia.org/wiki/Cosine_similarity">Cosine similarity</a> vectors, which has already become classical in this problem, which takes values ‚Äã‚Äãfrom 0 to 1, where 0 - the vectors are completely different, 1 - the vectors are the same: <br></p><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display"><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><merror><mtext>similarity&amp;#xA0;=&amp;#xA0;cos&amp;#xA0;(\&amp;#xA0;theta)&amp;#xA0;=&amp;#xA0;\&amp;#xA0;frac&amp;#xA0;{A&amp;#xA0;\&amp;#xA0;cdot&amp;#xA0;B}&amp;#xA0;{\&amp;#xA0;parallel&amp;#xA0;A&amp;#xA0;\&amp;#xA0;parallel&amp;#xA0;\&amp;#xA0;parallel&amp;#xA0;{B}&amp;#xA0;\&amp;#xA0;parallel}&amp;#xA0;=&amp;#xA0;\&amp;#xA0;frac&amp;#xA0;{\&amp;#xA0;sum_&amp;#xA0;{i&amp;#xA0;=&amp;#xA0;1}&amp;#xA0;^&amp;#xA0;n&amp;#xA0;A_iB_i}&amp;#xA0;{\&amp;#xA0;sqrt&amp;#xA0;{&amp;#xA0;\&amp;#xA0;sum_&amp;#xA0;{i&amp;#xA0;=&amp;#xA0;1}&amp;#xA0;^&amp;#xA0;n&amp;#xA0;A_i&amp;#xA0;^&amp;#xA0;2}&amp;#xA0;\&amp;#xA0;sqrt&amp;#xA0;{\&amp;#xA0;sum_&amp;#xA0;{i&amp;#xA0;=&amp;#xA0;1}&amp;#xA0;^&amp;#xA0;n&amp;#xA0;B_i&amp;#xA0;^&amp;#xA0;2}</mtext></merror></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><span class="noError" aria-hidden="true" style="display: inline-block;">similarity&nbsp;=&nbsp;cos&nbsp;(\&nbsp;theta)&nbsp;=&nbsp;\&nbsp;frac&nbsp;{A&nbsp;\&nbsp;cdot&nbsp;B}&nbsp;{\&nbsp;parallel&nbsp;A&nbsp;\&nbsp;parallel&nbsp;\&nbsp;parallel&nbsp;{B}&nbsp;\&nbsp;parallel}&nbsp;=&nbsp;\&nbsp;frac&nbsp;{\&nbsp;sum_&nbsp;{i&nbsp;=&nbsp;1}&nbsp;^&nbsp;n&nbsp;A_iB_i}&nbsp;{\&nbsp;sqrt&nbsp;{&nbsp;\&nbsp;sum_&nbsp;{i&nbsp;=&nbsp;1}&nbsp;^&nbsp;n&nbsp;A_i&nbsp;^&nbsp;2}&nbsp;\&nbsp;sqrt&nbsp;{\&nbsp;sum_&nbsp;{i&nbsp;=&nbsp;1}&nbsp;^&nbsp;n&nbsp;B_i&nbsp;^&nbsp;2}</span><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><merror><mtext>similarity&nbsp;=&nbsp;cos&nbsp;(\&nbsp;theta)&nbsp;=&nbsp;\&nbsp;frac&nbsp;{A&nbsp;\&nbsp;cdot&nbsp;B}&nbsp;{\&nbsp;parallel&nbsp;A&nbsp;\&nbsp;parallel&nbsp;\&nbsp;parallel&nbsp;{B}&nbsp;\&nbsp;parallel}&nbsp;=&nbsp;\&nbsp;frac&nbsp;{\&nbsp;sum_&nbsp;{i&nbsp;=&nbsp;1}&nbsp;^&nbsp;n&nbsp;A_iB_i}&nbsp;{\&nbsp;sqrt&nbsp;{&nbsp;\&nbsp;sum_&nbsp;{i&nbsp;=&nbsp;1}&nbsp;^&nbsp;n&nbsp;A_i&nbsp;^&nbsp;2}&nbsp;\&nbsp;sqrt&nbsp;{\&nbsp;sum_&nbsp;{i&nbsp;=&nbsp;1}&nbsp;^&nbsp;n&nbsp;B_i&nbsp;^&nbsp;2}</mtext></merror></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-5"> similarity = cos (\ theta) = \ frac {A \ cdot B} {\ parallel A \ parallel \ parallel {B} \ parallel} = \ frac {\ sum_ {i = 1} ^ n A_iB_i} {\ sqrt { \ sum_ {i = 1} ^ n A_i ^ 2} \ sqrt {\ sum_ {i = 1} ^ n B_i ^ 2} </script></p><br>  <i>Where</i> <math> </math><i><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>A</mi><mi>i</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.543ex" height="2.419ex" viewBox="0 -780.1 1094.8 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/singularis/blog/358664/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhGfte0eEaITlD-fNrQtQx7-VOObQ#MJMATHI-41" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/singularis/blog/358664/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhGfte0eEaITlD-fNrQtQx7-VOObQ#MJMATHI-69" x="1061" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>A</mi><mi>i</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-6"> A_i </script></i>   <i>and</i> <math> </math><i><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>B</mi><mi>i</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.564ex" height="2.419ex" viewBox="0 -780.1 1103.8 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/singularis/blog/358664/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhGfte0eEaITlD-fNrQtQx7-VOObQ#MJMATHI-42" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/singularis/blog/358664/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhGfte0eEaITlD-fNrQtQx7-VOObQ#MJMATHI-69" x="1074" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>B</mi><mi>i</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-7"> B_i </script></i>   <i>- components of the corresponding vectors.</i> <br><br><h3>  Experiments </h3><br><p>  So, we figured out what we have, and what we want, just in case again: </p><br><br><ol><li>  The hypothesis is that based on the vector representation of words, we can correct typos. </li><li>  We have a trained FastText model in just one work, the words in it are normalized, we can also get a vector for unknown words. </li><li>  The method of comparing words, or rather their vectors, is defined in the previous paragraph. </li></ol><br><p>  Now let's see what we can do, for the tests we take the following pairs - a word with a typo and the word in parentheses: <br>  man (person), chair (student), student (student), chilovennost (humanity), participate (participate), tactics (tactics), in general (generally), simpotichny (cute), create (make), watch (watch), algaritm (algorithm), lay down (put). </p><br><br>  Summary table with the results: <br><table><thead><tr><th>  A typo word </th><th>  Conceived word </th><th>  No. in the list of the nearest </th><th>  Metric value </th></tr></thead><tbody><tr><td>  man </td><td>  person </td><td>  one </td><td>  0.88227 </td></tr><tr><td>  stool </td><td>  student </td><td>  ten </td><td>  0.67878 </td></tr><tr><td>  student's </td><td>  student </td><td>  one </td><td>  0.85078 </td></tr><tr><td>  chilovennost </td><td>  humanity </td><td>  one </td><td>  0.75012 </td></tr><tr><td>  take part </td><td>  participate </td><td>  6 </td><td>  0.87767 </td></tr><tr><td>  tact </td><td>  tactics </td><td>  2 </td><td>  0.73543 </td></tr><tr><td>  in general </td><td>  at all </td><td>  (one) </td><td>  0.96243 </td></tr><tr><td>  simpotichny </td><td>  pretty </td><td>  (one) </td><td>  0.92399 </td></tr><tr><td>  create one </td><td>  to make </td><td>  2 </td><td>  0.91553 </td></tr><tr><td>  smtret </td><td>  watch </td><td>  one </td><td>  0.80055 </td></tr><tr><td>  algaritm </td><td>  algorithm </td><td>  (one) </td><td>  0.86162 </td></tr><tr><td>  lay down </td><td>  put </td><td>  ten </td><td>  0.81719 </td></tr></tbody></table><br>  Remarks: <br><br><ol><li>  If the number is indicated in brackets, it means that the word is not in the dictionary, but it could be in this place (based on the metric). </li><li>  It‚Äôs actually not so bad with a couple of lay-put, since the words above are ‚Äúput‚Äù, ‚Äúput aside‚Äù, ‚Äúlay out‚Äù, etc.  (see spoiler). </li><li>  Sometimes in the top of similar words there are words that are very different from queries (chair - driver), presumably this is due to a kind of collision of vectors - when approximately identical vectors are obtained for different n-grams. </li></ol><br><div class="spoiler">  <b class="spoiler_title">Top 10 nearest words for each</b> <div class="spoiler_text">  man: <br>  man 0.87035 <br>  chelba 0.80893 <br>  human 0.77607 <br>  check 0.74867 <br>  century 0.71127 <br>  shuttle 0.68631 <br>  human 0.63725 <br>  humanity 0.63615 <br>  granddaughters 0.59655 <br>  bee 0.59173 <br><hr><br>  stool: <br>  chair 0.73342 <br>  chair 0.70797 <br>  tarpaulin 0.67196 <br>  knocker 0.64903 <br>  tool 0.64340 <br>  foundation 0.61881 <br>  driver 0.60767 <br>  bridge 0.60279 <br>  sheepskin 0.60249 <br>  turbo 0.59757 <br>  student 0.58953 <br><hr><br>  student's: <br>  student 0.85685 <br>  youthful 0.75904 <br>  weighty 0.72052 <br>  chemical 0.71119 <br>  adolescent 0.70076 <br>  hysterical 0.69888 <br>  physical 0.69580 <br>  boyish 0.68713 <br>  phosphoric 0.68312 <br>  full 0.68136 <br><hr><br>  chileiness: <br>  humanity 0.75012 <br>  candor 0.74727 <br>  worth of 0.72961 <br>  proximity 0.72873 <br>  boundedness 0.72581 <br>  the value of 0.72350 <br>  cowardice 0.72186 <br>  ambiguity 0.72128 <br>  pregnancy 0.72121 <br>  significance 0.72100 <br><hr><br>  participate: <br>  health 0.93609 <br>  rampage 0.89396 <br>  persist 0.89216 <br>  to live in distress 0.88620 <br>  gloat 0.87975 <br>  participate 0.87767 <br>  rampage 0.87446 <br>  get drunk 0.86810 <br>  feel 0.86627 <br>  sympathize with 0.86622 <br><hr><br>  tactic: <br>  tact 0.87537 <br>  tactic 0.73542 <br>  stool 0.66532 <br>  label 0.65750 <br>  scab 0.65702 <br>  brush 0.64602 <br>  Annie 0.64291 <br>  grid 0.62549 <br>  taka 0.62321 <br>  convolution 0.61241 <br><hr><br>  in general: <br>  Post 0.57405 <br>  armament 0.51535 <br>  insignificant 0.50341 <br>  armed with 0.49465 <br>  unarmed 0.48958 <br>  communication 0.48076 <br>  dormitory 0.48069 <br>  journey 0.47493 <br>  low value 0.46655 <br>  report 0.46373 <br><hr><br>  simpotichny: <br>  personal 0.86102 <br>  street 0.84662 <br>  energetic 0.83907 <br>  cynical 0.82305 <br>  excellent 0.81775 <br>  typical 0.80783 <br>  factory 0.80701 <br>  egg 0.80283 <br>  secondary 0.79368 <br>  0.7946 howitzers <br><hr><br>  to create: <br>  to make 0.93598 <br>  make 0.91553 <br>  do 0.90678 <br>  make 0.87672 <br>  to finish 0.87297 <br>  trim 0.85775 <br>  close up 0.84235 <br>  wish for 0.82316 <br>  do 0.78098 <br>  do 0.77232 <br><hr><br>  see: <br>  see 0.80055 <br>  see 0.78115 <br>  look at 0.77926 <br>  third 0.73819 <br>  view 0.716420 <br>  view 0.71075 <br>  weathered 0.68950 <br>  peer 0.68513 <br>  motley 0.65353 <br>  look 0.65069 <br><hr><br>  algaritm: <br>  rhythm 0.85291 <br>  demi-season 0.69376 <br>  leggings 0.66552 <br>  cynicism 0.66278 <br>  stupid 0.65656 <br>  Shorin 0.65496 <br>  baritone 0.64623 <br>  frasenbruder 0.64395 <br>  harness 0.63321 <br>  trellis 0.63161 <br><hr><br>  lay down: <br>  attach 0.89386 <br>  put 0.89129 <br>  impose 0.87222 <br>  lay out 0.87199 <br>  set aside 0.84127 <br>  put down 0.83720 <br>  lay 0.83572 <br>  add 0.83549 <br>  apply 0.82764 <br>  put 0.81718 <br><hr><br></div></div><br><h2>  Conclusion </h2><br><p>  Using a vector representation can undoubtedly help in the task of correcting typos / errors, but using it alone is quite dangerous, as sometimes (though rarely) strong errors occur. </p><br>  In fact, this is another metric comparing two lines for similarity, but already a level higher than, for example, the same <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D1%2581%25D1%2581%25D1%2582%25D0%25BE%25D1%258F%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%2594%25D0%25B0%25D0%25BC%25D0%25B5%25D1%2580%25D0%25B0%25D1%2583_%25E2%2580%2594_%25D0%259B%25D0%25B5%25D0%25B2%25D0%25B5%25D0%25BD%25D1%2588%25D1%2582%25D0%25B5%25D0%25B9%25D0%25BD%25D0%25B0">Damerau-Levenshteyn distance</a> .  Using FastText as a supplement to other methods may well improve the quality of typo correction. </div><p>Source: <a href="https://habr.com/ru/post/358664/">https://habr.com/ru/post/358664/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../358654/index.html">Parsing 0.5Tb xml in a few hours. Search for organizations in the open data of the register of SMEs of the Federal Tax Service</a></li>
<li><a href="../358656/index.html">Welcome to Java Meetup at Raiffeisenbank UPD Broadcast</a></li>
<li><a href="../358658/index.html">Creating Destructive Meshes</a></li>
<li><a href="../358660/index.html">Open broadcast of the main hall of HolyJS</a></li>
<li><a href="../358662/index.html">Safe car sharing: components, main problems and Yandex competition</a></li>
<li><a href="../358668/index.html">How to learn anything?</a></li>
<li><a href="../358670/index.html">Laser hair removal for a virtual server</a></li>
<li><a href="../358672/index.html">Why you should stop using grocery roadmap and try GIST</a></li>
<li><a href="../358674/index.html">Javier Mertens: ‚ÄúCryptojacking is one of the most brilliant attacks I've seen.‚Äù</a></li>
<li><a href="../358676/index.html">Mathematical modeling haboo-future</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>