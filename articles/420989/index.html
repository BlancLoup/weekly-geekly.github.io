<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Why you should not throw out the Radeon, if you are carried away by machine learning?</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I had to collect my workstation as a student. It is quite logical that I preferred AMD computing solutions. because it  cheap  profitable in terms of ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Why you should not throw out the Radeon, if you are carried away by machine learning?</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/qk/l2/et/qkl2etw2okgtjnaiuwzytjudazy.jpeg" alt="image"></p><br><p>  I had to collect my workstation as a student.  It is quite logical that I preferred AMD computing solutions.  because it <del>  cheap </del>  profitable in terms of price / quality ratio.  I picked up the components for a long time, and eventually ended up at 40k with a set of FX-8320 and RX-460 2GB.  At first this kit seemed perfect!  My roommate and I had a little Monero mine, and my set showed 650h / s versus 550h / s on a set of i5-85xx and Nvidia 1050Ti.  True, from my set in the room it was a little hot at night, but it was decided when I purchased a tower cooler for the CPU. </p><a name="habracut"></a><br><h3 id="skazka-konchilas">  The tale is over </h3><br><p>  Everything was just like a fairy tale until I became interested in machine learning in the field of computer vision.  Even more precisely - until I had to work with input images with a resolution of more than 100x100px (up to this point, my 8-core FX played up briskly).  The first difficulty was the task of determining emotions.  4 layers of ResNet, an input image of 100x100 and 3000 images in a training set.  And now - 9 hours of learning 150 epochs on the CPU. <br>  Of course, because of this delay, the iterative development process suffers.  At work, we had Nvidia 1060 6GB and training of a similar structure (although there was a regression study to localize objects) on it flew in 15-20 minutes - 8 seconds per era from 3.5k images.  When you have such a contrast under your nose, breathing becomes even more difficult. </p><br><p>  Well, guess my first move after all this?  Yes, I went to bargain for 1050Ti from my neighbor.  With arguments about the uselessness of CUDA for him, with the offer of exchange for my card with a surcharge.  But all in vain.  And now I'm posting my RX 460 on Avito and reviewing the cherished 1050Ti on Citylink and TechnoPoint sites.  Even if the card was successfully sold, I would have to find another 10k (I am a student, albeit a working one). </p><br><h3 id="guglyu">  Google </h3><br><p>  Okay.  I'm going to google how to use a radeon under Tensorflow.  Knowing that this is an exotic task, I did not particularly hope to find something sensible.  Collect under Ubuntu, start it or not, get a brick - phrases snatched from the forums. </p><br><p>  And so I went the other way - I'm not googling "Tensorflow AMD Radeon", but "Keras AMD Radeon".  I instantly throws on the page <a href="https://github.com/plaidml/plaidml">PlaidML</a> .  I <a href="">get it</a> in 15 minutes (although I had to drop Keras to 2.0.5) and put on a network to learn.  The first observation is that the era is 35 seconds instead of 200. </p><br><h3 id="lezu-issledovat">  Climb climb </h3><br><p>  The authors PlaidML - <a href="http://vertex.ai/">vertex.ai</a> , included in the project group Intel (!).  The goal of the development is maximum cross-platform.  Of course, this adds confidence to the product.  Their <a href="http://vertex.ai/blog/benchmarking-deep-neural-nets-for-real-time-vision">article</a> says that PlaidML is competitive with Tensorflow 1.3 + cuDNN 6 due to "thorough optimization". </p><br><p>  However, we will continue.  <a href="http://vertex.ai/blog/kernel-generation">The following article</a> to some extent reveals the internal structure of the library.  The main difference from all other frameworks is the automatic generation of computing cores (in Tensorflow notation, ‚Äúcore‚Äù is the complete process of performing a specific operation in a graph).  For the automatic generation of nuclei in PlaidML, the exact dimensions of all tensors, constants, steps, the dimensions of convolutions and the boundary values ‚Äã‚Äãwith which we will have to work further are very important.  For example, it is argued that the further creation of effective kernels differs for batchsize 1 and 32 or for bundles of size 3x3 and 7x7.  Having this data, the framework itself will generate the most efficient way to parallelize and execute all operations for a particular device with specific characteristics.  If you look at Tensorflow, then when creating new operations we need to implement the core for them - and the implementations are <a href="https://www.tensorflow.org/extend/adding_an_op">very different</a> for single-threaded, multi-threaded, or CUDA-compatible cores.  Those.  PlaidML is clearly more flexible. </p><br><p>  We go further.  The implementation is written in the <a href="https://vertexai-plaidml.readthedocs-hosted.com/en/latest/writing_tile_code.html">Tile</a> self-written language.  This language has the following main advantages - the proximity of syntax to mathematical notations (and go crazy!): </p><br><p><img src="https://habrastorage.org/webt/5w/j-/t_/5wj-t_9lnzzlcln4enex-mltn9q.png" alt="image"></p><br><p>  And automatic differentiation of all declared operations.  For example, in TensorFlow, when creating a new user operation, it is strongly recommended to write a function for calculating gradients.  Thus, when creating our own operations in the Tile language, we only need to say <strong>WHAT</strong> we want to calculate, without thinking about <strong>HOW</strong> to consider this with regard to hardware devices. </p><br><p>  Additionally, optimization of work with DRAM and L1 cache analogue in the GPU is performed.  Recall the schematic device: </p><br><p><img src="https://habrastorage.org/webt/sx/kf/hn/sxkfhnnaru91nkmrvbwqesxnsxs.png" alt="image"></p><br><p>  For optimization, all available hardware data is used ‚Äî cache size, cache line width, DRAM bandwidth, and so on.  The main methods are to ensure the simultaneous reading of sufficiently large blocks from DRAM (an attempt to avoid addressing in different areas) and to ensure that the data loaded into the cache is used several times (an attempt to avoid reloading the same data several times). </p><br><p>  All optimizations take place during the first epoch of training, while greatly increasing the time of the first run: </p><br><p><img src="https://habrastorage.org/webt/3e/0v/of/3e0vofsvwm0y5urlztncxdxmurc.jpeg" alt="image"></p><br><p>  In addition, it is worth noting that this framework is tied to <a href="https://ru.wikipedia.org/wiki/OpenCL">OpenCL</a> .  The main advantage of OpenCL is that it is a <a href="https://habr.com/post/72247/">standard for heterogeneous systems and nothing prevents you from running the kernel on the CPU</a> .  Yes, it is here that lies one of the main secrets of the cross-platform PlaidML. </p><br><h3 id="zaklyuchenie">  Conclusion </h3><br><p>  Of course, training on the RX 460 is still going 5-6 times slower than 1060, but you can compare the price categories of video cards!  Then I got the RX 580 8gb (I was lent!) And the runtime of the epoch was reduced to 20 seconds, which is almost comparable. </p><br><p>  The vertex.ai blog has honest graphics (more is better): </p><br><p><img src="https://habrastorage.org/webt/no/bm/pz/nobmpzbv8-b_6coixtftrmwumrq.png" alt="image"></p><br><p>  It can be seen that PlaidML is competitive with Tensorflow + CUDA, but not exactly faster for current versions.  But PlaidML developers are probably not planning to enter into such an open fight.  Their goal is versatility, cross-platform. </p><br><p>  I'll leave here and not quite a comparative table with my performance measurements: </p><br><table><thead><tr><th>  Computing device </th><th>  Epoch runtime (batch - 16), s </th></tr></thead><tbody><tr><td>  AMD FX-8320 tf </td><td>  200 </td></tr><tr><td>  RX 460 2GB plaid </td><td>  35 </td></tr><tr><td>  RX 580 8 GB plaid </td><td>  20 </td></tr><tr><td>  1060 6GB tf </td><td>  eight </td></tr><tr><td>  1060 6GB plaid </td><td>  ten </td></tr><tr><td>  Intel i7-2600 tf </td><td>  185 </td></tr><tr><td>  Intel i7-2600 plaid </td><td>  240 </td></tr><tr><td>  GT 640 plaid </td><td>  46 </td></tr></tbody></table><br><p>  The last article in the vertex.ai blog and the last edits in the repository are dated May 2018.  It seems that if the developers of this tool do not stop releasing new versions and more and more people who are offended by Nvidia are familiar with PlaidML, then vertex.ai will soon be talked about much more often. </p><br><p>  Uncover your Radeons! </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/420989/">https://habr.com/ru/post/420989/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../420975/index.html">Do not do this in production.</a></li>
<li><a href="../420977/index.html">Tricks QComboBox + QTreeView</a></li>
<li><a href="../420979/index.html">Redux Basics (Text Tutorial, 2nd Edition)</a></li>
<li><a href="../420981/index.html">‚ÄúI can tell you about the common pain of all iOS developers‚Äù - 10 questions for a programmer, issue 2</a></li>
<li><a href="../420983/index.html">Researchers claim that it‚Äôs almost impossible to hide from Google.</a></li>
<li><a href="../420991/index.html">Pursuit race</a></li>
<li><a href="../420993/index.html">5 easy steps to create a server for testing android REST requests</a></li>
<li><a href="../420995/index.html">We select the password to the Indian TIN for two seconds, or why brute force math</a></li>
<li><a href="../420997/index.html">KDD 2018, fourth day, stands Nobel laureate</a></li>
<li><a href="../420999/index.html">Kivy. Xamarin. React Native. Three frameworks - one experiment (part 2)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>