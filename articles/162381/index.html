<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Build yourself: how we did the Amazon-style storage for small hosters</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="More and more Russian Internet projects want a server in the cloud and look towards Amazon EC2 and its analogues. The flight of large customers to the...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Build yourself: how we did the Amazon-style storage for small hosters</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/storage2/291/005/352/2910053523e3628155eab5f6d0f8c291.jpg"><br><br>  More and more Russian Internet projects want a server in the cloud and look towards Amazon EC2 and its analogues.  The flight of large customers to the West, we perceive as a challenge for runet hosters.  After all, they would also have ‚Äútheir own Amazon‚Äù, with preference and poetess.  To meet the demand of hosters for distributed data storage for deployment at relatively small capacities, we have made Parallels Cloud Server (PCS). <br><br>  In the post under the cut, I will talk about the architecture of the storage-part - one of the main highlights of the PCS.  It allows you to organize on conventional hardware storage system, in terms of speed and fault tolerance comparable to expensive SAN-storage.  The second post (it is already being prepared) will be of interest to developers, it will deal with things that we learned in the process of creating and testing the system. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <a name="habracut"></a><br><br><h3>  Intro </h3><br><br>  The vast majority of Habr's readers know about Parallels as a developer of a solution for running Windows on a Mac.  Another part of the audience knows about our container virtualization products - <a href="http://www.parallels.com/products/pvc/">Parallels Virtuozzo Containers</a> and <a href="http://www.parallels.com/products/server/baremetal/sp/">Parallels Server Bare Metal</a> , as well as open-source <a href="http://wiki.openvz.org/Main_Page">OpenVZ</a> .  The ideas that underlie the containers are to a greater or lesser degree used by cloud providers of the scale of Yandex and Google.  But in general, Parallels Skate is a software for small and / or fast-growing service providers.  Containers allow hosters to run multiple copies of operating systems on a single server.  This gives a 3‚Äì4 times higher density of virtual environments than hypervisors.  Additionally, container virtualization allows for the migration of virtual environments between physical machines, and this happens seamlessly for the client of the hoster or the cloud provider. <br><br>  As a rule, service providers to store client data use local disks of existing servers.  There are two problems with local storage.  First, data transfer is often complicated by the size of the virtual machine.  The file can be very large.  The second problem: the high availability of virtual machines is difficult to achieve if the server shuts down, say, as a result of a power failure. <br><br>  Both problems can be solved, but the cost of the solution is quite high.  There are storage systems such as <a href="http://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B5%25D1%2582%25D1%258C_%25D1%2585%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B5%25D0%25BD%25D0%25B8%25D1%258F_%25D0%25B4%25D0%25B0%25D0%25BD%25D0%25BD%25D1%258B%25D1%2585">SAN / NAS storage</a> .  These are such big boxes, whole racks, and in the case of a large provider - data centers for many, many petabytes.  Servers connect to them using any protocol and put the data there, or take it from there.  SAN storage provides redundant data storage, fault tolerance, it has its own monitoring of the status of disks, as well as the function of self-repair and correction of detected errors.  All this is very cool with one exception: even the simplest SAN storage will pull no less than $ 100 thousand, which for our partners - small hosters - is quite a lot of money.  At the same time, they want to have something similar, and they constantly tell us about it.  The task was drawn by itself: to offer them a solution of similar functionality with a noticeably lower purchase and ownership price. <br><br><h3>  PCS architecture: first touches </h3><br><br>  We pushed away from the fact that our interpretation of distributed storage should be as simple as possible.  And it came from a task that was also simple: the system should provide for the operation of virtual machines and containers. <br><br>  Virtual machines and containers have certain requirements for consistency of stored data.  That is, the result of operations on data somehow corresponds to the order in which they are produced. <br><br>  Recall how any journaling file system works.  She has a journal, she has metadata and the actual data.  The file system logs the data, waits until the end of this operation, after which only the data is written to the hard disk where it should be.  Then, a new entry falls on the space vacated in the journal, which after some time is sent to the space allocated to it.  In the event of a power down or in the event of a system crash, all data in the log will be there before the fixed point.  They can be lost, having written down already in a final place. <br><br>  There are many types of consistency.  The order in which all file systems expect to see their data is called immediate / strict consistency.  Let us translate this as ‚Äústrict consistency.‚Äù  There are two distinct features of strict consistency.  First, all readings return the values ‚Äã‚Äãjust written: old data is never visible.  Secondly, the data is visible in the same order in which they were recorded. <br><br>  Most surprisingly, cloud storage solutions do not provide such properties.  In particular, Amazon S3 Object Storage, used for various web-oriented services, provides completely different guarantees - the so-called eventual consistency, or final consistency in time.  These are systems in which the recorded data is guaranteed not immediately visible, but after some time.  For virtual machines and file systems, this is not appropriate. <br><br>  Additionally, we wanted our data storage system to have some more remarkable properties: <br><ul><li>  She should be able to grow as needed.  Storage capacity should increase dynamically when new disks are added. </li><li>  It should be able to allocate more space than there is on a single disk. </li><li>  It should be able to break the amount of allocated space into several disks, because 100 TB can not be put on one disk. </li></ul><br><br>  In the history with several disks there is a pitfall.  As soon as we begin to distribute the array of data across multiple disks, the probability of data loss increases dramatically.  If we have one server with one disk, then the probability that the disk will fail is not very big.  But if there are 100 disks in our storage, the probability that <b>at least</b> one disk will burn will greatly increase.  The more disks, the higher the risk.  Accordingly, the data must be stored redundantly, in several copies. <br><br>  The distribution of data on many disks has its advantages.  For example, it is possible to restore the original image of a broken disk in parallel with multiple live disks at the same time.  As a rule, such an operation takes only a few minutes in a cloud storage, in contrast to traditional RAID arrays, which will take several hours or even days to restore.  In general, the probability of data loss is inversely proportional to the square of the time required to restore them.  Accordingly, the faster we recover the data, the lower the likelihood that the data will be lost. <br><br>  It was solved: we divide the entire data array into pieces of a fixed size (64-128 MB in our case), we will replicate them in a given number of copies and distribute them throughout the cluster. <br><br>  Then we decided to simplify everything to the maximum.  First of all, it was clear that we did not need a regular POSIX-compatible file system.  It only takes to optimize the system for large objects - images of virtual machines - that take several tens of gigabytes.  Since the images themselves are rarely created / deleted / renamed, metadata changes rarely occur and these changes can not be optimized, which is very significant.  For the operation of a container or a virtual machine, inside objects there is a file system.  It is optimized for changing metadata, provides standard POSIX semantics, etc. <br><br><h3>  Attack of the Clones </h3><br><br>  You can often hear judgments that only individual cluster nodes fail at the providers.  Alas, it also happens that even data centers can lose their power supply entirely - the entire cluster falls.  From the very beginning we decided that we should take into account the possibility of such failures.  Let's go back a bit and think about why it‚Äôs difficult to ensure strict consistency of stored data in a distributed data warehouse.  And why did the major providers (the same Amazon S3) start with eventual consistency storage? <br><br>  The problem is simple.  Suppose we have three servers that store one copy of the object.  At some point changes occur in the object.  These changes manage to write two of the three servers;  the third was for some reason not available.  Next on our rack or in our data center power is lost, and when it returns, the servers begin to load.  And it may happen that the server to which the object changes were not recorded will be loaded first.  If you do not take any special measures, the client can access the old, irrelevant copy of the data. <br><br>  If we saw a large object (the image of a virtual machine in our case) into several pieces, everything becomes more complicated.  Suppose distributed storage divides an image of a file into two fragments, for which six servers will be needed.  If not all servers have time to make changes to these fragments, then after a power failure, when the servers are loaded, we already have four combinations of fragments, and two of them never existed in nature.  The file system for this is not designed at all. <br><br>  It turns out that it is necessary to somehow assign versions to objects.  This is realizable in several ways.  The first is to use a transactional file system (for example, BTRFS) and update the version along with the data update.  This is correct, but when using traditional (rotating) hard drives for the time being - slowly: performance drops by several times.  The second option is to use some kind of consensus algorithm (for example, <a href="http://en.wikipedia.org/wiki/Paxos_(computer_science)">Paxos</a> ), so that the servers that make the modification agree among themselves.  This is also slow.  The servers themselves, which are responsible for the data, cannot track the versionality of changes in object fragments, since  they don‚Äôt know if the data was changed by someone else.  Therefore, we concluded that data versions should be updated somewhere on the side. <br><br>  Versioning will be monitored by a metadata server (MDS).  At the same time, it is not necessary to update the version upon successful recording, it is necessary only when one of the servers did not record for any reason and we exclude it.  Therefore, in normal mode, data is recorded as quickly as possible. <br><br>  Actually, the solution architecture is made up of all these conclusions.  It consists of three components: <br><ul><li>  Clients that communicate with the storage over normal Ethernet. </li><li>  MDS, which stores information about where the files are and where the current versions of the fragments are located. </li><li>  The fragments themselves, distributed over local hard drives of servers. </li></ul><br><br>  Obviously, the MDS is the bottleneck in the whole architecture.  He knows everything about the system.  Therefore, it needs to be made highly available - so that if a machine fails, the system will be available.  There was a temptation to ‚Äúput‚Äù MDS in a MySQL database or in another popular database.  But this is not our method, because SQL servers are quite limited by the number of requests per second, which immediately puts an end to the scalability of the system.  In addition, to make a reliable cluster of SQL servers is even more difficult. We peered the solution <a href="http://en.wikipedia.org/wiki/Google_File_System">in the article about GoogleFS</a> .  In the original performance of GoogleFS, it did not fit our tasks.  It does not provide the desired strict consistency, since it is intended to add data to search engines, but not to modify this data.  The solution was as follows: MDS stores the full state of the objects and their versions in the entire memory.  It turns out not so much.  Each fragment describes only 128 bytes.  That is, a description of the state of the storage of several petabytes will fit into the memory of a modern server, which is acceptable.  Status Changes MDS writes metadata to the log.  The magazine is growing, albeit relatively slowly. <br><br>  Our task is to make it so that several MDSs can somehow negotiate among themselves and make the solution available even if one of them falls.  But there is a problem: the log file cannot grow forever, something will have to be done with it.  For this, a new log is created, and all changes begin to be written there, and in parallel with this, a memory snapshot is created, and the state of the system begins to be written asynchronously to snapshots.  After snapshot is created, you can delete the old log.  In the event of a system crash, you need to ‚Äúlose‚Äù the snapshot and log to it.  When the new log grows back, the procedure is repeated. <br><br><h3>  More hints </h3><br><br>  Here are a couple of tricks that we implemented in the process of creating our distributed cloud storage. <br><br>  <b>How is the high speed of recording fragments achieved?</b>  As a rule, three copies of data are used in distributed repositories.  If you write "in the forehead," the client must send a request to the three servers to write with new data.  It will turn out slowly, namely three times slower than the network bandwidth.  If we have Gigabit Ethernet, then in a second we will transfer only 30-40 MB of data to each copy, which is not an outstanding result, because the recording speed even on the HDD is significantly higher.  To use the resources of iron and the network more efficiently, we applied chain-replication.  The client sends data to the first server.  He, having received the first part (64 Kb), writes it to the disk, and immediately sends it to the other servers in parallel along the chain.  As a result, a large request begins to be written as early as possible, and is transmitted asynchronously to other participants.  It turns out that iron is used at 80% of maximum performance, even if we are talking about three copies of data.  Everything works great because Ethernet can simultaneously receive and send data (full duplex), i.e.  in reality, it produces two gigabits per second, and not one.  And the server that received the data from the client, with the same speed, sends them down the chain. <br><br>  <b>SSD caching.</b>  SSDs help speed up the operation of any storage.  The idea is not new, although it should be noted that in open source there are no really good solutions.  SAN-storage for a long time is used caching.  Hint is based on the ability of SSD to give out at random access performance is orders of magnitude higher than HDD can produce.  You start caching data on the SSD - you get the speed ten times higher.  Additionally, we count the checksums of all the data, and store them also on SSD-drives.  Periodically reading all the data, we check their availability and compliance with checksums.  The latter is sometimes also called scrabbing (if you remember, a skin scrub removes loose particles from it) and increases the reliability of the system, as well as allows you to detect errors before the data is needed in reality. <br><br>  There is another reason why SSD caching is important for virtual machines.  The fact is that with object storage, for example, Amazon S3 and its analogs, the delay in accessing an object is not very important.  As a rule, they are accessed via the Internet, and a delay of 10 ms is simply not noticeable there.  If we are talking about a virtual machine on the server of the hosting provider, then when the OS performs a series of consecutive synchronous requests, the delay accumulates and becomes very noticeable.  Moreover, all scripts and most applications are synchronous in nature, i.e.  perform operation by operation, not in parallel.  As a result, seconds and even tens of seconds are already noticeable in the user interface or in some responses to user actions. <br><br><h3>  Results &amp; Conclusions </h3><br><br>  As a result, we obtained a data storage system, which by properties is suitable for hosters to be deployed on their infrastructure, because it has the following properties: <br><br><ul><li>  Ability to execute virtual machines and containers directly from storage, because  Our system provides strong consistency semantics. </li><li>  Ability to scale to petabyte volumes on a variety of servers and disks. </li><li>  Support for checksums of data and SSD caching. </li></ul><br><br>  A few words about performance.  It turned out to be comparable with the performance of enapraise storage facilities.  We took on a test from Intel company a cluster of 14 servers with 4x HDD and received 13 thousand random I / O operations per second with very small (4 Kb) files on ordinary hard drives with SATA interface.  This is quite a lot.  SSD caching accelerated the work of the same storage by almost two orders of magnitude - we approached 1 million i / o operations per second.  The recovery rate of one terabyte of data in 10 minutes, and the greater the number of disks, the faster the recovery. <br><br>  SAN-storage with similar parameters will cost from several hundred thousand dollars.  Yes, a large company can afford its purchase and maintenance, but we made our distributed storage for hosters who would like to get a solution of similar parameters for incomparably less money and already on existing equipment. <br><br>  *** <br><br>  As stated at the beginning, ‚Äúto be continued.‚Äù  A post is being prepared on how the storage-part of Parallels Cloud Server was developed.  Suggestions are accepted in comments that you would be interested to read about.  I will try to accommodate them. </div><p>Source: <a href="https://habr.com/ru/post/162381/">https://habr.com/ru/post/162381/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../162371/index.html">Yandeks.Pochta - is controlled from the keyboard</a></li>
<li><a href="../162373/index.html">Social media aggregator Meople.net united in one place 10 basic social programs</a></li>
<li><a href="../162375/index.html">Copyright history. Part 7: Raid seizure by Pfizer</a></li>
<li><a href="../162377/index.html">Promotion for startups</a></li>
<li><a href="../162379/index.html">IE allows you to track mouse coordinates (even in another window)</a></li>
<li><a href="../162385/index.html">Taste BEM!</a></li>
<li><a href="../162387/index.html">Infographics: HTML5 history</a></li>
<li><a href="../162389/index.html">Fragment animation in Android</a></li>
<li><a href="../162393/index.html">Yota smartphone with eInk second screen</a></li>
<li><a href="../162395/index.html">Search engine from the past</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>