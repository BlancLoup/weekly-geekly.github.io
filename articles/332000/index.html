<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Autoencoders in Keras, Part 5: GAN (Generative Adversarial Networks) and tensorflow</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Content 


- Part 1: Introduction 
- Part 2: Manifold learning and latent variables 
- Part 3: Variational autoencoders ( VAE ) 
- Part 4: Conditional...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Autoencoders in Keras, Part 5: GAN (Generative Adversarial Networks) and tensorflow</h1><div class="post__text post__text-html js-mediator-article"><h3>  Content </h3><br><ul><li>  Part 1: <a href="https://habrahabr.ru/post/331382/">Introduction</a> <br></li><li>  Part 2: <a href="https://habrahabr.ru/post/331500/"><em>Manifold learning</em> and <em>latent</em> variables</a> <br></li><li>  Part 3: <a href="https://habrahabr.ru/post/331552/">Variational autoencoders ( <em>VAE</em> )</a> <br></li><li>  Part 4: <a href="https://habrahabr.ru/post/331664/"><em>Conditional VAE</em></a> <br></li><li>  <strong>Part 5: <em>GAN</em> (Generative Adversarial Networks) and tensorflow</strong> <br></li><li>  Part 6: <a href="https://habrahabr.ru/post/332074/"><em>VAE</em> + <em>GAN</em></a> <br></li></ul><br>  (Because of yesterday's bug with perezalitami pictures on habrastoreydzh, which happened through no fault of mine, I was forced to remove this article yesterday immediately after publication. I post it again.) <br><br>  With all the advantages of <strong><em>VAE</em></strong> variational autoencoders, which we dealt with in previous posts, they have one significant drawback: due to the poor way of comparing original and restored objects, the objects they generated are similar to the objects from the training set, but they are easily distinguishable from them (for example blurred). <br><br>  This disadvantage is much less manifested in another approach, namely in <em>generative competing networks</em> - <strong><em>GAN</em></strong> 's. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Formally, <em>GANs</em> , of course, do not belong to autoencoders, however there are similarities between them and variational autoencoders, they will also be useful for the next part.  So it will not be superfluous to meet them too. <br><br><h3>  <em>GAN in</em> brief </h3><br>  <strong><em>GAN</em></strong> 's were first proposed in <strong><em>[1, Generative Adversarial Nets, Goodfellow et al, 2014]</em></strong> and are now being actively studied.  Most state-of-the-art generative models in one way or another use <em>adversarial</em> . <br><br>  <strong><em>GAN</em></strong> scheme: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/faf/0a8/b00/faf0a8b002584baca17e693e877b871b.png"></div><br><br><a name="habracut"></a><br>  <strong><em>GAN</em></strong> 's consist of 2 neural networks: <br><ul><li>  1st ‚Äî generator random samples from some given distribution <img src="https://habrastorage.org/getpro/habr/post_images/a30/620/58e/a3062058ed53bbfca2cd5199c5a84843.svg" alt="P (Z)">  , eg <img src="https://habrastorage.org/getpro/habr/post_images/981/05d/016/98105d016f10d5a71b901afe90ec8fac.svg" alt="N (0, I)">  and generate objects from them <img src="https://habrastorage.org/getpro/habr/post_images/a34/481/c9a/a34481c9af6c87cde61d681ca0f42385.svg" alt="X_p = G (Z; \ theta_g)">  that go to the input of the second network, <br></li><li>  2nd - the discriminator receives objects from the sample as input <img src="https://habrastorage.org/getpro/habr/post_images/e58/d4d/2e2/e58d4d2e228243b9006317b2fac6833e.svg" alt="X_s">  and created by the generator <img src="https://habrastorage.org/getpro/habr/post_images/d86/66a/d9f/d8666ad9faf0084001cff22aee214db1.svg" alt="X_p">  , and learns to predict the probability that a particular object is real, producing a scalar <img src="https://habrastorage.org/getpro/habr/post_images/eba/ca2/803/ebaca28031e6f3669accfe0079acf050.svg" alt="D (X; \ theta_d)">  . <br></li></ul><br>  In this case, the generator trains to create objects that the discriminator does not distinguish from real ones. <br><br>  <strong>Consider the <em>GAN</em> learning process.</strong> <br><br>  The generator and the discriminator are trained separately, but within the same network. <br><br>  Make k discriminator learning steps: per step discriminator learning parameters <img src="https://habrastorage.org/getpro/habr/post_images/1a7/cb7/f4d/1a7cb7f4dbf690e51029385a6290a23b.svg" alt="\ theta_d">  updated to reduce cross-entropy: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/01a/59f/ac5/01a59fac58a6468becf3c9e59a84f42f.svg" alt="\ theta_d = \ theta_d - \ nabla _ {\ theta_d} \ left (\ log (D (X_s)) + \ log (1 - D (G (Z))) \ right)"></div><br>  Next step generator training: update the parameters of the generator <img src="https://habrastorage.org/getpro/habr/post_images/d1b/727/5ee/d1b7275ee7d1f8425c24d9fc851434f8.svg" alt="\ theta_g">  in the direction of increasing the logarithm of the probability of the discriminator to assign the real label to the generated object. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/10b/f9b/579/10bf9b57927876d869b0bb72a59456a5.svg" alt="\ theta_g = \ theta_g + \ nabla _ {\ theta_g} \ log (1 - D (G (Z)))"></div><br>  Training scheme: <br><img src="https://habrastorage.org/web/fc1/8b8/d78/fc18b8d78e5549acbc47cc5d4133bb84.png"><br><br>  In the left picture the discriminator learning step: the gradient (red arrows) flows from the loss only to the discriminator, where they are updated <img src="https://habrastorage.org/getpro/habr/post_images/1a7/cb7/f4d/1a7cb7f4dbf690e51029385a6290a23b.svg" alt="\ theta_d">  (green) in the direction of reducing the loss.  In the right picture, the gradient from the right part of the loss (identification generated object identification error) flows to the generator, and only the generator weights are updated. <img src="https://habrastorage.org/getpro/habr/post_images/d1b/727/5ee/d1b7275ee7d1f8425c24d9fc851434f8.svg" alt="\ theta_g">  (green) in the direction of <strong>increasing the</strong> probability of the discriminator to be mistaken. <br><br>  The task that the <em>GAN</em> solves is formulated as follows: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7d9/04b/209/7d904b209f780191b4772cf61a59fa44.svg" alt="\ min_G \ max_D \ mathbb {E} _ {X \ sim P} [\ log (D (X))] + \ mathbb {E} _ {Z \ sim P_z} [\ log (1 - D (G (Z )))]"></div><br><p>  For a given generator, the optimal discriminator gives the probability <img src="https://habrastorage.org/getpro/habr/post_images/325/cd6/3ba/325cd63ba269ed12ef61a8592d917c61.svg" alt="D ^ * (X) = \ frac {P (X)} {P_g (X) + P (X)},">  which is almost obvious, I suggest thinking about it for a second. </p><br><br>  In <strong><em>[1] it is</em></strong> shown that with sufficient power of both networks, this task has an optimum, in which the generator learned to generate the distribution <img src="https://habrastorage.org/getpro/habr/post_images/10e/85c/f7c/10e85cf7c9acd729f42a736b83e443bb.svg" alt="P_g (X)">  matching with <img src="https://habrastorage.org/getpro/habr/post_images/8e5/d52/dea/8e5d52dea71bd2983ce35b05f42587a7.svg" alt="P (X)">  , and everywhere on <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="X ^ {}">  discriminator gives probability <img src="https://habrastorage.org/getpro/habr/post_images/8b2/7da/395/8b27da3950a06d3b5f6f5ab6d54d136a.svg" alt="1/2">  . <br><br><img src="https://habrastorage.org/web/a82/d56/836/a82d56836dd446c899d9ed9018ef7bbe.png"><br><br>  Illustration of <strong><em>[1]</em></strong> <br><br>  Legend: <br><br><ul><li>  black dotted curve - true distribution <img src="https://habrastorage.org/getpro/habr/post_images/8e5/d52/dea/8e5d52dea71bd2983ce35b05f42587a7.svg" alt="P (X)">  , <br></li><li>  green - generator distribution <img src="https://habrastorage.org/getpro/habr/post_images/10e/85c/f7c/10e85cf7c9acd729f42a736b83e443bb.svg" alt="P_g (X)">  , <br></li><li>  blue - probability distribution <img src="https://habrastorage.org/getpro/habr/post_images/eba/ca2/803/ebaca28031e6f3669accfe0079acf050.svg" alt="D (X; \ theta_d)">  discriminator predict the class of a real object, <br></li><li>  the lower and upper straight lines are the set of all <img src="https://habrastorage.org/getpro/habr/post_images/d2d/297/e80/d2d297e8073685ab6fb84a0fb938ba3c.svg" alt="Z">  and a lot of all <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="X">  , arrows represent mapping <img src="https://habrastorage.org/getpro/habr/post_images/7b8/2d3/870/7b82d3870f559547209b7f870266ab78.svg" alt="G (Z; \ theta_g)">  . <br></li></ul><br>  On the picture: <br><br><ul><li>  (a) <img src="https://habrastorage.org/getpro/habr/post_images/8e5/d52/dea/8e5d52dea71bd2983ce35b05f42587a7.svg" alt="P (X)">  and <img src="https://habrastorage.org/getpro/habr/post_images/10e/85c/f7c/10e85cf7c9acd729f42a736b83e443bb.svg" alt="P_g (X)">  quite different, but the discriminator uncertainly distinguishes one from the other, <br></li><li>  (b) the discriminator after k learning steps already distinguishes them more confidently, <br></li><li>  (c) it allows the generator <img src="https://habrastorage.org/getpro/habr/post_images/97e/e51/358/97ee51358fb3717db21d7afd2ea8ead7.svg" alt="G">  guided by a good discriminator gradient <img src="https://habrastorage.org/getpro/habr/post_images/382/bca/d6d/382bcad6d8d09bcc3223582fd8b28a78.svg" alt="D">  , on the border of two distributions move <img src="https://habrastorage.org/getpro/habr/post_images/10e/85c/f7c/10e85cf7c9acd729f42a736b83e443bb.svg" alt="P_g (X)">  closer to <img src="https://habrastorage.org/getpro/habr/post_images/8e5/d52/dea/8e5d52dea71bd2983ce35b05f42587a7.svg" alt="P (X)">  , <br></li><li>  (d) as a result of many repetitions of steps (a), (b), (c) <img src="https://habrastorage.org/getpro/habr/post_images/e45/65c/97d/e4565c97d32af21cfc41c1d61d105a4a.svg" alt="P_g">  coincided with <img src="https://habrastorage.org/getpro/habr/post_images/d3a/eb7/444/d3aeb74440becc6f0f689b98c40429cb.svg" alt="P">  and the discriminator is no longer able to distinguish one from the other: <img src="https://habrastorage.org/getpro/habr/post_images/4a7/a3c/419/4a7a3c41991a1a20c2e8e6a2928b49fe.svg" alt="D (X) = 1/2">  .  Point optimum reached. <br></li></ul><br><h2>  Conditional gan </h2><br>  Just like in the last part we made <em>Conditional VAE</em> , simply passing the numbers to the encoder and decoder label, here we will transfer it to the generator and the discriminator <strong><em>[2]</em></strong> <br><br><img src="https://habrastorage.org/web/dc8/b42/8f5/dc8b428f5c8d45a9abba9262340f166a.png"><br><br><h2>  Code </h2><br>  Unlike the previous parts, where it was possible to manage with <em>just</em> one <em>keras</em> , there is a problem with this.  Namely, it is necessary to update in turn one or only parameters of the generator, or only the discriminator in the same network.  If you fake it, you can do it cleanly in <em>keras</em> , but for me it is easier and more useful to connect here and <em>tensorflow</em> . <br>  The <em>keras</em> blog has a small <a href="https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html">tutorial</a> <strong>[3]</strong> on how to do this. <br><br>  The benefit of <em>keras is</em> easily combined with <em>tensorflow</em> - not for nothing, he got into <em>tensorflow.contrib</em> . <br><br>  Let's start by importing the necessary modules and loading the dataset. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> IPython.display <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> clear_output <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dropout, BatchNormalization, Reshape, Flatten, RepeatVector <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Lambda, Dense, Input, Conv2D, MaxPool2D, UpSampling2D, concatenate <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.advanced_activations <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LeakyReLU <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model, load_model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.datasets <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mnist <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> to_categorical (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.astype(<span class="hljs-string"><span class="hljs-string">'float32'</span></span>) / <span class="hljs-number"><span class="hljs-number">255.</span></span> x_test = x_test .astype(<span class="hljs-string"><span class="hljs-string">'float32'</span></span>) / <span class="hljs-number"><span class="hljs-number">255.</span></span> x_train = np.reshape(x_train, (len(x_train), <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) x_test = np.reshape(x_test, (len(x_test), <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) y_train_cat = to_categorical(y_train).astype(np.float32) y_test_cat = to_categorical(y_test).astype(np.float32)</code> </pre> <br>  To work in <em>keras</em> and <em>tensorflow,</em> you need to simultaneously register a <em>tensorflow</em> session in <em>keras</em> , it is necessary for <em>keras to</em> create all internal variables within the session used. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> backend <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> K <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf sess = tf.Session() K.set_session(sess)</code> </pre><br>  Define the main global constants: <br><br><pre> <code class="python hljs">batch_size = <span class="hljs-number"><span class="hljs-number">256</span></span> batch_shape = (batch_size, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) latent_dim = <span class="hljs-number"><span class="hljs-number">2</span></span> num_classes = <span class="hljs-number"><span class="hljs-number">10</span></span> dropout_rate = <span class="hljs-number"><span class="hljs-number">0.3</span></span></code> </pre><br>  Now we will not train the model using the <em>.fit</em> method, but directly from <em>tensorflow</em> , so we will write an iterator that returns the next batch: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">gen_batch</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x, y)</span></span></span><span class="hljs-function">:</span></span> n_batches = x.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] // batch_size <span class="hljs-keyword"><span class="hljs-keyword">while</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_batches): <span class="hljs-keyword"><span class="hljs-keyword">yield</span></span> x[batch_size*i: batch_size*(i+<span class="hljs-number"><span class="hljs-number">1</span></span>)], y[batch_size*i: batch_size*(i+<span class="hljs-number"><span class="hljs-number">1</span></span>)] idxs = np.random.permutation(y.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]) x = x[idxs] y = y[idxs] train_batches_it = gen_batch(x_train, y_train_cat) test_batches_it = gen_batch(x_test, y_test_cat)</code> </pre><br>  Wrap <em>placeholder</em> 's for images, labels, and hidden variables in the incoming layers for <em>keras</em> models: <br><br><pre> <code class="python hljs">x_ = tf.placeholder(tf.float32, shape=(<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), name=<span class="hljs-string"><span class="hljs-string">'image'</span></span>) y_ = tf.placeholder(tf.float32, shape=(<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, num_classes), name=<span class="hljs-string"><span class="hljs-string">'labels'</span></span>) z_ = tf.placeholder(tf.float32, shape=(<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, latent_dim), name=<span class="hljs-string"><span class="hljs-string">'z'</span></span>) img = Input(tensor=x_) lbl = Input(tensor=y_) z = Input(tensor=z_)</code> </pre><br>  We will implement <strong><em>CGAN</em></strong> immediately, as it is only minimally different from the usual. <br>  We write a model of the generator.  <em>Keras</em> works with <em>scope</em> , and we need to separate the generator and the discriminator in order to train them separately <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.variable_scope(<span class="hljs-string"><span class="hljs-string">'generator'</span></span>): x = concatenate([z, lbl]) x = Dense(<span class="hljs-number"><span class="hljs-number">7</span></span>*<span class="hljs-number"><span class="hljs-number">7</span></span>*<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = Dropout(dropout_rate)(x) x = Reshape((<span class="hljs-number"><span class="hljs-number">7</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>, <span class="hljs-number"><span class="hljs-number">64</span></span>))(x) x = UpSampling2D(size=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>))(x) x = Conv2D(<span class="hljs-number"><span class="hljs-number">64</span></span>, kernel_size=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = Dropout(dropout_rate)(x) x = Conv2D(<span class="hljs-number"><span class="hljs-number">32</span></span>, kernel_size=(<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = Dropout(dropout_rate)(x) x = UpSampling2D(size=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>))(x) generated = Conv2D(<span class="hljs-number"><span class="hljs-number">1</span></span>, kernel_size=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) generator = Model([z, lbl], generated, name=<span class="hljs-string"><span class="hljs-string">'generator'</span></span>)</code> </pre><br>  Further, the discriminator model.  Here we need to add another label to the incoming image.  To do this, after applying the first convolutional layer, add labels to the filters.  First, the function that does it, then the discriminator model. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">add_units_to_conv2d</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(conv2, units)</span></span></span><span class="hljs-function">:</span></span> dim1 = int(conv2.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>]) dim2 = int(conv2.shape[<span class="hljs-number"><span class="hljs-number">2</span></span>]) dimc = int(units.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>]) repeat_n = dim1*dim2 units_repeat = RepeatVector(repeat_n)(lbl) units_repeat = Reshape((dim1, dim2, dimc))(units_repeat) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> concatenate([conv2, units_repeat]) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.variable_scope(<span class="hljs-string"><span class="hljs-string">'discrim'</span></span>): x = Conv2D(<span class="hljs-number"><span class="hljs-number">128</span></span>, kernel_size=(<span class="hljs-number"><span class="hljs-number">7</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>), strides=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(img) x = add_units_to_conv2d(x, lbl) x = LeakyReLU()(x) x = Dropout(dropout_rate)(x) x = MaxPool2D((<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) l = Conv2D(<span class="hljs-number"><span class="hljs-number">128</span></span>, kernel_size=(<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = LeakyReLU()(l) x = Dropout(dropout_rate)(x) h = Flatten()(x) d = Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)(h) discrim = Model([img, lbl], d, name=<span class="hljs-string"><span class="hljs-string">'Discriminator'</span></span>)</code> </pre><br>  Having defined models, we can apply them directly to place <em>holders</em> as ordinary <em>tensorflow</em> operations. <br><br><pre> <code class="python hljs">generated_z = generator([z, lbl]) discr_img = discrim([img, lbl]) discr_gen_z = discrim([generated_z, lbl]) gan_model = Model([z, lbl], discr_gen_z, name=<span class="hljs-string"><span class="hljs-string">'GAN'</span></span>) gan = gan_model([z, lbl])</code> </pre><br>  Now the loss is the error of determining the real image, and the loss generated by, as well as on the basis of, the losses of the generator and discriminator. <br><br><pre> <code class="python hljs">log_dis_img = tf.reduce_mean(-tf.log(discr_img + <span class="hljs-number"><span class="hljs-number">1e-10</span></span>)) log_dis_gen_z = tf.reduce_mean(-tf.log(<span class="hljs-number"><span class="hljs-number">1.</span></span> - discr_gen_z + <span class="hljs-number"><span class="hljs-number">1e-10</span></span>)) L_gen = -log_dis_gen_z L_dis = <span class="hljs-number"><span class="hljs-number">0.5</span></span>*(log_dis_gen_z + log_dis_img)</code> </pre><br>  Usually in <em>tensorflow</em> , passing to the optimizer a loss, it will try to minimize all the variables on which it depends.  We do not need this now: when training a generator, the error should not touch the discriminator, although it should flow through it and vice versa. <br><br>  To do this, in addition to the optimizer, you must pass a list of variables that it will optimize.  We will get these variables from the required <em>scope</em> using <em>tf.get_collection</em> <br><br><pre> <code class="python hljs">optimizer_gen = tf.train.RMSPropOptimizer(<span class="hljs-number"><span class="hljs-number">0.0003</span></span>) optimizer_dis = tf.train.RMSPropOptimizer(<span class="hljs-number"><span class="hljs-number">0.0001</span></span>) <span class="hljs-comment"><span class="hljs-comment">#     ()   generator_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, "generator") discrim_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, "discrim") step_gen = optimizer_gen.minimize(L_gen, var_list=generator_vars) step_dis = optimizer_dis.minimize(L_dis, var_list=discrim_vars)</span></span></code> </pre><br>  Initialize the variables: <br><br><pre> <code class="python hljs">sess.run(tf.global_variables_initializer())</code> </pre><br>  Separately, we will write the functions that we will call to train the generator and the discriminator: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    def step(image, label, zp): l_dis, _ = sess.run([L_dis, step_gen], feed_dict={z:zp, lbl:label, img:image, K.learning_phase():1}) return l_dis #    def step_d(image, label, zp): l_dis, _ = sess.run([L_dis, step_dis], feed_dict={z:zp, lbl:label, img:image, K.learning_phase():1}) return l_dis</span></span></code> </pre><br>  Code saving and visualization of images: <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># ,     ,    figs = [[] for x in range(num_classes)] periods = [] save_periods = list(range(100)) + list(range(100, 1000, 10)) n = 15 #   15x15  from scipy.stats import norm #     N(0, I),   ,    ,      grid_x = norm.ppf(np.linspace(0.05, 0.95, n)) grid_y = norm.ppf(np.linspace(0.05, 0.95, n)) grid_y = norm.ppf(np.linspace(0.05, 0.95, n)) def draw_manifold(label, show=True): #     figure = np.zeros((28 * n, 28 * n)) input_lbl = np.zeros((1, 10)) input_lbl[0, label] = 1. for i, yi in enumerate(grid_x): for j, xi in enumerate(grid_y): z_sample = np.zeros((1, latent_dim)) z_sample[:, :2] = np.array([[xi, yi]]) x_generated = sess.run(generated_z, feed_dict={z:z_sample, lbl:input_lbl, K.learning_phase():0}) digit = x_generated[0].squeeze() figure[i * 28: (i + 1) * 28, j * 28: (j + 1) * 28] = digit if show: #  plt.figure(figsize=(10, 10)) plt.imshow(figure, cmap='Greys') plt.grid(False) ax = plt.gca() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) plt.show() return figure n_compare = 10 def on_n_period(period): clear_output() #   output #     y draw_lbl = np.random.randint(0, num_classes) print(draw_lbl) for label in range(num_classes): figs[label].append(draw_manifold(label, show=label==draw_lbl)) periods.append(period)</span></span></code> </pre><br></div></div><br>  Now we will <em>train</em> our <em>CGAN</em> . <br><br>  It is important that at the very beginning the discriminator does not begin to win too much, otherwise the learning will stop.  Therefore, internal cycles are added here both for the discriminator and for the generator, and the output from them, when one network almost catches up with another. <br><br>  If the discriminator immediately wins the decoder, and the training does not even have time to start, then you can try to slow down the discriminator‚Äôs training or start anew several times. <br><br><pre> <code class="python hljs">batches_per_period = <span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-comment"><span class="hljs-comment">#     k_step = 5 #  ,          for i in range(5000): print('.', end='') #    b0, b1 = next(train_batches_it) zp = np.random.randn(batch_size, latent_dim) #    for j in range(k_step): l_d = step_d(b0, b1, zp) b0, b1 = next(train_batches_it) zp = np.random.randn(batch_size, latent_dim) if l_d &lt; 1.0: break #    for j in range(k_step): l_d = step(b0, b1, zp) if l_d &gt; 0.4: break b0, b1 = next(train_batches_it) zp = np.random.randn(batch_size, latent_dim) #    if not i % batches_per_period: period = i // batches_per_period if period in save_periods: on_n_period(period) print(l_d)</span></span></code> </pre><br><br>  Gif drawing code: <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> matplotlib.animation <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> FuncAnimation <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">make_2d_figs_gif</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(figs, periods, c, fname, fig, batches_per_period)</span></span></span><span class="hljs-function">:</span></span> norm = matplotlib.colors.Normalize(vmin=<span class="hljs-number"><span class="hljs-number">0</span></span>, vmax=<span class="hljs-number"><span class="hljs-number">1</span></span>, clip=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) im = plt.imshow(np.zeros((<span class="hljs-number"><span class="hljs-number">28</span></span>,<span class="hljs-number"><span class="hljs-number">28</span></span>)), cmap=<span class="hljs-string"><span class="hljs-string">'Greys'</span></span>, norm=norm) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">"Label: {}\nBatch: {}"</span></span>.format(c, <span class="hljs-number"><span class="hljs-number">0</span></span>)) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">update</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(i)</span></span></span><span class="hljs-function">:</span></span> im.set_array(figs[i]) im.axes.set_title(<span class="hljs-string"><span class="hljs-string">"Label: {}\nBatch: {}"</span></span>.format(c, periods[i]*batches_per_period)) im.axes.get_xaxis().set_visible(<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) im.axes.get_yaxis().set_visible(<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> im anim = FuncAnimation(fig, update, frames=range(len(figs)), interval=<span class="hljs-number"><span class="hljs-number">100</span></span>) anim.save(fname, dpi=<span class="hljs-number"><span class="hljs-number">80</span></span>, writer=<span class="hljs-string"><span class="hljs-string">'imagemagick'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> label <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(num_classes): make_2d_figs_gif(figs[label], periods, label, <span class="hljs-string"><span class="hljs-string">"./figs4_5/manifold_{}.gif"</span></span>.format(label), plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>,<span class="hljs-number"><span class="hljs-number">10</span></span>)), batches_per_period)</code> </pre><br></div></div><br><h2>  Results: </h2><br><h3>  Gan </h3><br>  Variety digits for regular <em>GAN</em> (without label transfer) <br> <a href=""><img src="https://habrastorage.org/web/75e/902/348/75e9023483d843f6a51e3ae4b5c26adf.png" width="600"></a> <br><br>  It is worth noting that the numbers are better than in <em>VAE</em> (without labels) <br><br><div class="spoiler">  <b class="spoiler_title">Learning gif</b> <div class="spoiler_text"> <a href=""><img src="https://habrastorage.org/web/cc9/d27/62b/cc9d2762b11c42d58d4bf769f73d773b.gif" width="600"></a> <br></div></div><br><h3>  CGAN </h3><br>  Varieties of numbers for each label <br><br> <a href=""><img src="https://habrastorage.org/web/40b/73e/d63/40b73ed635c242219701ba72e084b916.png" width="350"></a> <a href=""><img src="https://habrastorage.org/web/a95/94b/d76/a9594bd765ec4fe3ae3da7c072b675fb.png" width="350"></a> <a href=""><img src="https://habrastorage.org/web/f0f/551/fd3/f0f551fd375d4f50ad3fbb3b14d9da5d.png" width="350"></a> <a href=""><img src="https://habrastorage.org/web/d00/5b3/dd0/d005b3dd0dbf4f71afbc597f0d1086bb.png" width="350"></a> <a href=""><img src="https://habrastorage.org/web/a39/239/ef3/a39239ef3cfc4a26817e8c342d1254fe.png" width="350"></a> <a href=""><img src="https://habrastorage.org/web/517/2e1/940/5172e1940f9546c5b07c736fb9e805b9.png" width="350"></a> <a href=""><img src="https://habrastorage.org/web/325/6cc/90f/3256cc90fab148b485df821f500e3917.png" width="350"></a> <a href=""><img src="https://habrastorage.org/web/3dd/d82/71c/3ddd8271c57d40d7a23325f5bd173794.png" width="350"></a> <a href=""><img src="https://habrastorage.org/web/72a/058/f54/72a058f54ba9405b8d21622819eced43.png" width="350"></a> <a href=""><img src="https://habrastorage.org/web/ec3/aa0/db8/ec3aa0db82fe49c5b351c9086c409168.png" width="350"></a> <br><br><div class="spoiler">  <b class="spoiler_title">Heavy gifs</b> <div class="spoiler_text"> <a href=""><img src="https://habrastorage.org/web/031/e8a/bb6/031e8abb6f914ea99523a2c32499fd3e.gif" width="330"></a> <a href=""><img src="https://habrastorage.org/web/3f3/88a/c73/3f388ac7387349409bc75e094171fd96.gif" width="330"></a> <br> <a href=""><img src="https://habrastorage.org/web/f1b/d04/4aa/f1bd044aa50e4156aeaf592aad858816.gif" width="330"></a> <a href=""><img src="https://habrastorage.org/web/813/ce5/a65/813ce5a659c544658c2eb596ad3fb16b.gif" width="330"></a> <a href=""><img src="https://habrastorage.org/web/a3b/35a/fbd/a3b35afbd2c44664b4e20e43aba02718.gif" width="330"></a> <a href=""><img src="https://habrastorage.org/web/b70/794/6f5/b707946f5d3445698486ec19396f554b.gif" width="330"></a> <a href=""><img src="https://habrastorage.org/web/da0/f00/45f/da0f0045f8204a11bc9e16618e83ed77.gif" width="330"></a> <a href=""><img src="https://habrastorage.org/web/a79/aaa/6b7/a79aaa6b7a564dbf929c362992a79004.gif" width="330"></a> <a href=""><img src="https://habrastorage.org/web/c5b/125/d83/c5b125d83fcf44a5a0cad598e748d4d7.gif" width="330"></a> <a href=""><img src="https://habrastorage.org/web/9cb/e1b/d21/9cbe1bd2125b44b3a611160e5ebcae21.gif" width="330"></a> <br></div></div><br><h2>  Useful links and literature </h2><br>  Original article: <br>  [1] Generative Adversarial Nets, Goodfellow et al, 2014, <a href="">https://arxiv.org/abs/1406.2661</a> <br><br>  Conditional GANs: <br>  [2] Conditional Generative Adversarial Nets, Mirza, Osindero, 2014, <a href="">https://arxiv.org/abs/1411.1784</a> <br><br>  Tutorial about using <em>keras</em> with <em>tensorflow</em> : <br>  [3] <a href="https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html">https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html</a> </div><p>Source: <a href="https://habr.com/ru/post/332000/">https://habr.com/ru/post/332000/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../331990/index.html">Tale about NePetu, or rather not about Petya</a></li>
<li><a href="../331992/index.html">‚ÄúIceberg instead of Oscar!‚Äù Or as I tried to learn the basics of DataScience on kaggle</a></li>
<li><a href="../331994/index.html">Ruining people not beer</a></li>
<li><a href="../331996/index.html">Rasking through the parts of a particularly hardy iron bullion S line, where 768 GB of RAM</a></li>
<li><a href="../331998/index.html">Using Python and Excel to process and analyze data. Part 2: libraries for working with data</a></li>
<li><a href="../332004/index.html">Savings on matches or data recovery from the grating HDD Seagate ST3000NC002-1DY166</a></li>
<li><a href="../332006/index.html">"Factory Method" in the development for Android. The best way to handle pushes</a></li>
<li><a href="../332008/index.html">Retrospective. 10 years of Yota and 10 interesting facts about the company</a></li>
<li><a href="../332012/index.html">NetApp HCI is the new generation hyperconvergent system for working with data</a></li>
<li><a href="../332016/index.html">‚ÄúIt is important to prioritize‚Äù: about testing at Sberbank Technologies</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>