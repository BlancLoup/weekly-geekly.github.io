<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How and why we optimized the algorithm of cleaning SLAB-caches in the Linux kernel</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The growing popularity of containers and their use in conjunction with control groups revealed a serious scalability problem, which leads to a signifi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How and why we optimized the algorithm of cleaning SLAB-caches in the Linux kernel</h1><div class="post__text post__text-html js-mediator-article">  The growing popularity of containers and their use in conjunction with control groups revealed a serious scalability problem, which leads to a significant drop in performance on large machines.  The problem is that the bypass time of SLAB caches depends quadratically on the number of containers, and the active consumption of large amounts of memory in a short period can cause the system to leave in a busy loop, consuming 100% of the processor time.  Today I would like to tell you how we solved this problem by changing the algorithm of accounting for using the SLAB-cache objects by the control group memcg and optimizing the function shrink_slab (). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/99b/7f3/83b/99b7f383beadeec2c814a6792e507b6c.jpg" alt="Memory clear"><br><a name="habracut"></a><br>  Why in general there was a question of optimization of processes in a kernel?  It all started with the fact that one of our customers, actively using containers and memory control groups (memcg), drew attention to the strange peaks of processor resource consumption that occur from time to time.  Normal system loading was about 50%, and at peak times 100% of the processor time was occupied, and almost all of it was consumed by the kernel (sys time). <br>  The node itself was multi-user, and about 200 OpenVZ containers were launched on it.  The analysis showed that a large number of users created nested Docker containers and multi-level hierarchies of control memory groups.  Each top-level user container contained about 20 mount points and 20 memory control groups (memcg) created by systemd.  In addition, there were mount points and control groups created by the above mentioned Docker.  Simply put, the node was heavily loaded, and the load on it was much stronger than the average for all our other customers.  We were interested in finding the reason for the appearance of these peaks, since the same problem could also appear on less loaded machines, where it was barely noticeable (for example, give peaks at + 5% sys time, which degrade performance). <br><br>  By manipulating the perf, I managed to catch the peak and remove the trace.  It turned out that most of the CPU time is spent on clearing SLAB caches, namely, superblock caches: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <pre><code class="markdown hljs"><span class="hljs-bullet"><span class="hljs-bullet">- </span></span>100,00% 0,00% kswapd0 [kernel.vmlinux] [k] kthread - 99,31% balance<span class="hljs-emphasis"><span class="hljs-emphasis">_pgdat - 82,11% shrink_</span></span>zone - 61,69% shrink<span class="hljs-emphasis"><span class="hljs-emphasis">_slab - 58,29% super_</span></span>cache<span class="hljs-emphasis"><span class="hljs-emphasis">_count + 54,56% list_</span></span>lru<span class="hljs-emphasis"><span class="hljs-emphasis">_count_</span></span>one</code> </pre> <br><br>  Here it is worth making an explanation and dwell on this issue.  Everyone knows that the kernel caches unused data for a while before finally releasing the memory.  The kernel makes extensive use of this principle.  For example, the page cache contains pages of data related to the file, which significantly speeds up re-access to them when reading (because it does not need to re-access the disk).  In our case, the problem arose with the superblock metadata cache contained in two LRU lists: s_dentry_lru and s_inode_lru. <br><br>  <b>LRU (Least Recently Used)</b> <b><br></b> <br>  struct lru_list points to an array of linked lists, and each active memcg corresponds to one element (list_lru_one) in this array.  When a certain SLAB object ceases to be used by the kernel, the kernel adds it to one of the connected lists of the array (depending on which memcg object it belongs to, or, roughly speaking, which memcg process belongs to when it created this object).  The array itself is described as follows (lru_list :: node :: memcg_lrus): <br><br><pre> <code class="cpp hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">struct</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">list_lru_memcg</span></span></span><span class="hljs-class"> {</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">struct</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">rcu_head</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">rcu</span></span></span><span class="hljs-class">;</span></span> <span class="hljs-comment"><span class="hljs-comment">/* array of per cgroup lists, indexed by memcg_cache_id */</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">struct</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">list_lru_one</span></span></span><span class="hljs-class"> *</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">lru</span></span></span><span class="hljs-class">[0];</span></span> <span class="hljs-comment"><span class="hljs-comment">/*    */</span></span> }; <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">struct</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">list_lru_one</span></span></span><span class="hljs-class"> {</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">struct</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">list_head</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">list</span></span></span><span class="hljs-class">;</span></span> <span class="hljs-comment"><span class="hljs-comment">/*    */</span></span> <span class="hljs-comment"><span class="hljs-comment">/* may become negative during memcg reparenting */</span></span> <span class="hljs-keyword"><span class="hljs-keyword">long</span></span> nr_items; <span class="hljs-comment"><span class="hljs-comment">/*     */</span></span> };</code> </pre><br>  lru [0] specifies a list of memcg related objects with ID 0; <br>  lru [1] specifies a list of memcg related objects with ID 1; <br>  ... <br>  lru [n] specifies a list of objects related to memcg with ID n; <br><br>  Our problem is LRU s_dentry_lru and s_inode_lru lists, and as the name suggests, they contain unused dentry objects and the inode of the file system. <br>  Later, when there is a shortage of memory in the system or a specific memcg, some of the list elements are finally released, and a special mechanism called the shrinker deals with this. <br><br>  <b>Shrinker</b> <b><br></b> <br>  When the kernel needs to allocate pages of memory, but there is no free memory on the NUMA node or in the system, a mechanism is started to clear it.  He is trying to discard or flush a number: 1) pages of file contents from page cache;  2) pages related to anonymous memory in a swap, and 3) cached objects of SLAB (this is the problem with which we are confronted). <br><br>  Dropping part of cached SLAB objects affects the release of pages indirectly: their size is usually much smaller than the page size, and there are hundreds of objects on one page.  When some objects are released, free memory gaps appear in the SLAB pages, which can be used to create other SLAB objects.  This algorithm is adopted in the core intent: it is simple and quite effective.  The interested reader can see the formula for selecting a part of objects for cleaning in the do_shrink_slab () function. <br><br>  This function performs the actual cleaning of part of objects, guided by the description passed to it in the struct shrinker: <br><br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">static</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">unsigned</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">long</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">do_shrink_slab</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(struct shrink_control *shrinkctl, struct shrinker *shrinker, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> priority)</span></span></span><span class="hljs-function"> </span></span>{ ‚Ä¶ <span class="hljs-comment"><span class="hljs-comment">/*   */</span></span> freeable = shrinker-&gt;count_objects(shrinker, shrinkctl); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (freeable == <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span>; total_scan = _(freeable); <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> (total_scan &gt;= batch_size) { <span class="hljs-comment"><span class="hljs-comment">/*   */</span></span> ret = shrinker-&gt;scan_objects(shrinker, shrinkctl); total_scan -= shrinkctl-&gt;nr_scanned; } ... }</code> </pre> <br>  Applied to the shrinker superblock, these functions are implemented as follows.  Each superblock maintains its own s_dentry_lru and s_inode_lru lists of unused objects related to it: <br><br><pre> <code class="cpp hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">struct</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">super_block</span></span></span><span class="hljs-class"> {</span></span> ... <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">struct</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">shrinker</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">s_shrink</span></span></span><span class="hljs-class">;</span></span> <span class="hljs-comment"><span class="hljs-comment">/* per-sb shrinker handle */</span></span> ... <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">struct</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">list_lru</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">s_dentry_lru</span></span></span><span class="hljs-class">;</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">struct</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">list_lru</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">s_inode_lru</span></span></span><span class="hljs-class">;</span></span> ‚Ä¶ };</code> </pre> <br><br>  The .count_objects method returns the number of objects: <br><br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">static</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">unsigned</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">long</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">super_cache_count</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(struct shrinker *shrink, struct shrink_control *sc)</span></span></span><span class="hljs-function"> </span></span>{ total_objects += list_lru_shrink_count(&amp;sb-&gt;s_dentry_lru, sc); total_objects += list_lru_shrink_count(&amp;sb-&gt;s_inode_lru, sc); <span class="hljs-comment"><span class="hljs-comment">/*     ) */</span></span> total_objects = vfs_pressure_ratio(total_objects); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> total_objects; }</code> </pre> <br><br>  The .scan_objects method actually releases objects: <br><br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">static</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">unsigned</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">long</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">super_cache_scan</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(struct shrinker *shrink, struct shrink_control *sc)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">/*     s_dentry_lru */</span></span> prune_dcache_sb(sb, sc); <span class="hljs-comment"><span class="hljs-comment">/*     s_inode_lru */</span></span> prune_icache_sb(sb, sc); }</code> </pre> <br>  The number of objects to be freed is passed in the sc parameter.  Also, there is a memcg, the objects of which must be dropped from the LRU: <br><br><pre> <b><code class="cpp hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">struct</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">shrink_control</span></span></span><span class="hljs-class"> {</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> nid; <span class="hljs-comment"><span class="hljs-comment">/* ID NUMA  */</span></span> <span class="hljs-keyword"><span class="hljs-keyword">unsigned</span></span> <span class="hljs-keyword"><span class="hljs-keyword">long</span></span> nr_to_scan; <span class="hljs-comment"><span class="hljs-comment">/*   */</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">struct</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">mem_cgroup</span></span></span><span class="hljs-class"> *</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">memcg</span></span></span><span class="hljs-class">;</span></span> <span class="hljs-comment"><span class="hljs-comment">/* memcg */</span></span> };</code></b> </pre><br>  Thus, prune_dcache_sb () selects a linked list from the array struct list_lru_memcg :: lru [] and works with it.  Prune_icache_sb () does the same. <br><br>  <b>Old shrinker traversal algorithm</b> <b><br></b> <br>  With the standard approach, ‚Äúdropping out‚Äù objects from SLAB when there is not enough memory in <br>  sc-&gt; target_mem_cgroup is as follows: <br><br><pre> <code class="cpp hljs">shrink_node() { ‚Ä¶ <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">struct</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">mem_cgroup</span></span></span><span class="hljs-class"> *</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">root</span></span></span><span class="hljs-class"> = </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">sc</span></span></span><span class="hljs-class">-&gt;</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">target_mem_cgroup</span></span></span><span class="hljs-class">;</span></span> <span class="hljs-comment"><span class="hljs-comment">/*      sc-&gt;target_mem_cgroup  */</span></span> memcg = mem_cgroup_iter(root, <span class="hljs-literal"><span class="hljs-literal">NULL</span></span>, &amp;reclaim); <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> { ‚Ä¶ shrink_slab(memcg, ...); ‚Ä¶ } <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> ((memcg = mem_cgroup_iter(root, memcg, &amp;reclaim))); ... }</code> </pre><br>  Pass through all the child memcg and call shrink_slab () for each of them.  Next, in the shrink_slab () function we go through all the shrinkers and for each of them call do_shrink_slab (): <br><br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">static</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">unsigned</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">long</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">shrink_slab</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">gfp_t</span></span></span></span><span class="hljs-function"><span class="hljs-params"> gfp_mask, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> nid, struct mem_cgroup *memcg, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> priority)</span></span></span><span class="hljs-function"> </span></span>{ list_for_each_entry(shrinker, &amp;shrinker_list, <span class="hljs-built_in"><span class="hljs-built_in">list</span></span>) { <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">struct</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">shrink_control</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">sc</span></span></span><span class="hljs-class"> = {</span></span> .nid = nid, .memcg = memcg, }; ret = do_shrink_slab(&amp;sc, shrinker, ...); } }</code> </pre><br>  Recall that for each superblock a shrinker is added to this list.  Calculate how many times do_shrink_slab () will be called for a case with 200 containers of 20 memcg and 20 mount points in each.  In total, we have 200 * 20 mount points and 200 * 20 control groups.  If there is a shortage of memory in the uppermost memcg, we will be forced to bypass all its child memcg (i.e., in general, all), and for each of them call each shrinker from the list of shrinker_list.  Thus, the kernel will make 200 * 20 * 200 * 20 = 16000000 calls to the do_shrink_slab () function. <br><br>  At the same time, the overwhelming number of calls to this function will be useless: containers are usually isolated from each other, and the probability that the CT1 container will use super_block2, created in CT2, is generally low.  Or, the same thing, if memcg1 is a control group from CT1, then the corresponding element of the array super_block2-&gt; s_dentry_lru-&gt; node-&gt; memcg_lrus-&gt; lru [memcg1_id] is an empty list, and there is no point in calling do_shrink_slab () for it. <br><br>  This problem can be modeled using a simple bash script (here we use data from the patchset, which was later transferred to the kernel): <br><pre> <code class="bash hljs"><span class="hljs-variable"><span class="hljs-variable">$echo</span></span> 1 &gt; /sys/fs/cgroup/memory/memory.use_hierarchy <span class="hljs-variable"><span class="hljs-variable">$mkdir</span></span> /sys/fs/cgroup/memory/ct <span class="hljs-variable"><span class="hljs-variable">$echo</span></span> 4000M &gt; /sys/fs/cgroup/memory/ct/memory.kmem.limit_in_bytes <span class="hljs-variable"><span class="hljs-variable">$for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> `seq 0 4000`; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> mkdir /sys/fs/cgroup/memory/ct/<span class="hljs-variable"><span class="hljs-variable">$i</span></span>; <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> $$ &gt; /sys/fs/cgroup/memory/ct/<span class="hljs-variable"><span class="hljs-variable">$i</span></span>/cgroup.procs; mkdir -ps/<span class="hljs-variable"><span class="hljs-variable">$i</span></span>; mount -t tmpfs <span class="hljs-variable"><span class="hljs-variable">$is</span></span>/<span class="hljs-variable"><span class="hljs-variable">$i</span></span>; touch s/<span class="hljs-variable"><span class="hljs-variable">$i</span></span>/file; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span></code> </pre><br>  Let's see what happens if we call the cache reset procedure 5 times in a row: <br><pre> <code class="bash hljs"><span class="hljs-variable"><span class="hljs-variable">$time</span></span> <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> 3 &gt; /proc/sys/vm/drop_caches</code> </pre> <br>  The first iteration lasts 14 seconds, because the cached objects do exist in memory: <i>0.00 user 13.78 system <b>0: 13.78 elapsed</b> 99% CPU.</i> <br>  The second iteration takes 5 seconds, although there are no more objects: <i>0.00user 5.59system <b>0: 05.60 elapsed</b> 99% CPU.</i> <br>  The third iteration takes 5 seconds: <i>0.00user 5.48system <b>0: 05.48elapsed</b> 99% CPU</i> <br>  The fourth iteration takes 8 seconds: <i>0.00user 8.35system <b>0: 08.35elapsed</b> 99% CPU</i> <br>  The fifth iteration takes 8 seconds: <i>0.00user 8.34system <b>0: 08.35elapsed</b> 99% CPU</i> <br><br>  It became obvious that the shrinker bypass algorithm used by the vanilla core is not optimal, and we need to change it to the best in terms of scalability. <br><br>  <b>New shrinker traversal algorithm</b> <b><br></b> <br>  From the new algorithm wanted to achieve the following: <br><br><ol><li>  release it from the flaws of the old and </li><li>  Do not add new locks.  Calling do_shrink_slab () only when it makes sense (that is, the corresponding linked list is not empty from the s_dentry_lru array or from the s_inode_lru array), but does not directly access the memory of linked lists. </li></ol><br>  It was clear that this can only be provided by a new data structure on top of heterogeneous shrinkers (there are shrinkers not only of the superblock, but also of other data objects not described in this article. The reader can familiarize themselves with them by searching on the keyword prealloc_shrinker () in the kernel code).  The new data structure should allow two states to be encoded: ‚Äúit makes sense to call do_shrink_slab ()‚Äù and ‚Äúit does not make sense to call do_shrink_slab ()‚Äù. <br><br>  Data structures of type IDA were rejected because  they use locks inside themselves.  The data structure of the bit field is fully suited to this role: it allows for atomic modification of individual bits, and in combination with memory barriers allows you to build an efficient algorithm without using locks. <br><br>  Each shrinker gets its unique id (shrinker :: id), and each memcg is a bitmap that can accommodate the largest id currently registered.  When the first element is added to the s_dentry_lru-&gt; node-&gt; memcg_lrus-&gt; lru [memcg_id] list, the bitmap of the corresponding memcg is set to 1 bit with the number shrinker-&gt; id.  The same in the case of s_inode_id. <br><br>  Now the loop in shrink_slab () can be optimized to bypass only the necessary shrinkers: <br><br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">unsigned</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">long</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">shrink_slab</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ ‚Ä¶ for_each_set_bit(i, <span class="hljs-built_in"><span class="hljs-built_in">map</span></span>, shrinker_nr_max) { ‚Ä¶ shrinker = idr_find(&amp;shrinker_idr, i); ‚Ä¶ do_shrink_slab(&amp;sc, shrinker, priority); ‚Ä¶ } }</code> </pre><br>  (Also, bit cleaning is implemented when the shrinker goes to the ‚Äúno sense to call do_shrink_slab ()‚Äù state. See the Github <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/%3Fid%3Df90280d6b7963fa8925258ed66b4f567fe73dfea">commit</a> for details. <br><br>  If you repeat the reset cache test, then using the new algorithm, it shows significantly better results: <br><pre> <code class="bash hljs"><span class="hljs-variable"><span class="hljs-variable">$time</span></span> <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> 3 &gt; /proc/sys/vm/drop_caches</code> </pre> <br>  First iteration: <i>0.00user 1.10system <b>0: 01.10elapsed</b> 99% CPU</i> <i><br></i>  Second iteration: <i>0.00user 0.00system <b>0: 00.01elapsed</b> 64% CPU</i> <i><br></i>  Third iteration: <i>0.00user 0.01system <b>0: 00.01elapsed</b> 82% CPU</i> <i><br></i>  Fourth iteration: <i>0.00user 0.00system <b>0: 00.01elapsed</b> 64% CPU</i> <i><br></i>  Fifth iteration: <i>0.00user 0.01system <b>0: 00.01elapsed</b> 82% CPU</i> <br>  The duration of the iterations from second to fifth is 0.01 seconds, <b>548 times faster than it was before.</b> <br><br>  Since similar actions to reset the caches occur with every lack of memory on the machine, this optimization significantly improves the performance of machines with a large number of containers and memory control groups.  <a href="https://lkml.org/lkml/2018/7/9/181">A set of patches</a> (17 pieces) was adopted into the vanilla core, and you can find it there, starting with version 4.19. <br><br>  In the process of reviewing the patches, a Google employee appeared, and it turned out that they had the same problem.  Therefore, the patches were additionally tested on a different type of load. <br>  As a result, the patchset was adopted from the 9th iteration;  and its entry into the vanilla core took about 4 months.  Also today, the patchset is included in our own Virtuozzo 7 core, starting with version vz7.71.9 </div><p>Source: <a href="https://habr.com/ru/post/435694/">https://habr.com/ru/post/435694/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../435684/index.html">Oak Ridge National Laboratory has solved the main problem of space device developers: plutonium-238 deficiency</a></li>
<li><a href="../435686/index.html">Pavel Durov liquidates Telegram Messenger LLP</a></li>
<li><a href="../435688/index.html">An example client-server application on Flutter</a></li>
<li><a href="../435690/index.html">[What's wrong with GraphQL] ... and how to deal with it</a></li>
<li><a href="../435692/index.html">Y Combinator: ‚ÄúAt first, some of the largest technology companies look like toys‚Äù</a></li>
<li><a href="../435696/index.html">Antiquities: Computer Advertising 1997</a></li>
<li><a href="../435698/index.html">Writing your own good memory manager</a></li>
<li><a href="../435700/index.html">8 worst questions on Vue.js interview</a></li>
<li><a href="../435702/index.html">Patent trolls start and win: how I was left without a game</a></li>
<li><a href="../435704/index.html">Architectural solutions for mobile games. Part 2: Command and their queues</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>