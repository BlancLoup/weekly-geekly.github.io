<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Colorize a black and white photo using a neural network of 100 lines of code</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Translation of the article Colorizing B & W Photos with Neural Networks . 

 Not so long ago, using neural networks, Amir Avni used a reddit / r / Col...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Colorize a black and white photo using a neural network of 100 lines of code</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/webt/9k/n-/tc/9kn-tc0x6fotuu7psevapq0w6da.jpeg" height="500"></div><br>  <i>Translation of the article <a href="https://blog.floydhub.com/colorizing-b%26w-photos-with-neural-networks/">Colorizing B &amp; W Photos with Neural Networks</a> .</i> <br><br>  Not so long ago, using neural networks, Amir Avni <a href="https://www.reddit.com/r/Colorization/">used a reddit / r / Colorization branch on Reddit</a> , where people gather who are fond of hand-painted historical black-and-white images in Photoshop.  All were amazed at the quality of the neural network.  What takes up to a month of manual work can be done in a few seconds. <br><br>  Let's reproduce and document the image processing process of Amir.  First, look at some of the achievements and failures (at the bottom - the latest version). <br><a name="habracut"></a><br><img src="https://habrastorage.org/webt/qz/nu/-8/qznu-8szkr4zezahqom011dkn6c.png"><br>  <i>Original black and white photos taken from Unsplash</i> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Today, black and white photographs are usually painted by hand in Photoshop.  Watch this video to get an idea of ‚Äã‚Äãthe enormous complexity of such work: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/vubuBrcAwtY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  It can take a month to color a single image.  We have to explore a lot of historical materials from that time.  Up to 20 layers of pink, green and blue shadows are superimposed on the face alone to create the right shade. <br>  This article is for beginners.  If you are not familiar with the terminology of in-depth training of neural networks, then you can read the previous articles ( <a href="https://blog.floydhub.com/my-first-weekend-of-deep-learning/">1</a> , <a href="https://blog.floydhub.com/coding-the-history-of-deep-learning/">2</a> ) and <a href="https://www.youtube.com/watch%3Fv%3DLxfUGhug-iQ">see the lecture by</a> Andrey Karpaty. <br><br>  In this article, you will learn how to build your own neural network for coloring images in three stages. <br><br>  In the first part, we will deal with the basic logic.  Let's build a frame of a neural network of 40 lines, this will be an ‚Äúalpha‚Äù version of a coloring bot.  This code is not very mysterious, it will help you to get acquainted with the syntax. <br><br>  In the next step, we will make a generalizing (generalize) neural network - a ‚Äúbeta‚Äù version.  She will already be able to color images that are not familiar to her. <br><br>  In the "final" version, we will combine our neural network with a classifier.  To do this, take <a href="https://research.googleblog.com/2016/08/improving-inception-and-image.html">Inception Resnet V2</a> , trained in 1.2 million images.  A neural network will teach coloring on images with <a href="https://unsplash.com/">Unsplash</a> . <br><br>  If you can not wait, here is <a href="https://www.floydhub.com/emilwallner/projects/color/43/code/Alpha-version/alpha_version.ipynb">Jupyter Notebook</a> with the alpha version of the bot.  You can also look at the three versions on <a href="https://www.floydhub.com/emilwallner/projects/color/43/code">FloydHub</a> and <a href="https://github.com/emilwallner/Coloring-greyscale-images-in-Keras">GitHub</a> , and also the code used in <a href="https://www.floydhub.com/emilwallner/projects/color/jobs">all the experiments</a> that were carried out on the FloydHub cloud video cards. <br><br><h2>  Basic logic </h2><br>  In this section, we will look at image rendering, talk about the theory of digital color and the basic logic of a neural network. <br><br>  Black and white images can be represented as a grid of pixels.  Each pixel has a brightness value in the range from 0 to 255, from black to white. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9g/ss/xl/9gssxlwzxzloxwvc6bsyql1sjwg.png"></div><br><br>  Color images consist of three layers: red, green and blue.  Suppose you need to decompose in three channels a picture with a green leaf on a white background.  You might think that the leaf will be presented only in the green layer.  But, as you can see, it is in all three layers, because the layers determine not only color, but also brightness. <br><br><img src="https://habrastorage.org/webt/qa/uo/ai/qauoaimgzzpkuocemsvhu7uosys.png"><br><br>  For example, to get white, we need to get an equal distribution of all colors.  If you add the same amount of red and blue, then the green will become brighter.  That is, in a color image using three layers color and contrast are encoded. <br><br><img src="https://habrastorage.org/webt/7m/v4/o0/7mv4o0fsqc0ekox1havhpcckev8.png"><br><br>  As in the black and white image, the pixels of each color image layer contain a value from 0 to 255. Zero means that this pixel in this layer does not have color.  If there are zeros in all three channels, then the result is a black pixel in the picture. <br><br>  As you know, the neural network establishes the relationship between input and output values.  In our case, the neural network should find the connecting features between black and white and color images.  That is, we are looking for properties by which we can compare the values ‚Äã‚Äãfrom the black and white grid with the values ‚Äã‚Äãfrom the three color ones. <br><br><img src="https://habrastorage.org/webt/1k/zl/hu/1kzlhuovpv7ovlq9n7lsxjozxbk.png"><br>  <i>f () is a neural network, [B &amp; W] is input data, [R], [G], [B] is output data.</i> <br><br><h2>  Alpha version </h2><br>  First, we will make a simple version of the neural network that will color the female face.  As you add new features, you will become familiar with the basic syntax of our model. <br><br>  For 40 lines of code, we move from the left image - black and white - to the middle one, which is made by our neural network.  The right picture is an original photograph, from which we made black and white.  The neural network was trained and tested on one image, we will talk about this in the section on the beta version. <br><br><img src="https://habrastorage.org/webt/ug/ab/8p/ugab8prmwfgqhfokxzpgah8imni.png"><br><br><h3>  Color space </h3><br>  First, we use the algorithm for changing color channels from RGB to Lab.  <i>L</i> means lightness, <i>a</i> and <i>b</i> are Cartesian coordinates that determine the position of a color in the range from green to red and from blue to yellow, respectively. <br><br>  As you can see, the image in the Lab space contains one layer of grayscale, and three color layers are packed in two.  Therefore, we can use the original black and white version in the final image.  It remains to calculate two more channels. <br><br><img src="https://habrastorage.org/webt/pj/xu/rw/pjxurwhbgvurpiksvj4zjs-ccxo.png"><br><br>  Scientific fact: 94% of our eye retina receptors are responsible for determining the brightness.  And only 6% of receptors recognize colors.  Therefore, for you, a black and white image looks much clearer than color layers.  This is another reason why we will use this image in the final version. <br><br><h3>  From grayscale to color </h3><br>  We take a layer with grayscale as input, and on its basis we will generate color layers a and b in the Lab color space.  We will take it as the L-layer of the final picture. <br><br><img src="https://habrastorage.org/webt/08/ps/nt/08psntalaojvjrnlo2gsjoqotbm.png"><br><br>  To get two layers from one layer, we use convolutional filters.  They can be represented as blue and red glass in 3D glasses.  Filters determine what we see in the picture.  They can emphasize or hide some part of the image so that our eye can extract the necessary information.  A neural network can also create a new image using a filter or reduce several filters into one image. <br><br>  In convolutional neural networks, each filter is automatically adjusted to make it easier to get the necessary output.  We will add hundreds of filters and then put them together and get layers a and b. <br><br>  Before proceeding to the details of the code, let's run it. <br><br><h3>  Deploying FloydHub Code </h3><br>  If you have not worked with FloydHub before, you can run the <a href="https://www.floydhub.com/">installation for now</a> and watch the <a href="https://www.youtube.com/watch%3Fv%3DbyLQ9kgjTdQ">five-minute video tutorial</a> or <a href="https://blog.floydhub.com/my-first-weekend-of-deep-learning/">step-by-step instructions</a> .  FloydHub is the best and simplest way to deeply learn models on cloud-based video cards. <br><br><h3>  Alpha version </h3><br>  After installing FloydHub, enter the following command: <br><br> <code><a href="https://github.com/emilwallner/Coloring-greyscale-images-in-Keras">git clone https://github.com/emilwallner/Coloring-greyscale-images-in-Keras</a></code> <br> <br>  Then open the folder and initialize FloydHub. <br><br> <code>cd Coloring-greyscale-images-in-Keras/floydhub <br> floyd init colornet</code> <br> <br>  The FloydHub web panel will open in your browser.  You will be prompted to create a new FloydHub project called colornet.  When you create it, go back to the terminal and execute the same initialization command. <br><br> <code>floyd init colornet</code> <br> <br>  Run the task: <br><br> <code>floyd run --data emilwallner/datasets/colornet/2:data --mode jupyter --tensorboard</code> <br> <br>  A few explanations: <br><br><ul><li>  With this command, we mounted a public dataset on FloydHub: <br><br> <code>--dataemilwallner/datasets/colornet/2:data</code> <br> <br>  On FloydHub you can view and use <a href="https://www.floydhub.com/emilwallner/datasets/cifar-10/1">this</a> and many other public datasets. </li><li>  Enable Tensorboard with the command - <code>--tensorboard</code> </li><li>  Run the task in Jupyter Notebook mode using the command <br><br> <code>--mode jupyter</code> </li> </ul><br>  If you can connect video cards to the task, add the <code>‚Äìgpu</code> flag to the <code>‚Äìgpu</code> .  Get about 50 times faster. <br><br>  Go to Jupyter Notebook.  On the FloydHub website in the Jobs tab, click on the Jupyter Notebook link and locate the file: <br><br> <code>floydhub/Alpha version/working_floyd_pink_light_full.ipynb</code> <br> <br>  Open the file and on all cells press Shift + Enter. <br><br>  Gradually increase the value of periods (epoch value) to understand how a neural network learns. <br><br> <code>model.fit(x=X, y=Y, batch_size=1, epochs=1)</code> <br> <br>  Start with epochs = 1, then increase to 10, 100, 500, 1000 and 3000. This value shows how many times the network is trained in the image.  As soon as you train the neural network, you will find the img_result.png file in the main folder. <br><br> <code># Get images <br> image = img_to_array(load_img('woman.png')) <br> image = np.array(image, dtype=float) <br> <br> # Import map images into the lab colorspace <br> X = rgb2lab(1.0/255*image)[:,:,0] <br> Y = rgb2lab(1.0/255*image)[:,:,1:] <br> Y = Y / 128 <br> X = X.reshape(1, 400, 400, 1) <br> Y = Y.reshape(1, 400, 400, 2) <br> model = Sequential() <br> model.add(InputLayer(input_shape=(None, None, 1))) <br> <br> # Building the neural network <br> model = Sequential() <br> model.add(InputLayer(input_shape=(None, None, 1))) <br> model.add(Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)) <br> model.add(Conv2D(8, (3, 3), activation='relu', padding='same')) <br> model.add(Conv2D(16, (3, 3), activation='relu', padding='same')) <br> model.add(Conv2D(16, (3, 3), activation='relu', padding='same', strides=2)) <br> model.add(Conv2D(32, (3, 3), activation='relu', padding='same')) <br> model.add(Conv2D(32, (3, 3), activation='relu', padding='same', strides=2)) <br> model.add(UpSampling2D((2, 2))) <br> model.add(Conv2D(32, (3, 3), activation='relu', padding='same')) <br> model.add(UpSampling2D((2, 2))) <br> model.add(Conv2D(16, (3, 3), activation='relu', padding='same')) <br> model.add(UpSampling2D((2, 2))) <br> model.add(Conv2D(2, (3, 3), activation='tanh', padding='same')) <br> <br> # Finish model <br> model.compile(optimizer='rmsprop',loss='mse') <br> <br> #Train the neural network <br> model.fit(x=X, y=Y, batch_size=1, epochs=3000) <br> print(model.evaluate(X, Y, batch_size=1)) <br> <br> # Output colorizations <br> output = model.predict(X) <br> output = output * 128 <br> canvas = np.zeros((400, 400, 3)) <br> canvas[:,:,0] = X[0][:,:,0] <br> canvas[:,:,1:] = output[0] <br> imsave("img_result.png", lab2rgb(canvas)) <br> imsave("img_gray_scale.png", rgb2gray(lab2rgb(canvas)))</code> <br> <br>  FloydHub command to run this network: <br><br> <code>floyd run --data emilwallner/datasets/colornet/2:data --mode jupyter --tensorboard</code> <br> <br><h3>  Technical explanation </h3><br>  Recall that at the entrance we have a grid representing a black and white image.  And at the exit - two grids with color values.  We created link filters between the input and output values.  We have a convolutional neural network. <br><br>  For training the network uses color images.  We converted from RGB color space to Lab.  The black and white layer is fed to the input, and two colored layers are obtained at the output. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tj/lw/0z/tjlw0zbxhqky-geusbsdpfkzf4y.png"></div><br><br>  We in one range compare (map) the calculated values ‚Äã‚Äãwith real, thereby comparing them with each other.  The boundaries of the range from -1 to 1. To compare the calculated values, we use the activation function tanh (hyperbolic tangential).  If you <a href="http://mathworld.wolfram.com/HyperbolicTangent.html">apply</a> it to any value, the function returns a value in the range from -1 to 1. <br><br>  Actual color values ‚Äã‚Äãvary from ‚Äî128 to 128. In Lab space, this is the default range.  If each value is divided by 128, then all of them will be in the range from -1 to 1. Such a ‚Äúnormalization‚Äù allows us to compare the error of our calculation. <br><br>  After calculating the resulting error, the neural network updates the filters to correct the result of the next iteration.  The whole procedure is repeated cyclically until the error becomes minimal. <br><br>  Let's understand the syntax of this code: <br><br> <code>X = rgb2lab(1.0/255*image)[:,:,0] <br> Y = rgb2lab(1.0/255*image)[:,:,1:]</code> <br> <br>  <b>1.0 / 255</b> means we use a 24-bit RGB color space.  That is, for each color channel we use values ‚Äã‚Äãin the range from 0 to 255. This gives us 16.7 million colors. <br><br>  But since the human eye can only recognize between 2 and 10 million colors, it does not make sense to use a wider color space. <br><br> <code>Y = Y / 128</code> <br> <br>  Lab color space uses a different range.  The color spectrum <b>ab</b> varies from ‚Äî128 to 128. If we divide all the values ‚Äã‚Äãof the output layer by 128, they will be within the range from -1 to 1, and then we can compare these values ‚Äã‚Äãwith those calculated by our neural network. <br><br>  After using the <code>rgb2lab()</code> function to convert the color space, we use the <b>[:,:, 0] to</b> select a black and white layer.  This is the input to the neural network.  [:,:, 1:] selects two color layers, red-green and blue-yellow. <br><br>  After learning the neural network, we perform the last calculation, which we translate into a picture. <br><br> <code>output = model.predict(X) <br> output = output * 128</code> <br> <br>  Here we feed a black and white image to the input and run it through a trained neural network.  We take all the output values ‚Äã‚Äãfrom -1 to 1 and multiply them by 128. So we get the correct colors in the Lab system. <br><br> <code>canvas = np.zeros((400, 400, 3)) <br> canvas[:,:,0] = X[0][:,:,0] <br> canvas[:,:,1:] = output[0]</code> <br> <br>  Create a black RGB canvas by filling all three layers with zeros.  Then copy the black and white layer from the test image and add two color layers.  We turn the resulting array of pixel values ‚Äã‚Äãinto an image. <br><br><h3>  What have we learned while working on the alpha version </h3><br><ul><li>  <b>Reading research is hard work</b> .  But it was enough to summarize the key points of the articles, and studying them became easier.  It also helped to include some details in this article. </li><li>  <b>You need to start small</b> .  Most of the implementations we found on the network consisted of 2‚Äì10 thousand lines of code.  This makes it hard to get an idea of ‚Äã‚Äãthe basic logic.  But if there is a simplified, basic version at hand, then it is easier to read and implement, and research. </li><li>  <b>Do not be lazy to understand other people's projects</b> .  We had to look at a few dozen projects on coloring images on Github to determine the content of our code. </li><li>  <b>Not everything works as planned</b> .  Perhaps, at first, your network will be able to create only red and yellow colors.  For the first time, we used the Relu activation function for final activation.  But it generates only positive values, and therefore the blue and green spectra are not available to it.  This disadvantage was solved by adding the activation function tanh to convert the values ‚Äã‚Äãalong the Y axis. </li><li>  <b>Understanding&gt; speed</b> .  Many of the implementations we saw were executed quickly, but it was difficult to work with them.  Therefore, we decided to optimize our code for the sake of adding new features, not execution. </li></ul><br><h2>  Beta version </h2><br>  Offer the alpha versions to color the image in which she was not trained, and immediately understand what the main drawback of this version is.  She can't handle it.  The fact is that the neural network remembered the information.  She did not learn to paint an unfamiliar image.  And we will fix this in the beta version - we will teach the neural network to generalize. <br><br>  Below is a beta version that colored test images. <br><br>  Instead of using Imagenet, we created <a href="https://www.floydhub.com/emilwallner/datasets/colornet">a public dataset</a> with higher quality images <a href="https://www.floydhub.com/emilwallner/datasets/colornet">on FloydHub</a> .  They are taken from <a href="https://unsplash.com/">Unsplash</a> - a site where pictures of professional photographers are laid out.  9500 training images and 500 verification images. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/sh/et/il/shetilxxtcns7jrzposx1wr3dsc.png"></div><br><br><h3>  Feature selector </h3><br>  Our neural network is looking for characteristics linking black and white images with their color versions. <br><br>  Imagine that you need to color black and white pictures, but you can see only nine pixels on the screen at a time.  You can view each picture from left to right and from top to bottom, trying to calculate the color of each pixel. <br><br><img src="https://habrastorage.org/webt/rn/h4/de/rnh4def0ugwg4yq7rjn6jy-_nms.png"><br><br>  Let these nine pixels are on the edge of the woman's nostrils.  As you understand, choosing the right color here is almost impossible, so you have to break the solution of the problem into stages. <br><br>  First, we look for simple characteristic structures: diagonal lines, only black pixels, and so on.  In each square of 9 pixels, we are looking for the same structure and delete everything that does not correspond to it.  As a result, we created 64 new images from 64 of our minifilters. <br><br><img src="https://habrastorage.org/webt/02/ck/z2/02ckz2ekth7ujzwf6wcwlgk81km.png"><br>  <i>Number of images processed by filters at each stage.</i> <br><br>  If we look through the images again, we will find the same small repeating structures that we have already identified.  To better analyze the image, reduce its size by half. <br><br><img src="https://habrastorage.org/webt/jz/h-/dw/jzh-dwq977nbezfq-tm4l4tuxgk.png"><br>  <i>Reduce the size in three stages.</i> <br><br>  We still have a 3x3 filter that needs to scan each image.  But if we apply our simpler filters to new squares of nine pixels, we can find more complex structures.  For example, a semicircle, a small dot or line.  Again and again, we find the same repeating structure in the picture.  This time we generate 128 new images processed by filters. <br><br>  After a couple of steps, the processed images will look like this: <br><br><img src="https://habrastorage.org/webt/fx/7-/te/fx7-tebw29f9j4g48fl-agdqvl8.png"><br><br>  Again: you start by looking for simple properties, such as edges.  As they are processed, the layers are combined into structures, then into more complex features, and in the end a face is obtained.  Details are explained in this video: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/AgkfIQ4IGaM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  The described process is very similar to computer vision algorithms.  Here we use the so-called convolutional neural network, which combines several processed images to understand the contents of the entire image. <br><br><h3>  From extracting properties to color </h3><br>  The neural network operates on the principle of trial and error.  First, it randomly assigns a color to each pixel.  Then, for each pixel, it calculates errors and adjusts the filters so that in the next attempt to improve the results. <br><br>  The neural network adjusts its filters based on the results with the largest error values.  In our case, the neural network decides whether to color or not, and how to place different objects in the picture.  First, she paints all objects in brown.  This color is most similar to all other colors, so with it, using it produces the smallest errors. <br><br>  Due to the monotony of the training data, the neural network tries to understand the differences between these or other objects.  She still can not calculate more accurate color shades, we will deal with this when creating a full version of the neural network. <br><br>  Here is the beta code: <br><br> <code># Get images <br> X = [] <br> for filename in os.listdir('../Train/'): <br> X.append(img_to_array(load_img('../Train/'+filename))) <br> X = np.array(X, dtype=float) <br> <br> # Set up training and test data <br> split = int(0.95*len(X)) <br> Xtrain = X[:split] <br> Xtrain = 1.0/255*Xtrain <br> <br> #Design the neural network <br> model = Sequential() <br> model.add(InputLayer(input_shape=(256, 256, 1))) <br> model.add(Conv2D(64, (3, 3), activation='relu', padding='same')) <br> model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2)) <br> model.add(Conv2D(128, (3, 3), activation='relu', padding='same')) <br> model.add(Conv2D(128, (3, 3), activation='relu', padding='same', strides=2)) <br> model.add(Conv2D(256, (3, 3), activation='relu', padding='same')) <br> model.add(Conv2D(256, (3, 3), activation='relu', padding='same', strides=2)) <br> model.add(Conv2D(512, (3, 3), activation='relu', padding='same')) <br> model.add(Conv2D(256, (3, 3), activation='relu', padding='same')) <br> model.add(Conv2D(128, (3, 3), activation='relu', padding='same')) <br> model.add(UpSampling2D((2, 2))) <br> model.add(Conv2D(64, (3, 3), activation='relu', padding='same')) <br> model.add(UpSampling2D((2, 2))) <br> model.add(Conv2D(32, (3, 3), activation='relu', padding='same')) <br> model.add(Conv2D(2, (3, 3), activation='tanh', padding='same')) <br> model.add(UpSampling2D((2, 2))) <br> <br> # Finish model <br> model.compile(optimizer='rmsprop', loss='mse') <br> <br> # Image transformer <br> datagen = ImageDataGenerator( <br> shear_range=0.2, <br> zoom_range=0.2, <br> rotation_range=20, <br> horizontal_flip=True) <br> <br> # Generate training data <br> batch_size = 50 <br> def image_a_b_gen(batch_size): <br> for batch in datagen.flow(Xtrain, batch_size=batch_size): <br> lab_batch = rgb2lab(batch) <br> X_batch = lab_batch[:,:,:,0] <br> Y_batch = lab_batch[:,:,:,1:] / 128 <br> yield (X_batch.reshape(X_batch.shape+(1,)), Y_batch) <br> <br> # Train model <br> TensorBoard(log_dir='/output') <br> model.fit_generator(image_a_b_gen(batch_size), steps_per_epoch=10000, epochs=1) <br> <br> # Test images <br> Xtest = rgb2lab(1.0/255*X[split:])[:,:,:,0] <br> Xtest = Xtest.reshape(Xtest.shape+(1,)) <br> Ytest = rgb2lab(1.0/255*X[split:])[:,:,:,1:] <br> Ytest = Ytest / 128 <br> print model.evaluate(Xtest, Ytest, batch_size=batch_size) <br> <br> # Load black and white images <br> color_me = [] <br> for filename in os.listdir('../Test/'): <br> color_me.append(img_to_array(load_img('../Test/'+filename))) <br> color_me = np.array(color_me, dtype=float) <br> color_me = rgb2lab(1.0/255*color_me)[:,:,:,0] <br> color_me = color_me.reshape(color_me.shape+(1,)) <br> <br> # Test model <br> output = model.predict(color_me) <br> output = output * 128 <br> <br> # Output colorizations <br> for i in range(len(output)): <br> cur = np.zeros((256, 256, 3)) <br> cur[:,:,0] = color_me[i][:,:,0] <br> cur[:,:,1:] = output[i] <br> imsave("result/img_"+str(i)+".png", lab2rgb(cur))</code> <br> <br>  FloydHub command to run the beta version of the neural network: <br><br> <code>floyd run --data emilwallner/datasets/colornet/2:data --mode jupyter --tensorboard</code> <br> <br><h3>  Technical explanation </h3><br>  From other neural networks that work with images, ours are different in that the location of pixels is important for it.  In coloring neural networks, the image size or aspect ratio remains unchanged.  And for other types of networks, the image is distorted as it approaches the final version. <br><br>  The layer of pooling with the maximum function used in the classifying networks increases the information density, but at the same time distorts the picture.  It evaluates information only, not image layout.  And in the coloring networks to halve the width and height, we use step 2 (stride of 2).  Information density is also increasing, but the picture is not distorted. <br><br><img src="https://habrastorage.org/webt/z0/nm/kz/z0nmkzn0tjfzyshnnlwss0b5b_4.png"><br><br>  Also, our neural network is different from other layers of upsampling and preserving the aspect ratio of the image.  Classifying networks only care about the final classification, so they gradually reduce the size and quality of the image as it runs through the neural network. <br><br>  Coloring neural networks do not change the aspect ratio of the image.  To do this, white fields are added using the parameter <code>*padding='same'*</code> , as in the illustration above.  Otherwise each convolutional layer would cut the images. <br><br>  To double the size of the image, the coloring neural network uses <a href="https://keras.io/layers/convolutional/">a resampling layer</a> . <br><br> <code>for filename in os.listdir('/Color_300/Train/'): <br> X.append(img_to_array(load_img('/Color_300/Test'+filename)))</code> <br> <br>  This <code>for-loop</code> first counts the names of all files in the directory, traverses the directory and converts all the pictures into arrays of pixels, and finally merges them into a huge vector. <br><br> <code>datagen = ImageDataGenerator( <br> shear_range=0.2, <br> zoom_range=0.2, <br> rotation_range=20, <br> horizontal_flip=True)</code> <br> <br>  With the help of <a href="https://keras.io/preprocessing/image/">ImageDataGenerator,</a> you can turn on the image generator.  Then each image will be different from the previous ones, which will speed up learning of the neural network.  The <code>shear_range</code> sets the image tilt to the left or right, it can also be increased, rotated or reflected horizontally. <br><br> <code>batch_size = 50 <br> def image_a_b_gen(batch_size): <br> for batch in datagen.flow(Xtrain, batch_size=batch_size): <br> lab_batch = rgb2lab(batch) <br> X_batch = lab_batch[:,:,:,0] <br> Y_batch = lab_batch[:,:,:,1:] / 128 <br> yield (X_batch.reshape(X_batch.shape+(1,)), Y_batch)</code> <br> <br>  Apply these settings to the images in the Xtrain folder and generate new images.  Then we will extract the black and white layer for <code>X_batch</code> and two colors for the two color layers. <br><br> <code>model.fit_generator(image_a_b_gen(batch_size), steps_per_epoch=1, epochs=1000)</code> <br> <br>  The more powerful your video card, the more pictures you can process in it simultaneously.  For example, the described system can handle 50-100 images.  The value of the steps_per_epoch parameter is obtained by dividing the number of training images by the batch size (batch size). <br><br>  For example: if we have 100 pictures, and the size of the series is 50, then we get 2 stages in the period.  The number of periods determines how many times you will train the neural network on all pictures.  If you have 10 thousand pictures and 21 periods, then it will take about 11 hours on the Tesla K80 video card. <br><br><h3>  What have you learned </h3><br><ul><li>  <b>First, more experiments with small series, and then you can move on to large runs</b> .  We had mistakes even after 20‚Äì30 experiments.  If something is done, it does not mean that it works.  Bugs in neural networks are generally less noticeable than traditional programming errors.  For example, one of our most bizarre bugs was <a href="https://twitter.com/EmilWallner/status/916309564966006784">Adam hiccup</a> . </li><li>  <b>The more diverse dataset, the more brown will be in the images</b> .  If your dataset has <a href="https://github.com/2014mchidamb/DeepColorization/tree/master/face_images">very similar images</a> , then the neural network will work pretty well without using a more complex architecture.  But such a neural network will be worse to generalize. </li><li>  <b>Forms, forms and forms again</b> .  The size of the images must be accurate and proportional to each other during the entire operation of the neural network.  First, we used an image of 300 pixels, then several times we reduced it twice: to 150, 75, and 35.5 pixels.  In the last version, half a pixel was lost, which made it necessary to substitute a bunch of crutches, until it came to that it was better to use a deuce to the degree: 2, 4, 8, 16, 32, 64, 256, and so on. </li><li>  <b>Creating datasets</b> : a) <a href="http://osxdaily.com/2010/02/03/how-to-prevent-ds_store-file-creation/">Disable the</a> .DS_Store file, otherwise it will drive you crazy.  b) Show fiction.  To download the files, we used the <a href="https://github.com/emilwallner/useful-scripts/blob/master/auto_scroll_browser_window_console">console script</a> in Chrome and the <a href="https://chrome.google.com/webstore/detail/imagespark-ultimate-image/hooaoionkjogngfhjjniefmenehnopag">extension</a> .  c) Make copies of the source files you are processing and arrange the <a href="https://github.com/emilwallner/useful-scripts">scripts for cleaning</a> . </li></ul><br><h2>  Full version of the neural network </h2><br>  Our final version of the neural network contains four components.  We divided the previous network into an encoder and a decoder, and between them a fusion layer.  If you are not familiar with the classifying neural networks, we recommend reading this guide: <a href="http://cs231n.github.io/classification/">http://cs231n.github.io/classification/</a> . <br><br>  Input data simultaneously passes through the encoder and through the most powerful modern classifier - <a href="https://research.googleblog.com/2016/08/improving-inception-and-image.html">Inception ResNet v2</a> .  This is a neural network trained on 1.2 million images.  We extract the classification layer and merge it with the encoder output. <br><br><img src="https://habrastorage.org/webt/yg/_k/0_/yg_k0_lhwv-i8yca0gvzjxsy79q.png"><br><br>  A more detailed visual explanation: <a href="https://github.com/baldassarreFe/deep-koalarization">https://github.com/baldassarreFe/deep-koalarization</a> . <br><br>  If you transfer training from the classifier to the coloring network, it will be able to understand what is depicted in the picture, and therefore compare the representation of the object with the coloring scheme. <br><br>  Here are some test images, only 20 images were used to train the network. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ac/dx/ww/acdxwwezczvtp1uwvis5bnw397u.png"></div><br><br>  Most of the pictures are crooked.  But thanks to a large test set (2500 images), there are some decent ones.  Studying the network on a larger sample gives more stable results, but still most of the pictures turned out brown.  Here is a complete list of <a href="https://www.floydhub.com/emilwallner/projects/color">experiments</a> and test images. <br><br>  The most common architectures from various research projects are: <br><br><ul><li>  Manually add small colored dots to the image to give the network a hint ( <a href="http://www.cs.huji.ac.il/~yweiss/Colorization/">link</a> ). </li><li>  We find a similar image and transfer colors from it (more <a href="https://dl.acm.org/citation.cfm%3Fid%3D2393402">here</a> and <a href="https://arxiv.org/abs/1505.05192">here</a> ). </li><li>  The residual encoder layer and the merging classification layer ( <a href="http://tinyclouds.org/colorize/">link</a> ). </li><li>  We merge hypercolumns from the classifying network (more details <a href="https://arxiv.org/pdf/1603.08511.pdf">here</a> and <a href="https://arxiv.org/pdf/1603.06668.pdf">here</a> ). </li><li>  We unite the final classification between the encoder and the decoder (more <a href="http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/data/colorization_sig2016.pdf">here</a> and <a href="https://github.com/baldassarreFe/deep-koalarization/blob/master/report.pdf">here</a> ). </li></ul><br>  <b>Color spaces</b> : Lab, YUV, HSV and LUV (more <a href="http://cs231n.stanford.edu/reports/2016/pdfs/219_Report.pdf">here</a> and <a href="https://arxiv.org/abs/1605.00075">here</a> ) <br><br>  <b>Losses</b> : standard error, classification, weighted classification ( <a href="https://arxiv.org/pdf/1603.06668.pdf">link</a> ). <br><br>  We chose the ‚Äúmerge layer‚Äù architecture (fifth on the list) because it gave the best results.  It is also easier to understand and easier to reproduce in <a href="https://keras.io/">Keras</a> .  Although this is not the strongest architecture, but for the beginning it will fit. <br><br>  The structure of our neural network is borrowed from the <a href="https://github.com/baldassarreFe/deep-koalarization/blob/master/report.pdf">work of</a> Federico Baldasarre and his colleagues, and adapted to work with Keras.  Note: In this code, a functional API is used instead of the sequential model Keras.  [ <a href="https://keras.io/getting-started/functional-api-guide/">Documentation</a> ] <br><br> <code># Get images <br> X = [] <br> for filename in os.listdir('/data/images/Train/'): <br> X.append(img_to_array(load_img('/data/images/Train/'+filename))) <br> X = np.array(X, dtype=float) <br> Xtrain = 1.0/255*X <br> <br> #Load weights <br> inception = InceptionResNetV2(weights=None, include_top=True) <br> inception.load_weights('/data/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5') <br> inception.graph = tf.get_default_graph() <br> embed_input = Input(shape=(1000,)) <br> <br> #Encoder <br> encoder_input = Input(shape=(256, 256, 1,)) <br> encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input) <br> encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output) <br> encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output) <br> encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output) <br> encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_output) <br> encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output) <br> encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output) <br> encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output) <br> <br> #Fusion <br> fusion_output = RepeatVector(32 * 32)(embed_input) <br> fusion_output = Reshape(([32, 32, 1000]))(fusion_output) <br> fusion_output = concatenate([encoder_output, fusion_output], axis=3) <br> fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output) <br> <br> #Decoder <br> decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output) <br> decoder_output = UpSampling2D((2, 2))(decoder_output) <br> decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output) <br> decoder_output = UpSampling2D((2, 2))(decoder_output) <br> decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output) <br> decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output) <br> decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output) <br> decoder_output = UpSampling2D((2, 2))(decoder_output) <br> model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output) <br> <br> #Create embedding <br> def create_inception_embedding(grayscaled_rgb): <br> grayscaled_rgb_resized = [] <br> for i in grayscaled_rgb: <br> i = resize(i, (299, 299, 3), mode='constant') <br> grayscaled_rgb_resized.append(i) <br> grayscaled_rgb_resized = np.array(grayscaled_rgb_resized) <br> grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized) <br> with inception.graph.as_default(): <br> embed = inception.predict(grayscaled_rgb_resized) <br> return embed <br> <br> # Image transformer <br> datagen = ImageDataGenerator( <br> shear_range=0.4, <br> zoom_range=0.4, <br> rotation_range=40, <br> horizontal_flip=True) <br> <br> #Generate training data <br> batch_size = 20 <br> def image_a_b_gen(batch_size): <br> for batch in datagen.flow(Xtrain, batch_size=batch_size): <br> grayscaled_rgb = gray2rgb(rgb2gray(batch)) <br> embed = create_inception_embedding(grayscaled_rgb) <br> lab_batch = rgb2lab(batch) <br> X_batch = lab_batch[:,:,:,0] <br> X_batch = X_batch.reshape(X_batch.shape+(1,)) <br> Y_batch = lab_batch[:,:,:,1:] / 128 <br> yield ([X_batch, create_inception_embedding(grayscaled_rgb)], Y_batch) <br> <br> #Train model <br> tensorboard = TensorBoard(log_dir="/output") <br> model.compile(optimizer='adam', loss='mse') <br> model.fit_generator(image_a_b_gen(batch_size), callbacks=[tensorboard], epochs=1000, steps_per_epoch=20) <br> <br> #Make a prediction on the unseen images <br> color_me = [] <br> for filename in os.listdir('../Test/'): <br> color_me.append(img_to_array(load_img('../Test/'+filename))) <br> color_me = np.array(color_me, dtype=float) <br> color_me = 1.0/255*color_me <br> color_me = gray2rgb(rgb2gray(color_me)) <br> color_me_embed = create_inception_embedding(color_me) <br> color_me = rgb2lab(color_me)[:,:,:,0] <br> color_me = color_me.reshape(color_me.shape+(1,)) <br> <br> # Test model <br> output = model.predict([color_me, color_me_embed]) <br> output = output * 128 <br> <br> # Output colorizations <br> for i in range(len(output)): <br> cur = np.zeros((256, 256, 3)) <br> cur[:,:,0] = color_me[i][:,:,0] <br> cur[:,:,1:] = output[i] <br> imsave("result/img_"+str(i)+".png", lab2rgb(cur))</code> <br> <br>  FloydHub command to run the full version of the neural network: <br><br> <code>floyd run --data emilwallner/datasets/colornet/2:data --mode jupyter --tensorboard</code> <br> <br><h3>  Technical explanation </h3><br>  <a href="https://keras.io/getting-started/functional-api-guide/">The Keras functional API is</a> great for concatenating or combining several models. <br><br>  First, download the <a href="https://research.googleblog.com/2016/08/improving-inception-and-image.html">Inception ResNet v2</a> neural network and load the weights.  Since we will use two models in parallel, we need to determine which ones.  This is done in <a href="https://www.tensorflow.org/">Tensorflow</a> , Keras backend. <br><br> <code>inception = InceptionResNetV2(weights=None, include_top=True) <br> inception.load_weights('/data/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5') <br> inception.graph = tf.get_default_graph()</code> <br> <br>  Create a series (batch) of the corrected images.  Translate them into b / w and run through the Inception ResNet model. <br><br> <code>grayscaled_rgb = gray2rgb(rgb2gray(batch)) <br> embed = create_inception_embedding(grayscaled_rgb)</code> <br> <br>  First you need to change the size of the pictures to feed their models.  Then, using the preprocessor, we will bring the pixels and values ‚Äã‚Äãin color to the desired format.  Finally, run the images through the Inception network and extract the final model layer. <br><br> <code>def create_inception_embedding(grayscaled_rgb): <br> grayscaled_rgb_resized = [] <br> for i in grayscaled_rgb: <br> i = resize(i, (299, 299, 3), mode='constant') <br> grayscaled_rgb_resized.append(i) <br> grayscaled_rgb_resized = np.array(grayscaled_rgb_resized) <br> grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized) <br> with inception.graph.as_default(): <br> embed = inception.predict(grayscaled_rgb_resized) <br> return embed</code> <br> <br>  Let's go back to the generator.  For each series we will generate 20 images of the format described below.  The Tesla K80 GPU took about an hour.  With this model, this video card can generate up to 50 images at a time without any memory problems. <br><br> <code>yield ([X_batch, create_inception_embedding(grayscaled_rgb)], Y_batch)</code> <br> <br>  This corresponds to the format of our colornet model. <br><br> <code>model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)</code> <br> <br>  <code>encoder_inputis</code> transferred to the Encoder model, its output data are then merged in the merge layer with <code>embed_inputin</code> ;  the output of the merge is fed to the input of the Decoder model, which returns the resulting data - <code>decoder_output</code> . <br><br> <code>fusion_output = RepeatVector(32 * 32)(embed_input) <br> fusion_output = Reshape(([32, 32, 1000]))(fusion_output) <br> fusion_output = concatenate([fusion_output, encoder_output], axis=3) <br> fusion_output = Conv2D(256, (1, 1), activation='relu')(fusion_output)</code> <br> <br>  In the merge layer, we first layer with 1000 categories (1000 category layer) multiplied by 1024 (32 * 32).  So we get from the Inception model 1024 rows of the final layer.  The grid 32 x 32 is transferred from a two-dimensional to a three-dimensional representation, with 1000 category pillars (category pillars).  The columns are then associated with the encoder model output.  We apply a convolutional network with 254 filters and a 1x1 kernel to the final results of the merge layer. <br><br><h3>  What have you learned </h3><br><ul><li>  <b>The terminology in the research papers was frightening</b> .  We spent three days looking for a way to implement the ‚Äúmerge model‚Äù in Keras.  It sounds so complicated that I simply did not want to take on this task, we tried to find tips that would facilitate our work. </li><li>  <b>Questions online</b> .  There was not a single comment on Keras's Slack channel, and the questions asked on the Stack Overflow were deleted.  But when we began to sort out the problem publicly in search of a simple answer, it became clearer to us how to solve this problem. </li><li>  <b>Mailing letters</b> .  On the forums, you can be ignored, but if you turn to people directly, they will be more responsive.           Skype! </li><li> <b>     ,      ,       </b> .  <a href="https://www.floydhub.com/emilwallner/projects/color/24/code/Experiments/transfer-learning-examples"> </a>    . </li><li> <b> ,  -    ,      </b> .  ,      ,   ,    .         .        .    ,         Google,      ‚ÄúEpoch 1/22‚Äù. </li></ul><br><h2>  What's next </h2><br>   ‚Äî    .     ,  . ,      .      : <br><br><ul><li>     . </li><li>   . </li><li>   ,     . </li><li>   (amplifier)    RGB.      ,        ,        . </li><li>   . </li><li>    .     ,      .        ¬´¬ª. </li></ul><br> <b>   -          FloydHub.</b> <br><br><ul><li>   -    woman.jpg        ( 400x400 ). </li><li>  -         Test,    FloydHub-.       Notebook   Test,    .      256x256 .       ,        -. </li></ul></div><p>Source: <a href="https://habr.com/ru/post/342388/">https://habr.com/ru/post/342388/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../342374/index.html">Typical problems of IT start-ups that hinder rapid development, and how to avoid them</a></li>
<li><a href="../342380/index.html">Load testing, process automation history</a></li>
<li><a href="../342382/index.html">Raiffeisen-Online Short-Lynch</a></li>
<li><a href="../342384/index.html">Kotlin Night Moscow in Avito on November 25</a></li>
<li><a href="../342386/index.html">Build your Security Operation Center of 5 items</a></li>
<li><a href="../342390/index.html">Russia vs Germany. Inside about the processes of those. support, quality service and Russian hackers</a></li>
<li><a href="../342392/index.html">Operation Windigo: Linux / Ebury Update</a></li>
<li><a href="../342394/index.html">Additional reports and processing, use of extensions in 1C Fresh</a></li>
<li><a href="../342398/index.html">Live broadcast of the press conference with the presentation of the startup ROI4CIO</a></li>
<li><a href="../342400/index.html">Targeting the Odnoklassniki Moderator application</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>