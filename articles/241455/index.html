<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How recommender systems work. Lecture in Yandex</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, my name is Michael Roizner. Recently, I gave a lecture to students of Small Shad Yandex about what recommendation systems are and what methods are...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">🔎</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">📜</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">⬆️</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">⬇️</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How recommender systems work. Lecture in Yandex</h1><div class="post__text post__text-html js-mediator-article">  Hi, my name is Michael Roizner.  Recently, I gave a lecture to students of Small Shad Yandex about what recommendation systems are and what methods are there.  Based on the lecture, I prepared this post. <br><br><iframe src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://video.yandex.ru/iframe/ya-events/m-69141-15041d132d7-3fb6da9534d20c32/&amp;xid=17259,15700019,15700186,15700191,15700253&amp;usg=ALkJrhiEI-JlvDefJ0QlERYqYiAyV7LOvg" width="450" height="147" frameborder="0" scrolling="no" allowfullscreen="1"></iframe><br><br>  Lecture plan: <br><ol><li>  Types and applications of recommender systems. </li><li>  The simplest algorithms. </li><li>  Introduction to linear algebra. </li><li>  Svd algorithm. </li><li>  Measuring the quality of recommendations. </li><li>  Development direction. </li></ol><br><a name="habracut"></a><br><div class="slideshow"><iframe src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=http://www.slideshare.net/slideshow/embed_code/39464146&amp;xid=17259,15700019,15700186,15700191,15700253&amp;usg=ALkJrhi5TyAbemwjxOsNb8-a6_Ke2xckFw" width="425" height="355" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe></div><br>  Let's start with a simple one: what are recommender systems in general, and how they are.  Probably everyone has already come across them on the Internet.  The first example is movie recommendation systems. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/files/196/e81/3fe/196e813fe26045efba660e09de211446.png" width="640"><br><br>  On imdb.com, users can rate movies on a ten-point scale.  Ratings are aggregated, it turns out the average movie rating.  On the same site there is a block with recommendations for a specific user.  If I went to the site and rated several films, imdb will be able to recommend me some more films.  A similar block is on Facebook. <br><br>  Something similar, but only for music, does last.fm.  He recommends me artists, albums, events that I should go to.  The Pandora service in Russia is almost unknown, since  it does not work with us, but in America it is very popular.  This is a personal radio, which gradually adapts to the user based on his ratings, and as a result plays only those tracks that he likes. <br><br><img src="https://habrastorage.org/files/f1c/761/6d9/f1c7616d94ac4cc4b844f0339e19818c.png" width="640"><br><br>  Another well-known area is product recommendation.  In the picture below we have, of course, Amazon.  If you bought something on Amazon, you will be hunted with additional offers: similar products or accessories.  This is also good for users (they don’t need to search for these products on their own), and of course, this is good for the store itself. <br><br><img src="https://habrastorage.org/files/9c3/905/a37/9c3905a3725e4453b42d1d4f249e61eb.png" width="640"><br><br>  We have listed three categories, but in fact they are much more: places on the map, news, articles, websites, concerts, theaters, exhibitions, videos, books, apps, games, travel, social connections and much more. <br><br><h2>  Types of recommendation systems </h2><br>  There are two main types of recommender systems.  Of course, there are more of them, but today we will consider these and especially the collaborative filtering. <br><br><ol><li>  Content-based <br><ul><li>  The user is recommended objects similar to those that this user has already used. </li><li>  Similarities are evaluated by feature content. </li><li>  Strong dependence on the subject area, the usefulness of the recommendations is limited. </li></ul></li><li>  Collaborative Filtering <br><ul><li>  The recommendation is based on the history of assessments of both the user and other users. </li><li>  A more universal approach often gives better results. </li><li>  There are problems (for example, cold start). </li></ul></li></ol><br>  Recommender systems appeared on the Internet for a long time, about 20 years ago.  However, the real upswing in this area happened about 5-10 years ago when the Netflix Prize competition took place.  Netflix did not rent digital copies at that time, but sent out VHS tapes and DVDs.  For them it was very important to improve the quality of the recommendations.  The better Netflix recommends movies to its users, the more films they rent.  Accordingly, the company's profit grows.  In 2006, they launched the Netflix Prize competition.  They posted openly the collected data: about 100 million marks on a five-point scale, indicating the ID of the users who put them.  The participants of the competition should have foreseen as best as possible what assessment a particular user would give to a certain film.  The quality of the prediction was measured using the <a href="https://ru.wikipedia.org/wiki/%25D1%25F0%25E5%25E4%25ED%25E5%25EA%25E2%25E0%25E4%25F0%25E0%25F2%25E8%25F7%25E5%25F1%25EA%25EE%25E5_%25EE%25F2%25EA%25EB%25EE%25ED%25E5%25ED%25E8%25E5">RMSE</a> (mean square deviation) metric.  Netflix already had an algorithm that predicted user ratings with a quality of 0.9514 using the RMSE metric.  The task was to improve the prediction by at least 10% - to 0.8563.  The winner was promised a prize of $ 1,000,000. The competition lasted about three years.  For the first year, the quality was improved by 7%, then everything slowed down a bit.  But at the end, two teams sent their solutions with a difference of 20 minutes, each of which passed a threshold of 10%, the quality was the same with the accuracy of the fourth mark.  In the task, over which many teams had been beating for three years, everyone decided some twenty minutes.  The late team (like many others who participated in the competition) was left with nothing, but the competition itself very strongly spurred development in this area. <br><br><h2>  Simple algorithms </h2><br>  To begin, we formalize our problem.  What we have?  We have a set of users 𝑢 ∈ 𝑈, a set of objects 𝑖 ∈ 𝐼 (movies, tracks, products, etc.) and a set of events (𝑟 <sub>𝑢𝑖</sub> , 𝑢, 𝑖, ...) ∈ 𝒟 (actions that users perform with objects).  Each event is set by user 𝑢, object 𝑖, its result 𝑟 <sub>𝑢𝑖</sub> and, possibly, some other characteristics.  We are required to: <br><br><ul><li>  predict preference: <br>  𝑟̂ <sub>𝑢𝑖</sub> = Predict (𝑢, 𝑖, ...) ≈ 𝑟 <sub>𝑢𝑖</sub> . </li><li>  personal recommendations: <br>  𝑢 ⟼ (𝑖 <sub>1</sub> , ..., 𝑖 <sub>𝘒</sub> ) = Recommend (, ...). </li><li>  similar objects: <br>  ⟼ (𝑖 <sub>1</sub> , ..., 𝑖 <sub>𝑀</sub> ) = Similar <sub>𝑀</sub> (). </li></ul><br><h4>  Rating table </h4><br>  Suppose we are given a table with user ratings: <br><br><img src="https://habrastorage.org/files/827/8c7/4dc/8278c74dce8b45e992220121e8a08ae5.png" width="640"><br><br>  You need to predict as best you can what ratings should be in the cells with question marks: <br><br><img src="https://habrastorage.org/files/624/69d/7d9/62469d7d95944cf9b25d4096107a1596.png" width="640"><br><br><h4>  User clustering </h4><br>  Recall that the main idea of ​​collaborative filtering is that similar users usually like similar objects.  Let's start with the simplest method. <br><br><ul><li>  Choose a conditional measure of similarity of users according to their rating history 𝑠𝑖𝑚 (𝑢, 𝑣). </li><li>  We will unite users into groups (clusters) so that similar users will end up in the same cluster: 𝑢 ⟼ 𝐹 (𝑢). </li><li>  The user’s score for an object will be predicted as the average cluster rating for this object: </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/files/ff5/cdd/b59/ff5cddb593bc492c95191ec74b9a7ce2.png" width="250"></div><br><br>  This algorithm has several problems: <br><ul><li>  Nothing to recommend to new / atypical users.  For such users, there is no suitable cluster with similar users. </li><li>  It does not take into account the specificity of each user.  In a sense, we divide all users into some classes (templates). </li><li>  If nobody in the cluster has evaluated the object, then the prediction will fail. </li></ul><br><h4>  User-based </h4><br>  Let's try to slightly improve the previous method and replace hard clustering with the following formula: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/a3e/8e6/a5e/a3e8e6a5efe14fb4b1c7b447673d611c.png" width="450"></div><br><br>  However, this algorithm also has its own problems: <br><ul><li>  Nothing to recommend to new / atypical users.  For these users, we still can not find similar. </li><li>  Cold start - new objects are not recommended to anyone. </li></ul><br><h4>  Item-based </h4><br>  There is an absolutely symmetric algorithm: <br><div style="text-align:center;"><img src="https://habrastorage.org/files/2e2/0a1/021/2e20a10212704e9ea159df97bcdc7aee.png" width="450"></div><br>  In the previous method, we were repelled by the idea that, say, the user would like the movie if his friends liked it.  Here we believe that the user will like the movie if he liked similar movies. <br><br>  Problems: <br><ul><li>  Cold start - new objects are not recommended to anyone. </li><li>  Recommendations are often trivial. </li></ul><br><h4>  Common problems of the listed methods </h4><br>  All of these methods have the following disadvantages: <br><ul><li>  The problem of cold start. </li><li>  Bad predictions for new / atypical users / objects. </li><li>  Trivial recommendations. </li><li>  Resource intensity calculations.  In order to make predictions, we need to keep in mind all the ratings of all users. </li></ul><br><br>  Therefore, we turn to a more complex algorithm, which is almost devoid of these shortcomings. <br><br><h2>  SVD Algorithm </h2><br>  For this algorithm, we need several concepts from linear algebra: vectors, matrices, and operations with them.  I will not give here all the definitions, if you need to refresh this knowledge, then all explanations are in the video of the lecture from about 33 minutes. <br><br>  SVD (Singular Value Decomposition), translated as a singular decomposition of the matrix.  The singular decomposition theorem states that any matrix 𝐀 of size 𝑛 × 𝑚 has a decomposition into a product of three matrices: 𝑈, Ʃ and 𝑉 <sup>𝑇</sup> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/14f/83c/3ec/14f83c3ec960472b84ec2a8102dcfe26.png" width="250"></div><br>  The matrices 𝑈 and 𝑉 are orthogonal, and Ʃ is diagonal (although not square). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/7ce/267/603/7ce2676036fc4d9faef2e6784ec8737c.png" width="400"></div><br>  Moreover, the lambda in the matrix Ʃ will be ordered by non-increasing.  Now we will not prove this theorem, just use the decomposition itself. <br><br>  But what do we have from the fact that we can some kind of matrix in the product of three even more incomprehensible matrices.  For us, the following will be interesting.  In addition to the usual decomposition, it is still truncated, when among the lambdas, only the first 𝑑 numbers remain, and the rest we assume to be zero. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/7f5/6e0/fbc/7f56e0fbcae54627b4971a1976a12391.png" width="300"></div><br>  This is equivalent to the fact that for matrices 𝑈 and 𝑉 we leave only the first <br>  columns, and the matrix Ʃ is cut to square 𝑑 × 𝑑. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/d93/d9a/6e6/d93d9a6e6ca5440884a1a5501555f2f5.png" width="300"></div><br>  So, it turns out that the obtained matrix 𝐀 ′ well approximates the original matrix 𝐀 and, moreover, is the best low-rank approximation in terms of the standard deviation. <br><br>
<h4>  SVD for recommendations </h4><br>  How to use all this for recommendations?  We had a matrix, we decomposed it into a product of three matrices.  What is not exactly laid out, and approximately.  Simplify everything a bit by designating the product of the first two matrices for a single matrix: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/1d5/8ad/5fe/1d58ad5fe6a44bffaff537491f4cb458.png" width="500"></div><br>  Now let's digress a little from all these matrices and concentrate on the resulting algorithm: to predict the user's rating 𝑈 for the movie , we take some vector 𝑝 <sub>𝑢</sub> (set of parameters) for this user and vector for this movie <sub>𝑖</sub> .  Their scalar product will be the prediction we need: 𝑟̂ <sub>𝑢𝑖</sub> = ⟨𝑝 <sub>𝑢</sub> , 𝑞 <sub>𝑖⟩</sub> . <br><br>  The algorithm is quite simple, but it gives surprising results.  It does not just allow us to predict estimates.  Using it, we can identify hidden signs of objects and user interests by user history.  For example, it may happen that at the first coordinate of the vector each user will have a number indicating whether the user looks more like a boy or a girl, the second coordinate is a number reflecting the approximate age of the user.  At the same film, the first coordinate will show whether it is more interesting to boys or girls, and the second one - to what age group of users it is interesting. <br><br>  But not everything is so simple.  There are a few problems.  First, the estimation matrix  is not completely known to us, so we cannot simply take its SVD decomposition.  Secondly, the SVD decomposition is not unique: (𝑈Ω) Ʃ (𝑉Ω) = 𝑈Ʃ𝑉 <sub>𝑇</sub> , so even if we find at least some decomposition, it is unlikely that the first coordinate in it will correspond to the gender of the user, and the second will be the age. <br><br><h4>  Training </h4><br>  Let's try to deal with the first problem.  Here we need machine learning.  So, we cannot find the SVD decomposition of the matrix, since  we do not know the matrix itself.  But we want to take advantage of this idea and come up with a prediction model that will work in a manner similar to SVD.  Our model will depend on many parameters - vectors of users and movies.  For the given parameters, in order to predict the estimate, we take the user vector, the vector of the film and get their scalar product: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/143/596/3b2/1435963b2d4e4a398eeef577b4fb6500.png" width="350"></div><br>  But since we do not know vectors, they still need to be obtained.  The idea is that we have user ratings with which we can find the optimal parameters for which our model would predict these ratings as best we can: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/940/10b/1f1/94010b1f14a740708c9c10bbe1af6006.png" width="350"></div><br>  So, we want to find such parameters θ so that the error square is as small as possible.  But there is a paradox here: we want to make less mistakes <b>in the future</b> , but we do not know what estimates we will ask.  Accordingly, we cannot optimize this.  But we already know the ratings given by users.  Let's try to choose the parameters so that on those estimates that we already have, the error was as small as possible.  In addition, we add another term - the regularizer. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/0e1/f87/cc7/0e1f87cc71e743c19b6767ccfa1c6ba4.png" width="550"></div><br><h4>  Why do we need a regularizer? </h4><br>  Regularization is needed to combat retraining - a phenomenon when the constructed model well explains the examples from the training set, but it does not work well with examples that did not participate in the training.  In general, there are several methods to combat retraining; I would like to mention two of them.  First, you need to choose simple models.  The simpler the model, the better it summarizes the future data (this is similar to the well-known principle of Occam's razor).  And the second method is just regularization.  When we set up a model for a training set, we optimize the error.  Regularization is that we optimize not just an error, but an error plus some function of the parameters (for example, the norm of the vector of parameters).  This allows you to limit the size of the parameters in the solution, reduces the degree of freedom of the model. <br><br><h4>  Numerical optimization </h4><br>  How do we find the optimal parameters?  We need to optimize this functionality: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/3ad/e04/2f9/3ade042f977840c1a4b0012b83adbb44.png" width="550"></div><br>  There are many parameters: for each user, for each object we have our own vector, which we want to optimize.  We have a function depending on a large number of variables.  How to find her minimum?  Here we need a gradient - a vector of partial derivatives for each parameter. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/616/58a/75d/61658a75d1fb49eea49b9c22d9e58111.png" width="400"></div><br>  Gradient is very convenient to visualize.  The illustration shows the surface: a function of two variables.  For example, the altitude above sea level.  Then the gradient at any particular point is a vector directed in the direction where our function grows the most.  And if you run water from this point, it will flow in the direction opposite to the gradient. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/e9d/223/a45/e9d223a458b44d7ea6f12529ba699566.png" width="500"></div><br>  The most well-known method for optimizing functions is gradient descent.  Suppose we have a function of many variables, we want to optimize it.  We take some initial value, and then we look where we can move in order to minimize this value.  The gradient descent method is an iterative algorithm: it repeatedly takes the parameters of a certain point, looks at the gradient and steps against its direction: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/e96/04e/8c4/e9604e8c49f9410f921b5ba781df7b8a.png" width="350"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/files/b32/e57/d15/b32e57d152004364b4553002c4e911a5.png" width="500"></div><br>  The problems with this method lie in the fact that, firstly, in our case, it works very slowly and, secondly, it finds local, rather than global, minima.  The second problem is not so terrible for us, because  In our case, the value of the functional in local minima is close to the global optimum. <br><br><h4>  Alternating least squares </h4><br>  However, the gradient descent method is not always necessary.  For example, if we need to calculate the minimum for a parabola, there is no need to act by this method, we know for sure where its minimum is.  It turns out that the functionality that we are trying to optimize - the sum of the squares of errors plus the sum of the squares of all parameters - is also a quadratic functional, it is very similar to a parabola.  For each specific parameter, if we fix all the others, it will be just a parabola.  Those.  at least one coordinate we can accurately determine.  The Alternating Least Squares method is based on this consideration.  I will not dwell on it in detail.  Let me just say that in it we alternately accurately find minima in one coordinate or another: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/186/bf2/c22/186bf2c227e9426fb0034a1ef5d75ee9.png" width="500"></div><br>  We fix all the parameters of the objects, optimize exactly the parameters of the users, further fix the parameters of the users and optimize the parameters of the objects.  We act iteratively: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/3e2/5de/2ae/3e25de2aee7b4b15a05a8b76ee1606a1.png" width="400"></div><br>  It all works quickly enough, with each step can be parallelized. <br><br><h2>  Measuring the quality of recommendations </h2><br>  If we want to improve the quality of recommendations, we need to learn how to measure it.  For this, an algorithm trained on one sample — a training one — is tested on another - a test one.  Netflix suggested measuring the quality of recommendations for the RMSE metric: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/4f6/550/c16/4f6550c162ed4e8fbc277c5136c5cde3.png" width="400"></div><br>  Today it is the standard metric for predicting an estimate.  However, it has its drawbacks: <br><br><ul><li>  Each user has their own idea of ​​the rating scale.  Users who have a wider scatter of ratings will have a greater influence on the value of the metric than others. </li><li>  A mistake in predicting a high score has the same weight as a mistake in predicting a low score.  At the same time, it is more terrible to predict a rating of 9 instead of a real assessment of 7 than to predict 4 instead of 2 (on a ten-point scale). </li><li>  You can have an almost perfect RMSE metric, but have very poor ranking quality, and vice versa. </li></ul><br><br><h4>  Ranking metrics </h4><br>  There are other metrics - ranking metrics, for example, based on completeness and accuracy.  Let 𝑹 be the set of recommended objects, 𝑷 the set of objects that the user will actually like. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/f35/b22/3be/f35b223be37249ebaa86848742c9667b.png" width="300"></div><br>  There are also problems here: <br><br><ul><li>  There is no data about the recommended objects that the user has not rated. </li><li>  Optimizing these metrics directly is almost impossible. </li></ul><br><br><h4>  Other recommendation properties </h4><br>  It turns out that the perception of recommendations is influenced not only by the quality of ranking, but also by some other characteristics.  Among them, for example, variety (you should not give the user movies only on one topic or from one series), surprise (if you recommend very popular movies, such recommendations will be too banal and almost useless), novelty (many people like classic films, but recommendations usually used to discover something new) and many others. <br><br><h4>  Similar objects </h4><br>  Similarity of objects is not such an obvious thing.  There are different approaches to this task: <br><br><ul><li>  Similar objects are objects that are similar in their characteristics (content-based). </li><li>  Similar objects are objects that are often used together (“customers who bought 𝑖 also bought”). </li><li>  Similar objects are recommendations to the user who liked this object. </li><li>  Similar objects are simply guidelines in which the object acts as a context. </li></ul><br><br><h2>  Development directions </h2><br><h4>  Conceptual issues </h4><br><ul><li>  How to build lists of recommendations based on predictions?  (Plain top is not always the best idea.) </li><li>  How to improve the quality of recommendations, not predictions? </li><li>  How to measure the similarity of objects? </li><li>  How to justify the recommendations?  (It turns out that users perceive recommendations much better if they provide them with some explanations.) </li></ul><br><br><h4>  Technical issues </h4><br><ul><li>  How to solve the cold start problem for new users and new objects? </li><li>  How to quickly update the recommendations?  (The benefits of introducing recommendations into your service may depend on this.) </li><li>  How to quickly find objects with the highest prediction? </li><li>  How to measure the quality of online recommendations?  (The traditional approach with training and test samples may not work very well.) </li><li>  How to scale the system? </li></ul><br><br><h4>  How to consider additional information? </h4><br><ul><li>  How to take into account not only <em>explicit</em> , but also implicit feedback?  (Implicit feedback is often orders of magnitude greater.) </li><li>  How to take into account the context?  <em>(Context-aware recommendations)</em> </li><li>  How to take into account the signs of objects?  <em>(Hybrid systems)</em> </li><li>  How to take into account the connection between objects?  <em>(taxonomy)</em> </li><li>  How to take into account signs and user connections? </li><li>  How to account for information from other sources and subject areas?  <em>(Cross-domain recommendations)</em> </li></ul><br><br><h2>  Literature </h2><br><ul><li>  <em>Introduction to Algebra</em> , Kostrikin A.I. </li><li>  <em>Mathematical analysis</em> , Zorich V. A. </li><li>  <em><a href="">Machine learning</a></em> , Vorontsov KV, lecture course </li></ul></div><p>Source: <a href="https://habr.com/ru/post/241455/">https://habr.com/ru/post/241455/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../241443/index.html">Dnipropetrovsk Ciklum Speakers' Corner “Intro to WebGL with THREE.js (based on Source's“ Leaderboard in Reception ”)”, October 30</a></li>
<li><a href="../241445/index.html">Kharkiv Ciklum Speakers' Corner "Showcase: Manage Project with 100+ Engineers - Practices and Implementations" with Oleg Kupin, October 29</a></li>
<li><a href="../241447/index.html">Translation of hours in Russia on October 26 and icu4c</a></li>
<li><a href="../241449/index.html">Mathematical drawings</a></li>
<li><a href="../241453/index.html">Simple OneDrive client - it's not that simple</a></li>
<li><a href="../241457/index.html">In which countries does the Internet “sleep” at night?</a></li>
<li><a href="../241459/index.html">Statistics of Russian IT-specialists on stackoverflow.com and github.com</a></li>
<li><a href="../241461/index.html">Something Better A / B Testing</a></li>
<li><a href="../241463/index.html">You want a delicious beer, Arduino to help</a></li>
<li><a href="../241465/index.html">Advanced use of objects in javascript</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>