<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Optimization of garbage collection in a highly loaded .NET service</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Every day tens of thousands of employees from several thousand organizations around the world work in the Pyrus service. Responsiveness of the service...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Optimization of garbage collection in a highly loaded .NET service</h1><div class="post__text post__text-html js-mediator-article">  Every day tens of thousands of employees from several thousand organizations around the world work in the Pyrus service.  Responsiveness of the service (speed of processing requests), we consider an important competitive advantage, since it directly affects the impression of users.  The key metric for us is "the percentage of slow queries."  Studying its behavior, we noticed that once a minute there are pauses about 1000 ms long on the application servers.  During these intervals, the server does not respond and a queue of several dozen requests appears.  The search for the causes and the elimination of bottlenecks caused by garbage collection in the application will be discussed in this article. <br><br><img src="https://habrastorage.org/webt/fu/1s/j9/fu1sj9ixpj4nc633ikhwblbhlfs.jpeg"><br><a name="habracut"></a><br>  Modern programming languages ‚Äã‚Äãcan be divided into two groups.  In languages ‚Äã‚Äãlike C / C ++ or Rust, manual memory management is used, so programmers spend more time writing code, managing the lifetime of objects, and then debugging.  At the same time, bugs due to memory misuse are among the most difficult to debug, so most modern development is done in languages ‚Äã‚Äãwith automatic memory management.  These include, for example, Java, C #, Python, Ruby, Go, PHP, JavaScript, etc.  Programmers save development time, but they have to pay for this with additional execution time, which the program regularly spends on garbage collection ‚Äî freeing up memory occupied by objects to which the program has no links left.  In small programs, this time is negligible, but as the number of objects grows and the intensity of their creation increases, garbage collection begins to make a noticeable contribution to the total program execution time. <br><br>  Pyrus web servers run on the .NET platform, which uses automatic memory management.  Most garbage collections are blocking ('stop the world'), i.e.  for the time of their work, stop all threads (threads) of the application.  Non-blocking (background) assemblies actually also stop all threads, but for a very short period of time.  During the blocking of threads, the server does not process requests, existing requests hang, new ones are added to the queue.  As a result, requests that were processed at the moment of garbage collection are directly slowed down, requests are also slower executed immediately after the garbage collection is completed due to the accumulated queues.  This impairs the percentage of slow queries. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Armed with the recently published book <a href="https://www.amazon.com/Pro-NET-Memory-Management-Performance/dp/148424026X">Konrad Kokosa: Pro .NET Memory Management</a> (about how we brought its first copy to Russia in 2 days, you can write a separate post), entirely devoted to the topic of memory management in .NET, we began to study the problem. <br><br><h2>  Measurement </h2><br>  For profiling the Pyrus web server, we used the PerfView utility ( <a href="https://github.com/Microsoft/perfview">https://github.com/Microsoft/perfview</a> ), customized for profiling .NET applications.  The utility is based on the Event Tracing for Windows (ETW) mechanism and has minimal impact on the performance of the profiled application, which allows it to be used on the combat server.  In addition, the performance impact depends on what types of events and what information we collect.  We do not collect anything - the application works as usual.  PerfView also does not require any recompilation or application restart. <br><br>  Run the PerfView trace with the / GCCollectOnly parameter (trace time 1.5 hours).  In this mode, it collects only garbage collection events and has minimal impact on performance.  Let's look at the Memory Group / GCStats trace report, and the summary of the garbage collector events in it: <br><br><img src="https://habrastorage.org/webt/v4/ia/cd/v4iacdyso10-0toycwyijfm0zbm.png"><br><br>  Here we see several interesting indicators: <br><ul><li>  The average pause time of the assembly in the 2nd generation is 700 milliseconds, and the maximum pause is about a second.  This figure shows the time for which all threads in the .NET application are stopped; in particular, this pause will be added to all processed requests. <br></li><li>  The number of assemblies of the 2nd generation is comparable to the 1st generation and only slightly less than the number of assemblies of the 0th generation. <br></li><li>  The Induced column indicates 53 assemblies in the 2nd generation.  Induced assembly is the result of an explicit call to GC.Collect ().  In our code, we did not find a single call to this method, which means that some of the libraries used by our application are to blame. <br></li></ul><br>  We explain the observation about the number of garbage collections.  The idea of ‚Äã‚Äãdividing objects by their lifetime is based on the <a href="https://www.quora.com/What-is-the-generational-hypothesis-in-the-context-of-garbage-collection">generational hypothesis hypothesis</a> : a significant part of the created objects dies quickly, and most of the others live for a long time (in other words, few objects have an ‚Äúaverage‚Äù lifetime).  It is under this mode that the .NET garbage collector is sharpened, and in this mode, the second generation assemblies should be much less than the 0th generation.  That is, for optimal work of the garbage collector, we must adjust the work of our application to the hypothesis of generations.  We formulate the rule as follows: objects must either die quickly, not living to the older generation, or live to it and live there forever.  This rule also applies to other platforms that use automatic memory management with separation by generations, such as Java, for example. <br><br>  Interesting data for us can be extracted from another table in the GCStats report: <br><br><img src="https://habrastorage.org/webt/m5/7y/je/m57yjedgbkwfpbiwmjkvnbhgl4o.png"><br><br>  Here are the cases when an application tries to create a large object (in the .NET Framework objects with a size of&gt; 85000 bytes are created in LOH - Large Object Heap), and it has to wait until the end of the 2nd generation assembly, which occurs in parallel in the background.  These allocator pauses are not as critical as garbage collector pauses, since they only affect one stream.  Prior to this, we used the .NET Framework version 4.6.1, and in version 4.7.1 Microsoft finalized the garbage collector, now it allows you to allocate memory in the Large Object Heap during the background generation of the 2nd generation: <a href="https://docs.microsoft.com/ru-ru/dotnet/framework/whats-new/">https://docs.microsoft.com / en-us / dotnet / framework / whats-new / # common-language-runtime-clr</a> <br>  Therefore, we have updated to the latest at that time version 4.7.2. <br><br><h2>  2nd generation builds </h2><br>  Why do we have so many older assemblies?  The first assumption is that we have a memory leak.  To test this hypothesis, we look at the size of the second generation (we configured monitoring of the corresponding performance counters in Zabbix).  From the graphs of the size of the 2nd generation for 2 Pyrus servers, it is clear that its size first grows (mainly due to filling caches), but then stabilizes (large dips in the graph - a regular restart of the web service for updating the version): <br><br><img src="https://habrastorage.org/webt/gg/lc/ce/gglcce4tssnhzgcjfhesec9rcja.png"><br><br>  This means that there are no noticeable memory leaks, that is, a large number of assemblies of the 2nd generation arise for another reason.  The next hypothesis is a large memory traffic, i.e., many objects get into the 2nd generation, and many objects die there.  To find such objects in PerfView there is a / GCOnly mode.  From the trace reports, look at the 'Gen 2 Object Deaths (Coarse Sampling) Stacks', which contains a selection of objects dying in the 2nd generation, along with the call stacks of the places where these objects were created.  Here we see the following results: <br><br><img src="https://habrastorage.org/webt/h7/r2/d0/h7r2d0htyxsnaqrr_ekn_ybilti.png"><br><br>  Having opened the line, inside we see a stack of calls for those places in the code that create objects that live to the 2nd generation.  Among them: <br><ul><li>  System.Byte [] If you look inside, we will see that more than half are serialization buffers in JSON: <br></li></ul><br><img src="https://habrastorage.org/webt/la/up/6v/laup6v0mho5e1tbwjfkfmsgdhog.png"><br><br><ul><li>  Slot [System.Int32] [] (this is part of the HashSet implementation), System.Int32 [], etc.  This is our code that computes client caches ‚Äî those directories, forms, lists, friends, etc. that a given user sees and that are cached by him in a browser or in a mobile application: <br></li></ul><br><img src="https://habrastorage.org/webt/dx/et/jy/dxetjyvj2ande72qrod6leza6i8.png"><br><br><img src="https://habrastorage.org/webt/v6/k6/r-/v6k6r-wq0qeof0edb6h5jvct-he.png"><br><br>  Interestingly, buffers for JSON and for computing client caches are all temporary objects that live for a single request.  Why do they live to the 2nd generation?  Note that all these objects are arrays of rather large size.  And with a size of&gt; 85,000 bytes, the memory for them is allocated in the Large Object Heap, which is assembled only with the 2nd generation. <br><br>  To check, we will open the section 'GC Heap Alloc Ignore Free (Coarse Sampling) stacks' in the perfview / GCOnly results.  There we see the LargeObject line, in which PerfView groups the creation of large objects, and inside we will see all the same arrays that we saw in the previous analysis.  We confirm the root cause of problems with the garbage collector: we create many temporary large objects. <br><br><img src="https://habrastorage.org/webt/sy/kr/lk/sykrlkgbmvl9jyny5hl1_ftg4ee.png"><br><br><img src="https://habrastorage.org/webt/f9/6q/mp/f96qmplnj4devma1buedg6fpo8q.png"><br><br><h2>  Pyrus system changes </h2><br>  According to the measurement results, we identified the main directions of further work: the fight against large objects in the calculation of client caches and serialization in JSON.  There are several solutions to this problem: <br><ul><li>  The simplest thing is not to create large objects.  For example, if a large buffer B is used in consecutive data transformations A-&gt; B-&gt; C, then sometimes these transformations can be combined, turning into A-&gt; C, and getting rid of the creation of object B. This option is not always applicable, but it the easiest and most effective. <br></li><li>  Object pool  Instead of constantly creating new objects and throwing them out, loading the garbage collector, we can store a collection of free objects.  In the simplest case, when we need a new object, then we take it from the pool, or create a new one if the pool is empty.  When the object is no longer needed, we return it to the pool.  A good example is ArrayPool in .NET Core, which is also available in the .NET Framework as part of the System.Buffers Nuget package. <br></li><li>  Use small instead of large objects. <br></li></ul><br>  Consider separately both cases of large objects - the calculation of client caches and serialization in JSON. <br><br><h2>  Calculating client caches </h2><br>  The Pyrus web client and mobile applications cache data that is available to the user (projects, forms, users, etc.). Caching is used to speed up work, it is also necessary for working in offline mode.  Caches are calculated on the server and transferred to the client.  They are individual for each user, as they depend on their access rights, and are frequently updated, for example, when the directories to which he has access are changed. <br><br>  Thus, on the server a lot of client cache computations regularly occur, and many temporary short-lived objects are created.  If the user is in a large organization, he can get access to many objects, respectively, client caches for him will be large.  That is why we saw memory allocation for large temporary arrays in the Large Object Heap. <br><br>  Let us analyze the proposed options for eliminating the creation of large objects: <br><ul><li>  Full disposal of large objects.  This approach is inapplicable, since the data preparation algorithms use, among other things, sorting and union of sets, and they require temporary buffers. <br></li><li>  Using a pool of objects.  This approach has difficulties: <br><ul><li>  A variety of used collections and types of elements in them: HashSet, List and Array are used (the latter two can be combined).  The collections contain Int32, Int64, as well as all sorts of data classes.  For each type used, you will need your own pool, which will also store collections of different sizes. <br></li><li>  Difficult time of life collections.  To take advantage of the pool, objects will have to be returned to it after use.  This can be done if the object is used in the same method.  But in our case, the situation is more complicated, since many large objects travel between methods, put into data structures, are shifted to other structures, etc. <br></li><li>  Implementation.  There is an ArrayPool from Microsoft, but we also need a List and HashSet.  We did not find any suitable library, so classes would have to be implemented by ourselves. </li></ul></li><li>  Use small objects.  A large array can be divided into several small pieces, which I will not load with the Large Object Heap, but will be created in the 0th generation, and then proceed along the standard path in the 1st and 2nd.  We hope that they will not live to the 2nd, but will be collected by the garbage collector in the 0th, or at least in the 1st generation.  The advantage of this approach is that the changes to the existing code are minimal.  Difficulties: <br><ul><li>  Implementation.  We did not find suitable libraries, so classes would have to be written by ourselves.  The absence of libraries is understandable, since the ‚Äúcollections that do not load the Large Object Heap‚Äù scenario is a very narrow scope. </li></ul></li></ul><br>  We decided to go along the 3rd path and <strike>invent our bicycle</strike> to write a List and HashSet, not loading the Large Object Heap. <br><br><h2>  Piece list </h2><br>  Our ChunkedList &lt;T&gt; implements standard interfaces, including IList &lt;T&gt;, thanks to which minimal changes to the existing code are required.  And the Newtonsoft.Json library used by us is automatically able to serialize it, since it implements IEnumerable &lt;T&gt;: <br><br><pre><code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">sealed</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">ChunkedList</span></span>&lt;<span class="hljs-title"><span class="hljs-title">T</span></span>&gt; : <span class="hljs-title"><span class="hljs-title">IList</span></span>&lt;<span class="hljs-title"><span class="hljs-title">T</span></span>&gt;, <span class="hljs-title"><span class="hljs-title">ICollection</span></span>&lt;<span class="hljs-title"><span class="hljs-title">T</span></span>&gt;, <span class="hljs-title"><span class="hljs-title">IEnumerable</span></span>&lt;<span class="hljs-title"><span class="hljs-title">T</span></span>&gt;, <span class="hljs-title"><span class="hljs-title">IEnumerable</span></span>, <span class="hljs-title"><span class="hljs-title">IList</span></span>, <span class="hljs-title"><span class="hljs-title">ICollection</span></span>, <span class="hljs-title"><span class="hljs-title">IReadOnlyList</span></span>&lt;<span class="hljs-title"><span class="hljs-title">T</span></span>&gt;, <span class="hljs-title"><span class="hljs-title">IReadOnlyCollection</span></span>&lt;<span class="hljs-title"><span class="hljs-title">T</span></span>&gt; {</code> </pre> <br>  The standard List &lt;T&gt; has the following fields: an array for the elements and the number of filled elements.  In ChunkedList &lt;T&gt; there is an array of arrays of elements, the number of fully populated arrays, the number of elements in the last array.  Each of the arrays of elements with less than 85000 bytes: <br><br><img src="https://habrastorage.org/webt/72/zj/js/72zjjs9q6lcfud-l7nq8cy5prdi.png"><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> T[][] chunks; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> currentChunk; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> currentChunkSize;</code> </pre> <br>  Since ChunkedList &lt;T&gt; is rather complicated, we wrote detailed tests for it.  Any operation should be tested at least in 2 modes: in the ‚Äúsmall‚Äù, when the whole list fits in one piece up to 85000 bytes in size, and ‚Äúlarge‚Äù when it consists of more than one piece.  Moreover, for methods that change the size (for example, Add), there are even more scenarios: ‚Äúsmall‚Äù -&gt; ‚Äúsmall‚Äù, ‚Äúsmall‚Äù -&gt; ‚Äúlarge‚Äù, ‚Äúlarge‚Äù -&gt; ‚Äúlarge‚Äù, ‚Äúlarge‚Äù -&gt; ‚Äú little".  There are quite a few confusing boundary cases with which unit tests do a good job. <br><br>  The situation is simplified by the fact that some of the methods from the IList interface are not used, and they can not be implemented (such as Insert, Remove).  Their implementation and testing would be quite expensive.  In addition, the writing of unit tests is simplified by the fact that we do not need to invent a new functionality, ChunkedList &lt;T&gt; should behave in the same way as List &lt;T&gt;.  That is, all the tests are arranged as follows: create a List &lt;T&gt; and ChunkedList &lt;T&gt;, perform the same operations on them and compare the results. <br><br>  We measured performance using the BenchmarkDotNet library to make sure that we didn‚Äôt slow down our code too much when switching from List &lt;T&gt; to ChunkedList &lt;T&gt;.  Test, for example, adding items to the list: <br><br><pre> <code class="cs hljs">[<span class="hljs-meta"><span class="hljs-meta">Benchmark</span></span>] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> ChunkedList&lt;</span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">int</span></span></span><span class="hljs-function">&gt; </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">ChunkedList</span></span></span><span class="hljs-function">(</span><span class="hljs-params"></span><span class="hljs-function"><span class="hljs-params"></span>)</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> list = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ChunkedList&lt;<span class="hljs-keyword"><span class="hljs-keyword">int</span></span>&gt;(); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; N; i++) list.Add(i); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> list; }</code> </pre> <br>  And the same test using List &lt;T&gt; for comparison.  Results when adding 500 elements (everything fits into one array): <br><div class="scrollable-table"><table><tbody><tr><td>  Method </td><td>  Mean </td><td>  Error </td><td>  Stddev </td><td>  Gen 0 / 1k Op </td><td>  Gen 1 / 1k Op </td><td>  Gen 2 / 1k Op </td><td>  Allocated Memory / Op </td></tr><tr><td>  StandardList </td><td>  1.415 us </td><td>  0.0149 us </td><td>  0.0140 us </td><td>  0.6847 </td><td>  0.0095 </td><td>  - </td><td>  4.21 KB </td></tr><tr><td>  ChunkedList </td><td>  3.728 us </td><td>  0.0238 us </td><td>  0.0222 us </td><td>  0.6943 </td><td>  0.0076 </td><td>  - </td><td>  4.28 KB </td></tr></tbody></table></div><br>  Results when adding 50,000 items (split into several arrays): <br><div class="scrollable-table"><table><tbody><tr><td>  Method </td><td>  Mean </td><td>  Error </td><td>  Stddev </td><td>  Gen 0 / 1k Op </td><td>  Gen 1 / 1k Op </td><td>  Gen 2 / 1k Op </td><td>  Allocated Memory / Op </td></tr><tr><td>  StandardList </td><td>  146.273 us </td><td>  3.1466 us </td><td>  4.8053 us </td><td>  124.7559 </td><td>  124.7559 </td><td>  124.7559 </td><td>  513.23 KB </td></tr><tr><td>  ChunkedList </td><td>  287.687 us </td><td>  1.4630 us </td><td>  1.2969 us </td><td>  41.5039 </td><td>  20.5078 </td><td>  - </td><td>  256.75 KB </td></tr></tbody></table></div><br><div class="spoiler">  <b class="spoiler_title">Detailed description of the columns in the results</b> <div class="spoiler_text"><pre> <code class="cs hljs">BenchmarkDotNet=v0<span class="hljs-number"><span class="hljs-number">.11</span></span><span class="hljs-number"><span class="hljs-number">.4</span></span>, OS=Windows <span class="hljs-number"><span class="hljs-number">10.0</span></span><span class="hljs-number"><span class="hljs-number">.17763</span></span><span class="hljs-number"><span class="hljs-number">.379</span></span> (<span class="hljs-number"><span class="hljs-number">1809</span></span>/October2018Update/Redstone5) Intel Core i7<span class="hljs-number"><span class="hljs-number">-8700</span></span>K CPU <span class="hljs-number"><span class="hljs-number">3.70</span></span>GHz (Coffee Lake), <span class="hljs-number"><span class="hljs-number">1</span></span> CPU, <span class="hljs-number"><span class="hljs-number">12</span></span> logical and <span class="hljs-number"><span class="hljs-number">6</span></span> physical cores [Host] : .NET Framework <span class="hljs-number"><span class="hljs-number">4.7</span></span><span class="hljs-number"><span class="hljs-number">.2</span></span> (CLR <span class="hljs-number"><span class="hljs-number">4.0</span></span><span class="hljs-number"><span class="hljs-number">.30319</span></span><span class="hljs-number"><span class="hljs-number">.42000</span></span>), <span class="hljs-number"><span class="hljs-number">64b</span></span>it RyuJIT-v4<span class="hljs-number"><span class="hljs-number">.7</span></span><span class="hljs-number"><span class="hljs-number">.3324</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span> DefaultJob : .NET Framework <span class="hljs-number"><span class="hljs-number">4.7</span></span><span class="hljs-number"><span class="hljs-number">.2</span></span> (CLR <span class="hljs-number"><span class="hljs-number">4.0</span></span><span class="hljs-number"><span class="hljs-number">.30319</span></span><span class="hljs-number"><span class="hljs-number">.42000</span></span>), <span class="hljs-number"><span class="hljs-number">64b</span></span>it RyuJIT-v4<span class="hljs-number"><span class="hljs-number">.7</span></span><span class="hljs-number"><span class="hljs-number">.3324</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span> <span class="hljs-comment"><span class="hljs-comment">// * Hints * Outliers ListAdd.StandardList: Default -&gt; 2 outliers were removed ListAdd.ChunkedList: Default -&gt; 1 outlier was removed // * Legends * Mean : Arithmetic mean of all measurements Error : Half of 99.9% confidence interval StdDev : Standard deviation of all measurements Gen 0/1k Op : GC Generation 0 collects per 1k Operations Gen 1/1k Op : GC Generation 1 collects per 1k Operations Gen 2/1k Op : GC Generation 2 collects per 1k Operations Allocated Memory/Op : Allocated memory per single operation (managed only, inclusive, 1KB = 1024B) 1 us : 1 Microsecond (0.000001 sec)</span></span></code> </pre> <br></div></div><br>  If you look at the 'Mean' column, in which the average test run time is displayed, you can see that our implementation is slower than the standard by only 2-2.5 times.  Taking into account the fact that in a real code, operations with lists are only a small part of all performed actions, this difference becomes insignificant.  But the column 'Gen 2 / 1k op' (the number of assemblies of the 2nd generation for 1000 test runs) shows that we achieved the goal: with a large number of elements, ChunkedList does not create garbage in the 2nd generation, which was our task. <br><br><h2>  Piece set </h2><br>  Similarly, ChunkedHashSet &lt;T&gt; implements the ISet &lt;T&gt; interface.  When writing ChunkedHashSet &lt;T&gt; we reused the logic of breaking into small pieces, already implemented in ChunkedList.  To do this, we took a ready-made implementation of the HashSet &lt;T&gt; from the .NET Reference Source, which is available under the MIT license, and replaced the arrays with ChunkedList. <br><br>  In unit tests, we will also use the same trick as for the lists: we will compare the behavior of ChunkedHashSet &lt;T&gt; with the reference HashSet &lt;T&gt;. <br><br>  Finally, performance tests.  The main operation that we use is the union of sets, therefore we will test it: <br><br><pre> <code class="cs hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> ChunkedHashSet&lt;</span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">int</span></span></span><span class="hljs-function">&gt; </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">ChunkedHashSet</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params">[][] source</span></span></span><span class="hljs-function">)</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ChunkedHashSet&lt;<span class="hljs-keyword"><span class="hljs-keyword">int</span></span>&gt;(); <span class="hljs-keyword"><span class="hljs-keyword">foreach</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">var</span></span> arr <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> source) <span class="hljs-keyword"><span class="hljs-keyword">set</span></span>.UnionWith(arr); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">set</span></span>; }</code> </pre> <br>  And exactly the same test for the standard HashSet.  The first test for small sets: <br><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">var</span></span> source = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span>[][] { Enumerable.Range(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">300</span></span>).ToArray(), Enumerable.Range(<span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">600</span></span>).ToArray(), Enumerable.Range(<span class="hljs-number"><span class="hljs-number">300</span></span>, <span class="hljs-number"><span class="hljs-number">1000</span></span>).ToArray(), }</code> </pre> <br><div class="scrollable-table"><table><tbody><tr><td>  Method </td><td>  Mean </td><td>  Error </td><td>  Stddev </td><td>  Gen 0 / 1k Op </td><td>  Gen 1 / 1k Op </td><td>  Gen 2 / 1k Op </td><td>  Allocated Memory / Op </td></tr><tr><td>  StandardHashSet </td><td>  30.16 us </td><td>  0.1046 us </td><td>  0.0979 us </td><td>  9.3079 </td><td>  1.6785 </td><td>  - </td><td>  57.41 KB </td></tr><tr><td>  ChunkedHashSet </td><td>  73.54 us </td><td>  0.5919 us </td><td>  0.5247 us </td><td>  9.5215 </td><td>  1.5869 </td><td>  - </td><td>  58.84 KB </td></tr></tbody></table></div><br>  The second test for large sets that caused a problem with a bunch of large objects: <br><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">var</span></span> source = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span>[][] { Enumerable.Range(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">30000</span></span>).ToArray(), Enumerable.Range(<span class="hljs-number"><span class="hljs-number">10000</span></span>, <span class="hljs-number"><span class="hljs-number">60000</span></span>).ToArray(), Enumerable.Range(<span class="hljs-number"><span class="hljs-number">30000</span></span>, <span class="hljs-number"><span class="hljs-number">100000</span></span>).ToArray(), }</code> </pre> <br><div class="scrollable-table"><table><tbody><tr><td>  Method </td><td>  Mean </td><td>  Error </td><td>  Stddev </td><td>  Gen 0 / 1k Op </td><td>  Gen 1 / 1k Op </td><td>  Gen 2 / 1k Op </td><td>  Allocated Memory / Op </td></tr><tr><td>  StandardHashSet </td><td>  3,031.30 us </td><td>  32.0797 us </td><td>  28.4378 us </td><td>  699.2188 </td><td>  667.9688 </td><td>  664.0625 </td><td>  4718.23 KB </td></tr><tr><td>  ChunkedHashSet </td><td>  7,189.66 us </td><td>  25.6319 us </td><td>  23.9761 us </td><td>  539.0625 </td><td>  265.6250 </td><td>  7.8125 </td><td>  3280.71 KB </td></tr></tbody></table></div><br>  Results are similar with lists.  ChunkedHashSet is 2-2.5 times slower, but at the same time on large sets the 2nd generation loads 2 orders of magnitude less. <br><br><h2>  JSON serialization </h2><br>  The Pyrus web server provides several APIs that use different serialization.  We found the creation of large objects in the API used by bots and the synchronization utility (hereinafter the Public API).  Note that the main API uses its own serialization, which is not affected by this problem.  We wrote about this in the article <a href="https://habr.com/ru/post/227595/">https://habr.com/ru/en/post/227595/</a> , in the section ‚Äú2.  You don‚Äôt know where your application‚Äôs bottleneck is. ‚Äù  That is, the main API is already working well, and the problem manifested itself in the Public API as the number of requests and the amount of data in the responses grew. <br><br>  We are going to optimize the Public API.  For example, the main API, we know that you can return the response to the user in streaming mode.  That is, it is necessary not to create intermediate buffers containing the entire answer, but to write the response immediately to the stream. <br><br>  Upon closer inspection, we found that in the process of serializing the response, we create a temporary buffer for the intermediate result ('content' is an array of bytes containing JSON in UTF-8 encoding): <br><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">var</span></span> serializer = Newtonsoft.Json.JsonSerializer.Create(...); <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] content; <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> sw = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> StreamWriter(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> MemoryStream(), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> UTF8Encoding(<span class="hljs-literal"><span class="hljs-literal">false</span></span>)); <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">var</span></span> writer = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Newtonsoft.Json.JsonTextWriter(sw)) { serializer.Serialize(writer, result); writer.Flush(); content = ms.ToArray(); }</code> </pre> <br>  Let's trace where content is used.  For historical reasons, the Public API is based on WCF, for which XML is the standard request and response format.  In our case, there is a single 'Binary' element in the XML response, inside of which JSON encoded in Base64 is written: <br><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">RawBodyWriter</span></span> : <span class="hljs-title"><span class="hljs-title">BodyWriter</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">readonly</span></span> <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] _content; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">RawBodyWriter</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">byte</span></span></span></span><span class="hljs-function"><span class="hljs-params">[] content</span></span></span><span class="hljs-function">) : </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">base</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-literal"><span class="hljs-function"><span class="hljs-params"><span class="hljs-literal">true</span></span></span></span></span><span class="hljs-function">)</span></span> { _content = content; } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">protected</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">override</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">OnWriteBodyContents</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">XmlDictionaryWriter writer</span></span></span><span class="hljs-function">)</span></span> { writer.WriteStartElement(<span class="hljs-string"><span class="hljs-string">"Binary"</span></span>); writer.WriteBase64(_content, <span class="hljs-number"><span class="hljs-number">0</span></span>, _content.Length); writer.WriteEndElement(); } }</code> </pre> <br>  Note that a temporary buffer is not needed here.  JSON can be written directly to the XmlWriter buffer that WCF provides us with, encoding it in Base64 on the fly.  Thus, we will go the first way, getting rid of memory allocation: <br><br><pre> <code class="cs hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">protected</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">override</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">OnWriteBodyContents</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">XmlDictionaryWriter writer</span></span></span><span class="hljs-function">)</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> serializer = Newtonsoft.Json.JsonSerializer.Create(...); writer.WriteStartElement(<span class="hljs-string"><span class="hljs-string">"Binary"</span></span>); Stream stream = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Base64Writer(writer); Var sw = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> StreamWriter(stream, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> UTF8Encoding(<span class="hljs-literal"><span class="hljs-literal">false</span></span>)); <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">var</span></span> jsonWriter = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Newtonsoft.Json.JsonTextWriter(sw)) { serializer.Serialize(jsonWriter, _result); jsonWriter.Flush(); } writer.WriteEndElement(); }</code> </pre> <br>  Here, Base64Writer is a simple wrapper over an XmlWriter that implements the Stream interface, which writes to the XmlWriter as Base64.  In this case, of the entire interface, it is sufficient to implement only one Write method, which is called in the StreamWriter: <br><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">Base64Writer</span></span> : <span class="hljs-title"><span class="hljs-title">Stream</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">readonly</span></span> XmlWriter _writer; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Base64Writer</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">XmlWriter writer</span></span></span><span class="hljs-function">)</span></span> { _writer = writer; } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">override</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Write</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">byte</span></span></span></span><span class="hljs-function"><span class="hljs-params">[] buffer, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> offset, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> count</span></span></span><span class="hljs-function">)</span></span> { _writer.WriteBase64(buffer, offset, count); } &lt;...&gt; }</code> </pre> <br><h2>  Induced GC </h2><br>  Let's try to deal with the mysterious garbage collections.  We checked our code 10 times for the presence of GC.Collect calls, but this did not produce results.  It was possible to catch these events in PerfView, but the call stack is not particularly indicative (DotNETRuntime / GC / Triggered event): <br><br><img src="https://habrastorage.org/webt/ye/j0/qg/yej0qglbieyx_tg05hdgutajhmc.png"><br><br>  There is a small lead - calling RecycleLimitMonitor.RaiseRecycleLimitEvent before the induced garbage collection.  Let's follow the call stack of the RaiseRecycleLimitEvent method: <br><br><pre> <code class="cs hljs">RecycleLimitMonitor.RaiseRecycleLimitEvent(...) RecycleLimitMonitor.RecycleLimitMonitorSingleton.AlertProxyMonitors(...) RecycleLimitMonitor.RecycleLimitMonitorSingleton.CollectInfrequently(...) RecycleLimitMonitor.RecycleLimitMonitorSingleton.PBytesMonitorThread(...)</code> </pre> <br>  The names of the methods fully correspond to their functions: <br><ul><li>  In the RecycleLimitMonitor.RecycleLimitMonitorSingleton constructor, a timer is created that calls the PBytesMonitorThread at a certain interval. <br></li><li>  PBytesMonitorThread collects statistics on memory usage and, under some conditions, calls CollectInfrequently. <br></li><li>  CollectInfrequently calls AlertProxyMonitors, results in a bool, and calls GC.Collect () if it receives true.  He also keeps track of the time that has passed since the last call of the garbage collector, and does not cause it too often. <br></li><li>  AlertProxyMonitors goes through the list of running IIS web applications, for each raises the corresponding RecycleLimitMonitor object, and calls RaiseRecycleLimitEvent. <br></li><li>  RaiseRecycleLimitEvent raises the list of IObserver &lt;RecycleLimitInfo&gt;.  Handlers receive as a RecycleLimitInfo parameter, in which they can set the RequestGC flag, which is returned in CollectInfrequently, causing an induced garbage collection. <br></li></ul><br><br>  Further investigation reveals IObserver &lt;RecycleLimitInfo&gt; handlers are added in the RecycleLimitMonitor.Subscribe () method, which is called in the AspNetMemoryMonitor.Subscribe () method.  Also, in the AspNetMemoryMonitor class, the default IObserver &lt;RecycleLimitInfo&gt; handler is hung (the RecycleLimitObserver class), which cleans ASP.NET caches and sometimes requests garbage collection. <br><br>  The riddle of Induced GC is almost solved.  It remains to clarify the question, why this garbage collection is called.  RecycleLimitMonitor monitors memory usage of IIS (more precisely, the private bytes figure), and when its use approaches a certain limit, it starts to raise the RaiseRecycleLimitEvent event using a rather confusing algorithm.  The AspNetMemoryMonitor.ProcessPrivateBytesLimit value is used as the memory limit, and in turn the following logic is contained in it: <br><ul><li>  If IIS is configured to 'Private Memory Limit (KB)' for the Application Pool, then the value in kilobytes is taken from there. <br></li><li>  Otherwise, for 64-bit systems, 60% of physical memory is taken (for 32-bit, logic is more complicated). <br></li></ul><br>  The conclusion of the investigation is as follows: ASP.NET memory usage approaches its limit and begins to regularly trigger garbage collection.  'Private Memory Limit (KB)' was not set to a value, so ASP.NET was limited to 60% of the physical memory.  The problem was masked by the fact that the Task Manager server showed a lot of free memory and it seemed to be enough.  We increased the value of 'Private Memory Limit (KB)' in the settings of the Application Pool in IIS to 80% of the physical memory.  This encourages ASP.NET to use more available memory.  We also added the monitoring of the '.NET CLR Memory / # Induced GC' performance counter, so as not to miss the next time ASP.NET decides that it is approaching the limit of memory usage. <br><br><h2>  Repeated measurements </h2><br>  Let's see what happened to the garbage collection after all these changes.  Let's start with perfview / GCCollectOnly (tracing time - 1 hour), GCStats report: <br><br><img src="https://habrastorage.org/webt/8b/l3/fn/8bl3fnxpuymka28coyzbo0r5ak4.png"><br><br>  It is seen that the 2nd generation assemblies are now 2 orders of magnitude smaller than the 0th and 1st.  Also, the time of these assemblies has decreased.  Induced assemblies are no longer observed.  Let's look at the list of assemblies of the 2nd generation: <br><br><img src="https://habrastorage.org/webt/mx/oy/tv/mxoytvprkypunnhwtao6o6fboai.png"><br><br>  From the Gen column it can be seen that all assemblies of the 2nd generation became background ('2B' means 2nd generation, Background).  That is, most of the work is done in parallel with the execution of the application, and all threads are blocked for a short time (the 'Pause MSec' column).  Let's look at the pauses when creating large objects: <br><br><img src="https://habrastorage.org/webt/qp/04/hp/qp04hpcq35buinfnfjn5uuudnyg.png"><br><br>  It can be seen that the number of such pauses when creating large objects fell by several times. <br><br><h2>  Results </h2><br>  Thanks to the changes described in the article, it was possible to significantly reduce the number and duration of 2nd generation assemblies.  It was possible to find the cause of the induced assemblies, and get rid of them.  The number of assemblies of the 0th and 1st generation increased, but their average duration decreased (from ~ 200 msec to ~ 60 msec).  The maximum duration of assemblies of the 0th and 1st generations has decreased, but not so noticeably.  Assemblies of the 2nd generation became faster, long pauses up to 1000ms were completely gone. <br><br>  As for the key metric - ‚Äúpercentage of slow queries‚Äù, it decreased by 40% after all changes. <br><br>  Thanks to the work done, we realized which performance counters are needed to assess the situation with memory and garbage collection, adding them to Zabbix for continuous monitoring.  Here is a list of the most important, to which we pay attention, and find out the reason (for example, an increased flow of requests, a large amount of transmitted data, a bug in the application): <br><div class="scrollable-table"><table><tbody><tr><td>  Performance counter </td><td>  Description </td><td>  When should I pay attention </td></tr><tr><td>  \ Process (*) \ Private Bytes </td><td>  The amount of memory allocated for the application </td><td rowspan="3">  Values ‚Äã‚Äãgreatly exceed the threshold.  As a threshold, you can take the median for 2 weeks from the maximum daily indicators. </td></tr><tr><td>  \ .NET CLR Memory (*) \ # Gen 2 Collections </td><td>  The amount of memory in the older generation </td></tr><tr><td>  \ .NET CLR Memory (*) \ Large Object Heap size </td><td>  Memory size for large objects </td></tr><tr><td>  \ .NET CLR Memory (*) \% Time in GC </td><td>  The percentage of time spent on garbage collection </td><td>  The value is greater than 5%. </td></tr><tr><td>  \ .NET CLR Memory (*) \ # Induced GC </td><td>  The number of induced assemblies </td><td>  Value is greater than 0. </td></tr></tbody></table></div></div><p>Source: <a href="https://habr.com/ru/post/452298/">https://habr.com/ru/post/452298/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../452288/index.html">Situation: US mobile operators accused of illegal sale of subscribers geodata</a></li>
<li><a href="../45229/index.html">Classmates do not give to write about Vkontakte</a></li>
<li><a href="../452290/index.html">What hackers miss when breaking a bank on PHDays</a></li>
<li><a href="../452294/index.html">Webinar "Employee - backdoor: modern techniques of social engineering"</a></li>
<li><a href="../452296/index.html">Positive Hack Days 9: Competitive Intelligence Contest will be held on May 18</a></li>
<li><a href="../4523/index.html">BitTorrent Director hates DRM, but creates legal service</a></li>
<li><a href="../452302/index.html">PyConRu-2019 preliminary program: two Python Core Developer, speakers from Anaconda, Intel, JetBrains, Yandex</a></li>
<li><a href="../452304/index.html">AI from OpenAI learned to write poems, articles and news</a></li>
<li><a href="../45231/index.html">ESET Nod32 4 (ESET Smart Security 4) Beta</a></li>
<li><a href="../452312/index.html">British telecoms will pay compensation to subscribers for disconnections</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>