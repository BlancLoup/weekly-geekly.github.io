<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Kaggle - our excursion to the kingdom of overfit</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Kaggle is a platform for machine learning contests. On Habr√© they often write about her: 1 , 2 , 3 , 4 , etc. Kaggle contests are interesting and prac...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Kaggle - our excursion to the kingdom of overfit</h1><div class="post__text post__text-html js-mediator-article">  Kaggle is a platform for machine learning contests.  On Habr√© they often write about her: <a href="https://habrahabr.ru/post/305026/">1</a> , <a href="https://habrahabr.ru/post/254151/">2</a> , <a href="https://habrahabr.ru/post/264653/">3</a> , <a href="https://habrahabr.ru/post/248395/">4</a> , etc.  Kaggle contests are interesting and practical.  The first places are usually accompanied by good prizes (top contests - more than 100k dollars).  Recently, on Kaggle offered to recognize: <br><br><ul><li>  <a href="https://www.kaggle.com/c/diabetic-retinopathy-detection">Diabetes on the retina</a> </li><li>  <a href="https://www.kaggle.com/c/draper-satellite-image-chronology">Photos from satellites and their order</a> </li><li>  <a href="https://www.kaggle.com/c/avito-duplicate-ads-detection">Same advertisements</a> </li></ul><br>  And many many others. <br><br>  I have long wanted to try, but something always interfered.  I developed many systems related to image processing: the subject matter is close.  Skills lie more in the practical part and classical Computer Vision (CV) algorithms than in modern Machine Learning techniques, so it was interesting to evaluate my knowledge at the world level, and to improve understanding of convolutional networks. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      And suddenly it all came together.  Dropped a couple of weeks is not very busy schedule.  At kaggle held an interesting competition on a close topic. I updated myself comp.  And most importantly, he knocked out <a href="https://habrahabr.ru/users/vasyutka/" class="user_link">vasyutka</a> and <a href="https://habrahabr.ru/users/nikkolo/" class="user_link">Nikkolo</a> to form a company. <br><br>  I must say that we have not achieved the enchanting results.  But I consider 18th place out of 1.5 thousand participants to be quite good.  And considering that this is our first experience of participating in kaggle, that from 3 months of the competition we participated only 2.5 weeks, that all the results were obtained on one single video card - it seems to me that we performed well. <br><br>  What will this article be about?  First, about the problem itself and our method for solving it.  Secondly, about the process of solving CV tasks.  I wrote a lot of articles on Habr√© about machine vision ( <a href="https://habrahabr.ru/post/274725/">1</a> , <a href="https://habrahabr.ru/post/208090/">2</a> , <a href="https://habrahabr.ru/post/224339/">3</a> ), but the scribbling and theory is always better supported by example.  And to write articles for some commercial problem for obvious reasons is impossible.  Now finally tell about the process.  Moreover, here he is the most common, well illustrating how problems are solved.  Thirdly, the article about what goes after the solution of the idealized problem in a vacuum: what will happen when the problem collides with reality. <br><br><img src="https://habrastorage.org/files/220/060/3a2/2200603a2cb5438798dec072ee0c500c.jpg"><br><a name="habracut"></a><br><h2>  Task analysis </h2><br>  The task we started doing was as follows: ‚ÄúIdentify the driver in the photo to one of ten groups: careful driving, telephone in his right hand, telephone in his right ear, telephone in his left hand, telephone in his left ear, tuning music, drinking liquid , stretching back, hair-dressing (painting lips, scratching the back of his head), talking with a neighbor ‚Äù.  But, as it seems to me, it is better to look at the examples once: <br><br>  0 <img width="250" src="https://habrastorage.org/files/31f/9a4/d6b/31f9a4d6badb4318989a0b62252fc137.jpg">  one <img width="250" src="https://habrastorage.org/files/26f/c7b/6f7/26fc7b6f7e7f4f9c9742d1cbac440766.jpg">  2 <img width="250" src="https://habrastorage.org/files/eda/9a0/90a/eda9a090a94d45ddb9a50a759a35f099.jpg"><br>  3 <img width="250" src="https://habrastorage.org/files/0c6/885/46b/0c688546b0824224aa24df0c09994cf7.jpg">  four <img width="250" src="https://habrastorage.org/files/9cb/f69/082/9cbf6908289b4c578deae97acf90839e.jpg">  five <img width="250" src="https://habrastorage.org/files/876/2da/094/8762da094444454da5320fa35a3ff107.jpg"><br>  6 <img width="250" src="https://habrastorage.org/files/891/090/1bb/8910901bb68c42038e57dcc86880fbd5.jpg">  7 <img width="250" src="https://habrastorage.org/files/3c9/296/74f/3c929674ffab4d8aaf89917b149eff7d.jpg">  eight <img width="250" src="https://habrastorage.org/files/5fb/de7/6eb/5fbde76ebb7b44d6b7361a1ae62110d5.jpg"><br>  9 <img width="250" src="https://habrastorage.org/files/bc1/935/f8f/bc1935f8fe5747b291acb2095bfa74ce.jpg"><br><div class="spoiler">  <b class="spoiler_title">English</b> <div class="spoiler_text"><ul><li>  c0: safe driving </li><li>  c1: texting - right </li><li>  c2: talking on the phone - right </li><li>  c3: texting - left </li><li>  c4: talking on telephone - left </li><li>  c5: operating the radio </li><li>  c6: drinking </li><li>  c7: reaching behind </li><li>  c8: hair and makeup </li><li>  c9: talking to passenger </li></ul><br></div></div><br>  ******************************************** <br>  Everything seems clear and obvious.  But it is not so.  What class do these two examples belong to? <br><br><img width="300" src="https://habrastorage.org/files/665/cbc/1ea/665cbc1ea74c40db9fcab658a31ef95a.jpg"><img width="300" src="https://habrastorage.org/files/fc8/e0f/530/fc8e0f5307984033b13a18a585a5fd5f.jpg"><br><br>  The first example is grade 9, conversation.  The second example is the zero class, safe driving. <br>  According to our assessment, the accuracy of a person when recognizing a class on the basis is about 94%.  At the same time, the first and tenth grades make the most confusion.  At the beginning of our participation, the first places had an accuracy of approximately 97% of correct recognitions.  Yes Yes!  Robots are already better than people! <br><br>  Some details: <br><ul><li>  The size of the base for learning 22 thousand images. </li><li>  Approximately 2 thousand per class. </li><li>  There are 27 drivers in the training base. </li><li>  Test database - 79 thousand images. </li></ul><br>  Today, the main means for solving problems of this kind are convolutional neural networks.  They perform image analysis on many levels, independently highlighting key features and their relationships.  You can read about convolutional networks <a href="https://habrahabr.ru/company/nordavind/blog/253859/">here</a> , <a href="http://www.aboutbrain.ru/2014/02/27/%25D1%2581%25D0%25B2%25D0%25B5%25D1%2580%25D1%2582%25D0%25BE%25D1%2587%25D0%25BD%25D1%258B%25D0%25B5-%25D1%2581%25D0%25B5%25D1%2582%25D0%25B8-%25D0%25BD%25D0%25B5%25D0%25BE%25D0%25BA%25D0%25BE%25D0%25B3%25D0%25BD%25D0%25B8%25D1%2582%25D1%2580%25D0%25BE%25D0%25BD/">here</a> and <a href="http://mechanoid.kiev.ua/ml-lenet.html">here</a> .  Convolutional networks have a number of disadvantages: <br><ul><li>  It takes a lot of different data to learn.  Two thousand per class is less enough, although not enough.  But the fact that there are only 27 drivers is very limiting. </li><li>  Convolution networks are prone to "overfit".  When training, they may catch on with some insignificant sign, peculiar to several images, but which does not carry significant weight.  We have such a sign was the opening of the sun visor.  All the photos with him went to the 8th grade with a gun.  Anyway: when overfitting the grid, it simply can remember all 22 thousand input images.  Here is a good overfit <a href="https://habrahabr.ru/post/259191/">example</a> . </li><li>  Convolution networks are ambiguous due to the fact that they are complex.  Approximately the same solution on different frameworks can yield fundamentally different results.  A few slightly modified network parameters fundamentally change the result.  Many people call the configuration of networks something akin to art. </li></ul><br>  An alternative to convolutional networks may be the manual management of low-level features.  Highlight hands.  Face position  Facial expression.  Open / closed visor at the car. <br>  Convolution networks are many different.  The classic approach is to use the most common networks from the Zoo ( <a href="https://github.com/BVLC/caffe/wiki/Model-Zoo">caffe theano</a> <a href="https://github.com/albertomontesg/keras-model-zoo">keras</a> ).  This is primarily VGG16, VGG19, GoogleNet, ResNet.  For these networks, there are many variations, plus you can use techniques that accelerate learning.  Of course, this approach is used by all participants.  But the basic good result can be obtained only on it. <br><br><h2>  Our setup </h2><br>  All calculations in our work were carried out on a single GTX1080.  This is the latest game card from NVIDIA.  Not the best option from what is on the market, but quite good. <br>  We wanted to use a cluster with three Tesla in one of the works, but due to a number of technical difficulties this did not work out.  We also thought about using some old video card from a 4Gb laptop, but as a result we decided not to go this way, there was much less speed there. <br>  The framework used is <a href="https://github.com/BVLC/caffe">Caffe</a> .  Keras could also be used with Theano, which would certainly enhance our result due to a slightly different implementation of the training.  But we did not have time for this, so Caffe was used to the maximum. <br><br>  RAM - 16Gb, the maximum when training was used 10Gb.  The processor is the last i5. <br><br><div class="spoiler">  <b class="spoiler_title">If anyone is interested, but nothing special</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/221/5e4/fb5/2215e4fb5e9345eb9c67b79bfb2d0778.jpg"><br></div></div><br><br><h2>  A few words about the rules </h2><br>  I think that most of the readers never participated in kaggle, so I‚Äôll take a quick look at what the rules of the competition are: <br><ul><li>  Participants are presented with two sets of data.  Training - on which target results are signed.  Test - by which you need to make recognition. </li><li>  The participant must recognize all the images from the test set and send them to the site in csv-format (text file of the form ‚Äúimage number‚Äù - probability of the first class, probability of the second class, ....). </li><li>  In total, you can make 5 attempts to send. </li><li>  After each sending, the user is told the current percentage of the test base.  But, you need to take into account one interesting point.  The user is told the result is not for the entire database, but only for a small part of it.  In our problem, it was 39%. </li><li>  The final result of the competition is considered for the remaining piece of the base (61%) after the close of the competition.  This can lead to serious permutations of the participants. </li><li>  The first three places - prize.  If a participant enters them, he must publish his decision and the organizer will check it. </li><li>  The solution should not: <br><ul><li>  Contain proprietary data </li><li>  Use user-marked test selection during training.  In this case, it is permitted to mark out a training set. </li></ul><br></li></ul><br><br><h2>  A few words about the metric </h2><br>  Suppose we invented some recognition mechanism and recognized all the images.  How will they be further checked?  In this problem, we used the approach of calculating <a href="https://www.kaggle.com/c/state-farm-distracted-driver-detection/details/evaluation">multiclass logarithmic losses</a> .  In short, it can be written as: <br><img src="https://habrastorage.org/files/c4d/08d/5e8/c4d08d5e8fd8400a8ac0941ff2a717e9.png"><br>  y is the decision matrix.  A unit if the object belongs to a class. <br>  p - matrix of answers that the user has sent.  It is best to record the probability of belonging to a class. <br>  M number of classes <br>  N is the number of objects <br>  In the neighborhood of zero, the value is replaced by a constant. <br>  We estimated that the probability ‚Äú95%‚Äù corresponds to a logloss value of approximately ‚Äú0.2‚Äù, the value ‚Äú0.1‚Äù corresponds to a probability of ‚Äú97.5%‚Äù.  But this is a rough estimate. <br>  We will return to this function, but a little lower. <br><br><h2>  The first steps </h2><br>  The theory is good.  But where to start?  Let's start with the simplest: take the CaffeNet grid, which is attached to <a href="https://github.com/BVLC/caffe/blob/master/examples/pascal-multilabel-with-datalayer.ipynb">Caffe</a> and for which there is an example. <br>  After I did the same thing, I immediately got the result ‚Äú0.7786‚Äù, which was somewhere in the 500th place.  It's funny, but for many people the result was much worse.  At the same time, it is worth noting that 0.77 approximately corresponds to 80-85% of correct recognitions. <br>  We will not dwell on this grid, which is already quite outdated.  Take something standard modern.  To the standard can be considered: <br><ul><li>  <a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/">VGG family</a> : <br><ul><li>  VGG-16 </li><li>  VGG-19 </li></ul><br></li><li>  <a href="https://github.com/KaimingHe/deep-residual-networks">ResNet family</a> : <br><ul><li>  ResNet-50 </li><li>  ResNet-101 </li><li>  ResNet-152 </li></ul><br></li><li>  <a href="http://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf">Inception family</a> </li></ul><br>  About non-standard methods can be read below in the section "Ideas that have not rolled." <br>  Since we started somewhere in two and a half months after the start of the competition, it made sense to explore the forum.  The forum advised VGG-16.  The author of the post assured that he received a solution with a loss of "0.23" on the basis of this network. <br><br>  Consider the <a href="https://www.kaggle.com/c/state-farm-distracted-driver-detection/forums/t/20747/simple-lb-0-23800-solution-keras-vgg-16-pretrained">decision of the</a> author: <br><ol><li>  He used the pre-trained VGG network.  This greatly increases the speed of learning. </li><li>  He taught not one, but 8 grids.  When learning each grid, only 1/8 of the input database (22/8 thousand images) was fed to the input. </li><li>  The resulting solution gave a level of 0.3-0.27 </li><li>  He received the final result by adding the result of these 8 grids. </li></ol><br>  The decision to repeat with us failed.  However, many who could not do it.  Although the author laid out the training script on Keras'e.  Apparently on Keras it could be reached, but not on caffe.  The third place winner also counted VGG on Keras, and all the other grids on Caffe and Theano. <br><br>  As for us, pure VGG gave 0.4, which, of course, improved our result at that time, but only up to 300th place. <br><br>  As a result, we decided to give up VGG and tried out the training of the pre-trained ResNet-50 ( <a href="https://habrahabr.ru/post/303196/">here</a> you can read interestingly what it is).  Which immediately gave us 0.3-0.29. <br><br>  <i>A small remark: we never used the technique of ‚Äúbreaking the base into 8 parts‚Äù.</i>  <i>Although, most likely, it would bring us a little extra accuracy.</i>  <i>But such training would take several days, which was unacceptable to us.</i> <i><br><br></i>  <i>Why do I need to split the base into 8 parts and train independent grids?</i>  <i>Suppose the first of the grids is always mistaken in favor of situation A, when choosing from A and B. The second grid, on the contrary, decides B. Moreover, both grids are often mistaken about the real decision.</i>  <i>But the sum of the grids will more correctly estimate the risk of A / B.</i>  <i>In fact, in most disputable situations, it will deliver 50% - A, 50% - B. This approach minimizes our losses.</i>  <i>We achieved it differently.</i> <br><br>  To improve accuracy with 0.3, we did the following: <br><ul><li>  We trained several ResNet-50 grids with various hyperparameters.  All of them gave somewhere 0.3-0.28, but their sum was more. </li><li>  We trained the ResNet-101 net, which in itself gave somewhere around 0.25 </li><li>  We trained the ResNet-50 grid with image changes, which gave somewhere 0.26 </li></ul><br>  The image changes are the following: when training, instead of the original training picture, a rotated picture is given, the picture is cropped, the picture is noisy with noise.  This increases stability and accuracy. <br>  The final result of the addition of all the grids was at the level of "0.23-0.22." <br><br><h2>  To new heights </h2><br>  Result 0.22 was somewhere around 100 places.  This is a good result.  In fact, the maximum, which gives the correctly configured network.  But to go further you need to stop, think and reflect on what has been done. <br>  The easiest way to do this is to see Confusion Matrix.  In essence, this concept hides a budget of errors.  How and when are we mistaken.  Here is the matrix that we got: <br><br><img src="https://habrastorage.org/files/3c0/81a/d4f/3c081ad4f0e14484a7ef044848cd10d8.png"><br><br>  In this matrix, the x-axis are the objects of the original class.  On the y-axis - where they were assigned.  For example, out of 100% of the objects of the zero class, 72% were successfully assigned to him, 0.8% to the first class, 16.8% to the ninth. <br>  The following conclusions can be drawn from the matrix: <br><ul><li>  The greatest number of errors - zero and ninth grade.  This is logical.  I myself often cannot understand whether a person is talking or not.  Above in the text was an example. </li><li>  The second in the number of errors - the eighth grade.  Any class where the hand is next to the head can be referred to the eighth class, if the information is not enough.  This is talking on the phone.  These are conversations with a neighbor, with active gestures.  In some situations, when a person drinks a liquid - it can also be similar. </li></ul><br>  Therefore, it is necessary to develop an algorithm that can more correctly distinguish these three classes. <br>  In order to do this, we used the following ideas: <br><ul><li>  In the opinion of a person, 90% of the information allowing to distinguish between grade 0 and grade 9 is contained on the face  At the same time, in most cases, when classifying in grade 8 was wrong, the final decision is also made on the area around the face. </li><li>  All the grids above were sharpened to a resolution of 224 * 224.  At the same time, the quality of the face sags heavily. </li></ul><br>  So we need to keep the resolution around the face and use it to refine the 0-8-9 grades.  In total, we had three ideas on how to do this.  About two of them will be written below in the section "Ideas that are not rolled."  I rolled the following idea: <br>  We train a simple Haar classifier for face selection.  In principle, the face can even be well distinguished by color.  Given that we know well where the person should be. <br>  The competition rules did not prohibit manual marking of the training base.  Therefore, we noted somewhere on 400 images of the face.  And we got a very good selection of frames recognized automatically (faces were found correctly for 98-99% of frames): <br><br><img src="https://habrastorage.org/files/d8d/522/b79/d8d522b791e94995a2d0ac1bc92d3c37.jpg"><img src="https://habrastorage.org/files/209/0f8/b64/2090f8b64e6a462892010ee5e6759fcc.jpg"><img src="https://habrastorage.org/files/9e6/2dd/e53/9e62dde536b045b2b984223a3e1f96d3.jpg"><br><br>  Having trained ResNet-100 in images, we got an accuracy somewhere around 80%.  But the addition of learning results to the amount of networks used gave an additional 0.02 on the test sample, moving us to the thirties area. <br><br><h2>  Ideas that are not rolled </h2><br>  Break the slender outline of the narrative and take a small step to the side.  Without this step, everything is fine, but with this step it becomes clear what was going on in my head at that moment. <br>  There are much more ideas that do not produce results in any research task than ideas that produce results.  And sometimes it happens that ideas for one reason or another can not be used.  There is a small list of ideas that we have already tried to test at the time of entering thirtieth places. <br><br>  The first idea was as simple as a log.  We already wrote on Habr√© ( <a href="https://habrahabr.ru/company/recognitor/blog/277781/">1</a> , <a href="https://habrahabr.ru/post/277069/">2</a> ) about networks that can color images according to the class affiliation of objects.  It seemed to us that this is a very good approach.  You can teach the network to detect exactly what you need: telephones, hands, an open visor near the car.  We even spent two days on the layout, configuration and training of SegNet.  And suddenly we realized that SegNet has a closed non-OpenSource license.  And therefore we can not honestly use it.  I had to refuse.  And the results of automatic markup were promising (several approaches are shown here at once). <br><br>  The first: <br><br><img width="300" src="https://habrastorage.org/files/244/10c/0c7/24410c0c740a4cceba29cc83a9f1d117.png"><img width="300" src="https://habrastorage.org/files/996/0a8/d42/9960a8d4224b42c29940976a00c4c2a7.png"><br><br>  Second: <br><br><img width="300" src="https://habrastorage.org/files/92a/671/b80/92a671b80f4d4fbd85b918dbece6097e.png"><img width="300" src="https://habrastorage.org/files/171/f59/d75/171f59d754f74800a4358aa699aad4b9.png"><br><br>  And here is the markup process: <br><br><img width="300" src="https://habrastorage.org/files/e4c/402/08a/e4c40208a6ce40488f9302618dfe6bae.jpg"><img width="300" src="https://habrastorage.org/files/6d0/f40/185/6d0f40185bcd4f5fa4987279cab6c9b5.jpg"><br><br>  The second idea was that the resolution of 224 * 224 is not enough for us to make a decision: class 0 or 9. The greatest problems are caused by the loss of permission on faces.  But we knew that the face is almost always located in the upper left part of the image.  Therefore, we dragged the pictures, having received such cute tadpoles with the maximum resolution in places of interest to us: <br><br><img src="https://habrastorage.org/files/0ec/c33/271/0ecc33271a1a47e1ba20bc9ad110f610.jpg"><img src="https://habrastorage.org/files/aea/b8d/0ba/aeab8d0ba5584a089224b6e7b77f4010.jpg"><br><br>  Not a ride.  The result was something like the usual training + strongly correlated with what we had. <br><br>  The next idea was quite large and comprehensive.  <a href="https://www.kaggle.com/c/state-farm-distracted-driver-detection/forums/t/21994/heat-map-of-cnn-output">Posts on the</a> contest <a href="https://www.kaggle.com/c/state-farm-distracted-driver-detection/forums/t/21994/heat-map-of-cnn-output">forum</a> prompted us to think: what does the network see in reality?  What interests her? <br>  There is a whole selection of articles on this topic: <a href="https://habrahabr.ru/post/282071/">1</a> , <a href="https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf">2</a> , <a href="http://arxiv.org/pdf/1506.02753v4.pdf">3</a> <br><br>  The forum Kaggle cited such cool pictures: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/21b/137/4c7/21b1374c74019cdca199c140874e95ac.jpg" alt="image"><br><br>  Naturally, we decided to invent our own bicycle for this task.  Moreover, it is written very simply. <br><br>  We take the original image and begin to move the black square on it.  At the same time, we look at how the response of the system changes. <br><br><img src="https://habrastorage.org/files/9be/3bf/622/9be3bf6229d24e62a2c068f85e09a986.jpg"><br><br>  We will draw the result as a heat map. <br><br><img src="https://habrastorage.org/files/510/040/b30/510040b30db24c58a811bb834e29fbf4.jpg"><br><br>  The image here shows an example of incorrect classification of a class 9 image (conversation with a neighbor).  The image is classified as "drawn to the control of music."  And indeed, it seems.  After all, the network itself does not see 3d.  She sees a hand that lies in the direction of the dashboard switch.  So what that hand is on the foot. <br><br>  Having looked at another few dozens of errors, we realized that again everything rested there: the grid does not look at what is happening on the face. <br><br>  Therefore, we have come up with a different way of learning.  A set was fed to the network's input, where half of the pictures came directly from the training base, and half was overwhelmed with everything except the face: <br><br><img src="https://habrastorage.org/files/a4a/b9c/32a/a4ab9c32ac044ca8824fa93140f3e697.jpg"><br><br>  And about a miracle, the grid began to work much better by orders of magnitude!  For example, by a mistakenly classified man: <br><br><img src="https://habrastorage.org/files/3e5/c84/743/3e5c8474375b4c658c4e139a94c24e9d.jpg"><br>  Or, differently (as it was, as it became): <br><br><img src="https://habrastorage.org/files/368/7e4/1e8/3687e41e89934155a3a1834db32d051a.jpg"><img src="https://habrastorage.org/files/14c/be6/716/14cbe671635c4dc592010a89c52c3c1f.jpg"><br><br>  At the same time, the network worked well for the other classes (it noted the correct zones of ineres). <br>  We already wanted to celebrate the victory.  Loaded network for verification - worse.  Combined with our best - the final result has not improved.  I don't even remember if we began to add it to our best answer.  We thought for a long time that we had some kind of mistake, but did not find anything. <br><br>  It seems like on the one hand - the network has become more correct to look and a lot of what errors to correct.  On the other hand, somewhere began to do new.  But at the same time there was no significant statistical difference. <br><br>  Ideas that are not rolled was much more.  There was a Dropout, which gave us almost nothing.  There were additional different noises: also did not help.  But about this is nothing beautiful and you will not write. <br><br><h2>  Let's return to our sheep </h2><br>  We stopped somewhere around the 30th places.  Left a little.  A lot of ideas have already failed, 25 test projects have accumulated on the computer.  There was no improvement.  Our knowledge of neural networks gradually began to be exhausted.  So go google the current contest forum and the old kaggle forums.  And the solution was found.  It was called "pseudo labeling" and <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">"Semi-supervised learning"</a> .  And it leads to the dark side.  Rather gray.  But it was announced by the admins of the contest as legal. <br>  In short: we use a test sample for learning by marking it with an algorithm trained in a training sample.  Sounds weird and messy.  But if you think about it, it is fun.  By pushing into the network objects that are marked for it, we do not improve anything in a local perspective.  But.  First, we learn to highlight those signs that give the same result, but easier.  In essence, we teach convolutional levels in such a way that they learn to distinguish new features.  Maybe in some next image they stand out and help better.  Secondly, we protect the network from retraining and overfit by introducing pseudo-arbitrary data that will not worsen the network. <br><br>  Why does this path lead to the gray side?  Because formally using a test sample for training is prohibited.  But after all we here do not use it for training.  Only for stabilization.  Especially admins allowed. <br><br>  Result: + 10 positions.  We fall into the twenty. <br><br>  The final graph of the walk results looked something like this (in the beginning we did not spend all the attempts per day): <br><br><img width="800" src="https://habrastorage.org/files/9c7/5a9/22e/9c75a922e6314f24ae231b53e2175024.png"><br><br><h2>  And again about LogLoss </h2><br>  Somewhere in the beginning of the article I mentioned the fact that I will come back to LogLoss.  It's not that easy with him.  You can see that log (0) is minus infinity =&gt; if you suddenly put 0 in the class where the answer is one, then we get minus infinity. <br><br><img src="https://habrastorage.org/files/c4d/08d/5e8/c4d08d5e8fd8400a8ac0941ff2a717e9.png"><br><br>  Unpleasant  But the organizers have protected us from this.  They say that they replace the value under log with max (a, 10 ^ (- 15)).  So we get an additive -15 / N to the result.  That equals -0.000625 to the result for each wrong image for a public check and -0.0003125 for a closed one.  Ten images affect the error in the third digit.  And this is the position. <br>  But the error can be reduced.  Suppose instead of 10 ^ (- 15) we put 10 ^ (- 4).  Then if we make a mistake we get -4 / N.  But if we guess correctly, then we will also have losses.  Instead of log (1) = 0, we take log (0.9999), which is -4 * 10 ^ (- 5).  If we make a mistake every 10 attempts, then it is certainly more profitable for us than a loss of 10 ^ (- 15). <br><br>  And then the magic begins.  We need to combine 6-7 results to optimize the LogLoss metric. <br><br>  In total, we made 80 shipments of the result.  About 20-30 of them were dedicated to optimization of losses. <br><br>  I think that 5-6 places have been played for this account.  Although, as it seems to me, everyone is doing this. <br>  All this magic did <a href="https://habrahabr.ru/users/vasyutka/" class="user_link">Vasyutka</a> .<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I don't even know what the last option looked like. </font><font style="vertical-align: inherit;">Only his description, which we have kept, reached two paragraphs.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> What we didn't </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">At the end of the competition, we still have a small stack of ideas. </font><font style="vertical-align: inherit;">Probably, if we had time, it would have cost us five more positions. </font><font style="vertical-align: inherit;">But we understood that this was clearly not a way to the top 3, so we didn‚Äôt throw all the remaining forces into the fight.</font></font><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We have not investigated the partitioning of the base into small pieces and their independent learning. </font><font style="vertical-align: inherit;">In theory, this adds stability. </font><font style="vertical-align: inherit;">We used a similar approach in semi-supervised learning, but also did not fully explore the whole field of possibilities.</font></font></li><li>     ,       ,   .         . :         ,        .   -  2-3 .   :     ,    ,   ,    . </li><li>      (      ).   ,     .       ,  , ...     .    .       . </li><li>          <a href="https://events.yandex.ru/lib/talks/2431/"></a> .          ,    .  ,   ,     .     ,      .       /.     2  ‚Äî . </li></ul><br><br><h2>     </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">After the end of the competition, many participants publish their decisions. Here </font></font><a href="https://www.kaggle.com/c/state-farm-distracted-driver-detection/forums/t/22659/summary-of-methods-of-top-20-methods"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">there</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> is carried out the statistics on the top 20 of them. Now, about half of the top 20 have published solutions. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's start with the best published. </font></font><a href="https://www.kaggle.com/c/state-farm-distracted-driver-detection/forums/t/22631/3-br-power-solution"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Third place</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/d20/be1/ccc/d20be1ccc63393563aa44a96877690b1.gif" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/533/ab7/e5e/533ab7e5e8e4b756f8a82490c2ecf4ef.gif" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I will say right away. I do not like the decision and it seems to be a violation of the rules of the competition. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The authors of the decision noted that all the examples were shot sequentially. To do this, they analyzed the test sample in automatic mode and found adjacent frames. Neighboring frames are images with minimal changes =&gt; they belong to the same class =&gt; for all close decisions you can make a single answer.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And yes, it helps terribly. If you are talking on the phone holding it in your left hand - there are frames where the phone is not visible and it is not clear whether you are talking or scratching your head. But looking at the next frame, you can clarify everything. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I would not mind if statistics thus accumulated and background machine was subtracted. But to reverse engineer a video is for me beyond good and evil. Why-explain below. But, of course, the decision is up to the organizers. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I really like the fifth solution. It is so cool and so trivial. The thought occurred to me about ten times during the competition. But every time I swept it away. "What for?!". "Why will it even work at all ?!" "Too lazy to waste time on this hopelessness." </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I did not even discuss it with my comrades. As it turned out - in vain. The idea here is:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/0d6/6fe/13a/0d66fe13a678c90ed2b2f9a7e7e68331.jpg" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Take two pictures of the same class. Split in half and glue. Submit to training.</font></font> Everything. <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I don‚Äôt really understand why it works (except that it stabilizes the sample: 5 million samples are cool, it‚Äôs not 22 thousand). And the guys had 10 TitanX cards. Maybe it played an important role. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The sixth solution is poorly described, I did not understand it. Ninth is very similar to ours. And the accuracy is not much different. Apparently, the guys were able to train the network better and better. They did not describe in detail due to which a small increase. </font></font><br><br> <a href="https://github.com/toshi-k/kaggle-distracted-driver-detection"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The tenth decision</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> implemented a part of our ideas but a little bit differently: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/bc3/05b/612/bc305b6123babbd45389f5b680736f2e.png" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cut the area with a person to increase the resolution -&gt; apply for training. The same problems that we solved by cutting the face are solved. But, apparently, better. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">15 solution - everything is like ours. Even individuals were also cut out (plus the helm area from which we refused).</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But ... They trained 150 models and folded them. </font><font style="vertical-align: inherit;">150 !!! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">19, 20 solution - everything is like with us, but without faces. </font><font style="vertical-align: inherit;">And 25 trained models.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> From toys to business </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Suppose you are an insurance company that wants to implement a system for determining what the driver is doing. </font><font style="vertical-align: inherit;">You have assembled a base, several teams have proposed algorithms. </font><font style="vertical-align: inherit;">What we have:</font></font><br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Algorithm that uses data about neighboring frames. </font></font> This is strange.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> If you wanted your algorithm to work this way, you would suggest analyzing not photos, but videos. </font></font> Moreover.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This algorithm is almost impossible to tie with the video as such. </font><font style="vertical-align: inherit;">If you have algorithms that recognize a single frame - it can be very easily added to the algorithm that works with video. </font><font style="vertical-align: inherit;">The algorithm that uses the ‚Äú5 nearest frames‚Äù is very difficult.</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Algorithms that use 150 neural networks. </font><font style="vertical-align: inherit;">This is a huge computing power. </font><font style="vertical-align: inherit;">Such a product does not turn out to be massive. </font><font style="vertical-align: inherit;">2-3 networks is for a good maximum. </font><font style="vertical-align: inherit;">Ok, let 10 be the limit. </font><font style="vertical-align: inherit;">It's one thing if your job was to detect cancer. </font><font style="vertical-align: inherit;">There such costs are permissible. </font><font style="vertical-align: inherit;">There you need to fight for every percentage. </font><font style="vertical-align: inherit;">But your goal is a mass product.</font></font></li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But, still got some good and interesting models. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's go ahead. </font><font style="vertical-align: inherit;">And understand how these models work. </font><font style="vertical-align: inherit;">Unfortunately, I don‚Äôt have them, so I‚Äôll be testing ours, which gave the 18th result, which is not so bad in principle. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">From all articles which I wrote on Habr my </font></font><a href="https://habrahabr.ru/post/274725/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">favorite</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> about how it is necessary to collect base. </font><font style="vertical-align: inherit;">From this side we will come to the analysis. </font><font style="vertical-align: inherit;">What do we know about the collected database?</font></font><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">When recruiting the base drivers did not drive the car. </font><font style="vertical-align: inherit;">The car was traveling by truck, the drivers were given the task of what to do.</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The entire base was recruited during the daytime hours. </font><font style="vertical-align: inherit;">Apparently the workers. </font><font style="vertical-align: inherit;">No low sun. </font><font style="vertical-align: inherit;">No evening shots.</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">All situations are perfectionary. </font><font style="vertical-align: inherit;">People do not hold the phone with their left hand at the right ear. </font><font style="vertical-align: inherit;">All phones have the same plus or minus.</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The base was recruited in America. </font><font style="vertical-align: inherit;">You ask how I understood? </font><font style="vertical-align: inherit;">There is not a single car with a manual box ...</font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">For the first time, four situations will suffice. </font><font style="vertical-align: inherit;">But they are much more. </font><font style="vertical-align: inherit;">All situations can never be foreseen. </font><font style="vertical-align: inherit;">That is why the base should be typed real and not simulated.</font></font><br>  Go.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I made the frames below myself. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As influenced by the fact that when recruiting the base drivers did not drive. Driving is a rather complicated process. It is necessary to turn the head by 120 degrees, to look at traffic lights, to turn sharply at intersections. All this is not in the database. Consequently, such situations are defined as ‚Äúconversation with a neighbor.‚Äù</font></font> Here is an example: <br><br><img width="300" src="https://habrastorage.org/files/863/c50/cb5/863c50cb5fcf45e4b9e449914a7039e6.jpg"><img width="300" src="https://habrastorage.org/files/d01/ea2/212/d01ea221219d403a83652eb90965103d.jpg"><img width="300" src="https://habrastorage.org/files/a37/8ef/944/a378ef944d9c4c089008ea538fd83d9e.jpg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Day hours. This is a very big problem. Of course, you can make a system that will look at the driver at night including the IR illumination. But in the IR illumination a person looks completely different. It will be necessary to completely redo the algorithm. Most likely to train one per day and one at night. But there is not only the problem of the night. Evening - when it is still light but already dark for the camera and there are noises. The grid starts to get confused. Weights on the neurons are walking. The first of the pictures is recognized as a conversation with a neighbor. The second picture jumps between ‚Äúpulling back‚Äù, ‚Äúmeykap‚Äù (which is logical within the network, because I am reaching for the visor), ‚Äútalking with a neighbor‚Äù, ‚Äúsafe driving‚Äù. The sun on the face is a very unpleasant factor, you know ... </font></font><br><br><img src="https://habrastorage.org/files/14e/1eb/6ba/14e1eb6ba4fb40d788f645d0759f9159.jpg"><img src="https://habrastorage.org/files/c1e/7cd/727/c1e7cd7276b7416288ae53a9103bb7b8.jpg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">About perfectionism. Here is a more or less realistic situation:</font></font><br><br><img src="https://habrastorage.org/files/3d8/37c/d2a/3d837cd2a9894540968fa133e131639b.jpg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And the weights on the neurons: 0.04497785 0.00250986 0.23483475 0.05593431 0.40234038 0.01281587 0.00142132 0.00118973 0.19188504 0.0520909 </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Maximum on that phone at the left ear. </font><font style="vertical-align: inherit;">But the network has gone a little bit. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">About Russia generally keep quiet. </font><font style="vertical-align: inherit;">Almost all the frames with a hand on the gear knob are recognized as ‚Äúpulling back‚Äù: </font></font><br><br><img src="https://habrastorage.org/files/01c/6b5/8e1/01c6b58e12274835b75a8a118fddf0aa.jpg"><img src="https://habrastorage.org/files/1b3/4ac/e17/1b34ace178c04d10b2c8338066be401f.jpg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It‚Äôs obvious that there are a lot of problems. I didn‚Äôt add pictures where I tried to trick the network (by turning on the flashlight on my phone, weird caps, etc.). </font><font style="vertical-align: inherit;">This is real and it is deceiving the network.</font></font><br><br><h2>  Panic.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Why is everything so bad? </font><font style="vertical-align: inherit;">You promised 97% !!</font></font></h2><br>  And the problem is that NO computer vision system is done from the first iteration.  There should always be a startup process.  Starting with a simple test sample.  With a set of statistics.  With the correction of emerging problems.  And they will always be.  At the same time, 90% need to be corrected, not programmatically, but administratively.  I got caught trying to trick the system - get a pie.  Someone does not fix the camera?  Be kind, support humanly.  And not the fool himself. <br>  Making the development you need to be ready, that before you get the perfect result you will need to redo everything 2-3 times. <br><br>  And it seems to me that for this task everything is not bad, but, on the contrary, good.  The 95-97% test performance shown is good and means that the system is promising.  This means that the final system can be brought to plus or minus the same accuracy.  You just need to invest in the development of another 2-3 times more power. <br><br>  By the way.  Pro camera mount.  apparently, how the camera was fastened by picking up statistics prevents the passenger.  The way I fastened the camera - gives a slightly different picture for which the statistics are falling.  But does not interfere with the passenger.  I think that a solution with a camera that interferes with the passenger will be unsuitable, most likely they will be recompiling the base.  Or strongly dosobirat. <br><br>  It is also not clear where the images are planned to be processed.  In the module that will shoot?  Then you need to have a decent computational power there.  Send to server?  Then it is obviously single frames.  Save on the map and once a week to transfer from your home computer?  Inconvenient form factor. <br><br><h2>  How much time is spent on the task of such a plan from scratch </h2><br>  Dohrena.  I have never seen a task brought to release in less than half a year.  Probably, applications like Prism, of course, can be deployed in a month, or even faster, if the infrastructure is there.  But any task where the result is important and which is not an art project is a long time. <br>  Specifically for this task.  At least 70 people were involved in the recruiting of the base. 5 of which were at least 5 ‚Äî the attendants who drove the truck sat with a notebook.  And people 65-100 are people who entered the database (I do not know how many of them there are).  It‚Äôs hardly less than 1-2 months to organize such a move, to collect everything, check it out with the organizers.  The contest itself went 3 months, but you can make a good decision on the recruited database in 2-3 weeks (which we did).  But to bring this decision to the working sample can be released as early as 1-3 months.  It is necessary to optimize the solution for iron, to build transmission algorithms, to refine the solution so that it is idle in the absence of a driver, nothing is chased.  And so on and so forth.  Such things always depend on the problem statement and on where and how the solution should be used. <br>  And then the second stage begins: trial operation.  You need to put the system to people 40-50 in the car for a couple of days each and see how it works.  Draw conclusions when it does not work, summarize and rework the system.  And here begins the swamp, where it is almost impossible to estimate the implementation dates a priori before the start of work.  It will be necessary to make the right decisions: in what situations the system is being finalized, in which stubbing, and in which we will fight administratively.  This is the most difficult thing that few people can do. <br><br><img src="https://habrastorage.org/files/bed/32c/8c0/bed32c8c0a334228988d8475adad4e33.jpg"><br><br><h2>  findings </h2><br>  Conclusions will be on kaggle.  I like it.  I was glad to our level.  From time to time it seemed to me that we are behind the times, we choose not optimal methods in our work.  A hike all the rules.  From the side of assessing oneself in the surrounding world, kaggle is very good. <br><br>  From the problem solving side, kaggle is an interesting tool.  Probably this is one of the correct and comprehensive methods that allows you to conduct a good R &amp; D.  You just need to understand that the finished product does not smell here.  But to understand and appreciate the complexity and ways to solve the problem is normal. <br><br>  We will not participate yet.  Forces to fight for the first places you need to spend a lot.  But, if there is an interesting problem - why not. <br><br>  - PS They asked me to repeat this whole story at a Yandex training session.  And there it turns out they write.  If anyone needs: <br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/jgpP0ZbGGc8%3Ffeature%3Doembed&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhjfDLUTZUChdpkxdW3deAMiOUEJoQ" frameborder="0" allowfullscreen=""></iframe></div><p>Source: <a href="https://habr.com/ru/post/307078/">https://habr.com/ru/post/307078/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../307068/index.html">Blendercam documentation in Russian</a></li>
<li><a href="../307070/index.html">The place where the future of the Internet is created</a></li>
<li><a href="../307072/index.html">Development for SailfishOS: application styling</a></li>
<li><a href="../307074/index.html">Yandex.Maps (as well as Google Maps, etc.), when will you start using mesh networks?</a></li>
<li><a href="../307076/index.html">Digest of grocery design, July 2016</a></li>
<li><a href="../307082/index.html">X86 Economy with new Oracle SPARC S7 servers</a></li>
<li><a href="../307084/index.html">Why, oh why, these #? @! assholes use vi?</a></li>
<li><a href="../307086/index.html">How do Fault Tolerant servers differ from ‚Äúconsumer‚Äù consumer goods in a specific example?</a></li>
<li><a href="../307088/index.html">Interception of functions .NET / CLR</a></li>
<li><a href="../307090/index.html">State machines in the SimInTech dynamic simulation environment</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>