<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The first experience of participation in the kaggle-competition and work on the bugs</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I want to share the experience of my first participation in the kaggle contest (training Bag of Words ). And although I did not manage to achieve amaz...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The first experience of participation in the kaggle-competition and work on the bugs</h1><div class="post__text post__text-html js-mediator-article">  I want to share the experience of my first participation in the kaggle contest (training <a href="https://www.kaggle.com/c/word2vec-nlp-tutorial">Bag of Words</a> ).  And although I did not manage to achieve amazing results, I will talk about how I searched and found ways to improve the examples of the ‚Äútextbook‚Äù (for this purpose I will also briefly describe the examples), and also focus on the analysis of my mistakes.  I must warn you that the article will be of interest primarily to newcomers in the field of text mining.  However, I describe most of the methods briefly and simply, giving references to more precise definitions, since my purpose is to review the practice, not the theory.  Unfortunately, the competition has already ended, but it may still be useful to read the materials for it.  Link to the code for the article <a href="https://github.com/Jaylla/NlpKaggleTraining">here</a> . <br><a name="habracut"></a><br><h4>  Competition Overview </h4><br>  The task itself is to analyze the emotional coloring of the text.  To do this, take reviews and ratings of films from the site IMDb.  Reviews rated&gt; = 7 are considered positively colored, with less rating - negative.  Task: having trained the model on training data, where each text is given a mark (negative / positive), then predict this parameter for texts from the test set.  The prediction quality is estimated using a parameter called the <a href="https://ru.wikipedia.org/wiki/ROC-%25D0%25BA%25D1%2580%25D0%25B8%25D0%25B2%25D0%25B0%25D1%258F">ROC curve</a> .  You can read the link in detail, but the closer this parameter is to 1 - the more accurate the prediction. <br><br>  All examples are written in Python and use the <a href="http://scikit-learn.org/">scikit-learn</a> library, which allows you to use ready-made implementations of all the classifiers and vectorizers we need. <br><br><h4>  Methods for solving the problem </h4><br>  We have plain text at our disposal, and all data mining classifiers require numerical vectors at the input.  Therefore, the primary task is to determine how to convert a text block into a vector (vectorization). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The simplest method is Bag of Words, which begins the first example of a textbook.  The method consists in creating a common pool of used words, each of which is assigned its own index.  Suppose we have two simple texts: <br><blockquote>  John likes to watch movies.  Mary likes movies too. <br>  John also likes to watch football games. </blockquote><br>  For each unique word, we assign the following index: <br><br>  <i>"John": 1,</i> <i><br></i>  <i>Likes: 2,</i> <i><br></i>  <i>"To": 3,</i> <i><br></i>  <i>"Watch": 4,</i> <i><br></i>  <i>"Movies": 5,</i> <i><br></i>  <i>"Also": 6,</i> <i><br></i>  <i>"Football": 7,</i> <i><br></i>  <i>"Games": 8,</i> <i><br></i>  <i>"Mary": 9,</i> <i><br></i>  <i>"Too": 10</i> <br><br>  Now each of these sentences can be represented as a vector of dimension 10, in which the number x in the i-th position means that the word number i occurs in the text x times: <br><blockquote>  [1, 2, 1, 1, 2, 0, 0, 0, 1, 1] <br>  [1, 1, 1, 1, 0, 1, 1, 1, 0, 0] </blockquote><br>  Read more in <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">Wikipedia</a> or in this review <a href="http://habrahabr.ru/post/263171/">article</a> dedicated to the same competition. <br><br>  In general, everything is simple, but the devil is in the details.  First, the example removes words <i>(a, the, am, i, is ...)</i> that do not carry any semantic meaning.  Secondly, operations with this matrix are performed in RAM, thus, the amount of memory limits the allowable dimension of the matrix.  To avoid the <i>‚ÄúMemoryError‚Äù,</i> I had to reduce the pool of words to 7000 of the most frequent.  The <a href="https://ru.wikipedia.org/wiki/Random_forest">Random Forest classifier is</a> used as a classifier in all examples of the textbook. <br><br>  Then we are encouraged to experiment with various parameters, which we will do.  The first obvious thought is to add <a href="https://ru.wikipedia.org/wiki/%25D0%259B%25D0%25B5%25D0%25BC%25D0%25BC%25D0%25B0%25D1%2582%25D0%25B8%25D0%25B7%25D0%25B0%25D1%2586%25D0%25B8%25D1%258F">lemmatization</a> , i.e.  lead all words to their vocabulary forms.  To do this, use the function from the nltk library: <br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> WordNetLemmatizer wnl = WordNetLemmatizer() meaningful_words = [wnl.lemmatize(w) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> w <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> meaningful_words]</code> </pre> <br>  Another good idea is to slightly change the text vectorization method.  Instead of the simple characteristic ‚Äúhow many times a word was encountered in a text‚Äù, you can use a little more complicated, but also well-known - <a href="https://ru.wikipedia.org/wiki/TF-IDF">tf-idf</a> (it assigns value to words depending on their rarity in the collection of documents). <br>  Sending to check the results of the original and modified program, we get an improvement from 0.843 to 0.844.  This is not very much.  Using this example as a basis, you can experiment thoroughly and get much better results.  But I did not have much time, and, consequently, attempts (they are limited to the 5th per day).  Therefore, I proceeded to the following parts. <br><br>  The following parts of the textbook are built on a library called <a href="https://code.google.com/p/word2vec/">Word2vec</a> , which gives us a numerical representation of words.  Moreover, these vectors have interesting properties.  For example, the minimum distance between vectors will be in the most similar words. <br><br>  So, transforming all the words, we get a list of vectors for each review.  How to convert it into a single vector?  The first option is to simply calculate the arithmetic <a href="https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors">average</a> ( <a href="https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors">average vector</a> ).  The result is even worse than the Bag of Words (0.829). <br><br>  How could this method be improved?  It is clear that there is no sense in averaging all the words, too much among them is rubbish, which does not affect the emotional coloring of the text.  Intuitively, it seems that evaluative adjectives and, perhaps, some other words will have the most influence.  Fortunately, there are methods under the general name of <a href="https://en.wikipedia.org/wiki/Feature_selection">feature selection</a> , which allow us to estimate how strongly one or another parameter (in our case, a word) correlates with the value of the resulting variable (emotional coloring).  Apply one of these methods and take a look at the selected words: <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.feature_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> chi2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.feature_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SelectKBest select = SelectKBest(chi2, k=<span class="hljs-number"><span class="hljs-number">50</span></span>) X_new = select.fit_transform(train_data_features, train[<span class="hljs-string"><span class="hljs-string">"sentiment"</span></span>]) names = count_vectorizer.get_feature_names() selected_words = np.asarray(names)[select.get_support()] print(<span class="hljs-string"><span class="hljs-string">', '</span></span>.join(selected_words))</code> </pre><br>  The result is a list of words that confirms the theory: <br><br><blockquote>  acting, amazing, annoying, avoid, awful, bad, badly, beautiful, best, boring, brilliant, crap, dull, even, excellent, fantastic, favorite, love, loved, mess, minutes, money, no, nothing, oh, pathetic, perfect, plot, pointless, poor, ridiculous, save, script, stupid, superb, supposed, terrible, waste, wasted, why, wonderful, worse, worst </blockquote><br>  If we now calculate the average vector, but taking into account only the words from the top list (which we expand to 500 words after a couple of experiments), we will get a better result (0.846), which bypasses even (albeit not much) bag of centroids from the following example of this competition.  In this solution (let's designate it as average of top words), Random Forest was also used as a classifier. <br><br><h4>  Bug work </h4><br>  At this the number of my attempts, and the actual competition, came to an end, and I went to the forum to find out how the more experienced people solved this problem.  I will not touch on <a href="https://www.kaggle.com/c/word2vec-nlp-tutorial/forums/t/14966/post-competition-solutions">solutions</a> that have a truly excellent result (more than 0.96) because they are usually quite complex and multi-pass.  But I will point out some options that made it possible to obtain high accuracy using simple methods. <br><br>  For example, the <a href="https://www.kaggle.com/c/word2vec-nlp-tutorial/forums/t/11261/beat-the-benchmark-with-shallow-learning-0-95-lb">indication</a> that a good result was achieved with a simple tf-idf and <a href="https://ru.wikipedia.org/wiki/%25D0%259B%25D0%25BE%25D0%25B3%25D0%25B8%25D1%2581%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F_%25D1%2580%25D0%25B5%25D0%25B3%25D1%2580%25D0%25B5%25D1%2581%25D1%2581%25D0%25B8%25D1%258F">logistic regression</a> prompted me to explore other classifiers.  Other things being equal (TfidfVectorizer with a limit of 7000 columns) LogisticRegression gives the result - 0.88, <a href="https://ru.wikipedia.org/wiki/%25D0%259B%25D0%25B8%25D0%25BD%25D0%25B5%25D0%25B9%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2580%25D0%25B5%25D0%25B3%25D1%2580%25D0%25B5%25D1%2581%25D1%2581%25D0%25B8%25D1%258F">LinearRegression</a> - 0.91, <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html">Ridge regression</a> - 0.92. <br><br>  If I used linear regression in my solution (average of top words) instead of Random forest, I would get the result of 0.93 instead of 0.84.  Thus, my first mistake was that I thought that the vectorization method influenced more than the choice of the classifier.  Erroneous thoughts prompted me to the material of an educational article, but I should have checked everything myself. <br><br>  I extracted the second idea by looking more closely at the code for this example.  The caveat is exactly how the TfidfVectorizer was used.  A set of combined test and training data was taken, no restrictions were placed on the maximum number of columns, moreover, features were formed not only from individual words, but also from <a href="https://ru.wikipedia.org/wiki/N-%25D0%25B3%25D1%2580%25D0%25B0%25D0%25BC%25D0%25BC">word pairs</a> (ngram_range parameter = (1, 2)).  If your program does not fall from a MemoryError of such a volume, then this significantly improves the prediction accuracy (the author claimed a result of 0.95).  Conclusion number two - accuracy can be increased at the cost of a larger amount of computation, and not some particularly clever methods.  To do this, for example, you can resort to some service for cloud computing, if your own computer is not very powerful. <br><br>  As a conclusion I want to say that it was extremely interesting to participate in the kaggle competition, and to encourage those who have not yet decided for any reason.  Of course, there are much more difficult contests on kaggle, for the first time you should choose the task according to your strengths.  And the last tip - read the forum.  Even during the competition, they publish useful tips, ideas, and sometimes even whole solutions. </div><p>Source: <a href="https://habr.com/ru/post/263399/">https://habr.com/ru/post/263399/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../263381/index.html">Registration for MBLTdev 15 - international conference of mobile developers has begun</a></li>
<li><a href="../263387/index.html">Research interstitial ads on Google+ with advertisements</a></li>
<li><a href="../263391/index.html">Automate it</a></li>
<li><a href="../263393/index.html">Organizing collaborative web development environment</a></li>
<li><a href="../263397/index.html">Bit Kung Fu, or How to optimize traffic between a mobile application and a server</a></li>
<li><a href="../263403/index.html">My code does not work :-(</a></li>
<li><a href="../263405/index.html">Video of reports from Zabbix Moscow Meetup</a></li>
<li><a href="../263407/index.html">Dirty secrets of express programming courses</a></li>
<li><a href="../263409/index.html">The end of the IT era</a></li>
<li><a href="../263413/index.html">Profiling Android applications for battery consumption</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>