<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>What is such a Data? Or again about MapReduce</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="If you spent the last 10 years on a remote island, without the Internet and in isolation from civilization, then especially for you, we will try to te...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>What is such a Data? Or again about MapReduce</h1><div class="post__text post__text-html js-mediator-article"><blockquote> <em><a href="https://habrahabr.ru/company/intersystems/blog/310180/"><img src="https://habrastorage.org/getpro/habr/post_images/fa7/7e6/27c/fa77e627c184bec95f61df62ccca0b54.jpg" align="left" width="288" height="240" alt="Big Fish Small Fry by John Pollack"></a></em>  <em>If you spent the last 10 years on a remote island, without the Internet and in isolation from civilization, then especially for you, we will try to tell you again about the MapReduce concept.</em>  <em>The introduction will be small, in sufficient volume, for the implementation of the MapReduce concept in the InterSystems Cach√© environment.</em>  <em>If you are not very far removed the last 10 years, then immediately go to the second part, where we create the foundations of infrastructure.</em> <em><br><br></em> </blockquote><br><a name="habracut"></a><br><p>  Let's immediately decide, I am not a big fan of MapReduce, which could be guessed from my previous articles / translations - <a href="https://habrahabr.ru/post/270017/">Michael Stonebriker - "Hadoop at the Crossroads"</a> and <a href="https://habrahabr.ru/post/267697/">"Command Line Utilities can be 235 times faster than your Hadoop cluster"</a> [If to be more precisely, I am not a fan of Java implementations of Hadoop MapReduce, but this is already personal] </p><br><p>  In any case, despite all these reservations and shortcomings, there are many more reasons that force us to return to this topic and try to implement MapReduce in a different environment and in another language.  We will announce all this later, but before that we'll talk about BigData ... </p><br><h2>  When is Data big and when is it small? </h2><br><p>  A few years ago, everyone began to go crazy over BigData, no one really knew when his small data became large, and where was the limit, but everyone understood that it was fashionable, youth and ‚Äúso‚Äù to do.  As time went on, some people announced that BigData was no longer a buzzword (this is pretty funny, but Gartner <a href="http://www.kdnuggets.com/2015/08/gartner-2015-hype-cycle-big-data-is-out-machine-learning-is-in.html">actually removed</a> BigData with a willful decision from the basewords for 2016, justifying it by the fact that the term was split into others).  Regardless of the desire of Gartner, the term BigData is still among us, very much alive, and I think it's time to decide on his understanding. </p><br><p>  For example, do we fully understand when our ‚Äúnot very big data‚Äù turns into ‚ÄúBIG DATA‚Äù? </p><br><p>  The most specific (of the reasonable) answers were given by David Kanter, one of the most respected experts on processor architecture in general and x86 in particular1: </p><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-0" style="position: static; visibility: visible; display: block; transform: rotate(0deg); width: 500px; margin: 10px auto; max-width: 100%; min-width: 220px;" data-tweet-id="559034352474914816"></twitter-widget><script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script></div><br><p>  <em>FWIW, when I, while working at Intel, switched to a hardware team working on a ‚Äúnext generation processor‚Äù (don'task), I started by studying materials about <a href="http://www.realworldtech.com/nehalem/">the</a> <a href="http://www.realworldtech.com/nehalem/">Nehalem</a> <a href="http://www.realworldtech.com/nehalem/">processor architecture</a> on the David Kanter website, and not from HAS and MAS internal docks .</em>  <em>Because as David was better and clearer.</em> </p><br><p>  Those.  if you have ‚Äúonly a couple of terabytes‚Äù of data, then you will most likely be able to find the hardware configuration of the server machine sufficient for all data to fit in the server‚Äôs memory (with enough money and motivation, of course) and your data not really big. </p><br><p>  BigData starts when this approach with vertical scaling (finding a ‚Äúbetter‚Äù machine) stops working, because  with a certain size of data you can no longer buy a larger configuration, for any (reasonable) money.  And we must begin to grow in breadth. </p><br><h2>  Easier - better </h2><br><p>  Ok, having decided on what size our data has grown to the BigData term, we need to decide on the approaches that work on big data.  One of the first approaches that began to be massively applied to big data was <a href="https://en.wikipedia.org/wiki/MapReduce">MapReduce</a> .  There are many alternative software models working with big data that may even be better or more flexible than MapReduce, but it can definitely be considered the most simplified, although it may not be the most effective. </p><br><p>  Moreover, as soon as we begin to consider some kind of software platform, or database platform, for BigData support, we assume by default that the MapReduce script is supported on this platform by internal or external utilities. </p><br><p>  <em>In other words - without MapReduce, you cannot claim that your platform supports BigData!</em> </p><br><p>  ALARM - if you have not been on the moon for the last 10 years, you can safely skip the story about the basics of the MapReduce algorithm, most likely you already know.  For the rest, we will try (once again) to tell about how it all began, and how you can use it all at the end of 2016.  (Especially on platforms where MapReduce is not supported out of the box.) </p><br><p>  It has often been observed that the simplest approach to solving a problem allows one to obtain the best results, and it remains to live in a product for a long time.  Outside of the original plan of the authors.  Even if, as a result, he is not the most effective, but due to the fact that the community has already widely recognized him and studied everything, and he is simply good enough and solves problems.  Approximately this effect is observed with the MapReduce model - being very simple at its core, it is still widely used even after the <a href="http://www.datacenterknowledge.com/archives/2014/06/25/google-dumps-mapreduce-favor-new-hyper-scale-analytics-system/">original authors declared its death</a> . </p><br><h2>  Scaling in Cach√© </h2><br><p>  Historically, InterSystems Cach√© had enough tools in its arsenal, both for vertical and horizontal scaling.  As we <em>all</em> know (sad smile) Cach√© is not only a database server, but also an application server that can use ECP (Enterprise Cache Protocol) for horizontal scaling and high availability. </p><br><p>  The peculiarity of the ECP protocol - being a highly optimized protocol for the coherence of access to the same data on different cluster nodes, strongly rests on the write daemon performance on the central node of the database server.  ECP allows you to add additional counting nodes with processor cores, if the write-daemon load is not very high, but this protocol does not help scale your application horizontally, if each of the nodes involved generates more write activity.  The disk subsystem on the database server will still be a bottleneck. </p><br><p>  In fact, when working with big data, modern applications involve the use of a different, or even orthogonal approach, voiced above.  You need to scale the application horizontally using the disk subsystem on each of the cluster nodes.  Unlike ECP, where data is brought to a remote node, we, on the contrary, bring a code, the size of which is assumed small, to data on each node, the size of which is assumed to be very large (at least relative to the size of the data).  A similar type of partitioning, called sharding, will be implemented in the future in the Cach√© SQL engine in one of the <em>future</em> products.  But even <em>today</em> , <em>using the tools available in the platform</em> , we can implement something simple that would allow us to design a horizontally scalable system using modern, ‚Äúfashionable, youth‚Äù approaches.  For example, using MapReduce ... </p><br><h2>  Google MapReduce </h2><br><p>  The original implementation of MapReduce was written in <a href="http://static.googleusercontent.com/media/research.google.com/ru/archive/mapreduce-osdi04.pdf">Google in C ++</a> , but it turned out that the widespread paradigm began in the industry only with the implementation of Apache's MapReduce, which is in Java.  In any case, regardless of the implementation language, the idea remains the same, whether it is implemented in C ++, Java, Go, or Cach√© ObjectScript, as in our case. </p><br><p>  [Although, for the Cach√© ObjectScript implementation, we will use a couple of tricks available only for operations with multidimensional arrays, known as <a href="">globals</a> .  Just because we can] </p><br><div style="text-align:center;"><img src="https://habrastorage.org/files/56e/cc4/174/56ecc41743d0499f8f52272190f98af3.png"></div><br><p>  Figure 1. Execution of the MapReduce environment in the <a href="http://static.googleusercontent.com/media/research.google.com/ru/archive/mapreduce-osdi04.pdf">‚ÄúMapReduce: Simplified Data Processing on Large Clusters‚Äù, OSDI-2004</a> </p><br><p>  Let's go through the stages of the MapReduce algorithm drawn in the picture above: </p><br><ol><li><p>  At the entrance, we have a set of "files", or a potentially infinite stream of data, which we can split (partition) into several independent pieces of data; </p><br></li><li><p>  We also have a set of parallel executors (local inside the node or may be remote, on other nodes of the cluster) which we can designate as handlers for input data pieces (‚Äúmap‚Äù / ‚Äú <strong>map</strong> ‚Äù stage) </p><br></li><li><p>  These parallel handlers read the input data stream and output key-value pair (s) to the output stream.  The output stream can be written to output files or somewhere else (for example, to the cluster file system Google GFS, Apache HDFS, or to some other ‚Äúmagic place‚Äù replicating data to several cluster nodes); </p><br></li><li>  At the next stage, called ‚Äúconvolution‚Äù / ‚Äú <strong>reduce</strong> ‚Äù, we have another set of handlers that deal with (surprise) ... convolution.  They read, for a given key, the entire collection of data, and output the resulting data as successive ‚Äúkey-values‚Äù.  The output stream of this stage, similarly to the previous stage, is recorded in the magic cluster file systems or their analogues. </li></ol><br><p>  Note that the MapReduce approach is batch in nature.  It does not handle endless input data streams very well, due to batch implementation, and will wait for the completion of work at each of its stages (‚Äúmapping‚Äù or ‚Äúconvolution‚Äù) before moving further in the pipeline.  This is different from the more modern flow algorithms used, for example, in Apache Kafka, which, by their design, are aimed at processing ‚Äúinfinite‚Äù input streams. </p><br><p>  Knowledgeable people missed this section, and unaware, I think, are still confused.  Let's look at a classic example of word-count (counting words in a data stream), which is traditionally used to explain the implementation of MapReduce in different programming languages ‚Äã‚Äãand in different environments. </p><br><p>  So, let's say we need to count the number of words in the input collection (rather large) of files.  For clarity, we define that a word will be considered a sequence as a character between whitespace, i.e.  numbers, punctuation are also considered part of the word, this is certainly not very good, but in the framework of a simple example, this does not bother us. </p><br><p>  Being a C ++ developer in the depths of my soul, for me the algorithm becomes clear when I see an example in C ++.  If ‚Äúyou are not like this‚Äù, then do not worry, soon we will show it in a simplified form. </p><br><pre><code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"mapreduce/mapreduce.h"</span></span></span><span class="hljs-meta"> </span><span class="hljs-comment"><span class="hljs-meta"><span class="hljs-comment">// User's map function class WordCounter : public Mapper { public: virtual void Map(const MapInput&amp; input) { const string&amp; text = input.value(); const int n = text.size(); for (int i = 0; i &lt; n; ) { // Skip past leading whitespace while ((i &lt; n) &amp;&amp; isspace(text[i])) i++; // Find word end int start = i; while ((i &lt; n) &amp;&amp; !isspace(text[i])) i++; if (start &lt; i) Emit(text.substr(start,i-start),"1"); } } }; REGISTER_MAPPER(WordCounter); // User's reduce function class Adder : public Reducer { virtual void Reduce(ReduceInput* input) { // Iterate over all entries with the // same key and add the values int64 value = 0; while (!input-&gt;done()) { value += StringToInt(input-&gt;value()); input-&gt;NextValue(); } // Emit sum for input-&gt;key() Emit(IntToString(value)); } }; REGISTER_REDUCER(Adder); int main(int argc, char** argv) { ParseCommandLineFlags(argc, argv); MapReduceSpecification spec; // Store list of input files into "spec" for (int i = 1; i &lt; argc; i++) { MapReduceInput* input = spec.add_input(); input-&gt;set_format("text"); input-&gt;set_filepattern(argv[i]); input-&gt;set_mapper_class("WordCounter"); } // Specify the output files: // /gfs/test/freq-00000-of-00100 // /gfs/test/freq-00001-of-00100 // ... MapReduceOutput* out = spec.output(); out-&gt;set_filebase("/gfs/test/freq"); out-&gt;set_num_tasks(100); out-&gt;set_format("text"); out-&gt;set_reducer_class("Adder"); // Optional: do partial sums within map // tasks to save network bandwidth out-&gt;set_combiner_class("Adder"); // Tuning parameters: use at most 2000 // machines and 100 MB of memory per task spec.set_machines(2000); spec.set_map_megabytes(100); spec.set_reduce_megabytes(100); // Now run it MapReduceResult result; if (!MapReduce(spec, &amp;result)) abort(); // Done: 'result' structure contains info // about counters, time taken, number of // machines used, etc. return 0; }</span></span></span></span></code> </pre> <br><ul><li><p>  The program above is called with a list of files that need to be processed, transmitted via standard argc / argv. </p><br></li><li><p>  The MapReduceInput object is instantiated as a wrapper for accessing each file in the input list and is scheduled for execution by the WordCount class to process its data; </p><br></li><li><p>  MapReduceOutput is instantiated to redirect output to the GoogleGFS cluster file system (note / gfs / test / *) </p><br></li><li><p>  The classes Reducer (scroller, hmm) and Combiner (combinator) are implemented by the C ++ class Adder, the text of which is given in the same program; </p><br></li><li><p>  The Map function in the Mapper class, implemented in our case in the WordCouner class, receives data through the generic MapInput interface.  In our case, this interface will deliver data from files.  The class implementing this interface must implement the value () method, supplying the following string as string, and the length of the input data in the size () method; </p><br></li><li><p>  As part of the solution of our task, counting the number of words in a file, we will ignore the whitespace characters and count everything else between spaces as a separate word (regardless of the punctuation marks).  We write the found word to the output ‚Äústream‚Äù by calling the function Emit (word, "1"); </p><br></li><li><p>  The Reduce function in the implementation class of the Reducer interface (in our case, this is Adder) gets its input data through another generalized interface, ReduceInput.  This function will be called for a specific key (words from a file, in our case) from the key-value pair recorded at the previous Map stage.  This function will be called to handle a collection of values ‚Äã‚Äãadvising this key (in our case for the sequence "1").  As part of our assignment, the responsibility of the Reducer function is to count the number of such units at the input and output the total number to the output channel. </p><br></li><li>  If we have built a cluster of several nodes, or just launching a lot of handlers within the MapReduce algorithm, then the responsibility of the ‚Äúmaster‚Äù will split the stream of output key-value pairs into the corresponding collections, and redirect these collections to the input Reduce handlers. </li></ul><br><p>  The details of the implementation of such a master node will depend heavily on the implementation protocol of the clustering technology used, including  we omit the detailed account of this beyond the current narrative brackets.  In our case, for Cach√© ObjectScript, for some of the algorithms in question (like the current WordCount) the master can be implemented trivially, due to the use of globals and their nature as sorted but sparse arrays.  What more later. </p><br><ul><li>  In general, it is often necessary to have several Reduce steps, for example, in cases where it is impossible to process a complete collection of values ‚Äã‚Äãin one go.  And then additional (s) Combiner stage (s) will appear, which will additionally aggregate the results of the data from the previous Reduce stages. </li></ul><br><p>  If, after such a detailed description of the C ++ implementation, you still do not understand what MapReduce is, then let's try to portray this algorithm on several lines of the same known scripting language: </p><br><pre> <code class="python hljs">map(String key, String value): // key: document name // value: document contents <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> each word w <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> value: EmitIntermediate(w, <span class="hljs-string"><span class="hljs-string">"1"</span></span>); reduce(String key, Iterator values): // key: a word // values: a list of counts int result = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> each v <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> values: result += ParseInt(v); Emit(AsString(result));</code> </pre> <br><p>  As we see in this simplified example, the responsibility of the map function will produce a sequence of pairs &lt;key, value&gt;.  These pairs are mixed and sorted in the wizard, and the resulting collections of values ‚Äã‚Äãfor a given key are sent to the input of the reduce functions (convolution), which, in turn, are responsible for generating the output &lt;key, value&gt; pair.  In our case, it will be &lt;word, counter&gt; </p><br><p>  In the classical implementation of MapReduce, the transformation of a collection of pairs &lt;key, value&gt; into separate collections of &lt;key, value (s)&gt; is the time and resource-intensive operation itself.  In the case of Cach√© implementations, both because of the nature of the implementation of the btree * repositories and the ECP interconnect protocol, sorting and aggregation on the wizard does not become such a big task, which is implemented almost on an automatic machine, almost without charge.  We will tell about this on occasion in the following articles. </p><br><p>  <em>Perhaps this is enough for the introductory part - we have not yet touched upon the actual Cach√© ObjectScript implementation, although we have provided enough information to start implementing MapReduce in any language.</em>  <em>We will return to our implementation of MapReduce in the next article.</em>  <em>Stay on the line!</em> </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/310180/">https://habr.com/ru/post/310180/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../310168/index.html">The potential benefits of iOS 10 for developing and testing mobile applications (Translated article)</a></li>
<li><a href="../310170/index.html">Forcing to caching: fasten L2 Apache Ignite cache to Activiti</a></li>
<li><a href="../310172/index.html">Functional programming jargon</a></li>
<li><a href="../310174/index.html">Exposing antivirus tests</a></li>
<li><a href="../310176/index.html">Logeek Night in Voronezh</a></li>
<li><a href="../310182/index.html">The problem of dissonance between the narrative and the gameplay</a></li>
<li><a href="../310186/index.html">Responsive Font Size</a></li>
<li><a href="../310188/index.html">Registration is now open for Russia's first international Testathon</a></li>
<li><a href="../310192/index.html">Another way to get around PornHub lock</a></li>
<li><a href="../310196/index.html">MapReduce from scrap materials. Part II - Basic Implementation Interfaces</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>