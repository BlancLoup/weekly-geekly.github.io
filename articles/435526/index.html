<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Adventures with a home Kubernetes cluster</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Note trans. : The author of the article, Marshall Brekka, is holding the position of director for systems design at Fair.com, which offers its applica...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Adventures with a home Kubernetes cluster</h1><div class="post__text post__text-html js-mediator-article">  <i><b>Note</b></i>  <i><b>trans.</b></i>  <i>: The author of the article, Marshall Brekka, is holding the position of director for systems design at Fair.com, which offers its application for car leasing.</i>  <i>In his free time from work, he likes to apply his extensive experience to solve ‚Äúdomestic‚Äù tasks that are unlikely to surprise any geek (therefore, the question ‚ÄúWhy?‚Äù - applied to the actions described later - is a priori omitted).</i>  <i>So, in its publication, Marshall shares the results of the recent deployment of Kubernetes on ... ARM boards.</i> <br><br><img src="https://habrastorage.org/webt/ul/nj/do/ulnjdoyysctwv-34jhuyn-wvsp8.png"><br><br>  Like many other geeks, over the past years I have accumulated a variety of development boards like Raspberry Pi.  And like many geeks, they were gathering dust on the shelves with the idea that they would someday come in handy.  And for me, this day has finally come! <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      During the winter holidays, there were several weeks out of work, within which there was enough time to inventory all accumulated iron and decide what to do with it.  Here is what I had: <br><br><ul><li>  5-disk RAID enclosure with USB3 connection; </li><li>  Raspberry Pi Model B (OG model); </li><li>  CubbieBoard 1; </li><li>  Banana Pi M1; </li><li>  HP netbook (2012?). </li></ul><br>  Of the 5 listed iron components, I used perhaps RAID and a netbook as a temporary NAS.  However, due to the lack of USB3 support in the netbook, RAID didn‚Äôt use all the speed potential. <br><br><h2>  Life goals </h2><br>  Since working with RAID was not optimal when using a netbook, I set myself the following goals to get the best configuration: <br><br><ol><li>  NAS with USB3 and gigabit ethernet; </li><li>  the best way to manage software on a device; </li><li>  (bonus) the ability to stream multimedia content from RAID to Fire TV. </li></ol><br>  Since none of the available devices supported USB3 and gigabit ethernet, unfortunately, I had to make additional purchases.  The choice fell on the board <a href="https://libre.computer/products/boards/roc-rk3328-cc/">ROC-RK3328-CC</a> .  She had all the necessary specifications and sufficient support for operating systems. <br><br>  Having solved my hardware needs (and awaiting the arrival of this solution), I switched to the second goal. <br><br><h2>  Software management on the device </h2><br>  Part of my past projects related to development boards failed due to insufficient attention to reproducibility and documentation issues.  When creating a new configuration for my current needs, I did not bother to write down either the steps taken or the links to the publications on the blogs I followed.  And when, after months or years, something went wrong and I tried to fix the problem, I did not have an understanding of how everything was originally arranged. <br><br>  So I told myself that this time everything will be different! <br><br><img src="https://habrastorage.org/webt/dm/vb/iv/dmvbivkoa65wfd1ve5mo5wh5jdc.jpeg"><br><br>  And he turned to what I know well enough - to Kubernetes. <br><br>  Although K8s is too hard a solution to a rather simple problem, after almost three years of managing clusters using various tools (own, kops, etc.) in my main job, I am very familiar with this system.  In addition, deploying K8s outside the cloud environment, and even on ARM devices, all this seemed to be an interesting task. <br><br>  I also thought that since the available hardware does not satisfy the necessary requirements for the NAS, I will try to at least assemble a cluster from it and, possibly, some software that is not so demanding of resources will be able to work on older devices. <br><br><h2>  Kubernetes on ARM </h2><br>  At work, I did not have the opportunity to use the <code>kubeadm</code> utility to deploy clusters, so I decided that now was the time to try it in action. <br><br>  Raspbian was chosen as the operating system because it is famous for better support of the boards I have. <br><br>  I found a <a href="https://blog.hypriot.com/post/setup-kubernetes-raspberry-pi-cluster/">good article</a> on setting up Kubernetes on Raspberry Pi using HypriotOS.  Since I was not sure about the availability of HypriotOS for all of my boards, I adapted these instructions for Debian / Raspbian. <br><br><h3>  Required components </h3><br>  For a start, the installation of the following tools was required: <br><br><ul><li>  Docker, </li><li>  kubelet </li><li>  kubeadm </li><li>  kubectl. </li></ul><br>  The docker should be installed using a special script - the <a href="https://docs.docker.com/install/linux/docker-ce/debian/">convenience script</a> (as indicated for the Raspbian use case). <br><br><pre> <code class="bash hljs">curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh</code> </pre> <br>  After that, I installed the Kubernetes components according to the instructions from the Hypriot blog, adapting them so that specific dependencies are used for all dependencies: <br><br><pre> <code class="bash hljs">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"deb http://apt.kubernetes.io/ kubernetes-xenial main"</span></span> &gt; /etc/apt/sources.list.d/kubernetes.list apt-get update apt-get install -y kubelet=1.13.1-00 kubectl=1.13.1-00 kubeadm=1.13.1-00</code> </pre> <br><h3>  Raspberry pi b </h3><br>  The first difficulty arose when trying to bootstrap a cluster on a Raspberry Pi B: <br><br><pre> <code class="bash hljs">$ kubeadm init Illegal instruction</code> </pre> <br>  It turned out that the <a href="https://github.com/kubernetes/kubeadm/issues/253">support for ARMv6</a> had been <a href="https://github.com/kubernetes/kubeadm/issues/253">removed from Kubernetes</a> .  Well, I also have CubbieBoard and Banana Pi. <br><br><h3>  Banana pi </h3><br>  Initially, it seemed that the same sequence of actions for the Banana Pi would be more successful, but the <code>kubeadm init</code> command ended with a timeout when trying to wait for the control plane to work: <br><br><pre> <code class="plaintext hljs">error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster</code> </pre> <br>  Finding out what was happening with the containers using <code>docker ps</code> , I saw that both <code>kube-controller-manager</code> and <code>kube-scheduler</code> had been working for at least 4-5 minutes, but <code>kube-api-server</code> had risen only 1-2 minutes ago: <br><br><pre> <code class="bash hljs">$ docker ps CONTAINER ID COMMAND CREATED STATUS de22427ad594 <span class="hljs-string"><span class="hljs-string">"kube-apiserver --au‚Ä¶"</span></span> About a minute ago Up About a minute dc2b70dd803e <span class="hljs-string"><span class="hljs-string">"kube-scheduler --ad‚Ä¶"</span></span> 5 minutes ago Up 5 minutes 60b6cc418a66 <span class="hljs-string"><span class="hljs-string">"kube-controller-man‚Ä¶"</span></span> 5 minutes ago Up 5 minutes 1e1362a9787c <span class="hljs-string"><span class="hljs-string">"etcd --advertise-cl‚Ä¶"</span></span> 5 minutes ago Up 5 minutes</code> </pre> <br>  Obviously, the <code>api-server</code> was dying, or the strontium process killed and restarted it. <br><br>  While checking the logs, I saw very standard start-up procedures ‚Äî there was a record of the start of listening to the safe port and a long pause before the appearance of numerous errors in TLS handshakes: <br><br><pre> <code class="plaintext hljs">20:06:48.604881 naming_controller.go:284] Starting NamingConditionController 20:06:48.605031 establishing_controller.go:73] Starting EstablishingController 20:06:50.791098 log.go:172] http: TLS handshake error from 192.168.1.155:50280: EOF 20:06:51.797710 log.go:172] http: TLS handshake error from 192.168.1.155:50286: EOF 20:06:51.971690 log.go:172] http: TLS handshake error from 192.168.1.155:50288: EOF 20:06:51.990556 log.go:172] http: TLS handshake error from 192.168.1.155:50284: EOF 20:06:52.374947 log.go:172] http: TLS handshake error from 192.168.1.155:50486: EOF 20:06:52.612617 log.go:172] http: TLS handshake error from 192.168.1.155:50298: EOF 20:06:52.748668 log.go:172] http: TLS handshake error from 192.168.1.155:50290: EOF</code> </pre> <br>  And soon after this, the server is shutting down.  Googling has led to <a href="https://github.com/kubernetes/kubernetes/issues/61277">this problem</a> , indicating a possible reason for the slow operation of cryptographic algorithms on some ARM devices. <br><br>  I went ahead and thought that maybe the <code>api-server</code> receives too many duplicate requests from the <code>scheduler</code> and <code>controller-manager</code> . <br><br>  Extracting these files from the manifest directory will tell kubelet to stop the execution of the corresponding pods: <br><br><pre> <code class="bash hljs">mkdir /etc/kubernetes/manifests.bak mv /etc/kubernetes/manifests/kube-scheduler.yaml /etc/kubernetes/manifests.bak/ mv /etc/kubernetes/manifests/kube-controller-mananger.yaml /etc/kubernetes/manifests.bak/</code> </pre> <br>  Viewing the latest logs of the <code>api-server</code> showed that the process now went on, but still died after about 2 minutes.  Then I remembered that the manifesto could contain liveness tests with timeouts that are too low for such a slow device. <br><br>  Therefore, I checked <code>/etc/kubernetes/manifests/kube-api-server.yaml</code> - and in it, of course ... <br><br><pre> <code class="plaintext hljs">livenessProbe: failureThreshold: 8 httpGet: host: 192.168.1.155 path: /healthz port: 6443 scheme: HTTPS initialDelaySeconds: 15 timeoutSeconds: 15</code> </pre> <br>  Pod was killed after 135 seconds ( <code>initialDelaySeconds</code> + <code>timeoutSeconds</code> * <code>failureThreshold</code> ).  Increasing the <code>initialDelaySeconds</code> value to 120 ... <br><br>  <b>Success!</b>  Well, errors in the handshakes still occur (presumably from the kubelet), but the launch still took place: <br><br><pre> <code class="plaintext hljs">20:06:54.957236 log.go:172] http: TLS handshake error from 192.168.1.155:50538: EOF 20:06:55.004865 log.go:172] http: TLS handshake error from 192.168.1.155:50384: EOF 20:06:55.118343 log.go:172] http: TLS handshake error from 192.168.1.155:50292: EOF 20:06:55.252586 cache.go:39] Caches are synced for autoregister controller 20:06:55.253907 cache.go:39] Caches are synced for APIServiceRegistrationController controller 20:06:55.545881 controller_utils.go:1034] Caches are synced for crd-autoregister controller ... 20:06:58.921689 storage_rbac.go:187] created clusterrole.rbac.authorization.k8s.io/cluster-admin 20:06:59.049373 storage_rbac.go:187] created clusterrole.rbac.authorization.k8s.io/system:discovery 20:06:59.214321 storage_rbac.go:187] created clusterrole.rbac.authorization.k8s.io/system:basic-user</code> </pre> <br>  When the <code>api-server</code> up, I moved the YAML files for the controller and scheduler back to the manifests directory, after which they also started up normally. <br><br>  Now it's time to make sure that the download will pass successfully if you leave all the files in the source directory: just enough changes to the allowable delay in the initialization of the <code>livenessProbe</code> ? <br><br><pre> <code class="plaintext hljs">20:29:33.306983 reflector.go:134] k8s.io/client-go/informers/factory.go:132: Failed to list *v1.Service: Get https://192.168.1.155:6443/api/v1/services?limit=500&amp;resourceVersion=0: dial tcp 192.168.1.155:6443: i/o timeout 20:29:33.434541 reflector.go:134] k8s.io/client-go/informers/factory.go:132: Failed to list *v1.ReplicationController: Get https://192.168.1.155:6443/api/v1/replicationcontrollers?limit=500&amp;resourceVersion=0: dial tcp 192.168.1.155:6443: i/o timeout 20:29:33.435799 reflector.go:134] k8s.io/client-go/informers/factory.go:132: Failed to list *v1.PersistentVolume: Get https://192.168.1.155:6443/api/v1/persistentvolumes?limit=500&amp;resourceVersion=0: dial tcp 192.168.1.155:6443: i/o timeout 20:29:33.477405 reflector.go:134] k8s.io/client-go/informers/factory.go:132: Failed to list *v1beta1.PodDisruptionBudget: Get https://192.168.1.155:6443/apis/policy/v1beta1/poddisruptionbudgets?limit=500&amp;resourceVersion=0: dial tcp 192.168.1.155:6443: i/o timeout 20:29:33.493660 reflector.go:134] k8s.io/client-go/informers/factory.go:132: Failed to list *v1.PersistentVolumeClaim: Get https://192.168.1.155:6443/api/v1/persistentvolumeclaims?limit=500&amp;resourceVersion=0: dial tcp 192.168.1.155:6443: i/o timeout 20:29:37.974938 controller_utils.go:1027] Waiting for caches to sync for scheduler controller 20:29:38.078558 controller_utils.go:1034] Caches are synced for scheduler controller 20:29:38.078867 leaderelection.go:205] attempting to acquire leader lease kube-system/kube-scheduler 20:29:38.291875 leaderelection.go:214] successfully acquired lease kube-system/kube-scheduler</code> </pre> <br>  Yes, everything works, although such old devices, apparently, were not intended to launch the control plane, since repeated TLS connections cause significant brakes.  One way or another - the working installation of K8s on ARM is received!  Let's go further ... <br><br><h3>  RAID mounting </h3><br>  Since SD cards are not suitable for writing in the long term, for the most volatile parts of the file system, I decided to use more reliable storage ‚Äî in this case, RAID.  On it were divided into 4 sections: <br><br><ul><li>  50 GB; </li><li>  2 √ó 20 GB; </li><li>  3.9 TB. </li></ul><br>  I haven‚Äôt yet come up with a specific purpose for 20 GB partitions, but I wanted to leave additional possibilities for the future. <br><br>  In the <code>/etc/fstab</code> for the partition with 50 GB, the mount point was specified as <code>/mnt/root</code> , and for 3.9 TB - <code>/mnt/raid</code> .  After that, I mounted the directories with etcd and docker to the 50 GB partition: <br><br><pre> <code class="plaintext hljs">UUID=655a39e8-9a5d-45f3-ae14-73b4c5ed50c3 /mnt/root ext4 defaults,rw,user,auto,exec 0 0 UUID=0633df91-017c-4b98-9b2e-4a0d27989a5c /mnt/raid ext4 defaults,rw,user,auto 0 0 /mnt/root/var/lib/etcd /var/lib/etcd none defaults,bind 0 0 /mnt/root/var/lib/docker /var/lib/docker none defaults,bind 0 0</code> </pre> <br><h3>  Arrival ROC-RK3328-CC </h3><br>  When the new board was delivered, I installed the necessary components for the K8s <i>(see the beginning of the article)</i> and launched <code>kubeadm init</code> .  A few minutes of waiting is the success and output of the <code>join</code> command to run on other nodes. <br><br>  Fine!  No fuss with timeouts. <br><br>  And since RAID will also be used on this board, the mount setting will be required again.  To summarize all the steps: <br><br><h4>  1. Mount disks in / etc / fstab </h4><br><pre> <code class="plaintext hljs">UUID=655a39e8-9a5d-45f3-ae14-73b4c5ed50c3 /mnt/root ext4 defaults,rw,user,auto,exec 0 0 UUID=0633df91-017c-4b98-9b2e-4a0d27989a5c /mnt/raid ext4 defaults,rw,user,auto 0 0 /mnt/root/var/lib/etcd /var/lib/etcd none defaults,bind 0 0 /mnt/root/var/lib/docker /var/lib/docker none defaults,bind 0 0</code> </pre> <br><h4>  2. Installing Docker and K8s binaries </h4><br><pre> <code class="bash hljs">curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh</code> </pre> <br><pre> <code class="bash hljs">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"deb http://apt.kubernetes.io/ kubernetes-xenial main"</span></span> &gt; /etc/apt/sources.list.d/kubernetes.list apt-get update apt-get install -y kubelet=1.13.1-00 kubectl=1.13.1-00 kubeadm=1.13.1-00</code> </pre> <br><h4>  3. Configure a unique host name (important because many nodes are added) </h4><br><pre> <code class="bash hljs">hostnamectl <span class="hljs-built_in"><span class="hljs-built_in">set</span></span>-hostname k8s-master-1</code> </pre> <br><h4>  4. Initializing Kubernetes </h4><br>  I omit the phase with control plane, because I want to be able to plan normal pods on this node: <br><br><pre> <code class="bash hljs">kubeadm init --skip-phases mark-control-plane</code> </pre> <br><h4>  5. Installing Network Plugin </h4><br>  The information about this in the Hypriot article was a bit outdated, since the Weave network plugin is now also <a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">supported on ARM</a> : <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">export</span></span> KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f <span class="hljs-string"><span class="hljs-string">"https://cloud.weave.works/k8s/net?k8s-version=</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$(kubectl version | base64 | tr -d '\n')</span></span></span><span class="hljs-string">"</span></span></code> </pre> <br><h4>  6. Adding Node Labels </h4><br>  On this site, I'm going to start the NAS server, so I‚Äôll mark it with labels for future use in the scheduler: <br><br><pre> <code class="bash hljs">kubectl label nodes k8s-master-1 marshallbrekka.raid=<span class="hljs-literal"><span class="hljs-literal">true</span></span> kubectl label nodes k8s-master-1 marshallbrekka.network=gigabit</code> </pre> <br><h3>  Connect other nodes to the cluster </h3><br>  Setting up other devices (Banana Pi, CubbieBoard) was just as easy.  For them, you need to repeat the first 3 steps (by changing the settings for mounting disks / flash-media depending on their availability) and execute the <code>kubeadm join</code> command instead of <code>kubeadm init</code> . <br><br><h2>  Finding Docker Containers for ARM </h2><br>  Building most of the necessary Docker containers normally runs on a Mac, but for ARM everything is somewhat more complicated.  Having found many articles on how to use QEMU for this purpose, I still came to the <a href="https://github.com/linuxserver/">conclusion</a> that most of the applications I <a href="https://github.com/linuxserver/">need</a> are already assembled, and many of them are available on <a href="https://github.com/linuxserver/">linuxserver</a> . <br><br><h2>  Next steps </h2><br>  Still not having received the initial configuration of devices in such an automated / scripted form, as we would like, I at least compiled a set of basic commands (mount'y, calls <code>docker</code> and <code>kubeadm</code> ) and documented them in the Git-repository.  The rest of the applications used also got the YAML configurations for K8s stored in the same repository, so getting the necessary configuration from scratch is now very simple. <br><br>  In the future, I would like to achieve the following: <br><br><ol><li>  make master nodes highly accessible; </li><li>  add monitoring / notifications to know about failures in any components; </li><li>  change the DCHP settings of the router to use the DNS server from the cluster in order to simplify the detection of applications (who wants to remember the internal IP addresses?); </li><li>  Run <a href="https://metallb.universe.tf/">MetalLB</a> to <a href="https://metallb.universe.tf/">forward</a> cluster services to a private network (DNS, etc.). </li></ol><br><br><h2>  PS from translator </h2><br>  Read also in our blog: <br><br><ul><li>  ‚Äú <a href="https://habr.com/company/flant/blog/432748/">Kubernetes tips &amp; tricks: about allocating nodes and the load on the web application</a> ‚Äù; </li><li>  ‚Äú <a href="https://habr.com/company/flant/blog/427745/">Kubernetes tips &amp; tricks: access to dev sites</a> ‚Äù; </li><li>  ‚Äú <a href="https://habr.com/company/flant/blog/417509/">Kubernetes tips &amp; tricks: speeding up the bootstrap of large databases</a> ‚Äù; </li><li>  ‚Äú <a href="https://habr.com/company/flant/blog/417905/">11 ways to (not) become a victim of hacking at Kubernetes</a> ‚Äù; </li><li>  ‚Äú <a href="https://habr.com/company/flant/blog/415381/">Play with Kubernetes is a service for getting to know K8s in practice</a> .‚Äù </li></ul></div><p>Source: <a href="https://habr.com/ru/post/435526/">https://habr.com/ru/post/435526/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../435512/index.html">Developers from Royole showed a folding flexible smartphone</a></li>
<li><a href="../435514/index.html">Russia is developing a processor to accelerate neural networks</a></li>
<li><a href="../435518/index.html">Paying customs duty for online purchase will be much easier.</a></li>
<li><a href="../435520/index.html">We write our programming language, part 3: Translator Architecture. Parsing language structures and mathematical expressions</a></li>
<li><a href="../435522/index.html">Snapshots of events in Axonframework 3, improving performance</a></li>
<li><a href="../435528/index.html">5 reasons for success: why Amazon has become the most expensive company in the world</a></li>
<li><a href="../435530/index.html">Paid Subscriptions - Dependency of Auto Connection on Mobile</a></li>
<li><a href="../435532/index.html">Tornado vs Aiohttp: a journey into the wilds of asynchronous frameworks</a></li>
<li><a href="../435534/index.html">Data Science: entry level books</a></li>
<li><a href="../435536/index.html">Humanoid robots: the benefits and problems of anthropomorphic mechanisms</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>