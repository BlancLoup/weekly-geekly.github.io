<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>MongoDB for growth</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Greetings fighters invisible backend! 


 You have already read MongoDB reviews. Probably, there were excellent online courses at university.mongodb.c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>MongoDB for growth</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/web/ce7/d41/d4d/ce7d41d4d9f04a8b8668b785fb28fa94.png" alt="image" align="right">  Greetings fighters invisible backend! </p><br><p>  You have already read MongoDB reviews.  Probably, there were excellent online courses at <a href="https://university.mongodb.com/">university.mongodb.com</a> .  Of course, you already have a promising prototype project using MongoDB. <br><br clear="right"></p><br><p>  What can we expect from MongoDB at this stage? </p><br><ul><li>  Cheaper storage - reading from slave replicas saves the iops master, no RAID is required, the failure of one disk is not fatal. </li><li>  Increasing the speed of development - you can allow more carelessness in the design of data structures, because  we can easily fix everything on a running application. </li><li>  We increase the responsiveness of the application - regardless of the development, it is easy to increase the number of leading replicas or the number of shards to compensate for the increased load on the application. </li><li>  We increase the reliability of the application - regardless of the development, we remove a single point of failure. </li></ul><br><p>  And now, you are ready to get involved in a fight - to release the project to the public. </p><a name="habracut"></a><br><h2 id="o-chem-statya">  What is the article about? </h2><br><p>  This is an attempt to summarize your development experience with MongoDB.  My first practical acquaintance with this DBMS took place in 2010 in the form of a prototype project.  After completing the first 10gen online courses, I confidently applied MongoDB in parallel with SQL, but only on auxiliary and research projects. </p><br><p>  Since 2013, I have been working in <a href="https://www.smartcat.ai/">Smartcat</a> as a leading developer.  All new projects we started only with the use of MongoDB.  We do most of the development in C #, but there are projects in other languages.  In all projects I either designed the data scheme myself, or actively advised colleagues.  Throughout the life of the project, I watched the nature of the use of the DBMS.  In 2015, the main project of the Smartcat website was smoothly transferred from MSSQL to MongoDB. </p><br><p>  This is a description of what needs to be considered in the design so that the project is flexible in scaling. <br>  You can get into a situation where there is a successfully working prototype project.  While there are few users, all optimizations are postponed.  But, the service "spins up", users have already arrived, and the work can not be stopped.  And scaling is impossible without deep refactoring of the database data scheme, and simple queries that worked yesterday, today, put the system. </p><br><p>  My main area of ‚Äã‚Äãdevelopment is a C # backend, so examples will be in this vein. </p><br><p>  So, let's try to describe the stages of project development in terms of using MongoDB, from a single server to a shard cluster and the reasons that force us to move on to the next, more complex configuration of database servers. </p><br><h1 id="standalone">  Standalone </h1><br><p>  Of course, for the 24/7 project, we do not consider a single server.  But if this is an internal project or an office solution, then it will be fine.  It is only necessary to plan the possibility of stopping the service to remove the backup or to provide for software export / import of all the necessary data.  This configuration of special issues in the administration does not cause, but the reliability does not shine. </p><br><p>  Basically, single servers are used for development.  This can be a common server - in this case, it is convenient to invite a colleague to develop an application on a single database.  This can be a locally running service. </p><br><p>  Most importantly, we expect that the software solutions in the client code will work identically in terms of replicates.  And also, even though it feels like a distant prospect, in a shard cluster. </p><br><h1 id="replikaciya">  Replication </h1><br><p>  Replication is a necessary condition for the continuous work of a combat project.  We absolutely count on: </p><br><ul><li>  Automatic or manual recovery after the failure of individual servers-copies. </li><li>  The possibility of "freezing" the state of the database for research or backup removal. </li><li>  Moving from one data center to another without shutting down the site. </li></ul><br><p>  When working replicas we must constantly take care to: </p><br><ul><li>  Do not lose network connectivity and leading server. </li><li>  Slave is not far behind. </li><li>  Do not lose sync due to opLog exhaustion. </li></ul><br><p>  Maybe later we will be able to read something from the slave servers to offload our only writing server. </p><br><h2 id="sostav-serverov-replikaset-i-ozhidanie-bolshinstva-pri-zapisi">  The composition of the servers replicates and the majority of waiting when recording </h2><br><p>  In deployment schemes, we denote servers as: P - primary, S - slave, H - hidden, A - arbiter. <br>  Waiting to record most of the replicas hereinafter will be - <em>w: majority</em> . </p><br><p>  On non-critical projects (this is if we can afford manual recovery) the following configurations can be used: </p><br><ul><li>  PS - blocking the slave, you can make a backup, but at the same time hang all the expectations <em>w: majority</em> . </li><li>  PH - hidden replica blocking does not interfere with the application, but it cannot be used for reading. </li><li>  PSH - there are backups, <em>w: majority</em> too, but we are sensitive to server shutdown. </li></ul><br><p>  If cluster auto-recovery is required, then the minimum number of servers is 4 in the PSSH configuration.  This is the optimal configuration for us now.  Backups are regularly removed from the hidden replica, and stopping the service on any machine does not threaten the application.  Sometimes it takes a long time to decommission one of the servers.  You can stop backups at this time and include a hidden replica in the balancing, or you can add another server to the replica. </p><br><p>  By the way, configuration with the arbiter should be used very carefully.  The arbitrator increases the total number of servers, but does not write data.  It turns out that waiting for <em>w: majority</em> requires a large proportion of servers.  So, on the PSA configuration, in case of failure of P or S, the ability to write will be, but without <em>w: majority</em> . </p><br><p>  In general, when planning a configuration, you should list all combinations of server failures in order to understand which of them you can compensate automatically.  And for manual recovery, you should immediately prepare the instructions. </p><br><h2 id="ischerpanie-oplog">  OpLog Exhaustion </h2><br><p>  This is a log of write operations to the database.  Each slave server replicates it to itself from the master, and the write operations are played on the local copy of the data. </p><br><p>  The first problem is that opLog has a limited size and is overwritten cyclically.  The main danger is that with intensive recording the opLog history will cover a short time.  For any playback delay, for example, as a result of CPU overload or network latency or disk recording, the host server may lose the last recording.  In this case, it will stop the synchronization and go into the <em>RECOVERING</em> mode.  With intensive recording, there is a very high risk of several slave servers leaving this mode.  As a result, you can lose the quorum for selecting a master server, and the cluster will stop accepting data for recording.  Cluster recovery is a manual procedure. </p><br><p>  We call it - "destroyed" replicas. </p><br><p>  The second problem is that the volume of replication traffic can become an obstacle for the separation of servers into different data centers. </p><br><p>  We monitor the size of the opLog in seconds, the difference between the time of the first and last records.  For example, the minimum size of opLog - 10Ks - allows you to make a backup and not lose server synchronization.  If the average opLog size is 100Ks, then this is enough to have time to react when it drops sharply. </p><br><p>  When this happens: </p><br><ul><li>  Simultaneous raid users with adding content or editing. </li><li>  Error in business logic, when we start to update a lot of documents. </li></ul><br><p>  The saddest thing is that such errors are difficult to detect when profiling queries.  It is necessary to analyze opLog directly. </p><br><p>  Examples of errors in the backend code: </p><br><h3 id="flush-potoka-zapisi-v-gridfs">  Flush stream entries in GridFS </h3><br><p>  When exporting translation results, we stream files to a zip archive, and it is also streamed to GridFS.  The file chunk size was increased to 2 Mb, and the recorded files usually had a size of 200‚Äì900 Kb. </p><br><p>  After each file was written to the archive, the <em>Stream.Flush ()</em> method in the library we used was called.  And in MongoCSharpDriver v1.11 there is an interesting feature of working with the stream, which we did not immediately find out: when calling the <em>Stream.Flush ()</em> method, the current recorded chunk of the file was updated in the database. </p><br><p>  For example, when writing a file in 6 Mb, we expect that opLog will get 3 inserts of chunks of 2 Mb each, i.e.  we assumed that the data in OpLog would be slightly larger than the file size. </p><br><p>  The trouble came with the archive, in which there were many files of 3-5 Kb. </p><br><p>  To simplify the calculations, we will assume that we are recording one file of 2 Mb in size (that is, we expect to write to one 2 megabyte chunk).  The <em>Flush ()</em> method will be called every 4 Kb, and for the entire streaming time it will be called 512 times.  Moreover, the size of the update each time will increase from 4 Kb to 2 Mb linear.  This is an arithmetic progression with a delta of 4 Kb. <br>  The sum of the first 512 items <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><mn>4</mn><mi>K</mi><mi>b</mi><mo>+</mo><mn>2</mn><mi>M</mi><mi>b</mi><mo stretchy=&quot;false&quot;>)</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mn>2</mn><mo>&amp;#x2217;</mo><mn>521</mn><mo>=</mo><mn>513</mn><mi>M</mi><mi>b</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="31.51ex" height="2.66ex" viewBox="0 -832 13566.9 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-28" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-34" x="389" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMATHI-4B" x="890" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMATHI-62" x="1779" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-2B" x="2431" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-32" x="3431" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMATHI-4D" x="3932" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMATHI-62" x="4983" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-29" x="5413" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-2F" x="5802" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-32" x="6303" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-2217" x="7026" y="0"></use><g transform="translate(7748,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-35"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-32" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-31" x="1001" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-3D" x="9528" y="0"></use><g transform="translate(10584,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-35"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-31" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-33" x="1001" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMATHI-4D" x="12085" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMATHI-62" x="13137" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mn>4</mn><mi>K</mi><mi>b</mi><mo>+</mo><mn>2</mn><mi>M</mi><mi>b</mi><mo stretchy="false">)</mo><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>2</mn><mo>‚àó</mo><mn>521</mn><mo>=</mo><mn>513</mn><mi>M</mi><mi>b</mi></math></span></span><script type="math/tex" id="MathJax-Element-1"> (4Kb + 2Mb) / 2 * 521 = 513 Mb </script><br>  Instead of 2 Mb we lose in opLog 513 Mb - unexpectedly ... </p><br><p>  The first solution is to cache the file in memory before writing to the database, but now we are using the latest version of the driver v2.4.4.  There this problem is solved - the <em>Flush ()</em> method does not update the data in the database. </p><br>
<h3 id="obnovlenie-vlozhennogo-massiva">  Update Nested Array </h3><br><p>  Nested arrays in MongoDB can greatly simplify development.  But they should be used very carefully.  Here is another example from practice. </p><br><p>  In our data scheme, the document describes the task of exporting the file list.  The file list is described by a nested array, the element of which contains a link to the file and the readiness status.  Task processing looks like this: </p><br><ul><li>  read the document </li><li>  look for the next file in the nested array and process it, </li><li>  update the readiness status of the document. </li></ul><br><p>  At the development stage, the number of documents was limited to manually putting check marks in the UI.  Usually it was about 5, a maximum of 10 files. </p><br><p>  Particularly stubborn users could "click" page in 50 elements.  Therefore, the developer simplified the algorithm - the entire array was updated immediately.  The record size is approximately 300 bytes. </p><br><p>  10 updates - <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>300</mn><mo>&amp;#x2217;</mo><mn>10</mn><mo>&amp;#x2217;</mo><mn>10</mn><mo>=</mo><mn>29</mn><mi>K</mi><mi>b</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="21.013ex" height="2.057ex" viewBox="0 -780.1 9047.4 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-33"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-30" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-30" x="1001" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-2217" x="1723" y="0"></use><g transform="translate(2446,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-30" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-2217" x="3669" y="0"></use><g transform="translate(4392,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-30" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-3D" x="5671" y="0"></use><g transform="translate(6727,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-32"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-39" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMATHI-4B" x="7728" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMATHI-62" x="8617" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>300</mn><mo>‚àó</mo><mn>10</mn><mo>‚àó</mo><mn>10</mn><mo>=</mo><mn>29</mn><mi>K</mi><mi>b</mi></math></span></span><script type="math/tex" id="MathJax-Element-2"> 300 * 10 * 10 = 29 Kb </script></p><br><p>  It seems a little.  But, as you can see, the volume consumption of opLog became quadratically dependent on the number of files.  A little later, thanks to the development of the convenience of UI, it became possible to choose the export of all files attached to the project. </p><br><p>  Sequential export of several projects with 1000 files each - and the falling opLog was caught by forced shutdown of the task handler. </p><br><p>  The fix is ‚Äã‚Äãtrivial - updating the array element by its sequence number. </p><br><p>  Massive updates are irrelevant if the server is in standalone mode.  If the stored data is temporary, then it can even be a regular database operation mode. <br>  If development is underway for use under replication conditions, then in testing it is necessary to use a cluster with replication enabled.  And the tests themselves should contain examples with extreme values ‚Äã‚Äãof the embedded data, size and number of files processed. </p><br><h2 id="rollback-pri-smene-mastera">  Rollback when changing masters </h2><br><p>  Rollback - is the abolition of part of the write operations on the replica, if they are not confirmed by the majority.  This can happen after she, being a master, was turned off, and then she started working again. </p><br><p>  Changing the wizard may be unintentional: in case of server restart, emergency shutdown of the service or network failure.  Or it can be planned: when updating a service or OS, changing the size of opLog, moving, resync, testing a new server configuration or a database version. </p><br><p>  As a result, rollback can lose some of the recorded data, this is a feature of MongoDB replication.  For those documents (changes) that cannot be lost (for example, notes about payment for services) you should include <em>w: majority</em> .  In this case, after a successful wait, the cancellation of such an operation cannot occur. </p><br><p>  Default settings expect to write only to the leading server.  We assume a typical execution time of such an operation in 0.01 s.  If we expect <em>w: majority</em> , then the typical time increases to 0.06 s, and only if the majority of the replicas are in the same data center. </p><br><p>  Fortunately, for most operations there is no need to expect <em>w: majority</em> .  Some data can be obtained when re-calculating (for example, exporting a file), some of them are irrelevant in case of failure (for example, the last user work time). </p><br><p>  How not to arrange rollback within server maintenance: </p><br><ul><li>  If you need to move the wizard from a specific server, then <em>rs.stepDown ()</em> will <em>do</em> . </li><li>  If you need to make the master a specific server, then it needs to raise the priority. </li></ul><br><p>  If there is a suspicion that the data was canceled through rollback, then you can search for them in the rollback directory on the servers. </p><br><h2 id="sozdanie-indeksa">  Create index </h2><br><p>  If we do not want to block the database when building a new index, we build it in the background (option background: true).  But in terms of replication, the construction of any index will be launched on each slave c blocking playback of the opLog - there will be a jump in the lag, and if the recording intensity increases, it is not far to the collapse.  Also, if the index is built for a long time, the waiting <em>w: majority</em> can be interrupted. </p><br><p>  Before building the index, measure its creation time on similar data to estimate the downtime on the battle cluster. </p><br><p>  There is a way to build a large index without blocking the slave. </p><br><ol><li>  On each slave server, perform a sequence of actions: <br><ol><li>  We switch the server to a single mode. </li><li>  Run the construction of the desired index and wait for the end. </li><li>  We return the server to the replicas and wait for its synchronization. </li></ol></li><li>  Start building the desired index on the master server. </li></ol><br><h2 id="proverka-prohozhdeniya-resync">  Checking the passage of <em>resync</em> </h2><br><p>  Resync is copying all the replica data and rebuilding the indexes.  This may occur when a new database server is entered into a replica or, if you wish, you can defragment free space. </p><br><p>  The size of the database of a popular, loaded service may increase over time.  Accordingly, the time taken to copy data and build indexes, and the amount of memory for building indexes, is increasing. </p><br><p>  It is easy to miss the moment when you will not be able to easily attach another replica to a running cluster.  And this is an important procedure in the event of server loss, upgrade, or moving to another data center.  If this happens, you can try to copy the service data directory or remove the server disk image. </p><br><p>  I usually think that resync should take no more than <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mn>3</mn></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.487ex" height="2.66ex" viewBox="0 -832 1501.5 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-32" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-2F" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/335698/&amp;xid=25657,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiK9EqB2UQ4pK_GjIjdWBbFe_SaLw#MJMAIN-33" x="1001" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>2</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>3</mn></math></span></span><script type="math/tex" id="MathJax-Element-3"> 2/3 </script>  opLog size. </p><br><p>  By the way, there is a way to increase the size of opLog without data loss - <a href="https://docs.mongodb.com/v3.2/tutorial/change-oplog-size/">Change oplog size</a> </p><br><h2 id="bekap">  Backup </h2><br><p>  The process of copying data takes time, and to get their snapshot (snapshot), we use the <em>db.fsyncLock ()</em> command on the hidden replica to stop recording to all collections.  After that, using the mongodump utility, copy all the data into the backup storage. </p><br><p>  In the process of removing the backup, the state of the data is increasingly lagging behind the master.  Of course, you need to make sure that the time taken to remove the backup is no longer than the duration of the opLog. </p><br><p>  There are many more ways to make a backup: a copy of the disk image, a copy of the database directory, a snapshot of the virtual machine. </p><br><p>  Mongodump is the slowest way, but it has advantages: </p><br><ul><li>  Does not depend on the virtualization system. </li><li>  It does not depend on the version of the database (restoring old backups may require the deployment of the previous version of the database). </li><li>  You can restore selected collections or even some documents, and as practice has shown, fatal errors in code or during administration rarely affect more than one collection. </li></ul><br><h2 id="chtenie-s-vedomyh-serverov">  Reading from slave servers </h2><br><p>  A copy of the data, albeit a bit outdated, itself asks for it to be read.  This helps unload the processor and disk on the primary server, but unfortunately, not all data can be read from the slave servers. </p><br><p>  The average time lag of replicas in the local network is one and a half seconds.  If monitoring for lagging is set, then you can do, for example: </p><br><ul><li>  Calculation of statistics for the last day. </li><li>  Archiving irrelevant data. </li><li>  Reading "long" recorded files from the GridFS. </li></ul><br><p>  But the only recording server is still waiting for our next step. </p><br><h1 id="sharding">  Sharding </h1><br><p>  Sharding is a standard way to scale MongoDB horizontally.  In my opinion, it is rather difficult to determine at what point a transition from one replicates to a shard cluster can be justified. </p><br><p>  The administrative overhead and the cost of additional servers or virtualwoks are not easy to submit to the project management.  Instead of four servers, we must immediately go to eight, plus we need at least three servers for the configuration replicaset. </p><br><p>  As an alternative to the shard cluster, for a long time it will be possible to gradually increase the number of CPU cores, the size of RAM and the size of disks, as well as switch to RAID or replace HDD with SSD. </p><br><p>  Unfortunately, there is a limit to increasing the size of a single server.  For example, we rested on the speed of reading from disks.  At this time, our database was located in Azure.  The servers used a 512 Gb SSD with a maximum iops 2300. Of course, a simpler step is to upgrade the subscription to 1Tb with a maximum iops 5000, but further database growth made it impossible to remove and restore the backup in a reasonable time. </p><br><p>  The fact is that the time taken to remove the 450 Gb backup came close to 6 hours.  Further growth of the database led to the fact that it would have to increase the size of the opLog.  He was at that time in 32 Gb.  In addition, the average opLog duration was 12 hours (yes, this is a modest data stream of only 750 Kb / s), and any unexpected load, such as importing a very large file, caused the server from which the backup was taken to lose synchronization , and he had to be taken to resync for 12 hours with an obscure result. </p><br><p>  On the other hand, the introduction of the shard cluster reduced the backup time, so the size of the opLog could also be reduced.  The project has ceased to exceed iops.  Well, from the point of view of development and administration, migration to a shard cluster occurs once, and you can add new shards repeatedly and as needed. </p><br><p>  Adding shards is a simple way to compensate for the speed limits of the disk subsystem, to shorten the backup time and restore the database.  In the future, it will be possible to reduce the size of the opLog, the amount of RAM and CPU cores on each replica. <br>  It would be very good to achieve uniform load sharing for shards.  With the growth of the project, this makes it possible without additional development to keep the responsiveness of the database and administration at the required level.  Unfortunately, the uniformity of the load will strongly depend on the data scheme. </p><br><p>  When you turn on sharding, we have 2 main groups of problems. </p><br><ol><li>  Sharding collection is not possible if: <br><ul><li>  the update of the fields included in the sharding key is required; </li><li>  There are several unique keys on the collection; </li><li>  under the result of the findAndModify query, the data on different shards falls. </li></ul></li><li>  Unbalanced load on shards, if: <br><ul><li>  partially ordered identifiers are used; </li><li>  weak sharding key selectivity; </li><li>  queries without sharding key values. </li></ul></li></ol><br><h2 id="objectid">  Objectid </h2><br><p>  This is a globally unique identifier (similar to the GUID), and it is partially ordered.  In practice, this means that if there is an intensive addition of records to the collection, they will fall mainly on one shard.  Worse yet, fresh documents are usually the most requested - we also get a serious imbalance on the CPU and opLog. </p><br><p>  There is a sharding method using a hash, but it applies only to a single field. <br>  In this case, search queries and updates without specifying an identifier will load all shards at once. <br>  Those.  the benefit of sharding by hash will be only if you work with the document only by its identifier. </p><br><h2 id="gridfs">  Gridfs </h2><br><p>  Data scheme for storing unmodifiable files.  It seems that this is the first candidate for sharding, but, alas, not everything is so simple. </p><br><p>  In GridFS, chunks (‚Äúpieces‚Äù of the downloaded file) have the following scheme: </p><br><pre><code class="javascript hljs">{ <span class="hljs-string"><span class="hljs-string">"_id"</span></span> : &lt;ObjectId&gt;, "files_id" : &lt;TFileId&gt;, "n" : &lt;Int32&gt;, "data" : &lt;binary data&gt; }</code> </pre> <br><p>  The MongoDB driver usually tries to create a unique index by <em>{files_id: 1, n: 1}</em> </p><br><p>  There are 2 ways to separate chunks: </p><br><ol><li>  Only by file ID is an example of weak sharding key selectivity. <br>  This ensures that the file will not be shared between shards, which means that it will be easier to restore it from the backup.  If the file exceeds the maximum chunk size, it will become unmovable.  The risk of unbalance in the size of the data on the shard increases. </li><li>  By file identifier and chunk sequence number. <br>  The maximum selectivity of the sharding key is up to one document (file chunk) per sharding chunk. </li></ol><br><p>  If the file ID is standard (TFileId is ObjectId), then we have a difficult choice: </p><br><ol><li>  Adding an index on a hash and sharding on a hash from <em>files_id</em> leads to the risk of getting unmovable files and unbalance in place. </li><li>  Sharding by <em>{files_id: 1, n: 1}</em> threatens to unbalance the opLog and iops disk, and if there is an intensive addition of files, then it is better not to include sharding at all. </li></ol><br><p>  So it makes sense to first change the ID to GUID - this will save you from many problems in the future. </p><br><h2 id="unikalnost-klyucha">  Key uniqueness </h2><br><p>  You cannot turn on sharding on a collection that has several indexes with uniqueness. <br>  If, however, several fields are required for checking for uniqueness ‚Äî for example, an email and a phone number associated with one user ‚Äî then this case will have to be resettled into a separate collection. </p><br><p>  Schemes of work may be different: </p><br><ul><li><p>  Sequential inserts. <br>  Example: <br>  collection of users (index with uniqueness by email) </p><br><pre> <code class="javascript hljs">{ <span class="hljs-attr"><span class="hljs-attr">_id</span></span>: &lt;ObjectId&gt;, //  email: &lt;string&gt;, phone: &lt;string&gt; }</code> </pre> <br><p>  phones collection (phone index with uniqueness) </p><br><pre> <code class="javascript hljs">{ <span class="hljs-attr"><span class="hljs-attr">_id</span></span>: &lt;ObjectId&gt;, //  phone: &lt;string&gt; }</code> </pre> <br><p>  First, the insertion into phones is done, and if successful, the insertion into users. </p><br></li><li>  Delayed setting. <br>  In the scheme from the previous case - the phone is always stored separately, and the user is allowed to add it after registration. </li></ul><br><h2 id="vybor-klyucha-shardirovaniya-na-suschestvuyuschey-kollekcii">  Select sharding key on an existing collection </h2><br><p>  When designing a collection data scheme, we may not know (ignore) how we will shard it.  But there are no regular ways to change the sharding key, so it is especially important not to miss the key selection. </p><br><p>  The selection algorithm may be as follows: </p><br><ol><li>  Once again we recall the <a href="https://docs.mongodb.com/manual/reference/limits/">limitations</a> . </li><li>  We add others to the existing indexes with all possible (and meaningful) combinations of fields. </li><li>  We list all candidates for keys sharding.  For each valid index, you can select a key by its any prefix.  For example, for the index <em>{a: 1, b: 1, c: 1}</em> you can select the prefixes <em>{a: 1}</em> and <em>{a: 1, b: 1}</em> or the full key <em>{a: 1, b: 1, c: 1}</em> . <br>  Important!  If the field is in the index, but the shard key is built by a prefix without this field, then this field remains changeable. </li><li>  For each candidate we evaluate: <br><ul><li>  Selectivity, is there a limit on the growth of the minimum chunk. </li><li>  The ability to add specific search key values ‚Äã‚Äãto each search query. </li><li>  The ability to exclude the update of the field that fell into the key of sharding. </li><li>  The share of requests that are not isolated on a single shard. </li></ul></li><li>  Select the key with the maximum share of isolated requests. </li></ol><br><h2 id="sharding-po-prefiksu-indeksa-s-unikalnostyu">  Sharding on the prefix index with uniqueness </h2><br><p>  An example of key selection, when I had to stop the choice on a less selective sharding key. </p><br><p>  Entities in our Smartcat project: </p><br><ul><li>  Account (Account) - the owner of the documents. </li><li>  Document (Document) - a document for translation (identifier globally unique). </li><li>  Segment - proposal in the document. </li></ul><br><p>  Segment Fields: </p><br><ul><li>  accountId - account identifier. </li><li>  idInAccount - unique identifier within the account. </li><li>  documentId - document identifier. </li><li>  order - the sequence number of the segment in the document. </li><li>  other... </li></ul><br><p>  Indices: </p><br><ul><li>  <em>{documentId: 1, idInAccount: 1}</em> </li><li>  <em>{documentId: 1, order: 1}</em> </li><li>  <em>{accountId: 1, idInAccount: 1}, {unique: true}</em> </li></ul><br><p>  Most requests include a documentId.  So, we will choose from the first two indices, and this excludes the accountId sharding.  Then we will remove the uniqueness from the third index.  Each document has a globally unique identifier and cannot belong to two accounts.  Therefore, the first index can be made unique, and the second can be used to remove uniqueness. </p><br><p>  In our code, <em>idInAccount</em> is known only in one search query, i.e.  When choosing a full sharding key, document segments can be divided into different chunks, and search queries can refer to multiple shards. </p><br><p>  Consider the version of sharding by <em>{documentId: 1}</em> - this is valid because  it is a prefix with uniqueness.  We estimate the maximum volume of segments of a single document - this is approximately 50 Mb.  So, the document is perfectly included in the size of a single chard sharding (we have 64 Mb). </p><br><p>  Result: </p><br><ul><li>  We build an index with uniqueness on <em>{documentId: 1, idInAccount: 1}</em> </li><li>  We remove uniqueness with <em>{accountId: 1, idInAccount: 1}</em> </li><li>  Enable sharding by <em>{documentId: 1}</em> </li></ul><br><h2 id="udalenie-bd-ili-kollekciy">  Deleting a database or collections </h2><br><p>  On the combat service, this usually does not happen.  The database or collection is created, indexes are added, and sharding is turned on.  And then - long operation. </p><br><p>  With test stands more interesting.  As a rule, removing the database is a regular procedure.  With the advent of the shard cluster as part of the test bench, we began to detect a heterogeneous collection of collections depending on the router used (mongos).  It turned out that this is a known <a href="https://jira.mongodb.org/browse/SERVER-17397">problem</a> . </p><br><p>  In short, deleting the database or collection requires resetting the cache of each router and additional cleaning in the shard cluster configuration. </p><br><p>  But for test stands, you can do easier: </p><br><ul><li>  Delete all collections in the database.  Her lead shard does not change. </li><li>  Run scripts to add indexes. </li><li>  Run sharding enable scripts - this is a distributed transaction across all routers.  It guarantees the cleaning of old data in all caches. </li></ul><br><h1 id="rezyume">  Summary </h1><br><p>  Many decisions when choosing a data scheme for sharding or replication may appear to be a re-complication during the initial development of a project or prototyping. </p><br><p>  But if you go for simplification to the detriment of scaling, you should clearly reserve time for refactoring the data schema. </p><br><p>  Checklist for planning a data schema and index collection: </p><br><ol><li>  Save replication traffic - minimize document update size. </li><li>  Remember about rollback - business logic should support a break in write operations, as well as a sudden shutdown of the server. </li><li>  We do not interfere with sharding - on large collections we do not have more than one index with uniqueness, but it is better not to include uniqueness at all if it is not critical for business logic. </li><li>  We isolate search queries with a single shard - most search queries must include sharding key values. </li><li>  Balancing shards - carefully select the type of identifier, the best of them is the GUID. </li></ol><br><p>  That's all for now! <br>  Do not lose the cluster, fighters invisible backend! </p><br><p>  And special thanks to the colleague <a href="https://habrahabr.ru/users/nameless_one/" class="user_link">nameless_one</a> for <a href="https://habrahabr.ru/users/nameless_one/" class="user_link">editing</a> and advice! </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/335698/">https://habr.com/ru/post/335698/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../335688/index.html">Part 2. At first they steal, and when you win, they kill you</a></li>
<li><a href="../335690/index.html">Service out of the box: set up ServiceNow "in 60 seconds"</a></li>
<li><a href="../335692/index.html">Colorful code: how color helps in working with code</a></li>
<li><a href="../335694/index.html">Digital agency / production performance indicators (examples from the West)</a></li>
<li><a href="../335696/index.html">The idea of ‚Äã‚Äãblack and white cinema, correct bees and unarmed ninjas in site optimization</a></li>
<li><a href="../335700/index.html">Honest realtime on React and Redux, as the basis of auto auction</a></li>
<li><a href="../335702/index.html">Study of the safety of the transport system of Tbilisi - or how to ride a vehicle and earn money</a></li>
<li><a href="../335704/index.html">Mamba: open-source crypto-fiber</a></li>
<li><a href="../335706/index.html">Reducing prices for local SSD drives in ‚Äúdisplaced‚Äù instances and instances on demand</a></li>
<li><a href="../335708/index.html">5 reasons why you should build a small business - not a startup</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>