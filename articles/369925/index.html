<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>GFDM and tensors. Continuation</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="At first I did not want to delve into the tensors and describe them in passing, referring only to the functionality I used. However, I changed my mind...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>GFDM and tensors. Continuation</h1><div class="post__text post__text-html js-mediator-article">  At first I did not want to delve into the tensors and describe them in passing, referring only to the functionality I used.  However, I changed my mind and decided to tell more.  Welcome to the multidimensional world. <br><img src="https://habrastorage.org/files/557/124/17a/55712417a29a43f48098764024143a6d.png"><br><a name="habracut"></a><br>  Part 1 <a href="https://geektimes.ru/post/282844/">here</a> <br><div class="spoiler">  <b class="spoiler_title">Abbreviations</b> <div class="spoiler_text">  CP - CANDECOMP / PARAFAC <br>  CANDECOMP - Canonical decomposition <br>  PARAFAC - parallel Factors <br>  PARATCUK2 - PARAFAC / Tucker2 <br>  ALS - Alternating least squares <br>  PMNK - interleaved least squares method <br>  SVD - Singular value decomposition <br>  SLAU - System of linear algebraic equations <br>  GFDM - generalized frequency division multiplexing <br>  SECSI - SEmi-algebraic framework for approximat eCP decompositions via SImultaneous Matrix Diagonalizations <br></div></div><br><div class="spoiler">  <b class="spoiler_title">Used notation</b> <div class="spoiler_text">  As lowercase letters without bold type, scalar values ‚Äã‚Äãare indicated. Like this: <br><img src="https://habrastorage.org/files/bfe/519/a5a/bfe519a5ade24ac9adf116d12d5d43cc.png"><br>  As lowercase letters in bold type, vectors are indicated.  Like this: <br><img src="https://habrastorage.org/files/3af/c80/aa2/3afc80aa2f8a4a138936be96c7d0efd9.png"><br>  Matrices are indicated as uppercase letters in bold.  Like this: <br><img src="https://habrastorage.org/files/5d4/a38/a37/5d4a38a37352487ebba01f2d0b3bf57b.png"><br>  As uppercase calligraphic letters denoted by tensors.  Like this: <br><img src="https://habrastorage.org/files/8ee/9b0/3ce/8ee9b03ce5e64e2b87b8eab54df67d2c.png"><br>  Subscripts indicate the position in the matrix in rows and columns. <br><img src="https://habrastorage.org/files/e03/63a/9a1/e0363a9a1d1246f692ecc630cc588321.png"><br>  In scans, subscript characters inform about the space being deployed. <br><img src="https://habrastorage.org/files/941/4a1/166/9414a11665f44308ade6a33eed2f6015.png"><br>  Subscript characters in tensors inform about the frontal section of the tensor, unless otherwise specified. <br><img src="https://habrastorage.org/files/fb9/299/41a/fb929941a88248cdbbd8b043cd29dd60.png"><br></div></div><br><br><h4>  Tensors </h4><br>  Perhaps the best definition of a tensor would be the quote of Tamara G. Kolda: <blockquote>  A tensor is a multidimensional array </blockquote>  There is simply no place.  Ideas with tensors arose initially in linear algebra.  Therefore, the best way is to describe everything using linear algebra.  In linear algebra, we use scalars, vectors, matrices, and we can perform addition, subtraction, and multiplication operations.  The scalar consists of 1 element, the vector consists of <i>N</i> elements and the matrix consists of <i>MN</i> elements.  Tensors include everything that linear algebra can and even a bit more.  Giving examples from linear algebra: a vector is a first-order tensor, and a matrix is ‚Äã‚Äãa second-order tensor.  Consequently, the order of the tensor is determined by the number of spaces in which the tensor has more than one element.  The more spaces you have, the higher the order of the tensor is.  Simple enough, right? <br>  Let's immediately give an example of a third order tensor.  Meet with this tensor today you will work and you will see a bit of tensor magic on it.  I wrote his frontal sections below. <br><br><div style="text-align:center;"><img width="300" height="100" src="https://habrastorage.org/files/8b8/372/f97/8b8372f971f34559a5450063c5a9e273.png"></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Linear algebra defines to us the product between two matrices as follows: <br><br><div style="text-align:center;"><img width="120" height="20" src="https://habrastorage.org/files/5cd/947/936/5cd947936cc44d81a0643a19d2959b6e.png"></div><br><br>  In this work, two dimensions are specified and the operation between objects with dimensions greater than two is not defined.  Tensor algebra gives tools how to get around this restriction, using the features of more spaces in linear algebra.  To begin, I will introduce three matrix operations, without which you cannot step in tensor algebra and step: <br><br><div class="spoiler">  <b class="spoiler_title">Kronecker's work</b> <div class="spoiler_text">  So, in the Kronecker product (or as it is also called the tensor product) two matrices are involved, the result of their work will be a block matrix, each block of which is the second matrix multiplied by a certain element of the first matrix. <br><img src="https://habrastorage.org/files/f99/0dc/3b9/f990dc3b995d486ea8f8883f6dc73589.png"><br>  This operation is used to form a simplified channel model in MIMO. <br></div></div><br><div class="spoiler">  <b class="spoiler_title">artwork Khatri-Rao</b> <div class="spoiler_text">  The Khatri-Rao product is denoted by the following symbol and is a Kronecker product for all the corresponding columns.  In other words, each <i>i-</i> th column from matrix <b>A</b> is multiplied by the <i>i-</i> th column of matrix <b>B</b> and the result is the <i>i-th</i> column of the resulting matrix. <br><img src="https://habrastorage.org/files/3c7/dab/a9a/3c7daba9ac0a42288080f1cdd35f7737.png"><br>  This operation is most often used in third order tensors, for example through it I described the modulation matrix of the GFDM system. <br></div></div><br><div class="spoiler">  <b class="spoiler_title">Hadamard's Work</b> <div class="spoiler_text">  The Hadamard product is the simplest of all, it is an elementwise product of all positions in the matrices <b><i>A</i></b> and <b><i>B.</i></b> <br><img src="https://habrastorage.org/files/876/ebb/554/876ebb554a6041209b4c4e7091a19d41.png"><br></div></div><br><h4>  Basic operations in tensor algebra </h4><br><h4>  Scan </h4><br>  In tensor algebra, one of the key concepts is ‚Äúunfolding‚Äù (I will call it a sweep to reduce anglicisms) or the representation of a tensor through a matrix.  A sweep is a mapping of a tensor to one of its spaces.  The tensor in this operation is recorded as a matrix, the number of rows of which is equal to the number of elements of the developed space.  Elements in the row have the same sequence number for the specified space. <br><br><div class="spoiler">  <b class="spoiler_title">Feature</b> <div class="spoiler_text">  There are two types of recording elements in a line: according to the Lauer notation and according to the Matlab notation.  The most commonly used notation is Matlab, so I will use it.  According to the Matlab notation, elements go in ascending order in dimension, i.e.  first comes the element in the first dimension, then in the second, and so on, excluding the unfolding dimension. <br></div></div><br><br><div style="text-align:center;"><img width="300" height="100" src="https://habrastorage.org/files/47a/3e2/95f/47a3e295fd1f45e28ebc549f64423ca7.png"></div><br><br><div style="text-align:center;"><img width="300" height="100" src="https://habrastorage.org/files/37d/154/778/37d154778d1d4467b3c25c7d4a474e78.png"></div><br><br><div style="text-align:center;"><img width="460" height="60" src="https://habrastorage.org/files/8ca/2a1/dff/8ca2a1dffde74e488dd52711173f6d3a.png"></div><br><br>  The number of possible sweeps is equal to the order of the tensor or its dimension.  The tensor from our example has 3 sweeps.  Above are the sweeps for our tensor, and I hope it will be much easier for you to understand them than for explanations.  Tensors seemed to me much clearer in an intuitive sense, rather than by definition. <br><br><h4>  The product of a given space </h4><br>  The next operation I want to describe is the product of a tensor with a matrix in a given space.  It is written in the following way and in essence it is the product of two matrices, where the right-hand matrix is ‚Äã‚Äãthe development of the tensor in the multiplied space, and on the left the matrix with which is actually multiplied. <br><br><div style="text-align:center;"><img width="210" height="30" src="https://habrastorage.org/files/4f8/9c7/0ed/4f89c70ede8444a38064d6df7b4a55ec.png"></div><br><br>  The important point is that the multiplication is written to the right, and happens to the left of the tensor.  The result is a matrix which is a scan of the new tensor, which can be assembled back into the usual tensor.  It is important to note that as a result of this operation, the dimension of the space with which the multiplication occurs <b>can change</b> , the others <b>cannot</b> .  Example below <br><br><img src="https://habrastorage.org/files/12d/af6/d76/12daf6d767884996acd1ff03e40709e4.png"><br><br><h4>  Unit Rank Tensor </h4><br>  The unit rank tensor is such a tensor of dimension <i>N</i> , which can be obtained by multiplying one by <i>N</i> vectors, one for each space.  How it looks visually for the third-order tensor I showed below.  A unit rank tensor is analogous to a single component for an SVD matrix, where a complex matrix is ‚Äã‚Äãdecomposed into unit rank matrices.  A unit rank tensor can be both complex and real, depending on the requirements we make to it. <br><br><div style="text-align:center;"><img width="400" height="30" src="https://habrastorage.org/files/1a2/b57/387/1a2b5738726d422e8bd6692ca0e31a5e.png"></div><br><img src="https://habrastorage.org/files/557/124/17a/55712417a29a43f48098764024143a6d.png"><br><br><h4>  Decomposition of tensors </h4><br>  The most important and interesting in tensors is their decomposition.  I hope many of you know or have heard about decomposition of a matrix into singular values.  With it, we can present the matrix as the sum of matrices of unit rank, and evaluate their contribution to the common matrix.  Below is an example of such a decomposition. <br><br><img src="https://habrastorage.org/files/6b7/0ae/99d/6b70ae99df634aac826f9c7890b22af7.png"><br><br>  There is a very similar analogue in tensor algebra and it is called ‚ÄúCP‚Äù, it is also called ‚ÄúCANDECOMP‚Äù or ‚ÄúPARAFAC‚Äù.  ‚ÄúCP‚Äù decomposes any tensor for the sum of unit rank tensors.  This formulation has two forms of notation, the simplest of which actually records the tensor as the sum of the set of tensors of unit rank, while the other writes the decomposition through matrices in each of the spaces.  In addition to the decomposition of "CP" in tensor algebra there are a large number of its analogues, depending on the valuation of the components and other features. <br><br><div style="text-align:center;"><img width="300" height="80" src="https://habrastorage.org/files/40c/0aa/1a3/40c0aa1a31e245bebe6974db636e80ed.png"></div><br><img src="https://habrastorage.org/files/ebe/5ff/5fa/ebe5ff5fa30147ccabcde78dbd31334c.png"><br><br>  Look, if you write the vectors of each of the spaces one by one, you will get N matrices.  Each of the matrices represents a certain ‚Äúbasis‚Äù in its space.  This form of recording resembles the SVD matrix.  From these matrices one can get back the tensor, if we take the unit tensor and alternately multiply it by all matrix bases in the spaces. <br><br><div style="text-align:center;"><img width="300" height="40" src="https://habrastorage.org/files/f6a/38d/4dd/f6a38d4dd57143f6b3dbe4b2326f874b.png"></div><br><img src="https://habrastorage.org/files/011/214/71f/01121471f22c4438a219efa52ab36fc7.png"><br><br>  From the decomposition of the third-order tensor, it is easy to derive a sweep expression; it will allow one of the matrices to be separated by the usual matrix product.  Look below, see, the matrix <b><i>A is</i></b> recorded through a work with a different matrix, which in turn is a product of the work of Khatri-Rao.  Knowing the product of the product and the initial scan, we can calculate the matrix <b><i>A</i></b> using linear algebra.  This greatly simplifies the work of the decomposition calculation algorithms for third order tensors and gives convenient formulas for iterations. <br><br><div style="text-align:center;"><img width="230" height="30" src="https://habrastorage.org/files/c61/499/7f1/c614997f11f149929f8c292780331256.png"></div><br>  Why do we need this nonsense, we could do the SVD sweep?  Of course they could, but in the general case the decomposition of the tensor makes it much easier to describe the data, it becomes easier to compress or find patterns.  This is especially important for large amounts of data, since with increasing dimensions, the rank changes much less. <br><br><h4>  Decomposition problems </h4><br>  This expansion, for all its charms, has two problems.  First: the rank of this expansion you need to know for yourself.  This is often a non-trivial task, especially if the data has noise. <br><br><div class="spoiler">  <b class="spoiler_title">An example from my experience</b> <div class="spoiler_text">  The ordinate represents the normalized recovery error and the abscissa used is the rank of the decomposition.  I only decomposed the tensor with the sum of the signal and noise and only noise.  The assumption of rank had to be made only by the unusual behavior of the error of expansion.  Attention, the legend is confused. <br><br><img src="https://habrastorage.org/files/877/7af/db4/8777afdb496348f2ac89706a4007bab0.png"><br><br>  The algorithm was to search for a GPS signal when calculating the correlation by the polyphase Fourier transform.  In this case, the dimension of the output data on the correlator was equal to four (time, frequency, code, shift of the polyphase transformation). <br></div></div><br>  The second problem is the calculation of the decomposition itself.  The most common algorithm at the moment is <i>ALS</i> or <i>PMNK</i> .  There is also an algorithm with the romantic name <i>SECSI</i> , but I will probably tell about it in the future.  <i>PMNK</i> or interleaved least squares method is simple to disgrace, it has the following algorithm: <br><br><ul><li>  Set the rank of the expansion </li><li>  We generate random decomposition matrices of spaces. </li><li>  We fix all matrices except one and calculate the remaining matrix with the help of SLAE </li><li>  Repeat the previous paragraph for all matrices in turn, until the discrepancy becomes acceptable </li></ul><br><br><div style="text-align:center;"><img width="330" height="30" src="https://habrastorage.org/files/bf4/b0d/5e8/bf4b0d5e89404e6d8f30c088ddbe2fc8.png"></div><br><div style="text-align:center;"><img width="350" height="40" src="https://habrastorage.org/files/b38/012/4ad/b380124ad3dd4a30b4a1e791d74e3fa4.png"></div><br>  The drawbacks of this method are a great many, these are local minima, and computational complexity, but it still remains one of the most used algorithms at the present time. <br><br><h4>  GFDM communication </h4><br><br><img src="https://habrastorage.org/files/090/0fe/c9b/0900fec9bb02444d889c3c96ce56554e.png"><br><br>  And now let's move closer to GFDM technology.  There is another decomposition called PARATUCK2, and this is an abbreviation for the other two abbreviations "PARAFAC" and "TUCKER2".  <i>Yes, an abbreviation consisting of two other abbreviations, how wonderful it is.</i> <br><br>  This decomposition writes the tensor through three matrices and two tensors.  The matrix in the middle is called the decomposition kernel, the tensors are called unifying.  Extreme two matrices have no special name.  How to calculate the tensor from this expansion is a separate story. <br><br><img src="https://habrastorage.org/files/2cf/5e0/e04/2cf5e0e04c39456fa707de5326d49041.png"><br><br>  The tensor from the decomposition is considered in layers, in each layer only the corresponding layer of interrelation tensors is selected.  As a result, 5 matrices are obtained, multiplying which we obtain the values ‚Äã‚Äãof the tensor.  This operation is repeated for each of the layers, and the results are collected one by one.  The number of rows of the first matrix is ‚Äã‚Äãequal to the number of rows of the tensor, similarly with the columns of the third matrix and tensor, as well as with the depth of interrelation tensors.  It was very difficult for me to understand at the first moment.  <u>And what is most interesting, this model can really help you with data analysis.</u> <br><br>  So why do we need this decomposition?  It is quite complex and consists of five elements, how can it even help with GFDM?  Let's first recall and fix a bit about GFDM.  Below is the symbol generation method that the transmitter sends.  A data block can be formed as a matrix, where the rows are subcarriers, and the columns are temporary positions in the block.  The total data block is obtained by summing all the elements, in other words, it is necessary to multiply the element-wise vector of the subcarrier with the vector of the window function, and all this by the symbol that we transmit.  Thus, each symbol is located at the intersection of two vectors with which it is multiplied.  The operation is quite simple, but it is difficult to describe it through the product of the modulation matrix and the vector of symbols.  The form of the record through the modulation matrix is ‚Äã‚Äãthe main building block for building receivers. <br><br><img src="https://habrastorage.org/files/b51/d66/f31/b51d66f31a9a4180ae6b12c1b0aac90c.png"><br><br>  If you take a closer look, PARATUCK2 can really describe GFDM.  To do this, we need to take the vector row and column vector as the extreme matrices, and add window functions and subcarriers to the relationship tensors along the main diagonals.  If symbols appear in the core matrix, the result will be a vector in the third dimension and correspond to the transmitted data. <br><br><div class="spoiler">  <b class="spoiler_title">Example from experience</b> <div class="spoiler_text">  Initially, I additionally intended to describe the frequency-dependent channel through the values ‚Äã‚Äãin the vector next to the subcarrier tensor.  This was an erroneous assumption. <br></div></div><br><br><img src="https://habrastorage.org/files/6cd/7a6/e70/6cd7a6e70fed4580bcc624ce09f3c88d.png"><br><br>  Famously wrapped, is not it?  When I was writing a diploma, it was difficult to understand how this can be simplified and brought to an acceptable form, but it is possible!  Through this decomposition, one can simplify the expression of the modulation matrix into the product of two matrices.  However, I think to tell the next time, along with the theory of Zero-forcing, Matched filter and Minimal Mean squared error receivers. <br><br><div class="spoiler">  <b class="spoiler_title">References</b> <div class="spoiler_text">  TG Kolda and BW Bader, \ Tensor decompositions and applications, "SIAM Review, <br>  vol.  51, no.  3, pp.  455 {500, 2009. <br>  R. Bro, N. Sidiropoulos, and G. Giannakis, \ Parafac: Tutorial and applications, " <br>  Chemometrics Group, Food Technology, Royal Veterinary Agricultural University, 1997. <br>  L. de Lathauwer, B. de Moor, and J. Vanderwalle, \ A multilinear singular value decom- <br>  position, "SIAM J. MATRIX ANAL. APPL. Vol. 21, No. 4, pp. 1253 1278, 2000. <br>  KB Petersen and MS Pedersen, The Matrix Cookbook.  Petersen &amp; Pedersen ,, 2012. <br>  S. LIU and G. TRENKLER, \ Hadamard, khatri-rao, kronecker and other matrix <br>  products, "INTERNATIONAL JOURNAL OF INFORMATION AND SYSTEMS SCI- <br>  ENCES, 2006. <br>  ALF de Almeida, G. Favier, and LR Ximenes, \ Space-time-frequency (stf) mimo <br>  paratuck2 model, " <br>  IEEE Transaction in signal processing, vol.  61, no.  8 ,, APR, 2013. <br></div></div><br>  Have a nice week. <br></div><p>Source: <a href="https://habr.com/ru/post/369925/">https://habr.com/ru/post/369925/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../369913/index.html">Neural network Google Translate has made a single database of meanings of human words</a></li>
<li><a href="../369915/index.html">AI tried to understand the comics. Did not work out</a></li>
<li><a href="../369917/index.html">Silicon life can be synthesized on Earth</a></li>
<li><a href="../369919/index.html">France, Finland, UK and Canada will completely abandon the burning of coal</a></li>
<li><a href="../369921/index.html">The UK government may soon legitimize surveillance of its citizens.</a></li>
<li><a href="../369927/index.html">Prison telecommunications</a></li>
<li><a href="../369929/index.html">History of world epidemics, part 4</a></li>
<li><a href="../369931/index.html">Configure automatic hover camcorder for conferences in 30 minutes</a></li>
<li><a href="../369933/index.html">Researchers control mouse behavior with a wireless neuroimplant</a></li>
<li><a href="../369935/index.html">Startup financing processes migrate to blockchain</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>