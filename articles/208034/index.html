<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Machine learning. Yandex course for those who want to spend the New Year holidays with benefit</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="New Year holidays are a good time not only for recreation, but also for self-education. You can take your mind off everyday tasks and devote a few day...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Machine learning. Yandex course for those who want to spend the New Year holidays with benefit</h1><div class="post__text post__text-html js-mediator-article">  New Year holidays are a good time not only for recreation, but also for self-education.  You can take your mind off everyday tasks and devote a few days to learning something new that will help you all year (or maybe not one).  Therefore, we decided this weekend to publish a series of posts with lectures from the courses of the first semester of the School of Data Analysis. <br><br>  Today - about the most important.  Modern data analysis without it is impossible to imagine.  The course covers the main tasks of learning by precedent: classification, clustering, regression, reduction of dimension.  Methods for solving them, both classical and new, created in the last 10‚Äì15 years, are being studied.  The emphasis is on a deep understanding of the mathematical foundations, relationships, advantages and limitations of the methods under consideration.  Separate theorems are presented with proofs. <br><br><iframe src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://video.yandex.ru/iframe/ya-events/m-69601-150442cd519-a40e2fa0384c4fcc/&amp;xid=17259,15700022,15700186,15700191,15700253,15700255&amp;usg=ALkJrhh0-hIJGLTVPpGN6ha8I4k1-q9usg" width="450" height="253" frameborder="0" scrolling="no" allowfullscreen="1"></iframe>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Konstantin Vyacheslavovich Vorontsov, senior researcher at the Computing Center of the Russian Academy of Sciences, reads a course of lectures.  Deputy Director for Science, ZAO Forexis.  Deputy Head of the Department of "Intellectual Systems" FUPM MIPT.  Associate Professor of the Department "Mathematical Methods of Forecasting" of the Moscow Institute of Physics and Technology.  Expert company "Yandex".  Doctor of Physical and Mathematical Sciences. <br><a name="habracut"></a><br><br><h4>  <a href="">Lecture 1. Basic concepts and examples of applied problems.</a> </h4><br><ul><li>  Setting training objectives for precedents.  Objects and signs.  Types of scales: binary, nominal, ordinal, quantitative. </li><li>  Task types: classification, regression, forecasting, clustering. </li><li>  The basic concepts are: model of algorithms, training method, loss function and quality functional, principle of empirical risk minimization, generalizing ability, sliding control. </li><li>  Examples of applied problems. </li></ul><br><h4>  <a href="">Lecture 2. Bayesian classification algorithms, non-parametric methods</a> </h4><br><ul><li>  Probabilistic formulation of the classification problem.  Basic concepts: a priori probability, a posteriori probability, the likelihood function of a class. </li><li>  Medium risk functional.  Errors I and II kind. </li><li>  Optimal Bayes classifier. </li><li>  Distribution density estimation: three main approaches. </li><li>  Naive Bayes classifier. </li><li>  Non-parametric estimation of the distribution density by Parzen-Rosenblatt.  The choice of the kernel function.  Selection of window width, variable window width.  Parzen window method. </li><li>  Non-parametric naive Bayes classifier. </li><li>  Robust density estimation.  Sampling of the sample (dropout of emission objects). </li></ul><br><h4>  <a href="">Lecture 3. Parametric methods, normal discriminant analysis</a> </h4><br><ul><li>  Multidimensional normal distribution: geometric interpretation, sample estimates of parameters: expectation vector and covariance matrix. </li><li>  Quadratic discriminant.  Type of dividing surface.  The wildcard algorithm, its disadvantages and ways to eliminate them. </li><li>  Fisher linear discriminant. </li><li>  Problems of multicollinearity and retraining.  Regularization of the covariance matrix. </li><li>  Dimension reduction method. </li><li>  Distribution mixture model. </li><li>  EM-algorithm: the basic idea, the concept of hidden variables, E-step, M-step.  Constructive derivation of M-step formulas (without justification of convergence). </li></ul><br><h4>  <a href="">Lecture 4. EM-algorithm and network of radial basis functions.</a> </h4><br><ul><li>  Stop criterion, the choice of the initial approximation, the choice of the number of components. </li><li>  Stochastic EM-algorithm. </li><li>  A mixture of multidimensional normal distributions.  The network of radial basic functions (RBF) and the use of EM-algorithm for its configuration. </li><li>  Nearest Neighbor Method (kNN) and its generalizations. </li><li>  Selection of the number k according to the criterion of sliding control. </li></ul><br><h4>  <a href="">Lecture 5. Metric classification algorithms</a> </h4><br><ul><li>  Generalized metric classifier, indentation concept. </li><li>  Method of potential functions, gradient algorithm. </li><li>  Selection of reference objects.  Pseudocode: the PILLAR algorithm. </li><li>  Function of competitive similarity, algorithm FRiS-STOLP. </li><li>  Biological neuron, McCulloch-Pitts model. </li><li>  Linear classifier, indentation concept, continuous approximation of the threshold loss function. </li></ul><br><h4>  <a href="">Lecture 6. Linear classification algorithms</a> </h4><br><ul><li>  The quadratic loss function, the least squares method, coupling with Fisher‚Äôs linear discriminant. </li><li>  The stochastic gradient method and special cases: ADALINE adaptive linear element, Rosenblatt perceptron, Habb rule. </li><li>  Disadvantages of the stochastic gradient method and ways to eliminate them.  Acceleration of convergence, ‚Äúknocking out‚Äù of local minima.  The problem of retraining, weight reduction (weight decay). </li><li>  Hypothesis of exponentiality of the likelihood functions of classes. </li><li>  The Bayesian optimal classifier linearity theorem. </li><li>  Evaluation of the posterior probabilities of classes using sigmoid activation function. </li><li>  Logistic regression.  Maximum likelihood principle and logarithmic loss function. </li><li>  The stochastic gradient method, analogy with the Habb rule. </li></ul><br><h4>  <a href="">Lecture 7. Support Vector Machine (SVM)</a> </h4><br><ul><li>  Optimal separating hyperplane.  The concept of the gap between the classes (margin).  Cases of linear separability and the absence of linear separability. </li><li>  Relationship to the minimization of regularized empirical risk.  Piecewise linear loss function. </li><li>  The problem of quadratic programming and the dual problem.  The concept of support vectors. </li><li>  Recommendations for choosing a constant C. </li><li>  Kernel function (kernel functions), straightening space, Mercer's theorem. </li><li>  Ways of constructive construction of nuclei.  Examples of kernels. </li><li>  Comparison of SVM with Gaussian core and RBF network. </li></ul><br><h4>  <a href="">Lecture 8. Linear classification methods: generalizations and review</a> </h4><br><ul><li>  Theoretical substantiations of various continuous loss functions and various regularizers. </li><li>  Bayesian approach.  The principle of maximum joint likelihood of data and model. </li><li>  Some varieties of regularizers used in practice.  Quadratic (L2) regularizer.  L1- and L0- regularizers and their connection with feature selection. </li><li>  Relevant vectors method. </li><li>  Complexity approach.  Rademacher complexity and some of its properties.  The upper estimate of the probability of error for linear classifiers. </li></ul><br><h4>  <a href="">Lecture 9. Regression Recovery Techniques</a> </h4><br><ul><li>  The task of restoring regression, the method of least squares. </li><li>  One-dimensional non-parametric regression (smoothing): Nadaraya-Watson's estimate, kernel selection and smoothing window width. </li><li>  Multidimensional linear regression.  Singular decomposition. </li><li>  Regularization: ridge regression and Tibshirani lasso. </li><li>  Principal Component Method and the Karunen-Loeve Decorrelative Transformation. </li><li>  Robust regression: a simple LOWESS screening algorithm. </li></ul><br><h4>  <a href="">Lecture 10. Time Series Prediction</a> </h4><br><ul><li>  Additive and multiplicative time series models.  Trend, seasonality, calendar effects. </li><li>  Adaptive models: exponential smoothing, Holt-Winters and Theil-Wage models. </li><li>  Sliding control signal and Trigg-Lich model. </li><li>  Adaptive selection and composition of prediction models. </li><li>  Examples of applied tasks: traffic forecasting, number of visits, sales volumes. </li></ul><br><h4>  <a href="">Lecture 11. Neural networks</a> </h4><br><ul><li>  The structure of a multilayer neural network.  Activation functions. </li><li>  The problem of completeness.  The task is exclusive or.  Completeness of two-layer networks in the space of Boolean functions. </li><li>  Error propagation algorithm.  Formation of the initial approximation.  The problem of network paralysis. </li><li>  Methods to optimize the network structure.  Select the number of layers and the number of neurons in the hidden layer.  The gradual complication of the network.  Optimal thinning of the network (optimal brain damage). </li></ul><br><h4>  <a href="">Lecture 12. Clustering Algorithms</a> </h4><br><ul><li>  Setting the clustering problem.  Types of cluster structures. </li><li>  Graph clustering methods: algorithm for selecting connected components, FOREL algorithm, clustering quality functionals. </li><li>  Hierarchical clustering (taxonomy): agglomerative hierarchical clustering, dendrogram and monotony property, properties of compression, extension and reductiveness. </li><li>  Statistical clustering methods: EM-algorithm, k-means method. </li></ul><br><h4>  <a href="">Lecture 13. Methods of partial learning</a> </h4><br><ul><li>  Simple heuristic methods: SSL task features, self-training method, composition of classification algorithms. </li><li>  Modification of clustering methods: optimization approach, clustering with constraints. </li><li>  Modification of classification methods: transductive SVM, logistic regression, Expectation Regularization. </li></ul><br><h4>  Lectures 14-15.  Classifier compositions.  Boosting ( <a href="">part 1</a> , <a href="">part 2</a> ) </h4><br><ul><li>  Classifier composition: compositional learning tasks, classic AdaBoost algorithm, gradient boosting. </li><li>  Bagging and committee methods: bagging and random subspace method, simple and weighted voting, seniority voting. </li><li>  Mixtures of algorithms: the idea of ‚Äã‚Äãareas of competence, an iterative method of teaching a mixture, consistently increasing the mixture </li></ul><br><h4>  <a href="">Lecture 16. Estimates of generalizing ability</a> </h4><br><ul><li>  Tasks and criteria for choosing a teaching method: problems of choosing a model or a teaching method, empirical estimates of sliding control, analytical estimates and regularization criteria. </li><li>  The theory of generalizing ability: probability of retraining and VC-theory, Occam's razor, combinatorial theory of retraining. </li><li>  Methods for selecting attributes: brute force and greedy algorithms, df and wide, stochastic search. </li></ul><br><h4>  <a href="">Lecture 17. Methods of selection of signs.</a>  <a href="">Feature selection</a> </h4><br><ul><li>  The complexity of the task of selecting features.  Full bust. </li><li>  Add and remove method, step regression. </li><li>  Search in depth, the method of branches and borders. </li><li>  Truncated search in width, multi-row iterative algorithm MGUA. </li><li>  Genetic algorithm, its similarity with MSUA. </li><li>  Random search and Random search with adaptation (SPA). </li></ul><br><h4>  <a href="">Lecture 18. Logic classification algorithms</a> </h4><br><ul><li>  The concept of regularity and informativeness: definitions and notation, interpretability, informativeness. </li><li>  Methods for finding informative laws: a greedy algorithm, an algorithm based on feature selection, data binarization. </li><li>  Compositions of laws: decisive list, deciding trees, voting of regularities, decisive forests. </li></ul><br><h4>  <a href="">Lecture 19. Logic classification algorithms.</a>  <a href="">Decisive trees</a> </h4><br><ul><li>  The decisive list.  Greedy list synthesis algorithm. </li><li>  Decisive tree.  Pseudocode: ID3 greedy algorithm.  The disadvantages of the algorithm and how to eliminate them.  The problem of retraining. </li><li>  Reduction of the decisive trees: preduction and reduction. </li><li>  Convert the decision tree to a decision list. </li><li>  LISTBB algorithm. </li><li>  Alternating decision trees. </li><li>  Inattentive decision trees (oblivious decision tree). </li><li>  Decisive forest and boosting over decisive trees.  TreeNet algorithm. </li></ul><br><h4>  <a href="">Lecture 20. Logic classification algorithms.</a>  <a href="">Weighted vote</a> </h4><br><ul><li>  Methods for the synthesis of conjunctive patterns.  Pseudocode: KORA algorithm, TEMP algorithm. </li><li>  Heuristics that ensure the difference and usefulness of patterns.  Construction of Pareto-optimal laws.  Align the indentation distribution. </li><li>  Application of AdaBoost's boosting algorithm to regularities.  Criterion of informativeness in boosting. </li><li>  Examples of applied tasks: credit scoring, customer care forecasting. </li></ul><br><h4>  <a href="">Lecture 21. Search for associative rules</a> </h4><br><ul><li>  Methods for the synthesis of conjunctive patterns.  Pseudocode: KORA algorithm, TEMP algorithm. </li><li>  Heuristics that ensure the difference and usefulness of patterns.  Construction of Pareto-optimal laws.  Align the indentation distribution. </li><li>  Application of AdaBoost's boosting algorithm to regularities.  Criterion of informativeness in boosting. </li><li>  Examples of applied tasks: credit scoring, customer care forecasting. </li></ul><br><h4>  <a href="">Lecture 22. Collaborative iterations</a> </h4><br><ul><li>  Task setting and applications. </li><li>  Correlation models based on data storage, the task of recovering missing values ‚Äã‚Äãof the proximity function. </li><li>  Latent models: biclusterization and matrix expansions, probabilistic latent models, experiments from Yandex data. </li></ul><br><h4>  Lectures 23-24.  Thematic modeling ( <a href="">part 1</a> , <a href="">part 2</a> ) </h4><br><ul><li>  The task of thematic modeling: probabilistic thematic model, unigram model. </li><li>  Thematic models PLSA and LDA: probabilistic latent semantic model, Dirichlet latent distribution, empirical quality assessment of thematic models. </li><li>  Robust probabilistic thematic model: model with background and noise components, EM-algorithm for a robust model, sparseness of a robust model. </li></ul><br><h4>  <a href="">Lecture 25. Training with reinforcements</a> </h4><br><ul><li>  Multi-armed bandit: simple task setting, greedy and half-greedy strategies, adaptive strategies. </li><li>  Dynamic programming: complete problem statement, Bellman equation. </li><li>  The method of time differences. </li></ul><br><br>  <b>Update:</b> <a href="http://yadi.sk/d/V9p7E6uAFjHcD">all the lectures of the Machine Learning course in the form of an open folder on Yandex.Disk</a> . </div><p>Source: <a href="https://habr.com/ru/post/208034/">https://habr.com/ru/post/208034/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../208022/index.html">Siemens Logo! - ten years later</a></li>
<li><a href="../208026/index.html">STM32 and USB-HID - it‚Äôs just</a></li>
<li><a href="../208028/index.html">‚ÄúAnd these people forbid us to poke around‚Äù: Huawei and the NSA</a></li>
<li><a href="../208030/index.html">Discoveries of the Outgoing Year - Another Useful Sleep Function</a></li>
<li><a href="../208032/index.html">The clash of two cultures: "I can" and "I can not"</a></li>
<li><a href="../208036/index.html">OpenWorm - an international project to create a computer model of the worm</a></li>
<li><a href="../208038/index.html">NASA is testing a shape-changing robot wheel.</a></li>
<li><a href="../208040/index.html">As a programmer has come to the site about fashion</a></li>
<li><a href="../208042/index.html">3d printers. Review of achievements for 2013</a></li>
<li><a href="../208044/index.html">Computer game "Noosphere" - suggestions for the scenario for the first levels</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>