<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The variant of synchronous impulse neural network with feedback</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A warning 
 I warn you, the reasoning at the moment is purely theoretical. But it‚Äôs a very beautiful theory. Or I'm going crazy. I am interested in yo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The variant of synchronous impulse neural network with feedback</h1><div class="post__text post__text-html js-mediator-article"><h4>  A warning </h4><br>  I warn you, the reasoning at the moment is purely theoretical.  But it‚Äôs a very beautiful theory.  Or I'm going crazy.  I am interested in your opinion about my reasoning - is it worth starting a practical implementation? <br>  Frankly speaking, the reason for which all further reasoning was born is that I failed one bad idea - to teach a neural network to play Tetris myself.  Teaching the network with the teacher is unrealistic for a long time, because  the teacher is me, and there are definitely no right options for the movement of the figures in Tetris.  A genetic network learning algorithm has been applied.  The maximum that my population learned during the day of the virtual process of evolution and natural selection is to lay down the falling figures in three columns (in fact, she also learned to turn the figures so that they were more ‚Äúhorizontal‚Äù than ‚Äúvertical‚Äù, but learn how to turn the figures so that the lines were removed - it turned out not to the forces). <br><a name="habracut"></a><br><h4>  What I don't like about classic neural networks (a la perceptrons) </h4><br>  As a rule, when considering certain applications of neural networks, they imply the use of analog neural networks, like perceptrons, etc. <br>  Such neural networks took from the real, living, organisms only the most superficial form and essence.  It seems like the planes took the idea of ‚Äã‚Äãwings from the birds.  Although they are used completely differently (but with the same purpose).  In fact, various variations of perceptrons are a non-linear function of the sum of the same non-linear functions y = k1 * f (k2 * f (...) + k3 * f (....) + ...), and the learning algorithms simply select values coefficients k. <br>  Due to the too large difference between real neural networks and artificial ones, it is necessary to invent various surrogate methods of network training.  Naturally, nowhere in nature there is no learning of the network by the ‚Äúback-propagation method of error‚Äù, as well as no ‚Äúlearning without a teacher‚Äù in its pure form. <br>  I would like to implement a more believable neural network and experiment with it. <br><h4>  Basic principles of the functioning of "my" neural network </h4><br><br>  In accordance with my knowledge that neuroscientists are currently aware of the functioning of real neural networks, the basic principles should be as follows: <br><ol><li>  Transmission of information in the form of pulses </li><li>  Information coding - frequency (possibly modulated in amplitude) </li><li>  Ability to self-organize due to creation of new synaptic connections </li><li>  Ability to learn by specified criteria </li><li>  Ability to use some network parameters for optimization by genetic algorithms </li><li>  The possibility of implementing all three known types of memory of real networks - short-term, medium-term, long-term </li></ol><br><br>  I will explain the points. <br><h5>  1. Transmission of information in the form of pulses </h5><br>  This, I think, everyone knows - signals between living neurons are transmitted in the form of pulses. <br><h5>  2. Frequency coding information </h5><br>  The intensity of the signal from the receptors and further, in the living neural network, is encoded by the pulse frequency.  For example, the stronger the taste sensation in your tongue, the more often the impulses from the corresponding receptors arrive.  The amplitude of these pulses does not depend on the intensity of the stimulus (but depends on the "fatigue" of the nervous system). <br>  Those.  in addition to the neurons proper, an essential part of the neural network is the receptors and the so-called ‚Äúchemical synapses‚Äù.  Receptors are an analogue of the ADC, the intensity of exposure is encoded into pulses of a certain frequency.  Chemical synapses are analogous to a DAC, depending on the frequency of the pulses, they release more or less different substances that, for example, expand or constrict the vessels.  Here it must be said that the terms ADC and DAC are not entirely correct, rather it is ACP and CHAP (analog-frequency and frequency-analog converters). <br><h5>  3. Ability to self-organize and create new synaptic connections </h5><br>  It is known that new synaptic connections are constantly being created in living organisms.  The number of neurons we have is how much is given by nature and ‚Äúwill not grow anymore‚Äù, but the number of synaptic connections the brain can build up.  And in advance it is not set from where and where these communications appear.  In particular, it is one of the mechanisms of memory. <br><h5>  4. Ability to self-learning according to specified criteria </h5><br>  No one will argue that animals learn by trial and error.  It is a cross between teaching with a teacher and learning without a teacher.  In fact - the teacher is the effect of certain actions.  Invented neural network should be able to self-learn.  Some criteria of course are set in advance, but only very general criteria.  Something like instincts. <br><h5>  5. The ability to use some network parameters for optimization by genetic algorithms </h5><br>  It is known that there is a genetic memory.  At a minimum, we are able to control our lungs from birth.  breathe.  We do not learn this either with the teacher or without.  This is a genetic memory.  The network should be able to transmit some basic information to its descendants. <br><h5>  6. The possibility of implementing all three known types of memory of real networks - short-term, medium-term, long-term </h5><br>  Here I must say that there is also an instantaneous memory, lasting up to a second.  But instant memory is the memory of receptor neurons, and has no particular relation to the projected network. <br>  But the three main types of memory must be implemented. <br>  Short-term memory (in humans - tens of seconds) - due to the presence in the network of feedback, and the formation of a closed cycle of neurons, in which the pulses move "in a circle." <br>  Medium-term memory (in humans - a few days) - due to increased synaptic connections with repeated passage of the pulse.  This type of memory is also implemented in classical artificial neural networks, with various types of training. <br>  Long-term memory (in humans - for a long time, maybe even for a lifetime) - due to the formation of new synaptic connections and the involvement of previously unused neurons in the work. <br>  There is also a theory that there is a super-long-lasting ‚Äúnon-erasable‚Äù memory, such a ROM.  Those.  memories that will never be forgotten.  Presumably, its mechanism is associated with a change in the internal structure of the neuron, when it gives some preference to a certain synaptic connection, i.e.  connects to it an "additional amplifier", which is never removed again.  For simplification, we can do without super-long-term memory. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  General scheme of network functioning </h4><br>  The general algorithm of the network is as follows: <br>  1. Initialization.  There are only neurons.  There is no connection. <br>  2. Select some (any) neurons as input.  They are constantly giving input signals (coded frequency). <br>  3. Select some (any) neurons as a weekend.  With them constantly remove the output information. <br>  4. In the loop: <br>  4.1.  Another tact of the network (transmission of pulses). <br>  4.2.  Check the need to create new connections and create them if necessary. <br>  4.3.  Check the need to remove unused links and remove them (to reduce the resources used). <br><br>  Gradually, new synaptic connections will appear and some information will come out. <br><br>  Here it is convenient to explain why the network is called synchronous. <br>  In synchronous operation, we calculate the value at the output of the neuron, but so far we simply remember, and do not set the output to a new state.  After processing all neurons, we give the command to set new values ‚Äã‚Äãat the outputs of all neurons. <br>  With asynchronous operation - at the output of the neuron we set a new value after its calculation. <br>  In principle, asynchronous networks are closer to reality.  But in fact, it makes sense to implement them only in hardware, i.e.  when each neuron is assigned its own computing unit. <br><br><h4>  Pulse Neural Network Training </h4><br><br>  And so we launched the network created at the previous stage.  She began to give out some sort of messy information.  And we can not yet say - what do we want from it.  How to train such a neural network? <br>  I see here three ways, which are some analogs of teaching with a teacher, teaching without a teacher, and combined - ‚Äúmy own teacher‚Äù. <br>  It should be noted that internally self-learning of the network occurs automatically and constantly, due to the adjustment of communication parameters and the emergence of new connections. <br>  So, ways to train such a network. <br><h5>  Teaching with a teacher </h5><br>  The teacher can make the network understand whether it behaves well or badly;  Does it give the output we want to receive from it?  You can implement it in the form of a certain function, the approximate algorithm of which is very simple. <br>  Parameter - EMOTION_ of the TEACHER.  The parameter range is from -1 to 1, where -1 = very bad, 1 = very good, 0 = no evaluation. <br>  For all connections, set the weight of the connection W = W + W * EMOTION_CHAPTER * k, where k is the learning coefficient (can be adjusted, for example, genetically). <br>  Everything. <br><img src="http://rusuper.ru/for_habra/s_uchitelem.jpg" alt="image"><br>  Thus, with a positive value of the EMOTION_CHARTER parameter, we strengthen bonds, and strong bonds (leading to the correct result) are strengthened more strongly.  If the EMOTION_READER parameter is negative, we weaken the bonds, and strong bonds (leading to an incorrect result) are weakened more. <br><h5>  Teaching without a teacher </h5><br>  Here the concept is not yet fully developed, but there are some sketches.  If it will be interesting - I will tell in the following articles. <br><h5>  Self teacher </h5><br>  This method of self-study is possible after some time we have trained the network with the teacher.  It is implemented simply. <br>  We make some additional output of the network, let's call it INTERNAL_EMOTION, feed it to the same input where EMOTION_READER is fed (let's add them).  And in the process of learning with the teacher we follow the inner emotion of the network.  Suppose the teacher gives a signal of +0.5 (positive emotion), if the network performed the correct action and it had no internal emotions.  The teacher gives a signal of 0 if the network performed the correct action, but she had a negative emotion, and the teacher gives a signal of 1 (very good) if the network performed the correct action and she had a positive emotion.  Those.  we additionally train the network on the rule ‚Äúthe right action is a good inner emotion‚Äù. <br>  Since we connected the INTERNAL_EMOTION exit to the same input to which the TEACHER EMOTION was fed, and at the same time taught ‚Äúwhat is good and what is bad,‚Äù theoretically, the network itself will manage its further training, even without the participation of the teacher. <br><img src="http://rusuper.ru/for_habra/sam_sebe.jpg" alt="image"><br><br>  This concludes the introductory part. <br>  Details further, if these reflections at least someone interested. </div><p>Source: <a href="https://habr.com/ru/post/93223/">https://habr.com/ru/post/93223/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../93206/index.html">Chess</a></li>
<li><a href="../93209/index.html">ToCyrillic: Google Chrome Extension</a></li>
<li><a href="../93210/index.html">A brief introduction to SIM cards</a></li>
<li><a href="../93213/index.html">Multi-armed Shiva Syndrome: Knowledge vs Skills</a></li>
<li><a href="../93219/index.html">I'm 64-bit! And you?</a></li>
<li><a href="../93226/index.html">Hash Algorithms</a></li>
<li><a href="../93228/index.html">Terminal server elevation using the example of LTSP and CentOS 5.4</a></li>
<li><a href="../93230/index.html">Case NZXT Tempest Evo. And the whole world is not enough!</a></li>
<li><a href="../93231/index.html">I congress of Internet developers will be held May 20, 2010</a></li>
<li><a href="../93232/index.html">Printing is possible, any</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>