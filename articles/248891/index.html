<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Do not rush to throw out the old servers, you can assemble fast Ethernet-storage in an hour</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Once we installed new EMC array disk shelves for one of our large customers. When I left the facility, I paid attention to people in the form of a tra...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Do not rush to throw out the old servers, you can assemble fast Ethernet-storage in an hour</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/ceb/d62/5c4/cebd625c4b164451ab1a587a600fee03.jpeg"><br><br>  Once we installed new EMC array disk shelves for one of our large customers.  When I left the facility, I paid attention to people in the form of a transport company, who pulled out of the racks and prepared a large number of servers for loading.  Communication with the sysadmins of customers is tight, so it quickly became clear that these are servers ‚Äî old machines that have little memory and processing power, although there are plenty of disks.  Updating them is not profitable and the glands will be sent to the warehouse.  And they will be written off somewhere in a year when they become covered in dust. <br><br>  Iron, in general, not bad, just past generation.  <b>Naturally, it was a pity to throw it out.</b>  Then I offered to test the EMC ScaleIO.  In short, it works like this: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/files/9e5/6ee/bb6/9e56eebb69d8489485839a6a4cc58f53.png"><br><br>  ScaleIO is software that is installed on servers on top of the operating system.  ScaleIO consists of the server part, the client part and control nodes.  Disk resources are combined into one virtual single-level system. <a name="habracut"></a><br><br><h4>  How it works </h4><br>  To operate the system you need three control nodes.  They contain all information about the state of the array, its components and the ongoing processes.  This is a sort of array orchestrator.  For ScaleIO to function properly, at least one controlling node must be alive. <br><br>  The server part is small clients that combine free space on servers into a single pool.  On this pool, you can create the moon (one moon will be distributed across all servers in the pool) and give them to customers.  ScaleIO can use server RAM as a read cache.  The cache size is set for each server separately.  The larger the total volume, the faster the array will work. <br><br>  The client part is a block input-output device driver, representing distributed across different moon servers as a local disk.  So, for example, loon ScaleIO looks on Windows: <br><br><img src="https://habrastorage.org/files/f5a/b65/aa8/f5ab65aa86a74e5081c1b8855f80621c.png"><br><br>  The requirements for installing ScaleIO are minimal: <br><table><tbody><tr><td><br>  CPU <br></td><td><br>  Intel or AMD x86 <nobr>64-bit</nobr> <br></td></tr><tr><td><br>  Memory <br></td><td><br>  500 MB RAM for control node <br>  500 MB RAM per data node <br>  50 MB RAM for each client <br></td></tr><tr><td><br>  Supported Operating Systems <br></td><td><br>  Linux: CentOS <nobr>5.5-7.0,</nobr> Red Hat <nobr>5.5-7.0,</nobr> or SUSE 11 SP1, SP2, and SP3 <br>  Windows: 2008 R2, 2012, or 2012 R2 <br>  Hypervisors: <br>  ¬∑ VMware ESXi OS: 5.0, 5.1, or 5.5, managed by vCenter 5.1 or 5.5 only <br>  ¬∑ Hyper-V <br>  ¬∑ XenServer 6.1 <br>  ¬∑ RedHat KVM <br></td></tr></tbody></table><br>  Of course, all data is transmitted via Ethernet.  All I / O and bandwidth are available to any application in the cluster.  Each host writes to many nodes at the same time, which means that the throughput and the number of input / output operations can reach very large values.  An additional advantage of this approach is that ScaleIO does not require a thick interconnect between the nodes.  If the server is Ethernet 1Gb, the solution is suitable for streaming recording, archive or file garbage.  Here you can run a test environment or developers.  When using Ethernet 10Gb and SSD drives, we get a good solution for databases.  On SAS disks, you can upgrade datastores on VMware.  At the same time, virtual machines can work on the same servers from which space is given to the shared moon, because under ESX there is both a client and a server part.  I personally love this variation. <br><br>  With a large number of disks, according to probability theory, the risk of failure of any of the components increases.  The solution is interesting: in addition to the RAID groups at the controller level, data mirroring is used for different servers.  All servers are divided into fault-sets - a set of machines with a high probability of simultaneous failure.  Data is mirrored between the fault-sets in such a way that the loss of one of them will not lead to data unavailability.  One fault-set may include servers located in the same rack, or servers with different operating systems.  The last option is pleasant because you can roll out patches on all Linux or Windows machines at the same time, without fear of a cluster crashing due to operating system errors. <br><br><img src="https://habrastorage.org/files/eed/9ff/4e3/eed9ff4e3f3c4db5b3815c6df0a7202e.jpeg"><br><br><h4>  Tests </h4><br>  ScaleIO is installed using the installation manager.  It is necessary to download software packages for different operating systems and a csv-file with the desired result.  I took 8 servers, half from Windows, half from SLES.  Installation on all 8 took 5 minutes and a few clicks on the "Next" button. <br><br>  Here is the result: <br><br><img src="https://habrastorage.org/files/9cb/50b/4eb/9cb50b4eb1ce40fabe1eaf5c0834faba.png"><br><br><img src="https://habrastorage.org/files/d6c/b12/cae/d6cb12caed7d47ff83bc556d4e53a221.png"><br><br>  This is, by the way, a GUI through which you can control the array.  For lovers of the console, there is a detailed manual on cli-commands.  The console, as always, is more functional. <br><br>  For tests, I divided all data nodes into 2 Failover sets: with Windows OS and with SLES.  Mapim is our first host disk: <br><br><img src="https://habrastorage.org/files/7d2/86d/037/7d286d037a0641e1b0ff79ee7ab84c2f.png"><br><br>  Disk size is small, only 56 GB.  Further, according to the plan, tests for fault tolerance, but I do not want to wait for the end of the rebuild for more than 10 minutes. <br><br>  For load emulation, the easiest way is to use IOmeter.  If the disc falls off even for a second, I will definitely find out.  Unfortunately, there is no way to test performance in this test: the servers are virtual and the datastor is EMC CX3.  Normal equipment is occupied in production.  Here the first bytes ran: <br><br><img src="https://habrastorage.org/files/6e1/8c2/243/6e18c2243ed64fcf90b068bcf79a4d88.png"><br><br><img src="https://habrastorage.org/files/dd7/90d/6c7/dd790d6c709742258da0183ec0b77de1.png"><br><br>  As I wrote earlier, one fail-set can be turned off.  The fun begins.  It's nice to know that everything will be fine with the client's production in emergency situations, therefore at KROK we create such situations in our lab.  The best way to convince a customer about the reliability of a high availability solution is to turn off one of the two equipment racks.  Here I do the same: <br><br><img src="https://habrastorage.org/files/c92/836/e96/c92836e96e474f1d9faad9ec1e1a1d0d.png"><br><br><img src="https://habrastorage.org/files/e39/10b/681/e3910b68119b433cbfa3ab3064163f1c.png"><br><br>  The GUI shows that all nodes with the Windows operating system are not accessible, which means that the data are no longer protected by redundancy.  The pool has moved to Degraded status (red instead of green), and IOmeter continues to write.  For a host, a simultaneous failure of half of the nodes went unnoticed. <br><br>  Let's try to include 3 of the four nodes: <br><br><img src="https://habrastorage.org/files/2b2/877/899/2b28778995b143acacaf75eb47e37f63.png"><br><br>  Rebild started automatically, normal flight.  Interestingly, data redundancy will be restored automatically.  But since now the node with Windows is one less, they will take up more space.  As the redundancy recovers, the data will turn green. <br><br>  Half ready: <br><br><img src="https://habrastorage.org/files/474/b4c/9d4/474b4c9d45fb493dbd377e1cfe926093.png"><br><br>  And now everything: <br><br><img src="https://habrastorage.org/files/56e/ea6/31a/56eea631aea94c3b849e3714ee7b7f5e.png"><br><br>  Data has fully recovered redundancy.  At the 4th node now there is nothing.  When I turn it on, balancing will start inside one Failover set.  The same thing happens when adding new nodes. <br><br><img src="https://habrastorage.org/files/68e/310/2aa/68e3102aab6a4998b414d05f4bd7d960.png"><br><br>  Rebalance data started.  Interestingly, the data is copied from all the nodes, and not just from the nodes of the same file set.  After the end of the rebalance, all nodes take up 14% of the place.  IOmeter still writes data. <br><br><img src="https://habrastorage.org/files/060/2b4/1b7/0602b41b71c24351ac5144640f78e05b.png"><br><br>  The first iteration of tests is over, the second will be on another gland.  Need to test performance. <br><br><h4>  Price </h4><br>  Licensing policy ScaleIO - pay only for raw capacity.  The licenses themselves do not bind to the gland.  This means that you can put ScaleIO on the most ancient hardware, and in a couple of years you can replace it with modern servers without additional licensing.  It is a bit unusual after the standard licensing policy of arrays, when upgrade disks are more expensive than the same disks upon initial purchase. <br><br>  The ScaleIO price list is approximately $ 1,572 per terabyte of raw space.  Discounts are negotiated separately (I think if you have read this far, you do not need to explain what the ‚Äúprice‚Äù price is).  ScaleIO has analogues among open source solutions, and from other manufacturers.  But ScaleIO has an important plus - round-the-clock support for EMC and a huge implementation experience. <br><br>  Colleagues from EMC claim that you can save up to 60% of costs in a five-year term compared to mid-range storage.  This is achieved both by reducing the cost of licenses, and by reducing the requirements for iron, as well as power, cooling and, accordingly, a place in the data center.  As a result, the decision turned out quite "anti-crisis".  I think this year it will be useful to many people. <br><br><h4>  What else can you do </h4><br><ul><li>  ScaleIO can be divided into isolated domains (this is for cloud providers). </li><li>  Software can create snapshots and limit the speed of clients, </li><li>  Data on ScaleIO can be encrypted and laid out in pools with different performance. </li><li>  Linear scaling.  With each new host bandwidth, performance and capacity increase. </li><li>  Failover sets allow you to lose servers as long as there is free space to restore data redundancy. </li><li>  Variability.  You can collect cheap file trash on SATA-drives, and you can fast storage system with SSD-drives. </li><li>  It completely replaces mid-storage storage systems on some projects. </li><li>  You can use unused space on existing servers or donate a second life to the old hardware. </li></ul><br>  From minuses - a large number of ports in network switches and a large number of ip-addresses are required. <br><br>  <b>Typical applications</b> <br><ul><li>  The most obvious is to build a public or private cloud company.  The solution is easy to expand, nodes are easy to add and change. </li><li>  For infrastructure virtualization.  There are also clients for ESX, so nothing prevents us from making the ScaleIO pool (you can even on local disks of the same servers that ESX is on) and put virtual machines on it.  Thus, it is possible to provide good performance and additional fault tolerance at a relatively low cost. </li><li>  In the configuration with SSD and Ethernet 10G, ScaleIO is perfect for medium and small databases. </li><li>  In the configuration with capacious SATA-disks, you can make a cheap storage or archive, which, however, can not be placed on the tape. </li><li>  ScaleIO on SAS disks will be an excellent solution for developers and testers. </li><li>  Suitable for some video editing tasks, when you need a combination of reliability speed and low price (you need to test in the butt for specifics, of course). The system can be used to store large files like streaming from HD cameras. </li></ul><br><br><h4>  Total </h4><br>  Failover tests were successful.  Everything goes to the fact that the old servers will go back to performance testing, and then in combat use.  If the customer from the story at the beginning does as intended, someone will receive an award for the saved non-acidic budget. <br><br>  If you are interested in the solution, there is a desire to test it for specific tasks or just to discuss it - write to rpokruchin@croc.ru.  On the test iron, you can do everything, as in that joke about a new Finnish chainsaw and Siberian men. </div><p>Source: <a href="https://habr.com/ru/post/248891/">https://habr.com/ru/post/248891/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../248875/index.html">Meteor. And now loading photos</a></li>
<li><a href="../248879/index.html">Boost.DI: dependency injection in C ++</a></li>
<li><a href="../248881/index.html">Overview Friendly interactive shell (fish) and why it is better than bash</a></li>
<li><a href="../248887/index.html">Hacking a bitcoin exchange on Rails</a></li>
<li><a href="../248889/index.html">‚ÄúOpen Financial Data: Possibilities for Using It‚Äù</a></li>
<li><a href="../248893/index.html">Practice "Intel IoT". Galileo Gen2 - Linux & Arduino</a></li>
<li><a href="../248895/index.html">Buttons do not happen much</a></li>
<li><a href="../248897/index.html">C ++ 11 variadic templates and long arithmetic at compile time</a></li>
<li><a href="../248899/index.html">We write extensions with Roslyn by 2015 studios (part 1)</a></li>
<li><a href="../248901/index.html">Annotation to "Effective Modern C ++" by Scott Myers. Part 2</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>