<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>We save on matches: how to increase the locality in OpenStack using filters</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Author: Alexey Ovchinnikov 

 Quite often when creating a virtual machine on the cloud, there is a desire to associate it with some storage device. Qu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>We save on matches: how to increase the locality in OpenStack using filters</h1><div class="post__text post__text-html js-mediator-article">  <i>Author: Alexey Ovchinnikov</i> <br><br>  Quite often when creating a virtual machine on the cloud, there is a desire to associate it with some storage device.  Quite often, when creating a virtual machine on a cloud, you want it to work as fast as possible.  In the case when a storage device is connected to a virtual machine (VM), the exchange of information with it can significantly degrade the performance of the bundle.  Therefore, it is clear that if the storage device is located on the same physical node where the VM is deployed, the delay will be minimal.  What is not obvious is how to achieve such a convenient placement using the OpenStack platform. <br><br>  Unfortunately, OpenStack does not yet provide the means for such a fine tuning by default, however, being an open and easily extensible platform, OpenStack allows you to add yourself with similar functionality.  In this post I will discuss the features of the implementation of such add-ons and pitfalls that may occur during their development and use. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      I will begin my discussion with a simple question, namely how a VM can be placed on a particular node. <a name="habracut"></a><br><br>  As everyone (perhaps) is well aware, the scheduler (nova-scheduler component) is responsible for locating VMs on the nodes, therefore, in order to achieve the original goal, it is necessary to somehow modify its behavior so that it takes into account the distribution of storage devices.  The standard approach to this is to use scheduler filters.  Filters can influence the choice of a node by the scheduler, while the filters can be controlled from the command line, passing them the characteristics that the nodes selected by the scheduler must correspond to.  There are several standard filters that allow you to solve a fairly wide class of planning tasks and described in the <a href="http://docs.openstack.org/">OpenStack Docs</a> project documentation.  For less trivial tasks, there is always the possibility to develop your own filter.  This is what we will do now. <br><br><h4>  Some words about filters </h4><br>  The general idea of ‚Äã‚Äãfiltering planning is quite simple: the user specifies the characteristics that the node must respond to, after which the scheduler selects a set of nodes that respond to them.  Then VM can be started on one of the nodes selected at the previous stage.  On which, is determined by its loading and a number of other characteristics that are irrelevant at the filtration stage  Consider the filtering procedure in more detail. <br><br>  Quite often in the system there are several filters.  The scheduler first compiles a list of all available nodes, then applies each of the filters to this list, discarding unsuitable nodes at each iteration.  In this model, the filter problem is very simple: consider the node submitted to it at the input, and decide whether it meets the filtering criteria or not.  Each of the filters is an object of one of the filter classes, which has at least one method - <code>host_passes()</code> .  This method should take a node and filtering criteria as input and return <code>True</code> or <code>False</code> depending on whether the node meets the specified criteria.  All filter classes must inherit from the <code>BaseHostFilter()</code> base class defined in <code>nova.scheduler.filters</code> .  At startup, the scheduler imports all modules listed in the list of available filters.  Then, when the user sends a request to start the VM, the scheduler creates an object for each of the filter classes and uses them to screen out unsuitable nodes.  It is important to note that these objects exist during the same planning session. <br><br>  For example, consider the RAM filter, selecting nodes with a sufficient amount of memory.  This is a standard filter that has a fairly simple structure, so based on it you can develop more complex filters: <br><br>  class RamFilter (filters.BaseHostFilter): <br>  "" "Ram Filter with over subscription flag" "" <br><br>  def host_passes (self, host_state, filter_properties): <br>  "" "Only return hosts with mobile RAM available." "" <br>  instance_type = filter_properties.get ('instance_type') <br>  requested_ram = instance_type ['memory_mb'] <br>  free_ram_mb = host_state.free_ram_mb <br>  total_usable_ram_mb = host_state.total_usable_ram_mb <br><br>  memory_mb_limit = total_usable_ram_mb * FLAGS.ram_allocation_ratio <br>  used_ram_mb = total_usable_ram_mb - free_ram_mb <br>  usable_ram = memory_mb_limit - used_ram_mb <br>  if not usable_ram&gt; = requested_ram: <br>  LOG.debug (_ ("% (host_state) s does not have% (requested_ram) s MB" <br>  "Usable ram, it only has% (usable_ram) s MB usable ram."), <br>  locals ()) <br>  return false <br><br>  # save oversubscription limit for compute node to test against: <br>  host_state.limits ['memory_mb'] = memory_mb_limit <br>  return true <br><br>  To determine whether a given node is suitable for a future VM, the filter needs to know how much RAM is available on the node at the moment, as well as how much memory is required for the VM.  If it turns out that the node has less free memory than is necessary for the VM, then <code>host_passes()</code> returns <code>False</code> , and the node is removed from the list of available nodes.  All node status information is contained in the <code>host_state</code> argument, while the information needed to make a decision is placed in the <code>filter_properties</code> argument.  Constants reflecting some general planning strategy, such as <code>ram_allocation_ratio</code> , can be defined elsewhere, in configuration files or in the filter code, but this, by and large, is not essential, since everything necessary for planning can be passed to the filter using so-called scheduler tips. <br><br><h4>  Scheduler Tips </h4><br>  Scheduler hints are nothing more than a dictionary of key-value pairs that is contained in each request generated by the <code>nova boot</code> command.  If nothing is done, then this dictionary will remain empty and nothing interesting will happen.  If the user decides to transmit some hint and thus replenish the dictionary with hints, then this can be easily done using the <code>hint</code> key, for example, in the following command: <code>nova boot ‚Ä¶ --hint your_hint_name=desired_value</code> .  Now the dictionary with hints is not empty, it contains a transmitted pair.  If any extension of the scheduler is able to use this hint, then it just received information that should be considered when working.  If there is no such extension, then nothing will happen again.  The second case is not as interesting as the first one, so let's stop at the first one.  Let's see how the extension can use the tips. <br><br>  To use the prompts, they obviously need to be extracted from the query.  This procedure is also quite simple: all hints are stored in the <code>filter_properties</code> dictionary by the <code>scheduler_hints</code> key.  The following code snippet fully explains the procedure for getting hints: <br><br>  scheduler_hints = filter_properties ['scheduler_hints'] <br>  important_hint = scheduler_hints.get ('important_hint', False) <br><br>  In the scheduler in <code>nova scheduler_hints</code> always present in the request, so when developing your extension you can not expect any unpleasant surprises here, however, you should be careful when reading the value of the hint. <br><br>  Now we have the opportunity to receive arbitrary hints.  To achieve this goal, it remains to discuss how they should be used to <br><br><h4>  Increase the locality of connected storage devices! </h4><br>  Having knowledge of how to expand the functionality of the scheduler, you can easily design a filter that allows you to run VMs on the same nodes that have the storage devices of interest to the user physically.  Obviously, we will need to somehow distinguish the storage device that we are going to use.  Here we can come to the aid of the line volume_id, unique to each device.  From volume_id, you should in some way get the name of the node to which it belongs, and then select this node at the filtering stage.  Both of the last tasks should be solved by a filter, and in order for it to work, the filter needs to be informed of the node name with the help of the corresponding hint. <br><br>  First we use the hint mechanism to pass the volume_id to the filter.  To do this, we agree to use the name <code>same_host_volume_id</code> .  This is a simple task, solving which we immediately meet with the following, which is less obvious to decide: how to get the node name, knowing the identifier of the storage device?  Unfortunately, to all appearances, there is no easy way to solve this problem, so we will turn for help to the person responsible for data storage: the cinder component. <br><br>  You can use cinder services in different ways: for example, you can use a combination of API calls to get the metadata associated with a given volume_id, and then extract the node name from them.  However, this time we will use a simpler method.  We will use the ability of the cinderclient module to form the necessary requests and will work with what it returns: <br><br>  volume = cinder.cinderclient (context) .volumes.get (volume_id) <br>  vol_host = getattr (volume, 'os-vol-host-attr: host', None) <br><br>  It should be noted here that this approach will only work for the release of Grizzly and later, since the extension for cinder, which allows us to obtain information of interest to us, is available only in them. <br><br>  Further implementation is trivial - you need to compare <code>vol_host</code> with incoming names and return <code>True</code> only when they match.  Implementation details can be viewed either <a href="https://github.com/Mirantis/nova-scheduler-volume-affinity-filter">in the Grizzly package</a> or in the Havana implementation.  With some reflection on the resulting filter, the inevitable question arises: <br><br><h4>  Is this the best thing to do? </h4><br>  No, the considered method is neither optimal nor the only possible.  So, in a straightforward implementation, there is a problem with multiple calls to cinder, which is quite expensive, and a number of other problems that slow down the filter.  These problems are not significant for small clusters, but can lead to significant delays when working with a large number of nodes.  To improve the situation, you can modify the filter: for example, by entering the cache for the host name, which will limit one call to cinder for loading the VM, or add flags that will actually turn off the filter as soon as the desired host is detected. <br><br>  To summarize, I‚Äôll note that VolumeAffinityFilter is only the beginning of work on using locality to improve cloud performance, and there is room for development in this direction. <br><br><h4>  Instead of an afterword </h4><br>  The example I reviewed shows how you can develop a filter for the nova scheduler, which has a feature that distinguishes it from others.  This filter uses the API of another component of the OpenStack platform to fulfill its purpose.  Coupled with the addition of more flexibility, this approach can be detrimental to overall performance, since services can be located far from each other.  A possible solution to the problem of such fine-tuning can be to combine the schedulers of all services into one that has access to all cloud characteristics, but at the moment there is no simple and effective way to solve this problem. <br><br>  Original article <a href="http://www.mirantis.com/blog/travel-less-save-more-introducing-openstack-volume-affinity-filter/">in English</a> </div><p>Source: <a href="https://habr.com/ru/post/185390/">https://habr.com/ru/post/185390/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../185372/index.html">CORE technology</a></li>
<li><a href="../185374/index.html">Duck imprinted on a 3D printer prosthetic paw</a></li>
<li><a href="../185376/index.html">Auto-tuning and auto-updating of Escene IP phones software</a></li>
<li><a href="../185380/index.html">Prototype bitcoin exchanger</a></li>
<li><a href="../185384/index.html">Masyanya: crowdfunding triumph</a></li>
<li><a href="../185392/index.html">Bitcoin: Ordering and getting Little Jalapeno from Butterfly Labs</a></li>
<li><a href="../185394/index.html">ACM ICPC 2013 World Programming Championship in St. Petersburg</a></li>
<li><a href="../185396/index.html">3D printing and the role of this technology in the future: expert opinion from IBM</a></li>
<li><a href="../185398/index.html">681 megapixels of Mars, which almost no one saw</a></li>
<li><a href="../185402/index.html">So, you decided to create a security department ...</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>