<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Secure storage with DRBD9 and Proxmox (Part 1: NFS)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Probably anyone who has ever been puzzled by the search for high-performance software-defiined storage sooner or later heard about DRBD , and maybe ev...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Secure storage with DRBD9 and Proxmox (Part 1: NFS)</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/getpro/habr/post_images/f6a/771/043/f6a7710433c8887d6bbbe4792cc178e1.jpg" alt="image"></p><br><p>  Probably anyone who has ever been puzzled by the search for high-performance <strong>software-defiined storage</strong> sooner or later heard about <strong>DRBD</strong> , and maybe even dealt with it. </p><br><p>  True, at the peak of popularity of <strong>Ceph</strong> and <strong>GlusterFS</strong> , which work well in principle, and most importantly right out of the box, everyone just forgot a little about him.  Moreover, the previous version did not support the replication of more than two nodes, and because of which there were often problems with the <strong>split-brain</strong> , which obviously did not add to its popularity. </p><br><p>  The decision and the truth is not new, but quite competitive.  With relatively low CPU and RAM costs, <strong>DRBD</strong> provides really fast and secure synchronization at the <strong>block device</strong> level.  For all this time, LINBIT - DRBD developers are not standing still and are constantly modifying it.  Starting with the <strong>DRBD9</strong> version, <strong>it</strong> ceases to be just a network mirror and becomes something more. </p><br><p>  First, the idea of ‚Äã‚Äãcreating one <strong>distributed block device</strong> for several servers has receded into the background, and now LINBIT is trying to provide orchestration and management tools for multiple drbd devices in a cluster that are created on top of <strong>LVM</strong> and <strong>ZFS partitions</strong> . </p><br><p>  For example, DRBD9 supports up to 32 replicas, RDMA, diskless nodes, and new orchestration tools allow you to use snapshots, online migration and much more. </p><br><p>  Despite the fact that <strong>DRBD9</strong> has integration tools with <strong>Proxmox</strong> , <strong>Kubernetes</strong> , <strong>OpenStack</strong> and <strong>OpenNebula</strong> , at the moment they are in a transitional mode, when new tools are not yet supported everywhere, and the old ones will soon be announced as <em>deprecated</em> .  These are <strong>DRBDmanage</strong> and <strong>Linstor</strong> . </p><br><p>  I will take advantage of this moment so as not to go into great detail in each of them, but in more detail to consider the setting and principles of working with <strong>DRBD9</strong> itself. <a name="habracut"></a>  You will still have to deal with this, if only because the fail-safe configuration of the Linstor controller implies installing it on one of these devices. </p><br><p>  In this article I would like to tell you about <strong>DRBD9</strong> and the possibility of its use in <strong>Proxmox</strong> without third-party plug-ins. </p><br><h2 id="drbdmanage-i-linstor">  DRBDmanage and Linstor </h2><br><p>  Firstly, it is worth mentioning once again about <strong>DRBDmanage</strong> , which is very well integrated into <strong>Proxmox</strong> .  LINBIT provides a ready-made DRBDmanage plugin for Proxmox which allows you to use all its functions directly from the <strong>Proxmox</strong> interface. </p><br><p>  It looks really awesome, but unfortunately it has some downsides. </p><br><ul><li> First, the volume names, <strong>LVM-group</strong> or <strong>ZFS-pool</strong> must have the name <code>drbdpool</code> . </li><li>  Inability to use more than <strong>one</strong> pool per node </li><li>  Due to the specifics of the solution, the <strong>controller volume</strong> can only be located on a regular LVM and nothing else </li><li>  Periodic <strong>dbus</strong> glitches, which are closely used by <strong>DRBDmanage</strong> to interact with nodes. </li></ul><br><p>  As a result, LINBIT made the decision to replace all the complex logic of DRBDmanage with a simple application that communicates with the nodes using the usual <strong>tcp connection</strong> and works without any magic there.  This is how <strong>Linstor</strong> appeared. </p><br><p>  <strong>Linstor</strong> really works very well.  Unfortunately, the developers chose <strong>java</strong> as their main language for writing Linstor-server, but don't let that frighten you, since Linstor itself only deals with the <strong>distribution of</strong> DRBD <strong>configs</strong> and <strong>cutting</strong> LVM / ZFS sections on nodes. </p><br><blockquote>  Both solutions are free and distributed under the free <strong>GPL3</strong> license <strong>.</strong> </blockquote><p>  You can read about each of them and about setting up the above-mentioned plug-in for <strong>Proxmox</strong> at the <a href="https://docs.linbit.com/docs/users-guide-9.0/">official Proxmox wiki.</a> </p><br><h2 id="otkazoustoychivyy-nfs-server">  Failsafe NFS Server </h2><br><p>  Unfortunately at the time of this writing, <strong>Linstor</strong> has a ready-made integration only with <strong>Kubernetes</strong> .  But at the end of the year, drivers for the rest of <strong>Proxmox</strong> , <strong>OpenNebula</strong> , <strong>OpenStack</strong> are also expected. </p><br><p>  But so far there is no ready-made solution, and we don‚Äôt like the old one anyway.  Let's try to use DRBD9 in the old manner to organize <strong>NFS access</strong> to a common partition. </p><br><p>  However, this solution is also not without advantages, because the NFS server will allow you to organize <strong>competitive access</strong> to the storage file system from several servers without the need to use complex cluster file systems with DLM, such as OCFS and GFS2. </p><br><p>  In this case, you will be able to switch the roles of <strong>Primary</strong> / <strong>Secondary</strong> nodes simply by migrating the container with the NFS server in the Proxmox interface. </p><br><p>  You can also store any files inside this file system, as well as virtual disks and backups. </p><br><p>  In case you use <strong>Kubernetes,</strong> you can arrange <strong>ReadWriteMany</strong> access for your <strong>PersistentVolumes</strong> . </p><br><h2 id="proxmox-i-lxc-konteynery">  Proxmox and LXC containers </h2><br><p>  Now the question is: why Proxmox? </p><br><p>  In principle, to build such a scheme, we could use Kubernetes as well as the usual scheme with a cluster manager.  But <strong>Proxmox</strong> provides a ready-made, very multifunctional and at the same time simple and intuitive interface for almost everything you need.  It can <strong>clustering</strong> out of the box and supports the <strong>softdog</strong> based <strong>fencing</strong> mechanism.  And when using <strong>LXC containers,</strong> it allows to achieve minimum timeouts when switching. <br>  The resulting solution will not have a single <strong>point of failure</strong> . </p><br><p>  In essence, we will use Proxmox primarily as a <strong>cluster-manager</strong> , where we will be able to treat a separate <strong>LXC container</strong> as a service running in a classic HA cluster, with the only difference that its bundled system also comes with its <strong>root system</strong> .  That is, you do not need to install several eczemals of the service on each server separately; you can only do this once inside the container. <br>  If you have ever worked with <strong>cluster-manager software</strong> and providing <strong>HA</strong> for applications, you will understand what I mean. </p><br><h2 id="obschaya-shema">  General scheme </h2><br><p>  Our solution will resemble the standard replication scheme of some database. </p><br><ul><li>  We have <strong>three nodes</strong> </li><li>  On each node distributed <strong>drbd device</strong> . </li><li>  On the device, the usual file system ( <strong>ext4</strong> ) </li><li>  Only one server can be <strong>master</strong> </li><li>  The wizard runs the <strong>NFS server</strong> in the <strong>LXC container</strong> . </li><li>  All nodes access the device strictly via <strong>NFS.</strong> </li><li>  If necessary, the wizard can move to another node, along with the <strong>NFS server.</strong> </li></ul><br><p>  <strong>DRBD9</strong> has one very cool feature that simplifies things a lot: <br>  The drbd device will automatically become <strong>Primary</strong> at the moment when it is mounted on a node.  If the device is marked as <strong>Primary</strong> , any attempt to mount it on another node will result in an access error.  This ensures blocking and guaranteed protection against simultaneous access to the device. </p><br><p>  Why is this all so much easier?  Because when starting the container, <strong>Proxmox</strong> automatically mounts this device and it becomes <strong>Primary</strong> on this node, and when the container is stopped, it unmounts the device and the device becomes <strong>Secondary</strong> again. <br>  So we no longer need to worry about switching <strong>Primary</strong> / <strong>Secondary</strong> devices, Proxmox will do this <strong>automatically</strong> , Hurray! </p><br><h2 id="nastroyka-drbd">  DRBD setup </h2><br><p>  Well, well, the idea sorted out now move on to the implementation. </p><br><p>  By default, the <strong>module of the eighth version of drbd</strong> is supplied <strong>with the Linux kernel</strong> , unfortunately it does <strong>not suit</strong> us and we need to install the module of the ninth version. </p><br><p>  Connect the LINBIT repository and install everything you need: </p><br><pre> <code class="bash hljs">wget -O- https://packages.linbit.com/package-signing-pubkey.asc | apt-key add - <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"deb http://packages.linbit.com/proxmox/ proxmox-5 drbd-9.0"</span></span> \ &gt; /etc/apt/sources.list.d/linbit.list apt-get update &amp;&amp; apt-get -y install pve-headers drbd-dkms drbd-utils drbdtop</code> </pre> <br><ul><li>  <code>pve-headers</code> - kernel headers necessary for building the module </li><li>  <code>drbd-dkms</code> - kernel module in DKMS format </li><li>  <code>drbd-utils</code> - basic utilities for managing DRBD </li><li>  <code>drbdtop</code> is an interactive tool like top for DRBD only </li></ul><br><p>  After installing the <strong>module,</strong> check if everything is in order with it: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># modprobe drbd # cat /proc/drbd version: 9.0.14-1 (api:2/proto:86-113)</span></span></code> </pre> <br><p>  If you see the <strong>eighth version</strong> in the output of the command, something went wrong and the <strong>in-tree</strong> kernel module is loaded.  Check the <code>dkms status</code> for the reason. </p><br><p>  Each node will have the same <strong>drbd device</strong> running on top of the usual partitions.  First we need to prepare this section for drbd on each node. </p><br><p>  As such a partition can be any <strong>block device</strong> , it can be lvm, zvol, a disk partition or the entire disk.  In this article I will use a separate nvme disk with a partition for drbd: <code>/dev/nvme1n1p1</code> </p><br><p>  It is worth noting that device names tend to change sometimes, so it‚Äôs better to take the habit of using a permanent symlink on a device right away. </p><br><p>  You can find such a symlink for <code>/dev/nvme1n1p1</code> like this: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># find /dev/disk/ -lname '*/nvme1n1p1' /dev/disk/by-partuuid/847b9713-8c00-48a1-8dff-f84c328b9da2 /dev/disk/by-path/pci-0000:0e:00.0-nvme-1-part1 /dev/disk/by-id/nvme-eui.0000000001000000e4d25c33da9f4d01-part1 /dev/disk/by-id/nvme-INTEL_SSDPEKKA010T7_BTPY703505FB1P0H-part1</span></span></code> </pre> <br><p>  We describe our resource on all three nodes: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat /etc/drbd.d/nfs1.res resource nfs1 { meta-disk internal; device /dev/drbd100; protocol C; net { after-sb-0pri discard-zero-changes; after-sb-1pri discard-secondary; after-sb-2pri disconnect; } on pve1 { address 192.168.2.11:7000; disk /dev/disk/by-partuuid/95e7eabb-436e-4585-94ea-961ceac936f7; node-id 0; } on pve2 { address 192.168.2.12:7000; disk /dev/disk/by-partuuid/aa7490c0-fe1a-4b1f-ba3f-0ddee07dfee3; node-id 1; } on pve3 { address 192.168.2.13:7000; disk /dev/disk/by-partuuid/847b9713-8c00-48a1-8dff-f84c328b9da2; node-id 2; } connection-mesh { hosts pve1 pve2 pve3; } }</span></span></code> </pre> <br><p>  It is advisable to synchronize drbd to use a <strong>separate network</strong> . </p><br><p>  Now create the metadata for drbd and launch it: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm create-md nfs1 initializing activity log initializing bitmap (320 KB) to all zero Writing meta data... New drbd meta data block successfully created. success # drbdadm up nfs1</span></span></code> </pre> <br><p>  Repeat these actions on all three nodes and check the status: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm status nfs1 role:Secondary disk:Inconsistent pve2 role:Secondary peer-disk:Inconsistent pve3 role:Secondary peer-disk:Inconsistent</span></span></code> </pre> <br><p>  Now our disk is <strong>Inconsistent</strong> on all three nodes, this is because drbd does not know which disk should be taken as the original.  We need to mark one of them as <strong>Primary</strong> , so that its state is synchronized to the other nodes: </p><br><pre> <code class="bash hljs">drbdadm primary --force nfs1 drbdadm secondary nfs1</code> </pre> <br><p>  Immediately after this, <strong>synchronization</strong> will start: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm status nfs1 role:Secondary disk:UpToDate pve2 role:Secondary replication:SyncSource peer-disk:Inconsistent done:26.66 pve3 role:Secondary replication:SyncSource peer-disk:Inconsistent done:14.20</span></span></code> </pre><br><p>  We don‚Äôt have to wait until it is finished and we can follow up the next steps in parallel.  They can be performed on <strong>any node</strong> , regardless of its current state of the local disk in DRBD.  All requests will be automatically redirected to the device with the <strong>UpToDate</strong> state. </p><br><p>  Don't forget to activate the <strong>autorun of the</strong> drbd service on the nodes: </p><br><pre> <code class="hljs pgsql">systemctl <span class="hljs-keyword"><span class="hljs-keyword">enable</span></span> drbd.service</code> </pre> <br><h2 id="nastroyka-lxc-konteynera">  Configure LXC Container </h2><br><p>  Omit part of the configuration of the <strong>Proxmox cluster</strong> of three nodes, this part is well described in the <a href="https://pve.proxmox.com/wiki/Cluster_Manager">official wiki</a> </p><br><p>  As I said before, our <strong>NFS server</strong> will work in an <strong>LXC container</strong> .  We will keep the container itself on the <code>/dev/drbd100</code> device we just created. </p><br><p>  First we need to create a <strong>file system</strong> on it: </p><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">mkfs</span></span> -t ext4 -O mmp -E mmp_update_interval=<span class="hljs-number"><span class="hljs-number">5</span></span> /dev/drbd100</code> </pre> <br><p>  <strong>Proxmox</strong> by default includes <strong>multimount protection</strong> at the file system level, in principle we can do without it, because  DRBD by default has its own protection, it will simply prohibit the second <strong>Primary</strong> for the device, but caution will not harm us. </p><br><p>  Now download the Ubuntu template: </p><br><pre> <code class="hljs pgsql"># wget http://download.proxmox.com/images/<span class="hljs-keyword"><span class="hljs-keyword">system</span></span>/ubuntu<span class="hljs-number"><span class="hljs-number">-16.04</span></span>-standard_16<span class="hljs-number"><span class="hljs-number">.04</span></span><span class="hljs-number"><span class="hljs-number">-1</span></span>_amd64.tar.gz -P /var/lib/vz/<span class="hljs-keyword"><span class="hljs-keyword">template</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">cache</span></span>/</code> </pre> <br><p>  And create from it our container: </p><br><pre> <code class="hljs pgsql">pct <span class="hljs-keyword"><span class="hljs-keyword">create</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span> <span class="hljs-keyword"><span class="hljs-keyword">local</span></span>:vztmpl/ubuntu<span class="hljs-number"><span class="hljs-number">-16.04</span></span>-standard_16<span class="hljs-number"><span class="hljs-number">.04</span></span><span class="hljs-number"><span class="hljs-number">-1</span></span>_amd64.tar.gz \ <span class="hljs-comment"><span class="hljs-comment">--hostname=nfs1 \ --net0=name=eth0,bridge=vmbr0,gw=192.168.1.1,ip=192.168.1.11/24 \ --rootfs=volume=/dev/drbd100,shared=1</span></span></code> </pre> <br><p>  In this command, we specify that the <strong>root system of</strong> our container will be located on the <code>/dev/drbd100</code> and add the parameter <code>shared=1</code> to allow the container to be migrated between nodes. </p><br><p>  If something went wrong, you can always fix it through the <strong>Proxmox</strong> interface or in the <code>/etc/pve/lxc/101.conf</code> container <code>/etc/pve/lxc/101.conf</code> </p><br><p>  Proxmox will unpack the template and prepare <strong>the</strong> container <strong>root system</strong> for us.  After that we can run our container: </p><br><pre> <code class="hljs pgsql">pct <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span></code> </pre> <br><h2 id="nastroyka-nfs-servera">  Configure NFS server. </h2><br><p>  By default, Proxmox <strong>does not allow the</strong> launch of an <strong>NFS server</strong> in a container, but there are several ways to allow this. </p><br><p>  One of them is just to add <code>lxc.apparmor.profile: unconfined</code> to the config of our container <code>/etc/pve/lxc/100.conf</code> . </p><br><p>  Or we can <strong>enable NFS</strong> for all containers on an ongoing basis, for this you need to update the standard template for LXC on all nodes, add the following lines to <code>/etc/apparmor.d/lxc/lxc-default-cgns</code> : </p><br><pre> <code class="hljs nginx"> <span class="hljs-attribute"><span class="hljs-attribute">mount</span></span> fstype=nfs, mount fstype=nfs4, mount fstype=nfsd, mount fstype=rpc_pipefs,</code> </pre> <br><p>  After the changes, restart the container: </p><br><pre> <code class="hljs pgsql">pct shutdown <span class="hljs-number"><span class="hljs-number">101</span></span> pct <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span></code> </pre> <br><p>  Now let's log in to it: </p><br><pre> <code class="hljs perl">pct <span class="hljs-keyword"><span class="hljs-keyword">exec</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span> bash</code> </pre> <br><p>  Install updates and <strong>NFS server</strong> : </p><br><pre> <code class="hljs sql">apt-get <span class="hljs-keyword"><span class="hljs-keyword">update</span></span> apt-<span class="hljs-keyword"><span class="hljs-keyword">get</span></span> -y <span class="hljs-keyword"><span class="hljs-keyword">upgrade</span></span> apt-<span class="hljs-keyword"><span class="hljs-keyword">get</span></span> -y <span class="hljs-keyword"><span class="hljs-keyword">install</span></span> nfs-kernel-<span class="hljs-keyword"><span class="hljs-keyword">server</span></span></code> </pre> <br><p>  Create an <strong>export</strong> : </p><br><pre> <code class="hljs haskell"><span class="hljs-title"><span class="hljs-title">echo</span></span> '/<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class"> *(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">rw</span></span></span><span class="hljs-class">,</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">no_root_squash</span></span></span><span class="hljs-class">,</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">no_subtree_check</span></span></span><span class="hljs-class">)' &gt;&gt; /etc/exports mkdir /</span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class"> exportfs -a</span></span></code> </pre> <br><h2 id="nastroyka-ha">  HA Setup </h2><br><p>  At the time of writing this article, proxmox <strong>HA-manager</strong> has a <a href="https://bugzilla.proxmox.com/show_bug.cgi%3Fid%3D1842">bug</a> that prevents the HA container from successfully completing its work, as a result of which, the <strong>nfs server</strong> processes that are not fully killed by the <strong>kernel-space</strong> prevent the drbd device from going to <strong>Secondary</strong> .  If you have already encountered such a situation, do not panic and simply execute <code>killall -9 nfsd</code> on the node where the container was launched and then the drbd device should ‚Äúrelease‚Äù and it will go to the <strong>Secondary</strong> . </p><br><p>  To fix this bug, execute the following commands on all nodes: </p><br><pre> <code class="hljs pgsql">sed -i <span class="hljs-string"><span class="hljs-string">'s/forceStop =&gt; 1,/forceStop =&gt; 0,/'</span></span> /usr/<span class="hljs-keyword"><span class="hljs-keyword">share</span></span>/perl5/PVE/HA/Resources/PVECT.pm systemctl <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span> pve-ha-lrm.service</code> </pre> <br><p>  Now we can go to the <strong>HA-manager</strong> configuration.  Create a separate HA group for our device: </p><br><pre> <code class="hljs pgsql">ha-manager groupadd nfs1 <span class="hljs-comment"><span class="hljs-comment">--nodes pve1,pve2,pve3 --nofailback=1 --restricted=1</span></span></code> </pre> <br><p>  Our <strong>resource</strong> will work only on the nodes specified for this group.  Add our container to this group: </p><br><pre> <code class="hljs cs">ha-manager <span class="hljs-keyword"><span class="hljs-keyword">add</span></span> ct:<span class="hljs-number"><span class="hljs-number">101</span></span> --<span class="hljs-keyword"><span class="hljs-keyword">group</span></span>=nfs1 --max_relocate=<span class="hljs-number"><span class="hljs-number">3</span></span> --max_restart=<span class="hljs-number"><span class="hljs-number">3</span></span></code> </pre> <br><p>  That's all.  Simple, isn't it? </p><br><p>  The resulting <strong>nfs-ball</strong> can be immediately connected to Proxmox, for storing and running other virtual machines and containers. </p><br><h2 id="rekomendacii-i-tyuning">  Recommendations and tuning </h2><br><h5 id="drbd">  DRBD </h5><br><p>  As I noted above, it is always advisable to use a separate network for replication.  It is highly desirable to use <strong>10-gigabit network adapters</strong> , otherwise you all will rest on the speed of the ports. <br>  If replication seems slow enough, try tuning in some parameters for <strong>DRBD</strong> .  Here is the config that, in my opinion, is optimal for my <strong>10G network</strong> : </p><br><pre> <code class="hljs swift"># cat /etc/drbd.d/global_common.conf global { usage-<span class="hljs-built_in"><span class="hljs-built_in">count</span></span> yes; udev-always-use-vnr; } common { handlers { } startup { } options { } disk { <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-fill-target 10M; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-<span class="hljs-built_in"><span class="hljs-built_in">max</span></span>-rate 720M; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-plan-ahead <span class="hljs-number"><span class="hljs-number">10</span></span>; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-<span class="hljs-built_in"><span class="hljs-built_in">min</span></span>-rate 20M; } net { <span class="hljs-built_in"><span class="hljs-built_in">max</span></span>-buffers 36k; sndbuf-size 1024k; rcvbuf-size 2048k; } }</code> </pre> <br><p>  More information about each parameter you can get information from the <a href="https://docs.linbit.com/man/v9/drbd-conf-5/">official documentation DRBD</a> </p><br><h5 id="nfs-server">  NFS server </h5><br><p>  To speed up the work of the <strong>NFS server,</strong> an increase in the total number of running <strong>instances of the</strong> NFS <strong>server</strong> may help.  By default - <strong>8</strong> , I personally helped increase this number to <strong>64</strong> . </p><br><p>  To achieve this, update the <code>RPCNFSDCOUNT=64</code> parameter in <code>/etc/default/nfs-kernel-server</code> . <br>  And restart the daemons: </p><br><pre> <code class="hljs pgsql">systemctl <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span> nfs-utils systemctl <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span> nfs-<span class="hljs-keyword"><span class="hljs-keyword">server</span></span></code> </pre> <br><h5 id="nfsv3-vs-nfsv4">  NFSv3 vs NFSv4 </h5><br><p>  Do you know the difference between <strong>NFSv3</strong> and <strong>NFSv4</strong> ? </p><br><ul><li>  <strong>NFSv3</strong> is a <strong>stateless protocol;</strong> as a rule, it is better at crashing and recovering faster. </li><li>  <strong>NFSv4</strong> is a <strong>stateful protocol</strong> , it is faster and can be tied to certain tcp ports, but due to the presence of a state, it is more sensitive to failures.  It also has the ability to use authentication using Kerberos and a bunch of other interesting features. </li></ul><br><p>  However, when you run <code>showmount -e nfs_server</code> , the NFSv3 protocol is used.  Proxmox also uses NFSv3.  NFSv3 is also commonly used to organize booting of machines over a network. </p><br><p>  In general, if you have no particular reason to use NFSv4, try to use NFSv3, as it is less painful when experiencing any failures due to a lack of state as such. </p><br><p>  You can mount the ball using NFSv3 by specifying the <code>-o vers=3</code> parameter for the <strong>mount</strong> command: </p><br><pre> <code class="bash hljs">mount -o vers=3 nfs_server:/share /mnt</code> </pre> <br><p>  If you wish, you can disable NFSv4 for the server at all, to do this, add the option <code>--no-nfs-version 4</code> to the <code>RPCNFSDCOUNT</code> variable and restart the server, for example: </p><br><pre> <code class="bash hljs">RPCNFSDCOUNT=<span class="hljs-string"><span class="hljs-string">"64 --no-nfs-version 4"</span></span></code> </pre> <br><h2 id="iscsi-i-lvm">  iSCSI and LVM </h2><br><p>  Similarly, a normal <strong>tgt-daemon</strong> can be configured inside the container, iSCSI will produce much more performance for I / O operations, and the container will work more smoothly, in view of the fact that the tgt-server runs completely in user space. </p><br><p>  Typically, an exported <strong>LUN</strong> is sliced ‚Äã‚Äãinto multiple pieces using <strong>LVM</strong> .  However, there are several nuances that are worth considering, for example: how are LVM <strong>locks</strong> provided for sharing an exported group on several hosts. </p><br><p>  Perhaps these and other nuances I will describe in the <strong><a href="https://habr.com/post/417597/">next article</a></strong> . </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/417473/">https://habr.com/ru/post/417473/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../417461/index.html">Ivan Tulup: asynchronism in JS under the hood</a></li>
<li><a href="../417463/index.html">Unexpected meeting. Chapter 17</a></li>
<li><a href="../417465/index.html">Overview of text entry binding methods</a></li>
<li><a href="../417469/index.html">Five selfish reasons to work reproducibly</a></li>
<li><a href="../417471/index.html">Simple Solder MK936 SMD. DIY soldering station on SMD components</a></li>
<li><a href="../417475/index.html">Glusterfs + erasure coding: when you need a lot, cheap and reliable</a></li>
<li><a href="../417477/index.html">Hot workplace (Hot desking)</a></li>
<li><a href="../417479/index.html">Accelerate do-it-yourself string concatenation</a></li>
<li><a href="../417481/index.html">About generators in javascript es6 and why it‚Äôs not necessary to study them</a></li>
<li><a href="../417483/index.html">Comparing JS frameworks: React, Vue and Hyperapp</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>