<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to measure content relevance</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Content evaluation is one of the main components of the relevance formula. Knowledge of textual signs and the contribution of each of them to the eval...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to measure content relevance</h1><div class="post__text post__text-html js-mediator-article"> Content evaluation is one of the main components of the relevance formula.  Knowledge of textual signs and the contribution of each of them to the evaluation of the site will allow you to get closer to more professional work with the resource.  This article will review the model that allows you to restore the ranking formula for each specific request, indicates the significance of the definition of the site when promoting a specific request, and also addresses the issue of determining an unnatural text. <br><br>  <b>Restoration of the ranking formula</b> <br><br>  If we translate this problem into the field of mathematics, then the input data can be represented by a set of vectors, where each vector is the set of characteristics of each site, and the coordinates in the vector are the parameter by which the site is evaluated.  In the described vector space, a function must be defined that determines the ratio of the order of two objects to each other.  This function allows you to rank objects among themselves according to the principle ‚Äúmore - less‚Äù, however, at the same time, it is impossible to say just how much more or less of the other.  These types of tasks are related to the tasks of estimating ordinal regression. <br>  Our employees developed an algorithm based on a linear regression model with adjustable selectivity, which allowed us to restore the ranks of sites with a certain degree of error and predict the change in output with appropriate adjustments of site parameters.  The first step of the algorithm is to train the model.  In this case, the training sample represents the results of ranking sites within a single search query.  The orderliness of sites within the search query actually means that there is a certain direction in the feature space, to which the objects of the training sample must be designed in the right order.  This direction is required in the task of restoring the ranking formula.  However, judging by Figure 1, there may be many such areas. <br><img src="https://habrastorage.org/storage3/fa2/7df/e7a/fa27dfe7a41fc18f9dd517b6c5e78902.jpg" alt="image"><br>  Fig.  1. The choice of the guiding vector 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      To address this issue, the approach underlying the method of anchor points was considered, namely, the choice of such a direction that will ensure maximum removal of objects from each other. <br>  The next task that was solved was the choice of a learning strategy.  Two options were considered - an abbreviated learning strategy, which takes into account the order of the two corresponding elements, and a complete strategy, which takes into account the entire order of objects.  As a result of the experiments, an abbreviated strategy was chosen, which consists in solving the following equation: (1) <br><img src="https://habrastorage.org/storage3/333/291/5b5/3332915b518f8ab4eb112a2e5812dd04.jpg" alt="image">  where <img src="https://habrastorage.org/storage3/97f/a5a/ddf/97fa5addfa1f99a6038bdaad666e57e6.jpg" alt="image">  - solution of the standard quadratic programming problem with linear constraints: <img src="https://habrastorage.org/storage3/af7/c19/db7/af7c19db7ad85236316ad13b05b124da.jpg" alt="image">  where <br><img src="https://habrastorage.org/storage3/caa/a7a/ca5/caaa7aca5c63438cd45817802d76fb55.jpg" alt="image">  - symmetric matrix <br><img src="https://habrastorage.org/storage3/cbc/f3d/f71/cbcf3df714cc2d909f61320cd64cf1b7.jpg" alt="image">  - coefficient vector <br><img src="https://habrastorage.org/storage3/d05/00a/f35/d0500af3526f494603fe3491ccf0267b.jpg" alt="image">  - the difference of the vectors of characteristics <br><br>  This approach on different samples (100 signs and 500 signs on 20 different sets of search queries) showed good results (see Table 1). <br><br><a name="habracut"></a><br>  Table 1. Results of the reduced model <br><br><img src="https://habrastorage.org/storage3/584/ae7/ba6/584ae7ba655b4eb368d5a930812d5220.jpg" alt="image"><br><img src="https://habrastorage.org/storage3/31d/ea4/447/31dea44479945347b9ef52b9ac3b6fb4.jpg" alt="image"><br>  Fig.  2. Restored regression coefficients with n = 100 <br><img src="https://habrastorage.org/storage3/c14/e03/ba9/c14e03ba98b815e18005690426cfa917.jpg" alt="image"><br>  Fig.  3. Restored regression coefficients with n = 500 <br>  If we talk about the results on specific queries, the experiments performed give the following error indicator <br><br>  Table 2. Calculation Errors <br><br><img src="https://habrastorage.org/storage3/e0b/9ab/7a1/e0b9ab7a1633b22117eec2f11b9c5d9a.jpg" alt="image"><br><br>  When working on a project, this approach was used to predict positions with a specific change on the site.  Similar experiments were conducted on the basis of textual signs.  Initially, data were collected from the TOP20 sites for the request under consideration, then the data were standardized using an appropriate algorithm.  After that, the algorithm was performed directly on the calculation of "relevance" using the quadratic programming method. <br>  The obtained values ‚Äã‚Äãof the relevance of the site are sorted and the conclusion about the restored positions is drawn. <br><br>  Table 3. Restoration of positions <br><br><img src="https://habrastorage.org/storage3/9bb/d5c/f36/9bbd5cf36d530f055e0ddf8e6428151f.jpg" alt="image"><br><br>  It was found that the greatest impact on the position when ranking the request "tire fitting equipment" brings signs: the presence in the Yandex catalog, the entry of the first word from the query "tire", the entry in h1 of the first word of the query "tire", the entry in the title page of the second word of the query " equipment". <br>  Appropriate adjustments were made to the site parameters and the program was launched.  As a result, a forecast was given for the corresponding position. <br><br><img src="https://habrastorage.org/storage3/624/24a/60a/62424a60a1f365c64afb2bdc2cf6d116.jpg" alt="image"><br>  Fig.  4. Program restoring the ranking formula <br><br>  All these changes were made on the site. After the next update, the site took positions close to those predicted.  The initial position was 50, after these changes it amounted to the TOP20. <br><br><img src="https://habrastorage.org/storage3/238/91f/209/23891f209b5cab30207785e750a6c9a0.jpg" alt="image"><br>  Fig.5.  The results of the promotion request "tire equipment" <br><br>  <b>Text subject measurement</b> <br><br>  In working with the restoration of the ranking formula, the importance of measuring the thematic proximity of the subject matter of the text in relation to the subject of the entire site was confirmed.  Such a metric can be constructed based on the calculation of the cosine between the vectors of the relevant subject of the page relevant to the query and the entire site: (2) <br><img src="https://habrastorage.org/storage3/077/ca5/242/077ca5242827e3a5a3dd61fbc18f9fcc.jpg" alt="image"><br>  Where <img src="https://habrastorage.org/storage3/6d1/a0f/e13/6d1a0fe133e3f95e02ad2e4c75b26c23.jpg" alt="image">  and <img src="https://habrastorage.org/storage3/fd1/dc2/5a8/fd1dc25a86b2721dd80cfefb305f3fbc.jpg" alt="image">  accordingly, the vector designation of the theme of the site and the document in question. <br>  N is the number of words in the dictionary collection.  The weight of each word j in the document Di is calculated by the formula: (3) <br><img src="https://habrastorage.org/storage3/4f2/6d4/2eb/4f26d42eb73ded7a12dd5299b28b9e1e.jpg" alt="image"><br>  where countij is the number of occurrences of the word in the document, IDFwj is the inverse frequency of the word in the collection.  After calculating the weight of each word in the document, the vector is normalized: (4) <br><img src="https://habrastorage.org/storage3/c8d/454/f38/c8d454f386d997badeaae8ffc32c2bc6.jpg" alt="image"><br><br>  Similarly, the vector is constructed for the entire site, and the text of the site is obtained by combining the texts of all the documents included in it. <br>  Thus, the algorithm for determining the thematic value of the document can be represented as follows: <br>  1) A dictionary is defined, in which there are no rare and stop words, i.e.  IDF words that form the dictionary, lies in the range of meaningful words. <br>  2) An N-dimensional subject vector is built. <img src="https://habrastorage.org/storage3/1af/0ae/60f/1af0ae60f31e24bfc9891d493cfcb844.jpg" alt="image">  for the document in question <img src="https://habrastorage.org/storage3/4ae/a52/e28/4aea52e2831bfe6e1b943c3d42e054c1.jpg" alt="image">  using formulas 3 and 4. <br>  3) Built N-dimensional vector of subject <img src="https://habrastorage.org/storage3/38c/dcf/350/38cdcf35008f3fc8c39585955fd0f940.jpg" alt="image">  for the whole site <img src="https://habrastorage.org/storage3/698/f5e/059/698f5e059831236ed4f6e38c6c4c0ace.jpg" alt="image">  using formulas 3 and 4. <br>  4) Using (2) establishes the proximity of the vectors <img src="https://habrastorage.org/storage3/1af/0ae/60f/1af0ae60f31e24bfc9891d493cfcb844.jpg" alt="image">  and <img src="https://habrastorage.org/storage3/38c/dcf/350/38cdcf35008f3fc8c39585955fd0f940.jpg" alt="image">  .  The closer the vector, the thematic value of the document <img src="https://habrastorage.org/storage3/4ae/a52/e28/4aea52e2831bfe6e1b943c3d42e054c1.jpg" alt="image">  above. <br><br>  On the basis of this model, a program was written, allowing to determine the thematic similarity of the considered document and the textual component of the site itself.  The experiments were conducted on the basis of 3 groups of sites: with the same theme, with a similar theme, with different themes.  A total of 200 articles were processed.  As a result of processing, the following data was obtained for 20 groups of ‚Äú1 test document / 9 training documents‚Äù presented in the table. <br><br>  Table 4. Results of thematic completeness check <br><br><img src="https://habrastorage.org/storage3/fb5/d56/e31/fb5d56e3106abecb98e9f4bb60f4d3cb.jpg" alt="image"><br><br>  The table shows that the proposed method for determining the thematic completeness of an information resource works in practice: checked documents located on sites with a more comprehensive topic, have higher rates.  However, the shortcomings of the developed system were also revealed.  Firstly, sites often have uninformative or uninformative pages (order forms, feedback, contacts, etc.).  Secondly, when choosing a randomly specified number of training texts, you can select non-thematic pages.  Thirdly, non-specific content may come across as test texts, but they are similar in topics, for example, the spelling of a word.  Fourthly, there are sites that cover different thematic areas, while intersecting in meaning (online shopping, news sites, banks of abstracts). <br>  With the listed shortcomings, the overall picture allows us to evaluate the thematic fullness of the resource.  As an example, consider the site of the logistics theme with requests for equipment (there is a catalog on the site, in addition to information about logistics).  The site is registered in Yandex.Catalog and has a ‚Äúforwarding and transportation of goods‚Äù rubric: <br><img src="https://habrastorage.org/storage3/14f/69b/44b/14f69b44bde0045abc8e3dc9b246fdb9.jpg" alt="image"><br>  Fig.  6. The rubric assigned in the catalog. Yandex <br><br>  When using the method discussed above, it was concluded that the thematic completeness of the pages promoted is not complete with respect to the requests of the ‚Äútransportation and delivery from China‚Äù subject, but it is sufficiently large with respect to the subject ‚Äúequipment‚Äù.  The ratio of pages "logistics: equipment" was respectively "30: 200."  Accordingly, the position and traffic was only in requests related to equipment.  In this case, the priority was "logistics".  To solve the problem, a letter was written to Yandex in order to obtain detailed information.  However, the standard response of ‚ÄúPlato‚Äù about the improvement and development of the site was received, but in general everything is in order. <br>  As a solution, there was a choice between the development of the required topics on the site and the separation of two topics into different subdomains.  Won the need to get quick results.  TK was compiled for transferring the ‚Äúequipment‚Äù direction to the subdomain, and the main site saved information on ‚Äúlogistics‚Äù, as well as the development of the resource by adding new relevant subject pages.  The result of the changes is shown in Fig.  Requests for equipment successfully moved to the subdomain and took a positive position.  And after adding thematic pages on logistics and requests for transportation, they began to show a positive trend. <br><img src="https://habrastorage.org/storage3/899/0da/229/8990da22914828aa3d288187f8529f05.jpg" alt="image"><br>  Fig.  6. Results of promotion, after breeding topics <br><br>  Thus, due to the ‚Äúsubdomain + domain‚Äù scheme, it was possible to spread the topics without a loss and thereby increase the relevance of each of the topics separately and achieve a positive trend on requests. <br><br>  <b>Measuring the naturalness of the text</b> <br><br>  Requirements for entering Yandex.Catalog are being tightened.  Recently, we have to deal with the fact that when checking a site, Yandex employees report low-quality content.  Identifying this fact manually on a large site seems to be a problem.  Therefore, at present, work is underway to analyze the characteristics of these texts.  I will tell about some of them.  There are two main approaches to receiving spam text: the replacement of Russian letters with Latin letters and the generation of content devoid of meaning. <br>  The first approach is opened by identifying the modified words using an inverted frequency and comparing with the established empirical critical value.  Words formed by replacing Russian letters with similar Latin letters are rare words from the point of view of usage statistics.  With the help of the inverted frequency of the general collection can identify such words.  Each element of the text node <img src="https://habrastorage.org/storage3/fc6/242/8d9/fc62428d99049ee3d8d546d8d7a47e10.jpg" alt="image">  matched value <img src="https://habrastorage.org/storage3/aac/e7a/ace/aace7aace3f81ec75fa973d2f28f0671.jpg" alt="image">  using the inverted frequency function fh: (5) <br><img src="https://habrastorage.org/storage3/9ca/2ed/202/9ca2ed20268e864aa553f359b3746eac.jpg" alt="image"><br>  As a function of the inverted frequency were considered: (6), (7), (8). <br><img src="https://habrastorage.org/storage3/087/585/e44/087585e44efd1c2d3c84398eb2e6d4b9.jpg" alt="image"><br>  Here D is the number of documents in the collection, DF is the number of documents in which the lemma is found, CF is the number of occurrences of the lemma in the collection, TotalLemms is the total number of occurrences of all the lemmas in the collection.  Of these options, the best result in the experiment, as well as in the research of Gulin A. showed ICF (7), therefore <img src="https://habrastorage.org/storage3/1c9/72a/d1a/1c972ad1ab96acbb13a02b0fe678e3b7.jpg" alt="image">  where is the number of occurrences of the lemma in the text under consideration, is the total number of occurrences of all the lemmas in the set. <br>  The greater the value of the function fh, the less often the word occurs.  To obtain the ICF interval of significant words, a program was written, to the input of which texts of various contents were submitted (elimination of thematic influence).  The program processed about 500 MB of textual information.  As a result of processing, a dictionary of reverse frequencies of the ICF words in normal form was obtained.  The lemmatization of words was carried out using the mystem parser, Yandex.  All elements of the dictionary were sorted in order of increasing reverse frequency.  As a result of the analysis of this dictionary, the interval of significant words was obtained: [500;  191703]. <br>  To establish the criterion for identifying spam texts, a critical value H crit is also entered and Hp percent of words are counted, whose characteristic exceeds the critical value H crit determined by empirically: (9) <br><img src="https://habrastorage.org/storage3/879/2e4/469/8792e44698431f924a441f8f101a08c1.jpg" alt="image"><br>  As a critical mark, the percentage of insignificant words is 50% (the highest indicator of the frequency of official words is 37.60%, and the average of the words created by the author is 5.63%).  A large percentage of the use of Hp in one text of such word formations will indicate that the document is generated. <br>  However, there are not enough websites with such spam texts.  The second approach is more common.  There is a class of unnatural texts generated by the use of generators based on Markov chains.  Based on the research Pavlova A.S.  A model has been proposed to identify such texts. <br>  All textual component B of document D has a number of signs. <img src="https://habrastorage.org/storage3/b29/943/72c/b2994372c9bc292695252d768f8436a4.jpg" alt="image">  difficult to control by the author.  To build an automatic classifier of unnatural texts, selected features in machine learning are used.  The algorithm being developed is based on decision trees C4.5.  The algorithm itself for determining unnatural text is as follows: <br><img src="https://habrastorage.org/storage3/741/bd8/fbe/741bd8fbef08a7eb370aeb33077cbabf.jpg" alt="image"><br>  Fig.  7. Algorithm for determining generated content <br><br>  To obtain a decision tree, a base of natural texts in the amount of 2000 was prepared and a base of unnatural texts of 2000 volume also, some found on the Internet, some generated, the rest obtained by synonymizing sample documents or by translating from foreign languages.  The original collection was the ROMIP By.Web collection.  Generation and synonymization tools were found on the Internet (TextoGEN, Generating The Web 2.2, SeoGenerator and others). <br>  The resulting set of texts was divided into two equal parts.  The first group was used as a training set, and the second part was a test set.  Both samples had an equal number of sample documents and generated texts. <br>  For the learning process, a program was written that, for each text, built a vector that evaluates parameters affecting the definition of the naturalness of the text.  According to the study Pavlova A.S.  The greatest contribution to learning is a list of parameters that determine the text diversity and frequency of use of parts of speech.  The table contains a list of the most valuable features for the classification of Russian texts, for each feature the F-measure and type of feature are indicated. <br><br>  Table 5. The most valuable features for the classification of texts <br><img src="https://habrastorage.org/storage3/cb5/8fd/488/cb58fd488b6f756c9adf23d8586d7b03.jpg" alt="image"><br><br>  According to the obtained vectors P of each of the document D, a decision tree was built.  This procedure was carried out using the analytical platform Deductor Studio Academic version 5.2.  In the Deductor, the basis of the decision tree handler is a modified C4.5 algorithm that solves the classification problem.  As a result, a tree was built with 157 nodes and 79 rules.  In fig.  presented part of the resulting tree.  The obtained rules were used in the main program in determining the spam texts of the site. <br><br><img src="https://habrastorage.org/storage3/d50/79d/832/d5079d8320e3bafcf1f95dd4d4aadd14.jpg" alt="image"><br>  Fig.  8. Decision Tree.  Analytical platform Deductor 5.2. <br><br><img src="https://habrastorage.org/storage3/696/db2/9c1/696db29c10d00ca0cb5d607c53793542.jpg" alt="image"><br>  Fig.  9. The result of the program for analyzing texts <br><br>  In practice, this approach helped to discover the reason for the lack of dynamics on requests.  The program detected generated texts on all pages of the site categories.  During the investigation, it was found out that they represent the machine translation content of the same site, but of the English version. <br><br><img src="https://habrastorage.org/storage3/5ba/e7f/8ff/5bae7f8ffe4376c27db185c3b80598f9.jpg" alt="image"><br>  Fig.  10. Texts on category pages <br><br>  After editing these texts, even only on pages that were promoted, good dynamics was obtained: requests from the TOP500 immediately hit the TOP10 in 9 weeks. <br><br><img src="https://habrastorage.org/storage3/567/59d/324/56759d3245a00b96438148eb3294a65d.jpg" alt="image"><br>  Fig.  11. An example of a modified text. <br><br><img src="https://habrastorage.org/storage3/485/b00/5e5/485b005e5a91b3e6a052307c4f06917e.jpg" alt="image"><br>  Fig.  12. Changing positions by weeks after laying out. <br><br>  In conclusion, it should be noted that the development of the considered functionals is not obligatory!  It is useful in global search engine research.  When promoting a site, it is sufficient to develop an approach that allows point-to-point work with queries based on the analysis of the TOP.  There are many natural tools for this: <br>  1) Check how many relevant pages are available on request and compare with competitors - you can evaluate the text completeness of the site <br>  2) Pay attention to the highlighted words in verbose queries in the saved copy - help in writing texts, how far words can stand apart <br>  3) Use the query language.  For example, analyzing the issue of exact request and without quotes, you can identify problems with the text component <br>  4) Through the advanced search, search for specific sites and analyze which pages and why are higher than promoted <br>  5) Results of Webmasters. Yandex and Webmasters. Google, these metrics and GA will also help to identify problems and work with them. <br>  Purposeful activity on requests always gives a positive result. <br><br>  Authors of the article: Neyolova N.V.  (Candidate of Technical Sciences, head of the PP department of Ingate), Polenova Ye.A.  (Ingate team leader) </div><p>Source: <a href="https://habr.com/ru/post/195134/">https://habr.com/ru/post/195134/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../195120/index.html">Amazon introduced the Kindle HDX</a></li>
<li><a href="../195122/index.html">Correspondence Olympiad on sports programming for schoolchildren from NUST MISiS and Cognitive Technologies</a></li>
<li><a href="../195124/index.html">Why Opera Webkit</a></li>
<li><a href="../195128/index.html">RailsClub'Moscow 2013. Interview with Frederick Chang (Frederick Cheung)</a></li>
<li><a href="../195130/index.html">Another 10 TB storage from the Chinese - now distributes Alibaba (upd)</a></li>
<li><a href="../195136/index.html">Integration of web services into popular cms</a></li>
<li><a href="../195138/index.html">C ++ test case, sorting functor</a></li>
<li><a href="../195140/index.html">Adding Admob to Unity3d and withdrawing money from PayPal to a bank account in Russia</a></li>
<li><a href="../195142/index.html">Random Cat Generator in 8 Steps</a></li>
<li><a href="../195146/index.html">Evaluation of linear regression results</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>