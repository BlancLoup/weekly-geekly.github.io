<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Is data analysis on Scala a harsh necessity or a pleasant opportunity?</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Traditional tools in the field of Data Science are languages ‚Äã‚Äãsuch as R and Python - a relaxed syntax and a large number of libraries for machine lea...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Is data analysis on Scala a harsh necessity or a pleasant opportunity?</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/5q/1e/fr/5q1efrbkxbuk_ndqoensinfl80a.jpeg"></p><br><p>  Traditional tools in the field of Data Science are languages ‚Äã‚Äãsuch as <a href="https://www.r-project.org/">R</a> and <a href="https://www.python.org/">Python</a> - a relaxed syntax and a large number of libraries for machine learning and data processing allows you to quickly get some working solutions.  However, there are situations when the limitations of these tools become a significant hindrance - first of all, if you need to achieve high processing speed and / or work with really large data arrays.  In this case, the specialist must, reluctantly, turn to the help of the "dark side" and connect the tools in the "industrial" programming languages: <a href="https://www.scala-lang.org/">Scala</a> , <a href="https://www.java.com/">Java</a> and <a href="http://www.open-std.org/jtc1/sc22/wg21/">C ++</a> . </p><br><p>  But is this side really dark?  Over the years of development, the tools of the "industrial" Data Science have come a long way and today are quite different from their own versions 2-3 years ago.  Let's try using the example of the <a href="https://snahackathon.org/">SNA Hackathon 2019</a> task to <a href="https://snahackathon.org/">find</a> out how the Scala + Spark ecosystem can correspond to Python Data Science. </p><a name="habracut"></a><br><p>  In the framework of <a href="https://snahackathon.org/">SNA Hackathon 2019,</a> participants solve the problem of sorting the news feed of a social network user in one of three "disciplines": using data from texts, images, or logs of signs.  In this publication, we will understand how Spark can solve a problem based on the log of features by means of classical machine learning. </p><br><p>  In solving the problem, we will follow the standard path that any data analyst goes through when developing a model: </p><br><ul><li>  We will conduct a research analysis of the data, construct graphs. </li><li>  Let us analyze the statistical properties of features in the data, look at their differences between the training and test sets. </li><li>  We will carry out the initial selection of signs based on statistical properties. </li><li>  We calculate the correlations between the signs and the target variable, as well as the cross-correlation between the signs. </li><li>  Let's form the final set of features, train the model and check its quality. </li><li>  Let us analyze the internal structure of the model to identify growth points. </li></ul><br><p>  In the course of our "journey" we will get acquainted with such tools as the <a href="http://zeppelin.apache.org/">Zeppelin</a> interactive notebook, the <a href="http://spark.apache.org/docs/latest/ml-guide.html">Spark ML</a> machine learning library and its extension <a href="https://github.com/odnoklassniki/pravda-ml">PravdaML</a> , the <a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> <a href="https://github.com/odnoklassniki/pravda-ml">graph</a> package, the <a href="https://www.vegas-viz.org/">Vegas</a> visualization library, and, of course, <a href="http://spark.apache.org/">Apache Spark</a> in all its glory: ).  All code and experiment results are available on the <a href="https://www.zepl.com/viewer/notebooks/bm90ZTovL2RtaXRyeWJ1Z2F5Y2hlbmtvLzc2ZTJiNWU1YjQ1YzRjZTY5YWFlZGUzMmI4OTc0OWJiL25vdGUuanNvbg">Zepl</a> collaborative notebooks <a href="https://www.zepl.com/viewer/notebooks/bm90ZTovL2RtaXRyeWJ1Z2F5Y2hlbmtvLzc2ZTJiNWU1YjQ1YzRjZTY5YWFlZGUzMmI4OTc0OWJiL25vdGUuanNvbg">platform</a> . </p><br><h1 id="zagruzka-dannyh">  Data loading </h1><br><p>  The peculiarity of <a href="https://cloud.mail.ru/public/A9cF/bVSWJCJgt">data</a> laid out on <a href="https://snahackathon.org/">SNA Hackathon 2019</a> is that it is possible to process them directly using Python, but it is difficult: the source data is efficiently packed due to the capabilities of the Apache Parquet column format and when reading in the forehead memory is unpacked into several dozen gigabytes.  When working with Apache Spark, it is not necessary to completely load data into memory, the Spark architecture is designed to process data in chunks, loading from disk as necessary. </p><br><p>  Therefore, the first step - checking the distribution of data by day - is easily performed by boxed tools: </p><br><pre><code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> train = sqlContext.read.parquet(<span class="hljs-string"><span class="hljs-string">"/events/hackatons/SNAHackathon/2019/collabTrain"</span></span>) z.show(train.groupBy($<span class="hljs-string"><span class="hljs-string">"date"</span></span>).agg( functions.count($<span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>).as(<span class="hljs-string"><span class="hljs-string">"count"</span></span>), functions.countDistinct($<span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>).as(<span class="hljs-string"><span class="hljs-string">"users"</span></span>), functions.countDistinct($<span class="hljs-string"><span class="hljs-string">"instanceId_objectId"</span></span>).as(<span class="hljs-string"><span class="hljs-string">"objects"</span></span>), functions.countDistinct($<span class="hljs-string"><span class="hljs-string">"metadata_ownerId"</span></span>).as(<span class="hljs-string"><span class="hljs-string">"owners"</span></span>)) .orderBy(<span class="hljs-string"><span class="hljs-string">"date"</span></span>))</code> </pre> <br><p>  What the corresponding <a href="https://www.zepl.com/viewer/notebooks/bm90ZTovL2RtaXRyeWJ1Z2F5Y2hlbmtvLzc2ZTJiNWU1YjQ1YzRjZTY5YWFlZGUzMmI4OTc0OWJiL25vdGUuanNvbg/20190221-134145_1120174705%3FasIframe">graph will</a> display in Zeppelin: </p><br><p><img src="https://habrastorage.org/webt/jp/lh/pw/jplhpwwz4tfgdtrs1u_u-tqsvzi.png"></p><br><p>  I must say that the Scala syntax is quite flexible, and the same code might look, for example, like this: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> train = sqlContext.read.parquet(<span class="hljs-string"><span class="hljs-string">"/events/hackatons/SNAHackathon/2019/collabTrain"</span></span>) z.show( train groupBy $<span class="hljs-string"><span class="hljs-string">"date"</span></span> agg( count($<span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>) as <span class="hljs-string"><span class="hljs-string">"count"</span></span>, countDistinct($<span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>) as <span class="hljs-string"><span class="hljs-string">"users"</span></span>, countDistinct($<span class="hljs-string"><span class="hljs-string">"instanceId_objectId"</span></span>) as <span class="hljs-string"><span class="hljs-string">"objects"</span></span>, countDistinct($<span class="hljs-string"><span class="hljs-string">"metadata_ownerId"</span></span>) as <span class="hljs-string"><span class="hljs-string">"owners"</span></span>) orderBy <span class="hljs-string"><span class="hljs-string">"date"</span></span> )</code> </pre> <br><p>  Here you need to make an important warning: when working in a large team, where everyone approaches the writing of the Scala code exclusively from the point of view of their own taste, communication is significantly hampered.  So it is better to develop a unified concept of code style. </p><br><p>  But back to our task.  A simple analysis by day showed the presence of anomalous points on February 17 and 18;  it is likely that incomplete data is collected on these days, and the distribution of signs may be shifted.  This should be taken into account in further analysis.  In addition, the fact that the number of unique users is very close to the number of objects is striking, so it makes sense to study the distribution of users with different numbers of objects: </p><br><pre> <code class="scala hljs">z.show(filteredTrain .groupBy($<span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>).count .groupBy(<span class="hljs-string"><span class="hljs-string">"count"</span></span>).agg(functions.log(functions.count(<span class="hljs-string"><span class="hljs-string">"count"</span></span>)).as(<span class="hljs-string"><span class="hljs-string">"withCount"</span></span>)) .orderBy($<span class="hljs-string"><span class="hljs-string">"withCount"</span></span>.desc) .limit(<span class="hljs-number"><span class="hljs-number">100</span></span>) .orderBy($<span class="hljs-string"><span class="hljs-string">"count"</span></span>))</code> </pre> <br><p><img src="https://habrastorage.org/webt/dh/i-/pp/dhi-ppdka19zurhimccpq7tij5w.png"></p><br><p>  Expectedly we see the distribution, close to a power, with a very long tail.  In such tasks, as a rule, it is possible to achieve improvement in the quality of work by segmenting models for users with different levels of activity.  In order to check whether it is worth doing this, let's compare the distribution of the number of objects per user in the test set: </p><br><p><img src="https://habrastorage.org/webt/du/sy/xt/dusyxten5shm24an736n7akolnm.png"></p><br><p>  Comparison with the test shows that test users have at least two objects in the logs (since the ranking task is solved on the hackathon, this is a necessary condition for assessing quality).  In the future, I recommend looking more closely at the users in the training set, for which we will declare the User Defined Function with a filter: </p><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//  ,     "",   , //     val testSimilar = sc.broadcast(filteredTrain.groupBy($"instanceId_userId") .agg( functions.count("feedback").as("count"), functions.sum(functions.expr("IF(array_contains(feedback, 'Liked'), 1.0, 0.0)")).as("sum") ) .where("count &gt; sum AND sum &gt; 0") .select("instanceId_userId").rdd.map(_.getInt(0)).collect.sorted) //           // User Defined Function val isTestSimilar = sqlContext.udf.register("isTestSimilar", (x: Int) =&gt; java.util.Arrays.binarySearch(testSimilar.value, x) &gt;= 0)</span></span></code> </pre> <br><p>  Here you also need to make an important remark: it is precisely in terms of the definition of UDF that the use of Spark from under Scala / Java and from under Python is strikingly different.  While the PySpark code uses the basic functionality, everything works almost as fast, but when the redefined functions appear, the PySpark performance degrades by an order of magnitude. </p><br><h1 id="pervyy-ml-konveyer">  First ML conveyor </h1><br><p>  In the next step, we will try to calculate the main statistics on actions and grounds.  But for this we need the capabilities of SparkML, so first consider its overall architecture: </p><br><p><img src="https://habrastorage.org/webt/j7/ev/en/j7evenhyzzfvzouqfkbfq1j9hvu.png"></p><br><p>  SparkML is based on the following concepts: </p><br><ul><li>  Transformer - takes as input a data set and returns a modified set (transform).  As a rule, it is used to implement pre- and post-processing algorithms, extract features, and can also represent final ML-models. </li><li>  Estimator - takes a data set as input, and returns a Transformer (fit).  In a natural way, the Estimator can represent the ML algorithm. </li><li>  Pipeline is a special case of Estimator, consisting of a chain of transformers and estimators.  When the fit method is called, it goes through the chain, and if it sees a transformer, then it applies it to the data, and if it sees the estimator, it teaches the transformer with it, applies it to the data and goes further. </li><li>  PipelineModel - the result of the work Pipeline also contains within the chain, but consisting exclusively of transformers.  Accordingly, the PipelineModel itself is also a transformer. </li></ul><br><p>  A similar approach to the formation of ML-algorithms helps to achieve a clear modular structure and good reproducibility - and the model and the conveyors can be saved. </p><br><p>  To begin with, we will build a simple pipeline with which we can calculate the statistics of the distribution of actions (feedback field) of users in the training set: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> feedbackAggregator = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( <span class="hljs-comment"><span class="hljs-comment">//         (feedback)  one-hot  new MultinominalExtractor().setInputCol("feedback").setOutputCol("feedback"), //       new VectorStatCollector() .setGroupByColumns("date").setInputCol("feedback") .setPercentiles(Array(0.1,0.5,0.9)), //        new VectorExplode().setValueCol("feedback") )).fit(train) z.show(feedbackAggregator .transform(filteredTrain) .orderBy($"date", $"feedback"))</span></span></code> </pre> <br><p>  In this pipeline, <a href="https://github.com/odnoklassniki/pravda-ml">PravdaML</a> functionality is actively used - libraries with extended useful blocks for SparkML, namely: </p><br><ul><li>  MultinominalExtractor is used to encode the character of the type "array of strings" into a vector on the one-hot principle.  This is the only estimator in the pipeline (to build the encoding, you need to collect unique strings from the dataset). </li><li>  VectorStatCollector is used to calculate vector statistics. </li><li>  VectorExplode is used to convert the result into a format that is easy to visualize. </li></ul><br><p>  The result will be a graph showing that the classes in the dataset are not balanced, but the imbalance for the target class Liked is not extreme: </p><br><p><img src="https://habrastorage.org/webt/aa/db/x4/aadbx4el5s5lhuyput_cj4ns9zo.png"></p><br><p>  An analysis of a similar distribution among users similar to test ones (having both ‚Äúpositive‚Äù and ‚Äúnegative‚Äù in the logs) shows that it is biased towards a positive class: </p><br><p><img src="https://habrastorage.org/webt/x5/lu/px/x5lupxmayvvodo7lsb3meob1dou.png"></p><br><h1 id="statisticheskiy-analiz-priznakov">  Statistical analysis of signs </h1><br><p>  At the next stage we will conduct a detailed analysis of the statistical properties of the signs.  This time we need a bigger conveyor: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> statsAggregator = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">NullToDefaultReplacer</span></span>(), <span class="hljs-comment"><span class="hljs-comment">//          new AutoAssembler() .setColumnsToExclude( (Seq("date", "feedback") ++ train.schema.fieldNames.filter(_.endsWith("Id")) : _*)) .setOutputCol("features"), new VectorStatCollector() .setGroupByColumns("date").setInputCol("features") .setPercentiles(Array(0.1,0.5,0.9)), new VectorExplode().setValueCol("features") ))</span></span></code> </pre> <br><p>  Since now we need to work not with a separate field, but with all the signs at once, we will use two more useful utilities <a href="https://github.com/odnoklassniki/pravda-ml">PravdaML</a> : </p><br><ul><li>  NullToDefaultReplacer allows you to replace missing elements in the data with their default values ‚Äã‚Äã(0 for numbers, false for logic variables, etc.).  If you do not do this conversion, then NaN values ‚Äã‚Äãwill appear in the final vectors, which is fatal for many algorithms (although, for example, XGBoost can survive this).  An alternative to replacing with zeros can be replacing with mean values, this is implemented in NaNToMeanReplacerEstimator. </li><li>  AutoAssembler is a very powerful utility that analyzes the table layout and for each column chooses a vectorization scheme corresponding to the type of the column. </li></ul><br><p>  Using the resulting pipeline, we calculate the statistics for three sets (a trainer, a trainer with a user filter and a test one) and save into separate files: </p><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//   (   AutoAssembler  ) val trained = statsAggregator.fit(filteredTrain) //       - ,     . trained .transform(filteredTrain .withColumn("date", //  ,      ,     , //        All   functions.explode(functions.array(functions.lit("All"), $"date")))) .coalesce(7).write.mode("overwrite").parquet("sna2019/featuresStat") trained .transform(filteredTrain .where(isTestSimilar($"instanceId_userId")) .withColumn("date", functions.explode(functions.array(functions.lit("All"), $"date")))) .coalesce(7).write.mode("overwrite").parquet("sna2019/filteredFeaturesStat") trained .transform(filteredTest.withColumn("date", functions.explode(functions.array(functions.lit("All"), $"date")))) .coalesce(3).write.mode("overwrite").parquet("sna2019/testFeaturesStat")</span></span></code> </pre> <br><p>  Having received three datasets with statistics of signs, let's analyze the following things: </p><br><ul><li>  Do we have signs for which there are large emissions. <br>  - Such signs should be limited, or filtered records emissions. </li><li>  Do we have signs with a large offset of the average relative to the median. <br>  - Such an offset often occurs when there is a power distribution; it makes sense to log these logs. </li><li>  Is there a shift in the mean distributions between the training and test sets? </li><li>  How densely filled our matrix of signs. </li></ul><br><p>  To clarify these aspects will help us such a request: </p><br><pre> <code class="scala hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">compareWithTest</span></span></span></span>(data: <span class="hljs-type"><span class="hljs-type">DataFrame</span></span>) : <span class="hljs-type"><span class="hljs-type">DataFrame</span></span> = { data.where(<span class="hljs-string"><span class="hljs-string">"date = 'All'"</span></span>) .select( $<span class="hljs-string"><span class="hljs-string">"features"</span></span>, <span class="hljs-comment"><span class="hljs-comment">//         // ( ) functions.log($"features_mean" / $"features_p50").as("skewenes"), //    90-      //    90-  ‚Äî    functions.log( ($"features_max" - $"features_p90") / ($"features_p90" - $"features_p50")).as("outlieres"), //       ,  //    ($"features_nonZeros" / $"features_count").as("train_fill"), $"features_mean".as("train_mean")) .join(testStat.where("date = 'All'") .select($"features", $"features_mean".as("test_mean"), ($"features_nonZeros" / $"features_count").as("test_fill")), Seq("features")) //          .withColumn("meanDrift", (($"train_mean" - $"test_mean" ) / ($"train_mean" + $"test_mean"))) //      .withColumn("fillDrift", ($"train_fill" - $"test_fill") / ($"train_fill" + $"test_fill")) } //         val comparison = compareWithTest(trainStat).withColumn("mode", functions.lit("raw")) .unionByName(compareWithTest(filteredStat).withColumn("mode", functions.lit("filtered")))</span></span></code> </pre> <br><p>  At this stage, the issue of visualization is acute: using Zeppelin‚Äôs standard tools to immediately display all aspects is difficult, and notebooks with a large number of graphs start to slow down significantly due to swelling of the DOM.  This problem can be solved with the <a href="https://www.vegas-viz.org/">Vegas</a> -DSL library on Scala to build <a href="https://vega.github.io/vega-lite/">vega-lite</a> specifications.  Vegas not only provides richer rendering capabilities (comparable to matplotlib), but also draws them on Canvas without inflating the DOM :). </p><br><p>  The specification of the graphics we are interested in will look like this: </p><br><pre> <code class="scala hljs">vegas.<span class="hljs-type"><span class="hljs-type">Vegas</span></span>(width = <span class="hljs-number"><span class="hljs-number">1024</span></span>, height = <span class="hljs-number"><span class="hljs-number">648</span></span>) <span class="hljs-comment"><span class="hljs-comment">//   .withDataFrame(comparison.na.fill(0.0)) //           .encodeX("meanDrift", Quant, scale = Scale(domainValues = List(-1.0, 1.0), clamp = true)) //   -       .encodeY("train_fill", Quant) //       .encodeColor("outlieres", Quant, scale=Scale( rangeNominals=List("#00FF00", "#FF0000"), domainValues = List(0.0, 5), clamp = true)) //       .encodeSize("skewenes", Quant) //   -   (   ) .encodeShape("mode", Nom) .mark(vegas.Point) .show</span></span></code> </pre> <br><p>  The chart below should read this: </p><br><ul><li>  The x-axis represents the shift of the distribution centers between the test and training sets (the closer to 0, the more stable the feature). </li><li>  On the Y axis, the percentage of non-zero elements is plotted (the higher, the more data are available for a sign for a greater number of points). </li><li>  The size shows the shift of the average relative to the median (the larger the point, the more likely the power law of distribution for it). </li><li>  The color indicates the presence of emissions (the redder, the more emissions). </li><li>  Well, the form distinguishes the comparison mode: with a user filter in the training set or without a filter. </li></ul><br><p><img src="https://habrastorage.org/webt/op/hb/6g/ophb6gi3wa_uvsld9rre6ujzquu.png"></p><br><p>  So, we can draw the following conclusions: </p><br><ul><li>  Some signs need an emission filter - we will limit the maximum values ‚Äã‚Äãfor the 90th percentile. </li><li>  Part of the signs shows the distribution close to exponential - we will take the logarithm. </li><li>  Part of the signs in the test is not presented - we exclude them from training. </li></ul><br><h1 id="korrelyacionnyy-analiz">  Correlation analysis </h1><br><p>  After getting a general idea of ‚Äã‚Äãhow the signs are distributed and how they relate between the training and test sets, let us try to analyze the correlations.  To do this, set up a feature extractor based on previous observations: </p><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//             val expressions = filteredTrain.schema.fieldNames //          .filterNot(x =&gt; x == "date" || x == "audit_experiment" || idsColumns(x) || x.contains("vd_")) .map(x =&gt; if(skewedFeautres(x)) { //      s"log($x) AS $x" } else { //     cappedFeatures.get(x).map(capping =&gt; s"IF($x &lt; $capping, $x, $capping) AS $x").getOrElse(x) }) val rawFeaturesExtractor = new Pipeline().setStages(Array( new SQLTransformer().setStatement(s"SELECT ${expressions.mkString(", ")} FROM __THIS__"), new NullToDefaultReplacer(), new AutoAssembler().setOutputCol("features") )) //       val raw = rawFeaturesExtractor.fit(filteredTrain).transform( filteredTrain.where(isTestSimilar($"instanceId_userId")))</span></span></code> </pre> <br><p>  From the new machinery in this pipeline, the SQLTransformer utility, which allows for arbitrary SQL transformations of the input table, is noteworthy. </p><br><p>  When analyzing correlations, it is important to filter out the noise created by the natural correlation of one-hot features.  For this, I would like to understand which elements of the vector correspond to which source columns.  This task in Spark is solved with the help of column metadata (stored with the data) and attribute groups.  The following code block is used to filter pairs of attribute names that originate from the same column of type String: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> attributes = <span class="hljs-type"><span class="hljs-type">AttributeGroup</span></span>.fromStructField(raw.schema(<span class="hljs-string"><span class="hljs-string">"features"</span></span>)).attributes.get <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> originMap = filteredTrain .schema.filter(_.dataType == <span class="hljs-type"><span class="hljs-type">StringType</span></span>) .flatMap(x =&gt; attributes.map(_.name.get).filter(_.startsWith(x.name + <span class="hljs-string"><span class="hljs-string">"_"</span></span>)).map(_ -&gt; x.name)) .toMap <span class="hljs-comment"><span class="hljs-comment">//   ,          val isNonTrivialCorrelation = sqlContext.udf.register("isNonTrivialCorrelation", (x: String, y : String) =&gt; //    Scala-quiz   Option originMap.get(x).map(_ != originMap.getOrElse(y, "")).getOrElse(true))</span></span></code> </pre> <br><p>  Having a dataset with a vector column, it is quite simple to calculate cross-correlations using Spark tools, but the result will be a matrix, for unfolding of which you will need to podshamanit a little into a set of pairs: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> pearsonCorrelation = <span class="hljs-comment"><span class="hljs-comment">//    Pearson  Spearman Correlation.corr(raw, "features", "pearson").rdd.flatMap( //           _.getAs[Matrix](0).rowIter.zipWithIndex.flatMap(x =&gt; { //   ,   (  , //  ) val name = attributes(x._2).name.get //    ,     x._1.toArray.zip(attributes).map(y =&gt; (name, y._2.name.get, y._1)) } //     DataFrame )).toDF("feature1", "feature2", "corr") .na.drop //   .where(isNonTrivialCorrelation($"feature1", $"feature2")) //    . pearsonCorrelation.coalesce(1).write.mode("overwrite") .parquet("sna2019/pearsonCorrelation")</span></span></code> </pre> <br><p>  And, of course, visualization: we will again need the help of Vegas to draw a heat map: </p><br><pre> <code class="scala hljs">vegas.<span class="hljs-type"><span class="hljs-type">Vegas</span></span>(<span class="hljs-string"><span class="hljs-string">"Pearson correlation heatmap"</span></span>) .withDataFrame(pearsonCorrelation .withColumn(<span class="hljs-string"><span class="hljs-string">"isPositive"</span></span>, $<span class="hljs-string"><span class="hljs-string">"corr"</span></span> &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>) .withColumn(<span class="hljs-string"><span class="hljs-string">"abs_corr"</span></span>, functions.abs($<span class="hljs-string"><span class="hljs-string">"corr"</span></span>)) .where(<span class="hljs-string"><span class="hljs-string">"feature1 &lt; feature2 AND abs_corr &gt; 0.05"</span></span>) .orderBy(<span class="hljs-string"><span class="hljs-string">"feature1"</span></span>, <span class="hljs-string"><span class="hljs-string">"feature2"</span></span>)) .encodeX(<span class="hljs-string"><span class="hljs-string">"feature1"</span></span>, <span class="hljs-type"><span class="hljs-type">Nom</span></span>) .encodeY(<span class="hljs-string"><span class="hljs-string">"feature2"</span></span>, <span class="hljs-type"><span class="hljs-type">Nom</span></span>) .encodeColor(<span class="hljs-string"><span class="hljs-string">"abs_corr"</span></span>, <span class="hljs-type"><span class="hljs-type">Quant</span></span>, scale=<span class="hljs-type"><span class="hljs-type">Scale</span></span>(rangeNominals=<span class="hljs-type"><span class="hljs-type">List</span></span>(<span class="hljs-string"><span class="hljs-string">"#FFFFFF"</span></span>, <span class="hljs-string"><span class="hljs-string">"#FF0000"</span></span>))) .encodeShape(<span class="hljs-string"><span class="hljs-string">"isPositive"</span></span>, <span class="hljs-type"><span class="hljs-type">Nom</span></span>) .mark(vegas.<span class="hljs-type"><span class="hljs-type">Point</span></span>) .show</code> </pre> <br><p>  The result is better to look at <a href="https://www.zepl.com/viewer/notebooks/bm90ZTovL2RtaXRyeWJ1Z2F5Y2hlbmtvLzc2ZTJiNWU1YjQ1YzRjZTY5YWFlZGUzMmI4OTc0OWJiL25vdGUuanNvbg/20190225-123255_196451391%3FasIframe">Zepl-e</a> .  For a common understanding: </p><br><p><img src="https://habrastorage.org/webt/nm/d9/bm/nmd9bmrion4goaov9_vgx5vbjoy.png"></p><br><p>  The heat map shows that there are clearly some correlations.  Let us try to isolate the blocks of the most highly correlated features, for this we use the <a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> library: we turn the correlation matrix into a graph, filter the edges by weight, then we find the connectedness components and leave only nondegenerate (from more than one element).  This procedure is inherently similar to the application of the <a href="https://ru.wikipedia.org/wiki/DBSCAN">DBSCAN</a> algorithm and is as follows: </p><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//   (GrpahX   ID) val featureIndexMap = spearmanCorrelation.select("feature1").distinct.rdd.map( _.getString(0)).collect.zipWithIndex.toMap val featureIndex = sqlContext.udf.register("featureIndex", (x: String) =&gt; featureIndexMap(x)) //    val vertices = sc.parallelize(featureIndexMap.map(x =&gt; x._2.toLong -&gt; x._1).toSeq, 1) //    val edges = spearmanCorrelation.select(featureIndex($"feature1"), featureIndex($"feature2"), $"corr") //     .where("ABS(corr) &gt; 0.7") .rdd.map(r =&gt; Edge(r.getInt(0), r.getInt(1), r.getDouble(2))) //       val components = Graph(vertices, edges).connectedComponents() val reversedMap = featureIndexMap.map(_.swap) //    ,    ,   //   val clusters = components .vertices.map(x =&gt; reversedMap(x._2.toInt) -&gt; reversedMap(x._1.toInt)) .groupByKey().map(x =&gt; x._2.toSeq) .filter(_.size &gt; 1) .sortBy(-_.size) .collect</span></span></code> </pre> <br><p>  The result is presented in the form of a table: </p><br><p><img src="https://habrastorage.org/webt/4b/t2/bl/4bt2blcjzmxbzucnm7zynpvpj-q.png"></p><br><p>  Based on the results of clustering, we can conclude that the most correlated groups were formed around features related to the user's membership in the group (membership_status_A), as well as around the object type (instanceId_objectType).  To better simulate the interaction of features, it makes sense to apply model segmentation ‚Äî to train different models for different types of objects, separately for groups in which the user is and is not. </p><br><h1 id="mashinnoe-obuchenie">  Machine learning </h1><br><p>  We approach the most interesting thing - machine learning.  The pipeline for training the simplest model (logistic regression) using SparkML and <a href="https://github.com/odnoklassniki/pravda-ml">PravdaML</a> extensions is as follows: </p><br><pre> <code class="scala hljs"> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">SQLTransformer</span></span>().setStatement( <span class="hljs-string"><span class="hljs-string">""</span></span><span class="hljs-string"><span class="hljs-string">"SELECT *, IF(array_contains(feedback, 'Liked'), 1.0, 0.0) AS label FROM __THIS__"</span></span><span class="hljs-string"><span class="hljs-string">""</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">NullToDefaultReplacer</span></span>(), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">AutoAssembler</span></span>() .setColumnsToExclude(<span class="hljs-string"><span class="hljs-string">"date"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_objectId"</span></span>, <span class="hljs-string"><span class="hljs-string">"feedback"</span></span>, <span class="hljs-string"><span class="hljs-string">"label"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"features"</span></span>), <span class="hljs-type"><span class="hljs-type">Scaler</span></span>.scale(<span class="hljs-type"><span class="hljs-type">Interceptor</span></span>.intercept(<span class="hljs-type"><span class="hljs-type">UnwrappedStage</span></span>.repartition( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">LogisticRegressionLBFSG</span></span>(), numPartitions = <span class="hljs-number"><span class="hljs-number">127</span></span>)))</code> </pre> <br><p>  Here we see not only many familiar elements, but also several new ones: </p><br><ul><li>  LogisticRegressionLBFSG is a distributed logistic regression estimator. </li><li>  In order to achieve maximum performance from distributed ML-algorithms.  data should be optimally distributed by partition.  The utility UnwrappedStage.repartition will help with this, adding to the conveyor a repartitioning operation in such a way that it is used only at the training stage (after all, it is no longer necessary to make predictions in it). </li><li>  So that the linear model can give a good result.  the data must be scaled, for which the utility Scaler.scale is responsible.  However, the presence of two consecutive linear transformations (scaling and multiplying by the regression weights) leads to unnecessary costs, and it is desirable to collapse these operations.  When using <a href="https://github.com/odnoklassniki/pravda-ml">PravdaML,</a> the output will be a pure model with one conversion :). </li><li>  And, of course, for such models you need a free member, which we add using the Interceptor.intercept operation. </li></ul><br><p>  The resulting pipeline, applied to all data, gives per-user AUC 0.6889 (the validation code is available on <a href="https://www.zepl.com/viewer/notebooks/bm90ZTovL2RtaXRyeWJ1Z2F5Y2hlbmtvLzc2ZTJiNWU1YjQ1YzRjZTY5YWFlZGUzMmI4OTc0OWJiL25vdGUuanNvbg">Zepl</a> ).  It now remains to apply all of our research: filter the data, transform features and segment the model.  The final pipeline will look like this: </p><br><pre> <code class="scala hljs"> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">SQLTransformer</span></span>().setStatement(<span class="hljs-string"><span class="hljs-string">s"SELECT instanceId_userId, instanceId_objectId, </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${expressions.mkString(", ")}</span></span></span><span class="hljs-string"> FROM __THIS__"</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">SQLTransformer</span></span>().setStatement(<span class="hljs-string"><span class="hljs-string">""</span></span><span class="hljs-string"><span class="hljs-string">"SELECT *, IF(array_contains(feedback, 'Liked'), 1.0, 0.0) AS label, concat(IF(membership_status = 'A', 'OwnGroup_', 'NonUser_'), instanceId_objectType) AS type FROM __THIS__"</span></span><span class="hljs-string"><span class="hljs-string">""</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">NullToDefaultReplacer</span></span>(), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">AutoAssembler</span></span>() .setColumnsToExclude(<span class="hljs-string"><span class="hljs-string">"date"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_objectId"</span></span>, <span class="hljs-string"><span class="hljs-string">"feedback"</span></span>, <span class="hljs-string"><span class="hljs-string">"label"</span></span>, <span class="hljs-string"><span class="hljs-string">"type"</span></span>,<span class="hljs-string"><span class="hljs-string">"instanceId_objectType"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"features"</span></span>), <span class="hljs-type"><span class="hljs-type">CombinedModel</span></span>.perType( <span class="hljs-type"><span class="hljs-type">Scaler</span></span>.scale(<span class="hljs-type"><span class="hljs-type">Interceptor</span></span>.intercept(<span class="hljs-type"><span class="hljs-type">UnwrappedStage</span></span>.repartition( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">LogisticRegressionLBFSG</span></span>(), numPartitions = <span class="hljs-number"><span class="hljs-number">127</span></span>))), numThreads = <span class="hljs-number"><span class="hljs-number">6</span></span>) ))</code> </pre> <br><p>     <a href="https://github.com/odnoklassniki/pravda-ml">PravdaML</a> ‚Äî    CombinedModel.perType.       ,     numThreads = 6.             . </p><br><p> ,   ,  per-user AUC 0.7004.    ?  ,   " "    <a href="https://xgboost.readthedocs.io/en/latest/">XGBoost</a> : </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">SQLTransformer</span></span>().setStatement(<span class="hljs-string"><span class="hljs-string">""</span></span><span class="hljs-string"><span class="hljs-string">"SELECT *, IF(array_contains(feedback, 'Liked'), 1.0, 0.0) AS label FROM __THIS__"</span></span><span class="hljs-string"><span class="hljs-string">""</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">NullToDefaultReplacer</span></span>(), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">AutoAssembler</span></span>() .setColumnsToExclude(<span class="hljs-string"><span class="hljs-string">"date"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_userId"</span></span>, <span class="hljs-string"><span class="hljs-string">"instanceId_objectId"</span></span>, <span class="hljs-string"><span class="hljs-string">"feedback"</span></span>, <span class="hljs-string"><span class="hljs-string">"label"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"features"</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">XGBoostRegressor</span></span>() .setNumRounds(<span class="hljs-number"><span class="hljs-number">100</span></span>) .setMaxDepth(<span class="hljs-number"><span class="hljs-number">15</span></span>) .setObjective(<span class="hljs-string"><span class="hljs-string">"reg:logistic"</span></span>) .setNumWorkers(<span class="hljs-number"><span class="hljs-number">17</span></span>) .setNthread(<span class="hljs-number"><span class="hljs-number">4</span></span>) .setTrackerConf(<span class="hljs-number"><span class="hljs-number">600000</span></span>L, <span class="hljs-string"><span class="hljs-string">"scala"</span></span>) ))</code> </pre> <br><p> ,     ‚Äî XGBoost  Spark !         <a href="https://xgboost.readthedocs.io/en/latest/jvm/index.html">DLMC</a> ,    <a href="https://github.com/odnoklassniki/pravda-ml">PravdaML</a> ,       (  <a href="https://vk.com/video-133150806_456239018"> </a> ).  XGboost " "   10     per-user AUC 0.6981. </p><br><h1 id="analiz-rezultatov">   </h1><br><p> ,     ,  ,       .    SparkML     ,      .  <a href="https://github.com/odnoklassniki/pravda-ml">PravdaML</a>  :      Parquet            Spark: </p><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//     val perTypeWeights = sqlContext.read.parquet("sna2019/perType/stages/*/weights") //     20    ( //  ) val topFeatures = new TopKTransformer[Double]() .setGroupByColumns("type") .setColumnToOrderGroupsBy("abs_weight") .setTopK(20) .transform(perTypeWeights.withColumn("abs_weight", functions.abs($"unscaled_weight"))) .orderBy("type", "unscaled_weight")</span></span></code> </pre> <br><p>     Parquet,        <a href="https://github.com/odnoklassniki/pravda-ml">PravdaML</a> ‚Äî TopKTransformer,           . </p><br><p>      Vegas (   <a href="https://www.zepl.com/viewer/notebooks/bm90ZTovL2RtaXRyeWJ1Z2F5Y2hlbmtvLzc2ZTJiNWU1YjQ1YzRjZTY5YWFlZGUzMmI4OTc0OWJiL25vdGUuanNvbg/20190305-003431_52267072%3FasIframe">Zepl</a> ): </p><br><p><img src="https://habrastorage.org/webt/q7/_n/fn/q7_nfnto-hw-9nbdgjf3chhm0oc.png"></p><br><p> ,    -   .      XGBoost? </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> significance = sqlContext.read.parquet( <span class="hljs-string"><span class="hljs-string">"sna2019/xgBoost15_100_raw/stages/*/featuresSignificance"</span></span> vegas.<span class="hljs-type"><span class="hljs-type">Vegas</span></span>() .withDataFrame(significance.na.drop.orderBy($<span class="hljs-string"><span class="hljs-string">"significance"</span></span>.desc).limit(<span class="hljs-number"><span class="hljs-number">40</span></span>)) .encodeX(<span class="hljs-string"><span class="hljs-string">"name"</span></span>, <span class="hljs-type"><span class="hljs-type">Nom</span></span>, sortField = <span class="hljs-type"><span class="hljs-type">Sort</span></span>(<span class="hljs-string"><span class="hljs-string">"significance"</span></span>, <span class="hljs-type"><span class="hljs-type">AggOps</span></span>.<span class="hljs-type"><span class="hljs-type">Mean</span></span>)) .encodeY(<span class="hljs-string"><span class="hljs-string">"significance"</span></span>, <span class="hljs-type"><span class="hljs-type">Quant</span></span>) .mark(vegas.<span class="hljs-type"><span class="hljs-type">Bar</span></span>) .show</code> </pre> <br><p><img src="https://habrastorage.org/webt/i2/oy/zb/i2oyzbrjlo15x7u1uwadmeswocq.png"></p><br><p>  ,   ,   XGBoost,         ,    .           <a href="https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27"></a> .   ,  XGBoost     ,    ,   . </p><br><h1 id="vyvody">  findings </h1><br><p>  ,       :).     : </p><br><ol><li>    ,     Scala  Spark    ,      <a href="http://zeppelin.apache.org/"></a> ,  <a href="https://github.com/odnoklassniki/pravda-ml">  </a> ,  <a href="https://www.vegas-viz.org/"> </a> ,   <a href="https://www.zepl.com/viewer/notebooks/bm90ZTovL2RtaXRyeWJ1Z2F5Y2hlbmtvLzc2ZTJiNWU1YjQ1YzRjZTY5YWFlZGUzMmI4OTc0OWJiL25vdGUuanNvbg"> </a> . </li><li>    Scala  Spark        Python:    ETL  ML,    ,      ,     . </li><li>  ,   ,   ,    (,  )    ,     ,      . </li><li> ,     ,       .          ,      ,     , -, . </li></ol><br><p> ,       ,    ,        ,    -.        , ,  " <a href="http://newprolab.com/ru/scala/">   Scala</a> "  Newprolab. </p><br><p> ,  ,      ‚Äî   <a href="https://snahackathon.org/">SNA Hackathon 2019</a> . </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/442688/">https://habr.com/ru/post/442688/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../442676/index.html">Where dreams lead: Underground</a></li>
<li><a href="../442678/index.html">The largest project in stereolithography: a 3D mammoth skeleton printed on a 3D printer</a></li>
<li><a href="../442680/index.html">Sensory replacement technologies will allow you to see the world with the help of sounds: how the neuroplasticity of the human brain works</a></li>
<li><a href="../442682/index.html">What to collect and how to build a C ++ project</a></li>
<li><a href="../442686/index.html">DataPower Training Course</a></li>
<li><a href="../442690/index.html">Moon mission "Bereshit" - selfie against the backdrop of the Earth</a></li>
<li><a href="../442692/index.html">Blockchain without intermediaries: how we sent securities to a distributed registry</a></li>
<li><a href="../442694/index.html">One of the streaming giants launched in India and attracted a million users in a week.</a></li>
<li><a href="../442696/index.html">S for Security: Internet Security of Things and reports on InoThings ++ 2019</a></li>
<li><a href="../442698/index.html">Moscow Metro application for Windows Store</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>