<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>We work with neural networks: checklist for debugging</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The code for machine learning software products is often complex and rather confusing. Detection and elimination of bugs in it is a resource-intensive...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>We work with neural networks: checklist for debugging</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/84a/7dc/4b8/84a7dc4b81799226681bb176f1c518b0.png" alt="image"><br><br>  The code for machine learning software products is often complex and rather confusing.  Detection and elimination of bugs in it is a resource-intensive task.  Even the simplest <a href="https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Architecture/feedforward.html">neural networks with direct connection</a> require a serious approach to the network architecture, initialization of weights, network optimization.  A small mistake can lead to unpleasant problems. <br><br>  This article focuses on the debugging algorithm of your neural networks. <br><a name="habracut"></a><br><blockquote>  <b>Skillbox recommends:</b> Practical course <a href="https://skillbox.ru/python/%3Futm_source%3Dskillbox.media%26utm_medium%3Dhabr.com%26utm_campaign%3DPTNDEV%26utm_content%3Darticles%26utm_term%3Dneuronet">Python-developer from scratch</a> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>We remind:</b> <i>for all readers of "Habr" - a discount of 10,000 rubles when recording for any Skillbox course on the promotional code "Habr".</i> </blockquote><cut></cut><br><h3>  The algorithm consists of five stages: </h3><br><ul><li>  simple start; </li><li>  loss confirmation; </li><li>  checking intermediate results and connections; </li><li>  diagnostics of parameters; </li><li>  control work. </li></ul><br>  If something seems more interesting to you than the rest, you can skip ahead to these sections. <br><br><h3>  Easy start </h3><br>  A neural network with a complex architecture, regularization, and a learning speed planner is much harder to debug than a normal one.  We are a little tricky here, since the point itself is indirectly related to debugging, but this is still an important recommendation. <br><br>  A simple start is to create a simplified model and train it on one set (point) of data. <br><br>  <b>First create a simplified model.</b> <br><br>  For a quick start, we create a small network with a single hidden layer and check that everything works correctly.  Then we gradually complicate the model, checking every new aspect of its structure (additional layer, parameter, etc.), and move on. <br><br>  <b>We teach the model on a single set (point) of data</b> <br><br>  As a quick test of the performance of your project, you can use one or two data points for training to confirm whether the system is working correctly.  The neural network should show 100% accuracy of training and verification.  If this is not the case, then either the model is too small, or you already have a bug. <br><br>  Even if everything is fine, prepare the model for the passage of one or several eras before moving on. <br><br><h3>  Loss assessment </h3><br>  Loss assessment is the main way to clarify model performance.  You need to make sure that the loss corresponds to the task, and the loss functions are evaluated on the correct scale.  If you use more than one type of loss, then make sure that they are all of the same order and correctly scaled. <br><br>  It is important to be attentive to initial losses.  Check how close the actual result to the expected, if the model started with a random assumption.  The <a href="http://cs231n.github.io/neural-networks-3/">work of Andrey Karpati suggests the following</a> : ‚ÄúMake sure that you get the result you expect when you start working with a small number of parameters.  It is better to check the data loss immediately (with setting the degree of regularization to zero).  For example, for CIFAR-10 with the Softmax classifier, we expect the initial loss to be 2.302, because the expected diffuse probability is 0.1 for each class (since there are 10 classes), and the loss of Softmax is the negative logarithmic probability of the correct class as - ln (0.1) = 2.302 ". <br><br>  For a binary example, a similar calculation is simply done for each of the classes.  Here, for example, data: 20% 0's and 80% 1's.  The expected initial loss will be up to ‚Äì0.2ln (0.5) ‚Äì0.8ln (0.5) = 0.693147.  If the result is greater than 1, this may indicate that the weights of the neural network are not properly balanced or the data is not normalized. <br><br><h3>  We check intermediate results and connections </h3><br>  To debug a neural network, it is necessary to understand the dynamics of the processes within the network and the role of separate intermediate layers, since they are connected.  Here are some common errors you may encounter: <br><br><ul><li>  incorrect expressions for gradient updates; </li><li>  weight updates do not apply; </li><li>  disappearing or exploding gradients (exploding gradients). </li></ul><br>  If the gradient values ‚Äã‚Äãare zero, this means that the learning rate in the optimizer is too low, or that you are faced with an incorrect expression for updating the gradient. <br><br>  In addition, it is necessary to monitor the values ‚Äã‚Äãof the functions of activations, weights and updates of each of the layers.  For example, the value of parameter updates (weights and offsets) <a href="https://cs231n.github.io/neural-networks-3/">should be 1-e3</a> . <br><br>  There is a phenomenon called ‚ÄúDying ReLU‚Äù or <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">‚Äúvanishing gradient problem‚Äù</a> when ReLU neurons will display zero after studying a large negative bias value for its weights.  These neurons are never activated anywhere else in the data. <br><br>  You can use gradient checking to identify these errors by approximating the gradient using a numerical approach.  If it is close to the calculated gradients, then the backward propagation was implemented correctly.  To create a gradient check, check out these wonderful resources from CS231 <a href="http://cs231n.github.io/neural-networks-3/">here</a> and <a href="http://cs231n.github.io/optimization-1/">here</a> , as well as the Andrew Nga <a href="https://www.youtube.com/watch%3Fv%3DP6EtCVrvYPU">lesson</a> on this topic. <br><br>  <a href="https://www.linkedin.com/in/faizankshaikh/">Faizan Sheikh</a> points out three main methods for visualizing a neural network: <br><br><ul><li>  Preliminary - simple methods that show us the general structure of the trained model.  They include the output of forms or filters of individual layers of the neural network and parameters in each layer. </li><li>  Based on activation.  In them we decipher the activation of individual neurons or groups of neurons in order to understand their functions. </li><li>  Based on gradients.  These methods tend to manipulate the gradients that are formed from going forward and backward when training a model (including significance maps and class activation maps). </li></ul><br>  There are several useful tools for visualizing the activations and connections of individual layers, for example, <a href="https://conx.readthedocs.io/en/latest/Getting%2520Started%2520with%2520conx.html" rel="noopener">ConX</a> and <a href="https://www.tensorflow.org/guide/tensorboard_histograms" rel="noopener">Tensorboard</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d86/8ff/f33/d868fff33e5b7cb69563bfee29b9db08.png"><br><br><h3>  Parameter diagnostics </h3><br>  Neural networks have a lot of parameters that interact with each other, which complicates optimization.  Actually, this section is the subject of active research by specialists; therefore, the suggestions below should be considered only as tips, starting points from which to proceed. <br><br>  <b>Packet size</b> (batch size) - if you want the packet size to be large enough to get accurate estimates of the error gradient, but small enough so that the stochastic gradient descent (SGD) can order your network.  Small package sizes will lead to a rapid convergence due to noise in the learning process and further to optimization difficulties.  This is described in more detail <a href="https://arxiv.org/abs/1609.04836">here</a> . <br><br>  <b>Learning speed</b> ‚Äî too low will result in slow convergence or the risk of getting stuck in local minima.  At the same time, a high learning rate will cause a discrepancy in optimization, since you risk ‚Äújumping‚Äù through a deep, but narrow part of the loss function.  Try using speed planning to reduce it in the process of learning a neural network.  There <a href="http://cs231n.github.io/neural-networks-3/">is a large section on</a> CS231n <a href="http://cs231n.github.io/neural-networks-3/">on this issue</a> . <br><br>  <b>Gradient clipping</b> - cropping of parameter gradients during back propagation at the maximum value or limit rate.  Useful for solving problems with any exploding gradients that you may encounter in the third paragraph. <br><br>  <b>Batch normalization</b> - used to normalize the input data of each layer, which allows to solve the problem of internal covariate shift.  If you use Dropout and Batch Norma together, <a href="https://towardsdatascience.com/pitfalls-of-batch-norm-in-tensorflow-and-sanity-checks-for-training-networks-e86c207548c8">read this article</a> . <br><br>  <b>Stochastic Gradient Descent (SGD) ‚ÄîThere</b> are several varieties of SGD that use momentum, adaptive learning rates, and the Nesterov method.  At the same time, none of them has a clear advantage both in terms of learning efficiency and generalization ( <a href="http://ruder.io/optimizing-gradient-descent/">see details here</a> ). <br><br>  <b>Regularization</b> is crucial for building a generalized model, because it adds a penalty for the complexity of the model or extreme values ‚Äã‚Äãof the parameters.  This is a way to reduce the dispersion of the model without significantly increasing its displacement.  More <a href="http://cs231n.github.io/neural-networks-3/">information here</a> . <br><br>  To evaluate everything yourself, you need to disable regularization and check the data loss gradient yourself. <br><br>  <b>Dropout</b> is another method of streamlining your network to prevent congestion.  During training, the prolapse is carried out only by maintaining the activity of the neuron with a certain probability p (hyper parameter) or by setting it to zero in the opposite case.  As a result, the network must use a different subset of parameters for each training batch, which reduces changes in certain parameters that become dominant. <br><br>  Important: if you use both dropout and packet normalization, be careful with the order of these operations or even with their sharing.  All this is still actively discussed and supplemented.  Here are two important discussions on this topic <a href="https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout">on Stackoverflow</a> and <a href="https://arxiv.org/abs/1801.05134">Arxiv</a> . <br><br><h3>  Work control </h3><br>  It's about documenting workflows and experiments.  If you do not document anything, you can forget, for example, what training speed or weight of classes is used.  Thanks to the control, you can easily view and play back previous experiments.  This reduces the number of duplicate experiments. <br><br>  True, manual documentation can be a daunting task in case of a large amount of work.  Tools like Comet.ml come to the rescue to help you automatically log data sets, code changes, experiment history and production models, including key information about your model (hyperparameters, model performance indicators, and environment information). <br><br>  The neural network can be very sensitive to small changes, and this will lead to a drop in model performance.  Tracking and documenting work is the first step to take in standardizing the environment and modeling. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0e0/52c/de4/0e052cde4414da934bdde111cc0f0bf5.png"><br><br>  I hope that this post can be the starting point from which you will begin debugging your neural network. <br><blockquote>  <b>Skillbox recommends:</b> <br><br><ul><li>  Two-year practical course <a href="https://iamwebdev.skillbox.ru/%3Futm_source%3Dskillbox.media%26utm_medium%3Dhabr.com%26utm_campaign%3DWEBDEVPRO%26utm_content%3Darticles%26utm_term%3Dneuronet">"I am a web developer PRO"</a> . </li><li>  Online course <a href="https://skillbox.ru/c-sharp/%3Futm_source%3Dskillbox.media%26utm_medium%3Dhabr.com%26utm_campaign%3DCSHDEV%26utm_content%3Darticles%26utm_term%3Dneuronet">"C # developer with 0"</a> . </li><li>  Practical annual course <a href="https://skillbox.ru/php/%3Futm_source%3Dskillbox.media%26utm_medium%3Dhabr.com%26utm_campaign%3DPHPDEV%26utm_content%3Darticles%26utm_term%3Dneuronet">"PHP developer from 0 to PRO"</a> . <br></li></ul></blockquote></div><p>Source: <a href="https://habr.com/ru/post/444684/">https://habr.com/ru/post/444684/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../444672/index.html">Roskoschestvo presented a rating of wired and wireless headphones available in Russia</a></li>
<li><a href="../444674/index.html">Sony Xperia 10 and Xperia 10 Plus - Smart Looks</a></li>
<li><a href="../444676/index.html">CRM rating, tops, reviews - all lie?</a></li>
<li><a href="../444678/index.html">Uptime day: April 12, normal flight</a></li>
<li><a href="../444682/index.html">Shares of Sony and Nintendo collapsed after the launch of video streaming for gamers from Google</a></li>
<li><a href="../444686/index.html">Waves Smart Assets: Black and White Lists, Interval Trading</a></li>
<li><a href="../444688/index.html">Please stop talking about the repository template with Eloquent</a></li>
<li><a href="../444690/index.html">How researchers at Uber apply and scale knowledge about human behavior</a></li>
<li><a href="../444692/index.html">MOSDROID # 16 Sulfur at Redmadrobot</a></li>
<li><a href="../444694/index.html">As we predicted an outflow, approaching it as a natural disaster.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>