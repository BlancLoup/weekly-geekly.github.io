<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How I made AI to identify fake news with an accuracy of 95% and almost went crazy</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A brief retelling: we made a program that determines the reliability of news with an accuracy of 95% (on a validation sample) using machine learning a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How I made AI to identify fake news with an accuracy of 95% and almost went crazy</h1><div class="post__text post__text-html js-mediator-article">  <i>A brief retelling: we made a program that determines the reliability of news with an accuracy of 95% (on a validation sample) using machine learning and natural language processing technologies.</i>  <i>Download it <a href="https://goo.gl/2cvBmp">here</a> .</i>  <i>In reality, accuracy may be somewhat lower, especially after some time, as the canons of writing news articles will change.</i> <br><br>  Looking at the rapid development of machine learning and the processing of natural language, I thought: what the hell is not joking, maybe I will be able to create a model that would reveal news content with inaccurate information, and thus at least a little to smooth out the catastrophic consequences <a href="http://theconversation.com/the-real-consequences-of-fake-news-81179">that now spreading fake news</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6c/67/fn/6c67fn3snoa0obnnr56ubrtgoc8.jpeg"></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      It is possible to argue with this, but, in my opinion, the most difficult stage in creating your own machine learning model is collecting materials for training.  <a href="https://towardsdatascience.com/5-things-i-learned-training-an-ai-model-on-every-nba-player-32d906b28688">When I was teaching a face recognition model</a> , I had to collect photos of each of the NBA league players in the 2017/2018 season for several days.  Now I did not even suspect that I would have to spend several agonizing months immersed in this process and face very unpleasant and creepy things that people try to pass off as real news and reliable information. <br><a name="habracut"></a><br><h4>  Fake definition </h4><br>  The first obstacles came as a surprise to me.  After studying the fake news sites more closely, I quickly discovered that there are many different categories that may include false information.  There are articles with frank lies, there are some that give real facts, but then they are misinterpreted, there are pseudoscientific texts, there are just essays with the author‚Äôs opinions disguised as news notes, there is a satire, there are compiler articles consisting mainly of foreign tweets and quotes.  I googled a bit and found different classifications in which people tried to break up such sites into groups - ‚Äúsatire‚Äù, ‚Äúfake‚Äù, ‚Äúmisleading‚Äù and so on. <br><br>  I decided that we could just as well take them as a basis, and went through the list of sites listed in order to collect examples from there.  Almost immediately, a problem arose: some of the sites marked as ‚Äúfake‚Äù or ‚Äúmisleading‚Äù actually contained authentic articles.  I realized that it would not be possible to collect data from them without checking for trivial errors. <br><br>  Then I began to ask myself: is it necessary to take into account satire and subjective texts and, if so, where to refer them to reliable materials, fake or in a separate category? <br><br><h4>  Tonality analysis </h4><br>  After scoring a week over sites with fake news, I wondered if I was not too complicating the problem.  It may be worthwhile to simply take some of the already existing teaching models for the analysis of tonality and try to identify patterns.  I decided to make a simple tool that will collect data: headings, descriptions, information about the authors and the text itself, and send them to the model for tonality analysis.  For the latter, I used <a href="https://goo.gl/2cvBmp">Textbox</a> - it was convenient, because I can run it locally, on my machine, and quickly get results. <br><br>  The textbox gives an estimate of tonality that can be interpreted as positive or negative.  Then I bungled an algorithm that assigned the tonality of each kind of uploaded data (title, authors, text, and so on) to a certain degree of significance, and put the whole system together to see if I could get a general assessment of the news article. <br><br>  At first, everything seemed to be going well, but after the seventh or eighth loaded article the system began to sink.  In general, all this did not even come close to the tool for identifying fake news that I wanted to create. <br><br>  It was a complete failure. <br><br><h4>  Natural language processing </h4><br>  At this stage, my friend <a href="https://twitter.com/dahernan">David Hernandez</a> advised me to train the model to independently process the text.  In order to do this, we needed as many examples as possible from different categories of texts that, according to our plan, the model would have to be able to recognize. <br><br>  Since I was already tortured in my attempts to identify patterns in fake sites, we decided to just take and collect data from domains, the category of which we were precisely known in order to quickly gain a base.  Literally in a few days, my inferior tool collected a volume of content, which we considered sufficient to train the model. <br><br>  The result was bad.  Having dug our learning materials deeper, we found that they simply could not be accurately packaged in clearly limited categories, as we would like.  Somewhere, the fake news was mixed with normal, somewhere there were completely posts from third-party blogs, and some articles were 90% Trump tweets.  It became clear that our entire base would have to be processed from scratch. <br><br>  This is where the fun began. <br><br>  On one beautiful Saturday, I took up this endless process: manually reviewing each article, determining which category it falls into, and then clumsily copying it into a huge spreadsheet that was becoming increasingly immense.  In these articles came across truly disgusting, malicious, racist remarks, to which I initially tried not to pay attention.  But after the first few hundred, they began to put pressure on me.  Everything was ruffling before my eyes, color perception began to falter, and the state of mind was greatly depressed.  How did our civilization come to this?  Why are people unable to critically perceive information?  Do we still have any hope?  So it went on for several more days, while I tried to scrape together enough materials for the model to be meaningful. <br><br>  I caught myself not already sure what I meant by ‚Äúfake news‚Äù, that I get angry when I see a point of view in which I disagree, and with difficulty resist the temptation to take only what I think true to.  In the end, what generally can be considered true and what is not? <br><br>  But in the end I nevertheless reached the magic number I was aiming for, and with great relief sent the materials to David. <br><br>  The next day, he again conducted training.  I looked forward to the results. <br>  We achieved an accuracy of about 70%.  In the first minute it seemed to me that this is an excellent result.  But then, having tested the system on spot checks of several random articles from the Network, I realized that no one would really benefit from it. <br><br>  It was a complete failure. <br><br><h4>  Fakebox </h4><br>  We return to the stage of drawing on the board.  What am I wrong?  David suggested that perhaps simplifying the mechanism is the key to higher accuracy.  Following his advice, I seriously thought about what problem I'm trying to solve.  And then it hit me: maybe the solution is to identify not fake news, but reliable.  Reliable news is much easier to bring into a single category.  They are based on facts, present them briefly and clearly, and contain a minimum of subjective interpretation.  And reliable resources, where you can collect materials, enough for them. <br><br>  So I returned to the Internet and once again started to collect a new database for training.  I decided to distribute materials in two groups: true and untrue.  The satirical notes, articles with subjective opinions, fake news and everything else that did not contain strictly factual information and did not fit the standards of the Associated Press were treated as untrue. <br><br>  It took me a few more weeks.  Every day I spent several hours collecting new content from all the sites you can imagine, from <a href="http://theonion.com/">The Onion</a> to <a href="http://reuters.com/">Reuters</a> .  I loaded several thousand examples of true and untrue texts into a giant table, and every day their number increased by several hundred more.  Finally, I decided that there was enough material to try again.  I sent the table to David and could not wait for the results. <br><br>  Seeing accuracy above 95%, I almost jumped for joy.  So, we still managed to identify patterns in the writing of articles that distinguish reliable news from everything that should not be taken seriously. <br><br>  It was a success (well, in a sense)! <br><br><h4>  Fake news - fight </h4><br>  The whole point of these scams was to prevent the dissemination of false information, so I am very pleased to share the result with you.  We called the system <a href="https://goo.gl/8eTTJE">Fakebox</a> , and it's very easy to use.  You just need to insert the text of the article, which causes you to doubt, and click on the "Ananlyze" button. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/t2/wj/kt/t2wjktqwa2oa2xh68zmclkjr7ck.png" width="600"></div><br><br>  With the REST API, Fakebox can be integrated into any environment.  This is a <a href="http://docker.com/">Docker</a> container, so you can deploy and scale it wherever and as you like.  Shovel unlimited amounts of content at the speed you need and automatically tag everything that needs your attention. <br><br>  Remember: the system determines whether the text is written in a language characteristic of a reliable news article.  If it gives a very low score, it means that the text is not a fact-based news note in its classic form: it may be misinformation, satire, the author‚Äôs subjective opinion or something else. <br><br>  To summarize, we taught the model to analyze how the text was written and to determine whether it contains evaluative vocabulary, author's judgments, words with emotional coloring, or obscene expressions.  It may falter if the text is very short or mainly consists of quotes (or tweets) from other people.  Fakebox, of course, will not solve the problem of fake news definitively, but it can help identify those materials that need to be treated with skepticism.  Enjoy! </div><p>Source: <a href="https://habr.com/ru/post/347586/">https://habr.com/ru/post/347586/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../347576/index.html">Object in a case or Optional in Java 8 and Java 9. Part 2: ‚ÄúHow it is done in Java 8‚Äù</a></li>
<li><a href="../347578/index.html">‚ÄúWhen you tell a true story, they believe it much more‚Äù - Interview with Oleg Shelaev, part 2</a></li>
<li><a href="../347580/index.html">On Lee effect waves: Pitonizing DAF generation</a></li>
<li><a href="../347582/index.html">Personal experience: how we transferred the infrastructure from one data center to the US to another</a></li>
<li><a href="../347584/index.html">Disable triggers in ZABBIX on schedule</a></li>
<li><a href="../347588/index.html">Tough and flexible IT skills: everything is more and less serious than I would like to think</a></li>
<li><a href="../347590/index.html">Open science school hackathon DeepHack.Babel</a></li>
<li><a href="../347592/index.html">Telegram bot for complex quests</a></li>
<li><a href="../347594/index.html">Beautiful Chromium and gnarled memset</a></li>
<li><a href="../347596/index.html">The evolution of x86 architecture system calls</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>