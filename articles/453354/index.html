<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The classification of land cover using eo-learn. Part 3</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="When results are needed better than ‚Äúsatisfactory‚Äù 


 Part 1 
 Part 2 





 The transition zone from the winter season to the summer, is made up of ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The classification of land cover using eo-learn. Part 3</h1><div class="post__text post__text-html js-mediator-article"><p>  When results are needed better than ‚Äúsatisfactory‚Äù </p><br><p>  <a href="https://habr.com/ru/post/452284/">Part 1</a> <br>  <a href="https://habr.com/ru/post/452378/">Part 2</a> </p><br><p><img src="https://habrastorage.org/webt/c0/ls/b2/c0lsb2it_c9qwggm74kdk3uglw4.png"></p><br><p>  <em>The transition zone from the winter season to the summer, is made up of images of Sentinel-2.</em>  <em>You can see some differences in the types of cover on the snow, which was described in the previous article.</em> </p><a name="habracut"></a><br><h2 id="predislovie">  Foreword </h2><br><p> The past couple of weeks have been quite difficult.  We published the <a href="https://habr.com/ru/post/452284/">first</a> and <a href="https://habr.com/ru/post/452378/">second</a> parts of our articles on the classification of cover in the scale of the whole country with the help of the <code>eo-learn</code> framework.  <code>eo-learn</code> is an open source library for creating an interlayer between receiving and processing satellite images and machine learning.  In previous articles in the <a href="https://github.com/sentinel-hub/eo-learn/blob/1746fc4734fe5a906291d805801196f28089bcb0/examples/land-cover-map/SI_LULC_pipeline.ipynb">examples,</a> we indicated only a small subset of data and showed results only on a small percentage of the entire area of ‚Äã‚Äãinterest (AOI).  I know it looks at least not very impressive, and perhaps very rude on our part.  All this time you have been tormented by questions about how you can use this knowledge and move it to the <em>next</em> level. </p><br><p>  Do not worry, it is for this that you need the third article in this series!  Grab a cup of coffee and sit down ... </p><br><h2 id="all-our-data-are-belong-to-you">  All our Data are Belong to You! </h2><br><p>  Are you already sitting?  Maybe leave the coffee on the table for a second, because now you will hear the best news for today ... <br>  We at Sinergise decided to publish the full data set for Slovenia for 2017.  Is free.  You can easily access 200GB of data in the form of ~ 300 EOPatch fragments, each approximately in the size of 1000x1000, at a resolution of 10m!  You can read more about the EOPatch format in the <a href="https://medium.com/sentinel-hub/introducing-eo-learn-ab37f2869f5c">last post</a> about <code>eo-learn</code> , but in fact it is a container for <em>geo-temporal</em> EO (Earth Observation) and non-EO data: for example, satellite images, masks, maps, etc. </p><br><p><img src="https://habrastorage.org/webt/dc/nt/gy/dcntgywsu4la7pdpwegv5m6eskc.png"><br>  <em>EOPatch structure</em> ) </p><br><p>  We did not bungle when we downloaded this data.  Each EOPatch contains Sentinel-2 L1C images, their corresponding <a href="https://medium.com/sentinel-hub/sentinel-hub-cloud-detector-s2cloudless-a67d263d3025">s2cloudless</a> mask, and the official land cover map in raster format! </p><br><p>  Data is stored on AWS S3 at: <a href="http://eo-learn.sentinel-hub.com/">http://eo-learn.sentinel-hub.com/</a> </p><br><p>  Deserializing an EOPatch object is fairly simple: </p><br><pre> <code class="python hljs">EOPatch.load(<span class="hljs-string"><span class="hljs-string">'path_to_eopatches/eopatch-0x6/'</span></span>)</code> </pre> <br><p>  As a result, you will receive an object of the following structure: </p><br><pre> <code class="python hljs">EOPatch( data: { BANDS: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>), dtype=float32) } mask: { CLM: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=uint8) IS_DATA: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=uint8) IS_VALID: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=bool) } mask_timeless: { LULC: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=uint8) VALID_COUNT: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=int64) } meta_info: { maxcc: <span class="hljs-number"><span class="hljs-number">0.8</span></span> service_type: <span class="hljs-string"><span class="hljs-string">'wcs'</span></span> size_x: <span class="hljs-string"><span class="hljs-string">'10m'</span></span> size_y: <span class="hljs-string"><span class="hljs-string">'10m'</span></span> time_difference: datetime.timedelta(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">86399</span></span>) time_interval: (datetime.datetime(<span class="hljs-number"><span class="hljs-number">2017</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>), datetime.datetime(<span class="hljs-number"><span class="hljs-number">2017</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">31</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>)) } bbox: BBox(((<span class="hljs-number"><span class="hljs-number">370230.5261411405</span></span>, <span class="hljs-number"><span class="hljs-number">5085303.344972428</span></span>), (<span class="hljs-number"><span class="hljs-number">380225.31836121203</span></span>, <span class="hljs-number"><span class="hljs-number">5095400.767924464</span></span>)), crs=EPSG:<span class="hljs-number"><span class="hljs-number">32633</span></span>) timestamp: [datetime.datetime(<span class="hljs-number"><span class="hljs-number">2017</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>), ..., datetime.datetime(<span class="hljs-number"><span class="hljs-number">2017</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">25</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">15</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>)], length=<span class="hljs-number"><span class="hljs-number">80</span></span> )</code> </pre> <br><p>  Access to different EOPatch attributes is as follows: </p><br><pre> <code class="python hljs">eopatch.timestamp eopatch.mask[<span class="hljs-string"><span class="hljs-string">'LULC'</span></span>] eopatch.data[<span class="hljs-string"><span class="hljs-string">'CLM'</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] eopatch.data[<span class="hljs-string"><span class="hljs-string">'BANDS'</span></span>][<span class="hljs-number"><span class="hljs-number">5</span></span>][..., [<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>]]</code> </pre> <br><h3 id="eoexecute-order-66">  EOExecute Order 66 </h3><br><p>  Great, the data is loading.  While we are waiting for the completion of this process, let's take a look at the capabilities of a class that has not yet been discussed in these articles - <a href="https://eo-learn.readthedocs.io/en/latest/eolearn.core.eoexecution.html"><code>EOExecutor</code></a> .  This module is engaged in the implementation and monitoring of the pipeline and allows you to easily use multithreading.  No more searches on Stack Overflow on how to parallelize the pipeline or how to make the progress bar work in this mode - we have already done everything for you! </p><br><p>  Additionally, it handles any errors that occur and can generate a brief summary of the progress.  The last is the most important moment in order to be sure of the repeatability of their results in the future, so that the user would not have to spend precious working time searching for the parameters that he used last Thursday at 9 am after a whole night of revelry (alcohol and programming do not mix worth it!).  This class also generates a nice dependency graph for a pipeline that you can show your boss! </p><br><p><img src="https://habrastorage.org/webt/_o/x7/0q/_ox70q41_uiebqp7opyqbeu0nx0.png"><br>  <em>Pipeline dependency graph generated by <code>eo-learn</code></em> </p><br><h3 id="eksperimenty-s-mashinnym-obucheniem">  Machine learning experiments </h3><br><p>  As promised, this article is mainly intended for studying different models with <code>eo-learn</code> using the data provided by us.  Below we have prepared two experiments where we study the effect of clouds and different resampling algorithms during temporal interpolation on the final result.  After all this, we will start working with convolutional networks (CNN), and compare the results of the two approaches - pixel-by-pixel analysis by the decision tree and in-depth training using convolutional neural networks. </p><br><p>  Unfortunately, it is impossible to give a clear answer about what decisions should be made during the experiments.  You can study the subject area more deeply and make assumptions to decide whether the game is worth the candle, but in the end, all the same, the work will be reduced to a trial and error method. </p><br><h3 id="igraem-s-oblakami">  Playing with clouds </h3><br><p>  Clouds are a huge pain in the EO world, especially when it comes to machine learning algorithms, where you want to define them and remove them from the data set for interpolation using missing values.  But how much benefit from this procedure?  Is it worth it?  Ru√üwurm and K√∂rner, in their article <a href="https://www.researchgate.net/publication/322975904_Multi-Temporal_Land_Cover_Classification_with_Sequential_Recurrent_Encoders">Multi-Temporal Land Cover Classification with Sequential Recurrent Encoders,</a> even showed that for deep learning the filtering of clouds may be absolutely unimportant, since the classifier itself is able to detect clouds and ignore them. </p><br><p><img src="https://habrastorage.org/webt/gz/c8/zs/gzc8zsp0nrdjtgbewqqysxulaiu.png"><br>  Activation of the input layer (top) and the modulation layer (bottom) in the sequence of images of a specific fragment for a neural network.  You may notice that this fragment of the network has learned how to create masks of cloudiness and filter the results obtained.  (Page 9 at <a href="https://www.researchgate.net/publication/322975904_Multi-Temporal_Land_Cover_Classification_with_Sequential_Recurrent_Encoders">https://www.researchgate.net/publication/322975904_Multi-Temporal_Land_Cover_Classification_with_Sequential_Recurrent_Encoders</a> ) </p><br><p>  Briefly recall the structure of the data filtering step (details in [previous article] ()).  After receiving the Sentinel-2 snapshots, we start filtering cloud snapshots.  All images in which the number of non-cloud pixels does not exceed 80% are subject to filtering out (the threshold values ‚Äã‚Äãmay differ for different zones of interest).  After that, to obtain pixel values ‚Äã‚Äãon arbitrary days, cloud masks are used to ignore such data. </p><br><p>  So there are four possible behaviors: </p><br><ol><li>  <strong>with</strong> filter images, <strong>given the</strong> cloud masks </li><li>  <strong>no</strong> filter snapshots, <strong>given the</strong> cloud mask </li><li>  <strong>with</strong> filter images, <strong>not including</strong> cloud masks </li><li>  <strong>no</strong> image filter, <strong>disregarding</strong> cloud masks </li></ol><br><p><img src="https://habrastorage.org/webt/rd/3i/ne/rd3ineypd8f0akhs41yve8mtgso.png"><br>  <em>Visual display of the stack of images from the satellite Sentinel-2.</em>  <em>Transparent pixels on the left imply missing pixels due to cloudiness.</em>  <em>The stack in the center shows the pixel values ‚Äã‚Äãafter filtering the images and their interpolation with the cloud mask (Case 4), and the stack on the right shows the result of the interpolation in the case without filtering the images and without cloud masks (1).</em>  <em>(Approx. Trans. - apparently, in the article a typo, and implied the opposite - in the center of the case 1, and the right - 4).</em> </p><br><p>  In the last article, we already performed a variation of case 1 and showed the results, so we will use them for comparison.  Preparing other conveyors and training the model sounds like an easy task ‚Äî you just have to make sure that we compare the correct values.  For this, it is enough to take the same set of pixels for training and validating the model. </p><br><p>  The results are shown in the table below.  You may notice that, in general, the effect of clouds on the model's performance is quite low!  This may be due to the fact that the reference card is of very good quality and the model is able to ignore most of the images.  In any case, this behavior cannot be guaranteed for any AOI, so do not rush to throw this step out of your models! </p><br><div class="scrollable-table"><table><thead><tr><th>  Model </th><th>  Accuracy [%] </th><th>  <a href="https://en.wikipedia.org/wiki/F1_score">F_1</a> [%] </th></tr></thead><tbody><tr><td>  Without filters, without mask </td><td>  92.8 </td><td>  92.6 </td></tr><tr><td>  Without filters, with mask </td><td>  94.2 </td><td>  93.9 </td></tr><tr><td>  With filter, without mask </td><td>  94.0 </td><td>  93.8 </td></tr><tr><td>  With filter, with mask </td><td>  94.4 </td><td>  94.1 </td></tr></tbody></table></div><br><h3 id="vliyanie-raznyh-podhodov-k-resemplingu">  The impact of different approaches to resampling </h3><br><p>  The choice of temporal resampling parameters is not obvious.  On the one hand, we need a detailed array of images that demonstrate well the details of the original images - I want to include the closest number of images to the original data.  On the other hand, we are limited by computational resources.  Reducing the resampling step doubles the number of frames after interpolation, and thus increases the number of features that are used during training.  Is such an improvement worth the resources spent on?  This is what we have to learn. </p><br><p>  For this experiment, we will use variation 1 from the previous step.  After interpolation, we do resampling with the following variations: </p><br><ol><li>  Uniform resampling with an interval of 16 days </li><li>  Uniform resampling with an interval of 8 days </li><li>  The choice of "best" dates, the number coincides with case 2. </li></ol><br><p>  Sample 3 is based on the largest total number of total dates for all EOPatch in the selected AOI <br><img src="https://habrastorage.org/webt/xg/qa/9w/xgqa9w17-oe4dbtxca22yejwhzo.png"><br>  <em>The graph shows the number of EOPatch fragments that contain data for each day of 2017 (blue).</em>  <em>The red lines show the optimal resampling dates, which are based on the Sentinel-2 snapshot dates for a given 2017 AOI.</em> </p><br><p>  Looking at the table below, you can see that the results are not very impressive, as in the past experience.  For cases 2 and 3, the amount of time spent grows about twice, but the difference with the initial approach is less than 1%.  Such improvements are too subtle for practical use, so it can be considered a 16-day interval suitable for the task. </p><br><div class="scrollable-table"><table><thead><tr><th>  Model </th><th>  Accuracy [%] </th><th>  F_1 [%] </th></tr></thead><tbody><tr><td>  Evenly every 16 days </td><td>  94.4 </td><td>  94.1 </td></tr><tr><td>  Evenly every 8 days </td><td>  94.5 </td><td>  94.3 </td></tr><tr><td>  Choosing the best dates </td><td>  94.6 </td><td>  94.4 </td></tr></tbody></table></div><br><p>  <em>The results of the overall accuracy and weighted F1 for different conveyors with a change in approach to resampling.</em> </p><br><h2 id="glubokoe-obuchenie-ispolzuem-svyortochnuyu-neyronnuyu-set-cnn">  Deep Learning: Using Convolutional Neural Network (CNN) </h2><br><p>  Deep learning has become a standard approach to many tasks, such as computer vision, natural language processing, and signal processing.  This is due to their ability to extract patterns from complex multidimensional input data.  Classical machine learning approaches (such as decision trees) have been used in many tasks related to temporal geodata.  On the other hand, convolutional networks were used to analyze the spatial correlation between adjacent images.  Basically, their use was limited to working with single shots. </p><br><p>  We wanted to study the architecture of deep learning models, and try to choose one that is able to analyze both spatial and temporal aspects of satellite data simultaneously. </p><br><p>  To do this, we used temporal full-convolutional networks (Temporal Fully-Convolutional Netvork, TFCN), or rather, the temporal extension to U-Net, implemented in TensorFlow.  More specifically, the architecture uses spatio-temporal correlations to improve the outcome.  An additional advantage is also the fact that the network structure allows a better representation of spatial relationships at different scales due to the process of encoding / decoding in U-net.  In the same way as in classical models, at the output we get a two-dimensional matrix of labels, which we will compare with the truth. </p><br><p><img src="https://habrastorage.org/webt/p0/jl/mg/p0jlmgxi9euwvodwonx4zrmezsw.png"></p><br><p>  We used the trained model to predict the labels on the test set, and the values ‚Äã‚Äãobtained were verified with the truth.  Overall, the accuracy was 84.4% and F1 was equal to 85.4%. </p><br><p><img src="https://habrastorage.org/webt/ol/z2/zj/olz2zjp3waghaak9hnirzcwa258.png"></p><br><p>  <em>Comparison of different predictions for our problem.</em>  <em>Visual image (left top), true reference map (top right), LightGBM model prediction (bottom left) and U-net prediction (bottom right)</em> </p><br><p>  These results show only the initial work on this prototype, which is not highly optimized for the current task.  Despite this, the results converge with some statisticians obtained in the area.  To unlock the potential of a neural network, an optimization of the architecture is required (feature set, network depth, number of bundles), as well as setting hyper parameters (learning rate, number of epochs, class weighting).  We expect to delve into this topic (ha-ha) even more, and plan to distribute our code when it is in an acceptable form. </p><br><h3 id="drugie-eksperimenty">  Other experiments </h3><br><p>  You can think of <em>many</em> ways to improve current results, but we can‚Äôt bust or try them all.  It is at this moment that you appear on the scene!  Show me what you can do with this dataset and help us improve results! </p><br><p>  For example, one of our colleagues in the near future will be engaged in the classification of cover, based on the temporal stack of <em>individual</em> images using convolutional networks.  The idea is that some surfaces, for example, artificial, can be distinguished without temporal features - quite spatial.  We will be happy to write a separate article when this work leads to results! </p><br><h3 id="ot-perevodchika">  From translator </h3><br><p>  Unfortunately, the next part of this series of articles did not come out, which means that the authors did not show examples of the source code with the construction of U-Net.  As an alternative, I can offer the following sources: </p><br><ol><li>  <em>U-Net: Convolutional Networks for Biomedical Image Segmentation - Olaf Ronneberger, Philipp Fischer, Thomas Brox</em> is one of the basic articles on U-Net architecture that does not involve temporal data. </li><li>  <a href="https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html">https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html</a> - The eo-learn documentation page, where the (possibly) more recent version of 1.2-part pipelines is located. </li><li>  <a href="https://github.com/divamgupta/image-segmentation-keras">https://github.com/divamgupta/image-segmentation-keras</a> - A repository with several networks implemented with keras.  I have some questions about the implementations (they are somewhat different from those described in the original articles), but in general the solutions are easily adapted for personal purposes and are quite working. </li></ol></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/453354/">https://habr.com/ru/post/453354/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../453346/index.html">Data Storage and Protection Technologies - Third Day at VMware EMPOWER 2019</a></li>
<li><a href="../453348/index.html">What's inside asyncio</a></li>
<li><a href="../45335/index.html">Garant-Park-Telecom (R01.RU) from November 19, 2008 registers domains in .COM, .ORG, .NET, .BIZ, .INFO, etc.</a></li>
<li><a href="../453350/index.html">Open broadcast of the main hall RIT ++ 2019</a></li>
<li><a href="../453352/index.html">How drones in Ghana deliver vital medicines</a></li>
<li><a href="../453356/index.html">Modern trends and recommendations for the development of large financial institutions</a></li>
<li><a href="../453362/index.html">Habra–°onf ‚Ññ1 - pobbrim behind the backend</a></li>
<li><a href="../453364/index.html">Rollout history that affected everything</a></li>
<li><a href="../453366/index.html">How to use commas in English: 15 rules and examples of errors</a></li>
<li><a href="../453368/index.html">Recover data from XtraDB tables without a structure file, using byte analysis of the ibd file</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>