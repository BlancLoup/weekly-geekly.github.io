<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Glusterfs + erasure coding: when you need a lot, cheap and reliable</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Glaster in Russia has very few people, and any experience is interesting. We have it big and industrial and, judging by the discussion in the last pos...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Glusterfs + erasure coding: when you need a lot, cheap and reliable</h1><div class="post__text post__text-html js-mediator-article">  Glaster in Russia has very few people, and any experience is interesting.  We have it big and industrial and, judging by the discussion in the <a href="https://habr.com/company/croccloudteam/blog/353666/">last post</a> , claimed.  I talked about the very beginning of the experience of transferring backups from Enterprise Storage to Glusterfs. <br><br>  This is not hardcore enough.  We did not stop and decided to collect something more serious.  Therefore, we will discuss such things as erasure coding, sharding, rebalancing and throttling, load testing, and so on. <br><br><img src="https://habrastorage.org/webt/8t/ol/2d/8tol2dsnki7fr_jfcvhxwldsxdk.jpeg">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  More theory of volumas / subevolumes </li><li>  hot spare </li><li>  heal / heal full / rebalance </li><li>  Conclusions after reboot 3 nodes (never do this) </li><li>  How does the record affect the subwoofer load at different speeds from different VMs and shard on / off </li><li>  rebalance after discing </li><li>  fast rebalance </li></ul><br><a name="habracut"></a><h3>  What they wanted </h3><br>  <b>The task is simple:</b> to collect a cheap but reliable stack.  Cheap as possible, reliable - so that it is not scary to keep our own files on sale.  Until.  Then, after long tests and backups to another storage system - also client. <br><br>  <b>Application (sequential IO)</b> : <br><br>  - Backups <br>  - Test Infrastructure <br>  - Test storage for heavy media files. <br>  We are here. <br>  - Combat file dumping and serious test infrastructures <br>  - Storage for important data. <br><br>  Like last time, the main requirement is the speed of the network between instances of Glaster.  10G at first is normal. <br><br><h3>  Theory: what is dispersed volume? </h3><br>  Dispersed volume is based on erasure coding (EC) technology, which provides quite effective protection against disk or server failures.  It's like RAID 5 or 6, but not quite.  It stores the encoded file fragment for each brik in such a way that only a subset of the fragments stored on the remaining bricks is required to restore the file.  The number of bricks that may not be available without losing access to the data is configured by the administrator during volume creation. <br><br><img src="https://habrastorage.org/webt/rt/br/t1/rtbrt12s-0oyc9avxlsp32qzsus.png"><br><br><h3>  What is a subvolume? </h3><br>  The essence of a subvolume in GlusterFS terminology is manifested along with distributed volums.  In distributed-disperced erasure, coding will work just within the framework of a subevolume.  And in the case of, for example, with distributed-replicated data will be replicated within the framework of the subevolume. <br>  Each of them is distributed to different servers, which allows them to loose or withdraw to sync.  In the figure, servers (physical) are marked in green, dashed lines are sub-volums.  Each of them is presented as a disk (volume) to an application server: <br><br><img src="https://habrastorage.org/webt/w-/fp/dj/w-fpdjqguwigusvhtprq_6su3z8.png"><br><br>  It was decided that the distributed-dispersed 4 + 2 configuration on 6 nodes looks quite reliable, we can lose 2 of any servers or 2 disks within each subvolume, while continuing to have access to the data. <br><br>  We had 6 old DELL PowerEdge R510 with 12 disk slots and 48x2TB 3.5 SATA disks.  In principle, if there are servers with 12 disk slots, and having disks on the market up to 12TB, we can collect a storage of up to 576TB of usable space.  But do not forget that even though the maximum HDD sizes continue to grow from year to year, their performance stays in place and a 10-12TB rebuild disk can take you a week. <br><br><img src="https://habrastorage.org/webt/xo/ud/6f/xoud6fl7sdtknj7vrbn_5fgslpu.png"><br><br>  <b>Volum creation:</b> <br>  A detailed description of how to prepare briks, you can read in my <a href="https://habr.com/company/croccloudteam/blog/353666/">previous post</a> <br><br><pre><code class="bash hljs">gluster volume create freezer disperse-data 4 redundancy 2 transport tcp \ $(<span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> {0..7} ; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> {sl051s,sl052s,sl053s,sl064s,sl075s,sl078s}:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick<span class="hljs-variable"><span class="hljs-variable">$i</span></span>/freezer ; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span>)</code> </pre> <br>  We create, but do not rush to run and mount, as we still have to apply several important parameters. <br><br>  <b>What we got:</b> <br><br><img src="https://habrastorage.org/webt/8j/il/e9/8jile9swj-qws3hgdjo3gtht3oo.png"><br><br>  Everything looks quite normal, but there is one nuance. <br><br>  <b>It consists in writing to the bricks of such a volum:</b> <br>  Files are placed alternately in subsubjects, and not evenly smeared on them, therefore, sooner or later we will be rested in its size, and not in the size of the whole volume.  The maximum size of the file that we can put in this storage is = the useful size of the subspace minus the space already occupied on it.  In my case, this is &lt;8 TB. <br><br>  <b>What to do?</b>  <b>How to be?</b> <br>  This problem is solved by sharding or stripe wolume, but, as practice has shown, the stripe works very badly. <br><br>  Therefore, we will try sharding. <br><br>  <b>What is sharding, in detail</b> <a href="http://blog.gluster.org/introducing-shard-translator/">here</a> . <br><br>  <b>What is sharding, briefly</b> : <br>  Each file that you put in the volume will be divided into parts (shards), which are relatively evenly decomposed in subs.  The shard size is specified by the administrator, the default value is 4 MB. <br><br>  <b>We enable sharding after creating a volume, but before it was launched</b> : <br><br><pre> <code class="bash hljs">gluster volume <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> freezer features.shard on</code> </pre> <br>  <b>We set the size of the shard (which one is optimal? Guys from oVirt recommend 512MB)</b> : <br><br><pre> <code class="bash hljs">gluster volume <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> freezer features.shard-block-size 512MB</code> </pre> <br>  Empirically, it turns out that the actual size of the shard in a bricket using dispersed voluum 4 + 2 is equal to shard-block-size / 4, in our case 512M / 4 = 128M. <br><br>  Each shard according to the logic of erasure coding is decomposed into bricks in the framework of a sub-curve like this: 4 * 128M + 2 * 128M <br><br>  <b>To draw the failure cases that the gluster of this configuration is going through:</b> <br>  In this configuration, we can survive the fall of 2 nodes or 2 any drives within one sub-rev. <br><br>  For the tests, we decided to slip the resulting storage under our cloud and run fio from the virtual machines. <br><br>  We turn on sequential recording with 15 VMs and do the following. <br><br>  <b>Rebut of the 1st node:</b> <br>  17:09 <br>  It looks uncritical (~ 5 seconds of inaccessibility by the ping.timeout parameter). <br><br>  17:19 <br>  Launched heal full. <br>  The number of heal entries is only growing, probably due to the large level of writing to the cluster. <br><br>  17:32 <br>  It was decided to turn off recording from VM. <br>  The number of heal entries has begun to decrease. <br><br>  17:50 <br>  heal is done. <br><br>  <b>Rebut 2 nodes:</b> <br><br>  <i>The same results are observed as with the 1st node.</i> <br><br>  <b>Rebut 3 nodes:</b> <br>  <i>The mount point was thrown by the Transport endpoint is not connected, the VMs received an ioerror.</i> <i><br></i>  <i>After switching on the nodes, Glaster recovered himself, without intervention from our side, and the process of treatment began.</i> <br><br>  But 4 out of 15 VMs could not get up.  I saw errors on the hypervisor: <br><br><pre> <code class="bash hljs">2018.04.27 13:21:32.719 ( volumes.py:0029): I: Attaching volume vol-BA3A1BE1 (/GLU/volumes/33/33e3bc8c-b53e-4230-b9be-b120079c0ce1) with attach <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> generic... 2018.04.27 13:21:32.721 ( qmp.py:0166): D: Querying QEMU: __com.redhat_drive_add({<span class="hljs-string"><span class="hljs-string">'file'</span></span>: u<span class="hljs-string"><span class="hljs-string">'/GLU/volumes/33/33e3bc8c-b53e-4230-b9be-b120079c0ce1'</span></span>, <span class="hljs-string"><span class="hljs-string">'iops_rd'</span></span>: 400, <span class="hljs-string"><span class="hljs-string">'media'</span></span>: <span class="hljs-string"><span class="hljs-string">'disk'</span></span>, <span class="hljs-string"><span class="hljs-string">'format'</span></span>: <span class="hljs-string"><span class="hljs-string">'qcow2'</span></span>, <span class="hljs-string"><span class="hljs-string">'cache'</span></span>: <span class="hljs-string"><span class="hljs-string">'none'</span></span>, <span class="hljs-string"><span class="hljs-string">'detect-zeroes'</span></span>: <span class="hljs-string"><span class="hljs-string">'unmap'</span></span>, <span class="hljs-string"><span class="hljs-string">'id'</span></span>: <span class="hljs-string"><span class="hljs-string">'qdev_1k7EzY85TIWm6-gTBorE3Q'</span></span>, <span class="hljs-string"><span class="hljs-string">'iops_wr'</span></span>: 400, <span class="hljs-string"><span class="hljs-string">'discard'</span></span>: <span class="hljs-string"><span class="hljs-string">'unmap'</span></span>})... 2018.04.27 13:21:32.784 ( instance.py:0298): E: Failed to attach volume vol-BA3A1BE1 to the instance: Device <span class="hljs-string"><span class="hljs-string">'qdev_1k7EzY85TIWm6-gTBorE3Q'</span></span> could not be initialized Traceback (most recent call last): File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/ic/instance.py"</span></span>, line 292, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> emulation_started c2.qemu.volumes.attach(controller.qemu(), device) File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/c2/qemu/volumes.py"</span></span>, line 36, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> attach c2.qemu.query(qemu, drive_meth, drive_args) File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/c2/qemu/_init_.py"</span></span>, line 247, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> query <span class="hljs-built_in"><span class="hljs-built_in">return</span></span> c2.qemu.qmp.query(qemu.pending_messages, qemu.qmp_socket, <span class="hljs-built_in"><span class="hljs-built_in">command</span></span>, args, suppress_logging) File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/c2/qemu/qmp.py"</span></span>, line 194, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> query message[<span class="hljs-string"><span class="hljs-string">"error"</span></span>].get(<span class="hljs-string"><span class="hljs-string">"desc"</span></span>, <span class="hljs-string"><span class="hljs-string">"Unknown error"</span></span>) QmpError: Device <span class="hljs-string"><span class="hljs-string">'qdev_1k7EzY85TIWm6-gTBorE3Q'</span></span> could not be initialized qemu-img: Could not open <span class="hljs-string"><span class="hljs-string">'/GLU/volumes/33/33e3bc8c-b53e-4230-b9be-b120079c0ce1'</span></span>: Could not <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> image <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> determining its format: Input/output error</code> </pre><br>  <b>Hardly repay 3 nodes with sharding turned off</b> <br><br><pre> <code class="bash hljs">Transport endpoint is not connected (107) /GLU/volumes/e0/e0bf9a42-8915-48f7-b509-2f6dd3f17549: ERROR: cannot <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> (Input/output error)</code> </pre> <br>  We also lose data, it can not be restored. <br><br>  <b>Gently extinguish 3 nodes with sharding, will there be data corruption?</b> <br>  Yes, but significantly less (coincidence?), Lost 3 disks out of 30. <br><br>  <b>Findings:</b> <br><br><ol><li>  Heal of these files hangs endlessly, rebalance does not help.  We come to the conclusion that the files that were actively recorded when the 3rd node was turned off were lost forever. </li><li>  Never reboot more than 2 nodes in a 4 + 2 configuration in production! </li><li>  How not to lose data if you really want to reboot 3 + nodes?  P stop writing to the mount point and / or stop volume. </li><li>  Replacing a node or brika should be done as soon as possible.  For this, it is highly desirable to have, for example, 1-2 a la hot-spare bricks in each node for quick replacement.  And one more spare node with bricks in case of a node's drop. </li></ol><br><img src="https://habrastorage.org/webt/-m/rj/ti/-mrjtikde2imxdydeka4w-hvjjk.png"><br><br>  It is also very important to test disk replacement cases. <br><br>  <b>Departures of briks (disks):</b> <b><br></b>  <b>17:20</b> <br>  Knock brik: <br><br><pre> <code class="bash hljs">/dev/sdh 1.9T 598G 1.3T 33% /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick6</code> </pre> <br>  <b>17:22</b> <br><pre> <code class="bash hljs">gluster volume replace-brick freezer sl051s:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick_spare_1/freezer sl051s:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick2/freezer commit force</code> </pre> <br>  One can see such a drawdown at the moment of brik replacement (recording from 1 source): <br><br><img src="https://habrastorage.org/webt/jk/96/ns/jk96ns2kzhy0radczfpxlfpodso.png"><br><br>  The replacement process is quite long, with a small write to cluster and default settings of 1 TB is treated for about a day. <br><br>  <b>Adjustable treatment options:</b> <br><br><pre> <code class="bash hljs">gluster volume <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> cluster.background-self-heal-count 20 <span class="hljs-comment"><span class="hljs-comment"># Default Value: 8 # Description: This specifies the number of per client self-heal jobs that can perform parallel heals in the background. gluster volume set cluster.heal-timeout 500 # Default Value: 600 # Description: time interval for checking the need to self-heal in self-heal-daemon gluster volume set cluster.self-heal-window-size 2 # Default Value: 1 # Description: Maximum number blocks per file for which self-heal process would be applied simultaneously. gluster volume set cluster.data-self-heal-algorithm diff # Default Value: (null) # Description: Select between "full", "diff". The "full" algorithm copies the entire file from source to # sink. The "diff" algorithm copies to sink only those blocks whose checksums don't match with those of # source. If no option is configured the option is chosen dynamically as follows: If the file does not exist # on one of the sinks or empty file exists or if the source file size is about the same as page size the # entire file will be read and written ie "full" algo, otherwise "diff" algo is chosen. gluster volume set cluster.self-heal-readdir-size 2KB # Default Value: 1KB # Description: readdirp size for performing entry self-heal</span></span></code> </pre> <br>  <i>Option: disperse.background-heals</i> <i><br></i>  <i>Default Value: 8</i> <i><br></i>  <i>Description: this heals can be used</i> <i><br><br></i>  <i>Option: disperse.heal-wait-qlength</i> <i><br></i>  <i>Default Value: 128</i> <i><br></i>  <i>Description: This is what you can wait for.</i> <i><br><br></i>  <i>Option: disperse.shd-max-threads</i> <i><br></i>  <i>Default Value: 1</i> <i><br></i>  <i>Description: Maximum number of parallel heals SHD</i>  <i>If you don‚Äôt have any storage space, it‚Äôs possible to keep your bricks.</i> <i><br><br></i>  <i>Option: disperse.shd-wait-qlength</i> <i><br></i>  <i>Default Value: 1024</i> <i><br></i>  <i>Description: This is what you can wait for</i> <i><br><br></i>  <i>Option: disperse.cpu-extensions</i> <i><br></i>  <i>Default Value: auto</i> <i><br></i>  <i>Description: accelerate the galois field computations.</i> <i><br><br></i>  <i>Option: disperse.self-heal-window-size</i> <i><br></i>  <i>Default Value: 1</i> <i><br></i>  <i>Description: Maximum number of blocks (128KB).</i> <br><br>  Stand out: <br><br><pre> <code class="bash hljs">disperse.shd-max-threads: 6 disperse.self-heal-window-size: 4 cluster.self-heal-readdir-size: 2KB cluster.data-self-heal-algorithm: diff cluster.self-heal-window-size: 2 cluster.heal-timeout: 500 cluster.background-self-heal-count: 20 cluster.disperse-self-heal-daemon: <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> disperse.background-heals: 18</code> </pre> <br>  With the new parameters, 1 TB of data was consumed in 8 hours (3 times faster!) <br><br>  <b>The unpleasant moment is that the result is a larger brik than was</b> <br><br>  <b>was:</b> <pre> <code class="bash hljs">Filesystem Size Used Avail Use% Mounted on /dev/sdd 1.9T 645G 1.2T 35% /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick2</code> </pre> <br>  <b>became:</b> <pre> <code class="bash hljs">Filesystem Size Used Avail Use% Mounted on /dev/sdj 1.9T 1019G 843G 55% /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/hot_spare_brick_0</code> </pre> <br>  Need to understand.  Probably, it's about inflating thin disks.  With the subsequent replacement of the increased bricks, the size remained the same. <br><br>  <b>Rebalancing:</b> <br>  <i>After expanding or shrinking (using the add-brick and remove-brick commands respectively), you need to adjust the data among the servers.</i>  <i>In all non-replicated volume, all bricks should be replaced.</i>  <i>In a replicated volume, it should not be up.</i> <br><br>  <b>Shaping rebalancing:</b> <br><br>  <i>Option: cluster.rebal-throttle</i> <i><br></i>  <i>Default Value: normal</i> <i><br></i>  <i>Description: Sets the maximum number of parallel file migrations during the rebalance operation.</i>  <i>The default value is the maximum of [($ (processing units) - 4) / 2), 2] files to b</i> <i><br></i>  <i>e migrated at a time.</i>  <i>It will be possible to record the amount of (($ (processing units) - 4) / 2), 4]</i> <br><br>  <i>Option: cluster.lock-migration</i> <i><br></i>  <i>Default Value: off</i> <i><br></i>  <i>Description: If you want a re-locks associated with a file during rebalance</i> <br><br>  <i>Option: cluster.weighted-rebalance</i> <i><br></i>  <i>Default Value: on</i> <i><br></i>  <i>Description: When enabled, files will be selected.</i>  <i>Otherwise, all bricks will have the same probability.</i> <br><br>  <b>Comparing the record, and then reading the same fio parameters (more detailed results of the performance tests - in lichku):</b> <br><br><pre> <code class="bash hljs">fio --fallocate=keep --ioengine=libaio --direct=1 --buffered=0 --iodepth=1 --bs=64k --name=<span class="hljs-built_in"><span class="hljs-built_in">test</span></span> --rw=write/<span class="hljs-built_in"><span class="hljs-built_in">read</span></span> --filename=/dev/vdb --runtime=6000</code> </pre><br><img src="https://habrastorage.org/webt/fw/up/j0/fwupj0gj9m6vn25bepslaaox01e.png"><br><br><img src="https://habrastorage.org/webt/xi/yf/pd/xiyfpdsecqbfc52fudoz4nwklty.png"><br><br><img src="https://habrastorage.org/webt/nh/xp/es/nhxpestbf-gfjkcwogumbbqabc4.jpeg"><br><br>  <b>If interested, compare the speed of rsync to traffic to Gluster nodes:</b> <br><br><img src="https://habrastorage.org/webt/6i/cz/ox/6iczoxword1qaauuhkwm3vfkk-q.png"><br><br><img src="https://habrastorage.org/webt/42/ka/eq/42kaeqkdbcuzhq8rwdrqybgkc5u.png"><br><br>  <i>It is seen that approximately 170 MB / s / traffic to 110 MB / s / payload.</i>  <i>It turns out that this is 33% of additional traffic, as well as 1/3 of Erasure Coding redundancy.</i> <br><br>  <b>The memory consumption on the server side with the load and almost without it does not change:</b> <br><br><img src="https://habrastorage.org/webt/nz/sz/zc/nzszzcu0eqrck4gpmhvazhm8x7i.png"><br><img src="https://habrastorage.org/webt/nz/sz/zc/nzszzcu0eqrck4gpmhvazhm8x7i.png"><br><br>  <b>The load on the cluster hosts at maximum load on the Volyum:</b> <br><br><img src="https://habrastorage.org/webt/vb/u3/3c/vbu33cgi-rmgjn7c2w1guz5ps3c.png"></div><p>Source: <a href="https://habr.com/ru/post/417475/">https://habr.com/ru/post/417475/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../417463/index.html">Unexpected meeting. Chapter 17</a></li>
<li><a href="../417465/index.html">Overview of text entry binding methods</a></li>
<li><a href="../417469/index.html">Five selfish reasons to work reproducibly</a></li>
<li><a href="../417471/index.html">Simple Solder MK936 SMD. DIY soldering station on SMD components</a></li>
<li><a href="../417473/index.html">Secure storage with DRBD9 and Proxmox (Part 1: NFS)</a></li>
<li><a href="../417477/index.html">Hot workplace (Hot desking)</a></li>
<li><a href="../417479/index.html">Accelerate do-it-yourself string concatenation</a></li>
<li><a href="../417481/index.html">About generators in javascript es6 and why it‚Äôs not necessary to study them</a></li>
<li><a href="../417483/index.html">Comparing JS frameworks: React, Vue and Hyperapp</a></li>
<li><a href="../417485/index.html">[bookmark] Cheat system administrator for Linux network tools</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>