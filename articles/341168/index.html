<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Virtuozzo Storage: Actual Operating Experience, Optimization and Problem Solving Tips</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This article focuses on the actual experience of operating clusters based on Virtuozzo Storage. 
 During the year of active implementation and use of ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Virtuozzo Storage: Actual Operating Experience, Optimization and Problem Solving Tips</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/webt/jj/ez/tl/jjeztljhrnattlhud90753kdbck.png"></div><br>  This article focuses on the actual experience of operating clusters based on Virtuozzo Storage. <br>  During the year of active implementation and use of the platform on our hosting servers, as well as in creating clusters for our clients, we have gathered quite a lot of tips, comments and recommendations.  If you are thinking about the implementation of this platform, you can take into account our experience in designing your cluster. <br><a name="habracut"></a><br><hr><br><div class="spoiler">  <b class="spoiler_title">Our other publications</b> <div class="spoiler_text"><ul><li>  <a href="https://habrahabr.ru/company/acronis/blog/198354/">Zabbix 2.2 riding on nginx + php-fpm and mariadb</a> </li><li>  <a href="https://habrahabr.ru/company/acronis/blog/198448/">HAPRoxy for Percona or Galera on CentOS.</a>  <a href="https://habrahabr.ru/company/acronis/blog/198448/">Its configuration and monitoring in Zabbix</a> </li><li>  <a href="https://habrahabr.ru/company/acronis/blog/198934/">‚ÄúPerfect‚Äù www-cluster.</a>  <a href="https://habrahabr.ru/company/acronis/blog/198934/">Part 1. Frontend: NGINX + Keepalived (vrrp) on CentOS</a> </li><li>  <a href="https://habrahabr.ru/company/acronis/blog/204190/">"Perfect" cluster.</a>  <a href="https://habrahabr.ru/company/acronis/blog/204190/">Part 2.1: Virtual hetzner cluster</a> </li><li>  <a href="https://habrahabr.ru/company/acronis/blog/209934/">"Perfect" cluster.</a>  <a href="https://habrahabr.ru/company/acronis/blog/209934/">Part 2.2: Highly available and scalable web server, the best technologies to guard your business</a> </li><li>  <a href="https://habrahabr.ru/post/253869/">"Perfect" cluster.</a>  <a href="https://habrahabr.ru/post/253869/">Part 3.1 Implementing MySQL Multi-Master Cluster</a> </li><li>  <a href="https://habrahabr.ru/post/264487/">Acceleration and optimization of PHP-site.</a>  <a href="https://habrahabr.ru/post/264487/">What technologies should be chosen when setting up a server for PHP</a> </li><li>  <a href="https://habrahabr.ru/post/264775/">Comparison of Drupal code execution speed for PHP 5.3-5.6 and 7.0.</a>  <a href="https://habrahabr.ru/post/264775/">"Battle of code optimizers" apc vs xcache vs opcache</a> </li><li>  <a href="https://habrahabr.ru/post/319210/">Bitrix Start Performance on Proxmox and Virtuozzo 7 &amp; Virtuozzo Storage</a> </li></ul></div></div><br><h3>  To be or not to be?  Pros and Cons Virtuozzo </h3><br>  In short, be.  Here is a list of the pros and cons that we found when using Virtuozzo: the <b>pros</b> : <br><br><ul><li>  Cluster is convenient.  You can turn off the virtual machine on one server and immediately turn it on on another, no copying of data or time for transfer.  If the server fell, then you can immediately lift all the virtual machines from the fallen server on one of the free ones. </li><li>  The concept of free space on a particular server no longer exists.  All servers can share space, limited only by the number of disks in the cluster and your license. </li><li>  Technical support exists and responds very promptly by phone and email. </li><li>  There is a direct and only dealer in the Russian Federation, so you can purchase licenses for a legal entity in rubles. </li><li>  Virtual machines based on vz work quite productively and <a href="https://habrahabr.ru/post/319210/">even much more efficiently than, for example, on Proxmox</a> </li><li>  There is a convenient monitoring tool that allows you to get all the necessary information about the work of the cluster visually and informatively.  You can freely parse the data for Zabbix, Monin or Nagios. </li><li>  Thanks to readykernel, most of the 0-day vulnerabilities in the kernel are eliminated day-to-day and without rebooting the hypervisor. </li><li>  The pfcache technology saves memory on the hypervisor (see below for more on this). </li><li>  Anyway, Virtuozzo Storage is a network file system, and its operation is highly dependent on network performance.  But in Virtuozzo, you can use a local ssd data cache for quick reading and a local ssd log for writing.  Plus, Virtuozzo Storage is trying to transfer the data associated with a particular virtual machine to the hypervisor, where this virtual machine is running. </li><li>  You can have several types of disks in a cluster (ssd, hdd + ssd-cache and hdd), while you can freely move the virtual machines between them.  In the event that the fast drives run out, your machines will automatically start using disks of a different type until a place appears on the fast ones. </li><li>  It is strictly contraindicated to keep disks in a local raid.  The main advantage for us is that copies of data are not stored on one server, but on several at once, which is much more reliable than Raid. </li></ul><br>  <b>Unfortunately, there are also disadvantages (:</b> <br><br><ul><li>  This is quite expensive (compared to ceph + kvm), especially on large projects and data volumes. </li><li>  Once or twice a month, one of the hypervisors may hang for no apparent reason. </li><li>  To physically free up space in the cluster from remote data inside virtual machines, it is necessary to run a very heavy pcompact procedure (you can read about it below). </li><li>  Support is trying very hard, but it cannot solve really complex issues.  Usually such problems are solved in the next update.  In the meantime, you will be offered to upgrade and reboot (in the best traditions of The IT Crowd). </li><li>  There is a live migration, but if the container uses half-open sockets, it falls off with an error (that is, it does not exist). </li><li>  There is an automatic migration and launch of virtual machines from a fallen hypervisor, but the minimum number of servers in a cluster should be 5, the replication level is 3: 2 (that is, 3 copies of the same data block in the cluster).  Automation does not save, if the virtual machine has the status of running, but hung. </li><li>  Virtuozzo Storage components are completely unable to work normally in conditions of a small amount of completely free RAM.  The banal Linux disk cache will lead to the crash of specific CS (daemons working with Virtuozzo Storage disks) and even to the fall of virtual machines or the entire hypervisor. </li><li>  The Virtuozzo Automator control panel is not suitable for real use, rather, to view resource and load statistics, and other alternative control panels are not found. </li><li>  Api to automate typical operations not detected.  I had to write my own, but not everything went smoothly with him.  In fact, we perform typical operations through the bash console, but, as a result, some operations may not be performed for no apparent reason.  For example, we have automated the procedure for migrating a virtual machine from the hypervisor to the hypervisor through a series of simple actions: shutting down the container on the old hypervisor, migrating and actually launching on the final hypervisor.  Sometimes the startup procedure does not work, because it may happen too slowly, and our self-written api just falls off.  Plus, it is not clear how to be if several tasks are run simultaneously on a single hypervisor. </li></ul><br>  So, with pluses and minuses more or less figured out, now the real <b>recommendations</b> : 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  Installation </h3><br>  If you want to have at least some ability to manage installation settings, or you need to add a new node to the old cluster created six months ago (for example, when booting from disk), select the second item - the cli setting. <br><br>  In this case, however, you lose the opportunity to get a beautiful cluster control panel. <br><br>  If you want to add disks to the cluster during the installation phase (but do not do it better), then make sure that they are fully formatted, otherwise the installer will draw you beautiful errors in the middle of the installation. <br><br>  For swap and pfcache systems, fast ssd disks are needed.  Do not miss this moment, otherwise it will be difficult to redo it.  If everything is less clear with the swap, this is what pfcache is and what it is with, it is not immediately clear.  In fact, in all virtual machines, folders are scanned at a specific path, all libraries and executable files are cached, and the hash log is placed in a special local virtual disk, which, in turn, is stored on the system partition.  Further, when launching any binary file inside a virtual machine, the hash log (which is on the disk) is analyzed, and if such a binary is already running, the new one does not start, but simply creates a link in memory to the existing one.  Now imagine what happens if the log disk is not ssd.  And if there still will be stored swap?  ) <br><br>  <b>Here's how to transfer the log:</b> <br><br><pre><code class="hljs pgsql">service pfcached stop ploop umount /vz/pfcache.hdd/DiskDescriptor.xml mv /vz/pfcache.hdd /mnt/ssd2/ sed <span class="hljs-string"><span class="hljs-string">'/^PFCACHE_IMAGE=/ s~.*~PFCACHE_IMAGE="/mnt/ssd2/pfcache.hdd"~'</span></span> -i /etc/vz/pfcache.conf ploop mount -m /vz/pfcache /mnt/ssd2/pfcache.hdd/DiskDescriptor.xml service pfcached <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span></code> </pre> <br>  There is a lot of information on the Internet, how to connect a file swap on a fast disk and disable the old slow swap, we will not duplicate it.  But be sure to consider all the recommendations on the disks and the network, which we analyze below.  And also plan that the cluster servers should be at least three. <br><br>  <b>Be sure to turn on the automatic time synchronization during the installation phase</b> ; if not, it‚Äôs not hard to fix: <br><br><pre> <code class="bash hljs">yum install ntp -y systemctl stop ntpd ntpdate 165.193.126.229 0.ru.pool.ntp.org 1.ru.pool.ntp.org 2.ru.pool.ntp.org 3.ru.pool.ntp.org systemctl start ntpd systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> ntpd</code> </pre> <br>  After installation, <b>we immediately make a number of changes to the standard sysctl kernel settings</b> , but I do not recommend doing this for no apparent reason.  Better to consult with support. <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"fs.file-max=99999999"</span></span> &gt;&gt; /etc/sysctl.d/99-files.conf <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"kernel.sysrq=1"</span></span> &gt;&gt; /etc/sysctl.d/99-reboot.conf <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"kernel.panic=1"</span></span> &gt;&gt; /etc/sysctl.d/99-reboot.conf sysctl -w fs.file-max=99999999 sysctl -w kernel.panic=1 sysctl -w kernel.sysrq=1</code> </pre><br>  <b>File limit settings, bash prompts, screen options</b> <br><br><pre> <code class="bash hljs">cat &gt; /etc/security/limits.d/nofile.conf &lt;&lt; EOL root soft nofile 1048576 root hard nofile 1048576 * soft nofile 1048576 * hard nofile 1048576 * hard core 0 EOL cat &gt; /etc/profile.d/bash.sh &lt;&lt; EOL PS1=<span class="hljs-string"><span class="hljs-string">'\[\033[01;31m\]\u\[\033[01;33m\]@\[\033[01;36m\]\h \[\033[01;33m\]\w \[\033[01;35m\]\$ \[\033[00m\] '</span></span> EOL sed -i <span class="hljs-string"><span class="hljs-string">'s/Defaults\ requiretty/#Defaults\ requiretty/g'</span></span> /etc/sudoers</code> </pre> <br>  <b>I highly recommend having a local data cache for virtual machines</b> , it should be on the ssd disk.  This will allow your virtual machines not to receive data via the network, but to take them locally from the cache, which will speed up the work of the cluster and virtual machines many times. <br><br>  To do this, edit / etc / fstab, where livelinux is the cluster name, / vz / client_cache, cachesize = 50000 0 0 is the log path to the ssd with the size of MB. <br><br><pre> <code class="bash hljs">vstorage://livelinux /vstorage/livelinux fuse.vstorage _netdev,cache=/vz/client_cache,cachesize=50000 0 0</code> </pre><br><h3>  Network </h3><br>  You need a fast network.  You should not raise a cluster with a network of 1 Gbps, because it is immediately shot in the leg.  When there are a lot of machines and data, you will have serious problems.  The file system is networked, work with the disk occurs through the network, balancing and restoring replication too.  If you score 100% of the net, avalanche drops and brakes will begin.  In no case do not need to run a cluster on a 1 GB network, unless of course it is a test cluster. <br><br>  Not very good, but a possible alternative to 10 GB of optics is to bond 2 gigabit network cards.  This can give a peak of up to 1.5 GB, and in very good and clear weather even 2 GB. <br>  At the same time, the type of bonding is preferably 802.3ad.  He, in turn, requires configuration on the side of your network equipment.  Also, make sure that you choose xmit_hash_policy = layer3 + 4, since this is the most productive option (according to Virtuozzo). <br><br>  Example from <b>/ etc / sysconfig / network-scripts / ifcfg-bond0</b> <br><br><pre> <code class="bash hljs">BONDING_OPTS=<span class="hljs-string"><span class="hljs-string">"miimon=1 updelay=0 downdelay=0 mode=802.3ad xmit_hash_policy=layer3+4"</span></span></code> </pre> <br>  In this case, note that bonding will work only when the server is working simultaneously with two other servers.  Those.  direct exchange between only two servers will occur at a speed of 1 GB, but when there are already three servers, you will be able to exchange data at a speed of about 1.5 GB. <br><br>  When the first node of the cluster is installed, the network should already work, and Virtuozzo should be able to send broadcast requests.  In other words, if you don't have uplink in the network interface, during the final stage of the installation, Virtuozzo will silently stop and hang for no reason.  In fact, she simply tries to send a request to the network via broadcast, and if she doesn‚Äôt get it, she stops the installation. <br><br>  And, of course, it is highly desirable for you to have separate network interfaces and networks for virtual machines and for cluster operation. <br><br>  After installing Virtuozzo, <b>you need to create a network bridge and add a real network interface there</b> .  This is necessary for the operation of your virtual machines with a shared network. <br><br><pre> <code class="bash hljs">prlsrvctl net add network1 prlsrvctl net <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> network1 -t bridged --ifname enp1s0d1 <span class="hljs-comment"><span class="hljs-comment">#enp1s0d1   </span></span></code> </pre><br><h3>  Discs </h3><br>  <b>Do not add disks to Storage during the installation of the hypervisor</b> .  Rather, one still have to, but after installation it will need to be removed and added again.  The fact is that by default, Virtuozzo included all the disks in the tier0 group.  This is the cluster's default disk group, and they are also the slowest.  To divide the types of disks in a cluster into ssd and not ssd, you will have to unload all the disks that have already been added, and add them again to the desired tier.  Changing a tier disk that is already in a cluster is not possible.  Even if today you do not plan to have such a division, just do it now.  It will not be worse, but in the future you will get rid of many unnecessary difficulties. <br><br>  Another problem is that when removing disks, according to the instructions, tails remain in the form of services that try to start CS for a disk that is no longer there.  And if you deleted and added the same disk 2 times?  Then, at any restart of the service, the CS-disk will try to start twice.  I give a simple recipe for how to properly unload and remove CS without problems: <br><br><pre> <code class="bash hljs">cs=1071 <span class="hljs-comment"><span class="hljs-comment">#  -      cluster=livelinux #   vstorage -c $cluster rm-cs --wait $cs systemctl stop vstorage-csd.$cluster.$cs.service systemctl disable vstorage-csd.$cluster.$cs.service systemctl reset-failed vstorage-csd.$cluster.*.service systemctl | grep vstorage-csd</span></span></code> </pre> <br>  <b>To add a disc to the correct shooting range, you definitely need to prepare a completely empty disc with the command:</b> <br><br><pre> <code class="bash hljs">/usr/libexec/vstorage/prepare_vstorage_drive /dev/sdc --noboot</code> </pre> <br><pre> <code class="hljs pgsql">blkid # <span class="hljs-type"><span class="hljs-type">UUID</span></span>  mkdir /vstorage/livelinux-cs4 #     #     /etc/fstab <span class="hljs-type"><span class="hljs-type">UUID</span></span>="3de71ff9-724f-483a-8984-3b78fdb3b327" /vstorage/livelinux-cs4 ext4 defaults,lazytime <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> #   mount -<span class="hljs-keyword"><span class="hljs-keyword">all</span></span> #    livelinux ( )   <span class="hljs-number"><span class="hljs-number">2</span></span> (  ) vstorage -c livelinux make-cs -r /vstorage/livelinux-cs4/data -t <span class="hljs-number"><span class="hljs-number">2</span></span> #  hdd-   ssd-,     # -t <span class="hljs-number"><span class="hljs-number">1</span></span>   hdd+ssd , /vz/livelinux-cs6-sata-journal -     ssd, #-s <span class="hljs-number"><span class="hljs-number">30240</span></span>    . vstorage -c livelinux make-cs -r /vstorage/livelinux-cs7-sata/data -t <span class="hljs-number"><span class="hljs-number">1</span></span> -j /vz/livelinux-cs6-sata-journal -s <span class="hljs-number"><span class="hljs-number">30240</span></span> #  CS,       systemctl <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span> vstorage-csd.target</code> </pre> <br>  In addition, Linux reserves some space on the disk for logging to the / home folder. <br>  In our case, it is pointless, <b>turn off</b> (in the latest versions of the cluster, this is done when preparing disks, but it's better to be convinced) <br><br><pre> <code class="bash hljs">tune2fs -m 0 /dev/sdc1</code> </pre> <br><h3>  Memory </h3><br>  Imagine the situation: the server is the hypervisor for virtual machines and contains disk chunks (CS) for Virtuozzo Storage.  As a result, Linux diligently puts all data on physical disks into the disk cache of RAM and in every way seeks to fill it completely in order to speed up the system operation with disks.  CS (disks) and MDS (distributed file system) services cannot quickly get free pages of memory and have to try every time to free memory from a cache that is long.  Plus, the old editions of Virtuozzo Cluster could not work with memory if it is fragmented, i.e.  they required a whole block of free memory in RAM.  The result is simple - CS chunk services are falling, non-replicated copies of data are formed in the cluster, the cluster starts to replicate them, disk work increases, CS drops even more, replication blocks are even more ... As a result, we have a completely buried cluster and barely live virtual machines . <br><br>  We begin to understand and it turns out that, among other things, by default, Virtuozzo has memory management enabled, which does not imply any overcommit at all.  In other words, virtual machines expect that the memory will be free for their work, and that no disk cache will interfere.  But that is not all.  By default, containers can go beyond their own disk cache into the shared system disk hypervisor cache.  In this case, as a rule, the system cache is already filled with the CS data cache. <br><br>  <b>I present a number of recommendations that need to be implemented immediately, even before the first virtual machine on the cluster starts:</b> <br><br><pre> <code class="hljs pgsql">#             echo "PAGECACHE_ISOLATION=\"yes\"" &gt;&gt; /etc/vz/vz.conf #c  CS     ,      hdd-,    ,   CS       <span class="hljs-number"><span class="hljs-number">32</span></span> cat &gt; /etc/vz/vstorage-limits.conf &lt;&lt; EOL { "VStorage": { "Path": "vstorage.slice/vstorage-services.slice", "Limit": { "Max": <span class="hljs-number"><span class="hljs-number">34359738368</span></span> "Min": <span class="hljs-number"><span class="hljs-number">0</span></span>, "Share": <span class="hljs-number"><span class="hljs-number">0.7</span></span> }, "Guarantee": { "Max": <span class="hljs-number"><span class="hljs-number">4294967296</span></span>, "Min": <span class="hljs-number"><span class="hljs-number">0</span></span>, "Share": <span class="hljs-number"><span class="hljs-number">0.25</span></span> }, "Swap": { "Max": <span class="hljs-number"><span class="hljs-number">0</span></span>, "Min": <span class="hljs-number"><span class="hljs-number">0</span></span>, "Share": <span class="hljs-number"><span class="hljs-number">0</span></span> } } } EOL service vcmmd <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span> #  ,        prlsrvctl <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> <span class="hljs-comment"><span class="hljs-comment">--vcmmd-policy density prlsrvctl info | grep "policy"</span></span></code> </pre> <br>  <b>In addition, you can make some changes to the Linux settings in terms of working with memory:</b> <br><br><pre> <code class="hljs pgsql">touch /etc/sysctl.d/<span class="hljs-number"><span class="hljs-number">00</span></span>-<span class="hljs-keyword"><span class="hljs-keyword">system</span></span>.conf echo "vm.min_free_kbytes=1048576" &gt;&gt; /etc/sysctl.d/<span class="hljs-number"><span class="hljs-number">00</span></span>-<span class="hljs-keyword"><span class="hljs-keyword">system</span></span>.conf echo "vm.overcommit_memory=1" &gt;&gt; /etc/sysctl.d/<span class="hljs-number"><span class="hljs-number">00</span></span>-<span class="hljs-keyword"><span class="hljs-keyword">system</span></span>.conf echo "vm.swappiness=10" &gt;&gt; /etc/sysctl.d/<span class="hljs-number"><span class="hljs-number">00</span></span>-<span class="hljs-keyword"><span class="hljs-keyword">system</span></span>.conf echo "vm.vfs_cache_pressure=1000" &gt;&gt; /etc/sysctl.d/<span class="hljs-number"><span class="hljs-number">00</span></span>-<span class="hljs-keyword"><span class="hljs-keyword">system</span></span>.conf</code> </pre> <br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">sysctl</span></span> -w vm.swappiness=<span class="hljs-number"><span class="hljs-number">10</span></span> sysctl -w vm.overcommit_memory=<span class="hljs-number"><span class="hljs-number">1</span></span> sysctl -w vm.min_free_kbytes=<span class="hljs-number"><span class="hljs-number">1048576</span></span> sysctl -w vm.vfs_cache_pressure=<span class="hljs-number"><span class="hljs-number">1000</span></span></code> </pre> <br>  <b>And periodically flush the disk cache:</b> <br><br><pre> <code class="hljs ruby">sync &amp;&amp; echo <span class="hljs-number"><span class="hljs-number">3</span></span> &gt; <span class="hljs-regexp"><span class="hljs-regexp">/proc/sys</span></span><span class="hljs-regexp"><span class="hljs-regexp">/vm/drop</span></span>_caches</code> </pre> <br>  Resetting the cache allows you to temporarily solve the above problems, but you need to do it carefully.  Unfortunately, this is bad for the performance of the disk subsystem, since instead of the cache, data begins to be re-read from disks. <br><br>  <b>This is what the normal memory mode of the Virtuozzo Storage looks like.</b> <br><br><img src="https://habrastorage.org/webt/in/mo/ox/inmooxflt1h3qbi-p7q2xkdbcrk.png"><br><br>  As you can see, the disk cache is stable, there is enough free memory on the server.  Of course, there can be no talk of any real overcom memory.  You must take this into account. <br><br><h3>  Freeing up disk space, cleaning up trash </h3><br>  Containers and virtual machines in a cluster are stored as large flat files.  When something is removed inside a container or virtual machine, in fact, the place does not become free, just the data blocks in the cluster are reset.  To solve this problem, the developers of Virtuozzo wrote the pcompact tool.  This utility is started by crown on all servers simultaneously at 3 am and tries to defragment the images of virtual machines, as well as to delete and clear unallocated memory pages in the cluster (that is, to reclaim free space).  The operation itself creates a very high load on the network, on the disks, and also requires additional RAM for its work (sometimes quite a lot).  This can lead to a drop in cluster performance during this operation due to high utilization of the network and disks.  Also, high memory consumption (just free memory, disk cache is not unloaded) can lead to the effect described in the paragraph about memory. <br><br>  Those.  we have top network and disk utilization.  Plus, if there is not enough free memory, CS chunks start to fall, replication of missing data blocks begins.  As a result, avalanche-like problems occur on all nodes, on all virtual machines, and so on. <br><br>  In addition, by default, this operation lasts only 2 hours, after which it automatically stops.  This means that if you have a dozen very large virtual machines, there is a chance that cleaning up garbage will never reach them all, since it will not fit into a two-hour timeout. <br><br>  To reduce the negative impact of this process, we moved this task from the crown to anacron.  Then the procedure starts at an arbitrary time at night and is not interrupted until completion.  As a result, as a rule, the process works only on one node of the cluster at a time, which generally reduces the load and the risk of cluster degradation at night. <br><br><h3>  Monitoring </h3><br>  <b>The cluster has good proprietary monitoring tools, such as:</b> <br><br><pre> <code class="bash hljs">vstorage <span class="hljs-built_in"><span class="hljs-built_in">stat</span></span> --cluster livelinux <span class="hljs-comment"><span class="hljs-comment">#  </span></span></code> </pre> <br><pre> <code class="bash hljs">vstorage top --cluster livelinux <span class="hljs-comment"><span class="hljs-comment">#  </span></span></code> </pre> <br>  It is also possible to get data for zabbiks and build all the necessary charts right in it. <br><br>  On the example of <b>our internal</b> company <b>cloud</b> : <br><br>  Free space in various shooting galleries and a common licensed cluster location. <br><br><img src="https://habrastorage.org/webt/je/5e/--/je5e--p0bye-xd9iybqgi03tvhq.png"><br><br>  The status of the cluster, how many percent are rebalanced, how many chunks are urgently replicated.  Perhaps one of the most important graphs. <br><br><img src="https://habrastorage.org/webt/jq/ip/eg/jqipegnazt3prlxmgmrqd3s5fyo.png"><br><br>  The total network disk load of all virtual machines in the cluster and the speed of replication and rebalancing. <br><br><img src="https://habrastorage.org/webt/ty/ae/qb/tyaeqbbc07lvkzrk59n8xuxxfy4.png"><br><br>  The average and maximum disk queue in the cluster. <br><br><img src="https://habrastorage.org/webt/dm/_i/gg/dm_iggrlgpxbuzfqctcjrrgzopc.png"><br><br>  General statistics on IOPS cluster. <br><br><img src="https://habrastorage.org/webt/id/_b/xu/id_bxufkhbzsxdyidyitb9z08m4.png"><br><br>  Also, do not forget, by default, the firewall on Virtuozzo prohibits the operation of everything that is not explicitly allowed.  For zabbix-agent you will need a couple of firewall permission rules: <br><br><pre> <code class="bash hljs">firewall-cmd --permanent --add-port=10050/tcp firewall-cmd --permanent --add-port=10051/tcp firewall-cmd --permanent --add-port=5001/tcp systemctl restart firewalld</code> </pre> <br><h3>  And finally, some useful commands for working with a cluster </h3><br>  <b>Find out which virtual machine lives in the dash:</b> <br><br><pre> <code class="bash hljs">(<span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> `ls /vstorage/livelinux/private`; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> grep HOSTNAME /vstorage/livelinux/private/<span class="hljs-variable"><span class="hljs-variable">$i</span></span>/ve.conf; vstorage -c livelinux get-attr /vstorage/livelinux/private/<span class="hljs-variable"><span class="hljs-variable">$i</span></span>|grep tier; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span>) | (grep <span class="hljs-string"><span class="hljs-string">"HOSTNAME\|tier"</span></span> | tr <span class="hljs-string"><span class="hljs-string">'\n '</span></span> <span class="hljs-string"><span class="hljs-string">' '</span></span> | tr <span class="hljs-string"><span class="hljs-string">'H'</span></span> <span class="hljs-string"><span class="hljs-string">'\n'</span></span> | sed <span class="hljs-string"><span class="hljs-string">'s#tier=2#SSD#g'</span></span> | sed <span class="hljs-string"><span class="hljs-string">'s#tier=1#HDD#g'</span></span> | sed <span class="hljs-string"><span class="hljs-string">'s#tier=2#SSD#g'</span></span> | sed <span class="hljs-string"><span class="hljs-string">'s#OSTNAME=##g'</span></span> | sed <span class="hljs-string"><span class="hljs-string">'s#"##g'</span></span>) | awk <span class="hljs-string"><span class="hljs-string">'{print $2" "$1}'</span></span> | sort</code> </pre> <br>  <b>Manually starting the pcompact process</b> <br><br><pre> <code class="bash hljs">/usr/sbin/pcompact -v</code> </pre> <br>  By default, Virtuozzo Storage reserves some disk space, just in case.  If you strongly pressed, then you can <b>temporarily</b> lower the percentage of the reservation: <br><br><pre> <code class="hljs swift">vstorage -<span class="hljs-built_in"><span class="hljs-built_in">c</span></span> livelinux <span class="hljs-keyword"><span class="hljs-keyword">set</span></span>-config mds.alloc.fill_margin=<span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br>  <b>If you need to unmount the cluster file system, but you do not want to overload the server, you need to shut down all the virtual machines and run:</b> <br><br><pre> <code class="bash hljs">fusermount -uz /vstorage/livelinux <span class="hljs-comment"><span class="hljs-comment">#  </span></span></code> </pre> <br>  <b>Find out the statistics of the local data cache</b> <br><br><pre> <code class="bash hljs">watch cat /vstorage/livelinux/.vstorage.info/read_cache_info</code> </pre> <br>  <b>Find out the status of your licenses</b> <br><br><pre> <code class="bash hljs">pstorage -c livelinux view-license <span class="hljs-comment"><span class="hljs-comment">#   vzlicupdate vzlicview</span></span></code> </pre> <br>  <b>Learn your hwid</b> <br><br><pre> <code class="bash hljs">pstorage -c livelinux <span class="hljs-built_in"><span class="hljs-built_in">stat</span></span> --license-hwid</code> </pre> <br>  <b>Sometimes it happens that the size of the pfcache log does not fit into the allocated virtual disk.</b>  <b>There is the following solution</b> <br><br><pre> <code class="bash hljs">prl_disk_tool resize resize --size 15G --hdd /vz/pfcache.hdd/DiskDescriptor.xml</code> </pre> <br>  <b>Delete MDS service from server</b> <br><br><pre> <code class="bash hljs">vstorage -c livelinux rm-mds 11 <span class="hljs-comment"><span class="hljs-comment">#11-id  mds</span></span></code> </pre> <br>  <b>Start the MDS service on the local server</b> <br><br><pre> <code class="bash hljs">vstorage -c livelinux make-mds -a 172.17.0.254:2510 <span class="hljs-comment"><span class="hljs-comment">#    -r /vz/mds/data #   mds,  ssd  -b 172.17.0.255 -b 172.17.0.249 -b 172.17.0.248 -b 172.17.0.4 #  mds systemctl restart vstorage-mdsd.target systemctl enable vstorage-mdsd.target</span></span></code> </pre> <br>  <b>Change the default number of copies of virtual machine data on cluster servers</b> <br><br><pre> <code class="bash hljs">vstorage <span class="hljs-built_in"><span class="hljs-built_in">set</span></span>-attr -R /vstorage/livelinux replicas=2</code> </pre> <br>  <b>Change the number of copies of specific virtual machine data</b> <br><br><pre> <code class="bash hljs">vstorage <span class="hljs-built_in"><span class="hljs-built_in">set</span></span>-attr -R /vstorage/livelinux/private/a0327669-855d-4523-96b7-cf4d41ccff7e replicas=1</code> </pre> <br>  <b>Change the shooting range of all virtual machines, and set the default shooting range</b> <br><br><pre> <code class="bash hljs">vstorage <span class="hljs-built_in"><span class="hljs-built_in">set</span></span>-attr -R /vstorage/livelinux/private tier=2 vstorage <span class="hljs-built_in"><span class="hljs-built_in">set</span></span>-attr -R /vstorage/livelinux/vmprivate tier=2</code> </pre> <br>  <b>Change the shooting range of a specific virtual machine</b> <br><br><pre> <code class="bash hljs">vstorage <span class="hljs-built_in"><span class="hljs-built_in">set</span></span>-attr -R /vstorage/livelinux/private/8a4c1475-4335-40e7-8c95-0a3a782719b1 tier=2</code> </pre> <br>  <b>Stop and migrate all virtual machines on the hypervisor</b> <br><br><pre> <code class="bash hljs">hp=msk-07 <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> vm <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> `prlctl list | grep -v UUID | awk <span class="hljs-string"><span class="hljs-string">'{print $5}'</span></span>`; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> prlctl stop <span class="hljs-variable"><span class="hljs-variable">$vm</span></span>; prlctl migrate <span class="hljs-variable"><span class="hljs-variable">$vm</span></span> <span class="hljs-variable"><span class="hljs-variable">$hp</span></span>; ssh <span class="hljs-variable"><span class="hljs-variable">$hp</span></span> -C prlctl start <span class="hljs-variable"><span class="hljs-variable">$vm</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span></code> </pre> <br>  <b>Forcing a virtual machine to be forced on another server</b> , even if it was already running somewhere, but it was completely frozen or was not correctly migrated: <br><br><pre> <code class="bash hljs">prlctl register /vz/private/20f36a7f-f64d-46fa-b0ef-85182bc41433 --preserve-uuid --force</code> </pre> <br>  <b>Listing commands for creating and configuring a container</b> <br><br><pre> <code class="bash hljs">prlctl create test.local --ostemplate centos-7-x86_64 --vmtype ct prlctl <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> test.local --hostname test.local --cpus 4 --memsize 10G --swappages 512M --onboot yes prlctl <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> test.local --device-set hdd0 --size 130G prlctl <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> test.local --netif_add netif1 prlctl <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> test.local --ifname netif1 --network network1 prlctl <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> test.local --ifname netif1 --ipadd 172.17.0.244/24 prlctl <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> test.local --ifname netif1 --nameserver 172.17.0.1 prlctl <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> test.local --ifname netif1 --gw 172.17.0.1 prlctl start test.local prlctl enter test.local</code> </pre> <br>  Thank you very much for your attention. </div><p>Source: <a href="https://habr.com/ru/post/341168/">https://habr.com/ru/post/341168/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../341154/index.html">What every C programmer should know about Undefined Behavior. Part 3/3</a></li>
<li><a href="../341156/index.html">Security Week 43: The Great IoT Reap Is Coming, Like NATO Cyberconf Hackers Flying, Bad Exception Rabbit Ears ExPetr</a></li>
<li><a href="../341160/index.html">Remove radial distortion from photos and videos using the openCV library and the python language</a></li>
<li><a href="../341164/index.html">Your users do not need passwords.</a></li>
<li><a href="../341166/index.html">How to arrange an open source project</a></li>
<li><a href="../341170/index.html">Cloning a 50Gb database from Prod to Dev in 1 second without loss of integrity</a></li>
<li><a href="../341172/index.html">Connecting a remote COM controller to a computer‚Äôs USB port via unmatched lines</a></li>
<li><a href="../341178/index.html">SILVER: how I design iOS apps</a></li>
<li><a href="../341180/index.html">Paul Graham. All articles in Russian. Two years later</a></li>
<li><a href="../341182/index.html">Virtual Infrastructure Provider Development: 1cloud Experience</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>