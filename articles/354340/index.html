<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>We increase the disk mass without steroids. Overview of the Western Digital Ultrastar Data102 102 Disk Shelf and Storage Configuration</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="What are great jbods good for? 
 The new JBOD Western Digital on a disk of 102TB turned out to be powerful. In developing this JBOD, previous experien...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>We increase the disk mass without steroids. Overview of the Western Digital Ultrastar Data102 102 Disk Shelf and Storage Configuration</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/j4/bh/c6/j4bhc6ykkj_c9ivdpmqpy-8w-xi.png"><br><br><h2>  What are great jbods good for? </h2><br>  The new JBOD Western Digital on a disk of 102TB turned out to be powerful.  In developing this JBOD, previous experience with two generations of 60 disk shelves was taken into account. <br>  Data102 turned out to be rare for such giants balanced in volume and performance. <br><br>  Why do we need such large disc baskets when the popularity of hyper-convergent systems grows in the world? <br><a name="habracut"></a><br>  Tasks in which the requirements for storage volumes significantly exceed the requirements for computing power, can inflate the customer's budget to incredible sizes.  Here are just a few examples and scenarios: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ol><li>  Replication Factor 2 or 3, used in the construction of Scale-out systems, on several petabytes of data is an expensive solution. </li><li>  Intensive sequential read / write operations cause the cluster node to go beyond the local storage, which can lead to problems such as long-tail latency.  In this case, you should be extremely careful when building a network. </li><li>  Distributed systems do an excellent job with tasks like ‚Äúmany applications work with many of their files‚Äù and mediocre with writing and reading from a strongly connected cluster, especially in the N-to-1 mode. </li><li>  With tasks like ‚Äúincrease the depth of the video archive by 2 times‚Äù it is much cheaper to throw a big JBOD than to increase the number of servers in the cluster by 2 times. </li><li>  Using external storage systems with JBOD, we can clearly allocate volume and performance for our priority applications by reserving certain disks, caches, ports for them, while maintaining the necessary level of flexibility and scalability. </li></ol><br>  As a rule, disk shelves of the Data102 level are developed by disk manufacturers who understand well how to work with these disks and know all the pitfalls.  In such devices, everything is fine with the level of vibration and cooling, and the power consumption corresponds to the real data storage needs. <br><br><h2>  What is good about Western Digital's JBOD? </h2><br>  We are well aware that modular systems are limited in scalability by the capabilities of the controllers and that the network always creates delays.  But at the same time, such systems have lower IOps, GBps and TB storage costs. <br><br>  There are two things for which RAIDIX engineers loved Data102: <br><br><ol><li>  JBOD not only allows you to place&gt; 1 PB data per 4U.  It is really very fast and on streaming operations it is not inferior to many all-flash solutions: 4U, 1PB, 23 GB / s are good indicators for a disk array. </li><li>  The Data102 is easy to maintain and requires no tools, like a screwdriver. </li></ol><br>  Our testing team hates screwdrivers that they already dream of at night.  When they heard that HGST / WD is making a 102-disc monster, and presented how they would have to deal with 408 small cogs, strong alcohol ended in a nearby store. <br><br>  In vain they were afraid.  Taking care of the engineers, Western Digital has come up with a new way to mount the drive, which makes it easier to maintain.  The discs are attached to the chassis using fixing clips, without bolts and screws.  All discs are mechanically isolated using elastic fasteners on the rear panel.  New firmware servo and accelerometers perfectly compensate for vibration. <br><br><h2>  What's in the box? </h2><br>  In the box - the body of the basket, filled with discs.  You can buy at least 24 disks, and the solution is scaled by sets of 12 disks.  This is done in order to ensure proper cooling and to deal with vibration in the best possible way. <br><br>  By the way, the development of two assistive technologies - IsoVibe and ArcticFlow - made possible the birth of the new JBOD. <br><br>  <b>IsoVibe</b> consists of the following components: <br><br><ol><li>  Specialized drive firmware, which with the help of sensors controls the servos and predictively reduces the level of vibrations. </li><li>  Vibration insulated connectors on the back of the server (Fig. 1). </li><li>  And, of course, special mounting discs that do not require screws. </li></ol><br><img src="https://habrastorage.org/webt/la/pm/09/lapm09blfx_ovxn9s9p1ggfyeng.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">1. Vibration insulated connectors</font></i> <br><br>  Temperature is the second factor after vibration that kills hard drives.  At an average operating temperature above 55C, the mean time between the failures of the hard disk will be half the estimated one. <br><br>  Bad cooling particularly affects servers with a large number of disks and large disk shelves.  Often, the back rows of the discs are heated by more than 20 degrees more than the discs located near the cold corridor. <br><br>  <b>ArcticFlow</b> is Western Digital's patented shelf cooling technology, the meaning of which is to create additional ducts inside the chassis that allow you to pull cold air to the back rows of disks directly from the cold corridor, bypassing the front rows. <br><br><img src="https://habrastorage.org/webt/mt/5x/wl/mt5xwllfyt3trlj1plh7_mlqnvc.jpeg"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">2. The principle of operation of ArcticFlow</font></i> <br><br>  A separate stream of cold air is built to cool the I / O modules and power supplies. <br><br>  The result is a great thermal map of the operating shelf.  The temperature range between the front and rear rows of disks is 10 degrees.  The hottest disc is 49C at a temperature in the ‚Äúcold‚Äù corridor + 35C.  1.6W is spent on cooling each disc - two times less than other similar chassis.  Fans are quieter, vibration is less, drives live longer and work faster. <br><br><img src="https://habrastorage.org/webt/sm/ow/gi/smowgix0ki6yl6azalojm4hd6rm.jpeg"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">3. Temperature card Ultrastar Data 102</font></i> <br><br>  Considering the 12W power budget per single disk, the shelf can easily be made hybrid ‚Äî out of 102 disks, 24 can be SAS SSDs.  They can be installed and used in a hybrid mode, or by setting up SAS Zoning and transferring it to a host in need of all-flash. <br><br>  We also have a rack in the box.  To install JBOD you need a couple of physically strong engineers.  Here is what they will face: <br><br><ul><li>  Shelf assembly weighs 120 kg, and without discs - 32 kg </li><li>  The deep stand in this case starts from 1200 mm </li><li>  Well, add the SAS and power cables </li></ul>  . <br>  JBOD mounts and cabling are designed in such a way that maintenance can be performed hot.  Note also the vertical installation of an input / output module (IOM). <br><br>  Let's take a look at this system.  Front is simple and concise. <br><br><img src="https://habrastorage.org/webt/hd/7v/w7/hd7vw7icht8zzjpdif3vmqqfor4.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">4. Ultrastar Data 102. Front view</font></i> <br><br>  One of the most interesting features of JBOD is the installation of IO-modules on top! <br><br><img src="https://habrastorage.org/webt/xh/ur/cg/xhurcght28udaulaurbu_eo9imm.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">5. Ultrastar Data 102</font></i> <br><br><img src="https://habrastorage.org/webt/j2/m0/jo/j2m0joxbkmrbbabnfmcbsatnlgm.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">6. Ultrastar Data 102. Top view</font></i> <br><br><img src="https://habrastorage.org/webt/li/uv/i1/liuvi1nx3eepprlenhuuweusqys.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">7. Ultrastar Data 102. Top view without disks</font></i> <br><br>  At the back, JBOD has 6 SAS 12G ports for each IO-module.  Total we get 28,800 MBps of backend bandwidth.  Ports can be used to connect to hosts, and partially for cascading.  There are two ports for powering the system (80+ Platinum rated 1600W CRPS). <br><br><img src="https://habrastorage.org/webt/wa/xq/hp/waxqhp6-e1jpuy2whcwxcmkn3be.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">8. Ultrastar Data 102. Rear View</font></i> <br><br><h2>  Performance </h2><br>  As we said, Data102 is not just huge - it's fast!  The results of tests conducted by the vendor are: <br><br>  <b>On 12 servers:</b> <br>  <i>Sequential load</i> <br><ul><li>  Reading = 24.2GB / s max.  @ 1MB (237 MB / s per HDD max.) </li><li>  Record = 23.9GB / s max.  @ 1MB (234 MB / s per HDD max.) </li></ul><br>  <i>Random load</i> <br><ul><li>  4kB read with queue depth = 128:&gt; 26k IOps </li><li>  4kB write with 1‚Äì128 queue depth:&gt; 45k IOps </li></ul><br>  <b>On 6 servers:</b> <br>  <i>Sequential load</i> <br><ul><li>  Reading = 22.7GB / s max.  @ 1MB (223 MB / s per HDD max.) </li><li>  Record = 22.0GB / s max.  @ 1MB (216 MB / s per HDD max.) </li></ul><br>  <i>Random load</i> <br><ul><li>  4kB read with queue depth = 128:&gt; 26k IOps </li><li>  Entry with queue depth = 1‚Äì128:&gt; 45k IOps </li></ul><br><img src="https://habrastorage.org/webt/tp/iz/ud/tpizudzn-pz3f3igiaba75k0mjy.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">9. Parallel load from 12 servers</font></i> <br><br><img src="https://habrastorage.org/webt/t6/yt/i-/t6yti-mk5yikahg58mhhwpooxtc.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">10. Parallel load from 6 servers</font></i> <br><br><h2>  Control </h2><br>  There are two ways to manage JBOD from the software side: <br><br><ol><li>  By ses </li><li>  By redfish </li></ol><br>  RedFish allows you to find components by lighting the LED, get information about the "health" of the components, as well as update the firmware. <br>  By the way, the chassis supports T10 Power Disabling (Pin 3) to power off and reset individual drives. <br><br>  This is useful if you have a disk that hangs the entire SAS bus. <br><br><h2>  Typical configurations </h2><br>  In order to use the capabilities of such a JBOD with maximum benefit, we will need RAID controllers or software.  This is where RAIDIX software comes to the rescue. <br><br>  To create a fault-tolerant storage system, we need two storage nodes and one or more baskets with SAS disks.  If we do not want to implement protection against node failure or use data replication, then we can connect one server to the basket and use SATA disks. <br><br><h3>  Dual controller configuration </h3><br>  As controllers for RAIDIX-based storage systems, virtually any x86 server platform can be used: Supermicro, AIC, Dell, Lenovo, HPE, and many others.  We are constantly working on certifying new equipment and porting our code to various architectures (for example, Elbrus and OpenPower). <br><br>  For example, take the Supermicro platform and try to achieve the highest possible throughput and computational density.  When ‚Äúsizing‚Äù servers, we will use the PCI-E bus, where we will install the back-end and front-end controllers. <br><br>  We also need controllers to connect a disk shelf, at least two AVAGO 9300-8e.  Alternatively: a pair of 9400-8e or one 9405W-16e, but for the latter you need a full x16 slot. <br><br>  The next component is a slot for the sync channel.  This may be Infiniband or SAS.  (For tasks where bandwidth and latency are not critical, you can get by synchronizing through the basket without a dedicated slot.) <br><br>  And, of course, we will need slots for host interfaces, which must also be at least two. <br><br>  <b>Total:</b> each controller needs to have from 5 x8 slots (without a margin for further scaling).  To build low-cost systems targeting performance of 3-4 GB / s per node, we can get by with just two slots. <br><br><h4>  Controller Configuration Options </h4><br>  <b>Supermicro 6029P-TRT</b> <br>  The controllers can be placed in two 2U 6029P-TRT servers.  They are not the richest in terms of PCI-E slots, but they are equipped with a standard motherboard without raisers.  These boards are guaranteed to ‚Äúget‚Äù NVDIMM-N modules from Micron to protect the cache from power failures. <br><br>  To connect the disks take Broadcom 9400 8e.  Dirty cache segments will be synchronized via IB 100Gb. <br><br>  <b>Attention!</b>  The following configurations are designed for maximum performance and operation of all available options.  For your specific task, the specification can be significantly reduced.  Contact our partners. <br><br>  The configuration of the system that we got: <br><table><tbody><tr><th>  No </th><th>  Name </th><th>  Description </th><th>  P / N </th><th>  Qty per RAIDIX DC </th></tr><tr><td>  one </td><td>  Platform </td><td>  SuperServer 6029P-TRT </td><td>  SYS-6029P-TRT </td><td>  2 </td></tr><tr><td>  2 </td><td>  CPU </td><td>  Intel Xeon Silver 4112 Processor </td><td>  Intel Xeon Silver 4112 Processor </td><td>  four </td></tr><tr><td>  3 </td><td>  Memory </td><td>  16GB PC4-21300 2666MHz DDR4 ECC Registered DIMM Micron MTA36ASF472PZ-2G6D1 </td><td>  MEM-DR416L-CL06-ER26 </td><td>  12 </td></tr><tr><td>  four </td><td>  System disk </td><td>  SanDisk Extreme PRO 240GB </td><td>  SDSSDXPS-240G-G25 </td><td>  four </td></tr><tr><td>  five </td><td>  Hot-swap 3.5 "to 2.5" SATA / SAS Drive Trays </td><td>  Tool-less black hot-swap 3.5-to-2.5 ‚Äã‚Äãconverter HDD drive tray (Red tab) </td><td>  MCP-220-00118-0B </td><td>  four </td></tr><tr><td>  6 </td><td>  HBA for cache-sync </td><td>  Mellanox ConnectX-4 VPI adapter card, EDR IB (100Gb / s), dual-port QSFP28, PCIe3.0 x16 </td><td>  MCX456A-ECAT </td><td>  2 </td></tr><tr><td>  7 </td><td>  HBA for JBOD connection </td><td>  Broadcom HBA 9400-8e Tri-Mode Storage Adapter </td><td>  05-50013-01 </td><td>  four </td></tr><tr><td>  eight </td><td>  Ethernet patchcord </td><td>  Ethernet patch cord for cache sync 0.5m </td><td></td><td>  one </td></tr><tr><td>  9 </td><td>  Cable for cache sync </td><td>  Mellanox passive copper cable, VPI, EDR 1m </td><td>  MCP1600-E001 </td><td>  2 </td></tr><tr><td>  ten </td><td>  HBA for host connection </td><td>  Mellanox ConnectX-4 VPI adapter card, EDR IB (100Gb / s), dual-port QSFP28, PCIe3.0 x16 </td><td>  MCX456A-ECAT </td><td>  2 </td></tr><tr><td>  eleven </td><td>  SAS cable </td><td>  Ultrastar Data102 Cable IO HD mini-SAS to HD mini-SAS 2m 2Pack storage enclosure </td><td></td><td>  eight </td></tr><tr><td>  12 </td><td>  Jbod </td><td>  Ultrastar Data102 </td><td></td><td>  one </td></tr><tr><td>  13 </td><td>  RAIDIX </td><td>  RAIDIX 4.6 DC / NAS / iSCSI / FC / SAS / IB / SSD-cache / QoSmic / SanOpt / Extended 5 years support / unlimited disks / </td><td>  RX46DSMMC-NALL-SQ0S-P5 </td><td>  one </td></tr></tbody></table><br>  Here is an approximate diagram: <br><br><img src="https://habrastorage.org/webt/cr/87/ap/cr87apvzu7ggopcyypqybe_f7fu.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">11. Configuration based on Supermicro 6029P-TRT</font></i> <br><br>  <b>Supermicro 2029BT-DNR</b> <br>  If we want to compete for space in the server room, then Supermicro Twin, for example, 2029BT-DNR, can be taken as the basis for storage controllers.  These systems have 3 PCI-E slots and 1 IOM module each.  Among the IOM is the Infiniband we need. <br><br>  Configuration: <br><table><tbody><tr><th>  No </th><th>  Name </th><th>  Description </th><th>  P / N </th><th>  Qty per RAIDIX DC </th></tr><tr><td>  one </td><td>  Platform </td><td>  SuperServer 2029BT-DNR </td><td>  SYS-2029BT-DNR </td><td>  one </td></tr><tr><td>  2 </td><td>  CPU </td><td>  Intel Xeon Silver 4112 Processor </td><td>  Intel Xeon Silver 4112 Processor </td><td>  four </td></tr><tr><td>  3 </td><td>  Memory </td><td>  16GB PC4-21300 2666MHz DDR4 ECC Registered DIMM Micron MTA36ASF472PZ-2G6D1 </td><td>  MEM-DR416L-CL06-ER26 </td><td>  12 </td></tr><tr><td>  four </td><td>  System disk </td><td>  Supermicro SSD-DM032-PHI </td><td>  SSD-DM032-PHI </td><td>  2 </td></tr><tr><td>  five </td><td>  HBA for cache-sync </td><td>  Mellanox ConnectX-4 VPI adapter card, EDR IB (100Gb / s), dual-port QSFP28, PCIe3.0 x16 </td><td>  MCX456A-ECAT </td><td>  2 </td></tr><tr><td>  6 </td><td>  HBA for JBOD connection </td><td>  Broadcom HBA 9405W-16e Tri-Mode Storage Adapter </td><td>  05-50044-00 </td><td>  2 </td></tr><tr><td>  7 </td><td>  Ethernet patchcord </td><td>  Ethernet patch cord for cache sync 0.5m </td><td></td><td>  one </td></tr><tr><td>  eight </td><td>  Cable for cache sync </td><td>  Mellanox passive copper cable, VPI, EDR 1m </td><td>  MCP1600-E001 </td><td>  2 </td></tr><tr><td>  9 </td><td>  HBA for host connection </td><td>  Mellanox ConnectX-4 VPI adapter card, EDR IB (100Gb / s), dual-port QSFP28, PCIe3.0 x16 </td><td>  MCX456A-ECAT </td><td>  2 </td></tr><tr><td>  ten </td><td>  SAS cable </td><td>  Ultrastar Data102 Cable IO HD mini-SAS to HD mini-SAS 2m 2Pack storage enclosure </td><td></td><td>  eight </td></tr><tr><td>  eleven </td><td>  Jbod </td><td>  Ultrastar Data102 </td><td></td><td>  one </td></tr><tr><td>  12 </td><td>  RAIDIX </td><td>  RAIDIX 4.6 DC / NAS / iSCSI / FC / SAS / IB / SSD-cache / QoSmic / SanOpt / Extended 5 years support / unlimited disks </td><td>  RX46DSMMC-NALL-SQ0S-P5 </td><td>  one </td></tr></tbody></table><br>  Here is an approximate diagram: <br><br><img src="https://habrastorage.org/webt/e4/6b/ng/e46bngk_amu8yst3zigbck83bqm.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">12. Configuration based on Supermicro 2029BT-DNR</font></i> <br><br>  <b>1U platform</b> <br>  Often there are tasks that require the maximum density of large amounts of data, but not required, for example, full fault tolerance for controllers.  In this case, we take the 1U system as a basis and connect the maximum number of disk shelves to it. <br><br><h3>  Scale-out system </h3><br>  As a final exercise in our workout, we will build a horizontal-scalable system based on HyperFS.  To begin with, we will select 2 types of controllers - for data storage and for metadata storage. <br><br>  Storage controllers will be assigned to SuperMicro 6029P-TRT. <br><br>  To store the metadata, we use several SSD drives in the basket, which we will combine into RAID and return MDC via SAN.  For one storage system we can connect up to 4 JBOD cascade.  Total in one deep rack place X PB data with a single namespace. <br><table><tbody><tr><th>  No </th><th>  Name </th><th>  Description </th><th>  P / N </th><th>  Qty per RAIDIX DC </th></tr><tr><td>  one </td><td>  Platform </td><td>  SuperServer 6029P-TRT </td><td>  SYS-6029P-TRT </td><td>  2 </td></tr><tr><td>  2 </td><td>  CPU </td><td>  Intel Xeon Silver 4112 Processor </td><td>  Intel Xeon Silver 4112 Processor </td><td>  four </td></tr><tr><td>  3 </td><td>  Memory </td><td>  16GB PC4-21300 2666MHz DDR4 ECC Registered DIMM Micron MTA36ASF472PZ-2G6D1 </td><td>  MEM-DR416L-CL06-ER26 </td><td>  sixteen </td></tr><tr><td>  four </td><td>  System disk </td><td>  SanDisk Extreme PRO 240GB </td><td>  SDSSDXPS-240G-G25 </td><td>  four </td></tr><tr><td>  five </td><td>  Hot-swap 3.5 "to 2.5" SATA / SAS Drive Trays </td><td>  Tool-less black hot-swap 3.5-to-2.5 ‚Äã‚Äãconverter HDD drive tray (Red tab) </td><td>  MCP-220-00118-0B </td><td>  four </td></tr><tr><td>  6 </td><td>  HBA for cache-sync </td><td>  Mellanox ConnectX-4 VPI adapter card, EDR IB (100Gb / s), dual-port QSFP28, PCIe3.0 x16 </td><td>  MCX456A-ECAT </td><td>  2 </td></tr><tr><td>  7 </td><td>  HBA for JBOD connection </td><td>  Broadcom HBA 9400-8e Tri-Mode Storage Adapter </td><td>  05-50013-01 </td><td>  four </td></tr><tr><td>  eight </td><td>  Ethernet patchcord </td><td>  Ethernet patch cord for cache sync 0.5m </td><td></td><td>  one </td></tr><tr><td>  9 </td><td>  Cable for cache sync </td><td>  Mellanox passive copper cable, VPI, EDR 1m </td><td>  MCP1600-E001 </td><td>  2 </td></tr><tr><td>  ten </td><td>  HBA for host connection </td><td>  Mellanox ConnectX-4 VPI adapter card, EDR IB (100Gb / s), dual-port QSFP28, PCIe3.0 x16 </td><td>  MCX456A-ECAT </td><td>  2 </td></tr><tr><td>  eleven </td><td>  SAS cable </td><td>  Ultrastar Data102 Cable IO HD mini-SAS to HD mini-SAS 2m 2Pack storage enclosure </td><td></td><td>  eight </td></tr><tr><td>  12 </td><td>  Jbod </td><td>  Ultrastar Data102 </td><td></td><td>  one </td></tr><tr><td>  13 </td><td>  RAIDIX </td><td>  RAIDIX 4.6 DC / NAS / iSCSI / FC / SAS / IB / SSD-cache / QoSmic / SanOpt / Extended 5 years support / unlimited disks / </td><td>  RX46DSMMC-NALL-SQ0S-P5 </td><td>  one </td></tr><tr><td>  14 </td><td>  Platform (MDC HyperFS) </td><td>  SuperServer 6028R-E1CR12L </td><td>  SSG-6028R-E1CR12L </td><td>  one </td></tr><tr><td>  15 </td><td>  CPU (MDC HyperFS) </td><td>  Intel Xeon E5-2620v4 Processor </td><td>  Intel Xeon E5-2620v4 Processor </td><td>  2 </td></tr><tr><td>  sixteen </td><td>  Memory (MDC HyperFS) </td><td>  32GB DIM4 Crucial CT32G4RFD424A 32Gb DIMM ECC Reg PC4-19200 CL17 2400MHz </td><td>  CT32G4RFD424A </td><td>  four </td></tr><tr><td>  17 </td><td>  System Disk (MDC HyperFS) </td><td>  SanDisk Extreme PRO 240GB </td><td>  SDSSDXPS-240G-G25 </td><td>  2 </td></tr><tr><td>  18 </td><td>  Hot-swap 3.5 "to 2.5" SATA / SAS Drive Trays (MDC HyperFS) </td><td>  Tool-less black hot-swap 3.5-to-2.5 ‚Äã‚Äãconverter HDD drive tray (Red tab) </td><td>  MCP-220-00118-0B </td><td>  2 </td></tr><tr><td>  nineteen </td><td>  HBA (MDC HyperFS) </td><td>  Mellanox ConnectX-4 VPI adapter card, EDR IB (100Gb / s), dual-port QSFP28, PCIe3.0 x16 </td><td>  MCX456A-ECAT </td><td>  one </td></tr></tbody></table><br>  Here is an approximate wiring diagram: <br><br><img src="https://habrastorage.org/webt/j-/rx/-z/j-rx-zhrid8cae1olgrfenb0hko.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">13. Scale-Out System Configuration</font></i> <br><br><h2>  Conclusion </h2><br>  Working with large amounts of data, especially on write-intensive patterns, is a very difficult task for storage systems, the classic solution of which is to acquire shared-nothing scale-out systems.  The new JBOD from Western Digital and software RAIDIX will allow you to build storage systems of several petabytes and several dozen GBps of performance much cheaper than using horizontal-scalable systems, and we advise you to pay your attention to this solution. <br><br><h2>  UPD </h2><br>  Added system specification with Micron's NVMDIMM-N: <br><table><tbody><tr><th>  No </th><th>  Name </th><th>  Description </th><th>  P / N </th><th>  Qty per RAIDIX DC </th></tr><tr><td>  one </td><td>  Platform </td><td>  SuperServer 6029P-TRT </td><td>  SYS-6029P-TRT </td><td>  2 </td></tr><tr><td>  2 </td><td>  CPU </td><td>  Intel Xeon Silver 4112 Processor </td><td>  Intel Xeon Silver 4112 Processor </td><td>  four </td></tr><tr><td>  3 </td><td>  Memory </td><td>  16GB PC4-21300 2666MHz DDR4 ECC Registered DIMM Micron MTA36ASF472PZ-2G6D1 </td><td>  MEM-DR416L-CL06-ER26 </td><td>  12 </td></tr><tr><td>  four </td><td>  NVRAM </td><td>  16GB (x72, ECC, SR) 288-Pin DDR4 Nonvolatile RDIMM MTA18ASF2G72PF1Z </td><td>  MTA18ASF2G72PF1Z-2G6V21AB </td><td>  four </td></tr><tr><td>  five </td><td>  System disk </td><td>  SanDisk Extreme PRO 240GB </td><td>  SDSSDXPS-240G-G25 </td><td>  four </td></tr><tr><td>  6 </td><td>  Hot-swap 3.5 "to 2.5" SATA / SAS Drive Trays </td><td>  Tool-less black hot-swap 3.5-to-2.5 ‚Äã‚Äãconverter HDD drive tray (Red tab) </td><td>  MCP-220-00118-0B </td><td>  four </td></tr><tr><td>  7 </td><td>  HBA for cache-sync </td><td>  Mellanox ConnectX-4 VPI adapter card, EDR IB (100Gb / s), dual-port QSFP28, PCIe3.0 x16 </td><td>  MCX456A-ECAT </td><td>  2 </td></tr><tr><td>  eight </td><td>  HBA for JBOD connection </td><td>  Broadcom HBA 9400-8e Tri-Mode Storage Adapter </td><td>  05-50013-01 </td><td>  four </td></tr><tr><td>  9 </td><td>  Ethernet patchcord </td><td>  Ethernet patch cord for cache sync 0.5m </td><td></td><td>  one </td></tr><tr><td>  ten </td><td>  Cable for cache sync </td><td>  Mellanox passive copper cable, VPI, EDR 1m </td><td>  MCP1600-E001 </td><td>  2 </td></tr><tr><td>  eleven </td><td>  HBA for host connection </td><td>  Mellanox ConnectX-4 VPI adapter card, EDR IB (100Gb / s), dual-port QSFP28, PCIe3.0 x16 </td><td>  MCX456A-ECAT </td><td>  2 </td></tr><tr><td>  12 </td><td>  SAS cable </td><td>  Ultrastar Data102 Cable IO HD mini-SAS to HD mini-SAS 2m 2Pack storage enclosure </td><td></td><td>  eight </td></tr><tr><td>  13 </td><td>  Jbod </td><td>  Ultrastar Data102 </td><td></td><td>  one </td></tr><tr><td>  14 </td><td>  RAIDIX </td><td>  RAIDIX 4.6 DC / NAS / iSCSI / FC / SAS / IB / SSD-cache / QoSmic / SanOpt / Extended 5 years support / unlimited disks / </td><td>  RX46DSMMC-NALL-SQ0S-P5 </td><td>  one </td></tr></tbody></table></div><p>Source: <a href="https://habr.com/ru/post/354340/">https://habr.com/ru/post/354340/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../354328/index.html">HOPE X. Conference "Breaking the elevator: from the basement to the penthouse." Part 1. "Elevator equipment"</a></li>
<li><a href="../354332/index.html">Edimax Office1-2-3 and what it is eaten with</a></li>
<li><a href="../354334/index.html">How we build DevOps in a team of 125 developers</a></li>
<li><a href="../354336/index.html">Technical diary: the second half year of the development of a new mobile PvP</a></li>
<li><a href="../354338/index.html">Miners will not pass</a></li>
<li><a href="../354342/index.html">Internship for atypical programmers</a></li>
<li><a href="../354344/index.html">Quest to eliminate heart arrhythmia</a></li>
<li><a href="../354346/index.html">HR Meetup. How to survive the explosive growth of the IT team</a></li>
<li><a href="../354348/index.html">Development of load scripts for browser / mobile games. Part 1</a></li>
<li><a href="../354350/index.html">Elastic opens X-Pack source code</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>