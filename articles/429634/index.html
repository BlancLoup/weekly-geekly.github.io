<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>This is how the search for borrowing in Antiplagiat is arranged.</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="We have already told you about interesting statisticians of texts , reviewed articles on the use of autocoders in text analysis , surprised us with ou...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>This is how the search for borrowing in Antiplagiat is arranged.</h1><div class="post__text post__text-html js-mediator-article">  We have already told you about interesting <a href="https://habr.com/company/antiplagiat/blog/413361/">statisticians of texts</a> , <a href="https://habr.com/company/antiplagiat/blog/418173/">reviewed articles on the use of autocoders in text analysis</a> , surprised us with our fresh <a href="https://habr.com/company/antiplagiat/blog/354142/">search</a> algorithms <a href="https://habr.com/company/antiplagiat/blog/354142/">for translated borrowings</a> and <a href="https://habr.com/company/antiplagiat/blog/422941/">paraphrase</a> .  I decided to continue our corporate tradition and, first, to begin an article with ‚ÄúT‚Äù, and secondly, to tell: <br><br><ul><li>  how to quickly find a paragraph of text among hundreds of millions of articles; </li><li>  what the document turns into after downloading to the Antiplagiat system, and what to do with it further; </li><li>  how the report is formed, which almost no one looks, but it would be worth it; </li><li>  how to index not everything, but enough. </li></ul><br><img src="https://habrastorage.org/webt/-k/m9/wb/-km9wbr1u9eh1p8m8uxac-uo2bu.jpeg"><br><a name="habracut"></a><br><br><h2>  How it all began </h2><br>  In 2005, the rector of one of the major Moscow universities came to us at <a href="http://forecsys.ru/">Forecsys</a> for solving a very serious problem - in schools, students handed over totally written off diplomas and term papers.  We took several hundred works of excellent students and searched them on the net with simple queries.  More than half of the <s>‚Äúhonors pupils‚Äù</s> turned out to be swindlers who downloaded a diploma from the Internet and replaced only the title page.  More than half the best students, Karl!  What happened to ordinary students is difficult to even imagine.  The easiest job was to search for a request containing words with "ochepyatkami."  We have become clear the scale of the disaster.  It was necessary to urgently solve something.  Foreign English-speaking universities by that time already used decisions on searching for borrowings, but for some reason no one checked the work in Russian. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Foreign players did not want to adapt their decisions under the Russian language.  As a result, on March 17, 2005, the development of the first domestic system for borrowing search was launched.  The word "Antiplagiat" was coined a little later, and the domain antiplagiat.ru was registered on April 28, 2005.  We planned to release the site by September 1, 2005, but, as is often the case with programmers, we didn‚Äôt have time to do so.  The official birthday of our company is the day when antiplagiat.ru received its first users, namely, on September 4th.  You know, I am even happy about this, because during the corporate party on the occasion of the birthday of the company, everyone can safely celebrate, and not worry about, the first school day of their children. <br><br>  But something I digress.  In 2005, we created a kind of search engine in which, unlike Yandex and Google, the query is not two or three words, but a whole text consisting of several sentences.  Therefore, it is reasonable to use ‚ÄúAnti-plagiarism‚Äù if you have text from 1000 characters (about half a page). <br><br>  During the development of the service, a prototype was made on php (web-part) and Microsoft SQL Server (search engine).  It immediately became clear that this would not take off and would slowly work on several million documents.  Therefore, I had to cut my search engine.  Now the system is written in C # and python, uses PostgreSQL and MongoDB (in fact, a lot more, but more on that in the next article).  Our search engine is still completely self-developed.  <s>Put likes</s> Write in the comments if you want to learn about the history of the system, changing the processes of the company and the hardware on which Antiplagiat worked at different points in your life, and it works now. <br><br>  The word that gave the name of the company, has now become a household word.  Often in the search engine you can find such expressions as ‚Äúcheck for anti-plagiarism‚Äù, ‚Äúincrease anti-plagiarism‚Äù.  Anyone who is somehow connected with the area of ‚Äã‚Äãborrowing in Russia and the near abroad, trying to use the word "anti-plagiarism" to raise in the search results.  We are often asked about other "anti-plagiarism".  So, Antiplagiat is one, it is a trademark and name of our company. <br><br>  At the very beginning of the implementation of the borrowing search service, we decided that we would work with the text as a sequence of characters.  Immediately, various semantic constructions from texts, the search for meanings, the analysis of sentences, etc. were rejected.  The solution chosen by us gives two huge advantages - high search speed and relatively small amount of search indexes. <br><br>  To date, there are three products in our line.  They differ in functionality, but they contain in their basis the same principle of operation of the borrowing search.  In this article, I will talk about how our classic borrowing search is arranged - the functionality that became the basis of the service from the very beginning and has not changed conceptually until now.  The borrowing search scheme, as you see in the image, is simple and uncomplicated, like drawing an owl.  First we get the document from the user, then we extract the text from it.  Next we look for borrowings in this text, we get ‚Äúrevisions‚Äù (as we call the report on one search module) and, finally, we collect revisions into one big report, which we show as a result to the user. <br><br><img src="https://habrastorage.org/webt/ry/2x/5z/ry2x5zabqzvcxxedldusd5ozgz0.png"><br><br>  Let's see how it all happens in detail. <br><br><h2>  Extract text </h2><br>  First of all, Antiplagiat is a <b>text-</b> only borrowing search service, which means we need to extract text from all documents in order to continue working with it.  The system supports the ability to download documents in docx, doc, txt, pdf, rtf, odt, html, pptx and several more (never used) formats.  Also, you can upload all these documents in archives (7z, zip, rar).  This method was popular when we did not have the opportunity to download several documents at once through a web interface.  Below is a graph of the popularity of downloadable document formats in the corporate part of our system.  It shows how, over several years, doc is supplanted by docx, and the share of pdf is gradually increasing.  If you do not consider txt (extracting text for it is trivial), then for us the most enjoyable is pdf.  Abroad pdf is a de facto standard, articles are published in it, student papers are being prepared.  According to our statistics, pdf is gradually gaining popularity in Russia and the CIS countries.  We ourselves are promoting this format to the masses, recommending to upload documents in it. <br><br>  We have limited the document download formats for private clients to pdf and txt, which is why we have reduced resource consumption, reduced the cost of supporting a free service.  You do need to check the text and not test the system?  So what's the difference in what format to download it? <br><br>  The next easiest way to extract text is docx, because, in fact, it is a zip-archive with xml inside, it is rather simple to process it, and much can be done at a low level. <br><br>  The most difficult for us is doc.  This format has been closed for a long time, and now there is a bunch of its implementations.  The latest Microsoft Word, which did not support .docx (albeit through the Microsoft Office Compatibility Pack), was already released 20 years ago and was included in Microsoft Office 97. The format uses OLE inside itself, which later grew in COM and ActiveX, everything is binary, incompatible in some places between versions.  In general, a terrible dream of a modern programmer.  It‚Äôs good that the .doc format is gradually disappearing from the scene.  I think the time has come for us to help him retire.  Soon we will purposefully warn users that this format is outdated. <br><br><img src="https://habrastorage.org/webt/zb/kp/xh/zbkpxhnz4zfrjx_pi1s-oqnfupi.png"><br><br>  So back to the report.  We received the file and began to extract the text.  Together with the text, the system also extracts the positions of words on the pages, in order to be able to show our users the markup of the borrowing report on the document itself.  In addition, at the same stage, we are looking for technical bypasses of Antiplagiat. <br><br>  As soon as ‚ÄúAnti-plagiarism‚Äù appeared, showing the percentage of originality, there were also those who wanted to undergo a borrowing test with minimal effort, as well as people offering such a service for money.  The problem is that the numeric parameter asks for evaluation.  It's so easy - instead of reading the work using the system as a tool, do not read it, but evaluate it by the percentage of originality!  It is this trouble that gave rise to such a direction as the tuning of works (a change in the text in order to increase the percentage of originality of the work).  Read more about problems in university processes in the article <a href="http://www.unkniga.ru/innovation/tehnology/7119-o-poryadkeobnaruzheniya-zaimstvovaniy-v-rossijskihvuzah.html">"On the practice of detecting borrowings in Russian universities</a> . <a href="http://www.unkniga.ru/innovation/tehnology/7119-o-poryadkeobnaruzheniya-zaimstvovaniy-v-rossijskihvuzah.html">"</a> <br><br>  In foreign systems, searching for the problem of detecting technical rounds and countering them is practically not worth it.  The fact is that a very tough punishment will follow the one detected by the ‚Äúfeint with ears‚Äù - a deduction, and an indelible stain on scientific reputation, which is incompatible with a further career.  In our case, the situation before the comic is simple: ‚ÄúOh, this is a system that screwed up something!‚Äù, ‚ÄúOh, it's not me, it is itself!‚Äù.  The student will most likely be sent to rework.  The fact is that write off, alas, is not something embarrassing. <br><br>  But again distracted.  Another way to extract text is OCR.  We print the document on a virtual printer, and then we recognize it.  Read more about this in the article <a href="http://www.unkniga.ru/innovation/tehnology/8017-raspoznavanie-izobrazheniy-na-sluzhbe-u-antiplagiata.html">"Recognition of images in the service of" Antiplagiat "</a> . <br><br>  Now a little of our story about text extraction.  First, we extracted the texts using IFilters.  They are slow, only under Windows, and do not return formatting information (it‚Äôs not clear where the white text is on a white background, you can‚Äôt markup the borrowing blocks directly in the user's document).  We thought that these problems would be solved if we began to use paid libraries, but even here we found limitations: still under Windows, they do not see formulas, sometimes they fall on specially prepared documents (different libraries on different!).  The next idea was to OCR all incoming documents, but this approach is very resource intensive (processing only 10 pages per minute on a single core), and the text is not precisely extracted in some places. <br><br>  We did not find the silver bullet, although a couple of times we thought that this was it, Happiness.  However, later, having lived a little with this, we understood that it was an Experience again.  Extracting text balances on a thin line between performance (you need to extract text from hundreds of documents per minute), reliability (you need to extract text from everything), functionality (formatting, workarounds, this is all).  Now all of the above and a little more work for us.  We are constantly experimenting with this area and continue to look for our happiness. <br><br>  The text is extracted, bypasses are found and partially eliminated, we go to seek borrowing! <br><br><h2>  Borrowing search </h2><br>  The idea implemented in the search procedure was proposed by Ilya Segalovich and Yuri Zelenkov (you can read, for example, in the article: <a href="http://rcdl2007.pereslavl.ru/papers/paper_65_v1.pdf">Comparative analysis of methods for determining fuzzy duplicates for Web documents</a> ).  I'll tell you how it works for us.  Take, for example, the sentence: "Decree of the President of the Russian Federation of May 7, 2012 N 596" On the long-term state economic policy "." <br><br><ol><li>  We break sentences into words, throw out numbers, punctuation, stop words.  We lemmatize (normalize) all the words. </li><li>  Turning words into integers by hashing, we get an array of numbers. </li><li>  We take the first three hashes, then 2, 3, 4th hash, then 3, 4, 5th and so on until the end of the array of hashes.  These are shingles - tiles.  This method got its name because of such a tiled overlap of sets.  Each tile is merged into one object and hashed again. </li><li> Sort the resulting numbers, we get an ordered array of integers.  This is the basis for the search. </li></ol><br><img src="https://habrastorage.org/webt/wu/l1/h5/wul1h5pywezeyiglohu_v7so4jm.png"><br><br>  Now for the search, we need a magic function that, according to such a list of hashes, turns documents, ranked by decrease in the number of matched hashes, into a source document.  This function should work quickly, because  we want to search for billions of documents.  In order to quickly find such a set, we need a reverse index, which according to the hash returns a list of documents in which this hash exists.  We have implemented such a giant hash table.  Unlike our older search engines, we store this table in ssd, not in memory.  Such performance is enough for us.  Index search takes a small part of the time from the entire document processing cycle.  See how the search goes: <br><br>  Stage 1. Search by index <br><br>  For each hash of the query text, we obtain a list of identifiers of the source documents in which it occurs.  Next, we rank the list of identifiers of source documents by the number of hashes encountered from the query text.  We get a ranked list of documents candidates for the source of borrowing. <br><br>  Stage 2. Build a revision <br><br>  For a large text, the request of candidates may be about 10 thousand. This is still a lot to compare each document with the text of the request.  We act greedily, but decisively.  We take the first source document, make a comparison with the text-request and exclude from all other candidates those hashes that were already in this first document.  Remove from the list of candidates those who have zero hashes, re-sort the candidates according to the new number of hashes.  We take the first document from the new list, compare it with the source text, delete the hashes, delete the zero candidates, re-sort the candidates.  We do so 10-20 times, usually this is enough for the list to dry up or only those documents that have a match on several hashes remain in it. <br><br>  Using hashes of words allows us to perform comparison operations faster, save on memory and store not the texts of source documents, but their digital snapshots (TextSpirit, as we affectionately call them) obtained during indexing, thereby not violating copyright.  Selection of specific fragments of borrowing is done using the suffix tree. <br><br>  As a result of checking with a single search module, we get a revision in which there is a list of sources, their metadata and coordinates of borrowing blocks relative to the query text. <br><br><h2>  Report assembly </h2><br>  By the way, what if one of the 10-15 modules did not respond in time?  We are looking for collections of the RSL, eLibrary and the Guarantor.  These search modules are located on the territory of third-party organizations and cannot be transferred to our site for copyright reasons.  The point of failure here can always be a communication channel and various force majeure in data centers that are not controlled by us.  On the one hand, borrowing can be found in any search module, on the other hand, if one of the system components is not available, you can worsen the search quality, but give most of the result, warning the user that the result for some search modules is not ready yet.  Which option would you apply?  We use both of these options as appropriate. <br><br><img src="https://habrastorage.org/webt/wr/s9/kv/wrs9kvme5ob7myy4kpij6xo0ozk.png"><br><br>  Finally, all revisions are received, we begin to build the report.  Here an approach similar to the preparation of one audit is used.  It seems to be nothing complicated, but there are interesting problems here.  We have borrowings of two types.  Greens denote ‚ÄúCitations‚Äù - correctly issued (according to GOST) citations from the ‚ÄúCitation‚Äù module, expressions like ‚Äúrequired to be proved‚Äù from the ‚ÄúCommonly used expressions‚Äù module, normative legal documents from the Garant and Lekspro bases.  Orange marks all other borrowings.  Greens take precedence over orange, unless they are included entirely in the orange block. <br><br>  As a result, the report can be compared with the text printed on paper lying on the table, over which multicolored stripes (blocks of borrowing and citations) are fancifully overlapping each other.  What we see above is a report.  We have two indicators for each source: <br><br>  <b>The share in the report</b> is the ratio of the volume of borrowings, which is taken into account from this source, to the total volume of the document.  If the same text was found in several sources, then it is taken into account only in one of them.  If you change the configuration of the report (enable or disable sources), this indicator of the source may change.  In total, it gives the percentage of borrowings and citations (depending on the color of the source). <br><br>  <b>Text share</b> - the ratio of the volume borrowed from a given text source to the total document volume.  Shares in the text by source summing up does not make sense, it is easy to get 146% or even more.  This indicator does not change when the report changes. <br><br>  Naturally, the report can be edited.  This is a special function so that the expert checking the work turns off the borrowing of the author‚Äôs own work (it may reveal that this fragment is not only in the author‚Äôs own work, but also somewhere else) and the individual borrowing blocks changed the source type from borrowing on citation.  As a result of editing the report, the expert gets the real value of borrowing.  Any work for verification must be read.  It is convenient to do this by looking at the original document view, in which the blocks of borrowing are marked, and immediately, as you read, edit the report.  Unfortunately, this is quite a logical action, not all is committed, many are content with a percentage of originality, even without looking at the report. <br><br>  However, let‚Äôs go back a step and find out what falls into the index of the Internet search module created by Antiplagiat. <br><br><h2>  Internet indexing </h2><br>  Anti-plagiarism is largely focused on student work, scientific publications, final qualifying papers, dissertations, etc.  We index the Internet directionally - we are looking for large accumulations of scientific texts, abstracts, articles, theses, scientific journals, etc.  Indexing is as follows: <br><br><ol><li>  Our robot comes, it seems, and, guided by robots.txt (we have a good robot), loads documents with a reasonable load on each host (there are hundreds of sites working at the same time, so we can wait some time between page loads); </li><li>  The robot sends the document and its metadata to the processing queue, the text is extracted from the document; </li><li>  <b>The text is analyzed for ‚Äúquality‚Äù</b> - as you remember from the article about the dump, we are able to determine the genre of the document, add here simple heuristics for the volume and we understand whether the text came to us or some kind of rubbish; </li><li>  Quality text goes further and turns into hashes.  Hashes and metadata are sent to the main index of the Internet; </li><li>  We compare the incoming text with the previously indexed text.  <b>A newbie is added only if he is really new</b> , i.e.  90% of its hashes are not contained entirely in some other already indexed text.  If we already have the document, we add the url of this document to the attributes of our archive. </li></ol><br>  Thus, we index quality texts, and all indexed texts are significantly different.  The growth in the volume indexed on the Internet is shown in the figure below.  Now, on average, we add 15-20 million documents per month to the index. <br><br><img src="https://habrastorage.org/webt/ja/hb/3k/jahb3kuu2dmauiesunop3aih4vc.png"><br><br>  Notice that nowhere is the procedure for deleting from the index described?  And she is not!  We basically do not remove documents from the index.  We believe that if we were able to see something on the Internet, then other people could see this text and use it in one way or another.  In this regard, there is an interesting statistic of what was once on the Internet, and now it is no longer there.  Yes, just imagine, the expression ‚ÄúGone on the Internet will remain there forever,‚Äù is not true!  Something disappears from the Internet forever.  Are you curious about our statistics on this issue? <br><br><h2>  Conclusion </h2><br>  It's amazing how technical decisions made more than 10 years ago still remain relevant.  We are now preparing to release the 4th version of the index, it is faster, more technologically, better, but it is based on all the same solutions.  New directions of search have appeared - transferable borrowing, paraphrasing, but even there our index finds use, performing even a small but important part of the work. <br><br>  Dear readers, that you would be interested to learn more about our service? </div><p>Source: <a href="https://habr.com/ru/post/429634/">https://habr.com/ru/post/429634/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../429624/index.html">How NASA will use robots to create rocket fuel from Martian soil</a></li>
<li><a href="../429626/index.html">Hackers of the genome have demonstrated that no DNA will be anonymous</a></li>
<li><a href="../429628/index.html">How to create a chat bot for VKontakte using Python, Django and webhook</a></li>
<li><a href="../429630/index.html">"Monsters in games or make fear diverse"</a></li>
<li><a href="../429632/index.html">Causes of failure to hosters when added to directories</a></li>
<li><a href="../429636/index.html">Installing a 3CX PBX in the Amazon Lightsail Cloud</a></li>
<li><a href="../429638/index.html">Dump, extract: the architecture of complex chat bots</a></li>
<li><a href="../429640/index.html">Let's remove quaternions from all 3D engines.</a></li>
<li><a href="../429642/index.html">Security Week 46: let's update something</a></li>
<li><a href="../429644/index.html">Dimmable Spot GX53 with adjustable illumination angle</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>