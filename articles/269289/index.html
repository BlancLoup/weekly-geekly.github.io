<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How can you make a fault-tolerant storage system of data from domestic servers</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Cluster nodes: Etegro domestic server (2 AMD Opteron 6320, 16 GB RAM, 4 HDD) 

 Data storage systems used now in practice in Russia are conventionally...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How can you make a fault-tolerant storage system of data from domestic servers</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/728/e0d/d09/728e0dd0960b4e49a9e317fb687ee201.jpg"><br>  <i>Cluster nodes: Etegro domestic server (2 AMD Opteron 6320, 16 GB RAM, 4 HDD)</i> <br><br>  Data storage systems used now in practice in Russia are conventionally divided into three categories: <br><ul><li>  Very expensive high-end storage systems; </li><li>  Midrange arrays (tearing, HDD, hybrid solutions); </li><li>  And economical clusters based on SSD arrays and HDD arrays from ‚Äúeveryday‚Äù disks, often assembled by hand. </li></ul><br>  <b>And not the fact that the latest solutions are slower or less reliable than the Highland.</b>  Just use a completely different approach, which is not always suitable for the same bank, for example.  But it‚Äôs great for almost all mid-sized businesses or cloud solutions.  The general meaning is that we take a lot of cheap ‚Äúdomestic‚Äù hardware and connect them to a fault-tolerant configuration, compensating for the problems with the correct virtualization software.  An example is domestic RAIDIX, the <a href="http://habrahabr.ru/company/croc/blog/246155/">creation of</a> Petersburg colleagues. <br><br>  And so EMC came to this market, known for its damn expensive and reliable hardware with software that allows you to easily raise a VMware farm, a virtual storage system, and any application on the same x86 servers.  And the story began with the servers of Russian production. <br><a name="habracut"></a><br><h4>  Russian servers </h4><br>  The original idea was to take our domestic iron and pick up on it the pieces of infrastructure that are missing for import substitution.  For example, the same hotbed of virtual machines, VDI servers, butt storage systems and application servers. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The Russian server is such a miracle piece of hardware, which is 100% assembled in Russia and is 100% Russian by all standards.  In practice, they carry individual parts from China and other countries, buy domestic wires and assemble them in the territory of the Russian Federation. <br><br>  It turns out not so bad.  You can work, although the reliability is lower than the same HP.  But this is offset by the price of iron.  Then we come to a situation in which not the most stable hardware should be compensated by a good control software.  At this stage, we began experimenting with EMC ScaleIO. <br><br>  The experiment turned out to be good, during the experiments it turned out that iron is not very important.  That is - you can replace the proven, from well-known brands.  It turns out a little more expensive, but less problems with the service. <br><br>  As a result, the concept has changed: now we are simply talking about the benefits of ScaleIO on various hardware, including (and above all) from the lower price segment. <br><br><h4>  But to the point: test results </h4><br>  Here is the principle of operation of ScaleIO (http://habrahabr.ru/company/croc/blog/248891/) - we take servers, stuff them to capacity with disks (for example, the same SSD pieces that were intended to replace the HDD at one time) and we combine all this into a cluster: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0ac/1a5/63c/0ac1a563cb9e3befdcbba6b51fe4a1b2.png" alt="image"><br><br>  The configuration we tested in the lab this time is the integration of EMC ScaleIO and VMware.  Colleagues from Etegro kindly lent us 4 servers with 2 AMD Opteron processors (tm) Processor 6320 and 16 GB of RAM in each.  Each had 4 discs.  Not the most capacious configuration, I would prefer servers on 25 2.5-inch disks, but I have to work with what I have and not with what I want. <br><br>  Here are the servers in the rack: <br><br><img src="https://habrastorage.org/files/f83/f86/3c4/f83f863c42b94e94903a0b7cea2c1c48.jpg"><br><br>  <b>I divided the entire volume of disks in the server into 3 parts:</b> <br><ul><li>  10 Gb for ESX.  That is quite enough. </li><li>  30 GB for internal use ScaleIO, but more on that later. </li><li>  The rest of the place will be given through ScaleIO. </li></ul><br>  The first thing to do is install VMware.  ESX we will put on the network, so faster.  The task is not new, but a virtual machine with a PXE server has long taken a well-deserved place in my laptop. <br><br><img src="https://habrastorage.org/files/2e4/547/d11/2e4547d113744d25a9e3f8aae798c42d.jpg"><br><br>  As you can see, there is a lot of test equipment in our lab.  On the right, there are 4 more racks and 12 more on the first floor.  We can assemble almost any stand at the request of the customer. <br><br>  The technology of Software Defined Storage is such that each node can request a sufficiently large amount of information from other nodes.  This fact means that the bandwidth and response time of the BackEnd network is very important in such solutions.  10G Ethernet is well suited for this task, and the Nexus switches are already in this rack. <br><br>  Here is a diagram of the resulting solution: <br><br><img src="https://habrastorage.org/files/ebc/2af/411/ebc2af4112c04c42927a53e21ce95648.jpeg"><br><br>  Installing ScaleIO on VMware is very easy.  In fact, it consists of 3 points: <br><ol><li>  Install the plugin for vSphere; </li><li>  We open the plugin and run the installation, in the same paragraph you need to answer 15-20 questions of the wizard. </li><li>  We look, how ScaleIO independently creeps on servers. </li></ol><br><br><img src="https://habrastorage.org/files/01a/5b0/e4a/01a5b0e4abf54558a923e3e91ddcbc64.png"><br><br>  If the wizard is completed without errors, a special partition for ScaleIO will appear in vSphere, in which you can create moons and give them to ESX servers. <br><br><img src="https://habrastorage.org/files/664/334/6bc/6643346bce564ffea357400a259a606a.png"><br><br>  Or you can use the standard ScaleIO console by installing it on a local computer. <br><br><img src="https://habrastorage.org/files/6ea/030/0ba/6ea0300ba51743e2935d7dd2ad152671.png"><br><br>  Now a little test for performance.  I created a virtual machine with 2 disks of 50Gb on each host and divided them evenly across datastores.  Load generated using IOmeter.  Here are the maximum results that I managed to get on a load of 100% random, 70% read, 2k. <br><br><img src="https://habrastorage.org/files/87e/b59/efd/87eb59efd36b45aa83b96c5902d66d9b.png"><br>  <i>3000 application IO from 4 servers with SAS disks is a good result.</i> <br><br>  As an entertainment, I tried to "fill up" the system, pulling the nodes in different sequences and at different intervals.  If you pull the nodes one at a time and give ScaleIO enough time for a rebuild, then the virtual machines will work even if only one node remains.  If you turn off 3 nodes per minute, for example, access to the shared space will stop until those nodes are turned back on.  Data becomes available, and the array performs data integrity and rebuild (if necessary) in the background.  Thus, the solution is obtained sufficiently reliable in order to use it on combat missions. <br><br>  Perhaps all about virtualization.  It's time to summarize. <br><br><h4>  Summary </h4><br>  <b>Pros:</b> <br><ul><li>  <b>The price of the solution</b> (processor power + memory + disks) is competitive and in many cases will be significantly lower than the servers + storage in similar mono-vendor offers. </li><li>  <b>You can use any hardware</b> and get a replacement for ‚Äúbig beautiful‚Äù storage for the same tasks.  If needed, servers can be replaced with more powerful ones without purchasing any additional licenses for ScaleIO.  It is licensed poter byte without being tied to hardware. </li><li>  <b>The solution is convergent</b> .  Virtual machines and storage on the same server.  It is very convenient for almost any medium-sized business.  Flash storage is no longer fiction on this level. </li><li>  Plus <b>it requires less space in racks</b> , less power consumption. </li><li>  <b>Good balancing</b> - even distribution of IO across all disk resources. </li><li>  <b>The solution can be spread to 2 different sites</b> by adjusting the mirroring between them at the level of a single ScaleIO cluster. </li><li>  Virtual <b>synchronization</b> can be used <b>for synchronous and asynchronous replication</b> between clusters. </li></ul><br>  <b>Minuses:</b> <br><ul><li>  <b>First, you have to apply the brain.</b>  Expensive solutions, as a rule, are good because they are implemented very quickly and require almost no special training.  Considering that ScaleIO is, in fact, a self-assembled storage system, you will have to figure out the architecture, smoke a couple of forums for optimal configuration, run an experiment on your data to select the optimal configuration. </li><li>  <b>Secondly, for redundancy and fault tolerance, you pay disk space.</b>  The conversion ratio of the raw space to user space changes depending on the configuration, and you may need more disks than you originally thought. </li></ul><br><br>  I remind you, here you can read about the <a href="http://habrahabr.ru/company/croc/blog/248891/">software part in detail</a> , and I will be happy to answer questions in the comments or mail RPokruchin@croc.ru.  And a month later, on November 26, my colleagues and I will do an open test drive with a Scale IO vacuum K√§rcher, a sledgehammer, and other things until the men say, ‚ÄúAahhh, zarraz.‚Äù  <a href="http://www.croc.ru/agentscale/form.php">Register here</a> . </div><p>Source: <a href="https://habr.com/ru/post/269289/">https://habr.com/ru/post/269289/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../269273/index.html">Algorithm for extracting information in ABBYY Compreno. Part 2</a></li>
<li><a href="../269275/index.html">Creating a MySQL database within Microsoft Azure for students</a></li>
<li><a href="../269281/index.html">Oracle, typical SQL tasks. Guaranteed choice</a></li>
<li><a href="../269285/index.html">3 ways to use the operator? .. wrong in C # 6</a></li>
<li><a href="../269287/index.html">Multiklet became more accessible</a></li>
<li><a href="../269291/index.html">Job Search Guide for MDA Specialist (and a little about hierarchy analysis method, Xcore and Sirius)</a></li>
<li><a href="../269293/index.html">Implementation of monitoring and integration testing information system using Scalatest. Part 1</a></li>
<li><a href="../269295/index.html">PCEPTPDPTE</a></li>
<li><a href="../269297/index.html">Gaming Digest: September</a></li>
<li><a href="../269299/index.html">Studying net / context in Go</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>