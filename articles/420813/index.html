<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Behind the scenes of the network in Kubernetes</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Note trans. : The author of the original article, Nicolas Leiva, is Cisco solution architect who decided to share with his colleagues, network enginee...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Behind the scenes of the network in Kubernetes</h1><div class="post__text post__text-html js-mediator-article">  <i><b>Note</b></i>  <i><b>trans.</b></i>  <i>: The author of the original article, Nicolas Leiva, is Cisco solution architect who decided to share with his colleagues, network engineers, how the Kubernetes network is designed from the inside.</i>  <i>To do this, he explores its simplest configuration in a cluster, actively applying common sense, his knowledge of networks and standard Linux / Kubernetes utilities.</i>  <i>It turned out volumetric, but very clearly.</i> <br><br><img src="https://habrastorage.org/webt/gr/qw/d4/grqwd4putslwaijltw9yojfzjes.png"><br><br>  Aside from the fact that the Kelsey Hightower‚Äôs <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes The Hard Way</a> manual just works ( <a href="https://github.com/nleiva/kubernetes-the-hard-way">even on AWS!</a> ), I liked that the network was kept clean and simple;  and this is a great opportunity to understand the role, for example, of the Container Network Interface ( <a href="">CNI</a> ).  Having said that, I would add that the Kubernetes network is not really very intuitive, especially for beginners ... and also do not forget that "such a thing as a network for containers <a href="https://www.youtube.com/watch%3Fv%3Dt98CX8Tberc">simply does not exist</a> ." <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Although there are already some good materials on this topic (see links <a href="https://github.com/nleiva/kubernetes-networking-links">here</a> ), I could not find such an example that I would combine everything I needed with the conclusions of commands that network engineers love and hate so much, demonstrating what is actually happening behind the scenes.  That's why I decided to collect information from a variety of sources - I hope this will help you better understand how everything is connected with each other.  This knowledge is important not only to check yourself, but also to simplify the process of diagnosing problems.  You can follow this example in your cluster from <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes The Hard Way</a> : all IP addresses and settings are taken from there (as at the state of commits in May 2018, before using <a href="https://nabla-containers.github.io/">Nabla containers</a> ). <br><br>  And we will start from the end, when we have three controllers and three working nodes: <br><br><img src="https://habrastorage.org/webt/am/vs/6j/amvs6jnhsuxzwhyoyod6vjk8cby.png"><br><br>  You may notice that there are also at least three private subnets here!  A little patience, and they will all be considered.  Remember that although we refer to very specific IP prefixes, they are simply taken from <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes The Hard Way</a> , so they have only local significance, and you are free to choose any other address block for your environment in accordance with <a href="https://tools.ietf.org/html/rfc1918">RFC 1918</a> .  For the case of IPv6 there will be a separate article in the blog. <br><br><h2>  Host Network (10.240.0.0/24) </h2><br>  This is an internal network, of which all nodes are part.  It is determined by the <code>--private-network-ip</code> flag in the <a href="https://cloud.google.com/sdk/gcloud/reference/compute/instances/create">GCP</a> or by the <code>--private-ip-address</code> option in <a href="https://docs.aws.amazon.com/cli/latest/reference/ec2/run-instances.html">AWS</a> when allocating computing resources. <br><br><h3>  Initializing controller nodes in GCP </h3><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0 1 2; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> gcloud compute instances create controller-<span class="hljs-variable"><span class="hljs-variable">${i}</span></span> \ <span class="hljs-comment"><span class="hljs-comment"># ... --private-network-ip 10.240.0.1${i} \ # ... done</span></span></code> </pre> <br>  ( <a href="https://gist.github.com/nleiva/6fa403a643b665c0bb36f4dc9744ba1e"><code>controllers_gcp.sh</code></a> ) <br><br><h3>  Initializing controller nodes in AWS </h3><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0 1 2; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-built_in"><span class="hljs-built_in">declare</span></span> controller_id<span class="hljs-variable"><span class="hljs-variable">${i}</span></span>=`aws ec2 run-instances \ <span class="hljs-comment"><span class="hljs-comment"># ... --private-ip-address 10.240.0.1${i} \ # ... done</span></span></code> </pre> <br>  ( <a href="https://gist.github.com/nleiva/4a57c206899659c10348fdf14e84e07d"><code>controllers_aws.sh</code></a> ) <br><br><img src="https://habrastorage.org/webt/gt/cj/p6/gtcjp6fgkqbv1nvs2ea9d9hhueo.png"><br><br>  Each instance will have two IP addresses: private from the host's network (controllers - <code>10.240.0.1${i}/24</code> , workers - <code>10.240.0.2${i}/24</code> ) and public, assigned by the cloud provider, which we will talk about later how to get to <code>NodePorts</code> . <br><br><h3>  Gcp </h3><br><pre> <code class="bash hljs">$ gcloud compute instances list NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS controller-0 us-west1-c n1-standard-1 10.240.0.10 35.231.XXX.XXX RUNNING worker-1 us-west1-c n1-standard-1 10.240.0.21 35.231.XX.XXX RUNNING ...</code> </pre> <br><br><h3>  AWS </h3><br><pre> <code class="bash hljs">$ aws ec2 describe-instances --query <span class="hljs-string"><span class="hljs-string">'Reservations[].Instances[].[Tags[?Key==`Name`].Value[],PrivateIpAddress,PublicIpAddress]'</span></span> --output text | sed <span class="hljs-string"><span class="hljs-string">'$!N;s/\n/ /'</span></span> 10.240.0.10 34.228.XX.XXX controller-0 10.240.0.21 34.173.XXX.XX worker-1 ...</code> </pre> <br>  All nodes must be able to ping each other if the <a href="">security policies are correct</a> (and if <code>ping</code> installed on the host). <br><br><h2>  Network of hearth (10.200.0.0/16) </h2><br>  This is the network in which they live.  Each working node uses a subnet of this network.  In our case, <code>POD_CIDR=10.200.${i}.0/24</code> for a <code>worker-${i}</code> . <br><br><img src="https://habrastorage.org/webt/6i/yz/ih/6iyzihbxbzwugs4amvhfa9ysp5s.png"><br><br>  To understand how everything is set up, let's take a step back and look at the <a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/">network model Kubernetes</a> , which requires the following: <br><br><ul><li>  All containers can communicate with any other containers without using NAT. </li><li>  All nodes can communicate with all containers (and vice versa) without using NAT. </li><li>  The IP that sees the container should be the same as others see it. </li></ul><br>  Implementation of all this is possible in different ways, and Kubernetes passes the network configuration to the <a href="">CNI plugin</a> . <br><br><blockquote>  ‚ÄúThe CNI plugin is responsible for adding a network interface in the container's <b>network namespace</b> (for example, one end of a <b>veth pair</b> ) and making necessary changes on the host (for example, connecting the second end of the veth to the bridge).  Then he must assign the IP interface and configure the routes according to the section ‚ÄúIP Address Management‚Äù by calling the necessary IPAM plugin. ‚Äù  <i>(from <a href="">Container Network Interface Specification</a> )</i> </blockquote><br><img src="https://habrastorage.org/webt/5q/fs/vw/5qfsvwg2iuduy0q3doco11hbf-g.png"><br><br><h3>  Network Namespace </h3><br><blockquote>  ‚ÄúThe namespace wraps the global system resource into an abstraction, which is visible to the processes in this namespace in such a way that they have their own isolated copy of the global resource.  Changes in the global resource are visible to other processes in this namespace, but not visible to other processes. ‚Äù  <i>( <a href="http://man7.org/linux/man-pages/man7/namespaces.7.html">from namespaces man page</a> )</i> </blockquote><br>  Linux provides seven different namespaces ( <code>Cgroup</code> , <code>IPC</code> , <code>Network</code> , <code>Mount</code> , <code>PID</code> , <code>User</code> , <code>UTS</code> ).  Network ( <code>Network</code> ) namespaces ( <code>CLONE_NEWNET</code> ) define network resources that are available to the process: ‚ÄúEach network namespace has its own network devices, IP addresses, IP routing tables, <code>/proc/net</code> directory, port numbers, and so on‚Äù <i>( from the article " <a href="https://lwn.net/Articles/531114/">Namespaces in operation</a> ")</i> . <br><br><h3>  Virtual Ethernet devices (Veth) </h3><br><blockquote>  ‚ÄúVirtual network pair (veth) offers an abstraction in the form of a‚Äú pipe ‚Äùthat can be used to create tunnels between network name spaces or to create a bridge to a physical network device in another network space.  When the namespace is freed, all veth devices in it are destroyed. ‚Äù  <i>(from the <a href="http://man7.org/linux/man-pages/man7/network_namespaces.7.html">network namespaces man page</a> )</i> </blockquote><br>  Let's go down to the ground and see how it all relates to the cluster.  First, the <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">network plug-ins</a> in Kubernetes are diverse, and the CNI plug-ins are one of them ( <a href="https://kubernetes.io/blog/2016/01/why-kubernetes-doesnt-use-libnetwork/">why not CNM?</a> ).  <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">Kubelet</a> on each node tells the container's executable environment which <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">network plugin to</a> use.  The Container Network Interface ( <a href="">CNI</a> ) is located between the executable container environment and the network implementation.  And already CNI plugin configures the network. <br><br><blockquote>  ‚ÄúThe CNI plugin is selected by passing the command line <code>--network-plugin=cni</code> to Kubelet.  Kubelet reads a file from <code>--cni-conf-dir</code> (the default is <code>/etc/cni/net.d</code> ) and uses the CNI configuration from this file to configure the network for each hearth. ‚Äù  <i>(from <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">Network Plugin Requirements</a> )</i> </blockquote><br>  The real binaries of the CNI plugin are in <code>-- cni-bin-dir</code> (the default is <code>/opt/cni/bin</code> ). <br><br>  Please note that the call options for <a href=""><code>kubelet.service</code></a> include <code>--network-plugin=cni</code> : <br><br><pre> <code class="plaintext hljs">[Service] ExecStart=/usr/local/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --network-plugin=cni \\ ...</code> </pre> <br>  First of all, Kubernetes creates a network namespace for the hearth, even before calling any plugins.  This is implemented using a special <code>pause</code> container, which ‚Äúserves as the‚Äú parent container ‚Äùfor all pod containers‚Äù <i>(from the article ‚Äú <a href="https://www.ianlewis.org/en/almighty-pause-container">The Almighty Pause Container</a> ‚Äù)</i> .  Kubernetes then executes the CNI plugin to attach the <code>pause</code> container to the network.  All pod containers use the network namespace ( <code>netns</code> ) of this <code>pause</code> container. <br><br><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.1", "name": "bridge", "type": "bridge", "bridge": "cnio0", "isGateway": true, "ipMasq": true, "ipam": { "type": "host-local", "ranges": [ [{"subnet": "${POD_CIDR}"}] ], "routes": [{"dst": "0.0.0.0/0"}] } }</code> </pre> <br>  The <a href="">configuration</a> used <a href="">for CNI</a> indicates the use of the <code>bridge</code> plug-in to configure the Linux software bridge (L2) in the root namespace called <code>cnio0</code> (the <a href="">default name</a> is <code>cni0</code> ), which acts as a gateway ( <code>"isGateway": true</code> ). <br><br><img src="https://habrastorage.org/webt/bo/to/jp/botojpqu0f7fascfrbk-gen27a8.png"><br><br>  A veth-pair will also be configured to connect the hearth to the newly created bridge: <br><br><img src="https://habrastorage.org/webt/-6/tt/e7/-6tte7essirvuraiuypuln_syvm.png"><br><br>  To assign L3 information, such as IP addresses, the <a href="https://github.com/containernetworking/plugins/tree/master/plugins/ipam">IPAM plugin</a> ( <code>ipam</code> ) is called.  In this case, the <code>host-local</code> type is used, ‚Äúwhich stores the state locally on the host file system, which ensures the uniqueness of IP addresses on one host‚Äù <i>(from the <a href="https://github.com/containernetworking/plugins/tree/master/plugins/ipam/host-local"><code> host-local</code></a> )</i> .  The IPAM plugin returns this information to the previous plug-in ( <code>bridge</code> ), due to which all routes specified in the config can be configured ( <code>"routes": [{"dst": "0.0.0.0/0"}]</code> ).  If <code>gw</code> not specified, it <a href="">is taken from the subnet</a> .  The default route is also configured in the network pod names, pointing to the bridge (which is configured as the first IP subnet of the sub). <br><br>  And the last important detail: we requested masquerading ( <code>"ipMasq": true</code> ) of the traffic originating from the network podov.  In fact, we do not need NAT here, but this is the config in <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes The Hard Way</a> .  Therefore, for the sake of completeness, I must mention that the <code>bridge</code> plugin's <code>iptables</code> entries are configured for this particular example.  All packets from the pod, the recipient of which is not in the range of <code>224.0.0.0/4</code> , <a href="">will be behind NAT</a> , which does not quite meet the requirement ‚Äúall containers can communicate with any other containers without using NAT‚Äù.  Well, we'll prove why NAT is not needed ... <br><br> <a href="https://twitter.com/miekg/status/1011585886654550016/photo/1"><img src="https://habrastorage.org/webt/y-/hy/ub/y-hyubecllmzx9go5ehai4shl78.jpeg"></a> <br><br><h3>  Routing </h3><br>  Now we are ready to customize the hearth.  We will look at all the network spaces of the names of one of the working nodes and analyze one of them after creating the <code>nginx</code> deployment <a href="">from here</a> .  We use <code>lsns</code> with the <code>-t</code> option to select the desired type of namespace (i.e. <code>net</code> ): <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo lsns -t net NS TYPE NPROCS PID USER COMMAND 4026532089 net 113 1 root /sbin/init 4026532280 net 2 8046 root /pause 4026532352 net 4 16455 root /pause 4026532426 net 3 27255 root /pause</code> </pre> <br>  Using the <code>-i</code> option to <code>ls</code> we can find their inode numbers: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ls -1i /var/run/netns 4026532352 cni-1d85bb0c-7c61-fd9f-2adc-f6e98f7a58af 4026532280 cni-7cec0838-f50c-416a-3b45-628a4237c55c 4026532426 cni-912bcc63-712d-1c84-89a7-9e10510808a0</code> </pre> <br>  You can also list all network namespaces using <code>ip netns</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip netns cni-912bcc63-712d-1c84-89a7-9e10510808a0 (id: 2) cni-1d85bb0c-7c61-fd9f-2adc-f6e98f7a58af (id: 1) cni-7cec0838-f50c-416a-3b45-628a4237c55c (id: 0)</code> </pre> <br>  To see all the processes running in the network space <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> ( <code>4026532426</code> ), you can run, for example, the following command: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ls -l /proc/[1-9]*/ns/net | grep 4026532426 | cut -f3 -d<span class="hljs-string"><span class="hljs-string">"/"</span></span> | xargs ps -p PID TTY STAT TIME COMMAND 27255 ? Ss 0:00 /pause 27331 ? Ss 0:00 nginx: master process nginx -g daemon off; 27355 ? S 0:00 nginx: worker process</code> </pre> <br>  It is seen that in addition to the <code>pause</code> in this section, we launched <code>nginx</code> .  The <code>pause</code> container shares the <code>net</code> and <code>ipc</code> namespaces with all other pod containers.  Remember PID from <code>pause</code> - 27255;  we will return to it. <br><br>  Now let's see what <code>kubectl</code> tells about this <code>kubectl</code> : <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide | grep nginx nginx-65899c769f-wxdx6 1/1 Running 0 5d 10.200.0.4 worker-0</code> </pre> <br>  More details: <br><br><pre> <code class="bash hljs">$ kubectl describe pods nginx-65899c769f-wxdx6</code> </pre> <br><pre> <code class="plaintext hljs">Name: nginx-65899c769f-wxdx6 Namespace: default Node: worker-0/10.240.0.20 Start Time: Thu, 05 Jul 2018 14:20:06 -0400 Labels: pod-template-hash=2145573259 run=nginx Annotations: &lt;none&gt; Status: Running IP: 10.200.0.4 Controlled By: ReplicaSet/nginx-65899c769f Containers: nginx: Container ID: containerd://4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 Image: nginx ...</code> </pre> <br>  We see the name of the pod - <code>nginx-65899c769f-wxdx6</code> - and the ID of one of its containers ( <code>nginx</code> ), but not a word about <code>pause</code> so far.  Let's dig deeper the working node to match all the data.  Remember that <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes The Hard Way</a> does not use <a href="https://www.docker.com/">Docker</a> , so for details on the container we refer to the console utility <a href="https://github.com/containerd/containerd">containerd</a> - ctr <i>(see also the article ‚Äú <a href="https://habr.com/company/flant/blog/414875/">Integration of containerd with Kubernetes, replacing Docker, is ready for production</a> ‚Äù - <b>approx. Transl.</b> )</i> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr namespaces ls NAME LABELS k8s.io</code> </pre> <br>  Knowing the containerd namespace ( <code>k8s.io</code> ), you can get the <code>nginx</code> container ID: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers ls | grep nginx 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 docker.io/library/nginx:latest io.containerd.runtime.v1.linux</code> </pre> <br>  ... and <code>pause</code> too: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers ls | grep pause 0866803b612f2f55e7b6b83836bde09bd6530246239b7bde1e49c04c7038e43a k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux 21640aea0210b320fd637c22ff93b7e21473178de0073b05de83f3b116fc8834 k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 k8s.gcr.io/pause:3.1 io.containerd.runtime.v1.linux</code> </pre> <br>  The <code>nginx</code> container ID ending in <code>‚Ä¶983c7</code> is the same as what we got from <code>kubectl</code> .  Let's see if we can figure out which <code>pause</code> container belongs to the <code>nginx</code> hearth: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io task ls TASK PID STATUS ... d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 27255 RUNNING 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 27331 RUNNING</code> </pre> <br>  Remember that processes with PID 27331 and 27355 are running in the network name space <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> ? <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers info d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6 { <span class="hljs-string"><span class="hljs-string">"ID"</span></span>: <span class="hljs-string"><span class="hljs-string">"d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6"</span></span>, <span class="hljs-string"><span class="hljs-string">"Labels"</span></span>: { <span class="hljs-string"><span class="hljs-string">"io.cri-containerd.kind"</span></span>: <span class="hljs-string"><span class="hljs-string">"sandbox"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx-65899c769f-wxdx6"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.namespace"</span></span>: <span class="hljs-string"><span class="hljs-string">"default"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.uid"</span></span>: <span class="hljs-string"><span class="hljs-string">"0b35e956-8080-11e8-8aa9-0a12b8818382"</span></span>, <span class="hljs-string"><span class="hljs-string">"pod-template-hash"</span></span>: <span class="hljs-string"><span class="hljs-string">"2145573259"</span></span>, <span class="hljs-string"><span class="hljs-string">"run"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx"</span></span> }, <span class="hljs-string"><span class="hljs-string">"Image"</span></span>: <span class="hljs-string"><span class="hljs-string">"k8s.gcr.io/pause:3.1"</span></span>, ...</code> </pre> <br>  ... and: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ctr -n k8s.io containers info 4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7 { <span class="hljs-string"><span class="hljs-string">"ID"</span></span>: <span class="hljs-string"><span class="hljs-string">"4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7"</span></span>, <span class="hljs-string"><span class="hljs-string">"Labels"</span></span>: { <span class="hljs-string"><span class="hljs-string">"io.cri-containerd.kind"</span></span>: <span class="hljs-string"><span class="hljs-string">"container"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.container.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.name"</span></span>: <span class="hljs-string"><span class="hljs-string">"nginx-65899c769f-wxdx6"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.namespace"</span></span>: <span class="hljs-string"><span class="hljs-string">"default"</span></span>, <span class="hljs-string"><span class="hljs-string">"io.kubernetes.pod.uid"</span></span>: <span class="hljs-string"><span class="hljs-string">"0b35e956-8080-11e8-8aa9-0a12b8818382"</span></span> }, <span class="hljs-string"><span class="hljs-string">"Image"</span></span>: <span class="hljs-string"><span class="hljs-string">"docker.io/library/nginx:latest"</span></span>, ...</code> </pre> <br>  Now we know exactly which containers are running in this pod ( <code>nginx-65899c769f-wxdx6</code> ) and the network namespace ( <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> ): <br><br><ul><li>  nginx (ID: <code>4c0bd2e2e5c0b17c637af83376879c38f2fb11852921b12413c54ba49d6983c7</code> ); </li><li>  pause (ID: <code>d19b1b1c92f7cc90764d4f385e8935d121bca66ba8982bae65baff1bc2841da6</code> ). </li></ul><br><img src="https://habrastorage.org/webt/3h/cx/qq/3hcxqqv-mwlrm8ax9lu9jl0fixy.png"><br><br>  How is this one under ( <code>nginx-65899c769f-wxdx6</code> ) connected to the network?  Let's use the previously received PID 27255 from <code>pause</code> to launch commands in its network namespace ( <code>cni-912bcc63‚Äì712d-1c84‚Äì89a7‚Äì9e10510808a0</code> ): <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns identify 27255 cni-912bcc63-712d-1c84-89a7-9e10510808a0</code> </pre> <br>  For these purposes, we use <code>nsenter</code> with the <code>-t</code> option that defines the target PID, and <code>-n</code> without specifying a file to get into the network namespace of the target process (27255).  This is what the <code>ip link show</code> will say: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo nsenter -t 27255 -n ip link show 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 3: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 0a:58:0a:c8:00:04 brd ff:ff:ff:ff:ff:ff link-netnsid 0</code> </pre> <br>  ... and <code>ifconfig eth0</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo nsenter -t 27255 -n ifconfig eth0 eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.200.0.4 netmask 255.255.255.0 broadcast 0.0.0.0 inet6 fe80::2097:51ff:fe39:ec21 prefixlen 64 scopeid 0x20&lt;link&gt; ether 0a:58:0a:c8:00:04 txqueuelen 0 (Ethernet) RX packets 540 bytes 42247 (42.2 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 177 bytes 16530 (16.5 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0</code> </pre> <br>  Thus, it is confirmed that the IP address obtained earlier through the <code>kubectl get pod</code> is configured on the <code>eth0</code> interface of the submenu.  This interface is part of a <b>veth pair</b> , one end of which is in the hearth, and the other is in the root namespace.  To find out the interface of the second end, use <code>ethtool</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> cni-912bcc63-712d-1c84-89a7-9e10510808a0 ethtool -S eth0 NIC statistics: peer_ifindex: 7</code> </pre> <br>  We see that <code>ifindex</code> is 7. <code>ifindex</code> that it is in the root namespace.  This can be done using <code>ip link</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip link | grep <span class="hljs-string"><span class="hljs-string">'^7:'</span></span> 7: veth71f7d238@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master cnio0 state UP mode DEFAULT group default</code> </pre> <br>  To make sure of this completely, let's see: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo cat /sys/class/net/veth71f7d238/ifindex 7</code> </pre> <br>  Great, with the virtual link, now everything is clear.  With the help of <code>brctl</code> let's see who else is connected to the Linux-bridge: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ brctl show cnio0 bridge name bridge id STP enabled interfaces cnio0 8000.0a580ac80001 no veth71f7d238 veth73f35410 vethf273b35f</code> </pre> <br>  So, the picture is as follows: <br><br><img src="https://habrastorage.org/webt/yu/gc/t6/yugct6efi7ztep277en4msjuzv4.png"><br><br><h3>  Routing check </h3><br>  How do we actually forward traffic?  Let's look at the routing table in the network namespace: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo ip netns <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> cni-912bcc63-712d-1c84-89a7-9e10510808a0 ip route show default via 10.200.0.1 dev eth0 10.200.0.0/24 dev eth0 proto kernel scope link src 10.200.0.4</code> </pre> <br>  At least we know how to get to the root namespace ( <code>default via 10.200.0.1</code> ).  Now let's look at the host routing table: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ ip route list default via 10.240.0.1 dev eth0 proto dhcp src 10.240.0.20 metric 100 10.200.0.0/24 dev cnio0 proto kernel scope link src 10.200.0.1 10.240.0.0/24 dev eth0 proto kernel scope link src 10.240.0.20 10.240.0.1 dev eth0 proto dhcp scope link src 10.240.0.20 metric 100</code> </pre> <br>  We know how to forward packets to the VPC Router (the VPC <a href="https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Route_Tables.html">has a</a> ‚Äúhidden‚Äù <i>[implicit]</i> router, which <a href="https://cloud.google.com/vpc/docs/vpc">usually has a second address</a> from the main IP subnet address space).  Now: does the VPC Router know how to get to the network of each pod?  No, he does not know, so it is assumed that the routes will be configured by the CNI plugin or <a href="">manually</a> (as in the manual).  Apparently, <a href="https://github.com/aws/amazon-vpc-cni-k8s">AWS CNI-plugin</a> does exactly that for us in AWS.  Remember that there are <a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/">many plug-ins CNI</a> , and we consider an example of the <b>simplest network configuration</b> : <br><br><img src="https://habrastorage.org/webt/cn/v7/v_/cnv7v_qjfkidbtuljkbgkuzuaag.png"><br><br><h3>  Deep immersion in NAT </h3><br>  <code>kubectl create -f busybox.yaml</code> create two identical <code>busybox</code> containers with Replication Controller: <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: ReplicationController metadata: name: busybox0 labels: app: busybox0 spec: replicas: 2 selector: app: busybox0 template: metadata: name: busybox0 labels: app: busybox0 spec: containers: - image: busybox command: - sleep - "3600" imagePullPolicy: IfNotPresent name: busybox restartPolicy: Always</code> </pre> <br>  ( <a href="https://gist.github.com/nleiva/ed5be3164df3429652566f26717ce22f"><code>busybox.yaml</code></a> ) <br><br>  We get: <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE busybox0-g6pww 1/1 Running 0 4s 10.200.1.15 worker-1 busybox0-rw89s 1/1 Running 0 4s 10.200.0.21 worker-0 ...</code> </pre> <br>  Pings from one container to another must be successful: <br><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it busybox0-rw89s -- ping -c 2 10.200.1.15 PING 10.200.1.15 (10.200.1.15): 56 data bytes 64 bytes from 10.200.1.15: seq=0 ttl=62 time=0.528 ms 64 bytes from 10.200.1.15: seq=1 ttl=62 time=0.440 ms --- 10.200.1.15 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.440/0.484/0.528 ms</code> </pre> <br>  To understand the traffic flow, you can look at the packets using <code>tcpdump</code> or <code>conntrack</code> : <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 10.200.1.15 icmp 1 29 src=10.200.0.21 dst=10.200.1.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1280 src=10.200.1.15 dst=10.240.0.20 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1280 mark=0 use=1</code> </pre> <br>  The source IP from 10.200.0.21 is transmitted to the IP address of the node 10.240.0.20. <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 10.200.1.15 icmp 1 28 src=10.240.0.20 dst=10.200.1.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1280 src=10.200.1.15 dst=10.240.0.20 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1280 mark=0 use=1</code> </pre> <br>  In iptables, you can see that the counters are increasing: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo iptables -t nat -Z POSTROUTING -L -v Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> out <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> destination ... 5 324 CNI-be726a77f15ea47ff32947a3 all -- any any 10.200.0.0/24 anywhere /* name: <span class="hljs-string"><span class="hljs-string">"bridge"</span></span> id: <span class="hljs-string"><span class="hljs-string">"631cab5de5565cc432a3beca0e2aece0cef9285482b11f3eb0b46c134e457854"</span></span> */ Zeroing chain `POSTROUTING<span class="hljs-string"><span class="hljs-string">'</span></span></code> </pre> <br>  On the other hand, if you remove <code>"ipMasq": true</code> from the configuration of the CNI plug-in, you can see the following (this operation is performed exclusively for educational purposes - we do not recommend changing the config on a running cluster!): <br><br><pre> <code class="bash hljs">$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE busybox0-2btxn 1/1 Running 0 16s 10.200.0.15 worker-0 busybox0-dhpx8 1/1 Running 0 16s 10.200.1.13 worker-1 ...</code> </pre> <br>  Ping still has to go: <br><br><pre> <code class="bash hljs">$ kubectl <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -it busybox0-2btxn -- ping -c 2 10.200.1.13 PING 10.200.1.6 (10.200.1.6): 56 data bytes 64 bytes from 10.200.1.6: seq=0 ttl=62 time=0.515 ms 64 bytes from 10.200.1.6: seq=1 ttl=62 time=0.427 ms --- 10.200.1.6 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.427/0.471/0.515 ms</code> </pre> <br>  And in this case - without using NAT: <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 10.200.1.13 icmp 1 29 src=10.200.0.15 dst=10.200.1.13 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1792 src=10.200.1.13 dst=10.200.0.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1792 mark=0 use=1</code> </pre> <br>  So, we checked that "all containers can communicate with any other containers without using NAT". <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 10.200.1.13 icmp 1 27 src=10.200.0.15 dst=10.200.1.13 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=8 code=0 id=1792 src=10.200.1.13 dst=10.200.0.15 <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=0 code=0 id=1792 mark=0 use=1</code> </pre> <br><h2>  Cluster Network (10.32.0.0/24) </h2><br>  You may have noticed in the example with <code>busybox</code> that the IP addresses allocated to the <code>busybox</code> pod were different in each case.  What if we wanted to make these containers available for communication from other platforms?  It would be possible to take the current pod IP addresses, but they will change.  For this reason, you need to configure the resource <code>Service</code> , which will proxy requests to a variety of short-lived pods. <br><br><blockquote>  ‚ÄúService in Kubernetes is an abstraction that defines a logical set of hearths and a policy by which they can be accessed.‚Äù  <i>(from <a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/">Kubernetes Services</a> documentation)</i> </blockquote><br>  There are different ways to publish a service;  The default type is <code>ClusterIP</code> , which configures the IP address from the cluster CIDR block (that is, it is accessible only from the cluster).  One such example is the DNS Cluster Add-on configured in Kubernetes The Hard Way. <br><br><pre> <code class="plaintext hljs"># ... apiVersion: v1 kind: Service metadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: "KubeDNS" spec: selector: k8s-app: kube-dns clusterIP: 10.32.0.10 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP # ...</code> </pre> <br>  ( <a href="https://gist.github.com/nleiva/632e88a7639868dc6975624b95986821"><code>kube-dns.yaml</code></a> ) <br><br>  <code>kubectl</code> shows that <code>Service</code> remembers endpoints and translates them: <br><br><pre> <code class="bash hljs">$ kubectl -n kube-system describe services ... Selector: k8s-app=kube-dns Type: ClusterIP IP: 10.32.0.10 Port: dns 53/UDP TargetPort: 53/UDP Endpoints: 10.200.0.27:53 Port: dns-tcp 53/TCP TargetPort: 53/TCP Endpoints: 10.200.0.27:53 ...</code> </pre> <br>  How exactly? .. Again <code>iptables</code> .  Let's go through the rules created for this example.  Their full list can be seen with the <code>iptables-save</code> command. <br><br>  As soon as packets are created by the process ( <code>OUTPUT</code> ) or arrive at the network interface ( <code>PREROUTING</code> ), they pass through the following <code>iptables</code> chains: <br><br><pre> <code class="bash hljs">-A PREROUTING -m comment --comment <span class="hljs-string"><span class="hljs-string">"kubernetes service portals"</span></span> -j KUBE-SERVICES -A OUTPUT -m comment --comment <span class="hljs-string"><span class="hljs-string">"kubernetes service portals"</span></span> -j KUBE-SERVICES</code> </pre> <br>  The following targets correspond to TCP packets sent to port 53 at 10.32.0.10, and are broadcast to recipient 10.200.0.27 with port 53: <br><br><pre> <code class="bash hljs">-A KUBE-SERVICES -d 10.32.0.10/32 -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp cluster IP"</span></span> -m tcp --dport 53 -j KUBE-SVC-ERIFXISQEP7F7OF4 -A KUBE-SVC-ERIFXISQEP7F7OF4 -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp"</span></span> -j KUBE-SEP-32LPCMGYG6ODGN3H -A KUBE-SEP-32LPCMGYG6ODGN3H -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns-tcp"</span></span> -m tcp -j DNAT --to-destination 10.200.0.27:53</code> </pre> <br>  The same is true for UDP packets (receiver 10.32.0.10:53 ‚Üí 10.200.0.27:53): <br><br><pre> <code class="bash hljs">-A KUBE-SERVICES -d 10.32.0.10/32 -p udp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns cluster IP"</span></span> -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU -A KUBE-SVC-TCOU7JCQXEZGVUNU -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns"</span></span> -j KUBE-SEP-LRUTK6XRXU43VLIG -A KUBE-SEP-LRUTK6XRXU43VLIG -p udp -m comment --comment <span class="hljs-string"><span class="hljs-string">"kube-system/kube-dns:dns"</span></span> -m udp -j DNAT --to-destination 10.200.0.27:53</code> </pre> <br>  There are other types of <code>Services</code> in Kubernetes.  In particular, Kubernetes The Hard Way <code>NodePort</code> about <code>NodePort</code> - see <a href="">Smoke Test: Services</a> . <br><br><pre> <code class="bash hljs">kubectl expose deployment nginx --port 80 --<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> NodePort</code> </pre> <br>  <code>NodePort</code> publishes a service on the IP address of each node, placing it on a static port (it is called <code>NodePort</code> ).  The <code>NodePort</code> service can <code>NodePort</code> be accessed from outside the cluster.  You can check the selected port (in this case - 31088) with the help of <code>kubectl</code> : <br><br><pre> <code class="bash hljs">$ kubectl describe services nginx ... Type: NodePort IP: 10.32.0.53 Port: &lt;<span class="hljs-built_in"><span class="hljs-built_in">unset</span></span>&gt; 80/TCP TargetPort: 80/TCP NodePort: &lt;<span class="hljs-built_in"><span class="hljs-built_in">unset</span></span>&gt; 31088/TCP Endpoints: 10.200.1.18:80 ...</code> </pre> <br>  Under is now available from the internet as <code>http://${EXTERNAL_IP}:31088/</code> .  Here, <code>EXTERNAL_IP</code> is the public IP address of <b>any working instance</b> .  In this example, I used the public IP address <b>worker-0</b> .  The request is obtained by a node with an IP address of 10.240.0.20 (the cloud provider is in public NAT), but the service is actually running on another node ( <b>worker-1</b> , which can be seen at the endpoint's IP address - 10.200.1.18): <br><br><pre> <code class="bash hljs">ubuntu@worker-0:~$ sudo conntrack -L | grep 31088 tcp 6 86397 ESTABLISHED src=173.38.XXX.XXX dst=10.240.0.20 sport=30303 dport=31088 src=10.200.1.18 dst=10.240.0.20 sport=80 dport=30303 [ASSURED] mark=0 use=1</code> </pre> <br>  The packet is sent from <b>worker-0</b> to <b>worker-1</b> , where it finds its recipient: <br><br><pre> <code class="bash hljs">ubuntu@worker-1:~$ sudo conntrack -L | grep 80 tcp 6 86392 ESTABLISHED src=10.240.0.20 dst=10.200.1.18 sport=14802 dport=80 src=10.200.1.18 dst=10.240.0.20 sport=80 dport=14802 [ASSURED] mark=0 use=1</code> </pre> <br>  Is this scheme ideal?  Perhaps not, but it works.  In this case, the programmed <code>iptables</code> rules are: <br><br><pre> <code class="bash hljs">-A KUBE-NODEPORTS -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -m tcp --dport 31088 -j KUBE-SVC-4N57TFCL4MD7ZTDA -A KUBE-SVC-4N57TFCL4MD7ZTDA -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -j KUBE-SEP-UGTFMET44DQG7H7H -A KUBE-SEP-UGTFMET44DQG7H7H -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/nginx:"</span></span> -m tcp -j DNAT --to-destination 10.200.1.18:80</code> </pre> <br>  In other words, the address for the receiver of packets with port 31088 is broadcast on 10.200.1.18.  The port is also broadcast, from 31088 to 80. <br><br>  We did not touch on another type of services - <code>LoadBalancer</code> , - which makes the service publicly available with the help of the cloud provider‚Äôs load balancer, but the article has already turned out to be large. <br><br><h2>  Conclusion </h2><br>  It might seem that there is a lot of information here, but we only touched the tip of the iceberg.  In the future, I'm going to talk about IPv6, IPVS, eBPF and a couple of interesting CNI plug-ins. <br><br><h2>  PS from translator </h2><br>  Read also in our blog: <br><br><ul><li>  ‚Äú <a href="https://habr.com/company/flant/blog/346304/">An Illustrated Network Guide for Kubernetes</a> ‚Äù; </li><li>  " <a href="https://habr.com/company/flant/blog/332432/">Comparison of the performance of network solutions for Kubernetes</a> "; </li><li>  ‚Äú <a href="https://habr.com/company/flant/blog/359120/">Experiments with kube-proxy and unavailability of a node in Kubernetes</a> ‚Äù; </li><li>  " <a href="https://habr.com/company/flant/blog/326062/">Improving the reliability of Kubernetes: how to quickly notice that the node has fallen</a> "; </li><li> ¬´ <a href="https://habr.com/company/flant/blog/415381/">Play with Kubernetes ‚Äî      K8s</a> ¬ª; </li><li> ¬´ <a href="https://habr.com/company/flant/blog/331188/">   Kubernetes   </a> ¬ª <i>( ,        Kubernetes)</i> ; </li><li> ¬´ <a href="https://habr.com/company/flant/blog/329830/">Container Networking Interface (CNI) ‚Äî      Linux-</a> ¬ª. </li></ul></div><p>Source: <a href="https://habr.com/ru/post/420813/">https://habr.com/ru/post/420813/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../420799/index.html">MPS 2018.2: Generator Tests, GitHub Plugin, VCS Aspect, Migration Notifications, and More</a></li>
<li><a href="../420803/index.html">3D printing lessons. Plastic savings when printing non-functional models from 3Dtool</a></li>
<li><a href="../420805/index.html">[Translation] When to use parallel streams</a></li>
<li><a href="../420809/index.html">Security Week 31: Fifty shades of insecurity in Android</a></li>
<li><a href="../420811/index.html">Decentralized messenger and new generation telephone network</a></li>
<li><a href="../420815/index.html">How ‚Äúdecoding of the digital world‚Äù exploded the hall: top 10 reports DotNext 2018 Piter</a></li>
<li><a href="../420819/index.html">Top 10 Python tools for machine learning and data-science</a></li>
<li><a href="../420821/index.html">Rule 10: 1 in programming and writing</a></li>
<li><a href="../420825/index.html">Today will be the first match between OpenAI and Dota 2 professionals (people won). Understanding how the bot works</a></li>
<li><a href="../420827/index.html">Create a simple maven project using Java EE + WildFly10 + JPA (Hibernate) + Postgresql + EJB + IntelliJ IDEA</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>