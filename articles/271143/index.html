<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Rutube 2009-2015: the history of our iron</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="7 years have passed since Rutube became a part of Gazprom-Media Holding and a new stage of project development began. In this article we will talk abo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Rutube 2009-2015: the history of our iron</h1><div class="post__text post__text-html js-mediator-article">  7 years have passed since Rutube became a part of Gazprom-Media Holding and a new stage of project development began.  In this article we will talk about how we got the project at the end of 2008, and how it changed over the course of 7 years in terms of hardware.  Under the cut you will find a fascinating story and many many pictures (carefully, traffic!), So poke at Fichu (our office cat) and go ahead! <br><br> <a href="http://habrahabr.ru/company/rutube/blog/271143/"><img src="https://habrastorage.org/files/2ca/879/2db/2ca8792dba674a6089d888eebdaabc13.JPG"></a> <br><a name="habracut"></a><br><h2>  Start </h2><br>  At the end of 2008, Gazprom-Media Holding acquired the Rutube code and infrastructure.  The technical team, which at that time consisted of a technical director, system administrator and technical specialist (‚ÄúComputer asks to click‚Äú Enikey ‚Äù, where is it?), Had several racks with equipment in data centers‚Äú M10 ‚Äù,‚Äú COMSTAR-Direct "And" Kurchatnik. " <br><br>  Racks looked like this: <br><div style="text-align:center;"><img src="https://habrastorage.org/files/f51/9d3/b01/f519d3b01dcd4a278070d833342449b6.JPG" alt="Data Center M10" width="80%"></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <div style="text-align:center;"><img src="https://habrastorage.org/files/65a/d8e/26d/65ad8e26d778470f9afdd8a8c01359b7.JPG" alt="COMSTAR-Direct Data Center" width="80%"></div><br><br>  With a longing we recall the data center "M10", in which the quick-detachable sled can only be installed with the help of pliers and lightly tapping with a hammer.  But the Supermicro slide, fastened to the bolts, perfectly fixed in the racks, and the racks themselves were ready to withstand the full filling of the UPS devices. <br><br>  That only cost the location of racks in the COMSTAR-Direct data center when the rear door could not open completely, resting against the wall, and had to remove the door in order to crawl to the sled from the hinge side of the rack.  Even some nostalgia remained for this valuable experience! <br><br>  The equipment consisted of HP ProLiant DL140 G3 and HP ProLiant DL320 G5 servers, as well as Supermicro servers based on PDSMU, X7SBi motherboards.  The role of switches was performed by Allied Telesis and D-Link. <br><br>  By the way, we have already decommissioned and sold a part of this equipment, and some are still on sale - please contact us! <br><br><h2>  Development </h2><br>  Almost immediately it became clear that the current capacity is not enough for the development of the project, and it was decided to purchase several dozen Supermicro servers based on the X7DWU motherboard.  Cisco Catalyst 3750 switches used the network component. Since the beginning of 2009, we installed this equipment in the Synterra data center and in the ‚ÄúM10‚Äù. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/036/45c/f35/03645cf352fd4b68890d2709efb1f130.jpg" alt="Synterra" width="80%"></div><br><br>  Storage of content began to translate into industrial data storage.  The choice fell on NetApp: FAS3140 controllers with DS14 disk shelves.  Subsequently, the storage system was expanded with the FAS3170 and FAS3270 controllers using more advanced DS4243 shelves. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/633/456/73c/63345673ce6143dabfcad3cbaec5fe77.JPG" alt="NetApp + trash can" width="80%"></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/066/591/097/0665910973584033b74b10fab1e2011c.JPG" alt="NetApp + trash can" width="80%"></div><br><br>  By the summer of 2009, an ‚Äúunexpected‚Äù problem had arisen - since no one was specifically responsible for servicing the data centers, everyone who put iron there or performed switching did not feel like a host, but a guest.  From here drew jungle of wires and randomly scattered servers. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/db7/beb/0e5/db7beb0e59834f3d8e27706240be5145.JPG" alt="Synterra \ Switching" width="80%"></div><br><br>  It was decided to assign responsibility for this direction (hundreds of servers, dozens of racks and switches) to the dedicated employee.  Since then, the infrastructure has grown to five hundred servers, several dozen switches and racks, the employee has become a department of three people. <br><br>  At the same time, the purchase of new network equipment took place - the choice was made on Juniper (Juniper EX8208, EX4200, EX3200, EX2200 switches and MX480 switches).  And in the fall of 2009, when we received new equipment, we carried out large-scale work to restore order (at the Synterra data center) and to commission new equipment with a minimum interruption of service. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/e03/fe4/ea3/e03fe4ea35924dcc950a35f5f4deb328.JPG" alt="Synterra" width="80%"></div><br>  <font color="#999999"><i>We installed new network equipment, let down elements of the new SCS (at that time we were still embroidering patch panels).</i></font> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/0d0/a86/f57/0d0a86f5772044c3bcd1028ad0af7d1e.JPG" alt="Synterra" width="80%"></div><br>  <font color="#999999"><i>Decorated the garland with temporary patch cords to minimize service interruptions during operation.</i></font> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/41e/72f/dd7/41e72fdd7abc4cca8ea9ac8c5132e229.JPG" alt="Synterra" width="80%"></div><br><br>  As a result, came to this order.  The End-of-Row scheme is working, but has its own clear disadvantages.  A few years later, having expanded the network equipment fleet, they switched to the Top-of-Rack scheme. <br>  The final transfer to the new equipment took place on November 4 - the Day of National Unity. <br><br>  At the end of 2009, we launched our site in the M9 data center.  The main goal was to gain access to the hundreds of operators that are present at the Nine (even now in Moscow there is no real alternative to this institution).  Here we installed the Juniper MX480 router, the Juniper EX4200, EX2200 switches and the new Dell PowerEdge R410 servers. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/5d4/186/6e3/5d41866e3d1c4f6da5ae1a8468efd1dd.JPG" alt="Juniper MX480" width="80%"></div><br>  <font color="#999999"><i>Juniper MX480</i></font> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/122/7e7/439/1227e743947a4d9ba265cd6c040d9b55.JPG" alt="Juniper EX2200, EX4200" width="80%"></div><br>  <font color="#999999"><i>Juniper EX2200, EX4200</i></font> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/b8c/34c/2df/b8c34c2dfae246659d432e659491f56a.JPG" width="80%"></div><br>  <font color="#999999"><i>Then it seemed that the 52U racks on the ‚ÄúM9‚Äù were dimensionless, and now we can hardly fit them.</i></font> <br><br>  Previously, we took the servers not immediately to the data center, but in the office where the servers were checked and the initial setup of the servers before being sent to the data center. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/94c/b5e/975/94cb5e9750634e9180ff8ce87432bebf.JPG" width="80%"></div><br><br>  A cozy spacious server room with no windows and an air-conditioning system, in which, as a bonus, there was a certain farm manager, who constantly offered to dine for the company. <br><br>  Since 2010, we have been actively growing: new projects, new equipment, new racks in the data center.  In mid-2011, colleagues noticed that the employee responsible for hardware and the data center does not appear in the office even on the day of the advance payment and salary (they come to the card).  We missed you! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/5ae/17e/8b4/5ae17e8b49284f0b88ce4b3ab16ae273.jpg" width="80%"></div><br>  <font color="#999999"><i>A minute of glory (I realized that I was writing more for myself than for a habr)!</i></font> <br><br>  But nobody was going to slow down the pace.  In the new data center M77, we launched a new project (NTVPLUS.TV) and started building the second core RUTUBE.RU so that when the main data center RUTUB falls, it continues to work. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/7eb/d73/b4e/7ebd73b4e25f42639385fa391c940e74.JPG" width="80%"></div><br>  <font color="#999999"><i>A small batch of servers Sun Fire X4170 √ó 64.</i></font> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/946/19c/91f/94619c91fb5b40cfa27e91aaf04c7933.JPG" width="80%"></div><br>  <font color="#999999"><i>Juniper EX8216, EX4200, EX2200 and a bit of NetApp switches.</i></font> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/051/adb/5a9/051adb5a944845f493923ea4cc96a3d5.JPG" width="80%"></div><br>  <font color="#999999"><i>Another competition "manage to compress 100500 patch cords before launching the project."</i></font> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/5ff/266/7d1/5ff2667d16ed429eb057dffe1361b488.JPG" width="80%"></div><br>  <font color="#999999"><i>With SCS completed and data center launched.</i></font> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/13c/c32/8bb/13cc328bb3fe4c4a87d344ddff07879c.JPG" width="80%"></div><br>  <font color="#999999"><i>Here and NetApp FAS3170 with shelves DS4243 gradually filled with content.</i></font> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/225/eeb/41c/225eeb41c1d5456085dbc62ca9589b04.JPG" width="80%"></div><br>  <font color="#999999"><i>In the meantime, our system administrators are finishing setting up the Sun Fire X4170 √ó 64.</i></font> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/df0/727/194/df072719415841899286171feaf0c34a.JPG" width="80%"></div><br>  <font color="#999999"><i>A ‚Äúmain posting‚Äù completes the beauty (AKA order).</i></font> <br><br>  The year 2011 began with the continuation of the expansion of the second core in the M77 data center, when we received a new batch of Dell PowerEdge R410 servers and, within the framework of the new project (from the technology partner), servers on the Quanta platform. <br><br>  In the network infrastructure, 10G switches appeared more and more - the first swallow was Extreme Summit X650-24x.  Then there were more interesting Extreme Summit X670-48x. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/e1b/36b/b42/e1b36bb42bf54327b91c365344e08030.jpg" width="80%"></div><br>  <font color="#999999"><i>That's what was lacking in childhood to build a cardboard house.</i></font> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/2fd/6e6/5a8/2fd6e65a8525460ca5e6ade5ef0eaac6.jpg" width="80%"></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/c02/05a/ccc/c0205accc4324390bd17bb507cdecedb.jpg" width="80%"></div><br><br>  Not having time to exhale after finishing work at the data center, the M77 was relocated to the Synterra data center, where it was necessary to commission the Juniper EX8216 instead of the EX8208 (it was necessary to install more boards for connecting operators and servers). <br><br>  At the same time, we started the installation of our first DWDM complex (the active version), which connects the three main data centers "M9", "Synterra" and "M77" over dark optics.  Here we were helped by a domestic manufacturer - T8. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/c0c/5d6/3b4/c0c5d63b498442c9b1817d192b82519c.jpg" width="80%"></div><br>  <font color="#999999"><i>Juniper EX8216 and DWDM</i></font> <br><br>  In 2012, we had a department responsible for the data center and hardware (that is, instead of one employee, there were two).  Before that, of course, more than one person did all the work ‚Äî his network and system administrators actively helped him.  Since then, the department has tried to balance between order, unification, beauty and operational work within the framework of project development tasks. <br><br><h2>  Project nowadays </h2><br>  A new stage of development began in 2014, when they began to change the storage system, optimize the server infrastructure, launching new caching servers, and also (in 2015) replaced all the main network equipment, since the old one did not meet current needs. <br><br>  NetApp storage system faithfully served us for 5 years.  During this time, we realized that the maintenance and expansion of storage systems requires expenditures that are not commensurate with the rest of the subsystems.  We began the search for a more rational solution, which ended with the phased implementation of our own developed storage systems (the transition began in early 2014 and ended in autumn 2015).  Now the storage system consists of 12 disk servers (Supermicro, Quanta) and software written by our developers.  For us, this was a great solution, and at the moment NetApp has been removed from support and we use some of it as storage systems for various technological needs. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/286/d21/80e/286d2180e5e84cc699e2f8cc49355ccb.JPG" width="80%"></div><br><br>  In early 2014, they decided to upgrade the caching system, which at that time represented a hundred servers with 4 Gigabit interfaces and a hybrid disk subsystem (SAS + SSD). <br><br>  We decided to separate the servers that will give the ‚Äúhot‚Äù (actively viewed) content into a separate cluster.  These servers were Supermicro on an X9DRD-EF motherboard with two Intel Xeon E5-2660 v2 processors, 128 GB of RAM, 480 GB of SSD and 4 Intel X520-DA2 network cards.  Experienced to establish that such a server without any problems gives 65-70 Gbit / s (the maximum was 77 Gbit / s). <br><br>  In mid-2014, we replaced active DWDM with passive.  This allowed us to greatly increase its resources and begin to "plant" operators connected in one data center to other sites, reducing dependence on the failure of a specific border equipment. <br><br>  By the end of 2014, they launched a new cluster for ‚Äúcold‚Äù content, which replaced the remaining servers with an aggregate of 4 Gbit / s.  And again our choice fell on Supermicro on the X9DRD-EF motherboard, this time with two Intel Xeon E5-2620 v2 processors, 128 GB of RAM, 12 √ó 960 GB SSD and 2 Intel X520-DA2 network cards.  Each node in this cluster is capable of holding a load of up to 35 Gbit / s. <br><br>  Naturally, it is not only a matter of well-chosen hardware, but also remarkable self-written segmentation modules written by our system wonder-architect and a wonderful video balancer created by the development team.  Work on finding out the limits of this platform continues - there are slots for SSD and network cards. <br><br>  2015 was marked by the replacement of all major network equipment, including the transition from hardware load balancers to software (Linux + x86).  Instead of Juniper EX8216 switches, most of the EX4200, Extreme Summit X650-24x and X670-48x have been taken over by Cisco ASR 9912 routers and Cisco Nexus 9508, Cisco Nexus 3172PQ and Cisco Nexus 3048 switches. Of course, the development of our network subsystem is a reason for a separate large article . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/733/664/1d3/7336641d3e944f349a4372f27e6be3bb.JPG" alt="Cisco" width="80%"></div><br><br>  After working on replacing the old server hardware and network, the racks do not look as good as we would like.  In the foreseeable future, we will finish restoring order and publish a colorful article with photos as we enter 2016. </div><p>Source: <a href="https://habr.com/ru/post/271143/">https://habr.com/ru/post/271143/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../271131/index.html">GOTO or not GOTO that is the question</a></li>
<li><a href="../271133/index.html">Work with HealthKit. Part 1</a></li>
<li><a href="../271135/index.html">So, you decided to become a front-end provider: a practical guide for the growth of a novice developer.</a></li>
<li><a href="../271137/index.html">Alloy Discovery: the all-seeing eye of the system administrator</a></li>
<li><a href="../271139/index.html">Hosting Cafe</a></li>
<li><a href="../271151/index.html">Emoji - the word of 2015</a></li>
<li><a href="../271153/index.html">Bootstrap + Ember.js guide. Part 1: Modal windows in Amber or how to make friends Bootstrap Modal and Ember.js</a></li>
<li><a href="../271155/index.html">FreePBX 13. No longer beta</a></li>
<li><a href="../271157/index.html">Pure architecture in a go app. Part 3</a></li>
<li><a href="../271159/index.html">What technologies does Netflix use?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>