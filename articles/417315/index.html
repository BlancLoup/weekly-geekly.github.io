<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Database development in Dropbox. The path from one global MySQL database to thousands of servers</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="When only Dropbox was launched, one user at Hacker News commented that you can implement it with several bash scripts using FTP and Git. Now this can ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Database development in Dropbox. The path from one global MySQL database to thousands of servers</h1><div class="post__text post__text-html js-mediator-article">  When only Dropbox was launched, one user at Hacker News commented that you can implement it with several bash scripts using FTP and Git.  Now this can not be said at all, it is a large cloud file storage with billions of new files every day, which are not just stored in the database somehow, but so that any database can be restored to any point in the last six days. <br><br>  Under the cut is the transcript of the report of <strong>Glory Bakhmutov</strong> ( <a href="https://habr.com/users/m0sth8/" class="user_link">m0sth8</a> ) on Highload ++ 2017, how the databases in Dropbox have evolved and how they are organized now. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/hUFFsLoCRNU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <strong>About the speaker:</strong> Slava Bakhmutov is a site reliability engineer on the Dropbox team, she loves Go very much and sometimes appears in the golangshow.com podcast. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  Content <br></h2><br><ul><li>  <a href="https://habr.com/ru/company/oleg-bunin/blog/417315/">Briefly about Dropbox architecture</a> </li><li>  <a href="https://habr.com/ru/company/oleg-bunin/blog/417315/">The history of</a> database development and how the current Dropbox architecture is arranged </li><li>  <a href="https://habr.com/ru/company/oleg-bunin/blog/417315/">Simplest database operations</a> (file managers, backups, clones, promotions) </li><li>  <a href="https://habr.com/ru/company/oleg-bunin/blog/417315/">Automation</a> - which manages all databases and starts operations </li><li>  <a href="https://habr.com/ru/company/oleg-bunin/blog/417315/">Monitoring</a> </li><li>  <a href="https://habr.com/ru/company/oleg-bunin/blog/417315/">Testing, staging and DRT</a> </li></ul><br><img src="https://habrastorage.org/webt/dh/gt/p7/dhgtp7pgu4eyl6rc3ncfnaf9s3i.jpeg"><br><a name="habracut"></a><br><a name="dropbox_architecture"></a><h2>  Simple Dropbox Architecture </h2><br>  Dropbox appeared in 2008.  In essence, this is a cloud file storage.  When Dropbox was launched, the user at Hacker News commented that you can implement it with several bash scripts using FTP and Git.  But, nevertheless, Dropbox is developing, and now it is a fairly large service with more than 1.5 billion users, 200 thousand businesses and a huge number (several billion!) Of new files every day. <br><br>  <strong>What does Dropbox look like?</strong> <br><img src="https://habrastorage.org/webt/ed/em/km/edemkm1wqvlbv6jb0qobp5c2dgc.jpeg"><br><br>  We have several clients (web interface, API for applications that use Dropbox, desktop applications).  All these clients use the API and communicate with two large services, which can logically be divided into: <br><br><ol><li>  <strong>Metaserver</strong> . </li><li>  <strong>Blockserver</strong> . </li></ol><br>  Metaserver stores meta information about a file: size, comments to it, links to this file in Dropbox, etc.  The Blockserver stores information only about files: folders, paths, etc. <br><br>  <strong>How it works?</strong> <br><br>  For example, you have a video.avi file with some kind of video. <br><img src="https://habrastorage.org/webt/kc/i9/op/kci9op0l7f_tecaxetg7uzpzemw.jpeg">  <em><a href="https://blogs.dropbox.com/tech/2014/07/streaming-file-synchronization/">Link from the slide</a></em> <br><br><ul><li>  The client splits this file into several chunks (in this case 4 MB), calculates the checksum and sends a request to Metaserver: ‚ÄúI have an * .avi file, I want to download it, such hash sums‚Äù. </li><li>  Metaserver returns the answer: ‚ÄúI do not have these blocks, let's download!‚Äù Or he can answer that he has all or some of the blocks, and you need to download only the remaining ones. </li></ul><br><img src="https://habrastorage.org/webt/qa/zr/79/qazr79svhai2ouk6v8lta1zcp-u.jpeg">  <em><a href="https://blogs.dropbox.com/tech/2014/07/streaming-file-synchronization/">Link from the slide</a></em> <br><br><ul><li>  After that, the client goes to the Blockserver, sends the hash sum and the data block itself, which is stored on the Blockserver. </li><li>  Blockserver confirms operation. </li></ul><br><img src="https://habrastorage.org/webt/kl/dx/xw/kldxxw1ncgj4ytt1pqq5bh95iue.jpeg">  <em><a href="https://blogs.dropbox.com/tech/2014/07/streaming-file-synchronization/">Link from the slide</a></em> <br><br>  Of course, this is a very simplified scheme, the protocol is much more complicated: there is synchronization between clients within the same network, there are kernel drivers, the ability to resolve collisions, etc.  This is a rather complicated protocol, but schematically it works something like this. <br><img src="https://habrastorage.org/webt/ev/-x/_h/ev-x_hwfhwpfixld61yqiraelc4.jpeg"><br><br>  When a client saves something on Metaserver, all the information goes into MySQL.  Blockserver information about files, how they are structured, which blocks they consist of, also stores in MySQL.  Also, the Blockserver stores the blocks themselves in Block Storage, which, in turn, also stores information about where the block is, on which server, and how it is processed at the moment, in MYSQL. <br><br><blockquote>  In order to store exabytes of user files, we simultaneously save additional information into a database of several dozen petabytes scattered across 6 thousand servers. </blockquote><br><a name="history_development"></a><h2>  The history of database development </h2><br>  How did the database in Dropbox evolve? <br><img src="https://habrastorage.org/webt/oe/w9/ok/oew9okiqdeivyhvhycznly7kvbe.jpeg"><br><br>  In 2008, it all started with one Metaserver and one global database.  All the information that Dropbox needed to be saved somewhere, it saved to a single global MySQL.  This did not last long, because the number of users grew, and individual bases and tablets inside the bases swelled faster than others. <br><img src="https://habrastorage.org/webt/ry/lt/h9/rylth906zfbbcou6nz7ve_yqib8.jpeg"><br><br>  Therefore, in 2011 several tables were moved to separate servers: <br><br><ul><li>  <strong>User</strong> , with information about users, for example, logins and oAuth tokens; </li><li>  <strong>Host</strong> , with information about files from Blockserver; </li><li>  <strong>Misc</strong> , which did not participate in processing requests from production, but was used for service functions, like batch jobs. </li></ul><br><img src="https://habrastorage.org/webt/ja/ec/ja/jaecja2eklv8znsqhf5lt-dyez8.jpeg"><br><br>  But after 2012, Dropbox began to grow very much, since then <strong>we</strong> have grown by <strong>about 100 million users a year</strong> . <br><img src="https://habrastorage.org/webt/ie/cr/-s/iecr-syyi6qprj2zj45qx4l42k4.jpeg"><br><br>  It was necessary to take into account such a huge growth, and therefore at the end of 2011 we had shards - the base consisting of 1,600 shards.  Initially, only 8 servers with 200 shards each.  Now it‚Äôs 400 master servers with 4 shards each. <br><img src="https://habrastorage.org/webt/b3/v9/vy/b3v9vyjqzwau2kgmadhgb2vg0vo.jpeg">  <em><a href="https://youtu.be/VZ-zJEWi-Vo">Link from the slide</a></em> <br><br>  In 2012, we realized that creating tables and updating them in the database for each added business logic is very difficult, dreary and problematic.  Therefore, in 2012, we invented our own graph storage, which we called <strong>Edgestore</strong> , and since then all the business logic and meta information that the application generates is stored in Edgestore. <br><br>  Edgestore, in fact, abstracts MySQL from clients.  Clients have certain entities that are interconnected by links from the gRPC API to Edgestore Core, which converts this data to MySQL and somehow stores it there (basically it gives all this from the cache). <br><img src="https://habrastorage.org/webt/bj/s7/dz/bjs7dz7-cdsvjblcgftjdeybrgk.jpeg">  <em><a href="https://blogs.dropbox.com/tech/2016/08/reintroducing-edgestore/">Link from the slide</a></em> <br><br>  <strong>In 2015, we left Amazon S3</strong> , developed our own cloud storage called Magic Pocket.  It contains information about where the block file is located, on which server, about the movement of these blocks between servers, is stored in MySQL. <br><img src="https://habrastorage.org/webt/f_/bz/lm/f_bzlm3tk9e3lwqo64x4kfuhhok.jpeg">  <em><a href="https://blogs.dropbox.com/tech/2016/05/inside-the-magic-pocket/">Link from the slide</a></em> <br><br>  But MySQL is used in a very tricky way - in fact, as a large distributed hash table.  This is a very different load, mainly on reading random entries.  90% recycling is I / O. <br><br><h2>  Database architecture </h2><br>  First, we immediately identified some principles by which we build the architecture of our database: <br><br><ol><li>  <strong>Reliability and durability</strong> .  This is the most important principle and what customers expect from us - the data should not be lost. </li><li>  <strong>The optimal solution</strong> is an equally important principle.  For example, backups should be made quickly and recovered too quickly. </li><li>  <strong>Simplicity of the solution</strong> - both architecturally and from the point of view of maintenance and further development support. </li><li> <strong>Cost of ownership</strong> .  If something optimizes the solution, but is very expensive, it does not suit us.  For example, a slave that is behind the master by the day is very convenient for backups, but then you need to add another 1,000 to 6,000 servers - the cost of owning such a slave is very high. </li></ol><br>  All principles must be <strong>verifiable and measurable</strong> , that is, they must have metrics.  If we are talking about the cost of ownership, then we must calculate how many servers we have, for example, it goes under the database, how many servers it goes under the backups, and how much it costs for Dropbox.  When we choose a new solution, we count all the metrics and focus on them.  When choosing any solution, we are fully guided by these principles. <br><br><h2>  Basic topology </h2><br>  The database is organized approximately as follows: <br><br><ul><li>  In the main data center we have a master, in which all the records occur. </li><li>  The master server has two slave servers for which semisync replication occurs.  Servers often die (about 10 per week), so we need two slave servers. </li><li>  Slave servers are in separate clusters.  Clusters are completely separate rooms in the data center that are not connected to each other.  If one room burns down, the second remains quite a working one. </li><li>  Also in another data center we have a so-called pseudo master (intermediate master), which is actually just a slave, which has another slave. </li></ul><br><img src="https://habrastorage.org/webt/k6/6s/x6/k66sx6siyp6efjxxmfrot21ueha.jpeg"><br><br>  This topology is chosen because if we suddenly die the first data center, then in the second data center we already have <strong>almost complete topology</strong> .  We simply change all addresses in Discovery, and customers can work. <br><br><h3>  Specialized topologies </h3><br>  We also have specialized topologies. <br><br>  Topology <strong>Magic Pocket</strong> consists of one master-server and two slave-servers.  This is done because Magic Pocket itself duplicates data among the zones.  If it loses one cluster, it can recover all data from other zones via erasure code. <br><img src="https://habrastorage.org/webt/gk/o7/bi/gko7bifb-4ted4cvwmzgyn5lasw.jpeg"><br><br>  <strong>Active-active</strong> topology is a custom topology that is used in Edgestore.  It has one master and two slaves in each of the two data centers, and they are slaves for each other.  This is a very <strong>dangerous scheme</strong> , but Edgestore at its level knows exactly what data to which master at which range it can write.  Therefore, this topology does not break. <br><img src="https://habrastorage.org/webt/xe/nv/wx/xenvwx3ls9ct8fssverq3htck10.jpeg"><br><br><h3>  Instance </h3><br>  We have installed fairly simple servers with a configuration of 4-5 years old: <br><br><ul><li>  <strong>2x Xeon 10 cores;</strong> </li><li>  <strong>5TB (8 SSD Raid 0 *);</strong> </li><li>  <strong>384 GB memory.</strong> </li></ul><br>  * Raid 0 - because it is easier and faster for us to replace the whole server than the disks. <br><br><h4>  Single instance </h4><br>  On this server, we have one big MySQL instance, on which there are several shards.  This MySQL instance immediately allocates to itself almost all the memory.  Other processes are running on the server: proxy, statistics collection, logs, etc. <br><br><img src="https://habrastorage.org/webt/z8/yd/vw/z8ydvwabte3v8pytwbrl1vi8yc0.jpeg"><br><br>  This solution is good because: <br><br>  + It's <strong>easy to manage</strong> .  If you need to replace MySQL instance, simply replace the server. <br><br>  + <strong>Just make faylover</strong> . <br><br>  On the other hand: <br><br>  - It is problematic that any operations occur on the whole MySQL instance and immediately on all shards.  For example, if you need to make a backup, we make a backup of all shards at once.  If you need to make a faylover, we make a faylover of all four shards at once.  Accordingly, accessibility suffers 4 times more. <br><br>  - Problems with replication of one shard affect other shards.  Replication in MySQL is not parallel, and all shards work in one stream.  If something happens to one shard, then the rest are also victims. <br><br>  Therefore, we are now switching to another topology. <br><br><h4>  Multi instance </h4><br><img src="https://habrastorage.org/webt/lg/7x/ks/lg7xks5vbogjaf6slr7tidlc6ty.jpeg"><br><br>  In the new version, several MySQL instances are running on the server at once, each with one shard.  What is better? <br><br>  + We can <strong>carry out operations only on one particular shard</strong> .  That is, if you need a faylover, switch only one shard, if you need a backup, make a backup of only one shard.  This means that operations are greatly accelerated - 4 times for a four-shard server. <br><br>  + <strong>Shards almost don't affect each other</strong> . <br><br>  + <strong>Improved replication.</strong>  We can mix different categories and classes of databases.  Edgestore takes up a lot of space, for example, all 4 TB, and Magic Pocket only takes 1 TB, but it has 90% utilization.  That is, we can combine various categories that use I / O and machine resources in different ways, and run 4 replication threads. <br><br>  Of course, this solution has some disadvantages: <br><br>  - The biggest minus is <strong>much harder to manage all this</strong> .  We need some kind of smart planner who will understand where he can take this instance, where the optimal load will be. <br><br>  - <strong>Harder faylover</strong> . <br><br>  Therefore, we are only now switching to this decision. <br><br><h3>  Discovery </h3><br>  Clients must somehow know how to connect to the right database, so we have Discovery, which should: <br><br><ol><li>  Very quickly notify the client about topology changes.  If we changed the master and slave, customers should know about it almost instantly. <br></li><li>  The topology should not depend on the MySQL replication topology, because for some operations we change the MySQL topology.  For example, when we do split, at the preparatory step on the target master, where we will take some shards, some of the slave servers migrate to this target master.  Customers do not need to know about it. <br></li><li>  It is important that there is atomicity of operations and state verification.  It is impossible for two different servers of the same database to become master at the same time. <br></li></ol><br><h4>  How Discovery Developed </h4><br>  At first everything was simple: the address of the database in the source code in the config.  When we needed to update the address, it was just that everything deployed very quickly. <br><img src="https://habrastorage.org/webt/7d/yb/oo/7dyboo0h7eo4o9_9xp-n-6lzosy.jpeg"><br><br>  Unfortunately, this does not work if there are a lot of servers. <br><img src="https://habrastorage.org/webt/3u/26/pf/3u26pfl_s796zdaoix-zdplb9du.jpeg"><br><br>  Above is the very first Discovery, which we had.  There were database scripts that changed the label in ConfigDB ‚Äî it was a separate MySQL table, and clients already listened to this database and periodically took data from there. <br><img src="https://habrastorage.org/webt/ml/qn/mh/mlqnmhmmteylazgl4itjokes_mu.jpeg"><br><br>  The table is very simple; there is a database category, a shard key, a master / slave database class, a proxy, and a database address.  In fact, the client requested a category, database class, shard key, and he was returned a MySQL address at which he could already establish a connection. <br><img src="https://habrastorage.org/webt/vb/ht/en/vbhteniyw4x7s56a1xwckuz268e.jpeg"><br><br>  As soon as there were a lot of servers, Memcash was added and clients began to communicate with him. <br><br>  But then we reworked it.  MySQL scripts began to communicate through gRPC, through a thin client with a service that we called RegisterService.  When some changes occurred, the RegisterService had a queue, and he understood how to apply these changes.  RegisterService saved data in AFS.  AFS is our internal system, built on the basis of ZooKeeper. <br><img src="https://habrastorage.org/webt/mi/lm/yz/milmyzvyvayuv2av8pah4neh9zq.jpeg"><br><br>  The second solution, which is not shown here, directly used ZooKeeper, and this created problems because every shard was a node in ZooKeeper.  For example, 100 thousand clients connect to ZooKeeper, if they suddenly died because of some kind of bug all together, then 100 thousand requests will immediately come to ZooKeeper, which will simply drop it, and it will not be able to rise. <br><br>  Therefore, <strong>the AFS system</strong> was developed <strong>, which is used by the entire Dropbox</strong> .  In fact, it abstracts the work with ZooKeeper for all clients.  The AFS daemon locally rotates on each server and provides a very simple file API of the form: create a file, delete a file, request a file, receive notification of file changes, and compare and swap operations.  That is, you can try to replace the file with some version, and if this version has changed during the shift, the operation is canceled. <br><br>  Essentially, such an abstraction over ZooKeeper, in which there is a local backoff and jitter algorithms.  ZooKeeper no longer falls under load.  With AFS, we remove backups in S3 and in GIT, then the local AFS itself notifies customers that the data has changed. <br><img src="https://habrastorage.org/webt/ry/_0/wv/ry_0wvkyux23gicrlwmfceq0eoe.jpeg"><br><br>  In AFS, data is stored as files, that is, it is a file system API.  For example, the shard.slave_proxy file is the largest, it takes about 28 Kb, and when we change the shard category and the slave_proxy class, all clients that are subscribed to this file receive a notification.  They re-read this file, which has all the necessary information.  By shard key get the category and reconfigure the connection pool to the database. <br><br><a name="perations_databases"></a><h2>  Operations </h2><br>  We use very simple operations: promotion, clone, backups / recovery. <br><img src="https://habrastorage.org/webt/q3/5c/df/q35cdfiso51bhhiitqja71qbysc.jpeg"><br><br>  <strong>The operation is a simple state machine</strong> .  When we go into an operation, we perform some checks, for example, a spin-check, which checks on timeouts several times whether we can perform this operation.  After that, we do some preparatory action that does not affect external systems.  The actual operation itself. <br><br>  All steps inside the operation have a <strong>rollback step</strong> (undo).  If there is a problem with the operation, the operation tries to restore the system to its original position.  If everything is normal, then a cleanup occurs, and the operation is completed. <br><br>  We have such a simple state machine for any operation. <br><br><h4>  <strong>Promotion (change of master)</strong> </h4><br>  This is a very frequent operation in the database.  There were questions about how to do alter on a hot master-server that works - he will get a stake.  Simply, all these operations are performed on slave-servers, and then the slave changes with master places.  Therefore, the <strong>promotion operation is very frequent</strong> . <br><img src="https://habrastorage.org/webt/xx/79/jv/xx79jvszxb9wjffwqf4ld_euofo.jpeg"><br><br>  We need to update the kernel - we are doing swap, we need to update the version of MySQL - we update to the slave, we switch to master, we update there. <br><img src="https://habrastorage.org/webt/wc/op/o9/wcopo9jlmz9aeoynpv-hbbseois.jpeg"><br><br>  We have achieved a very fast promotion.  For example, we have <strong>for four shards now a promotion of the order of 10-15 s.</strong>  The graph above shows that when the promotion availability suffered by 0.0003%. <br><br>  But normal promotion is not so interesting, because it is the usual operations that are performed every day.  Faylover interesting. <br><br><h4>  <strong>Failover (replacement of a broken master)</strong> <br></h4><br>  Failover means that the database is dead. <br><br><ul><li>  If the server is really dead, this is just the perfect case. </li><li>  In fact, it happens that the servers are partially alive. </li><li>  Sometimes the server dies very slowly.  He refuses raid-controllers, disk system, some requests return answers, but some threads are blocked and do not return answers. </li><li>  It happens that the master is just overloaded and does not respond to our health check.  But if we make a promotion, the new master will also be overloaded, and it will only get worse. </li></ul><br>  We replace the deceased master servers about <strong>2-3 times a day</strong> , this is a fully automated process, no human intervention is needed.  The critical section takes about 30 seconds, and it has a bunch of additional checks on whether the server is actually alive, or maybe it has already died. <br><br>  Below is an exemplary diagram of how filer works. <br><img src="https://habrastorage.org/webt/ks/5d/6o/ks5d6oovtnchnmlr2zlgpwvmt5g.jpeg"><br><br>  In the selected section, we <strong>reboot the master server</strong> .  This is necessary because we have MySQL 5.6, and in it semisync replication is not lossless.  Therefore phantom reads are possible, and we need this master, even if it is not dead, to kill it as soon as possible so that the clients disconnect from it.  Therefore, we do a hard reset via Ipmi - this is the first most important operation we need to do.  In MySQL version 5.7, this is not so critical. <br><br>  <strong>Cluster sync.</strong>  Why do we need cluster synchronization? <br><img src="https://habrastorage.org/webt/gh/pa/go/ghpago_p12c1jittnig4rwhc1sg.jpeg"><br><br>  If we recall the previous picture with our topology, one master server has three slave servers: two in one data center, one in the other.  With the promotion, we need the master to be in the same main data center.  But sometimes, when the slave is loaded, with semisync it happens that the semisync-slave becomes a slave in another data center, because it is not loaded.  Therefore, we need to synchronize the entire cluster first, and then make a promotion on the slave in the data center we need.  This is done very simply: <br><br><ul><li>  We stop all I / O thread on all slave servers. </li><li>  After that, we already know for sure that the master is ‚Äúread-only‚Äù, since semisync has disconnected and no one else can write anything there. </li><li>  Then we select the slave with the largest retrieved / executed GTID Set, that is, with the largest transaction that it has either downloaded or already used. </li><li>  We migrate all slave servers to this selected slave, start the I / O thread, and they are synchronized. </li><li>  We are waiting for them to synchronize, after which we have the entire cluster becomes synchronized.  At the end, we check that all of us are executed GTID set set to the same position. </li></ul><br>  The second important operation is <strong>cluster synchronization</strong> .  Then begins the <strong>promotion</strong> , which happens as follows: <br><img src="https://habrastorage.org/webt/bd/s-/b8/bds-b8fbieqxhtc4dy9ookntiri.jpeg"><br><br><ul><li>  We choose any slave in the data center we need, we tell it that it is master, and we launch the operation of the standard promotion. </li><li>  We reconfigure all the slave servers to this master, stop replication there, use ACLs, drive users in, stop some proxy servers, maybe reboot something. </li><li>  In the end, we do read_only = 0, that is, we say that you can now write to master and update the topology.  From this point on, clients go to this master and everything works for them. </li><li>  Next we have non-critical post-processing steps.  In them we restart some services on this host, redraw the configuration, do additional checks that everything works fine, for example, that the proxy allows traffic to pass. </li><li>  After that, the whole operation is completed. </li></ul><br>  At any step, in case of an error, we are trying to rollback to the point we can.  That is, we cannot rollback to reboot.  But for operations for which this is possible, for example, reassignment - change master - we can return master to the previous step. <br><br><h4>  <strong>Backups</strong> </h4><br>  Backups are a very important topic in databases.  I do not know if you are making backups, but it seems to me that everyone should do them, this is already a beaten joke. <br><br>  <strong>Patterns of use</strong> <br><br>  ‚óè Add a new slave <br><br>  The most important pattern that we use when adding a new slave server, we just restore it from the backup.  It happens all the time. <br><br>  ‚óè Restore data to a point in the past <br><br>  Quite often, users delete data, and then ask them to recover, including from the database.  This rather frequent data recovery operation to a point in the past is automated. <br><br>  ‚óè Recover the entire cluster from scratch <br><br>  Everyone thinks that backups are needed in order to restore all data from scratch.  In fact, this operation almost never required us.  Last time we used it 3 years ago. <br><br><blockquote>  We look at backups as a product, so we tell customers that we have guarantees: <br><br><ol><li>  We can restore any database.  Under normal conditions, the expected recovery rate of 1Tb in 40 minutes. <br></li><li>  Any base can be restored to any location in the past six days. <br></li></ol></blockquote><br>  These are our main guarantees that we give to our customers.  The speed of 1 TB in 40 minutes, because there are restrictions on the network, we are not alone on these racks, traffic is still produced on them. <br><br><h4>  <strong>Cycle</strong> </h4><br>  We have introduced such an abstraction as a cycle.  In one cycle, we try to backup almost all of our databases.  We simultaneously spin 4 different cycles. <br><img src="https://habrastorage.org/webt/bv/-k/_z/bv-k_znrl7zi2obmotthihjogyo.jpeg"><br><br><ul><li>  The first cycle is performed <strong>every 24 hours</strong> .  We back up all our hijacked databases on HDFS, that's about a thousand or more hosts. </li><li>  <strong>Every 6 hours</strong> we make backups for unsharded databases, we still have some data on the Global DB.  We really want to get rid of them, but, unfortunately, they still exist. </li><li>  <strong>Every 3 days</strong> we save all the information from the S3 shard database. </li><li>  <strong>Every 3 days,</strong> we completely save all information on non-chard database to S3. </li></ul><br><img src="https://habrastorage.org/webt/e1/yx/3s/e1yx3s-1ikyhxnuzeympjsvem14.jpeg"><br><br>  All this is stored for several cycles.  Suppose if we store 3 cycles, then in HDFS we have the last 3 days, and the last 6 days in S3.  So we support our guarantee. <br><br>  This is an example of how they work. <br><img src="https://habrastorage.org/webt/sn/hz/1l/snhz1lmio2naq40wziys-jggaaw.jpeg"><br><br>  In this case, we have two long cycles running, which make backups of shardirovanny databases, and one short one.  At the end of each cycle, we will necessarily verify that the backups work, that is, we do recovery on a certain percentage of the database.  Unfortunately, we cannot recover all the data, but we definitely check some percentage of the data for the cycle.  As a result, we will have 100 percent of backups that we restored. <br><br>  We have certain shards that we always restore to watch the recovery rate, to monitor possible regressions, and there are shards that we restore just randomly to check that they have recovered and are working.  Plus, when cloning, we also recover from backups. <br><br><h5>  Hot backups </h5><br><img src="https://habrastorage.org/webt/4m/4b/kb/4m4bkboro5zunrljkwxbybj7jsi.jpeg"><br><br>  Now we have a hot backup for which we use the Percona tool xtrabackup.  We run it in the ‚Äîstream = xbstream mode, and it returns us on the working database, the stream of binary data.  Next we have a script-splitter, which this binary stream divides into four parts, and then we compress this stream. <br><br>  MySQL stores data on a disk in a very strange way and we have more than 2x compression.  If the database is 3 TB, then, as a result of compression, the backup takes approximately 1 500 GB.  Next, we encrypt this data so that no one can read it, and send it to HDFS and S3. <br><br>  In the opposite direction it works exactly the same. <br><img src="https://habrastorage.org/webt/j3/il/jm/j3iljma8c0rekqweak5ngqvaxkk.jpeg"><br><br>  We prepare the server, where we will install the backup, get the backup from HDFS or from S3, decode and decompress it, the splitter compresses it all and sends it to xtrabackup, which restores all data to the server.  Then crash-recovery occurs. <br><br>  Some time, the most important problem of hot backups was that crash-recovery takes quite a long time.  In general, you need to lose all transactions during the time you make a backup.  After that we lose binlog so that our server will catch up with the current master. <br><br>  <strong>How do we save binlogs?</strong> <br><br>  We used to save binlog files.  We collected the master files, alternated them every 4 minutes, or 100 MB each, and saved to HDFS. <br><br>  Now we use a new scheme: there is a certain Binlog Backuper, which is connected to replications and to all databases.  He, in fact, constantly merges binlog to himself and stores them on HDFS. <br><img src="https://habrastorage.org/webt/on/o3/ce/ono3cesuissuuwuzcglcfautfjo.jpeg"><br><br>  Accordingly, in the previous implementation we could lose 4 minutes of binary logs, if we lost all 5 servers, in the same implementation, depending on the load, we lose literally seconds.  Everything stored in HDFS and in S3 is stored for a month. <br><br><h5>  Cold backups </h5><br>  We are thinking of switching to cold backups. <br><br>  Prerequisites for this: <br><br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The speed of the channels on our servers has increased - it was 10 GB, it became 45 GB - you can recycle more. </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> I want to restore and create clones faster, because we need a smarter scheduler for multi instances and want very often to transfer the slave and master from the server to the server. </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The most important point - with a cold backup, we can guarantee that the backup works. </font><font style="vertical-align: inherit;">Because when we make a cold backup, we simply copy the file, then we start the database, and as soon as it starts up, we know that this backup works. </font><font style="vertical-align: inherit;">After pt-table-checksum we know for sure that the data on the file system is consistent.</font></font><br></li></ol><br> <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Warranties</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> that were obtained with cold backups in our experiments:</font></font><br><br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Under normal conditions, the expected recovery rate is 1TB in 10 minutes, because it is just copying files. </font><font style="vertical-align: inherit;">No need to do a crash-recovery, but this is the most problematic place.</font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Any base can be restored for any period of time for the last six days. </font></font><br></li></ol><br><img src="https://habrastorage.org/webt/2m/bd/ar/2mbdarekbku4hsxygdufshzyhnm.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In our topology, there is a slave in another data center that does almost nothing. </font><font style="vertical-align: inherit;">We periodically stop it, make a cold backup and run it back.</font></font> Everything is very simple. <br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Plans ++ </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">These are plans for the distant future. </font><font style="vertical-align: inherit;">When we update our Hardware Park, we want to add an additional spindle disk (HDD) of about 10 TB to each server, and make hot backups + crash recovery xtrabackup on it, and then download backups. </font><font style="vertical-align: inherit;">Accordingly, we will have backups on all five servers simultaneously, at different points in time. </font><font style="vertical-align: inherit;">This, of course, will complicate all the processing and operation, but will reduce the cost, because the HDD costs a penny, and a huge cluster of HDFS is expensive.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Clone </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> As I said, cloning is a simple operation: </font></font><br><br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> this is either recovery from backup and playing binary logs; </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> or the backup process immediately to the target server. </font></font><br></li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> In the diagram, where we copy to HDFS, the data is also simply copied to another server, where there is a receiver that receives all the data and restores it. </font></font><br><br><a name="automation"></a><h2>  Automation </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Of course, on 6,000 servers, no one does anything manually. </font><font style="vertical-align: inherit;">Therefore, we have various scripts and automation services, there are a lot of them, but the main ones are:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Auto-replace; </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> DBManager; </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Naoru, Wheelhouse </font></font></li></ul><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Auto-replace </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This script is needed when the server is dead, and you need to understand whether it is true, and what the problem is - maybe the network is broken or something else. It needs to be resolved as quickly as possible. </font></font><br><br> <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Availability</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> is a function of the time between the occurrence of errors and the time over which you can detect and repair this error. We can fix it very quickly - our recovery is very fast, so we need to determine the existence of a problem as soon as possible. </font></font><br><img src="https://habrastorage.org/webt/-k/em/tr/-kemtrpvhgocinlvlmnsuj1eq-s.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">On each MySQL server, the service that heartbeat writes is running. Heartbeat is the current timestamp. </font></font><br><img src="https://habrastorage.org/webt/cn/r8/fn/cnr8fn-fpy_ddnthddjem3afr4o.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There is also another service that writes the value of some predicates, for example, that master is in read-write mode. After that, the second service sends this heartbeat to the central repository.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We have an auto-replace script that works like this. </font></font><br><img src="https://habrastorage.org/webt/3u/r9/sx/3ur9sxfxf8dsxtgsgvjz3ublvye.jpeg"> <em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The scheme in the best quality and separately its enlarged fragments are in the </font></font><a href="https://drive.google.com/file/d/1IVD2ap9WhMhY7v5miajXI_LBH_dv5bWn/view"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">presentation of the</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> report, starting with 91 slides. </font></font></em> <br><br> <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What's going on here?</font></font></strong> <br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There is a main loop in which we check heartbeat in the global database. </font><font style="vertical-align: inherit;">We look, this service is registered or not. </font><font style="vertical-align: inherit;">Count the heartbeats, for example, are there two heartbeats in 30 seconds?</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Further, we look, whether their quantity satisfies the threshold value. </font><font style="vertical-align: inherit;">If not, then something is wrong with the server - since it did not send heartbeat.</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">After that, we do reverse check just in case - suddenly these two services have died, something with the network, or the global database cannot for some reason record heartbeat. </font><font style="vertical-align: inherit;">In reverse check, we connect to the broken database and check its status.</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If nothing has already helped, we look to see if the master position is progressing or not, whether recordings occur on it. </font><font style="vertical-align: inherit;">If nothing happens, then this server is definitely not working.</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The last step is auto-replace itself. </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Auto-replace is very conservative. He never wants to do a lot of automatic operations. </font></font><br><br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">First, we check if there have been any recent topology operations? </font><font style="vertical-align: inherit;">Maybe this server has just been added and something is not yet running on it.</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> We check if there were any replacements in the same cluster at some time interval. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We check what our failure limit is. </font><font style="vertical-align: inherit;">If we have many problems at once - 10, 20 - then we will not automatically solve them all, because we may inadvertently disrupt the operation of all databases.</font></font></li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Therefore, we </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">solve only one problem at a time</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Accordingly, for the slave server, we start cloning and simply remove it from the topology, and if it is master, then we launch the file shareer, the so-called emergency promotion.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> DBManager </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DBManager is a service for managing our databases. </font><font style="vertical-align: inherit;">He has:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> smart task scheduler that knows exactly when to start which job; </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> logs and all information: who, when and what launched - this is the source of truth; </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> synchronization point. </font></font></li></ul><br><img src="https://habrastorage.org/webt/xx/g6/pi/xxg6pitu-pau9ifyqivpa9wul3e.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> DBManager is quite simple architecturally. </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> There are clients that are either DBAs that do something through the web interface, or scripts / services that have written DBAs that access gRPC. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> There are external systems like Wheelhouse and Naoru, which through gRPC goes to DBManager. </font></font></li><li>  ,  ,  ,      . </li><li>    worker, ,     ,  ,   PID. Worker  ,   .  worker'      ,    , , ,   ACLS       -. </li><li>   SQL-     DBAgent ‚Äî  RPC .    -   ,   RPC . </li></ul><br>    web   DBManager,      ,    ,     ,           .. <br><img src="https://habrastorage.org/webt/yj/tq/3m/yjtq3mrfimptba1kizre2bwie48.jpeg"><br><br>    CLI ,           . <br><img src="https://habrastorage.org/webt/qi/rc/vp/qircvpuoutswvmcpnuca4wem9cu.jpeg"><br><br><h3> Remediations </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We also have a problem response system. </font><font style="vertical-align: inherit;">When something breaks in us, for example, the disk has failed, or some service does not work, </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Naoru</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> works </font><strong><font style="vertical-align: inherit;">. </font></strong><font style="vertical-align: inherit;">This is a system that works throughout Dropbox, everyone uses it, and it was built for just such small tasks. </font><font style="vertical-align: inherit;">I talked about Naoru in my </font><font style="vertical-align: inherit;">2016 </font></font><a href="https://youtu.be/4Fe3DyzPtlo"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">report</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wheelhouse</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> system </font><strong><font style="vertical-align: inherit;">is</font></strong><font style="vertical-align: inherit;"> based on the </font><strong><font style="vertical-align: inherit;">state</font></strong><font style="vertical-align: inherit;"> machine and is designed for long processes. </font><font style="vertical-align: inherit;">For example, we need to update the kernel on all MySQL on our entire cluster of 6000 machines. </font><font style="vertical-align: inherit;">Wheelhouse does this clearly - it updates on the slave server, launches the promotion, the slave becomes the master, it updates on the master server. </font><font style="vertical-align: inherit;">This operation can take a month or even two.</font></font><br><br><a name="monitoring"></a><h2>  Monitoring </h2><br><img src="https://habrastorage.org/webt/vo/n_/ys/von_ysxtakb7m5ctiadwzewi3dw.jpeg"><br><br>  It is very important. <br><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> If you do not monitor the system, then most likely it does not work. </font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We monitor everything in MySQL - all the information we can get from MySQL, we have somewhere saved, we can access it in time. </font><font style="vertical-align: inherit;">We save information on InnoDb, statistics on requests, on transactions, on the length of transactions, on the length of the transaction, on replication, on the network - all, all - a huge number of metrics.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Alert </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We have set up 992 alerts. </font><font style="vertical-align: inherit;">In fact, no one looks at the metrics, it seems to me that there are no people who come to work and start looking at the metrics chart, there are more interesting tasks. </font></font><br><img src="https://habrastorage.org/webt/q3/nj/sl/q3njslzahbxmh585uzeatrn4plm.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Therefore, there are alerts that are triggered when certain threshold values ‚Äã‚Äãare reached. </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We have 992 alerts, no matter what happens, we will know about it</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Incidents </font></font></h3><br><img src="https://habrastorage.org/webt/bi/ce/tp/bicetpzgstf2lggazas6t27ru10.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We have PagerDuty - this is a service through which alerts are sent to those in charge who start taking action. </font></font><br><img src="https://habrastorage.org/webt/6n/hl/sr/6nhlsrj28tloxq4xg5ld3a-mpvy.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In this case, an error occurred in the emergency promotion and immediately after that an alert was registered that the master had fallen. After that, the duty officer checked what prevented the emergency promotion, and did the necessary manual operations. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We are sure to analyze every incident that occurred, for each incident we have a task in the task tracker. Even if this incident is a problem in our alerts, we also create a task, because if the problem is in the logic of the alert and the thresholds, they need to be changed. Alerts should not just ruin people's lives. Alert is always painful, especially at 4 am.</font></font><br><br><a name="testing"></a><h2>  Testing </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As with monitoring, I am sure that everyone is involved in testing. </font><font style="vertical-align: inherit;">In addition to the unit tests with which we cover our code, we have integration tests in which we test:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> all the topologies that we have; </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> all operations on these topologies. </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If we have promotion operations, we test in the integration test of promotion operations. </font><font style="vertical-align: inherit;">If we have cloning, we do cloning for all the topologies we have. </font></font><br><br> <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sample Topology</font></font></strong> <br><img src="https://habrastorage.org/webt/dv/hh/va/dvhhvacdschtqr7s_ynecm0lyk0.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> We have a topology for all occasions: 2 data centers with multi instance, shards, no shards, clusters, one data center - in general, almost any topology - even those that we do not use, just to view .</font></font><br><img src="https://habrastorage.org/webt/f-/oy/pl/f-oypluoyzosnphjl_aukp4vsks.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In this file we just have the settings, which servers and with what we need to raise. </font><font style="vertical-align: inherit;">For example, we need to raise the master, and we say that it is necessary to do this with such data of the instances, with such databases at such ports. </font><font style="vertical-align: inherit;">We have almost everything going with the help of Bazel, which creates a topology on the basis of these files, starts the MySQL server, then the test is started. </font></font><br><img src="https://habrastorage.org/webt/bg/zd/b_/bgzdb_dwnggh3kf8uv9f7mn9jpe.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The test looks very simple: we indicate which topology is used. </font><font style="vertical-align: inherit;">In this test, we test auto_replace.</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> We create auto_replace service, we start it. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We kill the master in our topology, wait some time and see that the target-slave has become the master. </font><font style="vertical-align: inherit;">If not, the test fails.</font></font></li></ul><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Stages </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Stage environments are the same databases as in production, but they do not have user traffic, but there is some kind of synthetic traffic that is similar to production through Percona Playback, sysbench, and similar systems. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In Percona Playback, we record traffic, then lose it on the stage-environment with different intensity, we can lose 2-3 times faster. That is, it is an artificial, but very close to the real load. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This is necessary because in integration tests we cannot test our production. We cannot test the alert or that the metrics work. At testing we test alerts, metrics, operations, periodically we kill servers and see that they are normally assembled.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Plus, we test all the automations together, because in integration tests, most likely, one part of the system is tested, and in the staging all the automated systems work simultaneously. </font><font style="vertical-align: inherit;">Sometimes you think that the system will behave this way and not otherwise, but it may behave differently.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> DRT (Disaster recovery testing) </font></font></h3><br>  We also conduct tests in production - right on real databases.  This is called Disaster recovery testing.  Why do we need it? <br><br>  ‚óè We want to test our warranty. <br><br>  This is done by many large companies.  For example, Google has one service that worked so stably - 100% of the time - that all the services that used it decided that this service is really 100% stable and never drops.  Therefore, Google had to drop this service specifically for users to take into account this possibility. <br><br>  So we - we have a guarantee that MySQL works - and sometimes it does not work!  And we have a guarantee that it may not work for a certain period of time, customers should take this into account.  Periodically, we kill the production master, or, if we want to make a file share, we kill all the slave servers to see how semisync replication behaves. <br><br>  ‚óè Clients are ready for these errors (replacement and death of the master) <br><br>  Why is it good?  We had a case when with the promotion of 4 shards from 1600, the availability dropped to 20%.  It seems that something is wrong, for 4 shards out of 1600 there must be some other numbers.  Faylovera for this system occurred quite rarely, about once a month, and everyone decided: "Well, this is a faylover, it happens." <br><br>  At some point, when we switched to a new system, one person decided to optimize those two heartbeat recording services and merged them into one.  This service did something else and, eventually, was dying and heartbeats stopped recording.  It so happened that for this client we had 8 faylover per day.  All lay - 20% availability. <br><br>  It turned out that in this client keep-alive 6 hours.  Accordingly, as soon as the master was dying, all of our connections were kept for another 6 hours.  The pool could not continue to work - it keeps connections, it is limited and does not work.  It repaired. <br><br>  Doing a faylover again - no longer 20%, but still a lot.  Something's all wrong anyway.  It turned out that the bug in the implementation of the pool.  The pool, when requested, addressed many shards, and then put it all together.  If any shards were fake, some race condition occurred in the Go code, and the whole pool was clogged.  All these shards could not work anymore. <br><br>  Disaster recovery testing is very useful, because clients must be prepared for these errors, they must check their code. <br><br>  ‚óè Plus Disaster recovery testing is good because it goes into business hours and everything is in place, less stress, people know what will happen now.  This is not happening at night, and it's great. <br><br><h2>  Conclusion </h2><br>  1. Everything needs to be automated, never clap hands. <br>  Each time when someone climbs into the system with our hands, everything dies and breaks in us - every single time!  - even on simple operations.  For example, one slave died, the person had to add a second, but decided to remove the dead slave with his hands from the topology.  However, instead of the deceased, he copied the live command into the command - the master was left without a slave at all.  Such operations should not be done manually. <br><br>  2. Tests should be permanent and automated (and in production). <br>  Your system is changing, your infrastructure is changing.  If you checked it once and it seemed to work, it does not mean that it will work tomorrow.  Therefore, it is necessary to do automated testing every day, including in production. <br><br>  3. It is imperative to own clients (libraries). <br>  Users may not know how databases work.  They may not understand why they need timeouts, keep-alive.  Therefore, it is better to own these customers - you will be calmer. <br><br>  4. You need to define your principles for building a system and your guarantees, and always comply with them. <br><br>  Thus it is possible to support 6 thousand database servers. <br><br><div class="spoiler">  <b class="spoiler_title">In the questions after the report, and especially the answers to them, there is also a lot of useful information.</b> <div class="spoiler_text"><h2>  Questions and answers <br></h2><br><blockquote>  - What will happen if there is an imbalance in the load on shards - some kind of meta-information about some file turned out to be more popular?  Is it possible to dissolve this shard, or is the load on shards different nowhere by orders of magnitude? </blockquote><br>  It does not differ by orders of magnitude.  It is almost normally distributed.  We have throttling, that is, we can not overload the shard in fact, we throttle at the client level.  In general, it happens that some star puts a photo, and the shard almost explodes.  Then we ban this link. <br><br><blockquote>  - You said you have 992 alerts.  Is it possible in more detail, what is it - is it out of the box or is it being created?  If it is, then is it manual labor or something like machine learning? </blockquote><br>  This is all created manually.  We have our own internal system, called Vortex, where metrics are stored, it supports alerts.  There is a yaml-file in which it is written that there is a condition, for example, that backups should be performed every day, and if this condition is met, then the alert does not work.  If not, then an alert comes. <br><br>  This is our internal development, because few people can store as many metrics as we need. <br><br><blockquote>  - How strong should the nerves be to do a DRT?  You dropped, CODERED, does not rise, with every minute of panic more and more. </blockquote><br>  Generally working in databases is a real pain.  If the database is down, the service does not work, the entire Dropbox does not work.  This is a real pain.  DRT is useful in that it is a business watch.  That is, I am ready, I sit at my desk, I have drunk coffee, I am fresh, I am ready to do anything. <br><br>  Worse, when it happens at 4 am, and it is not DRT.  For example, the last major failure we had recently.  When infusing a new system, we forgot to set the OOM Score for our MySQL.  There was another service that read binlog.  At some point, our operator manually - again, manually!  - runs a command to delete some information in Percona checksum-table.  Just a normal delete, a simple operation, but this operation spawned a huge binlog.  Service read this binlog in memory, OOM Killer came and thinks, who would kill?  And we forgot to set the OOM Score, and it kills MySQL! <br><br>  40 masters die at 4 o'clock in the morning.  When 40 masters die, it's really very scary and dangerous.  DRT is not scary and not dangerous.  We lay for about an hour. <br><br>  By the way, DRT is a good way to rehearse such moments, so that we know exactly which sequence of actions is needed if something breaks down en masse. <br><br><blockquote>  - I would like to know more about switching master-master.  First, why not use a cluster, for example?  A cluster of databases, that is, not a master-slave with a switch, but a master-master application, so that if one falls, then it is not scary. </blockquote><br>  Do you mean something like group replication, galera cluster, etc.?  It seems to me that the group application is not yet ready for life.  Galera we, unfortunately, have not tried.  It‚Äôs great when the file is inside your protocol, but unfortunately they have so many other problems, and it‚Äôs not so easy to switch to this solution. <br><br><blockquote>  - It seems there is something like an InnoDb cluster in MySQL 8.  Did not try? </blockquote><br>  We still still have 5.6 worth.  I do not know when we will go to 8. Maybe we will try. <br><br><blockquote>  - In this case, if you have one big master, when switching from one to another, a queue is accumulated on the slave servers with a high load.  If the master is redeemed, is it necessary for the queue to run, for the slave to switch to master mode - or is it somehow done differently? </blockquote><br>  The load on the master is regulated by semisync.  Semisync restricts master slave write performance.  Of course, it may be that the transaction has arrived, semisync has worked, but the slaves have been losing this transaction for a very long time.  You must then wait until the slave loses this transaction to the end. <br><br><blockquote>  - But then new data will be sent to the master, and it will be necessary ... </blockquote><br>  When we start the promotion process, we disable I / O.  After this, master cannot write anything, because semisync replication.  A phantom reading may come, unfortunately, but this is another problem already. <br><br><blockquote>  ‚ÄúThese are all beautiful state machines ‚Äî what are the scripts written on and how difficult is it to add a new step?‚Äù  What should be done to the one who writes this system? </blockquote><br>  All scripts are written in Python, all services are written in Go.  This is our policy.  Logic is easy to change - just in the Python code that generated the state diagram. <br><br><blockquote>  - And you can learn more about testing.  How are the tests written, how do they deploy the nodes in the virtual machine - are these containers? </blockquote><br>  Yes.  Testing is going with us using Bazel.  There are some configuration files (json) and Bazel picks up a script that creates a topology for this test using this configuration file.  Different topologies are described there. <br><br>  All this works for us in docker-containers: either it works in CI, or on Devbox.  We have a Devbox system.  We are all developing on some kind of remote server, and this may work on it, for example.  There it also runs inside a Bazel, inside a docker container or in a Bazel Sandbox.  Bazel is very difficult, but cool. <br><br><blockquote>  - When you made 4 instances on one server, did you lose any memory efficiency? </blockquote><br>  Each instance has become smaller.  Accordingly, the lower MySQL memory operates, the easier it is to live.  It is easier for any system to operate with a small amount of memory.  In this place we have not lost anything.  We have the simplest C-groups that limit these instances on memory. <br><br><blockquote>  - If you have 6,000 servers store databases, can you tell how many billions of petabytes are stored in your files? </blockquote><br>  These are dozens of exabytes, we have poured data from Amazon over the course of the year. <br><br><blockquote>  - It turns out that you initially had 8 servers, they each had 200 shards, then 400 servers with 4 shards each.  You have 1600 shards - is this some kind of fixed value?  You can never do again?  Will it hurt if you need, for example, 3,200 shards? </blockquote><br>  Yes, it was originally 1600. This was done a little less than 10 years ago, and we still live.  But we still have 4 shards - 4 times we can still increase the place. <br><br><blockquote>  - How do the servers die, mainly for what reasons?  What happens more often, which is less common, and especially interesting, do spontaneous blockade blocks occur? </blockquote><br>  The most important thing is that the disks fly out.  Our RAID 0 drive crashed, the master died.  This is the main problem, but it is easier for us to replace this server.  Google is easier to replace the data center, we server yet.  Corruption checksum we almost did not happen.  To be honest, I do not remember when it was the last time.  Simply we update wizards often enough.  We have the life of a master is limited to 60 days.  It cannot live longer, after that we replace it with a new server, because for some reason something constantly accumulates in MySQL, and after 60 days we see that problems start to occur.  Maybe not in MySQL, maybe in Linux. <br><br>  We do not know what the problem is and do not want to deal with it.  We just limited the time to 60 days, and update the entire stack.  No need to stick to one master. <br><br><blockquote>  - You said that in the last 6 days you can recover from a backup to any state.  For example, a person uploaded a JPEG with the same name, then uploaded the same JPEG, but modified, then can you get the first version?  That is, it turns out, you keep the versioning of files and some metadata with versions?  If a person asks - I want to get the first version of the file, can you give it to him or not? </blockquote><br>  We store information about the file, about the blocks.  We can - Dropbox has the ability to recover files. <br><br><blockquote>  ‚ÄúHow do you clean it all up later?‚Äù  No problems with fragmentation on disks and so on?  A lot of data is erased from the disk, it turns out, after some time, when the version becomes unnecessary, rotten?  Suppose a person has uploaded 10 versions of files alternately.  Obviously, after 7 days in the backup, you will realize that you do not need the first 6 versions, and they need to be deleted.  Or are they stored forever? </blockquote><br>  In general, Dropbox has some guarantees for how long the version is stored.  This is slightly different.  There is a system that can recover files, and there the files are simply not deleted immediately, they are put in some kind of recycle bin. <br><br>  There is a problem when absolutely everything is removed.  There are files, there are blocks, but there is no information in the database how to assemble a file from these blocks.  At such a moment we can lose up to a certain point, that is, we recovered in 6 days, lost until the moment when this file was deleted, did not begin to delete it, restored it and gave it to the user. <br></div></div><br><blockquote>  Follow the blog or subscribe to the <a href="http://www.highload.ru/moscow/2018">newsletter</a> , <a href="https://www.facebook.com/HighLoadConference/">facebook</a> or <a href="https://www.youtube.com/profyclub">youtube channel</a> - we regularly publish fresh materials and updates in the preparation of <a href="http://www.highload.ru/moscow/2018">Highload ++ 2018</a> .  In the latter, you can take an active part, by <strong>September 1,</strong> sending an <a href="https://conf.ontico.ru/lectures/propose%3Fconference%3Dhl2018">application for a report</a> . <br></blockquote></div><p>Source: <a href="https://habr.com/ru/post/417315/">https://habr.com/ru/post/417315/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../417303/index.html">Hack to support buttons Android-headset under Windows</a></li>
<li><a href="../417305/index.html">Ultima Online: a look from behind the scenes</a></li>
<li><a href="../417307/index.html">Glaucoma - not heard of it? Meet the serial silent view killer</a></li>
<li><a href="../417309/index.html">ITSM-manager for happiness: how the profession of the future helps to expand the boundaries of desk services</a></li>
<li><a href="../417311/index.html">Creating a bot to participate in the AI ‚Äã‚Äãmini cup 2018 based on a recurrent neural network</a></li>
<li><a href="../417317/index.html">On Highload ++ 2018 full speed ahead</a></li>
<li><a href="../417319/index.html">Systems in the case or What is actually under the cover of the microprocessor case</a></li>
<li><a href="../417321/index.html">How do we look for online course teachers among developers?</a></li>
<li><a href="../417323/index.html">Problems of ensuring 100% availability of the project</a></li>
<li><a href="../417325/index.html">Open Day at Netology, Data Science theme</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>