<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The system of monitoring opinions using pointwise mutual information</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello. 
 If you are engaged in DataMining, analyzing texts for identifying opinions, or you are just interested in statistical models for evaluating t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The system of monitoring opinions using pointwise mutual information</h1><div class="post__text post__text-html js-mediator-article">  Hello. <br>  If you are engaged in DataMining, analyzing texts for identifying opinions, or you are just interested in statistical models for evaluating the emotional coloration of sentences - this article may be interesting. <br>  Further, in order not to waste the time of a potential reader on a pile of theory and reasoning, the results are immediately brief. <br>  The implemented approach works with approximately 55% accuracy in three classes: negative, neutral, positive.  As <a href="http://en.wikipedia.org/wiki/Sentiment_analysis">Wikipedia</a> says, 70% accuracy is approximately equal to the accuracy of human judgments on average (due to the subjectivity of each interpretation). <br>  It should be noted that there are quite a few utilities with an accuracy higher than that obtained by me, but the described approach can be rather simply improved (to be described below) and we end up with 65-70%.  If after all of the above, you still have a desire to read - welcome under cat. <br><a name="habracut"></a><br><br><h4>  Brief description of the principle </h4><br>  To determine the emotional color of the sentence (SO - sentiment orientation), it is necessary to understand what it is about.  This is logical.  But how to explain to the car what is good and what is bad? <br>  The first option, immediately appearing on the mind, is the sum of the number of bad / good words multiplied by the weight of each.  The so-called <a href="http://en.wikipedia.org/wiki/Bag_of_words_model">‚Äúbag of words‚Äù</a> approach.  A surprisingly simple and fast algorithm, combined with <a href="http://en.wikipedia.org/wiki/Rule-based_system">rule-</a> based preprocessing <a href="http://en.wikipedia.org/wiki/Rule-based_system">,</a> yielding good results (up to 60‚Äì80% accuracy depending on the <a href="http://en.wikipedia.org/wiki/Corpus_linguistics">case</a> ).  In essence, this approach is an example of a <a href="http://en.wikipedia.org/wiki/Language_model">unigram model</a> , which means that in the most naive case, the sentences ‚ÄúThis product rather than bad‚Äù and ‚ÄúThis product rather than good‚Äù will have the same SO.  This problem can be solved by moving from a unigram to a <a href="http://data.princeton.edu/wws509/notes/c6.pdf">multinomial</a> model.  It should also be noted that it is necessary to have a solid constantly updated dictionary containing bad and good terms + their weight, which can be specific depending on the data. <br><br>  An example of the simplest multinomial model is the <a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">naive Bayes method</a> .  On Habr√© there are several articles devoted to him, in particular <a href="http://habrahabr.ru/post/120194/">this</a> . <br>  The advantage of the multinomial model over the unigram model is that we can take into account the context in which a particular utterance was uttered.  This solves the problem with the sentences described above, but introduces a new constraint: if the selected <a href="http://en.wikipedia.org/wiki/N-gram">n-gram is</a> missing from the training set, then the SO on the test data will be 0. This problem has always been and will be.  It can be solved in two ways: by increasing the size of the training sample (not forgetting that you can simultaneously capture the effect of retraining), or by using smoothing (for example, <a href="http://en.wikipedia.org/wiki/Laplacian_smoothing">Laplace</a> or Good-Turing). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Finally, we smoothly approached the idea of ‚Äã‚ÄãPMI. <br>  Along with the Bayes formula <img src="https://habrastorage.org/storage2/29b/b25/c56/29bb25c56d9e6e6cabc30a234795f58b.png">  , we introduce the concept <br><img src="https://habrastorage.org/storage2/a59/c7f/040/a59c7f04092e9848d51d7e342e289be5.png"><br>  PMI - pointwise mutual information, point-to-point mutual information. <br>  in the above formula, A and B are words / bigrams / n-grams, P (A), P (B) are a priori probabilities of occurrence of the term A and B, respectively, in the training set (the ratio of the number of occurrences to the total number of words in the body), P (A near B) - the probability of the term A to meet together / next to the term B;  ‚ÄúNear‚Äù can be configured manually; by default, the distance is 10 terms left and right;  the base of the logarithm does not matter, for simplicity we take it equal to 2. <br>  A positive sign of the logarithm will mean positive color A compared to B, negative - negative. <br>  To find neutral reviews, you can take some kind of sliding window (in this work, the segment [-0.154, 0.154] is responsible for this).  The window can be either constant or floating depending on the data (shown below). <br>  From the above, we can come to the following statements: <br><img src="https://habrastorage.org/storage2/baf/171/3b3/baf1713b318ef989f900618a469d2d7d.png"><br>  Indeed, to determine to which class the statements ‚Äúgood weather‚Äù, ‚Äúgo fast‚Äù belong, it is enough to check in the training set how often ‚Äúgood weather‚Äù and ‚Äúgo fast‚Äù are found next to the known (set by a person depending on the data model and test sample ) good and bad words and set the difference. <br>  Let's go a little further and instead of comparing with the one pivot word from the negative and positive side, we will use a set of obviously good and bad words (here, for example, the following words were used: <br>  <i>Positive:</i> good, nice, excellent, perfect, correct, super <br>  <i>Negative</i> : bad, nasty, poor, terrible, wrong, awful <br>  Accordingly, the final formula <br><img src="https://habrastorage.org/storage2/295/f00/45c/295f0045c576f7d26be942c10877ed2a.png"><br><br>  So, with SO counting figured out, but how to choose suitable candidates? <br>  For example, we have a sentence ‚ÄúToday is a wonderful morning, it would be good to go to the lake.‚Äù <br>  It is logical to assume that the adjectives and adverbs add the emotional color to the sentence.  Therefore, in order to take advantage of this, we will construct a finite automaton, which, according to given patterns of parts of speech, will be selected from the proposal of candidates for the evaluation of SO.  It is not difficult to guess that the proposals will be considered a positive review if the sum SO of all candidates is&gt; 0.154. <br>  The following patterns were used in this work: <br><img src="https://habrastorage.org/storage2/308/6e9/b2e/3086e9b2e43d86b7a996d79a83431f48.png"><br>  In this case, the candidates will be: <br>  1. wonderful morning <br>  2. good to go <br><br>  It remains only to put everything together and test. <br><br><h4>  Implementation </h4><br><br>  <a href="http://depositfiles.com/files/uxusbsm63">Here</a> you will find Java sources.  There is little beauty - it was written just to try and decide whether it will be used further. <br>  Housing: Amazon Product Review Data (&gt; 5.8 million reviews) <a href="http://liu2.cs.uic.edu/data/">liu2.cs.uic.edu/data</a> <br>  With the help of <a href="http://lucene.apache.org/core/">Lucene, the</a> inverted index was built on this case, and the search was made by it. <br>  In the absence of data in the index, Google search engines (api) and Yahoo!  (with their operator around and near respectively).  But, unfortunately, due to the speed of work and the inaccuracy of the results (according to high-frequency queries, search engines give an approximate value of the number of results), the solution is not perfect. <br>  To determine the parts of speech and tokenization, the <a href="http://opennlp.apache.org/">OpenNLP</a> library was <a href="http://opennlp.apache.org/">used.</a> <br><br><h4>  Which is better </h4><br>  Based on the foregoing, the most preferred vectors of improvements are: <br>  1. Building a more complete tree of analysis of parts of speech to filter candidates <br>  2. Using a larger binder as a training set. <br>  3. If possible, use a learning corps from the same socialmedia as the test sample <br>  4. Formation of reference words (good | bad) depending on the source of data and subject <br>  5. Introducing negation into the template parse tree <br>  6. Definition of sarcasm <br><br><h4>  findings </h4><br>  In general, a system based on PMI can compete with systems based on the ‚Äúbag of words‚Äù principle, but in an ideal implementation these two systems should complement each other: in the absence of data in the training set, the system of counting specific words should come into play. <br><br><h4>  References: </h4><br>  1. Introduction to information retrieval.  K. Manning, P. Raghavan, H. Sch√ºtze <br>  2. Foundations of statistical natural language processing.  C. Manning, H. Schutze <br>  3. Thumbs Up or Thumbs Down?  Semantic Orientation Applied to Unsupervised Classification of Reviews.  Peter D. Turney </div><p>Source: <a href="https://habr.com/ru/post/140739/">https://habr.com/ru/post/140739/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../140733/index.html">npm2debian: debianization of npm packages</a></li>
<li><a href="../140734/index.html">Substitution of HTTP Server Header for various web servers</a></li>
<li><a href="../140735/index.html">The course of the young soldier "Sell services web designer." Part 1 "Positioning"</a></li>
<li><a href="../140736/index.html">Detecting the location of a host on an unmanaged network</a></li>
<li><a href="../140737/index.html">Scrum-ban</a></li>
<li><a href="../140740/index.html">Error correction codes. Software implementation options</a></li>
<li><a href="../140741/index.html">Badoo Technologies - open meeting within RIT ++ (admission free)</a></li>
<li><a href="../140743/index.html">Safari browser bug</a></li>
<li><a href="../140744/index.html">Grocery CRUD, or how I made my life easier for a week</a></li>
<li><a href="../140745/index.html">Java development environments, or from Netbeans to Eclipse</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>