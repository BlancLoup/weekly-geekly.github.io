<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The first decade of augmented reality</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Each truly revolutionary technology goes through four stages of development: an idea, a demo, a successful project, a mass product. This is clearly se...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The first decade of augmented reality</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/6d9/d55/e33/6d9d55e330ec4963aa8ef161738e1536.jpg"><br><br>  Each truly revolutionary technology goes through four stages of development: an idea, a demo, a successful project, a mass product.  This is clearly seen in the example of multi-touch devices.  Let's look back at previous years and see what the situation with the technology of augmented reality is, and let's fantasize what it can become as a truly mass product. <a name="habracut"></a><br><br>  In February 2006, Jeff Han (Jeff Han) at his speech at TED talks showed a demo of an experimental "multi-touch" interface (video below).  Today, many things from the presentation seem trivial, they can do each Android-smartphone for $ 50.  And then the audience, for the most part - sophisticated techies, were amazed and applauded.  What is commonplace today was awesome then.  A year later, the first iPhone appeared, and the multitouch idea was ‚Äúnullified‚Äù. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/ac0E6deG4AU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  If you look at the events of the previous decade, it turns out that there were four stages of launching a multitouch.  At first he was an interesting idea research laboratories.  Then came the first public demos.  The third stage is the first truly useful multitouch implemented in the iPhone.  And finally, in a few years, with the development of the iPhone and Android smartphones, massive sales of multitouch devices began.  This delay is reflected in the graph below - several years passed after 2007, even despite the cheaper iPhone, before smartphones became a truly mass commodity.  Similar stages of development are characteristic of many revolutionary technologies.  Rarely, when something is immediately obtained in the finished form.  At the same time, the dead-end technology branches also developed: <a href="https://en.wikipedia.org/wiki/Symbian">Symbian</a> in the West and <a href="https://en.wikipedia.org/wiki/I-mode">iMode</a> in Japan. <br><br><img src="https://habrastorage.org/files/740/2f2/6d5/7402f26d55e74d98bba6a68a626b95d3.png"><br>  I think today the augmented reality is somewhere between the second and third stages.  We have already seen great demos and first prototypes.  We have not yet received massive commercial products, but we are already close to this. <br><br><img src="https://habrastorage.org/files/274/b50/ef2/274b50ef2beb49298dbbda9c96f2d6fe.jpg"><br><br>  Microsoft ships Hololens.  This device well tracks its position in space, has its own computing processor.  However, it is rather cumbersome, has a very narrow field of view (much less than in the videos distributed by Microsoft) and costs $ 3,000.  Probably, the second version of the device can be expected in <a href="https://www.thurrott.com/hardware/90780/microsoft-accelerates-hololens-v3-development-sidesteps-v2">2019</a> .  It is very similar that Apple is also working on something similar, taking into account information about their vacancies and acquisitions, as well as the CEO comments (I suspect that all this is also related to miniaturization, improved power consumption, radio communications, and so on for Apple Watch and AirPod).  Probably, Google will also release something, or Facebook, or Amazon.  Interesting projects sawing a number of smaller companies and startups. <br><br><img src="https://habrastorage.org/files/e4e/b85/867/e4eb858675e141418df8a1b6e229c1c1.jpg"><br><br>  Meanwhile, Magic Leap (in which it has invested a16z) is working on its own wearable technology.  A number of videos have been released demonstrating the capabilities of the prototypes.  But one thing - to watch a video about the gadget, and quite another - to use it.  There is a huge difference between AR-video and everyday wear of such devices, when non-existent objects appear before your eyes.  I tried - very badly. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/kw0-JRa9n94" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  All this means that if we are today at the ‚ÄúJeff Khan‚Äù stage, then we can soon expect the appearance of an analogue of the iPhone 1 in the field of augmented reality.  And then, in 10 years or so, we can get a really massive product.  What can it be, augmented reality for billions of people? <br><br><img src="https://habrastorage.org/files/32d/d68/ea5/32dd68ea59d348289affe714dfbc31c6.png"><br><br>  The first step towards AR was Google Glass.  The screen ‚Äúhangs‚Äù the image in the space in front of you, but it does not interact with the real world.  In essence, Google Glass was conceptually very close to the smartwatch; it just had to look up and to the right, not down and to the left.  The glasses provided a screen for data output, but they had no idea about the environment.  Probably, a more advanced technology will allow the display of information on a virtual spherical screen around the user, where you can display windows, 3D-objects or something else. <br><br>  With the advent of ‚Äúreal augmented reality‚Äù, or ‚Äúhybrid reality‚Äù (mixed reality), devices will be able to warn about surrounding objects, they will be able to place virtual objects instead of them, so that we cannot be sure if what we are seeing is real.  Unlike Google Glass, future AR-glasses will be able to create three-dimensional maps of the surrounding space and constantly monitor the position of the head.  This will put on the wall "virtual TV", which will be there to "hang" while you are in the room.  Or even turn the entire wall into a display.  Or you can put it on a Minecraft coffee table (or <a href="https://en.wikipedia.org/wiki/Populous_%2528video_game%2529">Populous</a> ), and build a world with your hands, create mountains and rivers, like sculpturing with clay.  For that matter, anyone who wears the same glasses can see the same thing.  It will be possible to turn a wall or table in a meeting room into a display for the whole team, or for a family.  Or you can take that little robot over and hide it behind the sofa, and let your children search for it.  Of course, all this has intersections with virtual reality, especially when it comes to adding external cameras.  Hybrid reality will turn the whole world into a screen. <br><br>  Before that, it was only about <a href="https://ru.wikipedia.org/wiki/SLAM_(%25D0%25BC%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4)">SLAM</a> , that is, about creating a three-dimensional map of the room, but not about recognition.  You can go even further.  Suppose I met you at the mitap and see a VKontakte profile card above your head.  Or a note from the CRM system that says you are a key customer.  Or a note from the Truecaller database, according to which you are going to give me some insurance and say goodbye.  Or, as in the Black Mirror series, you can simply ‚Äúblock‚Äù specific people by placing flat black images instead.  That is, glasses do not just scan objects, but recognize them.  Here it will be a real addition to reality.  You do not just superimpose images on real objects, but do it meaningfully, turn them into a part of the real world.  On the one hand, you can enrich - or litter, this is how to look. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://player.vimeo.com/video/166807261" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  On the other hand, you can create very unobtrusive prompts and changes in the world around you.  For example, when traveling, it is not easy to translate each inscription into your native language, but also to choose a specific style.  If today you can put a <a href="http://www.slate.com/blogs/future_tense/2015/05/26/this_chrome_extension_replaces_the_word_millennials_with_snake_people.html">plugin</a> in your browser that replaces ‚Äúmillennial‚Äù with ‚Äúsnake people‚Äù, then what will the future technology of hybrid reality be capable of? <br><br>  If the glasses are compact enough, will you wear them all day?  If not, then with them you will lose many applications.  For example, you can use a combination of a smartphone or a watch, playing the role of an ‚Äúalways on‚Äù device, and glasses that you wear in order to ‚Äúview‚Äù the content correctly.  This would help solve some social issues that have arisen for Google Glass owners: getting a phone, looking at a watch or putting on ordinary glasses is a clear and understandable signal to others, and when you wear Google Glass in a bar, it‚Äôs not at all obvious to people that You stare at the wall, but you read something. <br><br>  All this leads to a logical question - is there a merging of augmented and virtual realities?  This is definitely possible.  Both technologies allow you to perform interrelated things, and imply the need to solve interrelated engineering tasks.  One of them is how to transfer a user to another world using a gadget, which is a VR device.  In theory, you should only look through the glass displays, that is, the glasses should be closed at the edges.  But for AR it is not necessary.  At the same time, the difficulty with AR lies in displaying the world around it, but at the same time cutting off the ‚Äúunnecessary‚Äù (a separate question - how to deal with bright sunlight).  AR-glasses are transparent, that is, the surrounding will be able to see the user's eyes.  Probably in 10-20 years most of the tasks of combining AR and VR will be solved, but today both technologies look like separately developing areas.  In the late 1990s, we argued whether the mobile Internet devices would have a separate radio module and display, plus headphones, plus a keyboard, or it would be a clamshell with a screen and a keyboard.  The industry was looking for the right form factor, and had to wait until 2007 (and even longer) before we came to a device with a display that displays and a keyboard.  Perhaps the ideal form factor for combining VR and AR has not yet been found. <br><br><img src="https://habrastorage.org/files/96c/578/5dd/96c5785ddf1449a28639d1cc0bfd9b15.png"><br><br>  Another question: how will you manage and interact with virtual objects.  Is there enough physical controllers for VR?  Is it easy to track hands (without modeling the movement of the fingers)?  Multitouch in smartphones implies physical interaction: we touch objects of interest to us.  But can we simulate a ‚Äútouch‚Äù to AR-objects?  Is this really a good interaction model for everyday interface?  Magic Leap allows you to create a sense of depth of space, so you believe that you can touch virtual objects.  But you want to use the interface in which your hands go through what should be monolithic?  Do I need to use voice control, and what restrictions will it impose (imagine fully voice control of your phone or computer, even with perfect voice recognition)?  Or is the solution to the problem of tracking eye movements?  For example, look at an object and blink twice to select it?  This kind of questions were solved in the development of personal computers and smartphones, along with the search for suitable form factors.  The answers are not obvious, and there are still more questions. <br><br>  The more you think about the task of embedding AR-objects and data into the world around you, the more it seems that this should be solved by a combination of AI and a physical interface.  For example, coming to you, what should I see - a card from VKontakte or Facebook?  When should I see a new incoming message - immediately or later?  Standing in front of a restaurant, should I say, ‚ÄúHey, Foursquare, is this a good place?‚Äù, Or should the operating system do it for me automatically?  And by whom will this behavior be controlled - OS, services added by me or a single ‚ÄúGoogle Brain‚Äù in the cloud?  Google, Apple, Microsoft and Magic Leap may have different points of view on this, but I believe that many things need to be automated (using AI).  If you recall the words of Eric Raymond that the computer should never ask you about something that it can calculate itself, then we can say that if the computer sees everything you see, then it should understand what you are looking at .  And after 10 years of development of AI, it is possible that we will be able to solve many of the problems now facing, which are still supposed to be solved manually.  When we moved from the window-keyboard-and-mouse model of the desktop interface to touch management and direct interaction with objects in smartphones, a number of issues were removed.  Just changed the level of abstraction.  Smartphones do not ask us where to save photos, or where you are when ordering a taxi, or which application to use to send a letter, or what is your password (if your smartphone is equipped with a fingerprint scanner).  There are no more questions (as well as a choice).  AR may well take a step in the same direction, because it will be something more than appearing before you in the air applications in small windows.  How Snapchat doesn‚Äôt work at all like a desktop Facebook site, and an intangible AR interface managed by AI, can change our ideas again. <br><br><img src="https://habrastorage.org/files/4b0/e1a/f4d/4b0e1af4d7ee431aaf0275feb531c7c4.png"><br><br>  The more AR-glasses will try to understand the world around you (and yourself), the more they will observe and send information about what they see on a myriad of different cloud devices, depending on the context, application methods and application model.  This is the face of a man, and you talk to him?  Send it (or its compressed abstraction - yes, network bandwidth will greatly affect this) in Salesforce, LinkedIn, TrueCaller, Facebook and Tinder.  Is it a pair of shoes?  Pinterest, Ozone and Net a Porter.  Or even send everything to Google.  These and many other situations raise issues related to privacy and security.  It's like with unmanned vehicles that will ever fill our streets: they constantly take off the surrounding space around them, and this is just a haven for observation.  And what will happen when everyone starts wearing AR-glasses - will it be possible to run away from it at all?  And if your glasses hack?  If your smart home is hacked, you will have poltergeists, and if you hack glasses, you will have visual hallucinations. <br><br>  Finally, another important question: how many people will wear AR glasses?  Will they be an additional accessory for mobile phones (for example, like a smart watch)?  Or even in every Brazilian or Indonesian backwater, you can buy a dozen models of Chinese AR-glasses for $ 50, like today Android smartphones?  (And what will happen to the mobile Internet?) It is still too early to predict.  In the late 1990s and early 2000s, we wondered if all of them would have the same type of mobile device, or some would have what we call smartphones today, and most will have traditional push-button cell phones, even primitive devices without a camera or color. the screen?  Looking back, you understand that these were disputes like "whether everyone will have personal computers, or someone will use typewriters (word processors)."  The logic of scaling and computing for general purposes led to the fact that first the PC, and then the smartphone, became the only universal devices.  Today, there are about 5 billion mobile smartphone owners on earth, of which 2.5-3 billion own smartphones - and it is obvious that the rest will follow.  So the question is: will most people use smartphones, and some (100 million? 500 million? 1 billion?) Will switch to AR-glasses as an accessory, or will a new universal product appear?  Any answer to this question is a flight of fancy, not a product of analysis.  But then, in 1995, they said that everyone would have regular phones. </div><p>Source: <a href="https://habr.com/ru/post/327804/">https://habr.com/ru/post/327804/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../327784/index.html">How using PVS-Studio can improve Visual C ++ 2017 Libraries</a></li>
<li><a href="../327786/index.html">GameDev from scratch: How to communicate with the player without words</a></li>
<li><a href="../327788/index.html">Ready native modules for node.js</a></li>
<li><a href="../327792/index.html">Development of a simulator of the evolution of single-celled organisms "The strongest survives"</a></li>
<li><a href="../327796/index.html">Conference Outsource-People 2017, Krakow (day one)</a></li>
<li><a href="../327806/index.html">Repair without screwdrivers, COM ports and 7 years of spare parts availability: how iron reliability is provided for stores</a></li>
<li><a href="../327810/index.html">No ruby ‚Äã‚Äãone</a></li>
<li><a href="../327820/index.html">The past and the future of cellular communication in areas where there is no GSM</a></li>
<li><a href="../327822/index.html">4 cool game development stuff</a></li>
<li><a href="../327824/index.html">Linux / Shishiga malware uses Lua scripts</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>