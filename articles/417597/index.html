<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Secure storage with DRBD9 and Proxmox (Part 2: iSCSI + LVM)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In a previous article, I looked at the possibility of creating a fault-tolerant NFS server using DRBD and Proxmox. It turned out pretty well, but we w...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Secure storage with DRBD9 and Proxmox (Part 2: iSCSI + LVM)</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/getpro/habr/post_images/101/70c/524/10170c52443d67bd757a09ef22ba39e2.jpg" alt="image"></p><br><p>  In a <a href="https://habr.com/post/417473/">previous article,</a> I looked at the possibility of creating a fault-tolerant NFS server using DRBD and Proxmox.  It turned out pretty well, but we will not stop there and now we will try to "squeeze all the juice" out of our store. </p><br><p>  In this article I will tell you how to create a fault-tolerant iSCSI target in a similar way, which, with the help of LVM, we will cut into small pieces and use for virtual machines. </p><br><p>  This approach will reduce the load and increase the speed of access to data several times, it is especially beneficial when competitive access to data is not required, for example in the case when you need to organize storage for virtual machines. </p><a name="habracut"></a><br><h2 id="para-slov-o-drbd">  A few words about DRBD </h2><br><p>  DRBD is a fairly simple and mature solution, the code of the eighth version is adopted in the Linux kernel.  In essence, the network mirror is RAID1.  In the ninth version, there is support for quorum and replication with more than two nodes. </p><br><p>  In fact, it allows you to combine block devices on several physical nodes into one common network share. </p><br><p>  Using DRBD you can achieve very interesting configurations.  Today we will talk about iSCSI and LVM. </p><br><p>  You can learn more about it by reading my <a href="https://habr.com/post/417473/">previous article</a> , where I described this solution in detail. </p><br><h2 id="para-slov-ob-iscsi">  A couple of words about iSCSI </h2><br><p>  iSCSI is a protocol for delivering a block device over a network. </p><br><p>  Unlike the NBD, it supports authorization, works without problems with network failures and supports many other useful functions, and most importantly shows very good performance. </p><br><p>  There is a huge number of its implementations, some of them are also included in the kernel and do not require any special difficulties for its configuration and connection. </p><br><h2 id="para-slov-ob-lvm">  A couple of words about LVM </h2><br><p>  It is worth mentioning that LINBIT has its own solution for Proxmox, it should work out of the box and allow you to achieve a similar result, but in this article I would not like to focus only on Proxmox and describe some more universal solution that is suitable for both Proxmox and something else, in this example, proxmox is used only as a means of orchestrating containers, in fact, you can replace it with another solution, for example, launch containers with Target in Kubernetes. </p><br><p>  As for Proxmox specifically, it works fine with shared LUN and LVM, using only its own standard drivers. </p><br><p>  The advantages of LVM include the fact that its use is not something revolutionary new and insufficiently run-in, but, on the contrary, it shows dry stability, which is usually required from storage.  It is worth mentioning that LVM is quite actively used in other environments, for example, in OpenNebula or in Kubernetes and is supported quite well there. </p><br><p>  Thus, you will receive universal storage that can be used in different systems (not only in proxmox), using only ready-made drivers, without much need to modify it with a file. </p><br><p>  Unfortunately, when choosing a solution for storage, you always have to make some compromises.  So here, this solution will not give you the same flexibility as for example Ceph. <br>  The virtual disk size is limited by the size of the LVM group, and the area marked up for a specific virtual disk will necessarily be preallocated - this greatly improves the speed of access to data, but does not allow for Thin-Provisioning (when the virtual disk takes up less space than it actually is).  It is worth mentioning that the performance of LVM sags quite a lot when using snapshots, and therefore the possibility of their free use is often excluded. </p><br><p>  Yes, LVM supports Thin-Provision pools, which are deprived of this disadvantage, but unfortunately their use is possible only in the context of one node and there is no possibility to share one Thin-Provision pool for several nodes in a cluster. </p><br><p>  But despite these shortcomings, due to its simplicity, LVM still does not allow competitors to bypass it and completely oust it from the battlefield. </p><br><p>  With a fairly small overhead, LVM still represents a very fast, stable and reasonably flexible solution. </p><br><h1 id="obschaya-shema">  General scheme </h1><br><ul><li>  We have <strong>three nodes</strong> </li><li>  On each node distributed <strong>drbd device</strong> . </li><li>  On top of the drbd device, an <strong>LXC container</strong> with iSCSI target is running. </li><li>  Target is connected to all three nodes. </li><li>  A <strong>LVM-group is</strong> created on the connected target. </li><li>  If necessary, <strong>LXC container</strong> can move to another node, along with <strong>iSCSI target</strong> </li></ul><br><h1 id="nastroyka">  Customization </h1><br><p>  With the idea sorted out now move on to the implementation. </p><br><p>  By default, the <strong>module of the eighth version of drbd</strong> is supplied <strong>with the Linux kernel</strong> , unfortunately it does <strong>not suit</strong> us and we need to install the module of the ninth version. </p><br><p>  Connect the LINBIT repository and install everything you need: </p><br><pre><code class="bash hljs">wget -O- https://packages.linbit.com/package-signing-pubkey.asc | apt-key add - <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"deb http://packages.linbit.com/proxmox/ proxmox-5 drbd-9.0"</span></span> \ &gt; /etc/apt/sources.list.d/linbit.list apt-get update &amp;&amp; apt-get -y install pve-headers drbd-dkms drbd-utils drbdtop</code> </pre> <br><ul><li>  <code>pve-headers</code> - kernel headers necessary for building the module </li><li>  <code>drbd-dkms</code> - kernel module in DKMS format </li><li>  <code>drbd-utils</code> - basic utilities for managing DRBD </li><li>  <code>drbdtop</code> is an interactive tool like top for DRBD only </li></ul><br><p>  After installing the <strong>module,</strong> check if everything is fine with it: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># modprobe drbd # cat /proc/drbd version: 9.0.14-1 (api:2/proto:86-113)</span></span></code> </pre> <br><p>  If you see the <strong>eighth version</strong> in the output of the command, something went wrong and the <strong>in-tree</strong> kernel module is loaded.  Check the <code>dkms status</code> for the reason. </p><br><p>  Each node will have the same <strong>drbd device</strong> running on top of the usual partitions.  First we need to prepare this section for drbd on each node. </p><br><p>  As such a partition can be any <strong>block device</strong> , it can be lvm, zvol, a disk partition or the entire disk.  In this article I will use a separate nvme disk with a partition for drbd: <code>/dev/nvme1n1p1</code> </p><br><p>  It is worth noting that device names tend to change sometimes, so it‚Äôs better to take the habit of using a permanent symlink on a device right away. </p><br><p>  You can find such a symlink for <code>/dev/nvme1n1p1</code> like this: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># find /dev/disk/ -lname '*/nvme1n1p1' /dev/disk/by-partuuid/847b9713-8c00-48a1-8dff-f84c328b9da2 /dev/disk/by-path/pci-0000:0e:00.0-nvme-1-part1 /dev/disk/by-id/nvme-eui.0000000001000000e4d25c33da9f4d01-part1 /dev/disk/by-id/nvme-INTEL_SSDPEKKA010T7_BTPY703505FB1P0H-part1</span></span></code> </pre> <br><p>  We describe our resource on all three nodes: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat /etc/drbd.d/tgt1.res resource tgt1 { meta-disk internal; device /dev/drbd100; protocol C; net { after-sb-0pri discard-zero-changes; after-sb-1pri discard-secondary; after-sb-2pri disconnect; } on pve1 { address 192.168.2.11:7000; disk /dev/disk/by-partuuid/95e7eabb-436e-4585-94ea-961ceac936f7; node-id 0; } on pve2 { address 192.168.2.12:7000; disk /dev/disk/by-partuuid/aa7490c0-fe1a-4b1f-ba3f-0ddee07dfee3; node-id 1; } on pve3 { address 192.168.2.13:7000; disk /dev/disk/by-partuuid/847b9713-8c00-48a1-8dff-f84c328b9da2; node-id 2; } connection-mesh { hosts pve1 pve2 pve3; } }</span></span></code> </pre> <br><p>  It is advisable to synchronize drbd to use a <strong>separate network</strong> . </p><br><p>  Now create the metadata for drbd and launch it: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm create-md tgt1 initializing activity log initializing bitmap (320 KB) to all zero Writing meta data... New drbd meta data block successfully created. success # drbdadm up tgt1</span></span></code> </pre> <br><p>  Repeat these actions on all three nodes and check the status: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm status tgt1 role:Secondary disk:Inconsistent pve2 role:Secondary peer-disk:Inconsistent pve3 role:Secondary peer-disk:Inconsistent</span></span></code> </pre> <br><p>  Now our disk is <strong>Inconsistent</strong> on all three nodes, this is because drbd does not know which disk should be taken as the original.  We need to mark one of them as <strong>Primary</strong> , so that its state is synchronized to the other nodes: </p><br><pre> <code class="bash hljs">drbdadm primary --force tgt1 drbdadm secondary tgt1</code> </pre> <br><p>  Immediately after this, <strong>synchronization</strong> will start: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm status tgt1 role:Secondary disk:UpToDate pve2 role:Secondary replication:SyncSource peer-disk:Inconsistent done:26.66 pve3 role:Secondary replication:SyncSource peer-disk:Inconsistent done:14.20</span></span></code> </pre><br><p>  We don‚Äôt have to wait until it is finished and we can follow up the next steps in parallel.  They can be performed on <strong>any node</strong> , regardless of its current state of the local disk in DRBD.  All requests will be automatically redirected to the device with the <strong>UpToDate</strong> state. </p><br><p>  Don't forget to activate the <strong>autorun of the</strong> drbd service on the nodes: </p><br><pre> <code class="hljs pgsql">systemctl <span class="hljs-keyword"><span class="hljs-keyword">enable</span></span> drbd.service</code> </pre> <br><h2 id="nastroyka-lxc-konteynera">  Configure LXC Container </h2><br><p>  Omit part of the configuration of the <strong>Proxmox cluster</strong> of three nodes, this part is well described in the <a href="https://pve.proxmox.com/wiki/Cluster_Manager">official wiki</a> </p><br><p>  As I said before, our <strong>iSCSI target</strong> will work in an <strong>LXC container</strong> .  We will keep the container itself on the <code>/dev/drbd100</code> device we just created. </p><br><p>  First we need to create a <strong>file system</strong> on it: </p><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">mkfs</span></span> -t ext4 -O mmp -E mmp_update_interval=<span class="hljs-number"><span class="hljs-number">5</span></span> /dev/drbd100</code> </pre> <br><p>  <strong>Proxmox</strong> by default includes <strong>multimount protection</strong> at the file system level, in principle we can do without it, because  DRBD by default has its own protection, it will simply prohibit the second <strong>Primary</strong> for the device, but caution will not harm us. </p><br><p>  Now download the Ubuntu template: </p><br><pre> <code class="hljs pgsql"># wget http://download.proxmox.com/images/<span class="hljs-keyword"><span class="hljs-keyword">system</span></span>/ubuntu<span class="hljs-number"><span class="hljs-number">-16.04</span></span>-standard_16<span class="hljs-number"><span class="hljs-number">.04</span></span><span class="hljs-number"><span class="hljs-number">-1</span></span>_amd64.tar.gz -P /var/lib/vz/<span class="hljs-keyword"><span class="hljs-keyword">template</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">cache</span></span>/</code> </pre> <br><p>  And create from it our container: </p><br><pre> <code class="hljs pgsql">pct <span class="hljs-keyword"><span class="hljs-keyword">create</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span> <span class="hljs-keyword"><span class="hljs-keyword">local</span></span>:vztmpl/ubuntu<span class="hljs-number"><span class="hljs-number">-16.04</span></span>-standard_16<span class="hljs-number"><span class="hljs-number">.04</span></span><span class="hljs-number"><span class="hljs-number">-1</span></span>_amd64.tar.gz \ <span class="hljs-comment"><span class="hljs-comment">--hostname=tgt1 \ --net0=name=eth0,bridge=vmbr0,gw=192.168.1.1,ip=192.168.1.11/24 \ --rootfs=volume=/dev/drbd100,shared=1</span></span></code> </pre> <br><p>  In this command, we specify that the <strong>root system of</strong> our container will be located on the <code>/dev/drbd100</code> and add the parameter <code>shared=1</code> to allow the container to be migrated between nodes. </p><br><p>  If something went wrong, you can always fix it through the <strong>Proxmox</strong> interface or in the <code>/etc/pve/lxc/101.conf</code> container <code>/etc/pve/lxc/101.conf</code> </p><br><p>  Proxmox will unpack the template and prepare <strong>the</strong> container <strong>root system</strong> for us.  After that we can run our container: </p><br><pre> <code class="hljs pgsql">pct <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span></code> </pre> <br><h2 id="nastroyka-iscsi-targeta">  Setting up an iSCSI target. </h2><br><p>  Of the entire set of <strong>targets</strong> , I chose <strong>istgt</strong> , since it has the highest performance and works in user space. </p><br><p>  Now let's log in to our container: </p><br><pre> <code class="hljs perl">pct <span class="hljs-keyword"><span class="hljs-keyword">exec</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span> bash</code> </pre> <br><p>  Install the update and <strong>istgt</strong> : </p><br><pre> <code class="hljs sql">apt-get <span class="hljs-keyword"><span class="hljs-keyword">update</span></span> apt-<span class="hljs-keyword"><span class="hljs-keyword">get</span></span> -y <span class="hljs-keyword"><span class="hljs-keyword">upgrade</span></span> apt-<span class="hljs-keyword"><span class="hljs-keyword">get</span></span> -y <span class="hljs-keyword"><span class="hljs-keyword">install</span></span> istgt</code> </pre> <br><p>  Create a file that we will give over the network: </p><br><pre> <code class="hljs haskell"><span class="hljs-title"><span class="hljs-title">mkdir</span></span> -p /<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class"> fallocate -l 740G /</span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class">/target1.img</span></span></code> </pre> <br><p>  Now we need to write the config for <strong>istgt</strong> <code>/etc/istgt/istgt.conf</code> : </p><br><pre> <code class="hljs sql">[Global] <span class="hljs-keyword"><span class="hljs-keyword">Comment</span></span> <span class="hljs-string"><span class="hljs-string">"Global section"</span></span> NodeBase <span class="hljs-string"><span class="hljs-string">"iqn.2018-07.org.example.tgt1"</span></span> PidFile /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/run/istgt.pid AuthFile /etc/istgt/auth.conf MediaDirectory /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/istgt LogFacility <span class="hljs-string"><span class="hljs-string">"local7"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">Timeout</span></span> <span class="hljs-number"><span class="hljs-number">30</span></span> NopInInterval <span class="hljs-number"><span class="hljs-number">20</span></span> DiscoveryAuthMethod <span class="hljs-keyword"><span class="hljs-keyword">Auto</span></span> MaxSessions <span class="hljs-number"><span class="hljs-number">16</span></span> MaxConnections <span class="hljs-number"><span class="hljs-number">4</span></span> MaxR2T <span class="hljs-number"><span class="hljs-number">32</span></span> MaxOutstandingR2T <span class="hljs-number"><span class="hljs-number">16</span></span> DefaultTime2Wait <span class="hljs-number"><span class="hljs-number">2</span></span> DefaultTime2Retain <span class="hljs-number"><span class="hljs-number">60</span></span> FirstBurstLength <span class="hljs-number"><span class="hljs-number">262144</span></span> MaxBurstLength <span class="hljs-number"><span class="hljs-number">1048576</span></span> MaxRecvDataSegmentLength <span class="hljs-number"><span class="hljs-number">262144</span></span> InitialR2T Yes ImmediateData Yes DataPDUInOrder Yes DataSequenceInOrder Yes ErrorRecoveryLevel <span class="hljs-number"><span class="hljs-number">0</span></span> [UnitControl] <span class="hljs-keyword"><span class="hljs-keyword">Comment</span></span> <span class="hljs-string"><span class="hljs-string">"Internal Logical Unit Controller"</span></span> AuthMethod CHAP Mutual AuthGroup AuthGroup10000 Portal UC1 <span class="hljs-number"><span class="hljs-number">127.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span>:<span class="hljs-number"><span class="hljs-number">3261</span></span> Netmask <span class="hljs-number"><span class="hljs-number">127.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span> [PortalGroup1] <span class="hljs-keyword"><span class="hljs-keyword">Comment</span></span> <span class="hljs-string"><span class="hljs-string">"SINGLE PORT TEST"</span></span> Portal DA1 <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span><span class="hljs-number"><span class="hljs-number">.11</span></span>:<span class="hljs-number"><span class="hljs-number">3260</span></span> [InitiatorGroup1] <span class="hljs-keyword"><span class="hljs-keyword">Comment</span></span> <span class="hljs-string"><span class="hljs-string">"Initiator Group1"</span></span> InitiatorName <span class="hljs-string"><span class="hljs-string">"ALL"</span></span> Netmask <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span>/<span class="hljs-number"><span class="hljs-number">24</span></span> [LogicalUnit1] <span class="hljs-keyword"><span class="hljs-keyword">Comment</span></span> <span class="hljs-string"><span class="hljs-string">"Hard Disk Sample"</span></span> TargetName disk1 TargetAlias <span class="hljs-string"><span class="hljs-string">"Data Disk1"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">Mapping</span></span> PortalGroup1 InitiatorGroup1 AuthMethod <span class="hljs-keyword"><span class="hljs-keyword">Auto</span></span> AuthGroup AuthGroup1 UseDigest <span class="hljs-keyword"><span class="hljs-keyword">Auto</span></span> UnitType Disk LUN0 <span class="hljs-keyword"><span class="hljs-keyword">Storage</span></span> /<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>/target1.img <span class="hljs-keyword"><span class="hljs-keyword">Auto</span></span></code> </pre> <br><p>  Restart istgt: </p><br><pre> <code class="hljs pgsql">systemctl <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span> istgt</code> </pre> <br><p>  At this point, the target setting is completed. </p><br><h2 id="nastroyka-ha">  HA Setup </h2><br><p>  Now we can go to the <strong>HA-manager</strong> configuration.  Create a separate HA group for our device: </p><br><pre> <code class="hljs pgsql">ha-manager groupadd tgt1 <span class="hljs-comment"><span class="hljs-comment">--nodes pve1,pve2,pve3 --nofailback=1 --restricted=1</span></span></code> </pre> <br><p>  Our <strong>resource</strong> will work only on the nodes specified for this group.  Add our container to this group: </p><br><pre> <code class="hljs cs">ha-manager <span class="hljs-keyword"><span class="hljs-keyword">add</span></span> ct:<span class="hljs-number"><span class="hljs-number">101</span></span> --<span class="hljs-keyword"><span class="hljs-keyword">group</span></span>=tgt1 --max_relocate=<span class="hljs-number"><span class="hljs-number">3</span></span> --max_restart=<span class="hljs-number"><span class="hljs-number">3</span></span></code> </pre> <br><h2 id="rekomendacii-i-tyuning">  Recommendations and tuning </h2><br><h5 id="drbd">  DRBD </h5><br><p>  As I noted above, it is always advisable to use a separate network for replication.  It is highly desirable to use <strong>10-gigabit network adapters</strong> , otherwise you all will rest on the speed of the ports. <br>  If replication seems slow enough, try tuning in some parameters for <strong>DRBD</strong> .  Here is the config that, in my opinion, is optimal for my <strong>10G network</strong> : </p><br><pre> <code class="hljs swift"># cat /etc/drbd.d/global_common.conf global { usage-<span class="hljs-built_in"><span class="hljs-built_in">count</span></span> yes; udev-always-use-vnr; } common { handlers { } startup { } options { } disk { <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-fill-target 10M; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-<span class="hljs-built_in"><span class="hljs-built_in">max</span></span>-rate 720M; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-plan-ahead <span class="hljs-number"><span class="hljs-number">10</span></span>; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-<span class="hljs-built_in"><span class="hljs-built_in">min</span></span>-rate 20M; } net { <span class="hljs-built_in"><span class="hljs-built_in">max</span></span>-buffers 36k; sndbuf-size 1024k; rcvbuf-size 2048k; } }</code> </pre> <br><p>  More information about each parameter you can get information from the <a href="https://docs.linbit.com/man/v9/drbd-conf-5/">official documentation DRBD</a> </p><br><h5 id="open-iscsi">  Open iSCSI </h5><br><p>  Since we do not use multipathing, in our case <a href="https://unix.stackexchange.com/a/198101/175694">it is recommended to</a> disable periodic connection checks on clients, as well as to increase waiting timeouts for session recovery in <code>/etc/iscsi/iscsid.conf</code> . </p><br><pre> <code class="hljs pgsql">node.conn[<span class="hljs-number"><span class="hljs-number">0</span></span>].timeo.noop_out_interval = <span class="hljs-number"><span class="hljs-number">0</span></span> node.conn[<span class="hljs-number"><span class="hljs-number">0</span></span>].timeo.noop_out_timeout = <span class="hljs-number"><span class="hljs-number">0</span></span> node.<span class="hljs-keyword"><span class="hljs-keyword">session</span></span>.timeo.replacement_timeout = <span class="hljs-number"><span class="hljs-number">86400</span></span></code> </pre> <br><h2 id="ispolzovanie">  Using </h2><br><h4 id="proxmox">  Proxmox </h4><br><p>  The resulting <strong>iSCSI target</strong> can be immediately connected to Proxmox, without having forgotten to uncheck <strong>Use LUN Directly</strong> . </p><br><p><img src="https://habrastorage.org/webt/uw/j3/pu/uwj3pusr-nf9bc7neisd5x-fcsg.png"></p><br><p>  Immediately after this, it will be possible to create LVM on top of it, do not forget to tick the <strong>shared one</strong> : </p><br><p><img src="https://habrastorage.org/webt/j1/ob/mw/j1obmwcwhz-e6krjix72pmiz118.png"></p><br><h4 id="drugie-sredy">  Other environments </h4><br><p>  If you plan to use this solution in a different environment, you may need to install a cluster extension for LVM at the moment from two implementations.  <strong>CLVM</strong> and <strong>lvmlockd</strong> . </p><br><p>  Setting up a <strong>CLVM is</strong> not so trivial and requires a running cluster manager. <br>  Where as the second method <strong>lvmlockd</strong> - not yet fully tested and is just starting to appear in stable repositories. </p><br><p>  I recommend reading a great <a href="https://ivirt-it.ru/clvm-lvmlockd/"><strong>article on blocking in LVM</strong></a> </p><br><p>  When using <strong>LVM</strong> with <strong>Proxmox,</strong> cluster addition is <a href="https://forum.proxmox.com/threads/when-is-clvm-needed.14703/">not required</a> , since volume management is provided by proxmox itself, which updates and monitors LVM metadata on its own.  The same goes for <strong>OpenNebula</strong> , which is clearly indicated by the <a href="https://docs.opennebula.org/stable/deployment/open_cloud_storage_setup/lvm_drivers.html">official documentation</a> . </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/417597/">https://habr.com/ru/post/417597/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../417587/index.html">Mass media: large-scale cyber attacks accelerated the growth of capitalization of companies from the information security industry</a></li>
<li><a href="../417589/index.html">Seven simple rules to make the Internet accessible to all.</a></li>
<li><a href="../417591/index.html">How to "learn" English in one year alone or an article for those who did not work out with English</a></li>
<li><a href="../417593/index.html">NewSQL = NoSQL + ACID</a></li>
<li><a href="../417595/index.html">Antiquities: Palm OS, efficient code and ugly photos</a></li>
<li><a href="../417599/index.html">Fintech Digest: financial regulators need AI in order to work in modern conditions</a></li>
<li><a href="../417601/index.html">Choose a server. What to look for? Check list</a></li>
<li><a href="../417603/index.html">Announcement of mobile mitap: What to do when the application has become large?</a></li>
<li><a href="../417605/index.html">Basics of 3D modeling for 3D printing</a></li>
<li><a href="../417607/index.html">A / V tests do not work. Check what you are doing wrong</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>