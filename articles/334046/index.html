<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to teach your neural network to generate poems</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I beg you to stop dreaming me 
 I love you my bride 
 White frost on your eyelashes 
 Kiss on the body wordless 

 Once at school, it seemed to me tha...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to teach your neural network to generate poems</h1><div class="post__text post__text-html js-mediator-article">  <em>I beg you to stop dreaming me</em> <br>  <em>I love you my bride</em> <br>  <em>White frost on your eyelashes</em> <br>  <em>Kiss on the body wordless</em> <br><br>  Once at school, it seemed to me that writing poetry was simple: you just need to arrange the words in the right order and select the right rhyme.  Traces of these hallucinations (or illusions, I do not distinguish them) met you in the epigraph.  Only this poem is, of course, not the result of my then creative work, but a product of a neural network trained on the same principle. <br><br>  Rather, the neural network is needed only for the first stage - the arrangement of words in the correct order.  The rules applied on top of the neural network predictions cope with rhyming.  Want to know more about how we implemented it?  Then welcome under cat. <br><a name="habracut"></a><br><h2>  Language models </h2><br><h4>  Definition </h4><br>  Let's start with the language model.  On Habr√©, I have met not too many articles about them - it would not be superfluous to recall what kind of beast it is. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Language models determine the likelihood of a sequence of words <img src="https://habrastorage.org/getpro/habr/post_images/627/868/a80/627868a80851ba6bc7cee1c63eafcebf.svg" alt="w_1, \ ldots, w_n">  in this language: <img src="https://habrastorage.org/getpro/habr/post_images/512/28f/d61/51228fd61c105f525176120804fd66f7.svg" alt="\ mathbf {P} (w_1, \ ldots, w_n)">  .  Let us move from this terrible probability to the product of conditional probabilities of a word from an already read context: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8ae/d34/808/8aed34808b2bd5cfde5eb345cdcc248e.svg" alt="\ mathbf {P} (w_1, \ ldots, w_n) = \ prod_ {i = 1} ^ n \ mathbf {P} (w_i | w_1, \ ldots, w_ {i-1})">  . <br><br>  In life, these conditional probabilities show what word we expect to see next.  Let us see, for example, the well-known words from Pushkin: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/956/239/601/95623960157b4e15a1b3f599aed62ed2.png" height="130"></div><br><br>  The language model that sits with us (in any case, with me) in my head suggests: after <em>honest ones</em> <em>mine is</em> unlikely to go again.  And here, and, of course, the <em>rules</em> - very much so. <br><br><h4>  N-gram language models </h4><br>  It seems the easiest way to build such a model is to use N-gram statistics.  In this case, we approximate the probability <img src="https://habrastorage.org/getpro/habr/post_images/55c/e90/4c3/55ce904c3b20dc5da7fe1868911bf90b.svg" alt="\ mathbf {P} (w_i | w_1, \ ldots, w_ {i-1}) \ approx \ mathbf {P} (w_i | w_ {i - N}, \ ldots, w_ {i-1})">  - rejecting words that are too far away as they do not affect the probability of a given occurrence. <br><br>  Such a model is easily implemented with the help of Counter on Python - and it turns out to be very heavy and not too variable.  One of its most noticeable problems is the lack of statistics: most of the 5-gram words, including those permissible by the language, simply cannot be found in any large body. <br><br>  To solve such a problem, Kneser ‚Äì Ney or Katz's backing-off is usually used.  For more information on N-gram smoothing methods, refer to Christopher Manning's famous book ‚ÄúFoundations of Statistical Natural Language Processing‚Äù. <br><br>  I want to note that I called the 5-grams of words for a reason: it is them (with anti-aliasing, of course) that Google demonstrates in the article ‚ÄúOne Billion Word Benchmark for Measuring Progress in Statistical Language Modeling‚Äù - and it shows results that are quite comparable to those of recurrent neural networks - which, in fact, will be discussed further. <br>  Neural Network Language Models <br><br>  The advantage of recurrent neural networks is the ability to use an infinitely long context.  Together with each word arriving at the input of a recurrent cell, a vector appears in it representing the entire previous history ‚Äî all the words processed to a given moment (the red arrow in the picture). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/dc1/7c2/c4e/dc17c2c4e9ac434eb5346ada2c412c9a.png"></div><br><br>  The possibility of using the context of unlimited length is, of course, only conditional.  In practice, classic RNNs suffer from gradient damping - in fact, the inability to remember the context further than a few words.  To combat this, special memory cells were invented.  The most popular are LSTM and GRU.  In the following, speaking of the recurrent layer, I will always mean LSTM. <br><br>  The red arrow in the picture shows the display of the word in its embedding.  The output layer (in the simplest case) is a fully connected layer with a size corresponding to the size of the dictionary, having softmax activation ‚Äî to obtain a probability distribution for the vocabulary words.  From this distribution, you can sample the next word (or just take the maximum possible). <br><br>  Already in the picture you can see a minus of such a layer: its size.  With a dictionary of several hundred thousand words, he can easily stop getting on a video card, and his body requires huge corpus of texts.  This is very clearly demonstrates the picture from the <a href="http://torch.ch/blog/2016/07/25/nce.html">blog torch</a> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/f53/813/d23/f53813d23d25464f99aded4842690fa7.png" height="300"></div><br><br>  To combat this, a very large number of different techniques were invented.  The most popular are hierarchical softmax and noise contrastive estimation.  Details about these and other methods should be read in the <a href="http://sebastianruder.com/word-embeddings-softmax/index.html">excellent article Sebastian Ruder</a> . <br><br><h4>  Language Model Assessment </h4><br>  A more or less standard loss function, optimized for multi-class classification, is the cross-entropy loss function.  In general, cross-entropy between vectors <img src="https://habrastorage.org/getpro/habr/post_images/97f/a10/8ab/97fa108abfcd32434b0b3973aa8c30a4.svg" alt="y">  and the predicted vector <img src="https://habrastorage.org/getpro/habr/post_images/74d/f78/194/74df781945a071be675429b30c4f9487.svg" alt="\ hat y">  recorded as <img src="https://habrastorage.org/getpro/habr/post_images/ec6/4bf/dc7/ec64bfdc750bad60e72be1844da787d9.svg" alt="H (y, \ hat {y}) = - \ sum_i y_i \ log_2 \ hat y_i">  .  It shows the proximity of the distributions given by <img src="https://habrastorage.org/getpro/habr/post_images/97f/a10/8ab/97fa108abfcd32434b0b3973aa8c30a4.svg" alt="y">  and <img src="https://habrastorage.org/getpro/habr/post_images/74d/f78/194/74df781945a071be675429b30c4f9487.svg" alt="\ hat y">  . <br>  When calculating cross-entropy for multi-class classification <img src="https://habrastorage.org/getpro/habr/post_images/cf0/137/ae6/cf0137ae69bff2b78b5ef43dd2d8fcba.svg" alt="\ hat y_i">  - this is the probability <img src="https://habrastorage.org/getpro/habr/post_images/eb4/e6c/9d8/eb4e6c9d80848742431282fecce98e45.svg" alt="i">  first class, and <img src="https://habrastorage.org/getpro/habr/post_images/97f/a10/8ab/97fa108abfcd32434b0b3973aa8c30a4.svg" alt="y">  - a vector obtained with one-hot-encoding (i.e., a bit vector in which a single unit is at the position corresponding to the class number).  Then <img src="https://habrastorage.org/getpro/habr/post_images/d6e/492/01a/d6e49201a7f5ac1f8a91c819a75305bc.svg" alt="H (y, \ hat y) = - \ log_2 \ hat y_k">  with some <img src="https://habrastorage.org/getpro/habr/post_images/f53/9e6/f19/f539e6f19b2524a3987b0a9b0369bf01.svg" alt="y_k = 1">  . <br><br>  Cross-entropy loss of the whole sentence <img src="https://habrastorage.org/getpro/habr/post_images/f99/879/16e/f9987916e1733935ca2468e6f5bdf8a5.svg" alt="w_1, \ ldot, w_n">  are obtained by averaging the values ‚Äã‚Äãover all words.  They can be written as: <img src="https://habrastorage.org/getpro/habr/post_images/30b/50b/605/30b50b605f6629ce4d5aaae17432837f.svg" alt="H (w_1, \ ldot, w_n) = - \ frac 1 n \ sum_k \ log_2 \ mathbf {P} (w_k | w_1, \ ldots, w_ {k-1})">  .  It can be seen that this expression corresponds to what we want to achieve: the probability of a real sentence from the language should be as high as possible. <br><br>  In addition, perplexity is a metric specific to language modeling: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/513/ebc/293/513ebc29301e1dfb9bd95d779f4070a3.svg" alt="PP (w_1, \ ldot, w_n) = 2 ^ {H (w_1, \ ldot, w_n)} = 2 ^ {- \ frac 1 n \ sum_k \ log_2 \ mathbf {P} (w_k | w_1, \ ldots, w_ {k-1})}">  . <br><br>  To understand its meaning, let's look at a model that predicts words from the dictionary equally likely, regardless of context.  For her <img src="https://habrastorage.org/getpro/habr/post_images/7cb/3bf/596/7cb3bf596e52d66a111d55f267224969.svg" alt="\ mathbf {P} (w) = \ frac 1 N">  where N is the size of the dictionary, and the perplexion will be equal to the size of the dictionary - N. Of course, this is a completely stupid model, but looking back at it, you can interpret the perplexion of real models as the level of ambiguity of word generation. <br><br>  For example, in the model with perplexia 100, the choice of the next word is also ambiguous, as the choice of a uniform distribution among 100 words.  And if such perplexion was achieved on a dictionary of 100,000, it turns out that we managed to reduce this ambiguity by three orders of magnitude compared to the ‚Äústupid‚Äù model. <br><br><h2>  The implementation of the language model for the generation of poems </h2><br><h4>  Building a network architecture </h4><br>  Recall now that for our task a language model is needed to select the most appropriate next word by the already generated sequence.  And from this it follows that in predicting you will never meet unfamiliar words (well, where will they come from).  Therefore, the number of words in the dictionary remains entirely in our power, which allows you to adjust the size of the resulting model.  Thus, we had to forget about such human achievements as symbolic embeddings for representing words (you can read about them, for example, <a href="https://arxiv.org/abs/1508.06615">here</a> ). <br><br>  Based on these prerequisites, we started with a relatively simple model, generally repeating the one shown in the picture above.  Two yellow LSTM layers followed by a fully connected layer acted as a yellow rectangle. <br><br>  The decision to limit the size of the output layer seems quite working.  Naturally, the dictionary should be limited in frequency - say, taking fifty thousand of the most frequent words.  But here another question arises: which architecture of the recurrent network is better to choose. <br><br>  There are two obvious options: use the many-to-many option (for each word, try to predict the following) or many-to-one (predict the word by the sequence of the preceding words). <br><br>  To better understand the essence of the problem, look at the picture: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/31e/0c6/aad/31e0c6aad6264d76b74ba1366f16af51.png" height="140"></div><br><br>  Here is a many-to-many version with a dictionary in which there is no place for the word ‚Äúink‚Äù.  A logical step is the substitution for it of a special token &lt;unk&gt; - an unfamiliar word.  The problem is that the model happily learns that after any word there may be an unfamiliar word.  As a result, the distribution produced by it turns out to be shifted in the direction of this particular unfamiliar word.  Of course, this is easily solved: you just need to sample from the distribution without this token, but still there is a feeling that the resulting model is somewhat crooked. <br><br>  An alternative option is to use many-to-one architecture: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/778/008/f2f/778008f2f07548eca98c18083e01681d.png" height="140"></div><br><br>  At the same time, it is necessary to cut all sorts of chains of words from the training set - which will lead to its noticeable swelling.  But all the chains for which the following words - is unknown, we can simply pass completely solving the problem with a frequent prediction &lt;unk&gt; token. <br><br>  This model had the following parameters (in terms of the <a href="http://keras.io/">keras</a> library): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/7fa/f31/ea1/7faf31ea127442a38118340ffdf6453a.png" height="300"></div><br><br>  As you can see, it is included in the 60,000 + 1 word plus the first token is the same &lt;unk&gt;. <br><br>  The easiest way is to repeat it with a small modification of the <a href="https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py">example</a> .  Its main difference is that the example demonstrates character-by-character text generation, while the above-described variant is based on word-by-word generation. <br><br>  The resulting model really generates something, but even the grammatical consistency of the resulting sentences is often not impressive (about the semantic load and say nothing).  The logical next step is to use pre-trained embeddingings for words.  Their addition simplifies the learning of the model, and the connections between words learned on a large body can give meaning to the generated text. <br><br>  The main problem: for Russian (as opposed to, for example, English), it is difficult to find good word form words.  With the available result, it became even worse. <br><br>  Let's try pochamanit a little with the model.  The lack of a network, apparently - in too many parameters.  The network simply does not learn.  To fix this, you should work with the input and output layers - the heaviest elements of the model. <br><br><h4>  Completion of the input layer </h4><br>  Obviously, it makes sense to reduce the dimension of the input layer.  This can be achieved simply by reducing the dimension of word-form emebingov - but it is more interesting to go the other way. <br><br>  Instead of representing the word in one index in a high-dimensional space, add morphological markup: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/e97/8a8/6e8/e978a86e8a874d8d946bb15e6a49a713.png"></div><br><br>  Each word will be described by a pair: its lemma and grammatical meaning.  Using lemmas instead of word forms allows you to greatly reduce the size of embeddings: the first thirty thousand lemmas correspond to several hundred thousand different words.  Thus, seriously reducing the input layer, we also increased the vocabulary of our model. <br><br>  As can be seen from the figure, the lemma has a part of speech assigned to it.  This is done in order to be able to use already pre-trained embeddings for lemmas (for example, from <a href="http://rusvectores.org/ru/">RusVectores</a> ).  On the other hand, embeddings for thirty thousand lemmas can be fully trained from scratch, initializing them by chance. <br><br>  We presented the grammatical meaning in the format of <a href="http://universaldependencies.org/ru/feat/all.html">Universal Dependencies</a> , since I had a <a href="http://www.dialog-21.ru/media/3895/anastasyevdgetal.pdf">model</a> at hand <a href="http://www.dialog-21.ru/media/3895/anastasyevdgetal.pdf">that was trained for Dialogue 2017</a> . <br><br>  When applying a grammatical meaning to the input of a model, it is translated into a bit mask: for each grammatical category, positions are allocated by the number of grammes in this category - plus one position for the absence of this category in the grammatical meaning (Undefined).  Bit vectors for all categories are glued together into one big vector. <br><br>  Generally speaking, the grammatical meaning could, like the lemma, be represented by an index and teach embedding for it.  But the bitmask network showed higher quality. <br><br>  This bit vector can, of course, be submitted directly to LSTM, but it is better to pass it through one or two fully connected layers to reduce the dimension and, at the same time, to detect connections between gram patterns. <br><br><h4>  Finalization of the output layer </h4><br>  Instead of the word index, one can predict the same lemma and grammatical meaning separately.  After that, you can sample the lemma from the resulting distribution and put it in a form with the most likely grammatical meaning.  A small minus of such an approach is that it is impossible to guarantee the presence of such a grammatical meaning in this lemma. <br><br>  This problem is easily fixed in two ways.  The honest way is to sample exactly the word from the pairs of realizable lemma + grammatical meaning (the probability of this word, of course, is the product of the probabilities of the lemma and the grammatical meaning).  A faster alternative is to choose the most likely grammatical meaning among the possible ones for the sampled lemma. <br><br>  In addition, the softmax-layer could be replaced with a hierarchical softmax or, in general, dragged down the implementation of <a href="https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss">noise contrastive estimation from tensorflow</a> .  But we, with our dictionary size, turned out to be enough and ordinary softmax.  At least, the above-mentioned tricks did not bring a significant increase in the quality of the model. <br><br><h4>  Final model </h4><br>  As a result, we have the following model: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/138/a36/0cd/138a360cd7ee4d71969d055fe5aa6850.png" height="320"></div><br><br><h4>  Training data </h4><br>  So far, we have not discussed in any way the important question of what we are learning.  For training, we took a large piece of <a href="http://stihi.ru/">stihi.ru</a> and added morphological markup to it.  After that, long lines were selected (at least five words) and trained on them. <br><br>  Each line was considered as independent - thus we struggled with the fact that neighboring lines are often weakly connected in meaning (especially on <a href="http://stihi.ru/">stihi.ru</a> ).  Of course, you can learn at once on a complete poem, and this could give an improvement in the quality of the model.  But we decided that we are faced with the task of building a network that is able to write grammatically connected text, and for such a purpose it is enough to learn only on strings. <br><br>  When learning, the terminating character was added to all lines, and the order of words in the lines was inverted.  Expand sentences needed to simplify the rhyming of words when generating.  The final character is needed in order to start generating the sentence from it. <br><br>  Among other things, for simplicity, we threw all the punctuation marks from the texts.  This was done because the network noticeably retrained under commas and other ellipsis: in the sample they were put literally after each word.  Of course, this is a strong omission of our model and there is hope to fix it in the next version. <br><br>  Schematically, the pre-processing of texts can be depicted as follows: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/cea/7b9/f75/cea7b9f757cd41be9c4b3ee93e448f9e.png" height="120"></div><br><br>  The arrows indicate the direction in which the model reads the sentence. <br><br><h2>  Generator implementation </h2><br><h4>  Filter Rules </h4><br>  Let us turn, finally, to the poetry generator.  We began with the fact that the language model is needed only to build hypotheses about the next word.  For the generator, rules are needed according to which poems will be built from a sequence of words.  Such rules work as language model filters: of all the possible variants of the next word, only those that are suitable remain - in our case, by meter and rhyme. <br><br>  Metric rules determine the sequence of stressed and unstressed syllables in a string.  They are usually recorded in the form of a template of pros and cons: plus means a stressed syllable, and a minus corresponds to unstressed.  For example, consider the metric pattern + - + - + - + - (in which one can suspect a four-stop trochee): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/59a/b39/bd0/59ab39bd020c49a78a12cbab62c80181.png" height="160"></div><br>  Generation, as already mentioned, goes from right to left - in the direction of the arrows in the picture.  Thus, after <em>mist,</em> filters will ban the generation of such words as a <em>blizzard</em> (stress is not there) or <em>bad weather</em> (an extra syllable).  If the word has more than 2 syllables, it passes the filter only when the stressed syllable does not fall on the ‚Äúminus‚Äù in the metric pattern. <br><br>  The second type of rules - restrictions on rhyme.  It is for them that we generate poems backwards.  The filter is used when generating the very first word in the line (which will be the last after the reversal).  If a string has already been generated, with which the given one should rhyme, this filter will immediately cut off all non-rhymed words. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/b37/7c1/cd2/b377c1cd26304a18b3b4602cdebf4bcc.png" height="95"></div><br><br>  Also, an additional rule was applied prohibiting to consider rhymes of a word form with the same lemma. <br><br>  You might have a question: where did we get the stress from the words, and how did we determine which words rhyme with which ones?  To work with stresses, we took a <a href="">large dictionary</a> and trained a classifier on this dictionary to predict the stresses of unfamiliar words (a story that deserves a separate article).  Rhyming is determined by simple heuristics based on the location of the stressed syllable and its letter composition. <br><br><h4>  Beam search </h4><br>  As a result of the operation of the filters, there could be no words left.  To solve this problem, we do a beam search (beam search), choosing at each step instead of one at once N paths with the highest probabilities. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/905/3fe/090/9053fe090ec14d36b60ef91cfa58cdbd.png" height="290"></div><br><br>  Total, generator input parameters - language model, metric pattern, rhyme pattern, N in radial search, rhyming heuristics parameters.  At the exit we have a ready poem.  As a language model in the same generator, you can use the N-gram model.  The filter system is easily customized and complemented. <br><br><h2>  Examples of poems </h2><br>  <em>So confusing to me now sad</em> <br>  <em>What will it live</em> <br>  <em>Not destined to circle on the way</em> <br>  <em>Feeling the bomzhik pain</em> <br><br>  <em>Lost somewhere in the alley</em> <br>  <em>Where are you my recollections</em> <br>  <em>I love you my relatives</em> <br>  <em>How many lies of betrayal and flattery</em> <br>  <em>Nothing else is needed</em> <br>  <em>For the sins of your voice</em> <br><br>  <em>Miss your window</em> <br>  <em>And gentle esters</em> <br>  <em>Love you with my warmth</em> <br>  <em>You shorthand</em> <br><br><h2>  Links </h2><br><ul><li>  <a href="https://github.com/IlyaGusev/rupo">Repository</a> <br></li><li>  <a href="https://pypi.python.org/pypi/rupo">Package in pypi</a> <br></li><li>  <a href="https://vk.com/autopoems">Public in VK with verses</a> <br></li><li>  <a href="">Model and dictionaries necessary for her work</a> <br></li><li>  <a href="https://github.com/oxford-cs-deepnlp-2017/lectures">Oxford NLP course prepared with the participation of DeepMind</a> <br></li><li>  <a href="">One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling</a> </li><li>  <a href="https://arxiv.org/abs/1602.02410">Exploring the Limits of Language Modeling</a> <br></li><li>  <a href="https://arxiv.org/abs/1508.06615">Character-Aware Neural Language Models</a> </li></ul><br>  The post was written jointly with Ilya Gusev.  Elena Ivashkovskaya, Nadezhda Karatsapova and Polina Matavina also took part in the project. <br><br>  The work on the generator was carried out within the framework of the course ‚ÄúIntellectual Systems‚Äù of the Computational Linguistics Department of the FIVT MIPT.  I would like to thank the author of the course, Konstantin Anisimovich, for the advice he gave in the process. <br><br>  Many thanks <a href="https://habr.com/users/atwice/" class="user_link">atwice</a> for help in reading the article. </div><p>Source: <a href="https://habr.com/ru/post/334046/">https://habr.com/ru/post/334046/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../334032/index.html">Experience of using business notebook FUJITSU LIFEBOOK E746: home instead of office, Linux instead of Windows</a></li>
<li><a href="../334034/index.html">Testing in Badoo ‚Äúfrom the height of bird flight‚Äù</a></li>
<li><a href="../334038/index.html">Configure push notifications for your service</a></li>
<li><a href="../334042/index.html">Gamecube - file system device</a></li>
<li><a href="../334044/index.html">Comparison of Russian cloud service operators</a></li>
<li><a href="../334048/index.html">Selenium: for apple lovers</a></li>
<li><a href="../334050/index.html">How do we in 1C: Enterprise work with data models (or ‚ÄúWhy do we not work with tables?‚Äù)</a></li>
<li><a href="../334052/index.html">1.Check Point to the maximum. The human factor in information security</a></li>
<li><a href="../334054/index.html">Evolution of web application attacks</a></li>
<li><a href="../334056/index.html">Dismissal is a little death. How to save a valuable specialist who decided to quit?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>