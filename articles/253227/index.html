<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Deep Learning, NLP, and Representations</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I offer the readers of ‚ÄúHabrakhabr‚Äù the translation of the ‚ÄúDeep Learning, NLP, and Representations‚Äù post by steep Christopher Olach. Illustrations fr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Deep Learning, NLP, and Representations</h1><div class="post__text post__text-html js-mediator-article">  <i>I offer the readers of ‚ÄúHabrakhabr‚Äù the translation of the <a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">‚ÄúDeep Learning, NLP, and Representations‚Äù</a> post by steep Christopher Olach.</i>  <i>Illustrations from the same place.</i> <br><br>  In recent years, methods using deep neural network learning have taken a leading position in pattern recognition.  Thanks to them, the bar for the quality of computer vision techniques has risen significantly.  Speech recognition is moving in the same direction. <br><br>  Results are results, but <b>why do they solve problems so cool?</b> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/files/bd8/51f/afe/bd851fafef0d47ada2e6bcefd668513c.png"><br><br>  The post highlights several impressive results of using deep neural networks in natural language processing (Natural Language Processing; NLP).  Thus, I hope to lucidly state one of the answers to the question of why deep neural networks <i>work</i> . <br><a name="habracut"></a><br><h2>  Neural networks with one hidden layer </h2><br>  A neural network with a hidden layer is universal: with a sufficiently large number of hidden nodes, it can construct an approximation of any function.  There is a frequently quoted (and more often incorrectly understood and applied) theorem. <br><br>  This is true because the hidden layer can simply be used as a ‚Äúlookup table‚Äù. <br><br>  For simplicity, consider the perceptron.  This is a very simple neuron that works if its value exceeds a threshold value, and does not work if it does not.  The perceptron has binary inputs and a binary output (i.e. 0 or 1).  The number of options for input values ‚Äã‚Äãis limited.  Each of them can be compared to a neuron in a hidden layer that works only for a given input. <br><br>  <i>Analysis of the ‚Äúconditions‚Äù for each individual entry will require</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/ffa/246/061/ffa2460613304d50d44ae11fdf512c1a.gif"></i>  <i>hidden neurons (with <b>n</b> data).</i>  <i>In fact, things are usually not so bad.</i>  <i>There may be ‚Äúconditions‚Äù under which several input values ‚Äã‚Äãfit, and there may be ‚Äúoverlapping‚Äù ‚Äúconditions‚Äù that reach the correct inputs at their intersection.</i> <br><br>  Then we can use the connections between this neuron and the neurons on the output to set the final value for this particular case. <br><br>  <i>Versatility is not only perceptrons.</i>  <i>Networks with sigmoids in neurons (and other activation functions) are also universal: with a sufficient number of hidden neurons, they can construct an arbitrarily accurate approximation of any continuous function.</i>  <i>Demonstrating this is much more difficult, since it is impossible to just and isolate the inputs from each other.</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/a68/93f/770/a6893f7704a240139610a519d7e8cc5a.png" width="300"></div><br><br>  Therefore, it turns out that neural networks with one hidden layer are indeed universal.  However, this is nothing impressive or surprising.  The fact that the model can work as a reference table is not the strongest argument in favor of neural networks.  This merely means that the model is in principle capable of coping with the task.  Universality is understood only as the fact that the network can adapt to any samples, but this does not mean that it is able to adequately interpolate the solution for working with new data. <br><br>  No, versatility still does not explain why neural networks work so well.  The correct answer lies somewhat deeper.  To understand, we first consider several specific results. <br><br><h2>  Word representations (word embeddings) </h2><br>  I will begin with a particularly interesting sub-area of ‚Äã‚Äãdeep learning - with word representations of words (word embeddings).  In my opinion, vector representations are now one of the coolest topics for research in deep learning, although they were first proposed by Bengio, et al.  more than 10 years ago. <br><br>  <i>Vector representations were first proposed in the works of <a href="http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/64">Bengio et al, 2001</a> and <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/BengioDVJ03.pdf">Bengio et al, 2003</a> several years before the resurrection of deep learning in 2006, when neural networks were not yet in vogue.</i>  <i>The idea of ‚Äã‚Äãdistributed representations as such is even older (see, for example, <a href="http://www.cogsci.ucsd.edu/~ajyu/Teaching/Cogs202_sp13/Readings/hinton86.pdf">Hinton 1986</a> ).</i> <br><br>  In addition, I think that this is one of those tasks with the help of which an intuitive understanding of why deep learning is so effective is best formed. <br><br>  Vector word representation <img src="https://habrastorage.org/getpro/habr/post_images/1dd/d63/79c/1ddd6379c6a5e31b4c60f8d5ce31954f.gif">  - a parameterized function that maps words from a certain natural language to large-dimension vectors (say, from 200 to 500 measurements).  For example, it might look like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/81f/562/8f4/81f5628f47fdb25668b729befc97eb74.gif"><br><img src="https://habrastorage.org/getpro/habr/post_images/e7c/f7a/a4c/e7cf7aa4c9a4de46109156274e56eeaa.gif"><br><br>  (Typically, this function is defined by the lookup table, which is determined by the matrix <img src="https://habrastorage.org/getpro/habr/post_images/000/34b/59a/00034b59a7cac5ee441cace3a3b5ed80.gif">  in which each word corresponds to a string <img src="https://habrastorage.org/getpro/habr/post_images/d34/45e/004/d3445e004057c363541f0359ffe8a0b1.gif">  ). <br><br>  <b>W is</b> initialized by random vectors for each word.  She will be trained to give meaningful values ‚Äã‚Äãto solve a problem. <br><br>  For example, we can train the network to determine if a 5-gram is ‚Äúcorrect‚Äù (a sequence of five words, for example, 'cat sat on the mat').  5 grams can be easily obtained from Wikipedia, and then half of them can be ‚Äúspoiled‚Äù by replacing some of the words with random words in each (for example, 'cat sat song the mat'), as this almost always makes 5 grams meaningless . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/994/59b/c2f/99459bc2f27c4c95b34dbe293d5ee91e.png" width="500"></div><br>  <i>A modular network to determine if a 5 gram is ‚Äúcorrect‚Äù ( <a href="http://arxiv.org/pdf/1102.1808v3.pdf">Bottou (2011)</a> ).</i> <br><br>  The model we are teaching will pass each word from a 5-gram through <b>W</b> , getting their vector representations as output, and feed them into another module, <b>R</b> , which will try to predict whether the 5-gram is ‚Äúcorrect‚Äù or not.  We want it to be like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e5b/84c/26f/e5b84c26f74920d3b0cdb9713ca8a2aa.gif"><br><img src="https://habrastorage.org/getpro/habr/post_images/369/1f4/48f/3691f448fb319801ca96f356d015995a.gif"><br><br>  In order to predict these values ‚Äã‚Äãaccurately, the network needs a good choice of parameters for <b>W</b> and <b>R.</b> <br><br>  However, this task is boring.  Probably, the solution found will help to find grammatical errors in the texts or something like that.  But what is really valuable here is the resulting <b>W.</b> <br><br>  (Actually, the whole point of the task in teaching <b>W.</b> We could consider solutions to other problems; so, one of the common ones is to predict the next word in a sentence. But this is not our goal now. In the rest of this section we will talk about many results vector representation of words and will not be distracted by highlighting the difference between approaches). <br><br>  In order to ‚Äúfeel‚Äù how the space of vector representations is arranged, you can depict them using the clever method of visualization of high-dimensional data - tSNE. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/d3b/18e/166/d3b18e166a824fae813c1ad024bd40a9.png" width="500"></div><br>  <i>Visualize vector representations of words with tSNE.</i>  <i>On the left is the ‚Äúarea of ‚Äã‚Äãnumbers‚Äù, on the right is the ‚Äúarea of ‚Äã‚Äãprofessions‚Äù (from <a href="http://www.iro.umontreal.ca/~lisa/pointeurs/turian-wordrepresentations-acl10.pdf">Turian et al. (2010)</a> ).</i> <br><br>  Such a ‚Äúmap of words‚Äù seems quite meaningful.  ‚ÄúSimilar‚Äù words are close, and if you look at which ideas are closer to this one, it turns out that at the same time the close ones are ‚Äúsimilar‚Äù. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/6c1/76f/4d5/6c176f4d5b024544b2a606adda9ff01a.png" width="500"></div><br>  <i>Whose vector representations are closer to the representation of this word?</i>  <i>( <a href="http://arxiv.org/pdf/1103.0398v1.pdf">Collobert et al. (2011)</a> .)</i> <br><br>  It seems natural that the network will match words with similar values ‚Äã‚Äãto close vectors.  If you replace the word with a synonym ("some sing well" <img src="https://habrastorage.org/getpro/habr/post_images/c38/7f0/1f9/c387f01f97486953cdee266ae2d91345.gif">  ‚ÄúThe few sing well‚Äù), the ‚Äúcorrectness‚Äù of the sentence does not change.  It would seem that the sentences at the entrance differ significantly, but since <b>W</b> "shifts" the representations of synonyms ("some" and "few") to each other, for R there is little change. <br><br>  This is a powerful tool.  The number of possible 5-grams is huge, while the size of the training sample is relatively small.  Rapprochement of representations of similar words allows us, taking one sentence, as if to work with a whole class of ‚Äúsimilar‚Äù to it.  The matter is not limited to the replacement of synonyms, for example, the possible substitution of a word from the same class (‚Äúwall of blue‚Äù <img src="https://habrastorage.org/getpro/habr/post_images/c38/7f0/1f9/c387f01f97486953cdee266ae2d91345.gif">  ‚ÄúRed wall‚Äù).  Moreover, it makes sense to simultaneously replace several words (‚Äúwall of blue‚Äù <img src="https://habrastorage.org/getpro/habr/post_images/c38/7f0/1f9/c387f01f97486953cdee266ae2d91345.gif">  ‚ÄúRed ceiling‚Äù).  The number of such ‚Äúsimilar phrases‚Äù grows exponentially with the number of words. <br><br>  <i>Already in the foundational work of the <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/BengioDVJ03.pdf">A Neural Probabilistic Language Model (Bengio, et al. 2003),</a> substantial explanations are given why vector representations are such a powerful tool.</i> <br><br>  Obviously, this property <b>W</b> would be very useful.  But how is she taught?  It is very likely that <b>W</b> encounters the ‚Äúblue wall‚Äù sentence many times and recognizes it as correct before seeing the ‚Äúred wall‚Äù sentence.  Shift ‚Äúred‚Äù closer to ‚Äúblue‚Äù improves network performance. <br><br>  We still have to deal with examples of uses of each word, but analogies allow us to generalize to new combinations of words.  With all the words, the meaning of which we understand, we have come across before, but the meaning of the sentence can be understood without ever having heard it before.  Neural networks are able to do the same. <br><br><img src="https://habrastorage.org/files/767/8e2/0d9/7678e20d9e13426599b8d43ca0c888bb.png" width="150"><br>  <i><a href="https://www.aclweb.org/anthology/N/N13/N13-1090.pdf">Mikolov et al.</a></i>  <i><a href="https://www.aclweb.org/anthology/N/N13/N13-1090.pdf">(2013a)</a></i> <br><br>  Vector representations have another much more remarkable property: it seems that the analogy relations between words are determined by the value of the difference vector between their representations.  For example, apparently, the vector of the difference between "male-female" words is constant: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/235/7a0/7c1/2357a07c1ebcd9f9f700e621ba6ecc70.gif"><br><img src="https://habrastorage.org/getpro/habr/post_images/611/36c/7fe/61136c7fe990b62243dd4b5d69e7266b.gif"><br><br>  Maybe it will not surprise anyone.  In the end, the presence of genitive pronouns means that replacing the word ‚Äúkills‚Äù the grammatical correctness of the sentence.  We write: "she is aunt," but "he is uncle."  Similarly, "he is the king" and "she is the queen."  If we see in the text ‚Äúshe is uncle‚Äù, most likely, this is a grammatical mistake.  If in half of the cases the words were replaced at random, then this must be our case. <br><br>  ‚ÄúOf course!‚Äù - we say, looking back at past experience.  - ‚ÄúVector representations will be able to represent the floor.  Surely there is a separate dimension for the floor.  And also for the plural / singular.  Yes, such a relationship and so easily recognized! " <br><br>  It turns out, however, that much more complex relationships are ‚Äúcoded‚Äù in the same way.  Just wonders in the sieve (well, almost)! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/4c7/328/1da/4c73281dacef4a45ba3e5a9521f6b3d6.png" width="500"></div><br>  <i>Pairs of relationships (from <a href="http://arxiv.org/pdf/1301.3781.pdf">Mikolov et al. (2013b)</a> .)</i> <br><br>  It is important that all these properties of <b>W</b> - side effects.  We did not impose requirements that the representations of similar words should be close to each other.  We did not try to customize the analogies using vector differences.  We just tried to learn to check whether the proposal was ‚Äúcorrect‚Äù, and the properties from somewhere came from themselves in the process of solving the optimization problem. <br><br>  It seems that the great strength of neural networks is that they automatically learn how to build the ‚Äúbest‚Äù data views.  In turn, data presentation is an essential part of solving many machine learning problems.  And vector representations of words are one of the most amazing examples of learning representations. <br><br><h2>  Common views (shared representations) </h2><br>  The properties of vector representations are curious, of course, but can we do something useful with their help?  In addition to silly little things like checking whether this or that 5-gram is ‚Äúcorrect‚Äù. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/4e8/b1b/2b4/4e8b1b2b49ae45ffbfbc7a234f079155.png" width="120"></div><br>  <i><b>W</b> and <b>F are</b> trained by customizing task <b>A.</b></i>  <i>Then <b>G</b> can learn to solve problem <b>B</b> using <b>W.</b></i> <br><br>  We have trained vector representations of words to cope well with simple puzzles, but, knowing their wonderful properties that we have already observed, we can assume that they will be useful for more general problems.  In fact, vector representations like these are terribly important: <br><blockquote>  ‚ÄúThe use of vector representations of words ... has recently become the main‚Äú secret of the company ‚Äùin many natural language processing systems, including solving the task of identifying named entities (named entity recognition), part-of-speech-tagging, parsing and definition of semantic roles (semantic role labeling) ". </blockquote><br>  ( <a href="http://nlp.stanford.edu/~lmthang/data/papers/conll13_morpho.pdf">Luong et al. (2013)</a> .) <br><br>  The overall strategy is to train a good presentation for task <b>A</b> and use it to solve task <b>B</b> ‚Äî one of the main focuses in the magic hat of deep learning.  In different cases, it is called differently: pretraining, transfer learning, and multi-task learning.  One of the strengths of this approach is that it allows you to train views on several types of data. <br><br>  You can crank this trick differently.  Instead of setting up views for one type of data and using them to solve problems of different types, you can display different types of data into a single view! <br><br>  One of the wonderful examples of using such a trick is vector representations of words for two languages ‚Äã‚Äãproposed by <a href="http://ai.stanford.edu/~wzou/emnlp2013_ZouSocherCerManning.pdf">Socher et al.</a>  <a href="http://ai.stanford.edu/~wzou/emnlp2013_ZouSocherCerManning.pdf">(2013a)</a> .  We can learn to ‚Äúembed‚Äù words from two languages ‚Äã‚Äãinto a single space.  In this paper, the words ‚Äúare embedded‚Äù from English and Mandarin (Mandarin adverb of Chinese). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/3db/9e8/339/3db9e833921e446aaa59db2a0f8163f8.png" width="200"></div><br><br>  We teach two vector views. <img src="https://habrastorage.org/getpro/habr/post_images/1d3/5c3/4f0/1d35c34f05324431d0d5acfffb00f930.gif">  and <img src="https://habrastorage.org/getpro/habr/post_images/385/238/9f7/3852389f7c7d763b5374506a40caa1e1.gif">  just like it did above.  However, we know that some words in English and Chinese have similar meanings.  So, we will optimize one more criterion: representations of translations known to us should be at a small distance from each other. <br><br>  Of course, as a result, we observe that the ‚Äúsimilar‚Äù words known to us fit together.  It is not surprising, because we are so optimized.  Much more interesting is this: translations that we did not know about are also nearby. <br><br>  Perhaps this does not surprise anyone in the light of our past experience with vector representations of words.  They ‚Äúattract‚Äù similar words to each other, therefore, if we know that English and Chinese words mean about the same thing, then the representations of their synonyms should be located nearby.  We also know that pairs of words in relationships, such as gender differences, differ by a constant vector.  It seems that if you ‚Äúhave enough‚Äù to translate, you can adjust the differences so that they are the same in two languages.  As a result, if the ‚Äúmale versions‚Äù words in both languages ‚Äã‚Äãare translated into each other, we automatically get that the ‚Äúfemale versions‚Äù are also correctly translated. <br><br>  Intuition suggests that the languages ‚Äã‚Äãmust have a similar ‚Äústructure‚Äù and that by forcibly linking them at the selected points, we pull up the rest of the representations in the right places. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/bd8/51f/afe/bd851fafef0d47ada2e6bcefd668513c.png"></div><br>  <i>Visualization of bilingual vector representations using t-SNE.</i>  <i>Green is Chinese, Yellow is English ( <a href="http://ai.stanford.edu/~wzou/emnlp2013_ZouSocherCerManning.pdf">Socher et al. (2013a)</a> ).</i> <br><br>  When dealing with two languages, we train a single representation for two similar data types.  But we can ‚Äúenter‚Äù into a single space and very different types of data. <br><br>  Recently, with the help of deep learning, they began to build models that ‚Äúfit‚Äù images and words into a single representation space. <br>  <i>In a previous paper, the joint distribution of tags and images was modeled, but here everything is a little different.</i> <br><br>  The basic idea is that we classify images by expressing a vector from the word representation space.  Pictures with dogs are displayed in vectors near the representation of the word "dog", with horses - near "horse", with cars - near "car".  And so on. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/6be/8ff/19d/6be8ff19df22424b9ecd2ffa86f60cb2.png" width="200"></div><br><br>  The most interesting thing happens when you check the model on new image classes.  So, what will happen if we propose to classify the image of the cat model, which was not specifically taught to recognize them, that is, to display in a vector close to the "cat" vector? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/7e1/3d0/44c/7e13d044c6c54daa93a84d7cff09023a.png" width="500"></div><br>  <i><a href="http://nlp.stanford.edu/~socherr/SocherGanjooManningNg_NIPS2013.pdf">Socher et al.</a></i>  <i><a href="http://nlp.stanford.edu/~socherr/SocherGanjooManningNg_NIPS2013.pdf">(2013b)</a></i> <br><br>  It turns out that the network copes well with new classes of images.  Images of cats are not displayed at random points in space.  On the contrary, they are stacked in the neighborhood of the ‚Äúdog‚Äù vector and rather close to the ‚Äúcat‚Äù vector.  Similarly, truck images are displayed at points close to the truck vector, which is close to the associated car vector. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/d82/626/048/d826260485d84fbeb08ac7f7ae5a34f4.png" width="500"></div><br>  <i><a href="http://nlp.stanford.edu/~socherr/SocherGanjooManningNg_NIPS2013.pdf">Socher et al.</a></i>  <i><a href="http://nlp.stanford.edu/~socherr/SocherGanjooManningNg_NIPS2013.pdf">(2013b)</a></i> <br><br>  Members of the Stanford group did this with 8 famous classes and two unknowns.  The results are already impressive.  But with such a small number of classes, there are few points on which to interpolate the relationship between images and semantic space. <br><br>  The Google research team has built a much larger version of the same;  they took 1000 categories instead of 8 - and at about the same time ( <a href="http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41473.pdf">Frome et al. (2013)</a> ), and then offered another option ( <a href="http://arxiv.org/pdf/1312.5650.pdf">Norouzi et al. (2014)</a> ).  The last two works are based on a strong image classification model ( <a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">Krizehvsky et al. (2012)</a> ), but the images in them fit into the space of vector representations of words in different ways. <br><br>  And the results are impressive.  If it is not possible to accurately match the correct vector to the images of unknown classes, then at least it is possible to get to the right neighborhood.  Therefore, if you try to classify images from unknown categories and significantly different from each other, the classes can at least be distinguished. <br><br>  Even if I have never seen the Eskulap's snake, or the armadillo, when their pictures are shown to me, I can tell who is depicted where, because I have a general idea what kind of appearance this animal can have.  And such networks are also capable of it. <br><br>  (We often used the phrase ‚Äúthese words are similar.‚Äù But it seems that one can get much stronger results based on the relationship between words. In our space of words, there is a constant difference between ‚Äúmale‚Äù and ‚Äúfemale versions.‚Äù But also in space image representations have reproducible properties that make it possible to see the difference between the sexes. Beard, mustache and bald head are well recognizable signs of a man. Chest and long hair (a less reliable feature), makeup and jewelery are obvious indicators.  female <i>I am well aware that the physical signs of the floor are deceptive, for example, I will not say that all bald -. men, or that all who have a bust - a woman, but that it is more likely true than not, help. we'd better set the initial values.</i> <i><br></i>  .  Even if you have never seen the king, then, having seen the queen (which you identified by the crown) with a beard, you will surely decide that you need to use the "male version" of the word "queen"). <br><br>  General views (shared embeddings) - a breathtaking area of ‚Äã‚Äãresearch;  they are a very convincing argument in favor of advancing the teaching of ideas on the fronts of deep learning. <br><br><h2>  Recursive neural networks </h2><br><br>  We began the discussion of vector representations of words from this network: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/a66/bf2/19b/a66bf219bcd045e184881a3561dccbc8.png" width="500"></div><br>  <i>The modular network (Modular Network), which teaches vector representations of words ( <a href="http://arxiv.org/pdf/1102.1808v3.pdf">Bottou (2011)</a> ).</i> <br><br>  The diagram shows a modular network <br><br><img src="https://habrastorage.org/getpro/habr/post_images/834/5c5/90f/8345c590f26d5a6c00f27ccbf5a93e83.gif"><br><br>  It is built from two modules, <b>W</b> and <b>R.</b>  Such an approach to building neural networks - from smaller ‚Äúneural network modules‚Äù - is not too widespread.  However, he showed himself very well in natural language processing tasks. <br><br>  The models about which it was told are strong, but they have one annoying limitation: they cannot change the number of inputs. <br><br>  You can cope with this by adding the associating module <b>A</b> , which ‚Äúmerges‚Äù two vector representations. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/8d7/0b1/3ec/8d70b13ec6fc4a32a271ec33a4aee550.png" width="500"></div><br>  <i>From <a href="http://arxiv.org/pdf/1102.1808v3.pdf">Bottou (2011)</a></i> <br><br>  "Merging" the sequence of words, <b>A</b> allows you to represent phrases and even whole sentences.  And since we want to ‚Äúmerge‚Äù a different number of words, the number of entries should not be limited. <br><br>  It is not a fact that it is correct to ‚Äúmerge‚Äù words in a sentence just in order.  The sentence 'the cat sat on the mat' can be disassembled like this: '((the cat) (sat (on (the mat))'. We can apply <b>A</b> using this arrangement of brackets: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/f04/b32/b5a/f04b32b5aab949d197a2881cca41202a.png" width="500"></div><br>  <i>From <a href="http://arxiv.org/pdf/1102.1808v3.pdf">Bottou (2011)</a></i> <br><br>  These models are often called recursive neural networks (recursive neural networks), since the output signal of one module is often fed to the input of another module of the same type.  Sometimes they are also called neural networks of a tree structure (tree-structured neural networks). <br><br>  Recursive neural networks have achieved considerable success in solving several natural language processing tasks.  For example, in <a href="http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf">Socher et al.</a>  <a href="http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf">(2013c)</a> they are used to predict the tonality of a sentence: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/aac/127/271/aac127271c2c4a5587e212219d19d513.png" width="500"></div><br>  <i>(From <a href="http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf">Socher et al. (2013c)</a> .)</i> <br><br>  The main goal is to create a ‚Äúreversible‚Äù representation of a sentence, that is, such that it is possible to restore a sentence with approximately the same meaning.  For example, you can try to enter the dissociating module <b>D</b> , which will perform the action opposite to <b>A</b> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/92e/d50/b62/92ed50b6236b4536b55e0701020d3c53.png" width="500"></div><br>  <i>From <a href="http://arxiv.org/pdf/1102.1808v3.pdf">Bottou (2011)</a></i> <br><br>  If this succeeds, then we will have an incredibly powerful tool.  For example, you can try to build the presentation of sentences for two languages ‚Äã‚Äãand use it for automatic translation. <br><br>  Unfortunately, it turns out, it is very difficult.  Terribly difficult.  But, having received grounds for hope, many are fighting over the solution of the problem. <br><br>  Recently, <a href="http://arxiv.org/pdf/1406.1078v1.pdf">Cho et al.</a>  <a href="http://arxiv.org/pdf/1406.1078v1.pdf">(2014)</a> progress was made in the presentation of phrases, with a model that ‚Äúencodes‚Äù a phrase in English and ‚Äúdecodes‚Äù it as a phrase in French.  Just see what the views are! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/0bd/aa4/627/0bdaa462766c44bca2b222b199ef841a.png" width="500"></div><br>  <i>A small piece of space representations compressed with tSNE ( <a href="http://arxiv.org/pdf/1406.1078v1.pdf">Cho et al. (2014)</a> .)</i> <br><br><h2>  Criticism </h2><br>  I heard that some of the above results were criticized by researchers from other fields, in particular, by linguists and specialists in natural language processing.  It is not the results themselves that are criticized, but the consequences that derive from them, and the methods of comparison with other approaches. <br><br>  I do not think that I am prepared so well to articulate exactly what the problem is.  I would be glad if someone did this in the comments. <br><br><h2>  Conclusion </h2><br>  In-depth training in the service of learning representations is a powerful approach that seems to answer the question of why neural networks are so effective.  In addition, there is an amazing beauty in it: why are neural networks effective?  Yes, because the best ways to present data appear by themselves during the optimization of multilayer models. <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Deep learning is a very young area where theories have not yet settled down and where views change quickly. With this reservation, I would say that, in my opinion, the training of representations using neural networks is now very popular. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This post covers many of the research findings that seem impressive to me, but my main goal is to prepare the ground for the next post, which will examine the links between deep learning, type theory and functional programming. If you're interested, in order not to miss it, you can subscribe to my RSS feed. </font></font><br><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Further, the author asks to report any inaccuracies in the comments, see the </font></font><a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">original article</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></i> <br><br><h2>  Thanks </h2><br>  Eliana Lorch, Yoshua Bengio, Michael Nielsen, Laura Ball, Rob Gilson  Jacob Steinhardt    . <br><br> <i> :         .</i> </div><p>Source: <a href="https://habr.com/ru/post/253227/">https://habr.com/ru/post/253227/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../253211/index.html">Why I don‚Äôt dislike Git: hidden integrity</a></li>
<li><a href="../253213/index.html">Device and operation of input / output ports of AVR microcontrollers. Part 1</a></li>
<li><a href="../253217/index.html">Meet Envoyer.io (Part 1)</a></li>
<li><a href="../253223/index.html">Charting and graphing with Doxygen</a></li>
<li><a href="../253225/index.html">MailChimp UX Team: Design [Part 4 of the Book]</a></li>
<li><a href="../253229/index.html">Weekly assembly Vivaldi 1.0.129.2</a></li>
<li><a href="../253233/index.html">Boost C ++ libraries on Android</a></li>
<li><a href="../253235/index.html">About installing Crashplan in FreeNAS (and solving the Cyrillic problem)</a></li>
<li><a href="../253237/index.html">Easy weekend or "wheelbarrow for pumping"</a></li>
<li><a href="../253238/index.html">Installing and running Android applications on Linux</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>