<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Computer vision, cloud development and competition</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Sudden horse from Spatial Memory for Context Reasoning in Object Detection (presented at ICCV 2017) 

 We have some news, but it‚Äôs boring to just writ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Computer vision, cloud development and competition</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/mv/14/ed/mv14ed5bgm1vna4llxtqo2ncqh4.jpeg" alt="image"><br><br>  <i>Sudden horse from Spatial Memory for Context Reasoning in Object Detection (presented at ICCV 2017)</i> <br><br>  We have some news, but it‚Äôs boring to just write about a competition where you can win a camera for your home or a vacancy for our cloud team.  Therefore, we will begin with information that will be of interest to everyone (ok, almost everyone - this will be a video analytics). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Recently, the largest computer vision technology conference, the International Conference on Computer Vision 2017, was completed. Teams of scientists and representatives of research departments of various corporations presented their projects on photo enhancement, image generation by description, peeping around the corner using light analysis, etc.  We will talk about several interesting solutions that can be used in the field of video surveillance. <br><a name="habracut"></a><br><h2>  Photos of the quality of "DSLRs" on mobile devices </h2><br><img src="https://habrastorage.org/webt/pd/7a/f6/pd7af6qag6rty_6p2sowoqnz3le.png" alt="image"><br><br>  Matrix surveillance cameras and smartphones are being improved from year to year, but it seems they never catch up with SLR cameras.  And one reason - the physical limitations of mobile devices. <br><br>  Researchers from the Swiss Federal Institute of Technology in Zurich <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Ignatov_DSLR-Quality_Photos_on_ICCV_2017_paper.pdf">presented an</a> algorithm that transforms the image obtained on the camera is not the highest quality, correctly "correcting" the details and colors.  The algorithm cannot create something in the image that is not there, but it can help improve photos not only by adjusting the brightness and contrast. <br><br>  Photographs are processed using a neural network, which improves both color reproduction and image sharpness.  The grid was trained on objects that were photographed simultaneously on the camera of a smartphone and on a digital camera.  Understanding what quality is optimal for a conditional object, the grid seeks to change the parameters of the image so as to correspond as closely as possible to the ‚Äúideal‚Äù image. <br><br>  The sophisticated image error perception function combines color, tonality and texture data.  The study shows that the enhanced images demonstrate quality comparable to photographs taken with SLR cameras, while the method itself can be applied to any type of digital camera. <br><br>  The current version of the image quality improvement algorithms can be tested on <a href="http://phancer.com/">phancer.com</a> - just upload any image. <br><br><h2>  Creating photo-realistic images from scratch </h2><br><img src="https://habrastorage.org/webt/ep/gk/xi/epgkxiojgdqimve8ux0jjesatsm.jpeg" alt="image"><br><br>  A large team of scientists ‚Äî 7 people on two continents from Rutgers University, Lihai University, China University of Hong Kong, and Baidu Research‚Äôs research unit ‚Äî <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_StackGAN_Text_to_ICCV_2017_paper.pdf">proposed a</a> way to use meshes to create photorealistic images based on text descriptions.  Something similar to the work of this artist, who creates a picture based on the images in his head - first a rough sketch appears on the canvas, and then more and more precise details. <br><br>  The computer first makes the first attempt to create an image based on the text description of the specified objects (and the knowledge base of the images known to it), and then a separate algorithm evaluates the resulting picture and makes suggestions for improving the image.  At the entrance, for example, there is a ‚Äúgreen bird‚Äù, a base of flowers and a base of famous birds.  There are a large number of images that correctly correspond to this textual description - and this is one of the problems. <br><br>  To generate images from textual descriptions, several folded generative-contention networks (SGAN) are used.  GAN Stage-I sketches a primitive sketch and adds the primary colors of the objects based on the text description data.  GAN Stage-II accepts Stage-I results and text descriptions as input and generates high-resolution images and photo-realistic details.  GAN Stage-II is able to correct defects and add interesting details.  Samples created by StackGAN are more plausible than those generated by other existing approaches. <br><br>  Since GAN Stage-I generates sketches for objects and for the background, GAN Stage II only needs to focus on the details and fix the defects.  GAN Stage-II learns to process textual information that GAN Stage-I did not take to work, and draws more detailed information about the object. <br><br><h2>  Handling complex interrelated events in video </h2><br><img src="https://habrastorage.org/webt/os/ev/bs/osevbsjqcc7oyywdbkvhec3dmao.png" alt="image"><br><br>  At Stanford University, they thought that there were too many events happening in the commercials.  For example, in the video ‚Äúa person plays the piano‚Äù, the video may also contain ‚Äúa dancing person‚Äù or ‚Äúapplauding a crowd of people‚Äù.  The new study proposed a model that allows you to identify all events and give them a description in natural language. <br><br>  The model is <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Krishna_Dense-Captioning_Events_in_ICCV_2017_paper.pdf">based on the basis of space-time descriptions</a> .  In fact, the computer was first trained on thousands of videos containing detailed descriptions of the context. <br><br>  Interestingly, in DeepMind, in order to solve a similar problem, <a href="https://arxiv.org/abs/1705.08168">they went the other way</a> and began to correlate the video sequence with sound in order to recognize objects without first understanding what is in the frame.  The Google algorithm consists of three parts: the first neural network processes images taken from the video, the second - the audio corresponding to these images, the third part learns to correlate images with a specific sound. <br><br>  Similar technologies in video surveillance can be used for convenient and fast search in the data archive. <br><br><h2>  Description of images in natural language </h2><br><img src="https://habrastorage.org/webt/3a/wb/fr/3awbfru28ftmojkrsvuccre-a0q.png" alt="image"><br><br>  Which of the two descriptions of the top photo seems more human to you: ‚ÄúA cow standing in a field with houses‚Äù or ‚ÄúA gray cow walking along a large green field in front of houses‚Äù?  Last probably.  But computers have no natural understanding of what makes a person intuitively choose the right (from our point of view) option. <br><br>  In the <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Dai_Towards_Diverse_and_ICCV_2017_paper.pdf">Towards Diverse and Natural Image Descriptions via a Conditional GAN</a> project, one neural network creates a description of the scene in the image, while the other compares this description with human-made and evaluates elements that better fit our own style of speech. <br><br>  A system was proposed that included several generative competitive neural networks, one of which selected a description for the image, and the second assessed how well the description corresponded to the visual content.  It was possible to achieve a level of recognition of objects and the relationships between them, that the context of the events being processed had no meaning.  Although the system has never seen a cow drinking milk through a straw, it can recognize this image because it has an idea of ‚Äã‚Äãwhat a cow looks like, milk, a straw, and what it means to drink. <br><br><h2>  The camera looks around the corner. </h2><br><img src="https://habrastorage.org/webt/tj/2k/p1/tj2kp1ohutmjswitnpfvhggwk0c.png" alt="image"><br><br>  A few years ago, engineers and physicists from Scotland <a href="">created a camera</a> that literally looked around the corner and tracked the movements of people and objects behind it. <br><br>  The solution consisted of a set of two devices - a ‚Äúphoton gun‚Äù, which scientists fired at the floor and wall, located on the opposite side of the corner, and a special light-sensitive matrix based on <a href="https://ru.wikipedia.org/wiki/%25D0%259B%25D0%25B0%25D0%25B2%25D0%25B8%25D0%25BD%25D0%25BD%25D1%258B%25D0%25B9_%25D1%2584%25D0%25BE%25D1%2582%25D0%25BE%25D0%25B4%25D0%25B8%25D0%25BE%25D0%25B4">avalanche photodiodes</a> , capable of recognizing even single photons. <br><br>  The photons from the gun beam, reflecting from the surface of the wall and the floor, collide and are reflected from the surface of all the objects that are behind the wall.  Some of them fall into the detector, reflecting again from the wall, which allows, based on the time of the beam, to determine the position, shape and appearance of what is hiding around the corner. <br><br>  The system worked extremely slowly - it took about three minutes to form the initial image.  In addition, the result was issued in the form of an image of 32 by 32 pixels, which actually did not allow to see anything in the picture, except for a rough silhouette. <br><br>  There have been several attempts to improve the quality, but the decision came from an unexpected side.  Experts from MIT and Google Research have <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Bouman_Turning_Corners_Into_ICCV_2017_paper.pdf">suggested</a> looking around the corner with the help of reflected light.  Looking very carefully at the light, which is visible from different angles, you can get an idea of ‚Äã‚Äãthe color and spatial arrangement of objects hidden around the corner. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/W6wzUMk9w-c" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  It looks like a scene from the American procedure.  The new image processing system does not require any special equipment; it will even work with the smartphone's camera, using only reflected light to detect objects or people and measure their speed and trajectory ‚Äî all in real time. <br><br>  Most objects reflect a small amount of light on the ground in your line of sight, creating a fuzzy shadow, which is called a "penumbra."  Using video of the penumbra, the CornerCameras system can sew a number of images, getting rid of superfluous noise.  Although the objects are not actually visible on the camera, you can see how their movements affect the penumbra to determine where they are and where they go. <br><br>  *** <br><br>  If you are interested in these projects, have your own ideas or want to get acquainted with our developments - come yourself or bring friends.  We need new people in the Cloud team to work on products based on cloud-based video surveillance and computer vision. <br><br>  For a successful recommendation after the developer‚Äôs design in the state - we give the iPhone X. And if the developer is with us going to paintball too, then the recommendation is also a protective glass for iPhone!  ;) <a href="https://ru.ivideon.com/job-russia-moscow-office-engineering-department-python-razrabotchik/">More about the job</a> (respond there). <br><br>  And the last for today: until November 19 (inclusive), follow the <a href="https://ru.ivideon.com/shop/ivideons-actions/ivideon_oco2_contest_2017/">link</a> .  You need to answer one question, leave your mail, throw a link to the competition to any social network and wait - a random number generator based on the entropy of atmospheric noise will select several participants whom we will award to a home Wi-Fi camera with Oco2. </div><p>Source: <a href="https://habr.com/ru/post/342618/">https://habr.com/ru/post/342618/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../342606/index.html">Automation of a warehouse in Bosch Rexroth: software, hardware, integration</a></li>
<li><a href="../342608/index.html">Tips for yourself in youth (design version)</a></li>
<li><a href="../342610/index.html">Learn OpenGL. Lesson 4.1 - Depth Test</a></li>
<li><a href="../342614/index.html">MEGA Accelerator Finalists: Five New Ideas for Shopping Malls</a></li>
<li><a href="../342616/index.html">Let's argue about Dart and Flutter at the meeting of the Russian-speaking community Dart in St. Petersburg</a></li>
<li><a href="../342620/index.html">Hello Logify, or monitor errors on installed applications</a></li>
<li><a href="../342624/index.html">About PVS-Studio on the eve of the open conference of the ISP RAS. V.P. Ivannikova</a></li>
<li><a href="../342626/index.html">Results hackathon HR-hack</a></li>
<li><a href="../342628/index.html">Tutu.ru: How to spend the IT Day on their own and inexpensively</a></li>
<li><a href="../342632/index.html">Optimize the speed of rendering web pages</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>