<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Python Testing with pytest. Getting started with pytest, Chapter 1</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Back Next 


 I found that Python Testing with pytest is an extremely useful introductory guide to the pytest testing environment. This already brings...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Python Testing with pytest. Getting started with pytest, Chapter 1</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/jl/jn/bb/jljnbbjr-ejh473xy_eccsmknpk.png">  <a href="https://habr.com/ru/post/426699/">Back</a> <a href="https://habr.com/ru/post/448788/">Next</a> <img src="https://habrastorage.org/webt/rw/dy/-g/rwdy-grsvbpcetjttrmecdkxtlk.png"></p><br><p>  <em>I found that Python Testing with pytest is an extremely useful introductory guide to the pytest testing environment.</em>  <em>This already brings me dividends in my company.</em> <em><br></em> <br>  Chris Shaver <br>  VP of Product, Uprising Technology </p><br><p><img src="https://habrastorage.org/webt/hd/--/9w/hd--9w134j0rxhmxftrflbbdopy.png"></p><a name="habracut"></a><br><p>  The examples in this book are written using Python 3.6 and pytest 3.2.  pytest 3.2 supports Python 2.6, 2.7 and Python 3.3+. </p><br><blockquote>  The source code for the Tasks project, as well as for all the tests shown in this book, is available via the <a href="https://pragprog.com/titles/bopytest/source_code" title="https://pragprog.com/titles/bopytest/source_code">link</a> on the book's web page at <a href="https://pragprog.com/titles/bopytest" title="https://pragprog.com/titles/bopytest">pragprog.com</a> .  You do not need to download the source code to understand the test code;  The test code is presented in a convenient form in the examples.  But to follow along with the project objectives, or to adapt test examples to test your own project (your hands are untied!), You should go to the book‚Äôs web page and download the work.  In the same place, on the book‚Äôs web page there is a link for <a href="https://pragprog.com/titles/bopytest/errata" title="https://pragprog.com/titles/bopytest/errata">errata</a> messages and a <a href="https://forums.pragprog.com/forums/438" title="https://forums.pragprog.com/forums/438">discussion forum</a> . </blockquote><p>  Under the spoiler is a list of articles in this series. </p><br><div class="spoiler">  <b class="spoiler_title">Table of contents</b> <div class="spoiler_text"><ul><li>  <a href="https://habr.com/ru/post/426699/"><strong>Introduction</strong></a> </li><li>  <a href="https://habr.com/ru/post/448782/"><strong>Chapter 1: Getting started with pytest</strong></a> (This article) </li><li>  <a href="https://habr.com/ru/post/448788/"><strong>Chapter 2: Writing Test Functions</strong></a> </li><li>  <a href="https://habr.com/ru/post/448786/"><strong>Chapter 3: Pytest Fixtures</strong></a> </li><li>  <a href="https://habr.com/ru/post/448792/"><strong>Chapter 4: Builtin Fixtures</strong></a> </li><li>  <a href="https://habr.com/ru/post/448794/"><strong>Chapter 5: Plugins</strong></a> </li><li>  <a href="https://habr.com/ru/post/448796/"><strong>Chapter 6: Configuration</strong></a> </li><li>  <a href="https://habr.com/ru/post/448798/"><strong>Chapter 7: Using pytest with other tools</strong></a> </li></ul></div></div><br><h2 id="poehali">  Go </h2><br><p>  This is a test: </p><br><blockquote>  <strong>ch1 / test_one.py</strong> </blockquote><br><pre><code class="plaintext hljs">def test_passing(): assert (1, 2, 3) == (1, 2, 3)</code> </pre> <br><p>  Here is what it looks like at startup: </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code/ch1 $ pytest test_one.py ===================== test session starts ====================== collected 1 items test_one.py . =================== 1 passed in 0.01 seconds ===================</code> </pre> <br><p><img src="https://habrastorage.org/webt/v0/ak/lu/v0aklucdm0i8nrqtk1v1dwb2oeg.png"></p><br><p>  A dot after <em>test_one.py</em> means that one test was run and it passed.  If you need more information, you can use <code>-v</code> or <code>--verbose</code> : </p><br><pre> <code class="plaintext hljs">$ pytest -v test_one.py ===================== test session starts ====================== collected 1 items test_one.py::test_passing PASSED =================== 1 passed in 0.01 seconds ===================</code> </pre> <br><p><img src="https://habrastorage.org/webt/ss/rb/7i/ssrb7i7io-h6ikadzulg5yncvyw.png"></p><br><p>  If you have a color terminal, PASSED and the bottom row are green.  Perfectly! </p><br><p>  This is a bad test: </p><br><blockquote>  <strong>ch1 / test_two.py</strong> </blockquote><br><pre> <code class="plaintext hljs">def test_failing(): assert (1, 2, 3) == (3, 2, 1)</code> </pre> <br><p>  How pytest shows failures during testing is one of the many reasons why developers love pytest.  Let's see what comes of it: </p><br><pre> <code class="plaintext hljs">$ pytest test_two.py ===================== test session starts ====================== collected 1 items test_two.py F =========================== FAILURES =========================== _________________________ test_failing _________________________ def test_failing(): &gt; assert (1, 2, 3) == (3, 2, 1) E assert (1, 2, 3) == (3, 2, 1) E At index 0 diff: 1 != 3 E Use -v to get the full diff test_two.py:2: AssertionError =================== 1 failed in 0.04 seconds ===================</code> </pre> <br><p><img src="https://habrastorage.org/webt/dr/sh/wu/drshwua2ie5btbcena-4xlewiz4.png"></p><br><p>  Fine!  The test test <em>test_failing</em> gets its section to show us why it did not pass. </p><br><p>  And pytest accurately reports that the first failure: index 0 is a mismatch. </p><br><p>  Much of this message is red, which makes it really stand out (if you have a color terminal). </p><br><p>  This is already a lot of information, but there is a line with a hint that says Use <code>-v</code> to get even more descriptions of the differences. </p><br><p>  Let's zayuzay this <code>-v</code> : </p><br><pre> <code class="plaintext hljs">$ pytest -v test_two.py ===================== test session starts ====================== collected 1 items test_two.py::test_failing FAILED =========================== FAILURES =========================== _________________________ test_failing _________________________ def test_failing(): &gt; assert (1, 2, 3) == (3, 2, 1) E assert (1, 2, 3) == (3, 2, 1) E At index 0 diff: 1 != 3 E Full diff: E - (1, 2, 3) E ? ^ ^ E + (3, 2, 1) E ? ^ ^ test_two.py:2: AssertionError =================== 1 failed in 0.04 seconds ===================</code> </pre> <br><p><img src="https://habrastorage.org/webt/u9/8l/dn/u98ldncvvmy1ha0ygy6krwekhvm.png"></p><br><p>  Wow! <br>  pytest adds a caret (^) to show us exactly what the difference is. <br>  If you're already impressed with how easy it is to write, read and run tests with pytest and how easy it is to read the output to see where the failure happened, well ... you haven't seen anything yet.  Where it came from, there are more miracles.  Stay and let me show you why I think that pytest is absolutely the best test platform. </p><br><p>  In the remainder of this chapter, you install pytest, look at the various ways to start it, and execute some of the most frequently used command line options.  In future chapters, you will learn how to write test functions that maximize the power of pytest, how to pull installation code into setup and removal sections, called fixtures, and how to use fixtures and plug-ins to really overload software testing. </p><br><p>  But first I have to apologize.  Sorry to test <code>assert (1, 2, 3) == (3, 2, 1)</code> , it‚Äôs so boring.  I hear snoring ?!  No one would have written such a test in real life.  Software tests consist of code that checks other software, which unfortunately will not always work positively.  A <code>(1, 2, 3) == (1, 2, 3)</code> will always work.  That is why we will not use too stupid tests like this in the rest of the book.  We will look at tests for a real software project.  We will use the sample project Tasks, which requires a test code.  Hopefully, it is simple enough to be easily understood, but not so easy to be boring. </p><br><p>  Another useful application of software tests is to test your assumptions about how the software under test works, which may include testing your understanding of third-party modules and packages and even building Python data structures. </p><br><p>  The Tasks project uses a Task structure based on the factory namedtuple method, which is part of the standard library.  The task structure is used as a data structure for transferring information between the user interface and the API. </p><br><p>  In the rest of this chapter, I will use Task to demonstrate running pytest and using some commonly used command line parameters. </p><br><p>  Here is the task: </p><br><pre> <code class="plaintext hljs">from collections import namedtuple Task = namedtuple('Task', ['summary', 'owner', 'done', 'id'])</code> </pre> <br><p>  Pytest and install it. </p><br><p>  The factory <em>namedtuple ()</em> function exists with Python 2.6, but I still discover that many Python developers do not know how cool it is.  At least, using the problem for test cases will be more interesting than <code>(1, 2, 3) == (1, 2, 3)</code> or <code>(1, 2)==3</code> . </p><br><p>  Before moving on to the examples, let's take a step back and talk about where to get the pytest and how to install it. </p><br><h2 id="dobyvaem-pytest">  We get pytest </h2><br><p>  Headquarters pytest <a href="https://docs.pytest.org/">https://docs.pytest.org</a> .  This is official documentation.  But it is distributed through PyPI (Python package index) at <a href="https://pypi.python.org/pypi/pytest">https://pypi.python.org/pypi/pytest</a> . </p><br><p>  Like other Python packages distributed through PyPI, use <strong>pip</strong> to install pytest to the virtual environment used for testing: </p><br><pre> <code class="plaintext hljs">$ pip3 install -U virtualenv $ python3 -m virtualenv venv $ source venv/bin/activate $ pip install pytest</code> </pre> <br><p>  If you are not familiar with virtualenv or pip, I will introduce you.  Read Appendix 1, ‚ÄúVirtual Environments,‚Äù on page 155 and in Appendix 2, on page 159. </p><br><p>  <strong>How about windows, python 2 and venv?</strong> </p><br><p>  The example for virtualenv and pip should work on many POSIX systems, such as Linux and macOS, as well as on many versions of Python, including Python 2.7.9 and later. </p><br><p>  The source <em>venv / bin / activate</em> in the line will not work for Windows, use <em>venv \ Scripts \ activate.bat</em> instead. <br>  Do this: </p><br><pre> <code class="plaintext hljs">C:\&gt; pip3 install -U virtualenv C:\&gt; python3 -m virtualenv venv C:\&gt; venv\Scripts\activate.bat (venv) C:\&gt; pip install pytest</code> </pre> <br><p>  For Python 3.6 and higher, you can do venv instead of virtualenv, and you do not have to worry about installing it first.  It is included in Python 3.6 and higher.  However, I heard that some platforms still behave better with virtualenv. </p><br><h2 id="zapuskaem-pytest">  Run pytest </h2><br><pre> <code class="plaintext hljs">$ pytest --help usage: pytest [options] [file_or_dir] [file_or_dir] [...] ...</code> </pre> <br><p>  Without arguments, pytest will examine your current directory and all the subdirectories for the test files and run the test code that it finds.  If you pass the pytest file name, directory name or list of them, they will be found there instead of the current directory.  Each directory specified on the command line is recursively searched for test code. </p><br><p>  For example, let's create a subdirectory called tasks and start with this test file: </p><br><blockquote>  <strong>ch1 / tasks / test_three.py</strong> </blockquote><br><pre> <code class="plaintext hljs">"""   Task.""" from collections import namedtuple Task = namedtuple('Task', ['summary', 'owner', 'done', 'id']) Task.__new__.__defaults__ = (None, None, False, None) def test_defaults(): """  ,      .""" t1 = Task() t2 = Task(None, None, False, None) assert t1 == t2 def test_member_access(): """  .field () namedtuple.""" t = Task('buy milk', 'brian') assert t.summary == 'buy milk' assert t.owner == 'brian' assert (t.done, t.id) == (False, None)</code> </pre> <br><p>  It is not necessary to identify the namedtuples. </p><br><p>  We‚Äôve gotten more tests to complete the _asdict () and _replace () functionality </p><br><p>  You can use <em><code>__new __.__ defaults__</code></em> to create Task objects without specifying all the fields.  The test <em>test_defaults () is</em> designed to demonstrate and test how defaults work. </p><br><p>  The <em><code>test_member_access()</code></em> test should demonstrate how to address members by the name nd not by index, which is one of the main reasons for using namedtuples. <br>  Let's add a couple more tests to the second file to demonstrate the <em><code>_asdict()</code></em> and <em><code>_replace()</code></em> functions </p><br><blockquote>  <strong>ch1 / tasks / test_four.py</strong> </blockquote><br><pre> <code class="plaintext hljs">"""   Task.""" from collections import namedtuple Task = namedtuple('Task', ['summary', 'owner', 'done', 'id']) Task.__new__.__defaults__ = (None, None, False, None) def test_asdict(): """_asdict()   .""" t_task = Task('do something', 'okken', True, 21) t_dict = t_task._asdict() expected = {'summary': 'do something', 'owner': 'okken', 'done': True, 'id': 21} assert t_dict == expected def test_replace(): """    fields.""" t_before = Task('finish book', 'brian', False) t_after = t_before._replace(id=10, done=True) t_expected = Task('finish book', 'brian', True, 10) assert t_after == t_expected</code> </pre> <br><p>  To run pytest you have the ability to specify files and directories.  If you do not specify any files or directories, pytest will look for tests in the current working directory and subdirectories.  It searches for files starting with test_ or ending with _test.  If you run pytest from the ch1 directory, without commands, you will run tests for four files: </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code/ch1 $ pytest ===================== test session starts ====================== collected 6 items test_one.py . test_two.py F tasks/test_four.py .. tasks/test_three.py .. =========================== FAILURES =========================== _________________________ test_failing _________________________ def test_failing(): &gt; assert (1, 2, 3) == (3, 2, 1) E assert (1, 2, 3) == (3, 2, 1) E At index 0 diff: 1 != 3 E Use -v to get the full diff test_two.py:2: AssertionError ============== 1 failed, 5 passed in 0.08 seconds ==============</code> </pre> <br><p><img src="https://habrastorage.org/webt/ut/2o/kw/ut2okwbdqprgbriqvfswsjd-7ti.png"></p><br><p>  To perform only our new task tests, you can provide pytest with all the file names you want to run or the directory, or call pytest from the directory where our tests are located: </p><br><pre> <code class="plaintext hljs">$ pytest tasks/test_three.py tasks/test_four.py ===================== test session starts ====================== collected 4 items tasks/test_three.py .. tasks/test_four.py .. =================== 4 passed in 0.02 seconds =================== $ pytest tasks ===================== test session starts ====================== collected 4 items tasks/test_four.py .. tasks/test_three.py .. =================== 4 passed in 0.03 seconds =================== $ cd /path/to/code/ch1/tasks $ pytest ===================== test session starts ====================== collected 4 items test_four.py .. test_three.py .. =================== 4 passed in 0.02 seconds ===================</code> </pre> <br><p><img src="https://habrastorage.org/webt/or/77/t4/or77t4go_qqok-v8qnenx5rxcjy.png"></p><br><p>  The pytest part of the execution where pytest passes and finds which tests to run is called test discovery.  pytest was able to find all the tests that we wanted to run, because we called them according to the pytest naming conventions. </p><br><p>  Below is a brief overview of the naming conventions so that your test code can be detected using pytest: </p><br><ul><li>  Test files must be named <code>test_&lt;something&gt;.py</code> or <code>&lt;something&gt;_test.py</code> . </li><li>  Test methods and functions should be called <code>test_&lt;something&gt;</code> . </li><li>  Test classes should be called <code>Test&lt;Something&gt;</code> . </li></ul><br><p>  Since our test files and functions start with <code>test_</code> , we are fine.  There are ways to change these detection rules if you have a bunch of tests with different names. <br>  I will cover this in Chapter 6, ‚ÄúConfiguration,‚Äù on page 113. </p><br><p>  Let's take a closer look at the result of running only one file: </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code/ch1/tasks $ pytest test_three.py ================= test session starts ================== platform darwin -- Python 3.6.2, pytest-3.2.1, py-1.4.34, pluggy-0.4.0 rootdir: /path/to/code/ch1/tasks, inifile: collected 2 items test_three.py .. =============== 2 passed in 0.01 seconds ===============</code> </pre> <br><p>  The result tells us quite a bit. </p><br><blockquote>  ===== test session starts ==== </blockquote><p>  . </p><br><p>  pytest provides an elegant delimiter to start a test session.  A session is one pytest call, including all tests performed in several directories.  This session definition becomes important when I talk about the session area in relation to the pytest fixtures in defining the fixtures area, on page 56. </p><br><p>  The darwin platform is on my Mac.  On a Windows PC, the platform is different.  The following lists the versions of Python and pytest, as well as dependencies on pytest packages.  Both py and pluggy are packages developed by the pytest team to help with the pytest implementation. </p><br><blockquote>  <strong>rootdir: / path / to / code / ch1 / tasks, inifile:</strong> </blockquote><p>  <code>rootdir</code> is the topmost shared directory for all directories in which test code is searched.  The <code>inifile</code> (empty here) lists the configuration files used.  Configuration files can be <code>pytest.ini</code> , <code>tox.ini</code> or <code>setup.cfg</code> .  More information about the configuration files can be found in Chapter 6, ‚ÄúConfiguration,‚Äù on page 113. </p><br><blockquote>  <strong>collected 2 items</strong> </blockquote><p>  These are two test functions in the file. </p><br><blockquote>  <strong>test_three.py ..</strong> </blockquote><p>  <code>test_three.py</code> shows the file being tested.  There is one line for each test file.  Two points mean that the tests passed - one point for each test function or method.  Points are intended only for passing tests.  Failures, errors (errors), skips (gaps), xfails, and xpasses are denoted with F, E, s, x, and X, respectively.  If you want to see more points for passing tests, use the <code>-v</code> or <code>--verbose</code> option. </p><br><blockquote>  == 2 passed in 0.01 seconds == </blockquote><p>  This line refers to the number of passed tests and the time spent on the entire test session.  If there are non-passing tests, the number of each category will also be listed here. </p><br><p>  A test result is the primary way a user who performs a test or views a result can understand what happened during the test.  In pytest, test functions may have several different results, and not just pass or fail.  Here are the possible results of the test function: </p><br><ul><li>  PASSED (.): The test was successful. </li><li>  FAILED (F): Test failed (or XPASS + strict). </li><li>  SKIPPED (s): The test has been missed.  You can force pytest to skip the test using the decorators <code>@pytest.mark.skip()</code> or <code>pytest.mark.skipif()</code> , discussed in the skipping tests section, on page 34. </li><li>  xfail (x): The test should not have passed, was launched and failed.  You can force pytest to indicate that the test should fail using the <code>@pytest.mark.xfail()</code> decorator, described in test markings as failing, on page 37. </li><li>  XPASS (X): The test should not have passed, was launched and passed! .. </li><li>  ERROR (E): An exception occurred outside the testing function, or in fixture, discussed in chapter 3, pytest fixtures, on page 49, or in hook function, discussed in chapter 5, Plugins, on page 95. </li></ul><br><h2 id="vypolnenie-tolko-odnogo-testa">  Performing Only One Test </h2><br><p>  Perhaps the first thing you want to do after you start writing tests is to run only one.  Specify the file directly and add the name <code>::test_name</code> : </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code/ch1 $ pytest -v tasks/test_four.py::test_asdict =================== test session starts =================== collected 3 items tasks/test_four.py::test_asdict PASSED ================ 1 passed in 0.01 seconds =================</code> </pre> <br><p>  Now let's consider some options. </p><br><h2 id="ispolzovanie-opciy">  Using Options </h2><br><p>  We have already used the verbose, <code>-v</code> or <code>--verbose</code> option a couple of times, but there are still many options worth knowing about.  We are not going to use them all in this book, only some.  You can view the full list using the pytest option <code>--help</code> . </p><br><p>  Below are a few options that are quite useful when working with pytest.  This is not a complete list, but these options are enough for the beginning. </p><br><pre> <code class="plaintext hljs">$ pytest --help usage: pytest [options] [file_or_dir] [file_or_dir] [...] ... subset of the list ... positional arguments: file_or_dir general: -k EXPRESSION only run tests which match the given substring expression. An expression is a python evaluatable expression where all names are substring-matched against test names and their parent classes. Example: -k 'test_method or test_other' matches all test functions and classes whose name contains 'test_method' or 'test_other', while -k 'not test_method' matches those that don't contain 'test_method' in their names. Additionally keywords are matched to classes and functions containing extra names in their 'extra_keyword_matches' set, as well as functions which have names assigned directly to them. -m MARKEXPR only run tests matching given mark expression. example: -m 'mark1 and not mark2'. --markers show markers (builtin, plugin and per-project ones). -x, --exitfirst exit instantly on first error or failed test. --maxfail=num exit after first num failures or errors. ... --capture=method per-test capturing method: one of fd|sys|no. -s shortcut for --capture=no. ... --lf, --last-failed rerun only the tests that failed at the last run (or all if none failed) --ff, --failed-first run all tests but run the last failures first. This may re-order tests and thus lead to repeated fixture setup/teardown ... reporting: -v, --verbose increase verbosity. -q, --quiet decrease verbosity. --verbosity=VERBOSE set verbosity ... -l, --showlocals show locals in tracebacks (disabled by default). --tb=style traceback print mode (auto/long/short/line/native/no). ... --durations=N show N slowest setup/test durations (N=0 for all). ... collection: --collect-only only collect tests, don't execute them. ... test session debugging and configuration: --basetemp=dir base temporary directory for this test run.(warning: this directory is removed if it exists) --version display pytest lib version and import information. -h, --help show help message and configuration info</code> </pre><br><h3 id="--collect-only">  --collect-only </h3><br><p>  The <code>--collect-only</code> parameter indicates which tests will be executed with the given parameters and configuration.  This option is convenient to first show that the output can be used as a reference for other examples.  If you start in the ch1 directory, you should see all the test functions that you have looked at so far in this chapter: </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code/ch1 $ pytest --collect-only =================== test session starts =================== collected 6 items &lt;Module 'test_one.py'&gt; &lt;Function 'test_passing'&gt; &lt;Module 'test_two.py'&gt; &lt;Function 'test_failing'&gt; &lt;Module 'tasks/test_four.py'&gt; &lt;Function 'test_asdict'&gt; &lt;Function 'test_replace'&gt; &lt;Module 'tasks/test_three.py'&gt; &lt;Function 'test_defaults'&gt; &lt;Function 'test_member_access'&gt; ============== no tests ran in 0.03 seconds ===============</code> </pre> <br><p>  The <code>--collect-only</code> is useful for checking the correctness of the choice of other options that test selects before running tests.  We will use it again with <code>-k</code> to show how this works. </p><br><h3 id="-k-expression">  -k EXPRESSION </h3><br><p>  The <code>-k</code> allows you to use an expression to define test functions. </p><br><p>  Very powerful option!  It can be used as a shortcut to run a single test if the name is unique, or run a test suite that has a common prefix or suffix in their names.  Suppose you want to run the tests <code>test_asdict()</code> and <code>test_defaults()</code> .  You can check the filter with: <code>--collect-only</code> : </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code/ch1 $ pytest -k "asdict or defaults" --collect-only =================== test session starts =================== collected 6 items &lt;Module 'tasks/test_four.py'&gt; &lt;Function 'test_asdict'&gt; &lt;Module 'tasks/test_three.py'&gt; &lt;Function 'test_defaults'&gt; =================== 4 tests deselected ==================== ============== 4 deselected in 0.03 seconds ===============</code> </pre> <br><p>  Aha  This is similar to what we need.  Now you can run them by removing <code>--collect-only</code> : </p><br><pre> <code class="plaintext hljs">$ pytest -k "asdict or defaults" =================== test session starts =================== collected 6 items tasks/test_four.py . tasks/test_three.py . =================== 4 tests deselected ==================== ========= 2 passed, 4 deselected in 0.03 seconds ==========</code> </pre> <br><p>  Oops!  Just a point.  So they went through.  But were they the right tests?  One way to find out is to use <code>-v</code> or <code>--verbose</code> : </p><br><pre> <code class="plaintext hljs">$ pytest -v -k "asdict or defaults" =================== test session starts =================== collected 6 items tasks/test_four.py::test_asdict PASSED tasks/test_three.py::test_defaults PASSED =================== 4 tests deselected ==================== ========= 2 passed, 4 deselected in 0.02 seconds ==========</code> </pre> <br><p>  Aha  These were the right tests. </p><br><h3 id="-m-markexpr">  -m MARKEXPR </h3><br><p>  Markers are one of the best ways to mark a subset of test functions for co-launch.  As an example, one of the ways to run <code>test_replace()</code> and <code>test_member_access()</code> , even if they are in separate files, is to flag them.  You can use any marker name.  Say you want to use <code>run_these_please</code> .  Note the tests using the <code>@pytest.mark.run_these_please</code> decorator, like this: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pytest ... @pytest.mark.run_these_please <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">test_member_access</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> ...</code> </pre> <br><p>  Now the same for <code>test_replace()</code> .  Then you can run all the tests with the same marker using <code>pytest -m run_these_please</code> : </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code/ch1/tasks $ pytest -v -m run_these_please ================== test session starts =================== collected 4 items test_four.py::test_replace PASSED test_three.py::test_member_access PASSED =================== 2 tests deselected =================== ========= 2 passed, 2 deselected in 0.02 seconds =========</code> </pre> <br><p>  A marker expression does not have to be a single marker.  You can use options such as <code>-m "mark1 and mark2"</code> for tests with both markers, <code>-m "mark1 and not mark2"</code> for tests that are marked 1, but not 2, <code>-m "mark1 or mark2"</code> for tests with one of, etc., I will discuss markers in more detail in the Marking Verification Methods, on page 31. </p><br><h3 id="-x---exitfirst">  -x, --exitfirst </h3><br><p>  <em>Pytest's</em> normal behavior is to run all the tests it finds.  If the test function detects a failure <em>assert</em> or <em>exception</em> , the execution of this test is stopped, and the test fails.  And then <em>pytest</em> runs the next test.  For the most part, this is what you need.  However, especially when debugging a problem, it immediately interferes with the entire test session when the test is not correct.  This is what the <code>-x</code> option does.  Let's try it on the six tests we currently have: </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code/ch1 $ pytest -x ====================== test session starts ==================== collected 6 items test_one.py . test_two.py F ============================ FAILURES ========================= __________________________ test_failing _______________________ def test_failing(): &gt; assert (1, 2, 3) == (3, 2, 1) E assert (1, 2, 3) == (3, 2, 1) E At index 0 diff: 1 != 3 E Use -v to get the full diff test_two.py:2: AssertionError =============== 1 failed, 1 passed in 0.38 seconds ============</code> </pre><br><p>  At the top of the output you can see that all six tests (or ‚Äúitems‚Äù) were collected, and on the bottom line you see that one test failed and one passed, and <em>pytest</em> displayed the line ‚ÄúInterrupted‚Äù to let us know that it is stopped.  Without <code>-x</code> all six tests would be run.  Let's repeat it again without <code>-x</code> .  We also use <code>--tb=no</code> to disable the stack trace, since you have already seen it and you do not need to see it again: </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code/ch1 $ pytest --tb=no =================== test session starts =================== collected 6 items test_one.py . test_two.py F tasks/test_four.py .. tasks/test_three.py .. =========== 1 failed, 5 passed in 0.09 seconds ============</code> </pre> <br><p>   ,   <code>-x</code> , pytest    <em>test_two.py</em>    . </p><br><h3 id="--maxfailnum"> --maxfail=num </h3><br><p>  <code>-x</code>       .   ,      ,    ,   <code>--maxfail</code> ,  ,    .            ,      .     ,    <code>--maxfail = 2</code> ,    ,  <code>--maxfail = 1</code>    ,  <code>-x</code> : </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code/ch1 $ pytest --maxfail=2 --tb=no =================== test session starts =================== collected 6 items test_one.py . test_two.py F tasks/test_four.py .. tasks/test_three.py .. =========== 1 failed, 5 passed in 0.08 seconds ============ $ pytest --maxfail=1 --tb=no =================== test session starts =================== collected 6 items test_one.py . test_two.py F !!!!!!!!! Interrupted: stopping after 1 failures !!!!!!!!!! =========== 1 failed, 1 passed in 0.19 seconds ============</code> </pre> <br><p> E    <code>--tb=no</code> ,   . </p><br><h3 id="-s-and---capturemethod"> -s and --capture=method </h3><br><p>  <code>-s</code>    ‚Äî    ,     <em>stdout</em> ,           .     <code>--capture=no</code> .   ,        .      ,      ,     ,      .  <code>-s</code>  <code>--capture=no</code>    .         <code>print()</code> ,       . </p><br><p>  ,           , <code>-l/--showlocals</code> ,      ,    . </p><br><p>     <code>--capture=fd</code>  <code>--capture=sys</code> . ‚Äî  <code>--capture=sys</code>  <code>sys.stdout/stderr</code>  mem-.  <code>--capture=fd</code>    1  2   . </p><br><p>    <code>sys</code>  <code>fd</code>  . ,  ,        .    <code>-s</code> .    ,   <code>-s</code> ,      . </p><br><p>          ;    .   ,       ,      . </p><br><h3 id="-lf---last-failed"> -lf, --last-failed </h3><br><p>              .   <code>--lf</code>     : </p><br><p>  ,     <code>--tb</code> ,    ,         . </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code/ch1 $ pytest --lf =================== test session starts =================== run-last-failure: rerun last 1 failures collected 6 items test_two.py F ======================== FAILURES ========================= ______________________ test_failing _______________________ def test_failing(): &gt; assert (1, 2, 3) == (3, 2, 1) E assert (1, 2, 3) == (3, 2, 1) E At index 0 diff: 1 != 3 E Use -v to get the full diff test_two.py:2: AssertionError =================== 5 tests deselected ==================== ========= 1 failed, 5 deselected in 0.08 seconds ==========</code> </pre> <br><h3 id="ff---failed-first"> ‚Äìff, --failed-first </h3><br><p>  <code>--ff/--failed-first</code>     ,   <code>--last-failed</code> ,     ,    : </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code/ch1 $ pytest --ff --tb=no =================== test session starts =================== run-last-failure: rerun last 1 failures first collected 6 items test_two.py F test_one.py . tasks/test_four.py .. tasks/test_three.py .. =========== 1 failed, 5 passed in 0.09 seconds ============</code> </pre> <br><p>  <code>test_failing()</code>  <code>test\_two.py</code>   <code>test\_one.py</code> . ,  <code>test_failing()</code>     , <code>--ff</code>       </p><br><h3 id="-v---verbose"> -v, --verbose </h3><br><p>  <code>-v/--verbose</code>      .     ,       ,        . </p><br><p>      ,           <code>--ff</code>  <code>--tb=no</code> : </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code/ch1 $ pytest -v --ff --tb=no =================== test session starts =================== run-last-failure: rerun last 1 failures first collected 6 items test_two.py::test_failing FAILED test_one.py::test_passing PASSED tasks/test_four.py::test_asdict PASSED tasks/test_four.py::test_replace PASSED tasks/test_three.py::test_defaults PASSED tasks/test_three.py::test_member_access PASSED =========== 1 failed, 5 passed in 0.07 seconds ============</code> </pre> <br><p><img src="https://habrastorage.org/webt/bw/7f/_4/bw7f_4irv2astplpl5tobqp_xqs.png"></p><br><p>           FAILED   PASSED. </p><br><h3 id="-q---quiet"> -q, --quiet </h3><br><p>  <code>-q/--quiet</code>  <code>-v/--verbose</code> ;      .        <code>--tb=line</code> ,          . </p><br><p>  - <code>q</code> : </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code/ch1 $ pytest -q .F.... ======================== FAILURES ========================= ______________________ test_failing _______________________ def test_failing(): &gt; assert (1, 2, 3) == (3, 2, 1) E assert (1, 2, 3) == (3, 2, 1) E At index 0 diff: 1 != 3 E Full diff: E - (1, 2, 3) E ? ^ ^ E + (3, 2, 1) E ? ^ ^ test_two.py:2: AssertionError 1 failed, 5 passed in 0.08 seconds</code> </pre> <br><p>  <code>-q</code>    ,    .      <code>-q</code>     (  <code>--tb=no</code> ),    ,        . </p><br><h3 id="-l---showlocals"> -l, --showlocals </h3><br><p>    <code>-l/--showlocals</code>         <code>tracebacks</code>   . </p><br><p>            .     <code>test_replace()</code>   </p><br><pre> <code class="plaintext hljs">t_expected = Task('finish book', 'brian', True, 10)</code> </pre> <br><p>  on </p><br><pre> <code class="plaintext hljs">t_expected = Task('finish book', 'brian', True, 11)</code> </pre> <br><p> 10  11   .       .         <code>--l/--showlocals</code> : </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code/ch1 $ pytest -l tasks =================== test session starts =================== collected 4 items tasks/test_four.py .F tasks/test_three.py .. ======================== FAILURES ========================= ______________________ test_replace _______________________ @pytest.mark.run_these_please def test_replace(): """replace() should change passed in fields.""" t_before = Task('finish book', 'brian', False) t_after = t_before._replace(id=10, done=True) t_expected = Task('finish book', 'brian', True, 11) &gt; assert t_after == t_expected E AssertionError: assert Task(summary=...e=True, id=10) == Task(summary='...e=True, id=11) E At index 3 diff: 10 != 11 E Use -v to get the full diff t_after = Task(summary='finish book', owner='brian', done=True, id=10) t_before = Task(summary='finish book', owner='brian', done=False, id=None) t_expected = Task(summary='finish book', owner='brian', done=True, id=11) tasks\test_four.py:28: AssertionError =========== 1 failed, 3 passed in 0.08 seconds ============</code> </pre> <br><p><img src="https://habrastorage.org/webt/c1/f_/vk/c1f_vkcnpoeplpn9arlb-fgj85a.png"></p><br><p>   <code>t_after</code> , <code>t_before</code>  <code>t_expected</code>      ,       assert-. </p><br><h3 id="--tbstyle"> --tb=style </h3><br><p>  <code>--tb=style</code>       .    pytest        ,    ,    .  <em>tracebacks</em>    ,  ,   .    <code>--tb=style</code> . ,    ,  short, line  no. <code>short</code>    <em>assert</em>   <strong>E</strong>  ; <code>line</code>     ; <em>no</em>   . </p><br><p>     <code>test_replace()</code> ,    ,       . <code>--tb=no</code>    </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code/ch1 $ pytest --tb=no tasks =================== test session starts =================== collected 4 items tasks/test_four.py .F tasks/test_three.py .. =========== 1 failed, 3 passed in 0.04 seconds ============</code> </pre> <br><blockquote> --tb=line in many cases is enough to tell what's wrong. If you have a ton of failing tests, this option can help to show a pattern in the failures: </blockquote><p> <code>--tb=line</code>    ,  ,   .      ,        : </p><br><pre> <code class="plaintext hljs">$ pytest --tb=line tasks =================== test session starts =================== collected 4 items tasks/test_four.py .F tasks/test_three.py .. ======================== FAILURES ========================= /path/to/code/ch1/tasks/test_four.py:20: AssertionError: assert Task(summary=...e=True, id=10) == Task( summary='...e=True, id=11) =========== 1 failed, 3 passed in 0.05 seconds ============</code> </pre> <br><p>    verbose tracebacks <code>--tb=short</code> : </p><br><pre> <code class="plaintext hljs">$ pytest --tb=short tasks =================== test session starts =================== collected 4 items tasks/test_four.py .F tasks/test_three.py .. ======================== FAILURES ========================= ______________________ test_replace _______________________ tasks/test_four.py:20: in test_replace assert t_after == t_expected E AssertionError: assert Task(summary=...e=True, id=10) == Task( summary='...e=True, id=11) E At index 3 diff: 10 != 11 E Use -v to get the full diff =========== 1 failed, 3 passed in 0.04 seconds ============</code> </pre> <br><p>   ,   ,  . </p><br><p>     ,     . </p><br><p> pytest <code>--tb=long</code>       traceback. pytest <code>--tb=auto</code>         <em>tracebacks</em> ,      .    . pytest <code>--tb=native</code>     <em>traceback</em>   . </p><br><h3 id="--durationsn"> --durations=N </h3><br><p>  <code>--durations=N</code>  ,       .     ;    N  tests/setups/teardowns   .    <code>--durations=0</code> ,            . </p><br><p>      ,   <code>time.sleep(0.1)</code>    . , : </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code/ch1 $ pytest --durations=3 tasks ================= test session starts ================= collected 4 items tasks/test_four.py .. tasks/test_three.py .. ============== slowest 3 test durations =============== 0.10s call tasks/test_four.py::test_replace 0.00s setup tasks/test_three.py::test_defaults 0.00s teardown tasks/test_three.py::test_member_access ============== 4 passed in 0.13 seconds</code> </pre> <br><p>     <em>sleep</em>      ,      .        : call(), (setup)  (teardown).      ,                    ,  ,  ,  .       3, pytest Fixtures,  . 49. </p><br><h3 id="--version"> --version </h3><br><p>  <code>--version</code>   pytest  ,    : </p><br><pre> <code class="plaintext hljs">$ pytest --version This is pytest version 3.0.7, imported from /path/to/venv/lib/python3.5/site-packages/pytest.py</code> </pre> <br><p>    pytest   , pytest     site-packages   . </p><br><h3 id="-h---help"> -h, --help </h3><br><p>  <code>-h/--help</code>  ,   ,     pytest.     ,   stock- pytest,       ,      ,  . </p><br><p>  <code>-h</code> : </p><br><ul><li> : pytest [] [file_or_dir] [file_or_dir] [...] </li><li>      ,      </li><li>  ,   ini   ,        6, ,  . 113 </li><li>   ,      pytest (    6, ,  . 113) </li><li>   ,  pytest <code>--markers</code>      ,    2,   ,  . 23 </li><li>   ,  pytest <code>--fixtures</code>       ,    3,  pytest,  . 49 </li></ul><br><p>        : </p><br><pre> <code class="plaintext hljs">(shown according to specified file_or_dir or current dir if not specified)</code> </pre> <br><p>   ,  ,            .   ,         pytest    conftest.py,    - (hook functions),   ,     . </p><br><p>    pytest   conftest.py               .     conftest.py  ini,   pytest.ini   6 ¬´¬ª,   113. </p><br><h2 id="uprazhneniya">  Exercises </h2><br><ol><li><p>    ,  <code>python -m virtualenv</code>  <code>python -m venv</code> .    ,        ,    ,       ,   ,      .      ,      .   1,  ,  . 155,     . </p><br></li><li><p>        . </p><br><pre> <code class="plaintext hljs">- $ source venv/bin/activate - $ deactivate On Windows: - C:\Users\okken\sandbox&gt;venv\scripts\activate.bat - C:\Users\okken\sandbox&gt;deactivate</code> </pre> <br></li><li><p>  pytest    . .  2, pip,   159,     .    ,      pytest,       ,     . </p><br></li><li><p>    .    ,          .  pytest   . </p><br></li><li><p>   assert.    assert something == something_else;   , : </p><br><ul><li> assert 1 in [2, 3, 4] </li><li> assert a &lt; b </li><li> assert 'fizz' not in 'fizzbuzz' </li></ul><br></li></ol><br><h2 id="chto-dalshe">  What's next </h2><br><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In this chapter, we looked at where to get pytest and the various ways to start it. </font><font style="vertical-align: inherit;">However, we did not discuss what is included in the test functions. </font><font style="vertical-align: inherit;">In the next chapter, we will look at writing test functions, parameterizing them so that they are called with different data, and grouping tests into classes, modules, and packages.</font></font></p><br><p><img src="https://habrastorage.org/webt/jl/jn/bb/jljnbbjr-ejh473xy_eccsmknpk.png">  <a href="https://habr.com/ru/post/426699/">Back</a> <a href="https://habr.com/ru/post/448788/">Next</a> <img src="https://habrastorage.org/webt/rw/dy/-g/rwdy-grsvbpcetjttrmecdkxtlk.png"></p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/448782/">https://habr.com/ru/post/448782/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../448762/index.html">Riot on Picaba. Users massively go to Reddit</a></li>
<li><a href="../448766/index.html">Create a mono-repository with lerna & yarn workspaces</a></li>
<li><a href="../448768/index.html">Multi-level lighting control: fault tolerance solutions and products</a></li>
<li><a href="../448774/index.html">SQL to CSV using DBMS_SQL</a></li>
<li><a href="../448780/index.html">What gives software for recruiting money</a></li>
<li><a href="../448786/index.html">Python Testing with pytest. CHAPTER 3 pytest Fixtures</a></li>
<li><a href="../448788/index.html">Python Testing with pytest. Chapter 2, Writing Test Functions</a></li>
<li><a href="../448790/index.html">SpaceVIL - cross-platform GUI framework for development on .Net Core, .Net Standard and JVM</a></li>
<li><a href="../448792/index.html">Python Testing with pytest. Builtin Fixtures, Chapter 4</a></li>
<li><a href="../448794/index.html">Python Testing with pytest. Plugins, CHAPTER 5</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>