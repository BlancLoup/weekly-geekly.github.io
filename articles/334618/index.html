<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Traffic Sign Recognition with CNN: Image Preprocessing Tools</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! Continuing a series of materials from a graduate of our program Deep Learning , Cyril Danilyuk, on the use of convolutional neural networks ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Traffic Sign Recognition with CNN: Image Preprocessing Tools</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr!  Continuing a series of materials from a graduate of our program <a href="http://newprolab.com/ru/deeplearning/">Deep Learning</a> , Cyril Danilyuk, on the use of convolutional neural networks for pattern recognition - CNN (Convolutional Neural Networks) <br><br><h2>  Introduction </h2><br>  Over the past few years, the field of computer vision (CV) is experiencing, if not a rebirth, then a huge surge of interest in itself.  In many ways, this increase in popularity is associated with the evolution of neural network technologies.  For example, convolutional neural networks (convolutional neural networks or CNN) have selected a large chunk of tasks for generating features previously solved by the classical CV: HOG, SIFT, RANSAC, etc. methods. <br><br>  Mapping, image classification, building a route for drones and unmanned vehicles - many tasks related to feature generation, classification, image segmentation can be effectively solved using convolutional neural networks. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/web/ee0/30c/92e/ee030c92e1f64e3da92f0c90c6bb5cc1.png"><br>  <sub>MultiNet as an example of a neural network (three in one), which we will use in one of the following posts.</sub>  <sub><a href="https://arxiv.org/pdf/1612.07695.pdf">A source.</a></sub> <br><a name="habracut"></a><br>  It is assumed that the reader has a general understanding of the work of neural networks.  The network has a huge number of posts, courses and books on this topic.  For example: <br><br><ul><li>  <a href="http://www.deeplearningbook.org/contents/mlp.html">Chapter 6: Deep Feedforward Networks</a> - a chapter from the book Deep Learning by I.Goodfellow, Y.Bengio and A.Courville.  Highly recommend. </li><li>  <a href="http://cs231n.github.io/">CS231n Convolutional Neural Networks for Visual Recognition</a> is a popular course from Fei-Fei Li and Andrej Karpathy from Stanford.  The course contains excellent materials focusing on practice and design. </li><li>  <a href="https://www.youtube.com/watch%3Fv%3DPlhFWT7vAEw%26feature%3Dyoutu.be">Deep Learning</a> - course from Nando de Freitas from Oxford. </li><li>  <a href="https://www.udacity.com/course/intro-to-machine-learning--ud120">Intro to Machine Learning</a> - a free course from Udacity for beginners with an accessible presentation of the material, affects a large number of topics in machine learning. </li></ul><br>  Tip: To make sure that you know the basics of neural networks, <a href="http://cs231n.github.io/neural-networks-case-study/">write</a> your network from scratch and <a href="http://playground.tensorflow.org/">play</a> with it! <br><br>  Instead of repeating the basics, this series of articles focuses on several specific neural network architectures: STN (spatial transformer network), IDSIA (convolutional neural network for the classification of road signs), NVIDIA's neural network for end-to-end autopilot development and MultiNet for recognition and the classification of road markings and signs.  Let's get started! <br><br>  The topic of this article is to show several tools for image preprocessing.  The overall pipeline usually depends on a specific task, but I would like to dwell on the tools.  Neural networks are not the same magical black boxes that they like to present in the media: you can not just take and ‚Äúthrow‚Äù the data into the grid and wait for the magical results.  By the rule of shit in - shit out at best, you will get a score worse by a few points.  And, most likely, you just won't be able to train the network and no fancy techniques like normalization of batch or dropout will help you.  Thus, the work must begin with the data: their cleaning, normalization and normalization.  Additionally, it is worth thinking about expanding (data augmentation) of the original image dataset using affine transformations such as rotation, shifts, changing the scale of pictures: this will help reduce the likelihood of retraining and ensure better class invariance to transformations. <br><br><h2>  Tool 1: Visualization and Exploration Data Analysis </h2><br>  Within this and the next posts we will use <a href="http://benchmark.ini.rub.de/%3Fsection%3Dgtsrb%26subsection%3Ddataset">GTSRB</a> - dataset for the recognition of road signs in Germany.  Our task is to train a road sign classifier using tagged data from GTSRB.  In general, the best way to get an idea of ‚Äã‚Äãthe available data is to build a histogram of the distribution of the train, validation and / or test data sets: <br><br><img src="https://habrastorage.org/web/e50/e00/08e/e50e0008ead947c580b32f17ab4d96cc.gif"><br><br>  Basic information about our dataset: <br><br><pre><code class="python hljs">Number of training examples = <span class="hljs-number"><span class="hljs-number">34799</span></span> Number of validation examples = <span class="hljs-number"><span class="hljs-number">4410</span></span> Number of testing examples = <span class="hljs-number"><span class="hljs-number">12630</span></span> Image data shape = (<span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>) Number of classes = <span class="hljs-number"><span class="hljs-number">43</span></span></code> </pre> <br>  At this stage, <code>matplotlib</code> is your best friend.  Despite the fact that using only <code>pyplot</code> you can perfectly visualize the data, <code>matplotlib.gridspec</code> allows <code>matplotlib.gridspec</code> to merge 3 graphics together: <br><br><pre> <code class="python hljs">gs = gridspec.GridSpec(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, wspace=<span class="hljs-number"><span class="hljs-number">0.25</span></span>, hspace=<span class="hljs-number"><span class="hljs-number">0.1</span></span>) fig = plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">12</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>)) ax1, ax2, ax3 = [plt.subplot(gs[:, i]) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">3</span></span>)]</code> </pre> <br>  <code>Gridspec</code> very flexible.  For example, for each histogram you can set its width, as I did above.  <code>Gridspec</code> considers the axis of each histogram independently of the others, which allows you to create complicated graphs. <br><br><img src="https://habrastorage.org/web/3b7/003/b82/3b7003b8205e411d9cbd0ffdc167ca9f.png" alt="image" align="left"><br>  As a result, just one graph can tell a lot about our data set.  Below are 3 tasks that can be solved using a well-constructed schedule: <br><br><ul><li>  <strong>Image visualization.</strong>  The graphics immediately show a lot of too dark or too light images, so a sort of data normalization must be done to eliminate the brightness variation. </li><li>  <strong>Check the sample for imbalance.</strong>  If instances of a class prevail in the sample, it is necessary to use <a href="http://www.chioka.in/class-imbalance-problem/">undersampling or oversampling methods.</a> </li><li>  <strong>Check that the train, validation, and test sample distributions are similar.</strong>  This can be checked by looking at the histograms above, or using the <a href="https://en.wikipedia.org/wiki/Spearman%2527s_rank_correlation_coefficient">Spearman rank correlation coefficient.</a>  (via <code>scipy</code> ) </li></ul><br><h2>  Tool 2: IPython Parallel for scikit-image </h2><br>  In order to improve the convergence of the neural network, it is necessary to bring all the images to a single illumination by (as recommended in the <a href="http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf">LeCun</a> article on the recognition of road signs) converting their color gamut into grayscale.  This can be done both with the help of OpenCV, and with the help of the excellent Python library <code>scikit-image</code> , which can be easily installed with the help of pip (OpenCV also requires self-compilation with a bunch of dependencies).  Image contrast normalization will be performed using <a href="https://en.wikipedia.org/wiki/Adaptive_histogram_equalization">adaptive histogram normalization</a> (CLAHE, contrast limited adaptive histogram equalization): <br>  <code>skimage.exposure.equalize_adapthist</code> . <br><br>  I note that <code>skimage</code> processes images one after another, using only one processor core, which is obviously inefficient.  To parallelize image preprocessing, use the IPython Parallel library ( <code>ipyparallel</code> ).  One of the advantages of this library is its simplicity: you can implement parallelized CLAHE with just a few lines of code.  First, in the console (with <code>ipyparallel</code> installed), start the local ipyparallel cluster: <br><br><pre> <code class="python hljs">$ ipcluster start</code> </pre> <br><img src="https://habrastorage.org/web/2c4/9d6/36c/2c49d636ca0542779b7cba4a10be01ef.png"><br><br>  Our approach to parallelization is very simple: we divide the sample into batchy and process each batch independently of the others.  Once all the batch files are processed, we merge them back into one data set.  My implementation of CLAHE is shown below: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> skimage <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> exposure <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">grayscale_exposure_equalize</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(batch_x_y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""Processes a batch with images by grayscaling, normalization and histogram equalization. Args: batch_x_y: a single batch of data containing a numpy array of images and a list of corresponding labels. Returns: Numpy array of processed images and a list of labels (unchanged). """</span></span> x_sub, y_sub = batch_x_y[<span class="hljs-number"><span class="hljs-number">0</span></span>], batch_x_y[<span class="hljs-number"><span class="hljs-number">1</span></span>] x_processed_sub = numpy.zeros(x_sub.shape[:<span class="hljs-number"><span class="hljs-number">-1</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(x_sub)): <span class="hljs-comment"><span class="hljs-comment"># Grayscale img_gray = numpy.dot(x_sub[x][...,:3], [0.299, 0.587, 0.114]) # Normalization img_gray_norm = img_gray / (img_gray.max() + 1) # CLAHE. num_bins will be initialized in ipyparallel client img_gray_norm = exposure.equalize_adapthist(img_gray_norm, nbins=num_bins) x_processed_sub[x,...] = img_gray_norm return (x_processed_sub, y_sub)</span></span></code> </pre> <br>  Now that the transformation itself is ready, we will write a code that applies it to each batch from the training set: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> multiprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ipyparallel <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> ipp <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess_equalize</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X, y, bins=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">256</span></span></span></span><span class="hljs-function"><span class="hljs-params">, cpu=multiprocessing.cpu_count</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">()</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" A simplified version of a function which manages multiprocessing logic. This function always grayscales input images, though it can be generalized to apply any arbitrary function to batches. Args: X: numpy array of all images in dataset. y: a list of corresponding labels. bins: the amount of bins to be used in histogram equalization. cpu: the number of cpu cores to use. Default: use all. Returns: Numpy array of processed images and a list of labels. """</span></span> rc = ipp.Client() <span class="hljs-comment"><span class="hljs-comment"># Use a DirectView object to broadcast imports to all engines with rc[:].sync_imports(): import numpy from skimage import exposure, transform, color # Use a DirectView object to set up the amount of bins on all engines rc[:]['num_bins'] = bins X_processed = np.zeros(X.shape[:-1]) y_processed = np.zeros(y.shape) # Number of batches is equal to cpu count batches_x = np.array_split(X, cpu) batches_y = np.array_split(y, cpu) batches_x_y = zip(batches_x, batches_y) # Applying our function of choice to each batch with a DirectView method preprocessed_subs = rc[:].map(grayscale_exposure_equalize, batches_x_y).get_dict() # Combining the output batches into a single dataset cnt = 0 for _,v in preprocessed_subs.items(): x_, y_ = v[0], v[1] X_processed[cnt:cnt+len(x_)] = x_ y_processed[cnt:cnt+len(y_)] = y_ cnt += len(x_) return X_processed.reshape(X_processed.shape + (1,)), y_processed</span></span></code> </pre> <br>  Finally, apply the written function to the training set: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># X_train: numpy array of (34799, 32, 32, 3) shape # y_train: a list of (34799,) shape X_tr, y_tr = preprocess_equalize(X_train, y_train, bins=128)</span></span></code> </pre> <br>  As a result, we use not one, but all the processor cores (32 in my case) and get a significant increase in performance.  Sample received images: <br><br><img src="https://habrastorage.org/web/0b4/c12/843/0b4c1284381d48a5a17c119e0dccebd6.png"><br>  <sub>The result of the normalization of images and the transfer of their color range in grayscale</sub> <br><br><img src="https://habrastorage.org/web/873/f47/e00/873f47e001b543499ac554d7f2575138.png"><br>  <sub>Normalization of distribution for RGB images (I used another function for rc [:]. Map)</sub> <br><br>  Now the whole process of data preprocessing takes several tens of seconds, so we can test different values ‚Äã‚Äãof the number of intervals <code>num_bins</code> in order to visualize them and select the most suitable: <br><br><img src="https://habrastorage.org/web/20e/ac8/b35/20eac8b350ad4cf9b6975612b0d33a27.png"><br>  <sub>num_bins: 8, 32, 128, 256, 512</sub> <br><br>  Choosing a larger number of <code>num_bins</code> increases the contrast of the images, while at the same time highlighting their background, which makes the data noisy.  Different values ‚Äã‚Äãof <code>num_bins</code> can also be used to augment the dataset contrast by contrast so that the neural network does not retrain due to the background of the images. <br><br>  Finally, use ipython magic <code>%store</code> to save the results for future reference: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Same images, multiple bins (contrast augmentation) %store X_tr_8 %store y_tr_8 # ... %store X_tr_512 %store y_tr_512</span></span></code> </pre> <br><h2>  Tool 3: Online data augmentation </h2><br>  It is not a secret to anyone that adding new diverse data to a sample reduces the likelihood of retraining a neural network.  In our case, we can construct artificial images by transforming the existing images with the help of rotation, specular reflection and affine transformations.  Despite the fact that we can conduct this process for the entire sample, save the results and then use them, the more elegant way will be to create new images on the fly (online) so that you can quickly adjust the data augmentation parameters. <br><br>  To begin with, we denote all the planned transformations using <code>numpy</code> and <code>skimage</code> : <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> skimage <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> transform <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> skimage.transform <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> warp, AffineTransform <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">rotate_90_deg</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X)</span></span></span><span class="hljs-function">:</span></span> X_aug = np.zeros_like(X) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i,img <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(X): X_aug[i] = transform.rotate(img, <span class="hljs-number"><span class="hljs-number">270.0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X_aug <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">rotate_180_deg</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X)</span></span></span><span class="hljs-function">:</span></span> X_aug = np.zeros_like(X) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i,img <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(X): X_aug[i] = transform.rotate(img, <span class="hljs-number"><span class="hljs-number">180.0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X_aug <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">rotate_270_deg</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X)</span></span></span><span class="hljs-function">:</span></span> X_aug = np.zeros_like(X) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i,img <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(X): X_aug[i] = transform.rotate(img, <span class="hljs-number"><span class="hljs-number">90.0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X_aug <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">rotate_up_to_20_deg</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X)</span></span></span><span class="hljs-function">:</span></span> X_aug = np.zeros_like(X) delta = <span class="hljs-number"><span class="hljs-number">20.</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i,img <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(X): X_aug[i] = transform.rotate(img, random.uniform(-delta, delta), mode=<span class="hljs-string"><span class="hljs-string">'edge'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X_aug <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">flip_vert</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X)</span></span></span><span class="hljs-function">:</span></span> X_aug = deepcopy(X) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X_aug[:, :, ::<span class="hljs-number"><span class="hljs-number">-1</span></span>, :] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">flip_horiz</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X)</span></span></span><span class="hljs-function">:</span></span> X_aug = deepcopy(X) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X_aug[:, ::<span class="hljs-number"><span class="hljs-number">-1</span></span>, :, :] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">affine_transform</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X, shear_angle=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.0</span></span></span></span><span class="hljs-function"><span class="hljs-params">, scale_margins=[</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.8</span></span></span></span><span class="hljs-function"><span class="hljs-params">, </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1.5</span></span></span></span><span class="hljs-function"><span class="hljs-params">], p=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1.0</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""This function allows applying shear and scale transformations with the specified magnitude and probability p. Args: X: numpy array of images. shear_angle: maximum shear angle in counter-clockwise direction as radians. scale_margins: minimum and maximum margins to be used in scaling. p: a fraction of images to be augmented. """</span></span> X_aug = deepcopy(X) shear = shear_angle * np.random.rand() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> np.random.choice(len(X_aug), int(len(X_aug) * p), replace=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>): _scale = random.uniform(scale_margins[<span class="hljs-number"><span class="hljs-number">0</span></span>], scale_margins[<span class="hljs-number"><span class="hljs-number">1</span></span>]) X_aug[i] = warp(X_aug[i], AffineTransform(scale=(_scale, _scale), shear=shear), mode=<span class="hljs-string"><span class="hljs-string">'edge'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X_aug</code> </pre> <br>  Scaling and <code>rotate_up_to_20_deg</code> turns <code>rotate_up_to_20_deg</code> increase the size of the sample, while maintaining the images belonging to the original classes.  Reflections (flips) and rotations by 90, 180, 270 degrees can, on the contrary, change the meaning of the sign.  To track such transitions, we will create a list of possible transformations for each road sign and the classes into which they will be converted (below is an example of a part of such a list): <br><table><tbody><tr><th>  label_class </th><th>  label_name </th><th>  rotate_90_deg </th><th>  rotate_180_deg </th><th>  rotate_270_deg </th><th>  flip_horiz </th><th>  flip_vert </th></tr><tr><td>  13 </td><td>  Yield </td><td></td><td></td><td></td><td></td><td>  13 </td></tr><tr><td>  14 </td><td>  Stop </td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>  15 </td><td>  No vehicles </td><td>  15 </td><td>  15 </td><td>  15 </td><td>  15 </td><td>  15 </td></tr><tr><td>  sixteen </td><td>  Vehicles over <br>  3.5 ton <br>  prohibited </td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>  17 </td><td>  No entry </td><td></td><td>  17 </td><td></td><td>  17 </td><td>  17 </td></tr></tbody></table>  <sub>Part of <a href="https://gist.github.com/dnkirill/8aab1c5dbe0471795cdf93729a572d49">the conversion table.</a></sub>  <sub>The values ‚Äã‚Äãin the cells indicate the class number that will receive this image after transformation.</sub>  <sub>Empty cells mean that this conversion is not available for this class.</sub> <br><br>  Note that the column headings correspond to the names of the transforming functions defined earlier, so that during processing you can add transformations: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-comment"><span class="hljs-comment"># Generate an augmented dataset using a transform table augmentation_table = pd.read_csv('augmentation_table.csv', index_col='label_class') augmentation_table.drop('label_name', axis=1, inplace=True) augmentation_table.dropna(axis=0, how='all', inplace=True) # Collect all global functions in global namespace namespace = __import__(__name__) def apply_augmentation(X, how=None): """Apply an augmentation function specified in `how` (string) to a numpy array X. Args: X: numpy array with images. how: a string with a function name to be applied to X, should return the same-shaped numpy array as in X. Returns: Augmented X dataset. """ assert augmentation_table.get(how) is not None augmentator = getattr(namespace, how) return augmentator(X)</span></span></code> </pre> <br>  Now we can build a pipeline that applies all the available functions (transformations) listed in <code>augmentation_table.csv</code> to all classes: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">flips_rotations_augmentation</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X, y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""A pipeline for applying augmentation functions listed in `augmentation_table` to a numpy array with images X. """</span></span> <span class="hljs-comment"><span class="hljs-comment"># Initializing empty arrays to accumulate intermediate results of augmentation X_out, y_out = np.empty([0] + list(X.shape[1:]), dtype=np.float32), np.empty([0]) # Cycling through all label classes and applying all available transformations for in_label in augmentation_table.index.values: available_augmentations = dict(augmentation_table.ix[in_label].dropna(axis=0)) images = X[y==in_label] # Augment images and their labels for kind, out_label in available_augmentations.items(): X_out = np.vstack([X_out, apply_augmentation(images, how=kind)]) y_out = np.hstack([y_out, [out_label] * len(images)]) # And stack with initial dataset X_out = np.vstack([X_out, X]) y_out = np.hstack([y_out, y]) # Random rotation is explicitly included in this function's body X_out_rotated = rotate_up_to_20_deg(X) y_out_rotated = deepcopy(y) X_out = np.vstack([X_out, X_out_rotated]) y_out = np.hstack([y_out, y_out_rotated]) return X_out, y_out</span></span></code> </pre> <br>  Fine!  Now we have 2 ready data augmentation functions: <br><br><ul><li>  <code>affine_transform</code> : customizable affine transformations without rotation (the name I chose was not very good, because the rotation is one of the affine transformations). </li><li>  <code>flips_rotations_augmentation</code> : random rotations and transformations based on <code>augmentation_table.csv</code> , changing image classes. </li></ul><br>  The final step is to create a batch generator: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">augmented_batch_generator</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X, y, batch_size, rotations=True, affine=True, shear_angle=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.0</span></span></span></span><span class="hljs-function"><span class="hljs-params">, scale_margins=[</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.8</span></span></span></span><span class="hljs-function"><span class="hljs-params">, </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1.5</span></span></span></span><span class="hljs-function"><span class="hljs-params">], p=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.35</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""Augmented batch generator. Splits the dataset into batches and augments each batch independently. Args: X: numpy array with images. y: list of labels. batch_size: the size of the output batch. rotations: whether to apply `flips_rotations_augmentation` function to dataset. affine: whether to apply `affine_transform` function to dataset. shear_angle: `shear_angle` argument for `affine_transform` function. scale_margins: `scale_margins` argument for `affine_transform` function. p: `p` argument for `affine_transform` function. """</span></span> X_aug, y_aug = shuffle(X, y) <span class="hljs-comment"><span class="hljs-comment"># Batch generation for offset in range(0, X_aug.shape[0], batch_size): end = offset + batch_size batch_x, batch_y = X_aug[offset:end,...], y_aug[offset:end] # Batch augmentation if affine is True: batch_x = affine_transform(batch_x, shear_angle=shear_angle, scale_margins=scale_margins, p=p) if rotations is True: batch_x, batch_y = flips_rotations_augmentation(batch_x, batch_y) yield batch_x, batch_y</span></span></code> </pre> <br>  Combining datasets with different numbers of <code>num_bins</code> in CLAHE into one big train, we will feed it into the resulting generator.  Now we have two types of augmentation: by contrast and using affine transformations, which are applied to the batch on the fly: <br><br><img src="https://habrastorage.org/web/0ef/79d/ed2/0ef79ded2efe4d01b95bb4a0bd6b6865.gif"><br>  <sub>Generated with augmented_batch_generator images</sub> <br><br>  <strong>Note:</strong> augmentation is needed for train set.  We also process the test set, but do not augment it. <br><br>  Let's check that we inadvertently did not violate the distribution of classes on the extended train compared to the original dataset: <br><br><img src="https://habrastorage.org/web/780/176/ea7/780176ea797545bfa31bb0bcf2185deb.gif"><br>  <sub>Left: histogram of data distribution from augmented batch generator.</sub>  <sub>Right: the original train.</sub>  <sub>As you can see, the values ‚Äã‚Äãare different, but the distributions are similar.</sub> <br><br><h2>  Transition to neural networks </h2><br>  After the data preprocessing is completed, all the generators are ready and the data is ready for analysis, we can proceed to the training.  We will use a double convolutional neural network: the STN (spatial transformer network) accepts pre-processed batchy of images from the generator and focuses on the road signs, while the IDSIA neural network recognizes the road sign on the images received from the STN.  The next post will be devoted to these neural networks, their training, quality analysis and demo version of their work.  Follow new posts! <br><br><img src="https://habrastorage.org/web/deb/f72/564/debf725645d44839ba57f77031554210.png"><br>  <sub>Left: The original pre-processed image.</sub>  <sub>Right: a transformed STN image that takes as input the IDSIA for classification.</sub> </div><p>Source: <a href="https://habr.com/ru/post/334618/">https://habr.com/ru/post/334618/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../334604/index.html">Hack Sleep. Repeat</a></li>
<li><a href="../334608/index.html">In the second quarter, a 40% increase in the number of attacked devices was recorded.</a></li>
<li><a href="../334610/index.html">When will there be enough IP addresses for everyone?</a></li>
<li><a href="../334612/index.html">Translation of Hans Buvald‚Äôs article ‚ÄúBasic Test Design Principles‚Äù</a></li>
<li><a href="../334616/index.html">6 tips on launching a start-up designer anything</a></li>
<li><a href="../334620/index.html">Red Team: teamwork during penetration testing</a></li>
<li><a href="../334626/index.html">How much does it cost to translate Habr?</a></li>
<li><a href="../334630/index.html">How to create a business proposal, based on the result of R & D and technology</a></li>
<li><a href="../334632/index.html">Comparison of prices for traffic from cloud providers</a></li>
<li><a href="../334636/index.html">Introducing dependencies through fields is bad practice</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>