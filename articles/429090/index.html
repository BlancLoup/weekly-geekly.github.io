<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Reinforcement Training</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! 

 We have opened a new stream for the ‚ÄúMachine learning‚Äù course, so wait for the articles related to this discipline, so to speak, in the near...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Reinforcement Training</h1><div class="post__text post__text-html js-mediator-article">  Hello! <br><br>  We have opened a new stream for the <a href="https://otus.pw/4a40/">‚ÄúMachine learning‚Äù</a> course, so wait for the articles related to this discipline, so to speak, in the near future.  Well, of course, open seminars.  And now let's look at reinforcement learning. <br><br>  Reinforcement training is an important type of machine learning where the agent learns to behave in the environment, performing actions and seeing results. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In recent years, we have seen a lot of success in this fascinating field of research.  For example, <a href="https://deepmind.com/research/dqn/">DeepMind and Deep Q Learning Architecture</a> in 2014, <a href="https://deepmind.com/research/alphago/">victory over the Go champion with AlphaGo</a> in 2016, <a href="https://blog.openai.com/openai-baselines-ppo/">OpenAI and PPO</a> in 2017, among others. <br><br><img src="https://habrastorage.org/webt/_q/bo/5c/_qbo5cmhnkdgpvhwcipzifybbeg.png"><a name="habracut"></a><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>DeepMind DQN</i> <br><br>  In this series of articles we will focus on studying the various architectures used today to solve the problem of learning with reinforcement.  These include Q-learning, Deep Q-learning, Policy Gradients, Actor Critic and PPO. <br><br>  In this article you will learn: <br><br><ul><li>  What is reinforcement learning, and why rewards are central </li><li>  Three reinforcement training approaches </li><li>  What does ‚Äúdeep‚Äù mean in deep learning with reinforcement </li></ul><br>  It is very important to master these aspects before plunging into the implementation of reinforcement training agents. <br><br>  The idea of ‚Äã‚Äãreinforcement training is that the agent learns from the environment, interacting with her and receiving rewards for taking actions. <br><br><img src="https://habrastorage.org/webt/iu/8r/w1/iu8rw1wpn1ubnc5zkssp6azkous.png"><br><br>  Learning through interaction with the environment comes from our natural experience.  Imagine that you are a child in the living room.  You see a fireplace and walk up to it. <br><br><img src="https://habrastorage.org/webt/si/fb/7j/sifb7jkorawb7spj6iivlojdgly.png"><br><br>  Nearby is warm, you feel good (positive reward +1).  You understand that fire is a positive thing. <br><br><img src="https://habrastorage.org/webt/nw/cs/an/nwcsanbxf1aauksjabgtkpwrd6a.png"><br><br>  But then you try to touch the fire.  Oh!  He burned his hand (negative reward -1).  You have just realized that fire is positive when you are at a sufficient distance, because it produces heat.  But if you get close to him - burn yourself. <br><br>  That is how people learn through interaction.  Reinforcement learning is simply a computational approach to learning through action. <br><br>  <b>Reinforcement learning process</b> <b><br></b> <br><img src="https://habrastorage.org/webt/lz/0b/it/lz0bitmkpzmwkrvwyfbetdj5ho8.png"><br><br>  As an example, imagine an agent is learning to play Super Mario Bros.  The reinforcement learning process (Reinforcement Learning - RL) can be modeled as a loop that works as follows: <br><br><ul><li>  The agent receives the S0 state from the environment (in our case, we receive the first frame of the game (state) from Super Mario Bros (environment)) </li><li>  Based on this state of S0, the agent takes action A0 (the agent will move to the right) </li><li>  The environment is moving to a new state S1 (new frame) </li><li>  The environment gives some reward to R1 agent (not dead: +1) </li></ul><br>  This RL cycle produces a sequence of <b>states, actions, and rewards.</b> <br>  The agent's goal is to maximize the expected accumulated reward. <br><br>  <b>The central idea of ‚Äã‚Äãthe reward hypothesis</b> <br><br>  Why is the agent's goal to maximize the expected accrued remuneration?  Well, reinforcement learning is based on the idea of ‚Äã‚Äãa reward hypothesis.  All goals can be described by maximizing the expected accumulated reward. <br><br>  <b>Therefore, in reinforcement training, in order to achieve the best behavior, we need to maximize the expected accumulated reward.</b> <br><br>  The accumulated reward at each time step t can be written as: <br><br><img src="https://habrastorage.org/webt/zn/by/di/znbydiiglnylw5d_bsro8mj96f4.gif"><br><br>  This is equivalent to: <br><br><img src="https://habrastorage.org/webt/ti/ku/p8/tikup8qeypsldwfqd1yqttboonm.png"><br><br>  However, in reality, we cannot simply add such rewards.  Earlier rewards (at the beginning of the game) are more likely because they are more predictable than rewards in the longer term. <br><br><img src="https://habrastorage.org/webt/q3/pj/pg/q3pjpgo4x-blfbjxkrd2qtlhefw.png"><br><br>  Suppose your agent is a small mouse, and your opponent is a cat.  Your goal is to eat the maximum amount of cheese before the cat eats you.  As we see in the diagram, the mouse is more likely to eat the cheese next to it than the cheese near the cat (the closer we are to it, the more dangerous it is). <br><br>  As a result, a cat's reward, even if it is more (more cheese), will be reduced.  We are not sure we can eat it.  To lower the reward, we do the following: <br><br><ul><li>  We determine the discount rate, called gamma.  It must be between 0 and 1. </li><li>  The more gamma, the less discount.  This means that the learning agent is more concerned with long-term rewards. </li><li>  On the other hand, the smaller the gamma, the greater the discount.  This means that the priority is short-term remuneration (the nearest cheese). </li></ul><br>  The accumulated expected remuneration, taking into account discounting, is as follows: <br><br><img src="https://habrastorage.org/webt/1o/j-/i9/1oj-i95d4zwuj9wgt1vjpp5-qbc.png"><br><br>  Roughly speaking, each reward will be reduced by a gamut of time.  As the time step increases, the cat becomes closer to us, so a future reward is becoming less and less likely. <br><br>  <b>Episodic or continuous tasks</b> <br><br>  A task is an instance of a learning problem with reinforcement.  We can have two types of tasks: episodic and continuous. <br><br>  <b>Episodic task</b> <br><br>  In this case, we have a starting point and an ending point <b>(the terminal state).</b>  <b>This creates an episode</b> : a list of states, actions, rewards, and new states. <br>  Take for example Super Mario Bros: the episode begins with the launch of the new Mario and ends when you are killed or reach the end of the level. <br><br><img src="https://habrastorage.org/webt/w8/jr/dk/w8jrdkdy31kbnbkpg5g1pbn6luw.png"><br>  <i>Start a new episode</i> <br><br>  <b>Continuous tasks</b> <br><br>  <b>These are tasks that go on forever (without a terminal state)</b> .  In this case, the agent must learn to choose the best actions and simultaneously interact with the environment. <br><br>  For example, an agent that performs automated stock trading.  For this task there is no starting point and terminal state.  <b>The agent continues to work until we decide to stop it.</b> <br><br><img src="https://habrastorage.org/webt/uk/ih/ul/ukihulbmt8ffshfb4pkhssvwvsk.jpeg"><br><br>  <b>Monte Carlo vs time difference method</b> <br><br>  There are two ways to learn: <br><br><ul><li>  Collecting rewards at the end of the episode, and then calculating the maximum expected future reward - the Monte Carlo approach </li><li>  Evaluation of remuneration at every step - a temporary difference </li></ul><br>  <b>Monte Carlo</b> <br><br>  When the episode ends (the agent reaches the ‚Äúterminal state‚Äù), the agent looks at the total accumulated reward to see how well he has done.  In the Monte Carlo approach, the reward is obtained only at the end of the game. <br><br>  Then we start a new game with added knowledge.  <b>The agent makes the best decisions with each iteration.</b> <br><br><img src="https://habrastorage.org/webt/m0/gh/l7/m0ghl7rlxmsvdmtdgtrjhd8kpcc.png"><br><br>  Let's give an example: <br><br><img src="https://habrastorage.org/webt/q3/pj/pg/q3pjpgo4x-blfbjxkrd2qtlhefw.png"><br><br>  If we take the labyrinth as the environment: <br><br><ul><li>  We always start with the same starting point. </li><li>  We stop the episode if the cat eats us or we move&gt; 20 steps. </li><li>  At the end of the episode we have a list of states, actions, rewards and new states. </li><li>  The agent summarizes the total Gt reward (to see how well he coped). </li><li>  It then updates V (st) in accordance with the above formula. </li><li>  Then a new game is launched with new knowledge. </li></ul><br>  By running more and more episodes, the <b>agent will learn to play better and better.</b> <br><br>  <b>Temporary differences: learning at every time step</b> <br><br>  The Temporal Difference Learning (TD) method will not wait for the end of the episode to update the maximum possible reward.  It will update V depending on the experience gained. <br><br>  This method is called TD (0) or <b>step-by-step TD (updates the utility function after any single step).</b> <br><br><img src="https://habrastorage.org/webt/pw/03/xt/pw03xtwh65lmrhsqffyz1nr70lu.png"><br><br>  TD methods only expect the next <b>time step to update values.</b>  At time t + 1 <b>, a TD target is formed using the reward Rt + 1 and the current estimate V (St + 1).</b> <br><br>  A TD goal is an estimate of the expected: in fact, you update the previous estimate of V (St) to the goal within one step. <br><br>  <b>Compromise Exploration / Operation</b> <br><br>  Before considering the various strategies for solving problems of learning with reinforcement, we must consider another very important topic: a compromise between exploration and exploitation. <br><br><ul><li>  Intelligence finds more environmental information. </li><li>  Exploitation uses known information to maximize reward. </li></ul><br>  Remember that the goal of our RL agent is to maximize the expected accumulated reward.  However, we can fall into the common trap. <br><br><img src="https://habrastorage.org/webt/vt/xr/s7/vtxrs70s3g0ryq_bzn0tenuw4ke.png"><br><br>  In this game, our mouse can have an infinite number of small pieces of cheese (+1 each).  But at the top of the maze there is a giant piece of cheese (+1000).  However, if we focus only on remuneration, our agent will never reach a gigantic piece.  Instead, he will use only the closest source of rewards, even if this source is small (exploitation).  But if our agent looks at the situation a little, he will be able to find a great reward. <br><br>  This is what we call the trade-off between exploration and exploitation.  We must define a rule that will help deal with this compromise.  In future articles you will learn different ways to do this. <br><br>  <b>Three reinforcement training approaches</b> <br><br>  Now that we have identified the basic elements of reinforcement learning, let's move on to three approaches to solving reinforcement learning objectives: cost-based, policy-based, and model-based. <br><br>  <b>Based on cost</b> <br><br>  In cost-based RL, the goal is to optimize the utility function V (s). <br>  A utility function is a function that informs us of the maximum expected reward that an agent will receive in each state. <br><br>  The value of each state is the total amount of remuneration that an agent can expect to accumulate in the future, starting with this state. <br><br><img src="https://habrastorage.org/webt/xf/am/qe/xfamqeqhethtlspy9rxfj_srzua.png"><br><br>  The agent will use this utility function to decide which state to select at each step.  The agent selects the state with the highest value. <br><br><img src="https://habrastorage.org/webt/xf/am/qe/xfamqeqhethtlspy9rxfj_srzua.png"><br><br>  In the maze example, at each step, we take the largest value: -7, then -6, then -5 (and so on) to achieve the goal. <br><br>  <b>Based on policy</b> <br><br>  In RL based on policy, we want to directly optimize the policy function œÄ (s) without using the utility function.  The policy is what determines the behavior of the agent at a given time. <br><br><img src="https://habrastorage.org/webt/8o/eh/rn/8oehrnzytpqnnbiryj-s_ohfns4.png"><br>  <i>action = policy (state)</i> <br>  We study the function of politics.  This allows us to compare each state with the best appropriate action. <br><br>  There are two types of policies: <br><br><ul><li>  Deterministic: politics in a given state will always return the same action. </li><li>  Stochastic: displays the probability of distribution by action. </li></ul><br><img src="https://habrastorage.org/webt/u-/mc/kt/u-mckt0bhgwx8-ziyv-rwi1e_u8.png"><br><br><img src="https://habrastorage.org/webt/9n/b3/zf/9nb3zfpdt6_ukndkwsuza8t9iva.png"><br><br>  As you can see, the policy directly indicates the best action for each step. <br><br>  <b>Based on the model</b> <br><br>  In RL, based on the model, we model the environment.  This means that we create a model of environmental behavior.  The problem is that each environment will need a different view of the model.  That is why we will not focus much on this type of training in the following articles. <br><br>  <b>Introducing Deep Learning with Reinforcement</b> <br><br>  Deep Reinforcement Learning (Deep Reinforcement Learning) introduces deep neural networks for solving reinforcement learning problems - hence the name ‚Äúdeep‚Äù. <br>  For example, in the next article we will work on Q-Learning (classical reinforcement learning) and Deep Q-Learning. <br><br>  You will see the difference in that in the first approach we use the traditional algorithm to create a table Q, which helps us to find out what action to take for each state. <br><br>  In the second approach, we will use a neural network (for approximation of remuneration based on state: the value of q). <br><br><img src="https://habrastorage.org/webt/bf/yn/5u/bfyn5uyg0bpq-wd-ab4-tgzj3b8.png"><br>  <i>The scheme, inspired by the training manual Q from Udacity</i> <i><br></i> <br><br>  That's all.  As always, we are waiting for your comments or questions here or you can ask the course instructor <a href="https://otus.pw/As3H/">Arthur Kadurin</a> in his <a href="https://otus.pw/XHJk/">open lesson</a> on networking. </div><p>Source: <a href="https://habr.com/ru/post/429090/">https://habr.com/ru/post/429090/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../429080/index.html">Microsoft breaks away from Amazon in revenue from cloud services</a></li>
<li><a href="../429082/index.html">Thailand without stereotypes</a></li>
<li><a href="../429084/index.html">The second life of electric oven "Kharkov"</a></li>
<li><a href="../429086/index.html">Backup beer party</a></li>
<li><a href="../429088/index.html">Run GraphQL queries using OdataToEntity</a></li>
<li><a href="../429092/index.html">Why stealth in space is still there</a></li>
<li><a href="../429094/index.html">Directional sound: technology that can replace headphones ‚Äî how it works</a></li>
<li><a href="../429096/index.html">Antiquities: ZX Spectrum, tape programs and high definition</a></li>
<li><a href="../429102/index.html">Sales of electric cars in Canada</a></li>
<li><a href="../429104/index.html">High kind data</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>