<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Parsim Wikipedia for NLP tasks in 4 teams</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The essence 


 It turns out for this purpose it is enough to run just such a set of commands: 


git clone https://github.com/attardi/wikiextractor.g...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Parsim Wikipedia for NLP tasks in 4 teams</h1><div class="post__text post__text-html js-mediator-article"><h1 id="sut">  <strong>The essence</strong> </h1><br><p>  It turns out for this purpose it is enough to run just such a set of commands: </p><br><pre><code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">git</span></span> clone https://github.com/attardi/wikiextractor.git cd wikiextractor wget http://dumps.wikimedia.org/ruwiki/latest/ruwiki-latest-pages-articles.xml.bz2 python3 WikiExtractor.py -o ../data/wiki/ --<span class="hljs-literal"><span class="hljs-literal">no</span></span>-templates --processes <span class="hljs-number"><span class="hljs-number">8</span></span> ../data/ruwiki-latest-pages-articles.xml.bz2</code> </pre> <br><p>  and then a little polish <a href="https://gist.github.com/snakers4/e0b0e68904db65671ca979639b337f7b"><strong>script</strong></a> for post-processing </p><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">python3</span></span> process_wikipedia.py</code> </pre> <br><p>  The result is a finished <code>.csv</code> file with your package. </p><a name="habracut"></a><br><p>  It is clear that: </p><br><ul><li>  <code>http://dumps.wikimedia.org/ruwiki/latest/ruwiki-latest-pages-articles.xml.bz2</code> can be changed to your desired language, more details here <a href="https://dumps.wikimedia.org/backup-index.html"><strong>[4]</strong></a> ; </li><li>  All information about the parameters of <code>wikiextractor</code> can be found in the manual (it seems even the official dock was not updated, unlike mana); </li></ul><br><p>  Script with post-processing converts wiki files to the following table: </p><br><table><thead><tr><th>  idx </th><th>  article_uuid </th><th>  sentence </th><th>  cleaned sentence </th><th>  cleaned sentence length </th></tr></thead><tbody><tr><td>  0 </td><td>  74fb822b-54bb-4bfb-95ef-4eac9465c7d7 </td><td>  Jean I de Chatillon (Comte de Pentevre) Jean I de Ch ... </td><td>  Jean i de Chatillon comte de pentevre Jean i de sha ... </td><td>  38 </td></tr><tr><td>  one </td><td>  74fb822b-54bb-4bfb-95ef-4eac9465c7d7 </td><td>  Was guarded by Robert de Vera, Count Oh ... </td><td>  was guarded by Robert de Vera, Count of Oxfor ... </td><td>  18 </td></tr><tr><td>  2 </td><td>  74fb822b-54bb-4bfb-95ef-4eac9465c7d7 </td><td>  However, this was opposed by Henry de Gromon, gr ... </td><td>  however, this was opposed by henry de grom ... </td><td>  14 </td></tr><tr><td>  3 </td><td>  74fb822b-54bb-4bfb-95ef-4eac9465c7d7 </td><td>  The king offered him another important person for his wife ... </td><td>  the king offered his wife another important branch ... </td><td>  48 </td></tr><tr><td>  four </td><td>  74fb822b-54bb-4bfb-95ef-4eac9465c7d7 </td><td>  Jean was released and returned to France at 138 ... </td><td>  Jean liberated returned france year wedding m ... </td><td>  52 </td></tr></tbody></table><br><p>  article_uuid is a pseudo-unique key, the order of proposals on the idea should be preserved after such pre-processing. </p><br><h1 id="zachem">  <strong>What for</strong> </h1><br><p>  Perhaps, at the moment, the development of ML-tools has reached such a level [8] that literally a couple of days are enough to build a working NLP model / pipeline.  Problems arise only in the absence of reliable datasets / ready embeds / ready language models.  The purpose of this article is to alleviate your pain a little, by showing that to handle the whole of Wikipedia (according to the idea of ‚Äã‚Äãthe most popular corps for training embeddingings in NLP), a couple of hours will be enough.  After all, if a couple of days are enough to build the simplest model, why spend a lot more time getting data for this model? </p><br><h1 id="princip-raboty-skripta">  <strong>The principle of the script</strong> </h1><br><p>  <code>wikiExtractor</code> saves <code>wikiExtractor</code> articles as text, separated by <code>&lt;doc&gt;</code> blocks.  Actually, the script is based on the following logic: </p><br><ul><li>  Take a list of all the files on the output; </li><li>  We divide files into articles; </li><li>  Remove all remaining HTML tags and special characters; </li><li>  With the help of <code>nltk.sent_tokenize</code> divide into sentences; </li><li>  So that the code does not grow to enormous size and remains readable, we assign a uuid to each article; </li></ul><br><p>  As a preprocessing of the text is simple (you can easily cut it to yourself) </p><br><ul><li>  Remove non-alphabetic characters; </li><li>  Delete stop words; </li></ul><br><h1 id="dataset-est-chto-teper">  <strong>Dataset is, what now?</strong> </h1><br><h2 id="osnovnoe-primenenie">  <strong>Main application</strong> </h2><br><p>  Most often in practice in NLP one has to face the task of constructing embeddings. </p><br><p>  To solve it, one of the following tools is usually used: </p><br><ul><li>  Ready vectors / word embeddings [6]; </li><li>  Internal states of CNN, trained on such tasks as how to determine false sentences / language modeling / classification [7]; </li><li>  The combination of the above methods; </li></ul><br><p>  In addition, it has been shown many times [9] that as a good baseline for embeddingd sentences, one can also take simply averaged (with a couple of minor details that we now omit) word vectors. </p><br><h2 id="drugie-varianty-ispolzovaniya">  <strong>Other uses</strong> </h2><br><ul><li>  We use random Wiki sentences as negative examples for triplet loss; </li><li>  We teach encoders for sentences using the definition of fake phrases [10]; </li></ul><br><h1 id="nemnogo-grafikov-dlya-russkoy-viki">  <strong>Some charts for Russian wiki</strong> </h1><br><p>  <strong>The distribution of the length of sentences for the Russian Wikipedia</strong> </p><br><p>  <strong>No logarithms (X values ‚Äã‚Äãare limited to 20)</strong> </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/2ae/6c2/81f/2ae6c281f87f81c49abdc2e72aba4206.png"></p><br><p>  <strong>Decimal logarithms</strong> </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/d31/091/a57/d31091a573d3a39b60158cc148fb00fe.png"></p><br><h1 id="ssylki">  <strong>Links</strong> </h1><br><ol><li>  Fast-text word <a href="https://fasttext.cc/docs/en/pretrained-vectors.html">vectors</a> trained on the wiki; </li><li>  Fast-text and Word2Vec <a href="http://rusvectores.org/ru/models/">models</a> for the Russian language; </li><li>  Awesome wiki extractor <a href="">library</a> for python; </li><li>  Official <a href="https://dumps.wikimedia.org/backup-index.html">page</a> with links for wiki; </li><li>  Our <a href="https://gist.github.com/snakers4/e0b0e68904db65671ca979639b337f7b">script</a> for post-processing; </li><li>  The main articles about word embeddings: <a href="http://arxiv.org/abs/1607.04606">Word2Vec</a> , <a href="http://arxiv.org/abs/1712.09405">Fast-Text</a> , <a href="http://arxiv.org/abs/1802.06893">tuning</a> ; </li><li>  Several current SOTA approaches: <br><ol><li>  <a href="http://arxiv.org/abs/1705.02364">InferSent</a> ; </li><li>  Generative <a href="https://blog.openai.com/language-unsupervised/">pre-training</a> CNN; </li><li>  <a href="http://arxiv.org/abs/1801.06146">ULMFiT</a> ; </li><li>  Contextual approaches for <a href="http://arxiv.org/abs/1802.05365">representing</a> words (Elmo); </li></ol></li><li>  Imagenet moment in <a href="https://thegradient.pub/nlp-imagenet/">NLP</a> ? </li><li>  Baselines for embeddingings offers <a href="https://openreview.net/pdf%3Fid%3DSyK00v5xx">1</a> , <a href="http://nlp.town/blog/sentence-similarity/">2</a> , <a href="https://arxiv.org/abs/1806.06259">3</a> , <a href="http://www.offconvex.org/2018/09/18/alacarte/">4</a> ; </li><li>  Definition of <a href="https://arxiv.org/abs/1808.03840">false phrases</a> for the sentence encoder; </li></ol></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/425507/">https://habr.com/ru/post/425507/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../425497/index.html">Tutu PHP Meetup # 2: Live Event Broadcast</a></li>
<li><a href="../425499/index.html">HyperX Impact DDR4 - SO-DIMM, which could! Or why in a laptop 64 GB of memory with a frequency of 3200 MHz?</a></li>
<li><a href="../425501/index.html">A / V tests on Android from A to Z</a></li>
<li><a href="../425503/index.html">Cassandra Sink for Spark Structured Streaming</a></li>
<li><a href="../425505/index.html">Analysis of the Linux kernel boot process</a></li>
<li><a href="../425511/index.html">Unobvious features of Rotativa for generating PDF in an ASP.NET MVC application</a></li>
<li><a href="../425513/index.html">Deputies seriously undertook the taxation of miners</a></li>
<li><a href="../425515/index.html">Apple blocks the possibility of independent repair of new MacBook models</a></li>
<li><a href="../425517/index.html">How Yandex created a global precipitation forecast using radars and satellites</a></li>
<li><a href="../425519/index.html">Our problems with space are the result of making the wrong decisions.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>