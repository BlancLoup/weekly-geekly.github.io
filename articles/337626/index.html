<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>DevOps with Kubernetes and VSTS. Part 1: Local History</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Last time, I often talk about containers, Docker and Kubernetes. Against this background, colleagues increasingly began to ask about, where are the Mi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>DevOps with Kubernetes and VSTS. Part 1: Local History</h1><div class="post__text post__text-html js-mediator-article">  Last time, I often talk about containers, Docker and Kubernetes.  Against this background, colleagues increasingly began to ask about, where are the Microsoft technologists here?  To explain, I found several materials, including this set of a couple of articles from Colin Dembovsky.  They have everything: Docker, Kubernetes and our technology.  I think that for readers of Habra this should also be interesting.  So, meet, translation of the first part. <br><br><img src="https://habrastorage.org/web/e1f/553/79a/e1f55379ab224485bc0f1b9b12fbc601.jpg"><br><br>  If you read my blog, you know that I am a fan of containers in general and <a href="http://docker.com/">Docker</a> in particular.  When was the last time you put software on bare metal?  Maybe only on a laptop, but even then the odds are slim.  Virtualization has fundamentally changed our attitude to data center resources, significantly increasing their density and efficiency of use.  The next step to increase the density of steel containers, only VMs are located on physical servers, and containers - in the VMs themselves.  Very soon, most of us will not work not only at the server level, but even at the VM level, all workloads will move into containers.  But this is in perspective. <br><a name="habracut"></a><br><h2>  A series of articles "We are talking about containers": </h2><br>  1. <a href="https://habrahabr.ru/company/microsoft/blog/334682/">Containers for rapid deployment</a> . <br>  2. <a href="https://habrahabr.ru/company/microsoft/blog/337626/">DevOps with Kubernetes and VSTS.</a>  <a href="https://habrahabr.ru/company/microsoft/blog/337626/">Part 1: Local history.</a> <br>  3. <a href="https://habrahabr.ru/company/microsoft/blog/337708/">DevOps with Kubernetes and VSTS.</a>  <a href="https://habrahabr.ru/company/microsoft/blog/337708/">Part 2: Cloud history.</a> <br>  4. A <a href="https://habrahabr.ru/company/microsoft/blog/338686/">node with infinite capacity for Kubernetes.</a> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Despite all the advantages of containers in terms of packaging applications, many still do not understand how to run containers in a production environment.  Installing one container will be an interesting and inspiring experience for any developer, but how about scaling containers or combining them into a cluster?  How will you watch your containers?  How to detect and fix failures?  Here we smoothly turn to the problem of container orchestration. <br><br>  In this article, we will look at local development approaches using Kubernetes and minikube.  Part 2 focuses on creating CI / CD pipelines for the Kubernetes cluster in Azure. <br><br><h2>  Battlefield - orchestration </h2><br>  There are three popular container orchestration systems - <a href="https://mesosphere.com/">Mesos</a> , <a href="https://kubernetes.io/">Kubernetes</a> and <a href="https://docs.docker.com/engine/swarm/">Docker Swarm Mode</a> .  I will not encourage you to stand under someone's flag (at least for now), conceptually they are all alike.  They all use the concept of ‚Äúconfiguration as code‚Äù to deploy multiple containers to multiple nodes.  Kubernetes offers a number of features that, in my opinion, will be a real breakthrough in the field of DevOps: <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configmap/">configuration maps (ConfigMaps)</a> , <a href="https://kubernetes.io/docs/concepts/configuration/secret/">secrets (Secrets)</a> and <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/">namespaces (namespaces)</a> . <br><br>  Without going into details, I would say that namespaces allow you to create different logical environments in a single cluster.  As an example, I‚Äôll give you the DEV namespace, where you can run small copies of your PROD environment for testing purposes.  Namespaces are also used to implement multi-tenancy or different security contexts.  Configuration maps (ConfigMaps) and secrets (Secrets) allow you to store configuration outside of containers, that is, you can run one image in different contexts without embedding specific code for a specific environment into the images themselves. <br><br><h2>  Kubernetes Workflow (Workflow) and Pipeline (Pipeline) </h2><br>  In this article, I will demonstrate a Kubernetes development approach.  In the first part, we will look at the development workflow, and in the second, the DevOps pipeline.  Fortunately, thanks to the <a href="https://github.com/kubernetes/minikube">MiniKube</a> ( <a href="https://github.com/kubernetes/minikube">Kubernetes</a> single node cluster running on VM) we can work with a full cluster on a laptop!  This means that you have the benefits of cluster technology (like ConfigMaps) without connecting to a production cluster. <br><br>  So, consider the workflow of the developer.  It will be something like: <br><br><ol><li>  Develop the code. </li><li>  Create an image based on a Dockerfile file or a batch of files generated using the docker-compose command. </li><li>  Start the service in the MiniKube (run the containers from your images). </li></ol><br>  As practice shows, thanks to Visual Studio 2017 (and (or) VS Code), Docker and MiniKube will not meet pitfalls on this path. <br><br>  Then you go to the DevOps pipeline, starting at its inception.  Based on your source files and Dockerfile files, images are created that are registered in the private container registry.  Next, you need to transfer the configuration to the Kubernetes cluster in order to launch / deploy new images.  Thanks to Azure and VSTS, we will create a DevOps pipeline in just a couple of clicks!  But this is the topic of the <a href="http://colinsalmcorner.com/post/devops-with-kubernetes-and-vsts-part-2">second part of</a> our article, now we are studying the workflow of the developer. <br><br><h2>  Preparing the development environment </h2><br>  For the demonstration, I will use Windows, but in Mac or Linux, the settings are similar.  To deploy a local development environment, you need to install: <br><br><ol><li>  <a href="https://docs.docker.com/engine/installation/">Docker</a> </li><li>  <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">Kubectl</a> </li><li>  <a href="https://github.com/kubernetes/minikube">Minikube</a> </li></ol><br>  You can use the links and perform the installation.  In the process, I ran into a small problem when launching MiniKube on Hyper-V ‚Äî by default, the MiniKube start command, which creates a VM for MiniKube, connects to the first Hyper-V virtual network found.  I had several networks, and the MiniKube connected to the internal one, which led to a crash.  I created a new virtual network called minikube in the Hyper-V console and made sure that it is an external network.  To create the MiniKube VM, I used the following command: <br><br><pre><code class="plaintext hljs">c: cd \ minikube start --vm-driver hyperv --hyperv-virtual-switch minikube</code> </pre> <br>  I had to execute the cd command in order to go to the root directory of c: \, without this, MiniKube could not create a VM. <br><br>  The external network is connected to my Wi-Fi access point.  This means that when I connect to a new network, the VM minikube gets a new IP address.  Instead of updating kubeconfig every time, I simply added the host string to my hosts file (on Windows, this is c: \ windows \ system32 \ drivers \ etc \ hosts): kubernetes, where IP is the IP address of the VM minikube obtained using commands minikube ip.  To update kubeconfig use the following command: <br><br><pre> <code class="plaintext hljs">kubectl config set-cluster minikube --server=https://kubernetes:8443 --certificate-authority=c:/users/&lt;user&gt;/.minikube/ca.crt</code> </pre> <br>  where &lt;user&gt; is your username;  thus, cert points to the ca.crt file created in your .minikube directory. <br><br>  Now, when you connect to a new network, you simply update the IP address in the hosts file, and the kubectl command will still work.  A certificate is generated for a node named kubernetes, so use this name. <br><br>  If everything works fine, you will get a concise answer to the kubectl get nodes command: <br><br><pre> <code class="plaintext hljs">PS:\&gt; kubectl get nodes NAME STATUS AGE VERSION minikube Ready 11m v1.6.4</code> </pre><br>  To launch the Kubernetes UI, simply enter the minikube dashboard command.  The following window will open in the browser: <br><br><img src="https://habrastorage.org/web/c97/ccc/7c7/c97ccc7c777041d78cffa9d87877194b.png"><br><br>  Finally, to ‚Äúreuse‚Äù the minikube docker context, run the following command: <br><br><pre> <code class="plaintext hljs">minikube docker-env | Invoke-Expression</code> </pre> <br>  This ensures that the minikube docker socket is shared.  By running the docker ps command, you will receive information about several working containers, these are the basic Kubernetes system containers.  This also means that you can create images here that can be launched by a minikube cluster. <br><br>  You now have a single node cluster.  You can start developing! <br><br><h2>  Go to the code </h2><br>  Not so long ago, I posted a blog article <a href="http://colinsalmcorner.com/post/aurelia-azure-and-vsts">Developing an Aurelia Project using Azure and VSTS</a> .  And since I already had a couple of ready-made .NET Core sites, I decided to try running them in the Kubernetes cluster.  Clone <a href="https://github.com/colindembovsky/AzureAureliaDemo">this repository</a> and check the docker branch.  I added a few files to the repository to provide the ability to create Docker images and specify the Kubernetes configuration.  Let's see what it looks like. <br><br>  The docker-compose.yml file defines a composite application of two images: api and frontend: <br><br><pre> <code class="plaintext hljs">version: '2' services: api: image: api build: context: ./API dockerfile: Dockerfile frontend: image: frontend build: context: ./frontend dockerfile: Dockerfile</code> </pre> <br>  The Dockerfile file for each service is as simple as possible: launching from an ASP.NET Core 1.1 image, copying application files to a container, opening port 80 and launching dotnet app.dll (frontend.dll and api.dll for each site) as an entry point for each container: <br><br><pre> <code class="plaintext hljs">FROM microsoft/aspnetcore:1.1 ARG source WORKDIR /app EXPOSE 80 COPY ${source:-obj/Docker/publish} . ENTRYPOINT ["dotnet", "API.dll"]</code> </pre> <br>  To prepare for the creation of images, run the dotnet restore, build and publish commands, the projects will be built and published.  Now you can proceed to creating images.  If there are ready-made images, we can configure the Kubernetes service to launch them in our minikube cluster. <br><br><h2>  Creating images </h2><br>  The easiest way to create images is to use Visual Studio.  Set up the docker-compose project as your start-up and run it.  Images will be created.  If you are not working with Visual Studio, create images by executing the following commands from the repository root directory: <br><br><pre> <code class="plaintext hljs">cd API dotnet restore dotnet build dotnet publish -o obj/Docker/publish cd ../frontend dotnet restore dotnet build dotnet publish -o obj/Docker/publish cd .. docker-compose -f docker-compose.yml build</code> </pre> <br>  Now, after running the docker images command, you will see the minikube containers, as well as the images for the frontend and api: <br><br><img src="https://habrastorage.org/web/f33/94b/555/f3394b55519440759739c06789bf5c70.png"><br><br><h2>  Service declaration - configuration as code </h2><br>  Now we can specify which services to run in the cluster.  In my opinion, one of the advantages of Kubernetes is that you have to declare your environment instead of running the script.  Such a declarative model is much better imperative, and now it is spreading more and more thanks to Chef, Puppet and PowerShell DSC.  Kubernetes allows us to specify the services to be started, as well as define the methods for their deployment.  Various Kubernetes objects can be defined using a simple yaml file.  We announce two services: api and frontend.  Server services (backend) are usually not available outside the cluster, but in this case our demo code is a one-page application, so the api service must be accessible from the outside. <br><br>  The list of services will change very rarely, these are services available in the cluster.  However, the base containers (in Kubernetes they are called hearths), of which the service consists, will change.  They change when upgrading, as well as when scaling.  To manage the containers that make up the service, use the Deployment construction.  Since the service and the deployment are quite closely related, I put them in one file.  That is, we have a file for the frontend service / deployment (k8s / app-demo-frontend-minikube.yml) and a file for the api service / deployment (k8s / app-demo-backend-minikube.yml).  If you see fit, you can put service and deployment definitions in separate files.  Examine the contents of the app-demo-backend.yml file: <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: Service metadata: name: demo-backend-service labels: app: demo spec: selector: app: demo tier: backend ports: - protocol: TCP port: 80 nodePort: 30081 type: NodePort --- apiVersion: apps/v1beta1 kind: Deployment metadata: name: demo-backend-deployment spec: replicas: 2 template: metadata: labels: app: demo tier: backend spec: containers: - name: backend image: api ports: - containerPort: 80 imagePullPolicy: Never</code> </pre> <br>  <b>Notes:</b> <br><br><ul><li>  Lines 1‚Äì15 announce the service. </li><li>  Line 4 indicates the name of the service. </li><li>  Lines 8‚Äì10 describe the selector for this service.  Any one with app = demo and tier = frontend tags will be used to load balance this service.  The service will know how to redirect traffic to its base modules associated with requests for this service that fall into the cluster.  This makes it easier to add, delete, and update containers, since all we need to do is change the selector.  The service will receive a static IP address, and its base modules will be dynamic addresses, which will change at different stages of the module life cycle.  Nevertheless, this process is completely transparent to us, because we will simply send requests to the service, and everything should work. </li><li>  Line 14 - we want this service to be available through port 30081 (associated with port 80 on the sub, as indicated in line 13). </li><li>  Line 15 - NodePort indicates that we want Kubernetes to provide the service with a port on the same IP address that the cluster uses.  For ‚Äúreal‚Äù clusters (on cloud service provider resources, such as Azure), we would change this setting to get an IP address from a cloud host. </li><li>  In lines 17‚Äì34, the Deployment construction is declared, which will ensure the availability of containers (pods) for this service.  If it fails, Deployment automatically starts a new one.  This design ensures the normal operation of the service. </li><li>  Line 22 indicates that we constantly need two instances of the container for this service. </li><li>  Lines 26 and 27 are important; they must match the selector labels from the service. </li><li>  Line 30 shows the name of the container in the pod (in our case there is only one container in this pod, as we planned). </li><li>  Line 31 shows the name of the image being launched - this is the name we specified in the docker-compose file for the backend image. </li><li>  In line 33 we open port 80 on this container for the cluster. </li><li>  Line 34 indicates that we do not want Kubernetes to retrieve the image, since we are going to create images in the context of minikube docker.  In the production cluster, we plan to specify other policies so that the cluster can receive updated images from the container registry (we will see this in the second part). </li></ul><br>  The definition of customer service (frontend service) is almost the same, except that it will take a little tinkering to configure.  Let's see what it looks like. <br><br><pre> <code class="plaintext hljs">spec: containers: - name: frontend image: frontend ports: - containerPort: 80 env: - name: "ASPNETCORE_ENVIRONMENT" value: "Production" volumeMounts: - name: config-volume mountPath: /app/wwwroot/config/ imagePullPolicy: Never volumes: - name: config-volume configMap: name: demo-app-frontend-config</code> </pre> <br>  <b>Notes:</b> <br><br><ul><li>  Line 30: the name of the container in the pod. </li><li>  Line 31: Specify the image name for this container, it must match the name in the docker-compose file. </li><li>  Lines 34‚Äì36: An example of specifying environment variables for a service. </li><li>  Lines 37‚Äì39: link to mount the volume (shown below) to connect the configuration file, from which Kubernetes finds out where the file should be mounted on the container‚Äôs file system.  In this case, Kubernetes will mount the volume named config-volume in the following path in the container: / app / wwwroot / config. </li><li>  Lines 41‚Äì44: they indicate the volume, in our case, the configMap volume for configuration (more on this later).  Here we ask Kubernetes to create a volume named config-volume (referenced by the volumeMount container) and use the data from the configMap named demo-app-frontend-config for the volume. </li></ul><br><h2>  Configuration management </h2><br>  Now we have several container images and we can run them in the minikube.  But before you begin, let's discuss the configuration.  If you have heard my speeches or read my blog, then you know that I am one of the main advocates of the ‚Äúcreate once, deploy many times‚Äù concept.  This is the basic principle of proper DevOps.  In the case of Kubernetes and containers, the situation is similar. <br><br>  However, for this you will need to place the configuration outside of your compiled code, that is, you need mechanisms such as configuration files.  If you are deploying to IIS or Azure App, you can simply use the web.config file (for DotNet Core this will be appsettings.json) and specify different values ‚Äã‚Äãfor different environments.  But how to deal with containers?  All application code is in a container, so you cannot use different versions of the configuration file, otherwise you will need different versions of the container itself, that is, the principle of a one-time creation will be violated. <br><br>  Fortunately, we can use pluggable volumes (container concept) in combination with secrets and / or configMaps (Kubernetes concept).  We can specify configMaps (essentially, key-value pairs) or secrets (masked or hidden key-value pairs) in Kubernetes, and then simply mount them by connecting volumes in containers.  This is really a powerful tool, since the definition of the hearth remains the same, but if we have another configMap, we get a different configuration!  We will see how this works when we deploy to a cloud cluster and use namespaces to isolate the development environment and production environment. <br><br>  configMaps can also be specified using the "Configuration as Code" method.  Here is the configuration of our configMap: <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: ConfigMap metadata: name: demo-app-frontend-config labels: app: demo tier: frontend data: config.json: | { "api": { "baseUri": "http://kubernetes:30081/api" } }</code> </pre> <br>  <b>Notes:</b> <br><br><ul><li>  Line 2: we specify that the definition is configMap. </li><li>  Line 4: the name of this card. </li><li>  Line 9: we indicate that this map uses the format file format, and set the file name - config.json. </li><li>  Lines 10-14: the contents of the configuration file. </li></ul><br><h4>  Retreat: the problem of symbolic links to static files </h4><br>  I really ran into one problem when mounting the configuration file using configMaps: inside the container, the path /app/www/config/config.json ends with a symbolic link.  I saw the idea of ‚Äã‚Äãusing configMap in a container in an excellent article by Anthony Chu, where he describes how to include the application.json file for use in the Startup.cs file. <br><br>  Obviously, he did not encounter the problem of a symbolic link in the Startup file.  However, for my demo client application, I create a configuration file that is used by the SPA application, and since it is on the client side, the configuration file must be provided from the DotNet Core application, simply in the form of html or js.  No problems.  We already have a call to UseStaticFiles in Startup, so it should just provide the file, right?  Unfortunately, this is not the case.  Most likely, it will send only the first few bytes from this file. <br><br>  I spent a couple of days to figure it out.  There is a special topic on Github, <a href="https://github.com/aspnet/StaticFiles/issues/202">you can read</a> if you're interested.  In short, the length of a symbolic link is not the length of the file itself, but the length of the path to the file.  StaticFiles middleware reads FileInfo.Length bytes when a file is requested, but since this length is not equal to the full length of the file, only the first few bytes are returned.  I created a <a href="">FileProvider</a> tool to work around this issue. <br><br><h2>  Running Images in Kubernetes </h2><br>  To run the newly created services in the minikube, we can simply use kubectl to apply the configurations.  Here is a list of commands (highlighted lines): <br><br><pre> <code class="plaintext hljs">PS:\&gt; cd k8s PS:\&gt; kubectl apply -f .\app-demo-frontend-config.yml configmap "demo-app-frontend-config" created PS:\&gt; kubectl apply -f .\app-demo-backend-minikube.yml service "demo-backend-service" created deployment "demo-backend-deployment" created PS:\&gt; kubectl apply -f .\app-demo-frontend-minikube.yml service "demo-frontend-service" created deployment "demo-frontend-deployment" created</code> </pre> <br>  Now we have a service!  You can open the dashboard with the minikube dashboard command and make sure that the status of the services is green: <br><br><img src="https://habrastorage.org/web/c66/c43/bfa/c66c43bfa6b04b01adbb484a2c39897d.png"><br><br>  To connect to customer service, enter <a href="http://kubernetes:30080/">http: // kubernetes: 30080</a> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/c08/b16/7f0/c08b167f0ce74dfc8f11069bf72207ef.png" width="400"></div><br>  The list (value1 and value2) are the values ‚Äã‚Äãreturned by the API service, that is, the client was able to successfully connect to the minikube server service! <br><br><h2>  Container Update </h2><br>  After updating your code, you will have to recreate the container (s).  After updating the configuration, run the kubectl apply command again to update the configMap.  Then, since we do not need high availability in the development environment, we can simply delete the running subnets and let the replication service restart them, but this time with the updated configuration and / or code.  Of course, in a production environment such liberties are not allowed, and in the second part I will show you how to perform sequential updates when we work with CI / CD in the Kubernetes cluster. <br><br>  But in the development environment, I get a list of pods, delete them all, and then watch as Kubernetes magically restarts containers (with new identifiers).  As a result, I get updated containers. <br><br><pre> <code class="plaintext hljs">PS:&gt; kubectl get pods NAME READY STATUS RESTARTS AGE demo-backend-deployment-951716883-fhf90 1/1 Running 0 28m demo-backend-deployment-951716883-pw1r2 1/1 Running 0 28m demo-frontend-deployment-477968527-bfzhv 1/1 Running 0 14s demo-frontend-deployment-477968527-q4f9l 1/1 Running 0 24s PS:&gt; kubectl delete pods demo-backend-deployment-951716883-fhf90 demo -backend-deployment-951716883-pw1r2 demo-frontend-deployment-477968527-bfzhv demo-frontend-deployment-477968527-q4f9l pod "demo-backend-deployment-951716883-fhf90" deleted pod "demo-backend-deployment-951716883-pw1r2" deleted pod "demo-frontend-deployment-477968527-bfzhv" deleted pod "demo-frontend-deployment-477968527-q4f9l" deleted PS:&gt; kubectl get pods NAME READY STATUS RESTARTS AGE demo-backend-deployment-951716883-4dsl4 1/1 Running 0 3m demo-backend-deployment-951716883-n6z4f 1/1 Running 0 3m demo-frontend-deployment-477968527-j2scj 1/1 Running 0 3m demo-frontend-deployment-477968527-wh8x0 1/1 Running 0 3m</code> </pre> <br>  Please note that the bog IDs change, as the scoops themselves are already different.  If we switch to the client side, we will see the updated code. <br><br><h2>  Conclusion </h2><br>  I‚Äôm really impressed with Kubernetes‚Äôs capabilities and how this platform is promoting the concept of ‚Äúinfrastructure as a code‚Äù.  You can simply deploy a cluster locally on your laptop using minikube and start developing in an environment as close as possible to production, and this is always a good idea.  You can use secrets and maps configMaps, similar to those used in production containers.  In general, this is an excellent approach to development, allowing you to adhere to best practices from the very beginning. <br><br><hr><br>  Ps. We thank Konstantin Kichinsky ( <a href="https://t.me/quantumquintum">Quantum Quintum</a> ) for the illustration of this article. </div><p>Source: <a href="https://habr.com/ru/post/337626/">https://habr.com/ru/post/337626/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../337616/index.html">PHP Digest number 116 - the latest news, materials and tools (August 27 - September 10, 2017)</a></li>
<li><a href="../337618/index.html">Using D-Bus system functions in Sailfish OS</a></li>
<li><a href="../337620/index.html">How, or rather, where we will work in 10 years</a></li>
<li><a href="../337622/index.html">STM32 without HAL and SPL</a></li>
<li><a href="../337624/index.html">"Man" of art: is artificial intelligence capable of creating?</a></li>
<li><a href="../337628/index.html">Pygest # 17. Releases, articles, interesting projects from the world of Python [August 29, 2017 - September 11, 2017]</a></li>
<li><a href="../337630/index.html">AMD is preparing to press Intel in the market of server solutions</a></li>
<li><a href="../337632/index.html">PostgreSQL: materialized views and FDW</a></li>
<li><a href="../337634/index.html">How we cheat customers. Software as a Service</a></li>
<li><a href="../337636/index.html">We have nothing to hide - everything is fair in the HPE 3PAR Data Reduction Guarantee</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>