<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Introduction to competitive networks</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello. In this article, I begin a series of stories about adversarial networks. As in the previous article, I prepared an appropriate docker-image in ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Introduction to competitive networks</h1><div class="post__text post__text-html js-mediator-article"><p>  Hello.  In this article, I begin a series of stories about adversarial networks.  As in the <a href="https://habr.com/company/otus/blog/358096/">previous article,</a> I prepared an appropriate docker-image in which everything is already ready to reproduce what is written here below.  I will not copy all the code from the example here, only its main parts, therefore, for convenience, I advise you to have it side by side for easier understanding.  The docker container is available <a href="https://hub.docker.com/r/spoilt333/adversarial/tags/">here</a> , and the laptop, utils.py and dockerfile <a href="https://github.com/spoilt333/adversarial">here</a> . </p><br><p>  Despite the fact that the competitive network framework was proposed by Ian Goodfellow in his already famous work <a href="">Generative Adversarial Networks, the</a> key idea came to him from his work on domain adaptation, so we will start the discussion of competitive networks with this topic. </p><br><p>  Imagine that you have two sources of data about similar sets of objects.  For example, it can be medical records of different socio-demographic groups (men / women, adults / children, Asians / Europeans ...).  Typical blood tests of representatives of different groups will differ, so a model that predicts, say, the risk of cardiovascular diseases (CVD) trained on representatives of one sample cannot be applied to representatives of another sample. </p><a name="habracut"></a><br><p>  A typical solution to this problem would be to add a characteristic identifying a sample to the model input, but, unfortunately, this approach has many drawbacks: </p><br><ol><li>  Unbalanced sample - more Asians than Europeans </li><li>  Different statistics - children are less likely to suffer from CVD than adults </li><li>  Insufficient marking of one of the samples - men born in the 1960s died in Afghanistan, therefore there is less data on CVD depending on the region of birth than for women. </li><li>  The data have a different set of features - blood tests of humans and mice, etc. </li></ol><br><p>  All these reasons can greatly complicate the learning process of the model.  But maybe you should not bother?  We will train one model for each sample and calm down.  It turns out - it is worth.  If you can level the difference in statistics from different training samples, then, in fact, you can make one sample larger than each of the original ones.  And if you have not two data sources, but much more? </p><br><p>  I will not talk about how to solve the problem of adapting domains to the ‚Äúpre-neural network‚Äù era, but will immediately show the basic architecture. </p><br><img src="https://habrastorage.org/webt/6c/16/_v/6c16_vu4qbugrkug8cdsqkhp5ni.png" alt="Network architecture from the article"><br><br>  In 2014, our compatriot Yaroslav Ganin, in collaboration with Viktor Lempitsky, published a very important article ‚Äú <a href="">Unsupervised Domain Adaptation by Backpropagation</a> ‚Äù (domain adaptation without a teacher using back-propagation error).  This article demonstrates how to transfer a classification model from one data source to another, without using labels for a second source.  The presented model consisted of 3 subnets: feature extractor (E), label predictor (P) and domain classifier ¬© interconnected as in the figure. <br><p>  A pair of networks E + P is an ordinary classifier, cut somewhere in the middle.  The layer where it is cut is called the feature layer.  Network C receives input from this layer and tries to guess from which source the example came.  The task of the E network is to extract such indications from the data so that, on the one hand, P can correctly guess the example label, and on the other hand, C cannot determine its source. </p><br><p>  In order to better understand why this is necessary and why it should work, let's talk about information.  We can say that each example contains information about its label and some other information.  In the case of MNIST, all this information can be recorded, for example, in the form of a b / w image with a size of 28x28 pixels.  If you can train the perfect auto-encoder on MNIST, then you can write the same information in a different form.  It is clear that in some cases the label information in the example itself may be incomplete.  For example, in the image it is not always possible to understand exactly which number was written, but a certain amount of information about the label is still contained in the image.  But, in addition to the label, the image has a number of explicit and a huge number of implicit properties: handwriting properties (thickness, slope, ‚Äúcurlicues‚Äù), location (in the center or with a shift), noise, etc.  When we train a classifier, we try to extract information about the tag as much as possible, but this can be done in a huge number of ways.  On the same MNIST, we can train 100 equally effective classifiers, each of which will have its own hidden representation of images, let alone the case when the data sources are different. </p><br><p>  Ganin's idea is that if we can maximize information with the help of a neural network, then nothing prevents us from minimizing it.  If we consider data from two different sources (for example, MNIST and SVHN), then we can say that each of the examples contains information about the label and the source.  If we are able to train a neural network E to extract features containing information about a tag, and do it in the same way, regardless of where the example came from, then the P network trained only with examples from one source should be able to predict the labels for the second source. </p><br><img src="https://habrastorage.org/webt/bf/me/kc/bfmekcy54f5hhhe5p29q5ewzkos.png" alt="Result table"><br><br>  Indeed, a neural network trained on examples from SVHN using domain adaptation determines the class of images from MNIST more accurately than a network trained only on SVHN - 71% accuracy against 59%.  At the same time, both models, of course, have never seen a single label from MNIST during training.  In fact, this means that you can transfer a trained classifier from one sample to another, even if you do not know the labels for the second sample. <br><p>  Despite the fact that the task of classifying numbers is quite simple, the training of the networks used in the article may require substantial resources, and besides, the code solving this problem can be easily found on the Internet.  Therefore, I will analyze another example of the use of this technique, and I hope it will help to better demonstrate the idea of ‚Äã‚Äã‚Äúsharing‚Äù information in the feature layer. </p><br><p>  Very often, when it comes to extracting information or presenting it in a special way, auto-encoders appear on the scene.  Let's learn how to present examples of MNIST in a compressed form, but we will do it in such a way that the compressed representation does not contain information about which figure was displayed.  Then, if the decoder of our network, having obtained the extracted features and the label of the original digit is able to restore the original image, then we can assume that the encoder does not lose any information other than the label.  At the same time, if the classifier according to the extracted features is not able to guess the label, then all the information about the label is forgotten. </p><br><p>  To do this, we will have to create and train 3 networks - the <em>encoder</em> (E), the <em>decoder</em> (D) and the <em>classifier</em> (C). </p><br><p>  This time we will make the <em>encoder</em> convolutional by adding a couple of convolutional layers, for this we will use the <strong>Sequential</strong> class. </p><br><pre><code class="python hljs">conv1 = nn.Sequential( nn.Conv2d(in_channels=<span class="hljs-number"><span class="hljs-number">1</span></span>, out_channels=<span class="hljs-number"><span class="hljs-number">16</span></span>, kernel_size=<span class="hljs-number"><span class="hljs-number">3</span></span>, padding=<span class="hljs-number"><span class="hljs-number">1</span></span>), nn.BatchNorm2d(num_features=<span class="hljs-number"><span class="hljs-number">16</span></span>), nn.ReLU(), nn.MaxPool2d(kernel_size=<span class="hljs-number"><span class="hljs-number">2</span></span>, stride=<span class="hljs-number"><span class="hljs-number">2</span></span>), nn.Dropout(<span class="hljs-number"><span class="hljs-number">0.2</span></span>) ) conv2 = nn.Sequential( nn.Conv2d(in_channels=<span class="hljs-number"><span class="hljs-number">16</span></span>, out_channels=<span class="hljs-number"><span class="hljs-number">32</span></span>, kernel_size=<span class="hljs-number"><span class="hljs-number">3</span></span>, padding=<span class="hljs-number"><span class="hljs-number">1</span></span>), nn.BatchNorm2d(num_features=<span class="hljs-number"><span class="hljs-number">32</span></span>), nn.ReLU(), nn.MaxPool2d(kernel_size=<span class="hljs-number"><span class="hljs-number">2</span></span>, stride=<span class="hljs-number"><span class="hljs-number">2</span></span>), nn.Dropout(<span class="hljs-number"><span class="hljs-number">0.2</span></span>) ) self.conv = nn.Sequential( conv1, conv2 )</code> </pre> <br><p>  In fact, it allows us to specify subnets at once, in this case it is a sequence of layers of convolution, minibatch normalization, activation functions, subsampling and dropout.  Information about these layers is available in large quantities on the Internet (or, for example, in our <a href="https://www.piter.com/product/glubokoe-obuchenie">book</a> ), so here I will not analyze them in detail. </p><br><p>  <strong>Sequential</strong> layers (or subnets) in the forward function can be used just like any other layer. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x)</span></span></span><span class="hljs-function">:</span></span> x = self.conv(x) x = x.view(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>*<span class="hljs-number"><span class="hljs-number">7</span></span>*<span class="hljs-number"><span class="hljs-number">32</span></span>) x = self.fc(x) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x</code> </pre> <br><p>  In order to make a <em>decoder</em> similar to the <em>encoder,</em> its last layers will be set using the transposed convolution </p><br><pre> <code class="python hljs">conv1 = nn.Sequential( nn.ConvTranspose2d(in_channels=<span class="hljs-number"><span class="hljs-number">32</span></span>, out_channels=<span class="hljs-number"><span class="hljs-number">16</span></span>, kernel_size=<span class="hljs-number"><span class="hljs-number">3</span></span>, stride=<span class="hljs-number"><span class="hljs-number">2</span></span>), nn.BatchNorm2d(num_features=<span class="hljs-number"><span class="hljs-number">16</span></span>), nn.ReLU(), nn.Dropout(<span class="hljs-number"><span class="hljs-number">0.2</span></span>) ) conv2 = nn.Sequential( nn.ConvTranspose2d(in_channels=<span class="hljs-number"><span class="hljs-number">16</span></span>, out_channels=<span class="hljs-number"><span class="hljs-number">1</span></span>, kernel_size=<span class="hljs-number"><span class="hljs-number">2</span></span>, padding=<span class="hljs-number"><span class="hljs-number">1</span></span>, stride=<span class="hljs-number"><span class="hljs-number">2</span></span>), nn.Tanh() )</code> </pre> <br><p>  The essential difference between the <em>decoder</em> is that it receives not only the signs obtained from the <em>encoder</em> , but also the label: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, y)</span></span></span><span class="hljs-function">:</span></span> x = torch.cat([x, y], <span class="hljs-number"><span class="hljs-number">1</span></span>) x = self.fc(x) x = x.view(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>) x = self.deconv(x) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x</code> </pre> <br><p>  <strong>torch.cat</strong> allows <strong>you</strong> to combine the signs and the label into one vector, and then we just restore the image from this vector. </p><br><p>  And the third network will be an ordinary <em>classifier</em> , which predicts the original image mark by the signs extracted by the <em>encoder</em> . <br>  The learning cycle of the entire model now looks like this: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> mnist_train: y_onehot = utils.to_onehot(y, <span class="hljs-number"><span class="hljs-number">10</span></span>) <span class="hljs-comment"><span class="hljs-comment"># train classifier C C.zero_grad() z = E(x) C_loss = NLL_loss(C(z), y) C_loss.backward(retain_graph=True) C_optimizer.step() # train decoder D and encoder E E.zero_grad() D.zero_grad() AE_loss = MSE_loss(D(z, y_onehot), x) C_loss = NLL_loss(C(z), y) FADER_loss = AE_loss - beta*C_loss FADER_loss.backward() D_optimizer.step() E_optimizer.step()</span></span></code> </pre> <br><p>  First, we use the <em>encoder</em> to extract the image features and update the weights of the <em>classifier</em> <strong>only</strong> so that it better predicts the label: </p><br><pre> <code class="python hljs">z = E(x) C_loss = NLL_loss(C(z), y) C_loss.backward(retain_graph=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) C_optimizer.step()</code> </pre> <br><p>  However, we ask PyTorch to save the computation graph in order to reuse it.  The next step is to teach the <em>auto-encoder</em> and impose an additional requirement to extract such features, which make it harder for the <em>classifier</em> to restore the label: </p><br><pre> <code class="python hljs">AE_loss = MSE_loss(D(z, y_onehot), x) C_loss = NLL_loss(C(z), y) FADER_loss = AE_loss - C_loss FADER_loss.backward() D_optimizer.step() E_optimizer.step()</code> </pre> <br><p>  Please note that the <em>classifier</em> weights <em>are</em> not updated, however, the <em>encoder</em> weights are updated in the direction of increasing the <em>classifier</em> error.  Thus, we alternately teach the <em>classifier</em> , the <em>auto-encoder</em> pursuing opposite goals.  This is the idea of ‚Äã‚Äãcompetitive networks. </p><br><p>  As a result of learning such a model, we want to obtain an <em>encoder that</em> extracts from the examples all the information necessary for restoring the example, with the exception of the label.  At the same time, we train the <em>decoder</em> using this information in conjunction with the label to be able to restore the original example.  But what if we give the <em>decoder a</em> different label?  In the image below, each line is obtained by reconstructing an image from the signs of one of the digits in combination with 10 possible marks.  The figures, taken as a basis, are located on the diagonal (more precisely, not the original examples themselves, but the reconstructed ones, but using the "correct" label). </p><br><p><img src="https://habrastorage.org/webt/v2/zm/dc/v2zmdccxhqwgn6jmal_zbzjkuua.gif" alt="Transferring the style between numbers"></p><br><p>  In my opinion, this example perfectly demonstrates the idea of ‚Äã‚Äãextracting information other than a label, since it is clear that in the same line all the numbers are ‚Äúwritten‚Äù in the same style.  In addition, it is clear that the string obtained from the digit "1" is unstable.  I explain this by the fact that the writing unit does not contain very much information about the style, perhaps, only the thickness of the line and the slope, but there is definitely no information about the curl.  Therefore, the rest of the numbers written in the same style can be quite diverse, although in each case the style will be one for the whole line, but it will change at different stages of training. </p><br><p>  It remains only to add that a similar approach was published on NIPS'17 in an article from the Facebook team.  Similarly, the model extracts features from photographs of faces and ‚Äúforgets‚Äù labels such as having a beard or glasses.  Here is an example of what happened in the article: </p><br><img src="https://habrastorage.org/webt/d2/9g/2g/d29g2gz-9emvadb19y2eodxqkts.png" alt="An example from the FADER Networks article">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Although in this post we drew ‚Äúnew‚Äù numbers, but for this we had to use already existing numbers to choose a style.  In the next article I will talk about how to generate images from scratch and why this particular model does not know how to do that. </div><p>Source: <a href="https://habr.com/ru/post/358946/">https://habr.com/ru/post/358946/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../358936/index.html">[Translation] Handling errors and transactions in SQL Server. Part 1. Error handling - quick start</a></li>
<li><a href="../358938/index.html">40 stupid CRM questions</a></li>
<li><a href="../358940/index.html">Switzerland Travel Guide</a></li>
<li><a href="../358942/index.html">Apollo graphql client - development of isomorphic (universal) applications on react.js</a></li>
<li><a href="../358944/index.html">How to speed up mobile search in half. Yandex lecture</a></li>
<li><a href="../358948/index.html">Data centers, similar to chicken coops, and work in Antarctica: a selection of unusual data centers</a></li>
<li><a href="../358950/index.html">Pyramid of tests in practice</a></li>
<li><a href="../358954/index.html">Multi-output in machine learning</a></li>
<li><a href="../358960/index.html">The digest of interesting materials for the mobile developer # 253 (May 14 - May 20)</a></li>
<li><a href="../358964/index.html">How I migrated the project from Angular 1 to React</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>