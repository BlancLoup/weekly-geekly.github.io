<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Looking for familiar faces</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the article I want to acquaint the reader with the task of identification: walk from the basic definitions to the implementation of one of the rece...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Looking for familiar faces</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/files/200/7b8/3e3/2007b83e3e9040b7b4b273546777ae58.png" alt="Hello"></div><br>  In the article I want to acquaint the reader with the task of identification: walk from the basic definitions to the implementation of one of the recent articles in this area.  The result should be an application that can search for the same people in the photos and, most importantly, an understanding of how it works. <br><a name="habracut"></a><br><h3>  The Bourne Identification (and not only the Bourne) </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/files/380/5c5/1a9/3805c51a9ccd4e9196fe075a50191c36.jpg"></div><br>  The task of identification is similar to the task of classification and historically arose from it, when instead of determining the class of an object, it became necessary to be able to determine whether an object has the required property or not.  In the identification task, the training set is a set of objects <img src="https://tex.s2cms.ru/svg/M%20%3D%20%5C%7B%20x_i%20%5C%7D_%7Bi%3D1%7D%5En" alt="M = \ {x_i \} _ {i = 1} ^ n">  each of which either has a property <img src="https://tex.s2cms.ru/svg/A" alt="A">  : <img src="https://tex.s2cms.ru/svg/A(x_i)%20%3D%201%2C%20%5C%3B%20i%20%3D%20%5Coverline%7B1%2C%20n%7D" alt="A (x_i) = 1, \;  i = \ overline {1, n}">  or not.  Moreover, all objects belong to the same class and it is impossible to make a representative selection of objects that do not have the specified property. <img src="https://tex.s2cms.ru/svg/A" alt="A">  . <br><div style="text-align:center;"><img src="https://habrastorage.org/files/634/fa6/2a3/634fa62a33ef40feba5f19cdef8f0d48.jpg"></div><br>  For example, if you want to separate the faces of people from the faces of monkeys, then this is a classification task - there are two classes, for each object you can specify a class and make a representative sample of both classes.  If it is required to determine to which person the face image belongs and these people are a finite fixed set, then this is also a classification task. <br><br>  Now imagine that you are developing an application that should determine a person from a photo of his face, and many people remembered in the database are constantly changing and, naturally, during use, the application will see people who were not in the training set - a real task, which Today‚Äôs world is no surprise.  However, it is no longer limited to the task of classification.  How to solve it? <br><br>  To be able to recognize a person, you must see him at least once.  Or rather, to have at least one of his photos or to remember it in the form of some image.  Then, when we are shown a new, previously unknown photograph, we will compare it with all the memorized images and will be able to give an answer: we have already seen this person and can identify him or this person has never met us before and we can only remember him.  Thus the task stated above is reduced to the following: having two photos <img src="https://tex.s2cms.ru/svg/(p_1%2C%20%5C%3B%20p_2)" alt="(p_1, \; p_2)">  determine whether they belong to the same person or not.  In other words, do they have the property of belonging to one person: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0AA(p_1%2C%20%5C%3B%20p_2)%20%3D%20%0A%5Cbegin%7Bcases%7D%0A1%20%26%5Ctext%7B%2C%20%7D%20p_1%20%5Ctext%7B%20%D0%B8%20%7D%20p_2%20%5Ctext%7B%20%D0%BF%D1%80%D0%B8%D0%BD%D0%B0%D0%B4%D0%BB%D0%B5%D0%B6%D0%B0%D1%82%20%D0%BE%D0%B4%D0%BD%D0%BE%D0%BC%D1%83%20%D1%87%D0%B5%D0%BB%D0%BE%D0%B2%D0%B5%D0%BA%D1%83%20%7D%5C%5C%0A0%20%26%5Ctext%7B%2C%20%D0%B8%D0%BD%D0%B0%D1%87%D0%B5%7D%0A%5Cend%7Bcases%7D%0A" alt="A (p_1, \; p_2) = \ begin {cases} 1 &amp; amp;  \ text {,} p_1 \ text {and} p_2 \ text {belong to one person} \\ 0 &amp; amp;  \ text {otherwise} \ end {cases}"></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/def/998/d8f/def998d8f5bf4d0dad86d7adf2a837c5.jpg"></div><br>  Here is the definition of the task of identification: according to the training sample (in the example: many pairs of faces <img src="https://tex.s2cms.ru/svg/M%20%3D%20%5C%7B%20(p_i%2C%20p_j)%20%5C%7D_%7Bi%2Cj%3D1%7D%5En" alt="M = \ {(p_i, p_j) \} _ {i, j = 1} ^ n">  a) build id <img src="https://tex.s2cms.ru/svg/F%3AR%5En%20%5Cto%20%5C%7B0%2C%201%5C%7D" alt="F: R ^ n \ to \ {0, 1 \}">  that can determine whether the object possesses the required attribute or not.  But discreteness is boring, it is much more interesting to know the degree of manifestation of the trait. <img src="https://tex.s2cms.ru/svg/A" alt="A">  the object is equal to probability <img src="https://tex.s2cms.ru/svg/p%20%5Cbig(%20A(x)%20%3D%201%20%5Cvert%20x%20%5Cbig)%20%5Cin%20R" alt="p \ big (A (x) = 1 \ vert x \ big) \ in R">  . <br><br>  We will set the last problem from the example in a slightly more formal and heroic <a href="https://keras.io/">way of solving</a> it, calling <a href="http://arxiv.org/">arxiv.org</a> , <a href="https://www.python.org/">Python,</a> and <a href="https://keras.io/">Keras</a> for help.  Face photos - matrix of <img src="https://tex.s2cms.ru/svg/R%5E%7Bm%20%5Ctimes%20n%7D" alt="R ^ {m \ times n}">  .  For two people, we want to know the probability of their belonging to one person.  Probability - a real number from 0 to 1. So, we are looking for a function <img src="https://tex.s2cms.ru/svg/F%3A%20R%5E%7Bm%20%5Ctimes%20n%7D%20%5Ctimes%20R%5E%7Bm%20%5Ctimes%20n%7D%20%5Cto%20%5B0%3B1%5D" alt="F: R ^ {m \ times n} \ times R ^ {m \ times n} \ to [0;  one]">  .  Since it is 2017 already in the yard, we will look for it using machine learning methods.  Teaching set, however, we will not have a lot of pairs from the definition, but a lot of photos of the faces of different people with labels, to whom it belongs - just like for the task of classification.  These sets are equivalent, but it is easier to work with the second one. <br><br><h3>  Superiority bourne </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/files/d6d/465/00f/d6d46500f41648018cd86b2bac1b0e3a.jpg"></div><br>  What is most important in solving the problems of machine learning?  Do you think the ability to search for an answer?  No, the main thing is the ability to verify this answer.  Function <img src="https://tex.s2cms.ru/svg/F(x%2C%20y)%20%3D%20%5Cxi%20%5Csim%20U%5B0%3B1%5D" alt="F (x, y) = \ xi \ sim U [0;  one]">  returning random numbers from the interval <img src="https://tex.s2cms.ru/svg/%5B0%3B%201%5D" alt="[0;  one]">  is quite a correct solution to the problem, but in practice it is useless.  How to find out without resorting to the "by eye" method?  In the classification problem, we can see <strong>accuracy</strong> on the validation set - the proportion of correctly classified examples.  For the identification task, this metric is not applicable.  So how to objectively assess the suitability of the solution to our problem? <br><br>  Let's introduce the concept of <strong>target</strong> attempts and <strong>imposter</strong> tests.  First we name the object <img src="https://tex.s2cms.ru/svg/x%20%5Cin%20X" alt="x \ in X">  for which it is known that <img src="https://tex.s2cms.ru/svg/p%20%5Cbig(%20A(x)%3D1%20%5Cvert%20x%20%5Cbig)%3D1" alt="p \ big (A (x) = 1 \ vert x \ big) = 1">  i.e.  the object has the required property with probability 1 (in our problem - a pair of individuals <img src="https://tex.s2cms.ru/svg/(p_1%2C%20p_2)" alt="(p_1, p_2)">  belonging to one person), and the second, respectively, the object <img src="https://tex.s2cms.ru/svg/x%20%5Cin%20X" alt="x \ in X">  such that <img src="https://tex.s2cms.ru/svg/p%20%5Cbig(%20A(x)%3D1%20%5Cvert%20x%20%5Cbig)%20%3D%200" alt="p \ big (A (x) = 1 \ vert x \ big) = 0">  .  Accordingly, consider the sets <img src="https://tex.s2cms.ru/svg/T%20%3D%20%5C%7B%20x%20%5Cin%20X%20%5Cbig%5Cvert%20p%5Cbig(A(x)%3D1%20%5Cvert%20x%5Cbig)%3D1%20%5C%7D" alt="T = \ {x \ in X \ big \ vert p \ big (A (x) = 1 \ vert x \ big) = 1 \}">  and <img src="https://tex.s2cms.ru/svg/I%20%3D%20%5C%7B%20x%20%5Cin%20X%20%5Cbig%5Cvert%20p%5Cbig(A(x)%3D1%20%5Cvert%20x%5Cbig)%3D0%20%5C%7D" alt="I = \ {x \ in X \ big \ vert p \ big (A (x) = 1 \ vert x \ big) = 0 \}">  which together will be our validation set: <img src="https://tex.s2cms.ru/svg/T%20%5Ccup%20I%20%3D%20%5Coverline%7BM%7D" alt="T \ cup I = \ overline {M}">  .  The considerations of his choice are absolutely standard, as for any machine learning task - it must be representative, i.e.  reflect all the required variability and be of sufficient size.  For example, if your face recognition system involves working in different lighting conditions, these conditions should be represented in the validation set (as in the training set, of course). <br><br>  Now let's take our constructed function. <img src="https://tex.s2cms.ru/svg/F" alt="F">  and build <img src="https://tex.s2cms.ru/svg/F(T)%20%3D%20%5C%7B%20F(t)%20%5Cvert%20t%20%5Cin%20T%20%5C%7D" alt="F (T) = \ {F (t) \ vert t \ in T \}">  - result of application <img src="https://tex.s2cms.ru/svg/F" alt="F">  to all <strong>target</strong> attempts.  Get the set of real numbers from the interval <img src="https://tex.s2cms.ru/svg/%5B0%3B%201%5D" alt="[0;  one]">  or <strong>scores</strong> .  These values ‚Äã‚Äãare a measure of how well our solution works when it is not tried to be deceived and they expect a positive response from it.  And <img src="https://tex.s2cms.ru/svg/F(T)" alt="F (t)">  - some sample that we consider to be representative.  We construct its empirical density - a histogram. <br><br>  So she looks for <img src="https://tex.s2cms.ru/svg/F(x%2C%20y)%20%3D%20%5Cxi%20%5Csim%20U%5B0%3B1%5D" alt="F (x, y) = \ xi \ sim U [0;  one]">  : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/5e3/c71/ad1/5e3c71ad19d54b28ba9061812f6c013f.png" alt="Distribution of target attempts for uniform f"></div><br>  Agree, the sense of such an identifier is a little - in half of the cases it will guess the answer, and in half it will be wrong. <br><br>  This distribution suits us better: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/5f0/594/f89/5f0594f897f34ae48003574138656cf9.png" alt="Distribution of target-attempts for good f"></div><br>  For <strong>target</strong> attempts, such a function will be more confident in their correctness than not. <br><br>  But consideration of the distribution of the target without the distribution of imposters is meaningless.  Let's do the same operation for them: build the distribution density <img src="https://tex.s2cms.ru/svg/F(I)" alt="F (I)">  and display on the same graph.  We get a similar picture: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/489/01b/cf3/48901bcf33dd497f8ce983ebcb9db164.png" alt="target distribution and imposter attempts"></div><br>  It becomes clear that for <strong>imposter</strong> attempts, our function will in most cases be inclined to the correct answer.  But these are still only visual observations; they do not give us any <strong>objective</strong> assessment. <br><br>  Suppose our system is fed a couple of images at the entrance.  She can calculate for them the probability that this is a <strong>target</strong> .  But it requires an unequivocal answer from her: whether it is the same person or not, whether to let him on a secret object or not.  Let's set some threshold <img src="https://tex.s2cms.ru/svg/d%20%5Cin%20%5B0%3B%201%5D" alt="d \ in [0; one]">  and if <img src="https://tex.s2cms.ru/svg/F(t)%20%3C%20d" alt="F (t) &amp; lt; d">  , we will respond negatively, otherwise - positively.  If a <img src="https://tex.s2cms.ru/svg/d%20%3D%200" alt="d = 0">  , the system will never know anyone, and if <img src="https://tex.s2cms.ru/svg/d%20%3D%201" alt="d = 1">  , then any two people will be considered the same.  The graph shows that our distributions are not separable and cannot be matched. <img src="https://tex.s2cms.ru/svg/d" alt="d">  so as to achieve perfect work in both cases.  What will happen, for example, if in the example above install <img src="https://tex.s2cms.ru/svg/d%20%3D%200.5" alt="d = 0.5">  ? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/b7c/00e/d00/b7c00ed0024443f98ff2dbab4adcbb53.png" alt="distribution of target and imposter attempts with a middle splitter"></div><br>  In how many cases will our system be wrong with <strong>target</strong> attempts?  Easy to count: <img src="https://tex.s2cms.ru/svg/%5Cbig%5Cvert%20%5C%7B%20x%20%5Cin%20F(T)%20%5Cvert%20x%20%3C%20d%20%5C%7D%20%5Cbig%5Cvert" alt="\ big \ vert \ {x \ in F (T) \ vert x &amp; lt; d \} \ big \ vert">  - the number of <strong>target</strong> attempts, which will be incorrectly classified as <strong>imposter</strong> attempts.  Similarly for the latter.  And now we give them the names and make not absolute, but relative: <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/FRR%20%3D%20%5Cfrac%7B%5Cbig%5Cvert%20%5C%7B%20x%20%5Cin%20F(T)%20%5Cvert%20x%20%3C%20d%20%5C%7D%20%5Cbig%5Cvert%7D%7B%5Cbig%5Cvert%20F(T)%20%5Cbig%5Cvert%7D" alt="FRR = \ frac {\ big \ vert \ {x \ in F (T) \ vert x &amp; lt; d \} \ big \ vert} {\ big \ vert F (T) \ big \ vert}"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/FAR%20%3D%20%5Cfrac%7B%5Cbig%5Cvert%20%5C%7B%20x%20%5Cin%20F(I)%20%5Cvert%20x%20%3E%20d%20%5C%7D%20%5Cbig%5Cvert%7D%7B%5Cbig%5Cvert%20F(I)%20%5Cbig%5Cvert%7D" alt="FAR = \ frac {\ big \ vert \ {x \ in F (I) \ vert x &amp; gt; d \} \ big \ vert} {\ big \ vert F (I) \ big \ vert}"></div><br><ul><li>  <strong>FRR</strong> (False Rejection Rate) - the proportion of incorrectly rejected <strong>target</strong> attempts. </li><li>  <strong>FAR</strong> (False Acceptance Rate) - the proportion of incorrectly taken <strong>imposter-</strong> attempts. </li></ul><br>  Let's now take some step. <img src="https://tex.s2cms.ru/svg/%5CDelta%20d%20%3D%20%5Cfrac%7B1%7D%7BN%7D" alt="\ Delta d = \ frac {1} {N}">  and calculate with it the values ‚Äã‚Äãof <strong>FRR</strong> and <strong>FAR</strong> for <img src="https://tex.s2cms.ru/svg/N" alt="N">  points from the interval <img src="https://tex.s2cms.ru/svg/%5B0%3B%201%5D" alt="[0; one]">  and display them on the same graph: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/4c4/a9b/62b/4c4a9b62bb6b46bd8771aca3a7df84b3.png" alt="far and frr"></div><br>  Now, for any chosen distance, we can say what proportion of <strong>target</strong> attempts will deviate and what proportion of <strong>imposter</strong> attempts will be taken.  Conversely, you can choose <img src="https://tex.s2cms.ru/svg/d" alt="d">  based on the task.  For example, for verification at a guarded object, where it is important not to miss an outsider, for obvious reasons you need <img src="https://tex.s2cms.ru/svg/d" alt="d">  providing the lowest possible <strong>FAR</strong> .  If you want the computer to recognize you and wish you good morning, and usually only you walk through the apartment, you can stop at a low <strong>FRR</strong> and a high enough <strong>FAR</strong> - nothing bad happens if the computer says hello to someone, calling him your name. <br><br>  Pay attention to the point of intersection of the graphs.  The value in it is called <strong>EER</strong> (Equal Error Rate). <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/EER%20%3D%20%5Carg%20%5Cmin_%7BFAR%7D%20%5Cbig%5Cvert%20FAR%20-%20FRR%20%5Cbig%5Cvert" alt="EER = \ arg \ min_ {FAR} \ big \ vert FAR - FRR \ big \ vert"></div><br>  If we choose <img src="https://tex.s2cms.ru/svg/d%20%3D%20d_%7Beer%7D" alt="d = d_ {eer}">  , <strong>FAR</strong> and <strong>FRR</strong> values ‚Äã‚Äãwill be equal.  <strong>EER</strong> is the very objective criterion to which we went.  It allows us to assess the quality of identification as a whole: this is the average error on our validation set.  You can also look at <strong>FAR</strong> with a fixed <strong>FRR</strong> and vice versa, but most often <strong>EER is</strong> used as a metric. <br><br>  In the example above, <strong>EER</strong> = 0.067.  This means that on average 6.7% of all <strong>target</strong> attempts will deviate and 6.7% of all <strong>imposter</strong> attempts will be accepted. <br><br>  Another important concept is the <strong>DET</strong> curve - the dependence of <strong>FAR</strong> on <strong>FRR</strong> on a logarithmic scale.  By its form it is easy to judge the quality of the system as a whole, to evaluate what value of one criterion can be obtained with a fixed second, and, most importantly, compare systems. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/0de/450/0de/0de4500debb04bcd8f79a89a9e2d4ea0.png" alt="det curve"></div><br>  <strong>ERR</strong> here is the intersection of a <strong>DET</strong> curve with a straight line. <img src="https://tex.s2cms.ru/svg/y%20%3D%20x" alt="y = x">  . <br><br>  A naive implementation in Python (you can be more optimal if you consider that <strong>FAR</strong> and <strong>FRR</strong> change only at points from <img src="https://tex.s2cms.ru/svg/F(T)%20%5Ccup%20F(I)" alt="F (T) \ cup F (I)">  ): <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">calc_metrics</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(targets_scores, imposter_scores)</span></span></span><span class="hljs-function">:</span></span> min_score = np.minimum(np.min(targets_scores), np.min(imposter_scores)) max_score = np.maximum(np.max(targets_scores), np.max(imposter_scores)) n_tars = len(targets_scores) n_imps = len(imposter_scores) N = <span class="hljs-number"><span class="hljs-number">100</span></span> fars = np.zeros((N,)) frrs = np.zeros((N,)) dists = np.zeros((N,)) mink = float(<span class="hljs-string"><span class="hljs-string">'inf'</span></span>) eer = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, dist <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(np.linspace(min_score, max_score, N)): far = len(np.where(imposter_scores &gt; dist)[<span class="hljs-number"><span class="hljs-number">0</span></span>]) / n_imps frr = len(np.where(targets_scores &lt; dist)[<span class="hljs-number"><span class="hljs-number">0</span></span>]) / n_tars fars[i] = far frrs[i] = frr dists[i] = dist k = np.abs(far - frr) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> k &lt; mink: mink = k eer = (far + frr) / <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> eer, fars, frrs, dists</code> </pre> <br>  With the control figured out: now, whatever function <img src="https://tex.s2cms.ru/svg/F" alt="F">  we have not chosen, we can calculate for it <strong>FAR</strong> , <strong>FRR</strong> and <strong>ERR</strong> on the validation set and construct several visual graphs. <br><br>  <strong>Important:</strong> in the identification tasks, what we called the validation set above is usually called the <strong>development set</strong> (development set, devset).  We will adhere to this notation in the future. <br><br>  <strong>Important:</strong> since any interval of the real axis <img src="https://tex.s2cms.ru/svg/R" alt="R">  can be displayed in the segment <img src="https://tex.s2cms.ru/svg/%5B0%3B%201%5D" alt="[0; one]">  , it is not at all necessary that the value of our function be in the range from zero to one.  Even without displaying into a single segment, any range of values ‚Äã‚Äãcan be considered without affecting the results. <br><br><h3>  Base preparation </h3><br>  There are <a href="https://www.kairos.com/blog/60-facial-recognition-databases">many</a> face recognition datasets.  Some are paid, some are available on request.  Some contain greater variability in lighting, others in facial position.  Some were taken under laboratory conditions, others are collected from photographs taken in their natural habitat.  Having clearly defined the data requirements, you can easily select a suitable dataset or assemble it from several.  For me, within the framework of this educational task, the requirements were as follows: datasets should be easily available for download, contain not very much data and contain variability in the position of the face.  The requirements were satisfied with three data sets, which I combined into one: <br><br><ul><li>  <a href="http://www.vision.caltech.edu/html-files/archive.html">Caltech faces</a> </li><li>  <a href="http://fei.edu.br/~cet/facedatabase.html">FEI Face Database</a> </li><li>  <a href="http://www.anefian.com/research/face_reco.htm">Georgia Tech face database</a> . </li></ul><br>  All of them are outdated for a long time and do not allow to build a high-quality modern facial recognition system, but they are ideal for learning. <br><br>  The base thus obtained turned out to be 277 subjects and ~ 4000 images, on average, 14 images per person.  Take 5-10% of subjects for <strong>development-</strong> sets, the rest use for training.  When training, the system should see only examples from the second set, and we will check it (consider <strong>EER</strong> ) at the first. <br><br>  The code for data separation is available <a href="https://github.com/meownoid/face-identification-tpe/blob/master/utils/organize_data.py">here</a> .  You only need to specify the path to the unpacked datasets listed above. <br><br>  Now the data need to be processed.  For a start, highlight faces.  We will not do this ourselves, but use the dlib library. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/a15/4f1/c1e/a154f1c1eeb94c9ba0562d3b261ee1ad.png"></div><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> dlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> skimage <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> io image = io.imread(image_path) detector = dlib.get_frontal_face_detector() face_rects = list(detector(image, <span class="hljs-number"><span class="hljs-number">1</span></span>)) face_rect = face_rects[<span class="hljs-number"><span class="hljs-number">0</span></span>]</code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/files/9ae/f8a/1ff/9aef8a1ffa95469bbd3045d72c450437.png"></div><br>  As you can see, using this library, you can get the bounding rectangle for a couple of lines of code.  And the dlib detector, <a href="https://www.youtube.com/watch%3Fv%3DLsK0hzcEyHI">in contrast to OpenCV</a> , works very well: from the entire base only a dozen faces it could not detect and did not create a single false response. <br><br>  The formal formulation of our task implies that all individuals must be the same size.  We will satisfy this requirement, at the same time aligning all faces so that the key points (eyes, nose, lips) are always in the same place of the image.  It is clear that such a measure can help us regardless of the chosen mode of study and certainly does not harm.  The algorithm is simple: <br><br><ol><li>  There are a priori positions of key points in the unit square. </li><li>  Knowing the selected image size, we calculate the coordinates of these points on our image by simple scaling. </li><li>  Select the key points of the next person. </li><li>  We construct an affine transformation that takes the second set of points to the first. </li><li>  Apply an affine transformation to the image and trim it. </li></ol><br>  The reference position of the key points will be found in the examples to dlib (face_template.npy, download <a href="https://yadi.sk/d/zIWpWyX73ACTAg">here</a> ). <br><br><pre> <code class="python hljs">face_template = np.load(face_template_path)</code> </pre> <br>  To find the key points on the image of the face, again, let's use dlib, using the already trained model, which can be found there, in the examples (shape_predictor_68_face_landmarks.dat, download <a href="https://yadi.sk/d/zIWpWyX73ACTAg">here</a> ). <br><br><pre> <code class="python hljs">predictor = dlib.shape_predictor(dlib_predictor_path) points = predictor(image, face_rect) landmarks = np.array(list(map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> p: [px, py], points.parts())))</code> </pre> <br>  Affine transformation is uniquely defined by three points: <br><br><pre> <code class="python hljs">INNER_EYES_AND_BOTTOM_LIP = [<span class="hljs-number"><span class="hljs-number">39</span></span>, <span class="hljs-number"><span class="hljs-number">42</span></span>, <span class="hljs-number"><span class="hljs-number">57</span></span>]</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/files/422/565/263/4225652631bd409889710f528adf023f.png"></div><br>  Let be <img src="https://tex.s2cms.ru/svg/(x_1%5E0%2C%20y_1%5E0)%2C%20(x_2%5E0%2C%20y_2%5E0)%2C%20(x_3%5E0%2C%20y_3%5E0)" alt="(x_1 ^ 0, y_1 ^ 0), (x_2 ^ 0, y_2 ^ 0), (x_3 ^ 0, y_3 ^ 0)">  - the starting points that we want to translate into <img src="https://tex.s2cms.ru/svg/(x_1%5E1%2C%20y_1%5E1)%2C%20(x_2%5E1%2C%20y_2%5E1)%2C%20(x_3%5E1%2C%20y_3%5E1)" alt="(x_1 ^ 1, y_1 ^ 1), (x_2 ^ 1, y_2 ^ 1), (x_3 ^ 1, y_3 ^ 1)">  .  Then the affine transform expressed by the matrix <img src="https://tex.s2cms.ru/svg/T" alt="T">  can be found from the relation <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Cbegin%7Bbmatrix%7D%0Ax_1%5E1%20%26%20x_2%5E1%20%26%20x_3%5E1%20%5C%5C%0Ay_1%5E1%20%26%20y_2%5E1%20%26%20y_3%5E1%20%5C%5C%0A1%20%26%201%20%26%201%0A%5Cend%7Bbmatrix%7D%3DT%0A%5Cbegin%7Bbmatrix%7D%0Ax_1%5E0%20%26%20x_2%5E0%20%26%20x_3%5E0%20%5C%5C%0Ay_1%5E0%20%26%20y_2%5E0%20%26%20y_3%5E0%20%5C%5C%0A1%20%26%201%20%26%201%0A%5Cend%7Bbmatrix%7D.%0A" alt="\ begin {bmatrix} x_1 ^ 1 &amp; amp; x_2 ^ 1 &amp; amp; x_3 ^ 1 \\ y_1 ^ 1 &amp; amp; y_2 ^ 1 &amp; amp; y_3 ^ 1 \\ 1 &amp; amp; 1 &amp; amp; 1 \ end {bmatrix} = T \ begin {bmatrix} x_1 ^ 0 &amp; amp; x_2 ^ 0 &amp; amp; x_3 ^ 0 \\ y_1 ^ 0 &amp; amp; y_2 ^ 0 &amp; amp; y_3 ^ 0 \\ 1 &amp; amp; 1 &amp; amp; 1 \ end {bmatrix}."></div><br>  Find it: <br><br><pre> <code class="python hljs">proper_landmarks = <span class="hljs-number"><span class="hljs-number">227</span></span> * face_template[INNER_EYES_AND_BOTTOM_LIP] current_landmarks = landmarks[INNER_EYES_AND_BOTTOM_LIP] A = np.hstack([current_landmarks, np.ones((<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>))]).astype(np.float64) B = np.hstack([proper_landmarks, np.ones((<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>))]).astype(np.float64) T = np.linalg.solve(A, B).T</code> </pre><br>  And apply to the image using the scipy-image library: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> skimage.transform <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tr wrapped = tr.warp( image, tr.AffineTransform(T).inverse, output_shape=(<span class="hljs-number"><span class="hljs-number">227</span></span>, <span class="hljs-number"><span class="hljs-number">227</span></span>), order=<span class="hljs-number"><span class="hljs-number">3</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'constant'</span></span>, cval=<span class="hljs-number"><span class="hljs-number">0</span></span>, clip=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, preserve_range=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span> ) wrapped /= <span class="hljs-number"><span class="hljs-number">255.0</span></span></code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/files/a70/310/de6/a70310de6aa243e99225c49e68814aec.png"></div><br>  The full preprocessing code, wrapped in a convenient api, can be found in the <a href="https://github.com/meownoid/face-identification-tpe/blob/master/preprocessing.py">preprocessing.py</a> file. <br><br>  The final chord of data preparation will be normalization: we calculate the average and standard deviation on the basis of the training and normalize each image on them.  Do not forget about the development-set.  See the code <a href="https://github.com/meownoid/face-identification-tpe/blob/master/utils/load_data.py">here</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/e4f/95d/71b/e4f95d71bd4a4f39b2804105d76bcdbc.jpg"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/files/3cb/b7d/6a0/3cbb7d6a0f604532847699d6ab994990.jpg"></div><br>  Collected, divided, aligned and normalized data can be downloaded <a href="https://yadi.sk/d/16GictwI3ANWHv">here</a> . <br><br><h3>  Bourne ultimatum </h3><br>  Data found and prepared, with a technique of testing sorted.  Half the battle is done, the easiest is to find <img src="https://tex.s2cms.ru/svg/F" alt="F">  solving a problem with a good <strong>EER</strong> .  Let us immediately determine that <strong>EER</strong> = 10% will fully suit us for such an educational task.  In fact, such a system can even be used in simple applications like searching for identical faces in two photographs. <br><br><h4>  Coin </h4><br>  Let's start the search from the very example of a bad function. <img src="https://tex.s2cms.ru/svg/F(x%2C%20y)%20%3D%20%5Cxi%20%5Csim%20U%5B0%3B1%5D" alt="F (x, y) = \ xi \ sim U [0; 1]">  .  For each pair of photos from the development set, we will get a random value, build a <strong>DET</strong> curve using them and find the <strong>EER</strong> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/2ab/a26/a72/2aba26a7280d481a811e6df9ca3ce1d1.png" alt="bad random function det-curve"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/files/80e/26f/0f3/80e26f0f3c924f5d91d5ddc802db386b.jpg" alt="fu"></div><br>  C <strong>EER</strong> = 49.5%, such an identifier is no better than a coin that we would toss when making each decision.  Of course, this is understandable without graphs, but our goal is to learn how to solve identification problems and to be able to objectively evaluate any decisions, even obviously bad ones.  In addition, it will from what to push off. <br><br><h4>  Distance </h4><br>  What is the function of two vectors from <img src="https://tex.s2cms.ru/svg/R%5E%7Bm%20%5Ctimes%20n%7D" alt="R ^ {m \ times n}">  returning a real number comes to mind first?  I am sure most of you will answer this question - distance.  Really, <img src="https://tex.s2cms.ru/svg/R%5E%7Bm%20%5Ctimes%20n%7D" alt="R ^ {m \ times n}">  - metric space, which means for any two elements <img src="https://tex.s2cms.ru/svg/x" alt="x">  and <img src="https://tex.s2cms.ru/svg/y" alt="y">  distance is determined from it <img src="https://tex.s2cms.ru/svg/d(x%2C%20y)%20%5Cin%20R" alt="d (x, y) \ in R">  which can be entered in different ways.  That's just need to consider it with a minus, since the distance varies from zero to plus infinity, and in the formalization adopted by us should be the opposite. <br><br>  Take, for example, the cosine distance: <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/-d(x%2C%20y)%20%3D%20%5Ccos(x%2C%20y)%20%3D%20%5Cfrac%7Bx%20%5Ccdot%20y%7D%7B%5CVert%20x%20%5CVert%20%5Ccdot%20%5CVert%20y%20%5CVert%7D" alt="-d (x, y) = \ cos (x, y) = \ frac {x \ cdot y} {\ Vert x \ Vert \ cdot \ Vert y \ Vert}"></div><br><br>  And we will do all the same operations on the development-set: <br><br><pre> <code class="python hljs">dev_x = np.load(<span class="hljs-string"><span class="hljs-string">'data/dev_x.npy'</span></span>) protocol = np.load(<span class="hljs-string"><span class="hljs-string">'data/dev_protocol.npy'</span></span>) dev_x = dev_x.mean(axis=<span class="hljs-number"><span class="hljs-number">3</span></span>).reshape(dev_x.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-number"><span class="hljs-number">-1</span></span>) dev_x /= np.linalg.norm(dev_x, axis=<span class="hljs-number"><span class="hljs-number">1</span></span>)[:, np.newaxis] scores = dev_x @ dev_x.T tsc, isc = scores[protocol], scores[np.logical_not(protocol)] eer, fars, frrs, dists = calc_metrics(tsc, isc)</code> </pre><br>  We get this <strong>DET</strong> curve: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/3ab/7d0/535/3ab7d05355c14eaf98393f70b8d5b994.png" alt="cosine det-curve"></div><br>  <strong>EER</strong> decreased by 16% and became equal to 34.18%.  Better, but still not applicable.  Of course, since we have so far only selected the function, without using the training set and machine learning methods.  However, the idea is sensible with distance: let's leave it and present our function <img src="https://tex.s2cms.ru/svg/F" alt="F">  as <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/F%20%3D%20d(f(x)%2C%20f(y))" alt="F = d (f (x), f (y))"></div><br>  Where <img src="https://tex.s2cms.ru/svg/d" alt="d">  - cosine distance, and <img src="https://tex.s2cms.ru/svg/f%3A%20R%5E%7Bm%20%5Ctimes%20n%7D%20%5Cto%20R%5Ek" alt="f: R ^ {m \ times n} \ to R ^ k">  - some function that we call the <strong>embedder</strong> , and the results of its work from the space <img src="https://tex.s2cms.ru/svg/R%5Ek" alt="R ^ k">  - <strong>embeddings</strong> .  It ‚Äúembeds‚Äù our images into some space of another (not necessarily smaller) dimension, considering the a posteriori experience obtained from the training set. <br><br><h4>  CNN </h4><br>  Great, you and I just made the task even easier.  It remains only to find a good function. <img src="https://tex.s2cms.ru/svg/f" alt="f">  All other parts of the system are already in place.  We will not beat around the bush - we all know that currently there is no model better coping with images than CNN (Convolutional Neural Networks).  Let's build a convoluted network of some not very complex architecture, for example: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/d57/4c0/252/d574c0252c84436bba828ec39fc772d9.png" alt="cnn architecture"></div><br><div class="spoiler">  <b class="spoiler_title">Model in keras</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Flatten, Dense, Dropout <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.convolutional <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Convolution2D, MaxPooling2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.advanced_activations <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PReLU <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential model = Sequential() model.add(Convolution2D(<span class="hljs-number"><span class="hljs-number">96</span></span>, <span class="hljs-number"><span class="hljs-number">11</span></span>, <span class="hljs-number"><span class="hljs-number">11</span></span>, subsample=(<span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>), input_shape=(dim, dim, <span class="hljs-number"><span class="hljs-number">3</span></span>), init=<span class="hljs-string"><span class="hljs-string">'glorot_uniform'</span></span>, border_mode=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(PReLU()) model.add(MaxPooling2D((<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), strides=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>))) model.add(Convolution2D(<span class="hljs-number"><span class="hljs-number">256</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, subsample=(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), init=<span class="hljs-string"><span class="hljs-string">'glorot_uniform'</span></span>, border_mode=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(PReLU()) model.add(MaxPooling2D((<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), strides=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>))) model.add(Convolution2D(<span class="hljs-number"><span class="hljs-number">384</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, subsample=(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), init=<span class="hljs-string"><span class="hljs-string">'glorot_uniform'</span></span>, border_mode=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(PReLU()) model.add(Convolution2D(<span class="hljs-number"><span class="hljs-number">384</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, subsample=(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), init=<span class="hljs-string"><span class="hljs-string">'glorot_uniform'</span></span>, border_mode=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(PReLU()) model.add(Convolution2D(<span class="hljs-number"><span class="hljs-number">256</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, subsample=(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), init=<span class="hljs-string"><span class="hljs-string">'glorot_uniform'</span></span>, border_mode=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(PReLU()) model.add(MaxPooling2D((<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), strides=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>))) model.add(Flatten()) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">2048</span></span>, init=<span class="hljs-string"><span class="hljs-string">'glorot_uniform'</span></span>)) model.add(PReLU()) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">256</span></span>, init=<span class="hljs-string"><span class="hljs-string">'glorot_uniform'</span></span>)) model.add(PReLU()) model.add(Dense(n_classes, init=<span class="hljs-string"><span class="hljs-string">'glorot_uniform'</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'categorical_crossentropy'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre><br></div></div><br><br>  And we will teach her to solve the classical classification problem on our training set: to determine which of the 250 subjects owns a face photo.  Everyone knows how to solve such a simple task, in keras for this, besides the code above, you will need more lines 5-6.  Let me just say that for the training base described in this article, it is vital to apply <a href="https://keras.io/preprocessing/image/">augmentation</a> , otherwise there is not enough data to achieve good results. <br><br>  You ask, what does the classification task have to do with it and how will its solution help us?  Do it right!  In order for the actions described below to make sense, it is necessary to make a very important <strong>assumption</strong> : <em>if the network learns well to solve the classification problem in a closed set, then the penultimate layer of dimension 256 will concentrate all the important information about the face image, even if the subject was not in the training set</em> . <br><br>  This technique of extracting low-dimensional characteristics from the last layers of a trained network is widespread and is called <strong>bottleneck</strong> .  By the way, the code for working with bottleneck in keras is located by <a href="https://github.com/meownoid/face-identification-tpe/blob/master/bottleneck.py">reference</a> . <br><br>  The network was trained, 256-dimensional signs from the development-set were learned.  We look at the <strong>DET-</strong> curve: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/214/425/7bf/2144257bfb5b4165920acf09306f0678.png" alt="det curve cnn + cos"></div><br>  The assumption turned out to be true, reducing the <strong>EER by</strong> another 13%, reaching a result of 21.6%.  Twice better than tossing a coin.  Is it even better?  Of course, you can build a bigger and more varied base, build a deeper CNN, apply various regularization methods ... But we are considering conceptual approaches, qualitative ones.  And the amount can always be increased.  I still have another trump card up my sleeve, but before putting it on the table, I‚Äôll have to digress a little. <br><br><h3>  The Bourne Evolution </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/files/cc0/8b3/aee/cc08b3aeedec4f779c6a2fa07c6d6f41.png"></div><br>  The key to improving our results lies in the realization that optimizing <img src="https://tex.s2cms.ru/svg/f" alt="f">  information can be used not only from the training set, but also information about the function <img src="https://tex.s2cms.ru/svg/d" alt="d">  .  Indeed, let's fix <img src="https://tex.s2cms.ru/svg/d" alt="d">  and we will train <img src="https://tex.s2cms.ru/svg/f" alt="f">  based on a priori knowledge of how <strong>embedding</strong> will then be used to produce a <strong>score</strong> .  For the first time, this approach was suggested by the guys from Google on <a href="https://arxiv.org/abs/1503.03832">FaceNet: A Unified Embedding for Face Recognition and Clustering</a> . <br><br>  The approach proposed by them was called <strong>TDE</strong> (Triplet Distance Embedding) and consisted of the following: let's build <img src="https://tex.s2cms.ru/svg/f" alt="f">  as a network from the source space <img src="https://tex.s2cms.ru/svg/R%5E%7Bm%20%5Ctimes%20n%7D" alt="R ^ {m \ times n}">  into the <strong>embedding</strong> space <img src="https://tex.s2cms.ru/svg/R%5Ek" alt="R ^ k">  without the need to solve an intermediate classification problem, we fix <img src="https://tex.s2cms.ru/svg/d" alt="d">  as Euclidean distance and take it into account in <strong>loss</strong> -functions.  How?  It is quite intuitive: we want the vectors of one subject to be in the target space as close as possible to each other and, furthermore, away from the vectors of other subjects. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/01c/96f/5e2/01c96f5e23c04150a95f31e1377be807.png"></div><br><br>  Teaching such a network was suggested using triples. <img src="https://tex.s2cms.ru/svg/(x_a%2C%20x_p%2C%20x_n)" alt="(x_a, x_p, x_n)">  where <img src="https://tex.s2cms.ru/svg/x_a" alt="x_a">  (anchor) and <img src="https://tex.s2cms.ru/svg/x_p" alt="x_p">  (positive) belong to the same subject, and <img src="https://tex.s2cms.ru/svg/x_n" alt="x_n">  (negative) - to another.  For all three vectors, build <strong>embeddings</strong> and <img src="https://tex.s2cms.ru/svg/f(x_a)" alt="f (x_a)">  , <img src="https://tex.s2cms.ru/svg/f(x_p)" alt="f (x_p)">  and <img src="https://tex.s2cms.ru/svg/f(x_n)" alt="f (x_n)">  .  Let us set some parameter in advance. <img src="https://tex.s2cms.ru/svg/%5Calpha" alt="\ alpha">  .  We assume that the troika is good if the ratio is <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%5CVert%20f(x_a)%20-%20f(x_n)%20%5CVert_2%5E2%20-%20%5CVert%20f(x_a)%20-%20f(x_p)%20%5CVert_2%5E2%20%3E%20%5Calpha%2C" alt="\ Vert f (x_a) - f (x_n) \ Vert_2 ^ 2 - \ Vert f (x_a) - f (x_p) \ Vert_2 ^ 2 &amp; gt; \ alpha,"></div><br><br>  which means that for a given anchor there is a gap between the spheres on which the positive and negative lies <img src="https://tex.s2cms.ru/svg/%5Calpha" alt="\ alpha">  .  If this relationship holds for all possible triples from the training set, we have ideally separated the data.  And it makes sense to train the network only on those triples for which this inequality is violated.  Based on the inequality, you can build a loss function for the network <img src="https://tex.s2cms.ru/svg/f" alt="f">  : <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/L(x_a%2C%20x_p%2C%20x_n%2C%20f)%20%3D%20%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%20%5CBig%5B%5CVert%20f(x_a%5Ei)%20-%20f(x_p%5Ei)%20%5CVert_2%5E2%20-%20%20%5CVert%20f(x_a%5Ei)%20-%20f(x_n%5Ei)%20%5CVert_2%5E2%20%2B%20%5Calpha%20%5CBig%5D." alt="L (x_a, x_p, x_n, f) = \ frac {1} {N} \ sum_ {i = 1} ^ N \ Big [\ Vert f (x_a ^ i) - f (x_p ^ i) \ Vert_2 ^ 2 - \ Vert f (x_a ^ i) - f (x_n ^ i) \ Vert_2 ^ 2 + \ alpha \ Big]."></div><br>  Using this approach, the authors reduced the error by 30% on <strong>datasets Labeled Faces in the Wild</strong> and <strong>YouTube Faces DB</strong> , which is undoubtedly very cool.  However, there is an approach and problems: <br><br><ul><li>  it takes <strong>a</strong> lot of data; </li><li>  slow learning; </li><li>  additional parameter <img src="https://tex.s2cms.ru/svg/%5Calpha" alt="\ alpha">  which is not clear how to choose; </li><li>  in many cases (mainly with a small amount of data) it manifests itself worse than <strong>softmax + bottleneck</strong> . </li></ul><br>  This is <strong>where TPE</strong> (Triplet Probabilistic Embedding) comes on the scene - the approach described in <a href="https://arxiv.org/abs/1604.05417">Triplet Probabilistic Embedding for Face Verification and Clustering</a> . <br><br>  Why enter an extra parameter <img src="https://tex.s2cms.ru/svg/%5Calpha" alt="\ alpha">  When can we demand respect for much simpler inequality?  Here it is: <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/d(f(x_a)%2C%20f(x_n))%20%3E%20d(f(x_a)%2C%20f(x_p))." alt="d (f (x_a), f (x_n)) &amp; gt; d (f (x_a), f (x_p))."></div><br><br>  It is simpler than the original and easily interpreted: we want the closest negative example to us to go further than the most positive positive example from us, but there should not be any gap between them.  Due to the fact that we do not stop updating the network when the distance <img src="https://tex.s2cms.ru/svg/%5Calpha" alt="\ alpha">  reached, <strong>embedding</strong> groups can be spaced apart <img src="https://tex.s2cms.ru/svg/R%5Ek" alt="R ^ k">  even better. <br><br>  We can calculate the probability that a triplet satisfies the given inequality: <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/p%20%3D%20%5Cfrac%7Be%5E%7Bd(f(x_a)%2C%20f(x_p))%7D%7D%7Be%5E%7Bd(f(x_a)%2C%20f(x_p))%7D%20%2B%20e%5E%7Bd(f(x_a)%2C%20f(x_n))%7D%7D." alt="p = \ frac {e ^ {d (f (x_a), f (x_p))}} {e ^ {d (f (x_a), f (x_p))} + e ^ {d (f (x_a), f (x_n))}}."></div><br>  Divide by <img src="https://tex.s2cms.ru/svg/e%5E%7Bd(f(x_a)%2C%20f(x_p))%7D" alt="e ^ {d (f (x_a), f (x_p))}">  : <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/p%20%3D%20%5Cfrac%7B1%7D%7B1%20%2B%20e%5E%7Bd(f(x_a)%2C%20f(x_n))%20-%20d(f(x_a)%2C%20f(x_p))%7D%7D%20%3D%20%5Csigma%20%5Cbig(%20d(f(x_a)%2C%20f(x_p))%20-%20d(f(x_a)%2C%20f(x_n))%20%5Cbig)" alt="p = \ frac {1} {1 + e ^ {d (f (x_a), f (x_n)) - d (f (x_a), f (x_p))}} = \ sigma \ big (d (f ( x_a), f (x_p)) - d (f (x_a), f (x_n)) \ big)"></div><br>  We will maximize the logarithm of probability, so the loss function will look like this: <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/L(x_a%2C%20x_p%2C%20x_n%2C%20f)%20%3D%20-%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi%3D1%7D%5EN%20%5Clog%20%5Csigma%20%5Cbig(%20d(f(x_a%5Ei)%2C%20f(x_p%5Ei))%20-%20d(f(x_a%5Ei)%2C%20f(x_n%5Ei))%20%5Cbig)." alt="L (x_a, x_p, x_n, f) = - \ frac {1} {N} \ sum_ {i = 1} ^ N \ log \ sigma \ big (d (f (x_a ^ i), f (x_p ^ i )) - d (f (x_a ^ i), f (x_n ^ i)) \ big)."></div><br>  And as a function <img src="https://tex.s2cms.ru/svg/f" alt="f">  The authors propose to use not a giant <strong>CNN</strong> , but a simple matrix: <img src="https://tex.s2cms.ru/svg/f(x)%20%3D%20Wx" alt="f (x) = Wx">  and teach it on the <strong>bottleneck</strong> signs we've already received.  Here are the results achieved by the authors of the work: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/b2f/e0f/381/b2fe0f381a1045d1aea39208781964f7.png" alt="original cnn-tpe results"></div><br>  As you can see, this approach works better than the original one and has many advantages: <br><br><ul><li>  less data required; </li><li>  extremely fast learning; </li><li>  no deep architecture required; </li><li>  can be used over existing and trained architecture. </li></ul><br>  We use this approach with you.  All you need is 20 lines of code: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">triplet_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(y_true, y_pred)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> -K.mean(K.log(K.sigmoid(y_pred))) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">triplet_merge</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(inputs)</span></span></span><span class="hljs-function">:</span></span> a, p, n = inputs <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> K.sum(a * (p - n), axis=<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">triplet_merge_shape</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(input_shapes)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (input_shapes[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>) a = Input(shape=(n_in,)) p = Input(shape=(n_in,)) n = Input(shape=(n_in,)) base_model = Sequential() base_model.add(Dense(n_out, input_dim=n_in, bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, weights=[W_pca], activation=<span class="hljs-string"><span class="hljs-string">'linear'</span></span>)) base_model.add(Lambda(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: K.l2_normalize(x, axis=<span class="hljs-number"><span class="hljs-number">1</span></span>))) a_emb = base_model(a) p_emb = base_model(p) n_emb = base_model(n) e = merge([a_emb, p_emb, n_emb], mode=triplet_merge, output_shape=triplet_merge_shape) model = Model(input=[a, p, n], output=e) predict = Model(input=a, output=a_emb) model.compile(loss=triplet_loss, optimizer=<span class="hljs-string"><span class="hljs-string">'rmsprop'</span></span>)</code> </pre><br>  If you want to use <strong>TPE</strong> in your projects, do not be lazy to read the original work, since I did not cover the main issue of training with triplets - the question of their selection.  There is enough random choice for our small task, but this is the exception rather than the rule. <br><br>  Let's train <strong>TPE</strong> on our <strong>bottleneck</strong> and take a look at the DET curve for the last time: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/a1a/60c/c76/a1a60cc766cf417e8d9b655a99942983.png" alt="det curve after tpe"></div><br>  <strong>An EER of</strong> 12% is very close to what we wanted.  This is twice as good as using CNN and 5 times better than random.  The result, of course, can be improved by using a deeper architecture and a large base, but for the understanding of the principle and such a result is satisfactory. <br><br>  Comparison of <strong>DET</strong> curves for all considered methods: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/df0/bd6/cd8/df0bd6cd862d494a819334a849a503a7.png" alt="det-curves of all considered methods"></div><br>  It remains to tie all kinds of machines and interfaces to our system, be it a web interface or an application on Qt, and the program for searching for identical faces in the photos is ready. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/f83/9d3/057/f839d305744d45e69660baf2c0986ce6.png" alt="sample application"></div><br>  The application can be found on <a href="https://github.com/meownoid/face-identification-tpe">GitHub</a> . <br><br>  Thanks for reading!  Put likes, subscribe to a profile, leave comments, learn machines good.  Additions are welcome. <br><br><h3>  Literature </h3><br><ul><li>  <a href="http://www.machinelearning.ru/wiki/images/a/a1/BayesML-2009-1.pdf">Various machine learning tasks</a> </li><li>  <a href="https://arxiv.org/abs/1503.03832">FaceNet: A Unified Embedding for Face Recognition and Clustering</a> </li><li>  <a href="https://arxiv.org/abs/1604.05417">Triplet Probabilistic Embedding for Face Verification and Clustering</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/317798/">https://habr.com/ru/post/317798/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../317786/index.html">Recursion formulas for calculating iterative summation errors of binary numbers of limited length</a></li>
<li><a href="../317788/index.html">Unit testing in modern teams</a></li>
<li><a href="../317792/index.html">Story about the first RDSFront & Meetup</a></li>
<li><a href="../317794/index.html">Security Week 50: crypto-socialization socialization, OpenVPN audit, Linux kernel vulnerability</a></li>
<li><a href="../317796/index.html">Creating tools for researching NES games</a></li>
<li><a href="../317800/index.html">Problems with hyper-v on wi-fi network</a></li>
<li><a href="../317802/index.html">‚ÄúUtilities‚Äù or ‚ÄúWindows Registry as a platform‚Äù</a></li>
<li><a href="../317804/index.html">Bug in Visual Studio 2017 RC using new C # 7 features</a></li>
<li><a href="../317806/index.html">Talk about Java 9 with Ivan Krylov on jug.msk.ru</a></li>
<li><a href="../317810/index.html">Sending HTML email</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>