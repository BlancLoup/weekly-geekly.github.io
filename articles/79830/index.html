<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Notes on NLP (Part 3)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="(Beginning: 1 , 2 ) Well, we approach the most interesting - analysis of sentences. This topic is multifaceted and multi-layered, so it's not very eas...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Notes on NLP (Part 3)</h1><div class="post__text post__text-html js-mediator-article">  (Beginning: <a href="http://habrahabr.ru/blogs/artificial_intelligence/79790/">1</a> , <a href="http://habrahabr.ru/blogs/artificial_intelligence/79819/">2</a> ) Well, we approach the most interesting - analysis of sentences.  This topic is multifaceted and multi-layered, so it's not very easy to approach it.  But the difficulties only harden :) Yes, and the weekend, the text is written easily ... <br><br>  Let's start with such a thing as <i>parsing sentences</i> (in English <i>parsing</i> ).  The essence of this process is to build a graph, ‚Äúin some way‚Äù reflecting the structure of the sentence. <a name="habracut"></a><br>  I say ‚Äúin any way‚Äù because today there is no single accepted system of principles on which the mentioned graph is built.  Even within the framework of one concept, the views of individual scientists on the relationship between words may differ (this is reminiscent of the differences in the interpretation of morphological phenomena, as discussed in the previous part). <br><br>  Probably, first of all, it is necessary to divide the methods of constructing a graph (usually a tree) of dependencies into phrase structure-based parsing and dependency parsing. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Representatives of the first school divide the sentence into "components", then each component is divided into its components - and so on until we reach the words.  This idea is well illustrated by a drawing from Wikipedia: <br><img src="https://habrastorage.org/getpro/geektimes/post_images/b08/8cc/0a9/b088cc0a921abbade53e1d54547e575d.png"><br><br>  Representatives of the second school connect dependent words to each other directly, without any auxiliary nodes: <br><img src="https://habrastorage.org/getpro/geektimes/post_images/67a/b10/baf/67ab10baf90e8f9e321c2e29998e5de8.png"><br><br>  At once I will say that my sympathies are on the side of the second approach (dependency parsing), but both of them deserve more detailed discussion. <br><br><h1>  Chomsky School </h1> The parsing "by constituents" clearly grew out of Chomsky's grammar.  If anyone does not know, Chomsky's grammar is a way to specify rules that describe sentences in a language.  With the help of such a grammar you can both generate phrases and analyze them.  For example, the following grammar describes a ‚Äúlanguage‚Äù consisting of an arbitrary number of letters a, followed by an arbitrary number of letters b: <br><br> <code>S -&gt; aS | bA | 'empty' <br> A -&gt; bA | 'empty' <br></code> <br>  Starting with the character S, you can generate any string of the form a ... ab ... b.  There is also a universal algorithm for parsing such a grammar.  After feeding him an input string and a set of grammar rules, you can get the answer - is the string the correct string within the given language or not.  You can also get a parse tree showing how the string is output from the initial character S. <br><br>  Suppose the aabb line matches this tree: <br><img src="https://habrastorage.org/getpro/geektimes/post_images/413/d6b/69d/413d6b69d1bb7a35922602423beb3573.png"><br><br>  The obvious advantage of this method is that Chomsky's grammar is a formalism known for a long time.  There are long-established parsing algorithms, known "formal properties" of grammars, i.e.  their expressive ability, complexity of processing, etc.  In addition, Chomsky grammars are successfully applied when compiling programming languages. <br><br>  Chomsky himself is primarily a linguist, and he tried on his work in a natural language, English, first of all.  Therefore, in English computational linguistics, the influence of his works is quite large.  Although ‚ÄúChomsky‚Äôs formalism,‚Äù as far as I know, is not used in the processing of texts in natural language (they are not developed enough for this), the spirit of his school lives on. <br><br>  A good example of a parser that builds such trees is <a href="http://nlp.stanford.edu:8080/parser/">Stanford parser</a> (there is an online demo). <br><br><h1>  Word relationship model </h1><br>  In general, this approach is also difficult to call particularly fresh.  All refer to the work of Lucien Tesniere of the fifties as a source.  Earlier thoughts are also mentioned (but from the same opera, what to call PLO PLO‚Äôs father, since he introduced the concept of the ‚Äúworld of ideas‚Äù, that is, abstract classes).  However, in computational linguistics, dependency parsing has long been in the background, while Chomsky's grammars were actively used.  Probably, the limitations of Chomsky‚Äôs approach particularly painfully hit languages ‚Äã‚Äãwith a looser (than in English) word order, so the most interesting work in the field of dependency parsing is still performed ‚Äúoutside‚Äù of the English-speaking world. <br><br>  The main idea of ‚Äã‚Äãdependency parsing is to connect dependent words to each other.  The center of almost any phrase is the verb (explicit or implied).  Further from the verb (action) you can ask questions: who does what he does, where he does and so on.  For attached entities, you can also ask questions (first of all, the question ‚Äúwhat‚Äù).  For example, for the above tree ‚ÄúI bought coffee in a big store,‚Äù you can reproduce such a chain of questions.  Root - bought (phrase action).  Who bought?  - I. What did you buy?  - Coffee.  Where did you buy it?  - In the shop.  In which shop?  - In big. <br><br>  Here, too, there are many technical subtleties and ambiguities.  You can handle the absence of the verb in different ways.  Usually, the verb ‚Äúto be‚Äù is still implied: ‚ÄúI [am] a student.‚Äù  In predicative sentences, the situation is more complicated: On the street is damp.  You can‚Äôt say it‚Äôs raw in the street :) It‚Äôs not always clear what depends on what and how to interpret it.  For example, "I will not go to work today."  How does the particle ‚Äúnot‚Äù compare with other words?  Alternatively, we can assume that the verb of ‚Äúnon-actions‚Äù is ‚Äúnot coming‚Äù (even if there is no such thing in Russian, but it is appropriate in meaning).  It is not entirely clear how to sculpt homogeneous members connected by union.  "I bought a coffee and a bun."  For example, you can sculpt to ‚Äúbought‚Äù the word ‚Äúand‚Äù, and to ‚Äúand‚Äù attach already ‚Äúcoffee‚Äù and ‚Äúbun‚Äù.  But there are other approaches.  A rather subtle moment arises in the interaction of words that form a certain unity: "I will go to work."  It is clear that ‚ÄúI will walk‚Äù is essentially a single verb (that is, an action) of the future tense, it is simply created in two words. <br><br>  If you want to look at this analyzer in action - I can advise the site of the company <a href="http://www.connexor.eu/technology/machinese/demo/syntax/">Connexor</a> . <br><br>  What dependency parsing is attractive?  Give different arguments.  For example, it is said that by connecting words together, we do not create additional entities, and, therefore, we simplify further analysis.  In the end, parsing is just another stage of word processing, and then you have to imagine what to do with the resulting tree.  In a sense, the dependency tree is ‚Äúcleaner‚Äù because it shows clear semantic links between the elements of a sentence.  Further, it is often claimed that dependency parsing is more suitable for languages ‚Äã‚Äãwith free word order.  In Chomsky, all the dependent blocks somehow really end up next to each other.  Here, in theory, it is possible to have connections between words at different ends of a sentence (although this is not technically so simple, but more on that later).  In principle, these arguments are enough for me to join Tenier‚Äôs camp :) <br><br>  It must be said that there are formal proofs of the proximity of the resulting trees.  Somewhere, the theorem that a tree of one species can be converted into a tree of another type and vice versa was slipping.  But in practice it does not work.  At least in my memory, no one tried to get a dependency tree by transforming the output of Stanford parser.  Apparently, not everything is so simple, and the mistakes are multiplying ... first, the Stanford parser makes a mistake, then the conversion algorithm makes a mistake ... and what happens at the end?  Error on error. <br><br>  (UPD: The above-mentioned guys from Stanford still <a href="http://nlp.stanford.edu/pubs/LREC06_dependencies.pdf">tested the</a> method of converting the output of their parser into a dependency structure. However, I must note that with this conversion, only <i>projective</i> trees are obtained, which are discussed <a href="http://habrahabr.ru/blogs/artificial_intelligence/79882/">in the fifth part</a> ). <br>  Probably enough for today.  We continue in the next section. </div><p>Source: <a href="https://habr.com/ru/post/79830/">https://habr.com/ru/post/79830/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../79820/index.html">CSS hack analysis for different browser versions</a></li>
<li><a href="../79821/index.html">Ray of hate - this time Beeline</a></li>
<li><a href="../79826/index.html">Copyright 2010 Updates</a></li>
<li><a href="../79827/index.html">How to teach ESXi to use USB disks as storage?</a></li>
<li><a href="../79828/index.html">New Year greetings from Google</a></li>
<li><a href="../79831/index.html">Registration for tfile.ru and AudioBit.ru is open.</a></li>
<li><a href="../79833/index.html">Pyastra - Python code translator to PIC architecture assembler</a></li>
<li><a href="../79835/index.html">Rating of Runet Trackers</a></li>
<li><a href="../79836/index.html">PC-BSD 8.0-BETA Released</a></li>
<li><a href="../79839/index.html">A passion for programming. Chapter 2. Acknowledgments</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>