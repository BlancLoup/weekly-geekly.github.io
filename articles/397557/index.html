<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Machine Neural Network is taught on realistic computer games</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Images from the computer game Grand Theft Auto V and semantic markup for learning neural network machine vision 

 Neural networks set new records in ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Machine Neural Network is taught on realistic computer games</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/47e/fcc/506/47efcc5062114f4b8ecc13630c9e361e.jpg"><br>  <i>Images from the computer game Grand Theft Auto V and semantic markup for learning neural network machine vision</i> <br><br>  Neural networks set new records in almost all competitions in computer vision, and are also increasingly used in other AI applications.  One of the key components of such incredible efficiency of neural networks is the availability of large data sets for their training and evaluation.  For example, Imagenet Large Scale Visual Recognition Challenge (ILSVRC) with more than 1 million images is used to evaluate modern neural networks.  But judging by the latest results (ResNet shows the result of only <a href="https://arxiv.org/abs/1512.03385">3.57% errors</a> ), researchers will soon have to compile more extensive data sets.  And then - even more extensive.  By the way, annotating such photos is a lot of work, some of which have to be done manually. <br><br>  Some computer vision system developers offer an alternative way to train and test such systems.  Instead of manually annotating training photos, they use synthesized frames from realistic computer games. <br><a name="habracut"></a><br>  This is quite a logical approach.  In modern games, graphics have reached such a level of realism that the synthesized images are slightly different from the photos of the real world.  At the same time, the game engine can generate an infinite number of such frames - this immediately fundamentally solves the problem of collecting millions of photos for training and evaluating a neural network. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Although the game engine uses a finite number of textures, there is a wide variety of combinations of angles of view, lighting, weather, and level of detail that provides a sufficient variety of data sets. <br><br>  This year, at once, two groups of researchers have tested in practice whether it is possible to use the generated frames from computer games for teaching neural networks.  A group of researchers from the Faculty of Informatics at the University of British Columbia (Canada) published a <a href="https://arxiv.org/abs/1608.01745">scientific article</a> for which they collected more than 60,000 frames from a computer game with road views similar to the <a href="http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/">CamVid</a> and <a href="https://www.cityscapes-dataset.com/">Cityscapes</a> data sets.  The researchers were able to prove that the neural network after training on synthetic images shows a similar level of errors, as after training on real photos.  Moreover, training on synthesized images using real photos shows an even better result. <br><br>  All 60,000 frames were made in virtual sunny weather, at a virtual time of 11:00, with a resolution of 1024 √ó 768 and maximum graphics settings (the name of the game was not disclosed due to copyright fears).  An unmanned car accidentally drove through the game streets, following the rules of the road.  Frames were taken 1 time per second.  Each of them is accompanied by automatic semantic segmentation (sky, pedestrian, cars, trees, the background - the segmentation is absolutely accurate and taken from the game), a deep image (depth image, a map with the marking of objects), as well as surface normals. <br><br>  In addition to the basic VG dataset, the researchers made another VG + dataset with a large amount of semantic information, not limited to five labels ‚Äî here the segmentation is not accurate.  The markup was done automatically with <a href="https://arxiv.org/abs/1511.02680">SegNet</a> . <br><br><img src="https://habrastorage.org/files/192/934/07c/19293407c01d4ab09be60d46e67c09e5.png"><br>  <i>Tightly tagged frames from the VG + set</i> <br><br>  To compare the effectiveness of neural network learning, CamVid and Cityscapes data sets (five tags), as well as CamVid + ‚Äã‚Äãand Cityscapes + data sets with extended tag sets, were prepared. <br><br><img src="https://habrastorage.org/files/d2b/3dd/1e0/d2b3dd1e02e042ebb4ec45ea61c20ba4.jpg"><br>  <i>Original CamVid photos with annotations</i> <br><br><img src="https://habrastorage.org/files/65f/888/ed1/65f888ed140f4456942204011e0ffd4a.png"><br>  <i>Two random images of Cityscapes + set with detailed annotations</i> <br><br>  For the semantic classification, the <a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf">Long convolutional neural network</a> with the simple FCN8 architecture over the 16-layer <a href="">Simonyan and Zisserman VGG was used</a> . <br><br>  The researchers conducted several experiments to evaluate the efficiency of object recognition by the neural network, which was trained on different data sets.  In almost all cases, a neural network trained on synthetic data showed a better result than a neural network trained on real photographs.  She showed the best result even when checking on real photos. <br><br>  For example, the table shows the assessment of the work of identical neural networks trained on three data sets (real photos, synthetic data from the game, mixed set) when recognizing objects on real photos from the CamVid + ‚Äã‚Äãand Cityscapes + sets. <br><br><img src="https://habrastorage.org/files/a64/eac/b8a/a64eacb8a5404f64897af9eed4dab2eb.png"><br><br>  As you can see, when teaching a neural network, it is best to complement synthetic images from a computer game with real photographs. <br><br>  The scientific article was <a href="https://arxiv.org/abs/1608.01745">published</a> on August 5, 2016 on arXiv.org, the second version - August 15 ( <a href="https://arxiv.org/pdf/1608.01745v2">pdf</a> ). <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  In addition to researchers from the University of British Columbia, <a href="http://download.visinf.tu-darmstadt.de/data/from_games/">another group of scientists from Darmstadt Technical University (Germany) and Intel Labs did similar work</a> almost simultaneously.  They took 24,966 frames from Grand Theft Auto V, an open world computer game, for training. Researchers came up with the same result: when used for training a data set of 2/3 of synthetic images and 1/3 of CamVid photos, accuracy The recognition turns out to be higher than only when using CamVid photos. <br><br><img src="https://habrastorage.org/files/19f/38c/bc3/19f38cbc32c74e2fa2af351b47113b0b.png"><br>  <i>Accuracy of recognition of various objects in photos from the CamVid set when learning by conventional methods and using frames from GTA V (bottom line)</i> <br><br>  At the same time, semi-automatic annotation in a specially designed editor significantly reduces the time for preparing a dataset for training a neural network.  For example, annotating one CamVid photo takes 60 minutes, one Cityscapes photo takes 90 minutes, and semi-automatic annotating a GTA V frame takes only 7 seconds, on average ( <a href="">video, editor demonstration</a> ). <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/JGAIfWG2MQQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  The work of researchers from Darmstadt Technical University and Intel Labs prepared for the European Conference on Computer Vision <a href="http://www.eccv2016.org/">ECCV'16</a> (October 11-14) and <a href="http://download.visinf.tu-darmstadt.de/data/from_games/data/eccv-2016-richter-playing_for_data.pdf">published</a> on the university website.  The authors have laid out the <a href="">source code</a> for reading labels and <a href="http://download.visinf.tu-darmstadt.de/data/from_games/">complete data sets</a> : both source photos and deep images with semantic markup.  The source code of the editor for annotating is likely to be published in the future. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  Thanks to the progress in creating realistic computer games, developers of artificial intelligence systems will have at their disposal an excellent platform for learning computer vision systems.  These systems will be used in unmanned vehicles and robots. <br><br>  Perhaps computer games can be used not only for computer vision, but also for creating natural patterns of behavior in society.  Only when learning AI should be careful to choose the game. </div><p>Source: <a href="https://habr.com/ru/post/397557/">https://habr.com/ru/post/397557/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../397547/index.html">Byrobot Petrone: the best (imho) drones for training kids. And for fights</a></li>
<li><a href="../397549/index.html">ProDOS 2.4 for Apple II: the first in 23 years OS update for Apple II</a></li>
<li><a href="../397551/index.html">Happy birthday, Stanislav Lem</a></li>
<li><a href="../397553/index.html">Kick NOW! The future is closer with Kickstarter</a></li>
<li><a href="../397555/index.html">Smart Home Apple HomeKit. First impressions</a></li>
<li><a href="../397559/index.html">European researchers have created a new composite material with variable transparency</a></li>
<li><a href="../397561/index.html">Audio Digest 9: Sound, Music, and Audio Blogging</a></li>
<li><a href="../397563/index.html">Proved the authenticity of the Codex Grolier - the fourth surviving Mayan Code</a></li>
<li><a href="../397565/index.html">How Lyft sees road transport in 10 years</a></li>
<li><a href="../397567/index.html">Nokia Bell Labs achieves 1Tbps data transfer over fiber</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>