<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Competition Pri-matrix Factorization on DrivenData with 1TB of data - how we took 3rd place (translation)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! I present to your attention the translation of the article " Animal detection in the jungle - 1TB + of data, 90% + . 
 Or what we learned ho...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Competition Pri-matrix Factorization on DrivenData with 1TB of data - how we took 3rd place (translation)</h1><div class="post__text post__text-html js-mediator-article"><p>  Hi, Habr!  I present to your attention the translation of the article " <a href="https://spark-in.me/post/jungle-animal-trap-competition-part-one">Animal detection in the jungle - 1TB + of data, 90% +</a> . </p><br><h1 id="ili-chemu-my-nauchilis-kak-vyigryvat-prizy-v-takih-sorevnovaniyah-poleznye-sovety--nekotorye-melochi">  Or what we learned how to win prizes in such competitions, useful tips + some trivia </h1><br><h2 id="tldr">  Tldr </h2><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  <em>The essence of the competition - for example, this is a random video with a leopard.</em>  <em>All videos last 15 seconds, and there are 400 thousand ...</em> </p><br><p><img src="https://pics.spark-in.me/upload/050784ca453863ce7d1f158f86acf9a4.jpg"><br>  <em>Final results at 3 am, when the competition ended - I was on the train, but my colleague submitted the application 10 minutes before the end of the competition</em> </p><br><p>  If you are interested to find out how we did it, what we learned, and how you can participate in such a one, then please under the cat. </p><a name="habracut"></a><br><h2 id="0-kriterii-vybora-sorevnovaniya-i-struktura-posta">  0. Criteria for choosing the competition and the structure of the post </h2><br><p>  In our <a href="https://spark-in.me/"><strong>blog,</strong></a> we have already written <strong><a href="https://spark-in.me/post/fish-object-detection-ssd-yolo">how and why to</a></strong> participate in competitions. </p><br><p>  Regarding the choice of this competition, it can be said that at the end of 2017 most of the competitions on the Kaggle were not so interesting and / or gave too little money at almost zero or so learning value and / or were with 100+ participants who sent their results on the first day because the last competitions were not so difficult.  Just make 20 models on your own.  The most vivid examples from the last <strong><a href="http://kaggle.com/c/cdiscount-image-classification-challenge">one</a></strong> <strong><a href="https://www.kaggle.com/c/statoil-iceberg-classifier-challenge">are</a></strong> interesting only in theory, and are nothing more than a casino with a GPU instead of chips. <br>  For these reasons (a decent prize, the lack of strong marketing support due to 100+ simple applications on the first day, challenge, interestingness and novelty) - we chose this <strong><a href="https://www.drivendata.org/competitions/49/deep-learning-camera-trap-animals/">competition</a></strong> . </p><br><div class="spoiler">  <b class="spoiler_title">Under the spoiler a quick picture, so as not to go to the site</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/le/xb/bj/lexbbjto8xgqkgbnxmn8vte8m7a.png"></p></div></div><br><p>  <strong>In a nutshell - you have ~ 200k videos for learning, ~ 80k videos for the test (and 120k untagged videos!).</strong>  <strong>Videos are marked entirely, in 24 classes of animals.</strong>  <strong>That is, video N has a certain class of animal (or its absence).</strong>  <strong>That is, video1 is class1, video2 is class2, etc.</strong> </p><br><p>  In this competition, I participated along with the <a href="http://t.me/thinline72">subscriber of</a> my telegram channel ( <a href="https://t.me/snakers4">channel</a> , <a href="http://snakers41.spark-in.me/">webcast</a> ).  For brevity, this post will be structured as follows: </p><br><ul><li>  TLDR section for people who want to quickly see working solutions and links; </li><li>  Code and jupyter laptops that are available <strong><a href="https://github.com/snakers4/jungle-dd-av-sk">here</a></strong> .  The code is encapsulated and laptops use <a href="https://github.com/ipython-contrib/jupyter_contrib_nbextensions">Jupyter extensions</a> (codefolding, table of content, collapsiblle headers) for readability - but almost no attention was paid to turning the code into a tutorial, so read at your own risk; </li><li>  All the code I wrote was written in Pytorch, my colleague mainly used Keras. </li></ul><br><h2 id="1-tldr1---pervonachalnyy-naivnyy-podhod--sbor-poleznyh-ssylok-po-teme">  1. TLDR1 - initial naive approach + collection of useful links on the topic </h2><br><p>  To get started, I put together a list of useful links in the order in which you probably should read them in order to solve a similar problem.  To begin with, you should be familiar with computer vision, basic mathematics (linear algebra, mathematical analysis and numerical methods), machine learning and basic architectures in it. </p><br><div class="spoiler">  <b class="spoiler_title">Under the spoiler, a lot of links to articles, papers and code samples</b> <div class="spoiler_text"><h3 id="ssylki-dlya-starta">  Links to start: </h3><br><ul><li>  A good <a href="https://blog.coast.ai/five-video-classification-methods-implemented-in-keras-and-tensorflow-99cad29cc0b5"><strong>post</strong></a> about simple, naive, but still effective models; </li><li>  <a href="http://drivendata.co/blog/pri-matrix-factorization-benchmark/"><strong>Example</strong></a> drivendata </li></ul><br><h3 id="luchshie-stati-pro-lstm-okazalos-chto-lstm--gru-ne-byli-luchshim-resheniem-no-v-nachale-my-igralis-s-nimi-chto-dalo-nam-nekotoryy-bonus-v-konechnom-reshenii">  The best articles about LSTM (it turned out that LSTM / GRU was not the best solution, but in the beginning we played with them, which gave us some bonus in the final solution): </h3><br><ul><li>  Understanding the concept of LSTM - <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"><strong>1</strong></a> , <a href="https://www.quora.com/What-is-an-intuitive-explanation-of-LSTMs-and-GRUs"><strong>2</strong></a> , <a href="https://www.slideshare.net/ananth/recurrent-neural-networks-lstm-and-gru"><strong>3</strong></a> , <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"><strong>4</strong></a> ; </li><li>  Understanding the concept of attention - <a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/"><strong>1</strong></a> , <a href="https://distill.pub/2016/augmented-rnns/"><strong>2</strong></a> in RNN; </li><li>  LSTM visualizations for text models - <a href="http://yerevann.github.io/2017/06/27/interpreting-neurons-in-an-LSTM-network/"><strong>1</strong></a> , <a href="http://blog.echen.me/2017/05/30/exploring-lstms"><strong>2</strong></a> . </li></ul><br><h3 id="primery-implementaciy-modeley-vyshe-na-pytorch">  Examples of model implementations above on Pytorch: </h3><br><ul><li>  Base examples - <a href="https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/recurrent_neural_network/main-gpu.py"><strong>1</strong></a> , <a href="https://gist.github.com/ririw/4f3a3b3c1828e6d781b624f378890cb0"><strong>2</strong></a> ; </li><li>  Advanced examples + attention - <a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch"><strong>1</strong></a> , <a href="https://github.com/MaximumEntropy/Seq2Seq-PyTorch/blob/master/model.py"><strong>2</strong></a> , <a href="http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"><strong>3</strong></a> , <a href="https://github.com/huggingface/torchMoji/blob/master/torchmoji/attlayer.py"><strong>4</strong></a> ; </li><li>  Interesting <a href="https://medium.com/huggingface/understanding-emotions-from-keras-to-pytorch-3ccb61d5a983"><strong>article</strong></a> about attention for Keras and Pytorch. </li></ul><br><h3 id="akademicheskie-stati-po-teme">  Academic related articles </h3><br><p>  It should be noted that academic works usually complicate and / or they are poorly reproduced and / or solve complex general problems or, on the contrary, contrived things - so read them with some degree of skepticism. </p><br><p>  Anyway, these works contain basic initial architectures and note that something like attention or learnable pooling increases accuracy: </p><br><ul><li>  The best-fit work of <strong><a href="http://cs.stanford.edu/people/karpathy/deepvideo">Large-scale Video Classification with Convolutional Neural Networks</a></strong> is too cool for our purposes, but contains useful examples of initial architectures; </li><li>  A somewhat naive but simple example of <a href="http://arxiv.org/abs/1706.04488"><strong>YouTube-8M Video Large-Scale Understanding with Deep Neural Networks</strong></a> for working with video classification; </li><li>  Work <a href="http://arxiv.org/abs/1706.06905"><strong>Learnable pooling with Context Gating for video classification</strong></a> , which notes that attention / learnable pooling can be useful for classifying videos; </li><li>  An interesting idea in <a href="https://arxiv.org/abs/1711.08238"><strong>Multi-Level Recurrent Residual Networks for Action Recognition</strong></a> is to use MRRN to identify fast movements; </li><li>  Theoretically, this work by <a href="https://arxiv.org/abs/1709.01829"><strong>Soft Proposal Networks for Weakly Supervised Object Localization</strong></a> would allow us <strong>to get bboxes with objects on video and train them to detect</strong> , but the code that the authors provided (despite the fact that python is stated there) contains custom C ++ drivers for CUDA, so we decided not to go this route. </li></ul></div></div><br><h2 id="2-tldr2---nailuchshie-rabotayuschie-podhody-nashi-payplayny-i-payplany-drugih-prizerov">  2. TLDR2 - the best working approaches.  Our pipelines and other prize winners </h2><br><h3 id="21-nailuchshie-podhody">  2.1 Best Practices </h3><br><h3 id="nash-3e-mesto">  Our (3rd place): </h3><br><p>  Approximately half of the competition, I teamed up with Savva Kolbachev.  Initially, before moving on to full-size videos, I tried some pieces with motion detection, gluing several 64x64-sized videos into one image, matrix decomposition.  Sawa tried to use LSTM + some basic encoders for 64x64 video, as he had a car with a 780GTX card so he could only use micro data (64x64 3GB 2FPS), but even that seemed to be enough to score good points for hitting in the top 10 list. </p><br><p>  <strong>Under the spoilers it will be possible to see briefly (in more detail below) our final pipeline and pipeline of the first place.</strong>  <strong>And all sorts of other things.</strong> </p><br><div class="spoiler">  <b class="spoiler_title">After trial attempts and errors, we stopped on the following pipeline:</b> <div class="spoiler_text"><p>  First, select 3 or 4 feature sets from the list of the best encoders (we tried different resnet with and without additions, inception4, inception-resnet2, densenet, nasnet and other models) - 45 frames per video.  Use metadata in the model.  Use a layer of attention in the model.  Then load all the obtained vectors into finite fully connected layers. </p><br><p>  The final solution gave ~ 90% + accuracy and 0.9 ROC AUC points for each class. </p><br><p><img src="https://pics.spark-in.me/upload/e54f4817c060190c4d1c80bd19187a67.jpg"><br>  <em>Several graphs (GRU + 256 hidden layers) among the best encoders - lines on the graphs from top to bottom - additional training in inception resnet2, densenet, resnet, inception4, inception-resnet2</em> </p><br><h3 id="dmytro-1e-mesto---ochen-prostoy-payplayn">  Dmytro (1st place) - very simple pipeline: </h3><br><ul><li>  Several pre-trained models (resnet, inception, xception) on random 32 frames from the video; </li><li>  Prediction of classes for 32 frames for each video; </li><li>  Calculate the histogram for these predictions to exclude time from the model; </li><li>  Run a simple meta model on these results. </li></ul></div></div><br><div class="spoiler">  <b class="spoiler_title">What we missed / tested incorrectly, again more detailed below</b> <div class="spoiler_text"><p>  When we trained the end-to-end model (pre-trained by Imagenet encoder + pre-trained GRU) - we got low error on the training set (0.03) and high on validation (0.13), which we considered a weakness of this approach.  It turned out that Dmytro got the same results, but nevertheless finished the experiment.  We abandoned this approach due to time constraints, but when we needed to make a selection of features from the pre-trained grid, I got more or less the same thing on my tests, but Sawa did not get the necessary values ‚Äã‚Äãon his pipeline.  This led to the fact that we spent the last week trying to use new encoders instead of donating what we have already received.  We also did not have time to try the additional training of encoders that we used.  And try to train pooling / attention for the features that we got from the encoders. </p><br><div style="text-align:center;"><img src="https://pics.spark-in.me/upload/aa1fed76edc3edcde66c7ff228320e5f.png"></div><br><p>  <i>An example of improvements that we tried - no increase compared with the conventional approach.</i> </p></div></div><br><h3 id="22-klyuchevye-killer-fichi-evrika-i-kontr-intuitivnye-momenty">  2.2 Key killer features, eureka and counter-intuitive moments </h3><br><div class="spoiler">  <b class="spoiler_title">Counter-intuitive moments:</b> <div class="spoiler_text"><ul><li>  Metadata from the video pushed into a simple grid gives ~ 0.6 points on the leader board (60-70% accuracy); </li><li>  I earned ~ 0.4 points on the board using only 64x64 videos, while another guy from the community claimed that he did only ~ 0.03 doing the same thing; </li><li>  A simple layer of minimax gave ~ 0.06-0.07 points; </li><li>  The poor performance of the end-to-end base encoder does not necessarily result in the poor performance of the entire pipeline. </li></ul></div></div><br><div class="spoiler">  <b class="spoiler_title">Key killer features and erick:</b> <div class="spoiler_text"><ul><li>  Without additional training - obtained features with skip-connections (that is, selection not only from the last fully connected layer, but also some intermediate features) work better than those selected only from the last layer.  On my GRU-256 benchmark, this gave ~ 0.01 gain points without any stakes; </li><li>  A significant increase was obtained by simple features - metadata + regular minimax; </li><li>  Even if some model behaves worse than the next - their ensemble will be better because of log loss. </li></ul></div></div><br><h2 id="3-bazovyy-analiz-dannyh-i-opisanie-metriki">  3. Basic data analysis and description, metrics </h2><br><h3 id="31-dataset">  3.1 Dataset </h3><br><p>  Basic analysis can be found <a href="https://github.com/snakers4/jungle-dd-av-sk/blob/master/notebooks/jungle.ipynb"><strong>here</strong></a> .  As I said before, the whole dataset weighs 1TB and the organizers of the competition shared it through a torrent, but you could download it directly (but rather slowly) </p><br><p>  There were three versions of dataset available: </p><br><ul><li>  Full 1TB </li><li>  Approximately 3GB of 64x64 video with 2FPS - surprisingly enough to achieve 75-80% accuracy and 0.03 points! </li><li>  16x16 version. </li></ul><br><p>  In general, dataset was of good quality - each video was poorly annotated, but considering the size, it was still good - responsive support, it was possible to swing through torrents (although it was dammed quite late with one seeder in the USA), and the validation part was just the coolest thing I've seen.  Our validation was always about 5% less than what we received on the board.  The whole competition took 2 months, but in my case it took 2-3 weeks only to download and unpack the archive. </p><br><h3 id="32-bazovyy-analiz">  3.2 Basic analysis </h3><br><p>  To be honest, I didn't really understand much about dataset, simply because it was huge, but it was easy to get some key insights. </p><br><div class="spoiler">  <b class="spoiler_title">Some pictures of basic analysis</b> <div class="spoiler_text"><div style="text-align:center;"><img src="https://pics.spark-in.me/upload/31b767fdeb5ed456856b15d014f15f34.png"></div><br><p>  <em>Actually dataset</em> </p><br><div style="text-align:center;"><img src="https://pics.spark-in.me/upload/b3326f2d510720618cc289f40a91436e.jpg"></div><br><p>  <em>Class labels - data is very unbalanced.</em>  <em>On the other hand, train / test was made cool - so the distribution was the same here and there.</em> </p><br><div style="text-align:center;"><img src="https://pics.spark-in.me/upload/b90ca3091685e4cd78f7e7750dde6476.jpg"></div><br><p>  <em>Some distributions</em> </p><br><div style="text-align:center;"><img src="https://pics.spark-in.me/upload/e1f92389c19ede829598d7015c2bd9d0.jpg"></div><br><p>  <em>Main component analysis - easy to distinguish day and night</em> </p><br><div style="text-align:center;"><img src="https://pics.spark-in.me/upload/e2b9c2cc353450fadb99ec2653d8c8c8.png"></div><br><p>  <em>Video size in bytes on the log10 scale.</em>  <em>Video without animals (blue) and with animals (orange).</em>  <em>No wonder - due to compression, videos without animals are smaller</em> </p></div></div><br><h3 id="33-metrika">  3.3 Metric </h3><br><p><math> </math> $$ display $$ AggLogLoss = - \ frac {1} {MN} \ sum_ {m = 1} ^ {M} \ sum_ {n = 1} ^ N [y_ {nm} \ log (\ hat y_ {nm} ) + (1-y_ {nm}) \ log (1- \ hat y_ {nm})] $$ display $$ </p><br><p>  Metric is just the average logloss for all 24 classes.  This is good because there is such a metric in almost every DL package.  It is quite easy to get normal points with her at once, but at the same time she is unintuitive and beloved.  Very sensitive to the slightest amount of false positive predictions.  Well, just adding new models improves this metric, which is not very good in theory. </p><br><h2 id="4-kak-my-reshali-zadachu-i-nashe-reshenie">  4. How we solved the problem and our solution </h2><br><h3 id="41-fichering">  4.1 Featureing </h3><br><p>  As we noted earlier, I selected features from different pre-trained models, like resnet152, inception-resnet, inception4 and nasnet.  We found that it is best to select features not only from the last layer before fully connected, but also from skip-connections. </p><br><p>  We also identified metadata like the width, height, and size of both datasets, micro and original.  Interestingly, their combination works much better than just the original dataset.  As a rule, metadata was very useful for identifying empty / non-empty videos, because empty videos usually weighed significantly less.  This made it possible to separate more than 25% of empty videos, which, by the way, is the largest class in terms of number: <br><img src="https://pics.spark-in.me/upload/08ae4bbd74bc5595c5f475ab4211de14.png" alt="https://pics.spark-in.me/upload/08ae4bbd74bc5595c5f475ab4211de14.png"></p><br><h3 id="42-razbienie-obuchayuschey-i-validacionnoy-vyborki">  4.2 Division of training and validation samples </h3><br><p>  The distribution of classes was very unbalanced.  For example, for the "lion" there were only two examples from the entire ~ 200k sample!  Moreover, these were videos with several animal tags, so this needed to be done in a more specific split.  Fortunately, we had the <a href="https://www.kaggle.com/c/planet-understanding-the-amazon-from-space">Planet</a> code from the <a href="https://www.kaggle.com/c/planet-understanding-the-amazon-from-space">Planet: Understanding the Amazon from Space competition</a> .  With such a break, our test score was always a bit worse on the board than on the board: <br><img src="https://pics.spark-in.me/upload/5718c075db29f0e8b6bd45c6cf70aa5a.png" alt="splintting"></p><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">multilabel_stratified_kfold_sampling</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(Y, n_splits=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">10</span></span></span></span><span class="hljs-function"><span class="hljs-params">, random_state)</span></span></span><span class="hljs-function">:</span></span> train_folds = [[] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_splits)] valid_folds = [[] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_splits)] n_classes = Y.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>] inx = np.arange(Y.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]) valid_size = <span class="hljs-number"><span class="hljs-number">1.0</span></span> / n_splits <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> cl <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">0</span></span>, n_classes): sss = StratifiedKFold(n_splits=n_splits, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, random_state=random_state+cl) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> fold, (train_index, test_index) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(sss.split(inx, Y[:,cl])): b_train_inx, b_valid_inx = inx[train_index], inx[test_index] <span class="hljs-comment"><span class="hljs-comment"># to ensure there is no repetetion within each split and between the splits train_folds[fold] = train_folds[fold] + list(set(list(b_train_inx)) - set(train_folds[fold]) - set(valid_folds[fold])) valid_folds[fold] = valid_folds[fold] + list(set(list(b_valid_inx)) - set(train_folds[fold]) - set(valid_folds[fold])) return np.array(train_folds), np.array(valid_folds)</span></span></code> </pre> </div></div><br><h3 id="43-chto-my-probovali-delat">  4.3.  What we tried to do </h3><br><p>  After selecting the features, we had a matrix for each video of the form (45.3000), where 45 is the number of frames, and 3000 is the number of features for each frame. </p><br><h4 id="chto-my-poprobovali-i-dobavili-v-konechnoe-reshenie">  What we tried and added to the final solution: </h4><br><ul><li>  We started with different RNNs, but they did not give us the best points and took a lot more time to learn (even for Keras, the implementation of CuDNN).  But we used several trained RNNs in the final architecture. </li><li>  Using RNN, we decided to try attention and this gave us a good improvement.  At first, we used <strong><a href="https://github.com/bfelbo/DeepMoji/blob/master/deepmoji/attlayer.py">this implementation</a></strong> on top of 2xCuDNNGRU with simple features.  But as soon as we learned that the removal of the RNN does not greatly impair the results, we began to spend more time searching for different Keras implementations of attention and found <strong><a href="https://gist.github.com/cbaziotis/6428df359af27d58078ca5ed9792bd6d">this one</a></strong> .  Attention worked for us like learnable pooling, which allowed us to move from matrix size (45,3000) to vector video size 3000. </li><li>  Max-min pooling gave a consistent increase in points and it worked much better than the default max \ avg pooling.  The idea was to get a better idea of ‚Äã‚Äãhow features change over time. </li><li>  Combining features from different models gave a better result, but it took more time and resources to learn. </li><li>  Training a separate model for recognizing empty / non-empty videos.  This gave a slightly better result for this particular class, but it was meaningful, since the class was the most numerous. </li><li>  Pseudo-marking (splitting) has always given a bit of growth for our models.  Usually the training went on a plateau after 12-15 epochs.  It seems that the pseudo-markup introduced some variety to the training and improved validation. </li><li>  Stakanie models) </li></ul><br><h3 id="chto-my-poprobovali-no-eto-ne-srabotalo-dlya-resheniya-zadachi">  What we tried, but it did not work to solve the problem: </h3><br><ul><li>  Focal loss: in general, it works quite well and adds some variety to the models, but was not very useful according to the competition metric.  But we will definitely try this for other projects. </li><li>  Deal with unbalanced classes: does not help according to metrics. </li><li>  1D / 3D Convolutions </li><li>  Optical flow </li></ul><br><h3 id="44-opisanie-konechnogo-resheniya">  4.4 Description of the final solution </h3><br><p><img src="https://habrastorage.org/webt/jb/oc/-f/jboc-fu8frqk-dffpjsesujpppu.jpeg" alt="Ultimate architecture"></p><br><p>  We trained 9 models, each with 5 folds, using dedicated features: </p><br><ul><li>  3 CuDNNGRU models with AttentionWeightedAverage and Max-Min polling layers based on resnet152, inception-resnet and inception4. </li><li>  4 models with Attention and Max-Min pooling layers based on resnet152, inception-resnet, inception4 and nasnet. </li><li>  1 concat model with Attention and Max-Min pooling layers (similar to the previous ones) but combined features from resnet152, inception-resnet, inception4. </li><li>  1 concat model only for empty \ not empty predictions. </li></ul><br><p>  We found that 15 epochs + 5 epochs for pseudo-markup should be enough to get a pretty decent result.  The batch size was 64 (44/20) for single-feature models and 48 (32/16) for nasnet and concat models.  In general, the larger batch was better.  Size selection depended on I / O disk and learning speed.  For the final result, the predictions from the models were put together through 2 fully connected layers of the metamodel using 10 folds.) </p><br><p>  We trained 9 models, each with 5 folds, using dedicated features: </p><br><ul><li>  3 CuDNNGRU models with <em>AttentionWeightedAverage</em> and <em>Max-Min polling</em> layers, based on <em>resnet152</em> , <strong>inception-resnet</strong> and <em>inception4</em> . </li><li>  4 models with <em>Attention</em> and <em>Max-Min pooling</em> layers based on <em>resnet152</em> , <em>inception-resnet</em> , <em>inception4</em> and <em>nasnet</em> . </li><li>  1 <em>concat</em> model with <em>Attention</em> and <em>Max-Min pooling</em> layers (similar to previous ones) but combined features from <em>resnet152</em> , <em>inception-resnet</em> , <em>inception4</em> . </li><li>  1 <em>concat</em> model for empty / non-empty predictions only. </li></ul><br><p>  We found that 15 epochs + 5 epochs for pseudo-markup should be enough to get a pretty decent result.  The batch size was 64 (44/20) for single-feature models and 48 (32/16) for nasnet and concat models.  In general, a bigger batch was always better.  Size selection depended on I / O disk and learning speed.  To get the final result, the predictions from the models were put together through 2 fully connected layers of the metamodel using 10 folds. </p><br><h2 id="5-alternativnye-podhody">  5. Alternative approaches </h2><br><p>  As far as we know, a couple more things could have been done.  For example, try detecting objects on 64x64 video, making bboxes and translating them to full-size videos.  Make this two-three phased pipeline.  Or try building bboxes from pre-trained models, but this is extremely difficult. </p><br><p>  We did more or less detect the objects, but decided not to go this way, because we considered it unreliable - we didn‚Äôt want to waste time on manual marking because of the huge amount of data, plus we didn‚Äôt believe that even 64x64 motion detection would be stable. </p><br><h2 id="6-bazovye-sovety-dlya-uchastnikov-sorevnovaniy">  6. Basic Tips for Competitors </h2><br><ul><li>  Try simple approaches </li><li>  Be sure you understand what works and why it works. </li><li>  Read all relevant paperwork on the topic, but do not spend too much time on one of the approaches if you are not sure that <br><ul><li>  it can be quickly and easily realized </li><li>  you are sure that it will work </li><li>  you can more or less combine the models from the works without implementing complex models from scratch </li></ul></li><li>  Not afraid of the complexity of the problem.  Modern libraries give you super power </li><li>  90% of what is written in the paper is garbage.  You do not have six months to test everything that you find.  In these works, the form is usually valued rather than the practice. </li><li>  Combining models should be done only at the very end, and then as the last thing you can think of </li><li>  Communicate with people, cooperate.  This is something like a stack of people) Together you study not twice as fast, but at ten. </li><li>  Share your code and approaches. </li></ul><br><h2 id="7-bazovye-sovety-dlya-issledovatelskih-i-prodakshn-modeley">  7. Basic Tips for Research and Production Models </h2><br><ul><li>  In 95% of cases, there are no stacks - usually it gives no more than 5-10% </li><li>  Your model and pipeline should be short, quick and convenient for engineers. </li><li>  All of the above is applicable for these purposes, but only taking into account the increased requirements for deployment. </li></ul><br><h2 id="8-nashe-oborudovanie">  8. Our equipment </h2><br><p>  We used 3 machines - my weak server with 1070Ti, but when we put the SSD there, it became limited by the size of the disk space.  There was also a Savva car with a weak GPU and a server of my friends with two 1080Ti. </p><br><div class="spoiler">  <b class="spoiler_title">Picture</b> <div class="spoiler_text"><p><img src="https://pics.spark-in.me/upload/b997952329176e30f8218e5794d39568.png" alt="https://pics.spark-in.me/upload/b997952329176e30f8218e5794d39568.png"></p></div></div><br><div class="spoiler">  <b class="spoiler_title">Cool vidos made by one of the participants (2nd place)</b> <div class="spoiler_text"><iframe width="560" height="315" src="https://www.youtube.com/embed/D8-t-gnBf1o" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div></div></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/348540/">https://habr.com/ru/post/348540/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../348526/index.html">How long is enough? Minimal passwords on the most popular sites</a></li>
<li><a href="../348530/index.html">V for validator</a></li>
<li><a href="../348532/index.html">How does Cisco monitor security on its internal network?</a></li>
<li><a href="../348536/index.html">7 ways to use groupingBy in Stream API</a></li>
<li><a href="../348538/index.html">25 materials about IaaS and virtual infrastructure: cases, guides and collections for developers</a></li>
<li><a href="../348542/index.html">Amazon S3 and all-all-all: choose object storage</a></li>
<li><a href="../348546/index.html">Why is it so hard to attract money in the open source?</a></li>
<li><a href="../348548/index.html">Selection: 7 Chrome extensions to bypass locks</a></li>
<li><a href="../348550/index.html">SLA philosophy: about query priorities</a></li>
<li><a href="../348552/index.html">Extending and using the Linux Crypto API</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>