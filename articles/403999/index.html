<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The cult of cargo for AI: the myth of superhuman artificial intelligence</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I heard that in the future, computer AIs will become so much smarter than us that they will take away all our jobs and resources, and people will die....">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The cult of cargo for AI: the myth of superhuman artificial intelligence</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/geektimes/post_images/f29/b7c/50b/f29b7c50b4fd97573f28f8508e94a9a0.jpg" alt="image"><br><br>  I heard that in the future, computer AIs will become so much smarter than us that they will take away all our jobs and resources, and people will die.  Is it so? <br><br>  This is the most common question I get asked on my speeches about AI.  The people who ask him are genuinely worried, and their anxiety comes from other people - experts who are asking the same question.  Among them you can meet the smartest of the living people today - for example, <a href="https://ru.wikipedia.org/wiki/%25D0%25A5%25D0%25BE%25D0%25BA%25D0%25B8%25D0%25BD%25D0%25B3,_%25D0%25A1%25D1%2582%25D0%25B8%25D0%25B2%25D0%25B5%25D0%25BD">Stephen Hawking</a> , <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B0%25D1%2581%25D0%25BA,_%25D0%2598%25D0%25BB%25D0%25BE%25D0%25BD">Ilon Musk</a> , <a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B5%25D0%25B3%25D0%25BC%25D0%25B0%25D1%2580%25D0%25BA,_%25D0%259C%25D0%25B0%25D0%25BA%25D1%2581">Max Tegmark</a> , <a href="https://ru.wikipedia.org/wiki/%25D0%25A5%25D0%25B0%25D1%2580%25D1%2580%25D0%25B8%25D1%2581,_%25D0%25A1%25D1%258D%25D0%25BC">Sam Harris</a> and <a href="https://ru.wikipedia.org/wiki/%25D0%2593%25D0%25B5%25D0%25B9%25D1%2582%25D1%2581,_%25D0%2591%25D0%25B8%25D0%25BB%25D0%25BB">Bill Gates</a> - and they all believe in the possibility of such a scenario.  At a recent conference devoted to the problems of AI, a committee of nine of the most knowledgeable people in the field of AI agreed that the appearance of a superhuman AI would soon be possible. <br><a name="habracut"></a><br><iframe width="560" height="315" src="https://www.youtube.com/embed/h0962biiZa4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      But this scenario of conquering the world of AI includes five assumptions that, as it turns out, after careful study of them, are not based on evidence.  These statements may be justified in the future, but now none of them have any evidence.  These assumptions are: <br><br>  1. AI is getting smarter than us, and its power is growing exponentially. <br>  2. We will create a general-purpose AI, similar to our own. <br>  3. We are able to create human intelligence based on silicon. <br>  4. Intellect is able to grow without limits. <br>  5. After the explosion of superintelligence, he will help us solve all our problems. <br><br>  As an objection to this orthodox canon, I will cite five heretical statements, which, it seems to me, are more justified. <br><br>  1. Intellect is not one-dimensional, so the concept of ‚Äúsmarter than people‚Äù does not make sense. <br>  2. Neither humans nor AI have any general consciousness. <br>  3. Emulation of human thinking on other media will be limited to the cost of its creation. <br>  4. The dimensions of the intellect are not infinite. <br>  5. Intellect is just one of the factors of progress. <br><br>  If the expectation of superhuman AI (FIC) is based on five key assumptions that have no evidence, then this idea is more like religious faith or myth.  Next, I will explain in detail each of my five counter-assumptions, and prove that the FIC is actually a myth. <br><br><h2>  one. </h2><br>  The most common misconception about AI begins with the misconception about natural intelligence.  It consists in that intelligence is one-dimensional.  Most techies are prone to portraying intelligence in the way that <a href="https://ru.wikipedia.org/wiki/%25D0%2591%25D0%25BE%25D1%2581%25D1%2582%25D1%2580%25D0%25BE%25D0%25BC,_%25D0%259D%25D0%25B8%25D0%25BA">Nick Bostrom</a> does in the book Super-Intelligence [Superintelligence] - as a one-dimensional line graph with increasing amplitude. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/2c1/36a/b2f/2c136ab2fc2e0fd8904bff67561b5252.png" alt="image"><br><br>  On one side is low intelligence, say, a small animal;  on the other - a high, let's say, a genius - as if the intellect can be represented as a sound level in decibels.  Of course, in this case it is easy to imagine that the loudness of the intellect continues to grow and eventually exceeds our highly intellectual level and becomes a super-loud intellect ‚Äî a roar!  - inaccessible to us and beyond the limits of the schedule. <br><br>  This model is topologically equivalent to a ladder on which each subsequent step of intelligence is one step higher than the previous one.  Younger animals are on the lower steps, and high-level AI will overtake us and be on the steps above.  The timeline of this event does not matter; only the ranking matters ‚Äî the metric of increasing intelligence. <br><img src="https://habrastorage.org/getpro/geektimes/post_images/f8e/b81/319/f8eb81319d9ae2b95c5e2d0b627c974c.jpg" alt="image"><br><br>  The problem with this model is that it is as mythical as the ladder of evolution.  Before Darwin, the natural world was viewed as a ladder on which younger animals were located below man.  Even after Darwin, it was customary to think of evolution as a ‚Äúladder‚Äù, according to which fish turned into reptiles, then mammals, then primates, into humans, and each stage is at a slightly higher ‚Äúevolutionary level‚Äù, and therefore considered smarter than the previous ones.  So the ladder of intellect is consistent with the ladder of existence.  But these models have a completely unscientific approach. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/57d/f3e/7ed/57df3e7eda77f345c53819dc8a1945bc.png" alt="image"><br><br>  A more accurate depiction of the natural evolution of a species is an expanding disk, as in the picture above, first proposed by <a href="https://en.wikipedia.org/wiki/David_Hillis">David Hillis</a> of the University of Texas, and based on DNA.  This genealogical <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B0%25D0%25BD%25D0%25B4%25D0%25B0%25D0%25BB%25D0%25B0">mandala</a> begins in the center with the most primitive forms of life, and then forks out in time.  Time moves outside, so the newest types of life that inhabit the planet today are around the perimeter of the circle.  This image emphasizes the fact of evolution, which is difficult to accept: each of the species living today is equally evolving.  People exist on this ring with cockroaches, clams, ferns, foxes and bacteria.  Each species has gone through an unbroken chain of three billion years of successful reproduction, which means that today's bacteria and cockroaches are as evolutionary developed as humans.  There is no stairs. <br><br>  Similarly, there is no ladder for the intellect.  Intellect is not one-dimensional.  This is a complex of many types and recognition modes, each of which is a continuum.  Take the simplest task of measuring the intelligence of animals.  If the intellect were one-dimensional, we would simply have to build in the right upward order the intellects of a parrot, dolphin, horse, squirrel, octopus, blue whale, cat, and gorilla.  But today we have no scientific evidence for the existence of such a line.  One of the reasons for this could be the lack of difference between the intellects of animals, but we also do not see this.  Zoology is full of amazing examples of differences in the thinking of animals.  But maybe they all have one relative "general-purpose intelligence"?  Perhaps, but for him we have no way of measuring and metrics.  We have many different metrics for many types of knowledge. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/5ff/9f5/eb6/5ff9f5eb643fccb4b764519d6a804739.png" alt="image"><br><br>  Instead of a single line with decibels, a more accurate model of intelligence will be a graph of its probabilistic space, such as the figure above, representing possible forms.  Intellect is a combinatorial continuum.  A set of nodes, each of which is a continuum, create complex and diverse structures in higher dimensions.  Some intelligences can be very complex and have a lot of thinking nodes.  Others may be easier, but reach out further, taking an angle in space.  These complexes, which we call intellects, can be viewed as symphonies involving many types of instruments.  They differ not only in volume, but also in tone, melody, color, tempo, etc.  They can be represented as an ecosystem.  And in this sense, the various component nodes of thinking depend on each other and are created together. <br><br>  As <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B8%25D0%25BD%25D1%2581%25D0%25BA%25D0%25B8%25D0%25B9,_%25D0%259C%25D0%25B0%25D1%2580%25D0%25B2%25D0%25B8%25D0%25BD_%25D0%259B%25D0%25B8">Marvin Minsky</a> said, human minds are communities of minds.  We work on thinking ecosystems.  We have many types of thinking inside that deal with many types of thinking: deduction, induction, symbolic logic, emotional intelligence, spatial logic, short-term memory and long-term memory.  The nervous system of our intestines is also a brain of some type with its mode of thinking.  We think not just with one brain, we think with our whole body. <br><br>  These sets of thinking vary from individual to individual and from species to species.  A squirrel can remember the exact location of several thousand acorns over the years, which completely impresses the human mind.  So in this kind of thinking, proteins are superior to humans.  This superpower is associated with other modes that are fading compared to ours, and this connection constitutes the mind of proteins.  In the animal kingdom, there are many other examples of possibilities that are superior to humans, and they are also included in different systems. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/2ba/9d4/63b/2ba9d463b13bdcb6d36b7c749a90a1af.jpg" alt="image"><br><br>  The same is true for AI.  Artificial minds are already superior to people in certain dimensions.  Your calculator is the genius of mathematics, Google‚Äôs memory already exceeds ours in a certain dimension.  We create AIs that stand out in certain modes.  We are able to do some of these things ourselves, but they do it better - for example, in the fields of probability or mathematics.  Other modes of thinking are not available to us - only a search engine can remember every word on six billion web pages.  In the future, we will come up with completely new types of thinking that we do not have, and indeed in nature.  Inventing artificial flight, we were inspired by biological flight regimes, mainly flapping of wings.  But we invented such flights ‚Äî with propellers and fixed wings ‚Äî that were unknown to the biological world.  This is someone else's flight.  In the same way, we will invent new modes of thinking that do not exist in nature.  In many cases, they will be new, specific, small modes designed for a particular work - perhaps a type of reasoning suitable only for statistics and probability theory. <br><br>  In other cases, the new mind will be a complex set of such types of thinking that we can use to solve problems that are not amenable to our intellect.  Many of the hardest problems of business and science may require a two-step solution.  The first stage: to invent a new mode of thinking, able to work with our mind.  Second: combine them to solve the problem.  As we solve problems that are not available to us earlier, we want to call this new type of thinking ‚Äúsmarter‚Äù than we are, but in fact it is just different from us.  The main advantage of AI is a different thinking from ours.  I think it is useful to think of AI as alien thinking (or artificial aliens).  Its singularity will be its main advantage. <br><br>  At the same time, we will integrate these different modes of thinking into more complex mind communities.  Some of these complexes will be more difficult than ours, because they will be able to solve problems that are inaccessible to us, and then someone wants to call them superhuman.  But we do not call Google CII, although its memory is beyond ours, since there are many things that we can do better than it.  These AI systems can surpass us in many dimensions, but none can do everything we do better than us.  This can be compared with the physical abilities of people.  The industrial revolution is already 200 years old, and although on the whole machines exceed the physical abilities of a person (speed of movement, weight lifting, precise cutting, etc.), there is no machine capable of surpassing the average person in everything he does. <br><br>  And although the community of minds in AI is becoming more and more complex, so far this complexity is difficult to measure scientifically.  We do not have good metrics for complexity that can determine whether a cucumber is more difficult than a Boeing 747 aircraft, or describe how their complexity differs.  This is one of the reasons why we don‚Äôt have good metrics for the mind.  It will be very difficult to determine whether mind A is more difficult than mind B, and therefore it is difficult to understand which of them is smarter.  Soon we will come to understand that the mind is not one-dimensional, and we are actually interested in all the many ways in which the intellect is able to function - all those nodes of knowledge that we have not yet discovered. <br><br><h2>  2 </h2><br>  The second error associated with human intelligence is our belief that our mind is universal, it is a general purpose mind.  This belief led to the appearance of the often claimed goal of AI researchers to create a generalized AI, OII.  However, if we consider the intellect as more space of possibilities, it does not have a generalized state.  The human intellect is not in a certain central position around which other specialized intellects revolve.  The human intellect is a very, very special type of intellect, which appeared as a result of millions of years of evolution in order for our species to survive on this planet.  If you place it in the space of all possible intelligences, it will be somewhere in the corner, just like our world is on the edge of a huge galaxy. <br><br>  We can certainly imagine and even invent a mind that resembles a universal Swiss knife.  He copes well with a bunch of tasks, but he does not cope with any of them very well.  The AI ‚Äã‚Äãwill follow the same engineering <a href="https://ru.wikipedia.org/wiki/%25D0%2590%25D1%2584%25D0%25BE%25D1%2580%25D0%25B8%25D0%25B7%25D0%25BC">maxim</a> , which all things created or born follow: you cannot optimize each dimension.  Only compromises are possible.  It will not be possible to create a generalized multifunctional unit, superior specialized.  A big mind, ‚Äúcapable of everything,‚Äù cannot do it all as well as specialized ones.  Since we believe in the generalization of our mind, we believe that thinking is not obliged to follow engineering compromises, that it will be possible to create intelligence that maximizes all modes of thinking.  But this is no evidence.  We have not yet invented enough variants of the mind to see the whole space (and so far we have dismissed the minds of animals, evaluating them with one-dimensional amplitude). <br><br><h2>  3 </h2><br>  Part of this belief comes from the concept of universal computing.  This assumption, formally described in 1950 as the ‚Äú <a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B5%25D0%25B7%25D0%25B8%25D1%2581_%25D0%25A7%25D1%2591%25D1%2580%25D1%2587%25D0%25B0_%25E2%2580%2594_%25D0%25A2%25D1%258C%25D1%258E%25D1%2580%25D0%25B8%25D0%25BD%25D0%25B3%25D0%25B0">Church-Turing thesis,</a> ‚Äù asserts that all calculations that reach a certain threshold are equivalent.  Therefore, for all calculations, there is a universal core, and whether they occur in one machine with many fast parts, or slow parts, or in a biological brain, these are all the same logical process.  And this means that it is possible to emulate any computational process (thinking) in any machine capable of ‚Äúuniversal‚Äù computations.  The adherents of singularity are based on this principle, expecting that we can create silicon brains that can accommodate the human mind, and that we can create an artificial mind that thinks like humans, only much smarter.  This hope is worth skepticism because it is based on a misunderstanding of the Church-Turing hypothesis. <br><br>  The starting point of this hypothesis is as follows: "In the presence of an infinite film (memory) and time, all calculations are equivalent."  The problem is that in reality the computer does not have infinite memory and time.  In the real world, real time is crucial, often even a matter of life and death.  Yes, all thinking can be equivalent if time is ignored.  Yes, you can emulate human thinking on any matrix, as long as you ignore time or the real limitations of data storage and memory.  However, by including time, it will be possible to significantly redefine this principle: "Two computing systems running on very different platforms will not be equivalent in time."  Or: ‚ÄúThe only way to get similar thinking patterns is to run them on equivalent platforms.‚Äù  The physical matter on which calculations work - especially when they become more complex - drastically affects the type of thinking that can function successfully in real time. <br><br>  I will continue these arguments and say that the only way to get close to the human process of thinking is to run the calculations on a soft and damp cloth, similar to a human one.  And this means that very large and complex AI, working on dry silicon, will give us big, complex and inhuman types of thinking.  If it becomes possible to create an artificial wet brain using grown human-like neurons, then I would say that his thoughts will be very similar to ours.  The benefits of such a wet brain are proportional to how similar we can make the foundation.  The cost of creating such a ‚Äúhuman computer‚Äù is enormous, and the closer the tissue is to the brain tissue, the more cost effective it will be to simply create a person.  In the end, we can do this in nine months. <br><br>  Moreover, as stated above, we think with the help of the whole body, and not just with just one brain.  We have a wealth of data showing that the nervous system of the intestine guides our "rational" decision-making process, that it is capable of learning and predicting events.  The more we model the human body system, the closer we come to its reproduction.  An intellect that works on a very different body (on dry silicon instead of wet carbon) will think differently. <br><br>  It seems to me that this is not a bug, but a feature.  As I mentioned in paragraph 2, the difference between thinking and human is the main advantage of AI.  This is another reason why it is wrong to say that he is "smarter than people." <br><br><h2>  four. </h2><br>  At the heart of the concept of superhuman intelligence ‚Äî in particular, the notion that such intelligence will constantly improve ‚Äî lies the belief that the scale of intelligence is infinite.  I do not see evidence of this.  An erroneous idea of ‚Äã‚Äãthe one-dimensionality of the intellect helps to believe this - but this is only faith.  There are no reliably infinite physical dimensions in science known to science.  The temperature is not infinite - there is a finite cold and a <a href="https://geektimes.ru/post/280530/">finite heat</a> .  Time and space are finite.  The final speed.  The numerical line in math may be infinite, but physical attributes have limitations.  It is reasonable to say that the mind itself is finite.  So the question is, where is the limit of intelligence?  We tend to believe that he is far beyond our reach, he is much ‚Äúhigher‚Äù than us, as far as we are ‚Äúhigher‚Äù than an ant.  If we leave the recurring problem of one-dimensionality, what evidence do we have that we have not already reached this limit?  Why can't we be at the maximum?  Or perhaps this limit is not far from us?  Why do we believe that intelligence is something that can expand forever? <br><br>  It is much better to approach this issue, considering our intellect as one of the <a href="https://aeon.co/essays/beyond-humans-what-other-kinds-of-minds-might-be-out-there">many possible types of intelligence</a> .  And even if every dimension of thinking and computing has a limitation, with the existence of hundreds of dimensions, there can be innumerable minds ‚Äî none of which will be infinite in any dimension.  When we meet or create these variants of minds, we can decide that some of them are superior to ours.  In my last book, The Inevitable [ <a href="https://www.amazon.com/Inevitable-Understanding-Technological-Forces-Future/dp/0525428089%3Fie%3DUTF8%26redirect%3Dtrue%26tag%3Dcooltools-20">The Inevitable</a> ], I outlined some types of such minds that can be somewhat superior to ours.  Here is a partial list: <br><br>  ‚Ä¢ Mind, similar to human, but working faster (this kind of AI is easiest to imagine). <br>  ‚Ä¢ Very slow mind, characterized by huge amounts of data storage. <br>  ‚Ä¢ The global superbrain, made up of millions of smoothly working primitive minds. <br>  ‚Ä¢ The overmind of the hive, consisting of many intelligent minds that are not aware of their belonging to the hive. <br>  ‚Ä¢ Cybernetic mind, consisting of intelligent minds, aware of their belonging to the collective mind. <br>  ‚Ä¢ A mind specifically designed to improve the performance of your mind personally, and useless for someone else. <br>  ‚Ä¢ Reason, able to imagine a more perfect mind, but unable to make it. <br>  ‚Ä¢ Reason, capable of making a perfect mind, but not aware of itself sufficiently to represent it. <br>  ‚Ä¢ The mind, capable of once successfully create a more perfect mind. <br>  ‚Ä¢ Reason, able to successfully create a more perfect mind, which in turn is able to create an even more perfect mind, etc. <br>  ‚Ä¢ A mind that has access to its source code and is able to influence its work. <br>  ‚Ä¢ Superlogical mind without emotions. <br>  ‚Ä¢ The mind, capable of solving common problems, but not aware of itself. <br>  ‚Ä¢ Self-conscious mind unable to solve common problems. <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Ä¢ Reason, for the development of which takes a lot of time and protects his mind. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Ä¢ Extremely slow mind, distributed over a large physical volume, in connection with which it is invisible to faster minds. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Ä¢ Mind capable of fast and accurate self-cloning. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Ä¢ A mind capable of self-cloning, after which it can remain combined with its clones. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Ä¢ Immortal mind, able to migrate between platforms. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Ä¢ Fast and dynamic mind, able to change the process and characteristics of their work. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Ä¢ Nano reason, the smallest possible from an energetic and physical point of view. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Ä¢ Mind specializing in predictions of events. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Ä¢ Reason, never erasing or forgetting anything.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Ä¢ Symbiotic mind in half-animal half-animal. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Ä¢ Cybernetic intelligence in half-half human. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Ä¢ Mind using quantum computing, with inaccessible logic to us. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Some are willing to call any mind from this list of FIC, but the very diversity and alienity of such minds force us to develop new terms and concepts about the intellect and mind. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Then, believers in FIC assume an exponential growth of intelligence (for some indefinite metric), possibly because they believe that it is already developing exponentially. However, so far there is no evidence that intelligence develops exponentially, no matter how measured. By exponential growth, I mean growth, at which the AI ‚Äã‚Äãdoubles its power at certain regular intervals.</font></font> Where is the proof?<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I can not find them. If they are not there today, why is it assumed that they will appear? As an exhibitor, so far only the amount of input data for AI, as well as resources devoted to the production of the highest intelligence, is growing. But the results of their work do not grow according to Moore's law. AIs don't get twice as smart every three years or even every 10 years. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I asked many experts on AI to provide evidence of the exponential growth of AI capabilities, but they all agreed that we do not have a metric for AI, and in general it does not work that way. When I asked </font></font><a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D1%2583%25D1%2580%25D1%2586%25D0%25B2%25D0%25B5%25D0%25B9%25D0%25BB,_%25D0%25A0%25D1%258D%25D0%25B9%25D0%25BC%25D0%25BE%25D0%25BD%25D0%25B4"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ray Kurzweil</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, the exponential wizard himself, where you can take evidence of the exponential growth of AI, he replied to me that the AI ‚Äã‚Äãdoes not grow exponentially, it grows in levels. He wrote: ‚ÄúIt requires an exponential improvement in computation and algorithmic complexity to add an additional level in the hierarchy ... So we can expect linear additions of levels, because adding each level requires exponentially more complexity, and the progress of our capabilities in this area is actually growing exponentially. There are not many levels left for us to reach the capabilities of the cerebral cortex, so my prediction for 2029 is still good for me.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Apparently, Ray wants to say that it is not the capabilities of the AI ‚Äã‚Äãthat are growing exponentially, but our costs of creating it, and its output simply rises by one level each time. This assumption is almost opposite to the exponential growth of intelligence. In the future, this may change, but today the AI ‚Äã‚Äãis clearly not growing exponentially. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Therefore, imagining an ‚Äúexplosion of intellect‚Äù, we should not present it as a chain reaction, but as a scattering reproduction of new variants. </font></font><a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25B5%25D0%25BC%25D0%25B1%25D1%2580%25D0%25B8%25D0%25B9%25D1%2581%25D0%25BA%25D0%25B8%25D0%25B9_%25D0%25B2%25D0%25B7%25D1%2580%25D1%258B%25D0%25B2"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The Cambrian explosion</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , not a nuclear explosion. The results of the accelerating technology will most likely not be superhuman, but extrahuman. Out of our experience, but not necessarily "above" him.</font></font><br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/3ac/61d/4aa/3ac61d4aadb2feb04c55657530506604.jpg" alt="image"><br><br><h2>  five. </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Another, unreliable belief in the coming of the FIC, not supported by evidence, is that the superintelligence of almost infinite power will quickly solve all our basic unsolved problems. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Many supporters of the explosion of intelligence believe that it will lead to an explosion of progress. I call this mythic faith "thinkism". This is the erroneous idea that the future steps of progress are unattainable only because of a lack of mental powers or intelligence. I note that the belief that thinking is a magic panacea for everything is common among many people who like to think.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Take a cure for cancer or prolong life. Such problems can not be solved by thinking. No speculation is enough to understand how cells age or how telomeres fall away. No superfood intellect can understand how a human body works simply by reading all the famous scientific literature in the world and considering it. No FIC can, simply thinking about all experiments with nuclear fusion, produce a working nuclear fusion scheme. To overcome the path from not knowing how something works, to understanding how something works, it takes much more than just thinking. In the real world, heaps of experiments are produced, each of which leads to the appearance of mountains of conflicting data that require further experiments to create a working hypothesis. Thinking about potential data will not give you the right data.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Thinking is only a part of science, perhaps even a small part of it. For example, we lack the necessary data to get closer to solving the problem of death. When working with living organisms, almost all experiments take time. Slow cell metabolism cannot be accelerated. Getting results takes years, months, at least days. If we want to find out what happens to subatomic particles, we cannot just think about them. We need to build huge, complex, cunning physical structures to find out. Even if the smartest physicists would be 1000 times smarter than they are now, without the collider they would not have learned anything new.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There is no doubt that FIC can accelerate the progress of science. We can do computer simulations of atoms or cells and speed them up many times, but two problems limit the utility of simulations. The first is that simulations and models can go faster than the studied processes only because we do not take into account something. This is the essence of the model or simulation. In addition, testing, testing and proof of these models also takes time and it needs to go at the speed of the simulated processes. Verification of the truth can not be accelerated.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">These simplified versions are useful in filtering out the most promising paths and speeding up progress. But in reality there is no excessiveness; everything that is real has its effect to some extent; this is one of the definitions of reality. If you start pumping up models and simulations with more and more data, it soon becomes clear that reality works faster than its 100% simulation. This is another definition of reality: the fastest possible version of all the present details and degrees of freedom. If you can model all the molecules in the cells and all the cells in the human body, then the simulation will not work as fast as the human body. No matter how much you think about it, you will need time to experiment, be it a real system or a simulation.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To be useful, AI needs to infiltrate the real world, and the world will set the speed of its innovations. Without conducting experiments, building prototypes, errors, and experimenting with reality, the intellect may think, but not produce results. No instant discoveries per minute, hour, day or year of the so-called appearance. "Superhuman AI" is not expected. Of course, advances in the development of AI will significantly increase the number of discoveries per unit of time, in particular, since AI unlike humans will ask questions that a person would not ask, but even extremely powerful intelligence, compared to us, does not guarantee instant progress. Solving problems requires much more than just intelligence.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Intellect alone cannot solve not only the problems of cancer and longevity, but also the problems of the intellect itself. The typical mantra of singularity supporters is that as soon as you make an AI that is ‚Äúsmarter than a man‚Äù, he suddenly thinks how and invents an AI, ‚Äúsmarter than himself,‚Äù who, in turn, will think even more properly and invent more intelligent AI, and as a result, everything will end with an explosion of intellectual power at an almost divine level. We have no evidence that just thinking about intelligence is enough to create new levels of intelligence. Such thinking is faith. We have a lot of evidence that, in addition to a large amount of knowledge, we need experiments, data, tests and mistakes, unusually posed questions and everything else beyond simple cleverness,to invent new kinds of successfully operating minds.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In conclusion, I can say that I could be wrong with my statements. We are in the early stages. We can open a universal metric for intelligence; open its infinity in all directions. Since we know very little about what intelligence is (not to mention consciousness), the probability of an AI singularity exceeds zero. It seems to me that all the evidence speaks in favor of a small probability of such a scenario, but this probability is non-zero. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So, although I disagree with his probability, I agree with the broader goals of </font></font><a href="https://ru.wikipedia.org/wiki/OpenAI"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenAI</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and intelligent people who are worried about FID - so that we need to create friendly AI and think about how to instill in them the values ‚Äã‚Äãthat coincide with ours. And although I think that FIC is a very distant existential threat (which should be considered), I think that its small probability (based on the available evidence) should not control our science, politics and development. Colliding an asteroid with Earth would be a disaster. Its probability is greater than zero (so we must support </font></font><a href="https://en.wikipedia.org/wiki/B612_Foundation"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">the B612 fund</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), but we should not allow the asteroid impact to control our decisions, for example, in the field of climate change, or space travel, or even urban planning.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Similarly, the evidence so far suggests that AI will most likely prove to be not superhuman, but extra-human, it will be hundreds of new types of thinking, different from human, none of which will be general-purpose AI, and none of them will become god, instantly solving all our problems. Instead, we will have a galaxy of limited intelligences working in unfamiliar dimensions, going beyond our capabilities in many of them, working together with us to solve existing problems and create new ones over time.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I understand the charm and appeal of the FIC God. This is something like Superman. But, like Superman, this is a mythical figure. Somewhere in the Universe Superman may exist, but the probability of its existence is extremely small. But myths can be useful, and not disappear after their invention. The idea of ‚Äã‚ÄãSuperman will not die. The idea of ‚Äã‚Äãa superhuman AI-singularity, once it originated, also will not die. But we must understand that now it is a religious, not a scientific idea. If we study the data we have today for intelligence, artificial and natural, we can only conclude that our reasoning about the mythical FII-god is just myths. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Many isolated islands of </font></font><a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B8%25D0%25BA%25D1%2580%25D0%25BE%25D0%25BD%25D0%25B5%25D0%25B7%25D0%25B8%25D1%258F"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Micronesia</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">first touched the rest of the world during the Second World War. The alien gods flew in their skies in noisy birds, dumped food and goods on their islands, and did not return. Religious cults appeared on the islands, praying for the gods to return and throw off more cargo. Even now, fifty years later, many are still waiting for the return of the goods. It is possible that FIC may be another cargo cult. After a hundred years, people will be able to look back at our time, at the time when the believers began to wait for the appearance of the AII, which will bring them unimaginable benefits. Decade after decade, they are waiting for the arrival of the FIC, confident that he and his benefits should appear very soon.</font></font><br><br>      .     ,   ,       ,        ‚Äì     , , , ,    ‚Äì     ,   , ,   .          ,            .      ,          (  ,   ), -,    ‚Äì    ,   . </div><p>Source: <a href="https://habr.com/ru/post/403999/">https://habr.com/ru/post/403999/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../403989/index.html">The path from the camp robotics to the creator of cyborgs</a></li>
<li><a href="../403991/index.html">Erythritol is a carbohydrate-free sugar for diabetics that does not affect the glycemic index.</a></li>
<li><a href="../403993/index.html">Ask Ethan: Why do sunshine look like sunshine?</a></li>
<li><a href="../403995/index.html">Here it is, our summer: vacation gadgets</a></li>
<li><a href="../403997/index.html">Set of young biohacker</a></li>
<li><a href="../404003/index.html">Chemistry of the computer world</a></li>
<li><a href="../404005/index.html">Small spool and expensive: in-ear headphones from Fostex</a></li>
<li><a href="../404007/index.html">GPD Win - explore a miniature laptop with a diagonal of 5.5 ", designed for games and emulators</a></li>
<li><a href="../404009/index.html">We manage the house through Telegram</a></li>
<li><a href="../404011/index.html">Savings on blood: a new biomaterial delivery system has been developed in the laboratory</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>